<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 87]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.LG](#cs.LG) [Total: 112]
- [cs.RO](#cs.RO) [Total: 49]
- [cs.AI](#cs.AI) [Total: 36]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Leveraging NTPs for Efficient Hallucination Detection in VLMs](https://arxiv.org/abs/2509.20379)
*Ofir Azachi,Kfir Eliyahu,Eyal El Ani,Rom Himelstein,Roi Reichart,Yuval Pinter,Nitay Calderon*

Main category: cs.CV

TL;DR: NTP-based lightweight detectors can efficiently detect VLM hallucinations; augmenting with linguistic NTPs and incorporating VLM scores yields best performance.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in vision-language models (VLMs) undermine reliability and detection is typically costly; aim for a fast, on-the-fly mechanism using internal signals (next-token probabilities, NTPs) to predict hallucinations.

Method: Create a dataset of 1,400 human-annotated VLM-generated statements labeled as hallucinated or not. Compute NTP features from the VLM, augment with linguistic NTPs (probabilities obtained by feeding only the generated text back into the VLM). Train traditional ML models on these features. Also experiment with integrating hallucination scores from VLMs into the NTP-based models.

Result: NTP features are effective predictors of hallucinations; lightweight ML models achieve performance comparable to strong VLMs. Adding linguistic NTPs improves detection. Combining VLM-based scores with NTP features yields better performance than either approach alone.

Conclusion: NTP-based, lightweight methods can reliably detect VLM hallucinations in real time, and their effectiveness can be enhanced by linguistic signals and by combining with VLM-derived scores, offering a practical path to more reliable VLM systems.

Abstract: Hallucinations of vision-language models (VLMs), which are misalignments
between visual content and generated text, undermine the reliability of VLMs.
One common approach for detecting them employs the same VLM, or a different
one, to assess generated outputs. This process is computationally intensive and
increases model latency. In this paper, we explore an efficient on-the-fly
method for hallucination detection by training traditional ML models over
signals based on the VLM's next-token probabilities (NTPs). NTPs provide a
direct quantification of model uncertainty. We hypothesize that high
uncertainty (i.e., a low NTP value) is strongly associated with hallucinations.
To test this, we introduce a dataset of 1,400 human-annotated statements
derived from VLM-generated content, each labeled as hallucinated or not, and
use it to test our NTP-based lightweight method. Our results demonstrate that
NTP-based features are valuable predictors of hallucinations, enabling fast and
simple ML models to achieve performance comparable to that of strong VLMs.
Furthermore, augmenting these NTPs with linguistic NTPs, computed by feeding
only the generated text back into the VLM, enhances hallucination detection
performance. Finally, integrating hallucination prediction scores from VLMs
into the NTP-based models led to better performance than using either VLMs or
NTPs alone. We hope this study paves the way for simple, lightweight solutions
that enhance the reliability of VLMs.

</details>


### [2] [Quasi-Synthetic Riemannian Data Generation for Writer-Independent Offline Signature Verification](https://arxiv.org/abs/2509.20420)
*Elias N. Zois,Moises Diaz,Salem Said,Miguel A. Ferrer*

Main category: cs.CV

TL;DR: A quasi-synthetic data generation framework in SPD (Riemannian) space improves writer-independent offline signature verification by using a small set of genuine SPD samples to generate synthetic writers via a Riemannian Gaussian Mixture, enabling effective metric learning with intra- and cross-dataset evaluation.


<details>
  <summary>Details</summary>
Motivation: Handwritten signature verification, especially writer-independent, struggles to generalize to unseen writers. Geometric representations (SPD covariances on Riemannian manifolds) are promising, but rely on real data for classifier training. The work aims to bridge data scarcity by generating synthetic data in SPD space.

Method: Construct SPD-based covariance descriptors from genuine samples. Fit a Riemannian Gaussian Mixture to seed SPD data; treat mixture centers as synthetic writers and variances as writer properties. Sample from centers to produce positive/negative SPD samples. Learn a metric using similar/dissimilar SPD pairs and evaluate on real datasets with intra- and cross-dataset protocols.

Result: Experiments on two popular signature datasets (Western and Asian styles) show low error rates and good generalization in both intra- and cross-dataset settings, validating the efficacy of quasi-synthetic SPD data for writer-independent verification.

Conclusion: Quasi-synthetic data generation in Riemannian SPD space can effectively augment training for writer-independent signature verification, highlighting the potential of synthetic data generation in geometric feature spaces.

Abstract: Offline handwritten signature verification remains a challenging task,
particularly in writer-independent settings where models must generalize across
unseen individuals. Recent developments have highlighted the advantage of
geometrically inspired representations, such as covariance descriptors on
Riemannian manifolds. However, past or present, handcrafted or data-driven
methods usually depend on real-world signature datasets for classifier
training. We introduce a quasi-synthetic data generation framework leveraging
the Riemannian geometry of Symmetric Positive Definite matrices (SPD). A small
set of genuine samples in the SPD space is the seed to a Riemannian Gaussian
Mixture which identifies Riemannian centers as synthetic writers and variances
as their properties. Riemannian Gaussian sampling on each center generates
positive as well as negative synthetic SPD populations. A metric learning
framework utilizes pairs of similar and dissimilar SPD points, subsequently
testing it over on real-world datasets. Experiments conducted on two popular
signature datasets, encompassing Western and Asian writing styles, demonstrate
the efficacy of the proposed approach under both intra- and cross- dataset
evaluation protocols. The results indicate that our quasi-synthetic approach
achieves low error rates, highlighting the potential of generating synthetic
data in Riemannian spaces for writer-independent signature verification
systems.

</details>


### [3] [Seedream 4.0: Toward Next-generation Multimodal Image Generation](https://arxiv.org/abs/2509.20427)
*Team Seedream,Yunpeng Chen,Yu Gao,Lixue Gong,Meng Guo,Qiushan Guo,Zhiyao Guo,Xiaoxia Hou,Weilin Huang,Yixuan Huang,Xiaowen Jian,Huafeng Kuang,Zhichao Lai,Fanshi Li,Liang Li,Xiaochen Lian,Chao Liao,Liyang Liu,Wei Liu,Yanzuo Lu,Zhengxiong Luo,Tongtong Ou,Guang Shi,Yichun Shi,Shiqi Sun,Yu Tian,Zhi Tian,Peng Wang,Rui Wang,Xun Wang,Ye Wang,Guofeng Wu,Jie Wu,Wenxu Wu,Yonghui Wu,Xin Xia,Xuefeng Xiao,Shuang Xu,Xin Yan,Ceyuan Yang,Jianchao Yang,Zhonghua Zhai,Chenlin Zhang,Heng Zhang,Qi Zhang,Xinyu Zhang,Yuwei Zhang,Shijia Zhao,Wenliang Zhao,Wenjia Zhu*

Main category: cs.CV

TL;DR: Seedream 4.0 is a unified, high-efficiency multimodal image generation system that handles text-to-image, image editing, and multi-image composition with fast inference and high-resolution output.


<details>
  <summary>Details</summary>
Motivation: To unify T2I, editing, and multi-image composition in a single framework with high quality and fast inference, enabling interactive and professional-grade creative workflows.

Method: A diffusion transformer with a powerful VAE to compress image tokens, large-scale pretraining on billions of text-image pairs, diverse data collection across hundreds of vertical scenarios, multi-modal post-training via a fine-tuned VLM for T2I and editing tasks, and inference accelerations including adversarial distillation, distribution matching, quantization, and speculative decoding.

Result: Seedream 4.0 achieves state-of-the-art results on T2I and multimodal editing, supports multi-image references and multiple outputs, enables high-resolution generation (1K-4K) with fast inference (as low as 1.8 seconds for 2K), and demonstrates strong generalization and potential for professional applications.

Conclusion: Seedream 4.0 extends traditional T2I into a more interactive, multimodal tool capable of precise editing and complex multi-reference generation, representing a significant step toward unified, high-performance multimodal generative AI.

Abstract: We introduce Seedream 4.0, an efficient and high-performance multimodal image
generation system that unifies text-to-image (T2I) synthesis, image editing,
and multi-image composition within a single framework. We develop a highly
efficient diffusion transformer with a powerful VAE which also can reduce the
number of image tokens considerably. This allows for efficient training of our
model, and enables it to fast generate native high-resolution images (e.g.,
1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning
diverse taxonomies and knowledge-centric concepts. Comprehensive data
collection across hundreds of vertical scenarios, coupled with optimized
strategies, ensures stable and large-scale training, with strong
generalization. By incorporating a carefully fine-tuned VLM model, we perform
multi-modal post-training for training both T2I and image editing tasks
jointly. For inference acceleration, we integrate adversarial distillation,
distribution matching, and quantization, as well as speculative decoding. It
achieves an inference time of up to 1.8 seconds for generating a 2K image
(without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream
4.0 can achieve state-of-the-art results on both T2I and multimodal image
editing. In particular, it demonstrates exceptional multimodal capabilities in
complex tasks, including precise image editing and in-context reasoning, and
also allows for multi-image reference, and can generate multiple output images.
This extends traditional T2I systems into an more interactive and
multidimensional creative tool, pushing the boundary of generative AI for both
creativity and professional applications. Seedream 4.0 is now accessible on
https://www.volcengine.com/experience/ark?launch=seedream.

</details>


### [4] [A Contrastive Learning Framework for Breast Cancer Detection](https://arxiv.org/abs/2509.20474)
*Samia Saeed,Khuram Naveed*

Main category: cs.CV

TL;DR: A semi-supervised contrastive learning framework using ResNet-50 on large unlabeled mammograms with a small labeled set achieves state-of-the-art breast cancer detection (96.7% accuracy) on INbreast and MIAS datasets.


<details>
  <summary>Details</summary>
Motivation: Overcome limited labeled data in mammography to build accurate deep learning-based CAD systems for early breast cancer detection.

Method: Train ResNet-50 under a contrastive learning (semi-supervised) regime using similarity-based objectives on a large pool of unlabeled mammograms, apply extensive augmentations, and fine-tune on a small labeled dataset.

Result: Achieves 96.7% accuracy in detecting breast cancer on benchmark datasets INbreast and MIAS, outperforming existing state-of-the-art.

Conclusion: Semi-supervised contrastive learning with unlabeled mammograms can significantly boost breast cancer detection performance when labeled data is scarce, suggesting practical value for CAD systems.

Abstract: Breast cancer, the second leading cause of cancer-related deaths globally,
accounts for a quarter of all cancer cases [1]. To lower this death rate, it is
crucial to detect tumors early, as early-stage detection significantly improves
treatment outcomes. Advances in non-invasive imaging techniques have made early
detection possible through computer-aided detection (CAD) systems which rely on
traditional image analysis to identify malignancies. However, there is a
growing shift towards deep learning methods due to their superior
effectiveness. Despite their potential, deep learning methods often struggle
with accuracy due to the limited availability of large-labeled datasets for
training. To address this issue, our study introduces a Contrastive Learning
(CL) framework, which excels with smaller labeled datasets. In this regard, we
train Resnet-50 in semi supervised CL approach using similarity index on a
large amount of unlabeled mammogram data. In this regard, we use various
augmentation and transformations which help improve the performance of our
approach. Finally, we tune our model on a small set of labelled data that
outperforms the existing state of the art. Specifically, we observed a 96.7%
accuracy in detecting breast cancer on benchmark datasets INbreast and MIAS.

</details>


### [5] [Are Foundation Models Ready for Industrial Defect Recognition? A Reality Check on Real-World Data](https://arxiv.org/abs/2509.20479)
*Simon Baeuerle,Pratik Khanna,Nils Friederich,Angelo Jovin Yamachui Sitcheu,Damir Shakirov,Andreas Steimer,Ralf Mikut*

Main category: cs.CV

TL;DR: Foundation models fail on real-world industrial QA data but work on public benchmarks; domain shift undermines zero-shot generalization for automated quality inspection.


<details>
  <summary>Details</summary>
Motivation: Assess whether recent foundation models can serve as zero-shot, generic quality-inspection tools across diverse products, potentially reducing labeling efforts.

Method: Evaluate multiple recent foundation models on two data sources: custom real-world industrial images and public datasets; compare zero-shot anomaly descriptions from prompts against ground truth; analyze cross-domain generalization.

Result: All tested FMs fail on real-world industrial data while performing well on public benchmark datasets.

Conclusion: Cautious optimism for FM generalization in real-world QA; domain-specific adaptation, data curation, or fine-tuning likely needed; public benchmarks may overstate generalization capabilities.

Abstract: Foundation Models (FMs) have shown impressive performance on various text and
image processing tasks. They can generalize across domains and datasets in a
zero-shot setting. This could make them suitable for automated quality
inspection during series manufacturing, where various types of images are being
evaluated for many different products. Replacing tedious labeling tasks with a
simple text prompt to describe anomalies and utilizing the same models across
many products would save significant efforts during model setup and
implementation. This is a strong advantage over supervised Artificial
Intelligence (AI) models, which are trained for individual applications and
require labeled training data. We test multiple recent FMs on both custom
real-world industrial image data and public image data. We show that all of
those models fail on our real-world data, while the very same models perform
well on public benchmark datasets.

</details>


### [6] [Shared Neural Space: Unified Precomputed Feature Encoding for Multi-Task and Cross Domain Vision](https://arxiv.org/abs/2509.20481)
*Jing Li,Oskar Bartosz,Chengyu Wang,Michal Wnuczynski,Dilshan Godaliyadda,Michael Polley*

Main category: cs.CV

TL;DR: Proposes a universal Neural Space (NS) with an encoder-decoder that pre-computes features across vision/imaging tasks, enabling multiple modules to share a common feature space. Uses a lightweight CNN backbone to enable efficient, hardware-friendly multi-task pipelines, demonstrated on modules like demosaicing, denoising, depth estimation, and semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: Current practice trains task-specific models that map inputs to disparate latent domains, which is inefficient for applications requiring sequences of modular tasks. A shared, transformation-aware feature space could reduce redundancy, improve generalization across domain shifts, and enable efficient multi-task vision pipelines.

Method: An encoder-decoder framework builds a universal feature space (NS) with transformation-aware representations. The backbone is lightweight CNN-based rather than a large transformer. Downstream modules (e.g., demosaicing, denoising, depth estimation, semantic segmentation) operate within the NS, sharing features.

Result: Conceptual framework with reduced redundancy and improved generalization across domain shifts. Demonstrates feasibility of implementing diverse imaging/vision modules within the NS to enable efficient, multi-task pipelines; no numerical benchmarks reported in the abstract.

Conclusion: The universal Neural Space provides a foundation for efficient, modular multi-task vision systems by sharing a common feature space across tasks, leveraging a lightweight CNN backbone for broader hardware compatibility.

Abstract: The majority of AI models in imaging and vision are customized to perform on
specific high-precision task. However, this strategy is inefficient for
applications with a series of modular tasks, since each requires a mapping into
a disparate latent domain. To address this inefficiency, we proposed a
universal Neural Space (NS), where an encoder-decoder framework pre-computes
features across vision and imaging tasks. Our encoder learns transformation
aware, generalizable representations, which enable multiple downstream AI
modules to share the same feature space. This architecture reduces redundancy,
improves generalization across domain shift, and establishes a foundation for
effecient multi-task vision pipelines. Furthermore, as opposed to larger
transformer backbones, our backbone is lightweight and CNN-based, allowing for
wider across hardware. We furthur demonstrate that imaging and vision modules,
such as demosaicing, denoising, depth estimation and semantic segmentation can
be performed efficiently in the NS.

</details>


### [7] [Data-Efficient Stream-Based Active Distillation for Scalable Edge Model Deployment](https://arxiv.org/abs/2509.20484)
*Dani Manjah,Tim Bary,Benoît Gérin,Benoît Macq,Christophe de Vleeschouwer*

Main category: cs.CV

TL;DR: A selective data acquisition strategy for edge learning: using high-confidence stream-based sampling together with a diversity-based selection to train edge models with limited transmissions, delivering strong performance with few labeled queries.


<details>
  <summary>Details</summary>
Motivation: Edge camera systems require regular model updates but operate under limited bandwidth and computational power. Large teacher models on a central server can annotate data to train smaller edge models, but transmitting data for labeling is costly. The goal is to maximize model quality while minimizing data transmissions.

Method: Proposes a high-confidence stream-based sampling strategy complemented by a diversity-based approach to select training images. This aims to maintain training effectiveness under a fixed training load (iterations) by prioritizing informative and diverse samples for labeling from the centralized teacher model.

Result: With a similar training load, the combination of a high-confidence streaming strategy and diversity-based sampling yields a high-quality model with minimal dataset queries.

Conclusion: High-confidence stream-based selection combined with diversity-aware sampling is effective for edge-cloud active learning, enabling high model quality with reduced labeling/transmission costs under edge constraints.

Abstract: Edge camera-based systems are continuously expanding, facing ever-evolving
environments that require regular model updates. In practice, complex teacher
models are run on a central server to annotate data, which is then used to
train smaller models tailored to the edge devices with limited computational
power. This work explores how to select the most useful images for training to
maximize model quality while keeping transmission costs low. Our work shows
that, for a similar training load (i.e., iterations), a high-confidence
stream-based strategy coupled with a diversity-based approach produces a
high-quality model with minimal dataset queries.

</details>


### [8] [InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On](https://arxiv.org/abs/2509.20524)
*Julien Han,Shuwen Qiu,Qi Li,Xingzi Xu,Mehmet Saygin Seyfioglu,Kavosh Asadi,Karim Bouyarmane*

Main category: cs.CV

TL;DR: InstructVTON is an instruction-following interactive virtual try-on system that replaces manual masking with automated, text-guided masks, enabling fine-grained styling control and multi-round generation, and is interoperable with existing VTON models.


<details>
  <summary>Details</summary>
Motivation: To address limitations of binary-mask-based inpainting in virtual try-on, which hinder complex styling, require precise masks, and constrain user interaction; aim to simplify the user experience and expand styling capabilities.

Method: Formulate try-on as an image-guided inpainting task. Use Vision-Language Models and image segmentation to automatically generate binary masks from user images and free-text style instructions. Enable multiple rounds of image generation and integrate with existing virtual try-on models.

Result: Demonstrates state-of-the-art styling control when used with current VTON models and shows interoperability, enabling complex styling scenarios that masking-based approaches struggle with.

Conclusion: InstructVTON improves user experience by removing the need for precise masks, enabling complex styling, and expanding the applicability of VTON through automated mask generation and iterative generation.

Abstract: We present InstructVTON, an instruction-following interactive virtual try-on
system that allows fine-grained and complex styling control of the resulting
generation, guided by natural language, on single or multiple garments. A
computationally efficient and scalable formulation of virtual try-on formulates
the problem as an image-guided or image-conditioned inpainting task. These
inpainting-based virtual try-on models commonly use a binary mask to control
the generation layout. Producing a mask that yields desirable result is
difficult, requires background knowledge, might be model dependent, and in some
cases impossible with the masking-based approach (e.g. trying on a long-sleeve
shirt with "sleeves rolled up" styling on a person wearing long-sleeve shirt
with sleeves down, where the mask will necessarily cover the entire sleeve).
InstructVTON leverages Vision Language Models (VLMs) and image segmentation
models for automated binary mask generation. These masks are generated based on
user-provided images and free-text style instructions. InstructVTON simplifies
the end-user experience by removing the necessity of a precisely drawn mask,
and by automating execution of multiple rounds of image generation for try-on
scenarios that cannot be achieved with masking-based virtual try-on models
alone. We show that InstructVTON is interoperable with existing virtual try-on
models to achieve state-of-the-art results with styling control.

</details>


### [9] [Innovative Deep Learning Architecture for Enhanced Altered Fingerprint Recognition](https://arxiv.org/abs/2509.20537)
*Dana A Abdullah,Dana Rasul Hamad,Bishar Rasheed Ibrahim,Sirwan Abdulwahid Aula,Aso Khaleel Ameen,Sabat Salih Hamadamin*

Main category: cs.CV

TL;DR: DeepAFRNet uses a VGG16-based embedding with cosine similarity to robustly identify distorted/altered fingerprints, achieving high accuracy on real-altered samples (SOCOFing Real-Altered) across Easy/Medium/Hard; threshold selection is critical.


<details>
  <summary>Details</summary>
Motivation: Biometric systems are vulnerable to altered fingerprints; need robust recognition using real altered samples.

Method: VGG16 backbone to extract embeddings; cosine similarity for matching; evaluation on SOCOFing Real-Altered subset across three difficulty levels; threshold-sensitivity analysis.

Result: Accuracies: 96.7%, 98.76%, 99.54% at strict thresholds; relaxing threshold from 0.92 to 0.72 drastically lowers performance (to 7.86%, 27.05%, 29.51%); per-level metrics reported.

Conclusion: Demonstrates readiness for real-world deployment with security and recognition resilience; addresses limitations of synthetic-alteration studies; emphasizes importance of threshold choice and per-level evaluation.

Abstract: Altered fingerprint recognition (AFR) is challenging for biometric
verification in applications such as border control, forensics, and fiscal
admission. Adversaries can deliberately modify ridge patterns to evade
detection, so robust recognition of altered prints is essential. We present
DeepAFRNet, a deep learning recognition model that matches and recognizes
distorted fingerprint samples. The approach uses a VGG16 backbone to extract
high-dimensional features and cosine similarity to compare embeddings. We
evaluate on the SOCOFing Real-Altered subset with three difficulty levels
(Easy, Medium, Hard). With strict thresholds, DeepAFRNet achieves accuracies of
96.7 percent, 98.76 percent, and 99.54 percent for the three levels. A
threshold-sensitivity study shows that relaxing the threshold from 0.92 to 0.72
sharply degrades accuracy to 7.86 percent, 27.05 percent, and 29.51 percent,
underscoring the importance of threshold selection in biometric systems. By
using real altered samples and reporting per-level metrics, DeepAFRNet
addresses limitations of prior work based on synthetic alterations or limited
verification protocols, and indicates readiness for real-world deployments
where both security and recognition resilience are critical.

</details>


### [10] [Large Pre-Trained Models for Bimanual Manipulation in 3D](https://arxiv.org/abs/2509.20579)
*Hanna Yurchyk,Wei-Di Chang,Gregory Dudek,David Meger*

Main category: cs.CV

TL;DR: Attention maps from a pre-trained Vision Transformer are converted into 3D voxel cues and fed into a voxel-based bimanual manipulation policy, yielding notable performance gains on the RLBench benchmark.


<details>
  <summary>Details</summary>
Motivation: To improve bimanual robotic manipulation by injecting semantically meaningful cues derived from self-supervised Vision Transformer attention into 3D voxel representations, bridging 2D saliency with 3D policy learning.

Method: 1) Extract attention maps from DINOv2 (self-supervised ViT) on RGB images. 2) Interpret these maps as pixel-level saliency scores. 3) Lift the saliency into a 3D voxel grid to create voxel-level semantic cues. 4) Inject these cues into a state-of-the-art voxel-based behavior cloning policy. 5) Evaluate on RLBench bimanual tasks and compare to the baseline voxel policy.

Result: Incorporating attention-guided voxel featurization yields an average absolute improvement of 8.2% and a relative gain of 21.9% across all RLBench bimanual tasks.

Conclusion: Attention-guided 3D semantic cues derived from pre-trained ViT attention maps effectively enhance voxel-based manipulation policies, demonstrating the viability of cross-modal priors for 3D perception and control in robotics.

Abstract: We investigate the integration of attention maps from a pre-trained Vision
Transformer into voxel representations to enhance bimanual robotic
manipulation. Specifically, we extract attention maps from DINOv2, a
self-supervised ViT model, and interpret them as pixel-level saliency scores
over RGB images. These maps are lifted into a 3D voxel grid, resulting in
voxel-level semantic cues that are incorporated into a behavior cloning policy.
When integrated into a state-of-the-art voxel-based policy, our
attention-guided featurization yields an average absolute improvement of 8.2%
and a relative gain of 21.9% across all tasks in the RLBench bimanual
benchmark.

</details>


### [11] [A Comparative Benchmark of Real-time Detectors for Blueberry Detection towards Precision Orchard Management](https://arxiv.org/abs/2509.20580)
*Xinyang Mu,Yuzhen Lu,Boyang Deng*

Main category: cs.CV

TL;DR: A comprehensive benchmark of YOLOv8-12 and RT-DETRv1-2 on a new blueberry dataset; RT-DETRv2-X and YOLOv12m achieve top accuracy; SSL yields modest gains; dataset and code released.


<details>
  <summary>Details</summary>
Motivation: Blueberry detection in natural environments is challenging due to variable lighting, occlusions, and motion blur; high-performance detectors require large, diverse datasets and appropriate speed/accuracy/memory trade-offs.

Method: Evaluated 36 model variants across YOLOv8-12 and RT-DETRv1-2 on a curated dataset of 661 canopy images with 85,879 labeled blueberries; measured mAP@50; analyzed inference times; fine-tuned models with Unbiased Mean Teacher SSL on 1,035 unlabeled ground-based images; evaluated accuracy changes and best-performing configurations.

Result: YOLOv12m achieved mAP@50 of 93.3%; RT-DETRv2-X achieved 93.6% (best RT-DETR variant). SSL boosted accuracy variably (-1.4% to +2.9%), with RT-DETR-v2-X reaching 94.8% mAP@50 after SSL; inference time varied by model size; mid-sized models offered favorable accuracy-speed trade-offs.

Conclusion: SSL can improve detection but effects are cross-model; further work needed to leverage cross-domain unlabeled data; dataset and software are publicly available to support ongoing research.

Abstract: Blueberry detection in natural environments remains challenging due to
variable lighting, occlusions, and motion blur due to environmental factors and
imaging devices. Deep learning-based object detectors promise to address these
challenges, but they demand a large-scale, diverse dataset that captures the
real-world complexities. Moreover, deploying these models in practical
scenarios often requires the right accuracy/speed/memory trade-off in model
selection. This study presents a novel comparative benchmark analysis of
advanced real-time object detectors, including YOLO (You Only Look Once)
(v8-v12) and RT-DETR (Real-Time Detection Transformers) (v1-v2) families,
consisting of 36 model variants, evaluated on a newly curated dataset for
blueberry detection. This dataset comprises 661 canopy images collected with
smartphones during the 2022-2023 seasons, consisting of 85,879 labelled
instances (including 36,256 ripe and 49,623 unripe blueberries) across a wide
range of lighting conditions, occlusions, and fruit maturity stages. Among the
YOLO models, YOLOv12m achieved the best accuracy with a mAP@50 of 93.3%, while
RT-DETRv2-X obtained a mAP@50 of 93.6%, the highest among all the RT-DETR
variants. The inference time varied with the model scale and complexity, and
the mid-sized models appeared to offer a good accuracy-speed balance. To
further enhance detection performance, all the models were fine-tuned using
Unbiased Mean Teacher-based semi-supervised learning (SSL) on a separate set of
1,035 unlabeled images acquired by a ground-based machine vision platform in
2024. This resulted in accuracy gains ranging from -1.4% to 2.9%, with
RT-DETR-v2-X achieving the best mAP@50 of 94.8%. More in-depth research into
SSL is needed to better leverage cross-domain unlabeled data. Both the dataset
and software programs of this study are made publicly available to support
further research.

</details>


### [12] [Region-of-Interest Augmentation for Mammography Classification under Patient-Level Cross-Validation](https://arxiv.org/abs/2509.20585)
*Farbod Bigdeli,Mohsen Mohammadagha,Ali Bigdeli*

Main category: cs.CV

TL;DR: ROI-based data augmentation during training on Mini-DDSM yields modest ROC-AUC gains with no inference cost; data-centric approach helps constrained mammography datasets but PR-AUC may not improve.


<details>
  <summary>Details</summary>
Motivation: Limited-resolution datasets and small sample sizes hinder deep learning for mammography; need simple, label-free augmentation that can improve performance without extra data or architectural changes.

Method: During training, replace full images with random ROI crops drawn from a precomputed bounding-box bank; optional jitter to increase variability; ROI augmentation is training-only; evaluate with strict patient-level cross-validation; report ROC-AUC, PR-AUC, throughput, and GPU memory.

Result: Best settings yield modest average ROC-AUC gains with fold variability; PR-AUC unchanged or slightly lower; ROI augmentation does not increase inference cost, and training-time efficiency metrics provided.

Conclusion: Simple, data-centric ROI augmentation can boost mammography classification in constrained settings without extra labels or model changes.

Abstract: Breast cancer screening with mammography remains central to early detection
and mortality reduction. Deep learning has shown strong potential for
automating mammogram interpretation, yet limited-resolution datasets and small
sample sizes continue to restrict performance. We revisit the Mini-DDSM dataset
(9,684 images; 2,414 patients) and introduce a lightweight region-of-interest
(ROI) augmentation strategy. During training, full images are probabilistically
replaced with random ROI crops sampled from a precomputed, label-free
bounding-box bank, with optional jitter to increase variability. We evaluate
under strict patient-level cross-validation and report ROC-AUC, PR-AUC, and
training-time efficiency metrics (throughput and GPU memory). Because ROI
augmentation is training-only, inference-time cost remains unchanged. On
Mini-DDSM, ROI augmentation (best: p_roi = 0.10, alpha = 0.10) yields modest
average ROC-AUC gains, with performance varying across folds; PR-AUC is flat to
slightly lower. These results demonstrate that simple, data-centric ROI
strategies can enhance mammography classification in constrained settings
without requiring additional labels or architectural modifications.

</details>


### [13] [Reflect3r: Single-View 3D Stereo Reconstruction Aided by Mirror Reflections](https://arxiv.org/abs/2509.20607)
*Jing Wu,Zirui Wang,Iro Laina,Victor Adrian Prisacariu*

Main category: cs.CV

TL;DR: Turns mirror reflections into an auxiliary virtual camera to enable single-image multi-view stereo, with a symmetric pose-loss, dynamic-scene extension, and a synthetic 16-scene benchmark; validated by experiments.


<details>
  <summary>Details</summary>
Motivation: Mirrors naturally provide stereo information in a single capture by showing real and reflected views simultaneously. The work aims to exploit this to simplify image acquisition and enable robust, generalizable 3D reconstruction using feed-forward models.

Method: Construct a physically valid virtual camera from the reflection to generate the virtual view in the pixel domain. Use a symmetric-aware loss to refine pose estimation. Extend the framework to dynamic scenes with per-frame geometry recovery. Provide a fully customizable synthetic dataset of 16 Blender scenes with ground-truth point clouds and camera poses.

Result: Extensive experiments on real-world and synthetic data demonstrate the effectiveness of leveraging mirror-induced auxiliary views for 3D reconstruction and pose estimation, including per-frame geometry recovery in dynamic scenes.

Conclusion: The approach simplifies imaging, aligns with modern feed-forward reconstruction models, and enables robust, generalizable 3D reconstruction from a single image by treating reflections as valid auxiliary views. It also offers a customizable synthetic benchmark for future research and extends naturally to dynamic scenes.

Abstract: Mirror reflections are common in everyday environments and can provide stereo
information within a single capture, as the real and reflected virtual views
are visible simultaneously. We exploit this property by treating the reflection
as an auxiliary view and designing a transformation that constructs a
physically valid virtual camera, allowing direct pixel-domain generation of the
virtual view while adhering to the real-world imaging process. This enables a
multi-view stereo setup from a single image, simplifying the imaging process,
making it compatible with powerful feed-forward reconstruction models for
generalizable and robust 3D reconstruction. To further exploit the geometric
symmetry introduced by mirrors, we propose a symmetric-aware loss to refine
pose estimation. Our framework also naturally extends to dynamic scenes, where
each frame contains a mirror reflection, enabling efficient per-frame geometry
recovery. For quantitative evaluation, we provide a fully customizable
synthetic dataset of 16 Blender scenes, each with ground-truth point clouds and
camera poses. Extensive experiments on real-world data and synthetic data are
conducted to illustrate the effectiveness of our method.

</details>


### [14] [Recov-Vision: Linking Street View Imagery and Vision-Language Models for Post-Disaster Recovery](https://arxiv.org/abs/2509.20628)
*Yiming Xiao,Archit Gupta,Miguel Esparza,Yu-Hsuan Ho,Antonia Sebastian,Hannah Weas,Rose Houck,Ali Mostafavi*

Main category: cs.CV

TL;DR: FacadeTrack links panoramic video to parcels, rectifies to facades, and extracts interpretable occupancy cues; a two-stage design improves F1/recall balance over a one-stage baseline, enabling auditable post-disaster occupancy assessments.


<details>
  <summary>Details</summary>
Motivation: Post-disaster occupancy data is vital for triage and resource allocation. Overhead imagery misses facade cues; street-view is sparse and hard to align with parcels. There is a need for a scalable, interpretable, auditable framework that integrates street-level cues with geospatial workflows.

Method: FacadeTrack links panoramic video to parcels, rectifies views to facades, and elicits interpretable attributes (e.g., entry blockage, temporary coverings, debris). It compares a one-stage rule-based approach with a two-stage design separating perception from conservative reasoning; evaluated on two post-Hurricane Helene surveys.

Result: Two-stage approach achieved precision 0.927, recall 0.781, F1 0.848, vs one-stage precision 0.943, recall 0.728, F1 0.822. Intermediate attributes and spatial diagnostics reveal error sources and support targeted quality control; auditable, scalable occupancy assessments suitable for geospatial and emergency-management workflows.

Conclusion: The two-stage design offers better balance between precision and recall, with interpretable cues and diagnostics enabling robust, auditable occupancy assessments that integrate into emergency-management pipelines.

Abstract: Building-level occupancy after disasters is vital for triage, inspections,
utility re-energization, and equitable resource allocation. Overhead imagery
provides rapid coverage but often misses facade and access cues that determine
habitability, while street-view imagery captures those details but is sparse
and difficult to align with parcels. We present FacadeTrack, a street-level,
language-guided framework that links panoramic video to parcels, rectifies
views to facades, and elicits interpretable attributes (for example, entry
blockage, temporary coverings, localized debris) that drive two decision
strategies: a transparent one-stage rule and a two-stage design that separates
perception from conservative reasoning. Evaluated across two post-Hurricane
Helene surveys, the two-stage approach achieves a precision of 0.927, a recall
of 0.781, and an F-1 score of 0.848, compared with the one-stage baseline at a
precision of 0.943, a recall of 0.728, and an F-1 score of 0.822. Beyond
accuracy, intermediate attributes and spatial diagnostics reveal where and why
residual errors occur, enabling targeted quality control. The pipeline provides
auditable, scalable occupancy assessments suitable for integration into
geospatial and emergency-management workflows.

</details>


### [15] [Human Semantic Representations of Social Interactions from Moving Shapes](https://arxiv.org/abs/2509.20673)
*Yiling Yun,Hongjing Lu*

Main category: cs.CV

TL;DR: Semantic representations, notably verb-based embeddings from descriptions, complement visual cues in predicting human judgments of social interactions in moving shapes; revealing a semantic structure in social perception.


<details>
  <summary>Details</summary>
Motivation: To determine what semantic representations humans use beyond raw visual features when interpreting social interactions in abstract, motion-based displays.

Method: Study 1: participants labeled animations by impression. Study 2: 27 interactions, human similarity judgments; compared with model predictions using visual features, labels, and semantic embeddings from animation descriptions; evaluated which capture human judgments.

Result: Semantic models add predictive value beyond visual features; verb-based embeddings best predict similarity judgments.

Conclusion: Social perception in simple displays reflects the semantic structure of social interactions, linking visual cues with abstract semantic representations.

Abstract: Humans are social creatures who readily recognize various social interactions
from simple display of moving shapes. While previous research has often focused
on visual features, we examine what semantic representations that humans employ
to complement visual features. In Study 1, we directly asked human participants
to label the animations based on their impression of moving shapes. We found
that human responses were distributed. In Study 2, we measured the
representational geometry of 27 social interactions through human similarity
judgments and compared it with model predictions based on visual features,
labels, and semantic embeddings from animation descriptions. We found that
semantic models provided complementary information to visual features in
explaining human judgments. Among the semantic models, verb-based embeddings
extracted from descriptions account for human similarity judgments the best.
These results suggest that social perception in simple displays reflects the
semantic structure of social interactions, bridging visual and abstract
representations.

</details>


### [16] [Enhancing Cross-View Geo-Localization Generalization via Global-Local Consistency and Geometric Equivariance](https://arxiv.org/abs/2509.20684)
*Xiaowei Wang,Di Wang,Ke Li,Yifeng Wang,Chengjian Wang,Libin Sun,Zhihong Wu,Yiming Zhang,Quan Wang*

Main category: cs.CV

TL;DR: EGS introduces an E(2)-Steerable CNN encoder and a global-local graph with a virtual super-node to improve cross-domain cross-view geo-localization, achieving state-of-the-art results on University-1652 and SUES-200.


<details>
  <summary>Details</summary>
Motivation: Two main challenges in CVGL: (1) robustness under severe appearance variations due to diverse UAV orientations and fields of view, hindering cross-domain generalization; (2) reliable correspondences that capture both global scene-level semantics and fine-grained local details.

Method: 1) Use an E(2)-Steerable CNN encoder to obtain rotation- and viewpoint-robust features. 2) Build a graph with a virtual super-node connected to all local nodes to aggregate global semantics and redistribute them to local regions, enforcing global-local consistency.

Result: Extensive experiments on University-1652 and SUES-200 show substantial performance gains and establish a new state-of-the-art in cross-domain CVGL.

Conclusion: EGS effectively improves cross-domain CVGL by addressing both robustness to appearance variations and the reliability of global-local correspondences.

Abstract: Cross-view geo-localization (CVGL) aims to match images of the same location
captured from drastically different viewpoints. Despite recent progress,
existing methods still face two key challenges: (1) achieving robustness under
severe appearance variations induced by diverse UAV orientations and fields of
view, which hinders cross-domain generalization, and (2) establishing reliable
correspondences that capture both global scene-level semantics and fine-grained
local details. In this paper, we propose EGS, a novel CVGL framework designed
to enhance cross-domain generalization. Specifically, we introduce an
E(2)-Steerable CNN encoder to extract stable and reliable features under
rotation and viewpoint shifts. Furthermore, we construct a graph with a virtual
super-node that connects to all local nodes, enabling global semantics to be
aggregated and redistributed to local regions, thereby enforcing global-local
consistency. Extensive experiments on the University-1652 and SUES-200
benchmarks demonstrate that EGS consistently achieves substantial performance
gains and establishes a new state of the art in cross-domain CVGL.

</details>


### [17] [DENet: Dual-Path Edge Network with Global-Local Attention for Infrared Small Target Detection](https://arxiv.org/abs/2509.20701)
*Jiayi Zuo,Songwei Pei,Qian Li*

Main category: cs.CV

TL;DR: Proposes a Dual-Path Edge Network for infrared small target detection that decouples edge refinement from semantic modeling via a Bidirectional Interaction Module (Local and Global Self-Attention) and a Multi-Edge Refiner (Taylor finite difference cascaded across scales with attention gating), aiming to improve edge accuracy and robustness in noisy, low-contrast scenes.


<details>
  <summary>Details</summary>
Motivation: Infrared small targets lack distinctive texture and are easily camouflaged in cluttered or noisy backgrounds. Capturing high-resolution spatial details while retaining robust semantic context is challenging, often causing feature misalignment and suboptimal detection. Existing methods rely on fixed gradient operators or simplistic attention, which struggle to extract edges under low contrast and high noise.

Method: A two-path architecture: (1) Bidirectional Interaction Module that uses Local Self-Attention and Global Self-Attention (Transformer-based) to capture multi-scale local and global dependencies and integrate long-range semantic relationships; (2) Multi-Edge Refiner that enhances fine-grained edge details with cascaded Taylor finite difference operators across multiple scales, controlled by an attention-driven gating mechanism to suppress noise and adapt to varying target sizes.

Result: The abstract reports a promising framework that combines semantic-level edge refinement and robust localization to improve infrared small target detection, with emphasis on precise edge localization and suppression of noise, though it does not provide quantitative metrics in the abstract.

Conclusion: The Dual-Path Edge Network presents a unified approach that addresses the mismatch between high-resolution spatial details and semantic context by coupling edge-focused refinement with transformer-based semantic modeling, potentially improving detection and localization performance in challenging infrared scenes.

Abstract: Infrared small target detection is crucial for remote sensing applications
like disaster warning and maritime surveillance. However, due to the lack of
distinctive texture and morphological features, infrared small targets are
highly susceptible to blending into cluttered and noisy backgrounds. A
fundamental challenge in designing deep models for this task lies in the
inherent conflict between capturing high-resolution spatial details for minute
targets and extracting robust semantic context for larger targets, often
leading to feature misalignment and suboptimal performance. Existing methods
often rely on fixed gradient operators or simplistic attention mechanisms,
which are inadequate for accurately extracting target edges under low contrast
and high noise. In this paper, we propose a novel Dual-Path Edge Network that
explicitly addresses this challenge by decoupling edge enhancement and semantic
modeling into two complementary processing paths. The first path employs a
Bidirectional Interaction Module, which uses both Local Self-Attention and
Global Self-Attention to capture multi-scale local and global feature
dependencies. The global attention mechanism, based on a Transformer
architecture, integrates long-range semantic relationships and contextual
information, ensuring robust scene understanding. The second path introduces
the Multi-Edge Refiner, which enhances fine-grained edge details using cascaded
Taylor finite difference operators at multiple scales. This mathematical
approach, along with an attention-driven gating mechanism, enables precise edge
localization and feature enhancement for targets of varying sizes, while
effectively suppressing noise. Our method provides a promising solution for
precise infrared small target detection and localization, combining structural
semantics and edge refinement in a unified framework.

</details>


### [18] [Beyond the Individual: Introducing Group Intention Forecasting with SHOT Dataset](https://arxiv.org/abs/2509.20715)
*Ruixu Zhang,Yuran Wang,Xinyi Hu,Chaoyu Mai,Wenxuan Liu,Danni Xu,Xian Zhong,Zheng Wang*

Main category: cs.CV

TL;DR: Defines Group Intention Forecasting (GIF) and introduces SHOT (a 1,979-clips basketball dataset) and GIFT (a framework for forecasting the emergence of group intentions from multi-view, multi-individual data). Demonstrates effectiveness of SHOT and GIFT on GIF tasks, laying groundwork for future research; dataset available online.


<details>
  <summary>Details</summary>
Motivation: Targets a gap in intention recognition where group-level intentions emerge from interactions among multiple individuals, not captured by existing single-agent focus. Aims to study, forecast, and model the evolution of group intentions using rich, multi-view social cues.

Method: SH0T dataset: 1,979 basketball clips from 5 camera views, annotated with 6 types of individual attributes; designed to capture multi-individual information, multi-view adaptability, and multi-level intention. GIFT framework: extracts fine-grained individual features and models evolving group dynamics to forecast when group intention emerges.

Result: Experimental results confirm the effectiveness of SHOT and GIFT, showing successful forecasting of group intention emergence and providing a strong foundation for GIF research.

Conclusion: SH0T and GIFT establish a foundation for future GIF research by providing a large-scale dataset and a forecasting framework; dataset is publicly available at the provided URL.

Abstract: Intention recognition has traditionally focused on individual intentions,
overlooking the complexities of collective intentions in group settings. To
address this limitation, we introduce the concept of group intention, which
represents shared goals emerging through the actions of multiple individuals,
and Group Intention Forecasting (GIF), a novel task that forecasts when group
intentions will occur by analyzing individual actions and interactions before
the collective goal becomes apparent. To investigate GIF in a specific
scenario, we propose SHOT, the first large-scale dataset for GIF, consisting of
1,979 basketball video clips captured from 5 camera views and annotated with 6
types of individual attributes. SHOT is designed with 3 key characteristics:
multi-individual information, multi-view adaptability, and multi-level
intention, making it well-suited for studying emerging group intentions.
Furthermore, we introduce GIFT (Group Intention ForecasTer), a framework that
extracts fine-grained individual features and models evolving group dynamics to
forecast intention emergence. Experimental results confirm the effectiveness of
SHOT and GIFT, establishing a strong foundation for future research in group
intention forecasting. The dataset is available at
https://xinyi-hu.github.io/SHOT_DATASET.

</details>


### [19] [Neptune-X: Active X-to-Maritime Generation for Universal Maritime Object Detection](https://arxiv.org/abs/2509.20745)
*Yu Guo,Shengfeng He,Yuxu Lu,Haonan An,Yihang Tao,Huilin Zhu,Jingxian Liu,Yuguang Fang*

Main category: cs.CV

TL;DR: A data-centric framework (Neptune-X) for maritime object detection that combines synthetic data generation (X-to-Maritime with Bidirectional Object-Water Attention) and attribute-aware sample selection (Attribute-correlated Active Sampling), plus a Maritime Generation Dataset, to boost accuracy in underrepresented maritime scenarios.


<details>
  <summary>Details</summary>
Motivation: Maritime object detection suffers from scarce annotated data and poor generalization across attributes (category, viewpoint, location, imaging environment). This leads to weak performance in open-sea and underrepresented scenarios and a lack of a dedicated generative maritime benchmark.

Method: Neptune-X comprises (1) X-to-Maritime, a multi-modality-conditioned generative model with Bidirectional Object-Water Attention to synthesize diverse, realistic maritime scenes; (2) Attribute-correlated Active Sampling to dynamically select synthetic samples based on their task relevance; (3) Maritime Generation Dataset, a benchmarking dataset covering a wide range of semantic conditions for generative maritime learning.

Result: Extensive experiments show strong improvements in detection accuracy, especially in challenging and underrepresented settings, and establish Neptune-X as a new benchmark for maritime scene synthesis.

Conclusion: A data-centric approach leveraging synthetic data and task-aware sample selection significantly enhances maritime detection and generalization, with code availability provided.

Abstract: Maritime object detection is essential for navigation safety, surveillance,
and autonomous operations, yet constrained by two key challenges: the scarcity
of annotated maritime data and poor generalization across various maritime
attributes (e.g., object category, viewpoint, location, and imaging
environment). % In particular, models trained on existing datasets often
underperform in underrepresented scenarios such as open-sea environments. To
address these challenges, we propose Neptune-X, a data-centric
generative-selection framework that enhances training effectiveness by
leveraging synthetic data generation with task-aware sample selection. From the
generation perspective, we develop X-to-Maritime, a multi-modality-conditioned
generative model that synthesizes diverse and realistic maritime scenes. A key
component is the Bidirectional Object-Water Attention module, which captures
boundary interactions between objects and their aquatic surroundings to improve
visual fidelity. To further improve downstream tasking performance, we propose
Attribute-correlated Active Sampling, which dynamically selects synthetic
samples based on their task relevance. To support robust benchmarking, we
construct the Maritime Generation Dataset, the first dataset tailored for
generative maritime learning, encompassing a wide range of semantic conditions.
Extensive experiments demonstrate that our approach sets a new benchmark in
maritime scene synthesis, significantly improving detection accuracy,
particularly in challenging and previously underrepresented settings.The code
is available at https://github.com/gy65896/Neptune-X.

</details>


### [20] [AI-Enabled Crater-Based Navigation for Lunar Mapping](https://arxiv.org/abs/2509.20748)
*Sofia McLeod,Chee-Kheng Chng,Matthew Rodda,Tat-Jun Chin*

Main category: cs.CV

TL;DR: STELLA is the first end-to-end crater-based navigation pipeline for long-duration lunar mapping, delivering metre-level positioning and sub-degree attitude accuracy across diverse viewing and illumination conditions, validated on the CRESENT datasets.


<details>
  <summary>Details</summary>
Motivation: Bridges the gap between crater-based navigation in short, nadir-view descent scenarios and long-duration lunar mapping with sparse, oblique imagery under varying illumination; provides a robust, end-to-end solution and a public dataset to enable realistic pose estimation benchmarking.

Method: An end-to-end STELLA pipeline comprising a Mask R-CNN crater detector, a descriptor-less crater identification module, a robust perspective-n-crater pose solver, and a batch orbit determination back-end. Evaluation uses CRESENT-365 (the first public year-long lunar mapping dataset with 15,283 images rendered from high-resolution DEMs with SPICE-derived Sun angles and Moon motion) and CRESENT+.

Result: STELLA maintains metre-level position accuracy and sub-degree attitude accuracy on average across wide ranges of viewing angles, illumination conditions, and lunar latitudes.

Conclusion: This work provides the first comprehensive assessment of crater-based navigation in a true lunar-mapping setting and offers guidance on operational conditions for future missions; the CRESENT datasets enable benchmarking and replication.

Abstract: Crater-Based Navigation (CBN) uses the ubiquitous impact craters of the Moon
observed on images as natural landmarks to determine the six degrees of freedom
pose of a spacecraft. To date, CBN has primarily been studied in the context of
powered descent and landing. These missions are typically short in duration,
with high-frequency imagery captured from a nadir viewpoint over well-lit
terrain. In contrast, lunar mapping missions involve sparse, oblique imagery
acquired under varying illumination conditions over potentially year-long
campaigns, posing significantly greater challenges for pose estimation. We
bridge this gap with STELLA - the first end-to-end CBN pipeline for
long-duration lunar mapping. STELLA combines a Mask R-CNN-based crater
detector, a descriptor-less crater identification module, a robust
perspective-n-crater pose solver, and a batch orbit determination back-end. To
rigorously test STELLA, we introduce CRESENT-365 - the first public dataset
that emulates a year-long lunar mapping mission. Each of its 15,283 images is
rendered from high-resolution digital elevation models with SPICE-derived Sun
angles and Moon motion, delivering realistic global coverage, illumination
cycles, and viewing geometries. Experiments on CRESENT+ and CRESENT-365 show
that STELLA maintains metre-level position accuracy and sub-degree attitude
accuracy on average across wide ranges of viewing angles, illumination
conditions, and lunar latitudes. These results constitute the first
comprehensive assessment of CBN in a true lunar mapping setting and inform
operational conditions that should be considered for future missions.

</details>


### [21] [Seeing Through Words, Speaking Through Pixels: Deep Representational Alignment Between Vision and Language Models](https://arxiv.org/abs/2509.20751)
*Zoe Wanying He,Sean Trott,Meenakshi Khosla*

Main category: cs.CV

TL;DR: Unimodal (vision-only and language-only) models trained on different modalities converge to a shared semantic representation, with alignment emerging in mid-to-late layers, robust to appearance but sensitive to semantic changes, mirroring human image-caption preferences and amplified by exemplar averaging.


<details>
  <summary>Details</summary>
Motivation: To understand where and how cross-modal alignment arises in unimodal networks, what cues drive it, whether it mirrors human judgments in richer image-text mappings, and how exemplar aggregation affects the shared semantic code.

Method: Systematic analysis across vision-only and language-only models to locate alignment in mid-to-late layers; test robustness to appearance changes and semantic alterations; use forced-choice 'Pick-a-Pic' tasks to compare human preferences with model embeddings; explore many-to-one image-caption settings and average embeddings across exemplars to assess effects on alignment.

Result: Alignment peaks in mid-to-late layers for both model types, indicating a shift from modality-specific to conceptually shared representations. Alignment is robust to appearance-only changes but collapses when semantics are altered. In 'Pick-a-Pic' tasks, human preferences align with model embeddings across vision-language pairs, bidirectionally when multiple captions map to one image. Averaging embeddings across exemplars amplifies rather than blurs alignment, suggesting a stronger shared semantic code.

Conclusion: Unimodal networks converge on a shared semantic representation that aligns with human judgments; this shared code strengthens with exemplar aggregation, consolidating the link between monomodal representations and multimodal semantic understanding.

Abstract: Recent studies show that deep vision-only and language-only models--trained
on disjoint modalities--nonetheless project their inputs into a partially
aligned representational space. Yet we still lack a clear picture of where in
each network this convergence emerges, what visual or linguistic cues support
it, whether it captures human preferences in many-to-many image-text scenarios,
and how aggregating exemplars of the same concept affects alignment. Here, we
systematically investigate these questions. We find that alignment peaks in
mid-to-late layers of both model types, reflecting a shift from
modality-specific to conceptually shared representations. This alignment is
robust to appearance-only changes but collapses when semantics are altered
(e.g., object removal or word-order scrambling), highlighting that the shared
code is truly semantic. Moving beyond the one-to-one image-caption paradigm, a
forced-choice "Pick-a-Pic" task shows that human preferences for image-caption
matches are mirrored in the embedding spaces across all vision-language model
pairs. This pattern holds bidirectionally when multiple captions correspond to
a single image, demonstrating that models capture fine-grained semantic
distinctions akin to human judgments. Surprisingly, averaging embeddings across
exemplars amplifies alignment rather than blurring detail. Together, our
results demonstrate that unimodal networks converge on a shared semantic code
that aligns with human judgments and strengthens with exemplar aggregation.

</details>


### [22] [FreeInsert: Personalized Object Insertion with Geometric and Style Control](https://arxiv.org/abs/2509.20756)
*Yuhong Zhang,Han Wang,Yiwen Wang,Rong Xie,Li Song*

Main category: cs.CV

TL;DR: Training-free 3D-guided object insertion into arbitrary scenes via 3D conversion, interactive editing, and diffusion adapters to ensure geometric control and style consistency.


<details>
  <summary>Details</summary>
Motivation: 2D diffusion-based editing struggles with precise geometry, style mismatch with backgrounds, and often requires training data; a training-free, geometry-aware approach is desirable.

Method: Convert the 2D object to 3D, perform interactive 3D edits, render from a chosen view to obtain a geometric control image, and fuse it with diffusion adapters for style/content control to generate the final image.

Result: Proposes geometrically controllable and style-consistent edited images without additional training, enabling shape and view-level control within diffusion-based editing.

Conclusion: A training-free framework that leverages 3D information can address core limitations of 2D image editing for personalized insertion, improving geometry and realism.

Abstract: Text-to-image diffusion models have made significant progress in image
generation, allowing for effortless customized generation. However, existing
image editing methods still face certain limitations when dealing with
personalized image composition tasks. First, there is the issue of lack of
geometric control over the inserted objects. Current methods are confined to 2D
space and typically rely on textual instructions, making it challenging to
maintain precise geometric control over the objects. Second, there is the
challenge of style consistency. Existing methods often overlook the style
consistency between the inserted object and the background, resulting in a lack
of realism. In addition, the challenge of inserting objects into images without
extensive training remains significant. To address these issues, we propose
\textit{FreeInsert}, a novel training-free framework that customizes object
insertion into arbitrary scenes by leveraging 3D geometric information.
Benefiting from the advances in existing 3D generation models, we first convert
the 2D object into 3D, perform interactive editing at the 3D level, and then
re-render it into a 2D image from a specified view. This process introduces
geometric controls such as shape or view. The rendered image, serving as
geometric control, is combined with style and content control achieved through
diffusion adapters, ultimately producing geometrically controlled,
style-consistent edited images via the diffusion model.

</details>


### [23] [CusEnhancer: A Zero-Shot Scene and Controllability Enhancement Method for Photo Customization via ResInversion](https://arxiv.org/abs/2509.20775)
*Maoye Ren,Praneetha Vaddamanu,Jianjin Xu,Fernando De la Torre Frade*

Main category: cs.CV

TL;DR: A zero-shot framework (CustomEnhancer) enhances identity customization in diffusion-based image synthesis via triple-flow latent-space manipulation and a fast ResInversion, achieving state-of-the-art scene diversity and identity fidelity without retraining controllers.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image diffusion approaches struggle with realistic scenes, limited user control, and weak perceptual identity in personalized models, and often require retraining controllers; there is a need for training-free, efficient methods for personalized generation.

Method: CustomEnhancer performs zero-shot enhancement by leveraging face swapping and pretrained diffusion models to derive additional representations; introduces a triple-flow fused PerGeneration approach that combines two counter-directional latent spaces to manipulate a pivotal space of the personalized model, unifying generation and reconstruction across three flows; adds training-free controls for precise personalization; introduces ResInversion, a pre-diffusion noise-rectification method that greatly accelerates inversion (129x faster than NTI).

Result: Experiments report state-of-the-art performance in scene diversity and identity fidelity, with effective training-free controls; ResInversion significantly speeds up inversion compared to NTI.

Conclusion: The framework achieves high-quality, controllable personalized generation without per-model controller retraining, and offers a far faster inversion method; code will be released upon acceptance.

Abstract: Recently remarkable progress has been made in synthesizing realistic human
photos using text-to-image diffusion models. However, current approaches face
degraded scenes, insufficient control, and suboptimal perceptual identity. We
introduce CustomEnhancer, a novel framework to augment existing identity
customization models. CustomEnhancer is a zero-shot enhancement pipeline that
leverages face swapping techniques, pretrained diffusion model, to obtain
additional representations in a zeroshot manner for encoding into personalized
models. Through our proposed triple-flow fused PerGeneration approach, which
identifies and combines two compatible counter-directional latent spaces to
manipulate a pivotal space of personalized model, we unify the generation and
reconstruction processes, realizing generation from three flows. Our pipeline
also enables comprehensive training-free control over the generation process of
personalized models, offering precise controlled personalization for them and
eliminating the need for controller retraining for per-model. Besides, to
address the high time complexity of null-text inversion (NTI), we introduce
ResInversion, a novel inversion method that performs noise rectification via a
pre-diffusion mechanism, reducing the inversion time by 129 times. Experiments
demonstrate that CustomEnhancer reach SOTA results at scene diversity, identity
fidelity, training-free controls, while also showing the efficiency of our
ResInversion over NTI. The code will be made publicly available upon paper
acceptance.

</details>


### [24] [CompressAI-Vision: Open-source software to evaluate compression methods for computer vision tasks](https://arxiv.org/abs/2509.20777)
*Hyomin Choi,Heeji Han,Chris Rosewarne,Fabien Racapé*

Main category: cs.CV

TL;DR: Introduces CompressAI-Vision, an open-source evaluation platform for compressing video inputs optimized for downstream computer vision tasks, supporting remote and split inference, and leveraged by MPEG for Feature Coding for Machines (FCM).


<details>
  <summary>Details</summary>
Motivation: As vision-based applications proliferate, there is a need for codecs that preserve task accuracy under compression; a unified benchmark and platform is required to fairly compare coding tools across diverse vision tasks, models, and datasets.

Method: A comprehensive evaluation platform that works with (and demonstrates with) standard codecs under development, evaluating compression performance in terms of bit-rate versus task accuracy across multiple datasets, for two inference scenarios: remote and split inference.

Result: The platform demonstrates compression gains in terms of bitrate vs. task accuracy on several datasets, and is released as open-source software (CompressAI-Vision) and adopted by MPEG for FCM standard development.

Conclusion: CompressAI-Vision provides a common ground for evaluating CV-oriented compression, enabling fair comparisons, reproducibility, and contribution to standardization efforts in machine-centric video coding.

Abstract: With the increasing use of neural network (NN)-based computer vision
applications that process image and video data as input, interest has emerged
in video compression technology optimized for computer vision tasks. In fact,
given the variety of vision tasks, associated NN models and datasets, a
consolidated platform is needed as a common ground to implement and evaluate
compression methods optimized for downstream vision tasks. CompressAI-Vision is
introduced as a comprehensive evaluation platform where new coding tools
compete to efficiently compress the input of vision network while retaining
task accuracy in the context of two different inference scenarios: "remote" and
"split" inferencing. Our study showcases various use cases of the evaluation
platform incorporated with standard codecs (under development) by examining the
compression gain on several datasets in terms of bit-rate versus task accuracy.
This evaluation platform has been developed as open-source software and is
adopted by the Moving Pictures Experts Group (MPEG) for the development the
Feature Coding for Machines (FCM) standard. The software is available publicly
at https://github.com/InterDigitalInc/CompressAI-Vision.

</details>


### [25] [Dual-supervised Asymmetric Co-training for Semi-supervised Medical Domain Generalization](https://arxiv.org/abs/2509.20785)
*Jincai Song,Haipeng Chen,Jun Qin,Na Zhao*

Main category: cs.CV

TL;DR: A dual-supervised asymmetric co-training (DAC) framework for cross-domain semi-supervised domain generalization in medical image segmentation. It addresses domain shifts between labeled/unlabeled data and train/test data by adding feature-level supervision and two auxiliary self-supervised tasks within a two-sub-model co-training setup to improve pseudo-label reliability and domain-invariant learning, showing robust results on Fundus, Polyp, and SCGM datasets.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation under SSDG often assumes labeled and unlabeled data exist for each source domain, which is unrealistic. Real-world scenarios involve cross-domain label gaps and domain shifts between labeled/unlabeled data, necessitating methods that generalize across unseen domains when labeled data is limited.

Method: Propose DAC: a dual-sub-model co-training framework with cross pseudo supervision. Each sub-model receives feature-level supervision to mitigate errors due to domain shifts between labeled/unlabeled data. Each sub-model also incorporates two distinct auxiliary self-supervised tasks to promote domain-invariant discriminative features and prevent model collapse.

Result: Extensive experiments on real-world medical image segmentation datasets (Fundus, Polyp, SCGM) demonstrate robust generalizability of DAC under cross-domain SSDG, outperforming baseline approaches by effectively handling cross-domain pseudo-labels and domain shifts.

Conclusion: DAC provides a practical and effective solution for CD-SSDG in medical image segmentation by combining cross pseudo supervision, feature-level guidance, and dual self-supervised tasks within a two-model co-training framework, achieving strong generalization to unseen domains.

Abstract: Semi-supervised domain generalization (SSDG) in medical image segmentation
offers a promising solution for generalizing to unseen domains during testing,
addressing domain shift challenges and minimizing annotation costs. However,
conventional SSDG methods assume labeled and unlabeled data are available for
each source domain in the training set, a condition that is not always met in
practice. The coexistence of limited annotation and domain shift in the
training set is a prevalent issue. Thus, this paper explores a more practical
and challenging scenario, cross-domain semi-supervised domain generalization
(CD-SSDG), where domain shifts occur between labeled and unlabeled training
data, in addition to shifts between training and testing sets. Existing SSDG
methods exhibit sub-optimal performance under such domain shifts because of
inaccurate pseudolabels. To address this issue, we propose a novel
dual-supervised asymmetric co-training (DAC) framework tailored for CD-SSDG.
Building upon the co-training paradigm with two sub-models offering cross
pseudo supervision, our DAC framework integrates extra feature-level
supervision and asymmetric auxiliary tasks for each sub-model. This
feature-level supervision serves to address inaccurate pseudo supervision
caused by domain shifts between labeled and unlabeled data, utilizing
complementary supervision from the rich feature space. Additionally, two
distinct auxiliary self-supervised tasks are integrated into each sub-model to
enhance domain-invariant discriminative feature learning and prevent model
collapse. Extensive experiments on real-world medical image segmentation
datasets, i.e., Fundus, Polyp, and SCGM, demonstrate the robust
generalizability of the proposed DAC framework.

</details>


### [26] [Real-Time Object Detection Meets DINOv3](https://arxiv.org/abs/2509.20787)
*Shihua Huang,Yongjie Hou,Longfei Liu,Xuanlong Yu,Xi Shen*

Main category: cs.CV

TL;DR: DEIMv2 extends the Dense O2O and MAL real-time DETR framework by integrating DINOv3 features, delivering a scalable family (X to Atto) that achieves state-of-the-art accuracy with favorable parameter budgets across GPUs, edge, and mobile devices.


<details>
  <summary>Details</summary>
Motivation: To push real-time object detection performance on diverse hardware by marrying multi-scale DETR-style detection with DINOv3 features, enabling higher accuracy at lower resource costs, and to close the gap between large-scale models and ultra-lightweight deployments.

Method: Enhances DEIM with DINOv3-pretrained/distilled backbones for X/L/M/S (via a Spatial Tuning Adapter to fuse single-scale DINOv3 outputs into multi-scale features). For Nano/Pico/Femto/Atto, uses HGNetv2 with depth/width pruning. Includes a simplified decoder and an upgraded Dense O2O component for efficient real-time DETR inference.

Result: Achieves 57.8 AP on DEIMv2-X with 50.3M parameters (beats prior X-scale models that used >60M for 56.5 AP). DEIMv2-S surpasses 50 AP with 9.71M params. DEIMv2-Pico reaches 38.5 AP with 1.5M params, competitive with YOLOv10-Nano while using ~50% fewer parameters.

Conclusion: DEIMv2 delivers a superior accuracy-per-parameter trade-off across sizes, establishing new state-of-the-art results for real-time DETR across GPU, edge, and mobile deployments.

Abstract: Benefiting from the simplicity and effectiveness of Dense O2O and MAL, DEIM
has become the mainstream training framework for real-time DETRs, significantly
outperforming the YOLO series. In this work, we extend it with DINOv3 features,
resulting in DEIMv2. DEIMv2 spans eight model sizes from X to Atto, covering
GPU, edge, and mobile deployment. For the X, L, M, and S variants, we adopt
DINOv3-pretrained or distilled backbones and introduce a Spatial Tuning Adapter
(STA), which efficiently converts DINOv3's single-scale output into multi-scale
features and complements strong semantics with fine-grained details to enhance
detection. For ultra-lightweight models (Nano, Pico, Femto, and Atto), we
employ HGNetv2 with depth and width pruning to meet strict resource budgets.
Together with a simplified decoder and an upgraded Dense O2O, this unified
design enables DEIMv2 to achieve a superior performance-cost trade-off across
diverse scenarios, establishing new state-of-the-art results. Notably, our
largest model, DEIMv2-X, achieves 57.8 AP with only 50.3 million parameters,
surpassing prior X-scale models that require over 60 million parameters for
just 56.5 AP. On the compact side, DEIMv2-S is the first sub-10 million model
(9.71 million) to exceed the 50 AP milestone on COCO, reaching 50.9 AP. Even
the ultra-lightweight DEIMv2-Pico, with just 1.5 million parameters, delivers
38.5 AP, matching YOLOv10-Nano (2.3 million) with around 50 percent fewer
parameters.

</details>


### [27] [DAC-LoRA: Dynamic Adversarial Curriculum for Efficient and Robust Few-Shot Adaptation](https://arxiv.org/abs/2509.20792)
*Ved Umrajkar*

Main category: cs.CV

TL;DR: A lightweight, PEFT-friendly framework (DAC-LoRA) that injects a dynamic adversarial curriculum into LoRA-based fine-tuning, guided by FOSC and a TRADES-like loss, to robustify vision-language models against adversarial attacks with minimal loss in clean accuracy.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models (e.g., CLIP) and their PEFT adaptations are vulnerable to adversarial inputs. There is a need for robust, efficient training that fits standard PEFT workflows without large sacrifices to accuracy in benign settings.

Method: Introduce Dynamic Adversarial Curriculum (DAC) into LoRA-based PEFT. Employ an iterative, progressively harder attack curriculum guided by the First-Order Stationary Condition (FOSC) and use a TRADES-inspired loss to balance robustness and clean accuracy. The framework is lightweight and broadly applicable to PEFT pipelines and can extend to other iterative attack methods.

Result: DAC-LoRA yields substantial improvements in adversarial robustness while keeping clean accuracy near baseline levels; the method is easy to integrate into existing PEFT pipelines and demonstrates broad applicability across VLMs and downstream tasks.

Conclusion: DAC-LoRA provides a general, efficient approach to robustify PEFT-finetuned Vision-Language Models. Its dynamic adversarial curriculum and FOSC/TRADES-inspired optimization offer a practical path to safer deployment in critical applications, with potential applicability beyond the specific setting studied.

Abstract: Vision-Language Models (VLMs) are foundational to critical applications like
autonomous driving, medical diagnosis, and content moderation. While
Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA enable their efficient
adaptation to specialized tasks, these models remain vulnerable to adversarial
attacks that can compromise safety-critical decisions. CLIP, the backbone for
numerous downstream VLMs, is a high-value target whose vulnerabilities can
cascade across the multimodal AI ecosystem. We propose Dynamic Adversarial
Curriculum DAC-LoRA, a novel framework that integrates adversarial training
into PEFT. The core principle of our method i.e. an intelligent curriculum of
progressively challenging attack, is general and can potentially be applied to
any iterative attack method. Guided by the First-Order Stationary Condition
(FOSC) and a TRADES-inspired loss, DAC-LoRA achieves substantial improvements
in adversarial robustness without significantly compromising clean accuracy.
Our work presents an effective, lightweight, and broadly applicable method to
demonstrate that the DAC-LoRA framework can be easily integrated into a
standard PEFT pipeline to significantly enhance robustness.

</details>


### [28] [Federated Domain Generalization with Domain-specific Soft Prompts Generation](https://arxiv.org/abs/2509.20807)
*Jianhan Wu,Xiaoyang Qu,Zhangcheng Huang,Jianzong Wang*

Main category: cs.CV

TL;DR: FedDSPG introduces a generative approach to federated domain generalization by learning domain-specific soft prompts (DSPs) for each domain and using a generator to synthesize DSPs for unseen target domains, achieving state-of-the-art results in FDG under CLIP prompting.


<details>
  <summary>Details</summary>
Motivation: In federated settings, domain shifts across clients complicate downstream-task adaptation. Existing FDG methods with prompt learning suffer from limited prompt diversity and poor handling of unknown domains, leading to suboptimal generalization and inefficiency.

Method: During training, train with domain-specific soft prompts (DSPs) for each domain and use a generator to infuse content and domain knowledge across clients. In inference, the generator produces DSPs for unseen target domains to guide downstream tasks, forming a generative FDG framework that extends prompt learning.

Result: Comprehensive evaluations on public datasets show FedDSPG outperforms existing strong baselines for FDG, achieving state-of-the-art performance.

Conclusion: The paper presents a novel generative framework for FDG that leverages domain-specific prompt generation, enhancing prompt diversity and enabling robust adaptation to unseen domains with improved efficiency.

Abstract: Prompt learning has become an efficient paradigm for adapting CLIP to
downstream tasks. Compared with traditional fine-tuning, prompt learning
optimizes a few parameters yet yields highly competitive results, especially
appealing in federated learning for computational efficiency. engendering
domain shift among clients and posing a formidable challenge for
downstream-task adaptation. Existing federated domain generalization (FDG)
methods based on prompt learning typically learn soft prompts from training
samples, replacing manually designed prompts to enhance the generalization
ability of federated models. However, these learned prompts exhibit limited
diversity and tend to ignore information from unknown domains. We propose a
novel and effective method from a generative perspective for handling FDG
tasks, namely federated domain generalization with domain-specific soft prompts
generation (FedDSPG). Specifically, during training, we introduce
domain-specific soft prompts (DSPs) for each domain and integrate content and
domain knowledge into the generative model among clients. In the inference
phase, the generator is utilized to obtain DSPs for unseen target domains, thus
guiding downstream tasks in unknown domains. Comprehensive evaluations across
several public datasets confirm that our method outperforms existing strong
baselines in FDG, achieving state-of-the-art results.

</details>


### [29] [Revolutionizing Precise Low Back Pain Diagnosis via Contrastive Learning](https://arxiv.org/abs/2509.20813)
*Thanh Binh Le,Hoang Nhat Khang Vo,Tan-Ha Mai,Trong Nhan Phan*

Main category: cs.CV

TL;DR: LumbarCLIP is a multimodal contrastive learning framework aligning lumbar spine MRI scans with radiology reports, achieving strong downstream classification performance; linear projection heads yield better cross-modal alignment.


<details>
  <summary>Details</summary>
Motivation: There is a need for robust, automated diagnostic models that jointly interpret medical images and accompanying text reports to improve accuracy and clinical decision support in musculoskeletal care.

Method: LumbarCLIP uses multiple vision encoders (ResNet-50, ViT, Swin) and a BERT-based text encoder. Dense embeddings are projected into a shared space via learnable heads (linear or non-linear) and trained with a soft CLIP loss. A curated axial MRI dataset paired with expert reports is used, followed by ablation studies showing linear heads outperform nonlinear ones.

Result: The model achieves state-of-the-art downstream classification performance (up to 95.00% accuracy and 94.75% F1-score on the test set) despite class imbalance. Ablations indicate linear projection heads provide better cross-modal alignment than non-linear variants.

Conclusion: LumbarCLIP offers a promising foundation for automated musculoskeletal diagnosis and clinical decision support by effectively aligning imaging and textual radiology descriptions in a shared embedding space.

Abstract: Low back pain affects millions worldwide, driving the need for robust
diagnostic models that can jointly analyze complex medical images and
accompanying text reports. We present LumbarCLIP, a novel multimodal framework
that leverages contrastive language-image pretraining to align lumbar spine MRI
scans with corresponding radiological descriptions. Built upon a curated
dataset containing axial MRI views paired with expert-written reports,
LumbarCLIP integrates vision encoders (ResNet-50, Vision Transformer, Swin
Transformer) with a BERT-based text encoder to extract dense representations.
These are projected into a shared embedding space via learnable projection
heads, configurable as linear or non-linear, and normalized to facilitate
stable contrastive training using a soft CLIP loss. Our model achieves
state-of-the-art performance on downstream classification, reaching up to
95.00% accuracy and 94.75% F1-score on the test set, despite inherent class
imbalance. Extensive ablation studies demonstrate that linear projection heads
yield more effective cross-modal alignment than non-linear variants. LumbarCLIP
offers a promising foundation for automated musculoskeletal diagnosis and
clinical decision support.

</details>


### [30] [Poisoning Prompt-Guided Sampling in Video Large Language Models](https://arxiv.org/abs/2509.20851)
*Yuxin Cao,Wei Song,Jingling Xue,Jin Song Dong*

Main category: cs.CV

TL;DR: PoisonVID is a black-box poisoning attack that disrupts prompt-guided sampling in VideoLLMs by learning a universal perturbation; it achieves 82–99% attack success across three sampling strategies and three VideoLLMs, highlighting safety risks and the need for robust defenses.


<details>
  <summary>Details</summary>
Motivation: VideoLLMs rely on advanced, increasingly selective frame sampling strategies. While vulnerabilities in earlier sampling methods were shown, the safety of prompt-guided sampling—a dominant approach—had not been studied. This work motivates analyzing the resilience of prompt-guided sampling to poisoning and emphasizes the urgency for defenses.

Method: PoisonVID employs a closed-loop optimization to learn a universal perturbation that suppresses harmful frame relevance scores. It constructs a depiction set from paraphrased harmful descriptions using a shadow VideoLLM and a lightweight language model (GPT-4o-mini). The attack is designed as a black-box poisoning strategy and is evaluated on three prompt-guided sampling strategies and three advanced VideoLLMs.

Result: PoisonVID attains 82%–99% attack success across the tested strategies and models, demonstrating the vulnerability of prompt-guided sampling to poisoning in VideoLLMs.

Conclusion: The findings reveal a security risk in current prompt-guided sampling methods and underscore the need to develop defenses and more robust sampling strategies for VideoLLMs in order to mitigate such poisoning attacks.

Abstract: Video Large Language Models (VideoLLMs) have emerged as powerful tools for
understanding videos, supporting tasks such as summarization, captioning, and
question answering. Their performance has been driven by advances in frame
sampling, progressing from uniform-based to semantic-similarity-based and, most
recently, prompt-guided strategies. While vulnerabilities have been identified
in earlier sampling strategies, the safety of prompt-guided sampling remains
unexplored. We close this gap by presenting PoisonVID, the first black-box
poisoning attack that undermines prompt-guided sampling in VideoLLMs. PoisonVID
compromises the underlying prompt-guided sampling mechanism through a
closed-loop optimization strategy that iteratively optimizes a universal
perturbation to suppress harmful frame relevance scores, guided by a depiction
set constructed from paraphrased harmful descriptions leveraging a shadow
VideoLLM and a lightweight language model, i.e., GPT-4o-mini. Comprehensively
evaluated on three prompt-guided sampling strategies and across three advanced
VideoLLMs, PoisonVID achieves 82% - 99% attack success rate, highlighting the
importance of developing future advanced sampling strategies for VideoLLMs.

</details>


### [31] [Punching Above Precision: Small Quantized Model Distillation with Learnable Regularizer](https://arxiv.org/abs/2509.20854)
*Abdur Rehman,S M A Sharif,Md Abdur Rahaman,Mohamed Jismy Aashik Rasool,Seongwan Kim,Jaeho Lee*

Main category: cs.CV

TL;DR: A two-parameter learnable regularizer (GoR) for quantization-aware training with knowledge distillation that adaptively balances task-specific and distillation losses, reducing gradient conflicts and improving convergence for small quantized models. Extended with EKD-GoR ensemble distillation using multiple teachers; achieves state-of-the-art results across vision and language tasks and enables faster edge inference, with potential to surpass full-precision under optimal conditions.


<details>
  <summary>Details</summary>
Motivation: Existing QAT-KD methods suffer from heterogeneous gradient magnitudes between task-specific and distillation losses, especially under low-bit quantization, leading to imbalanced training and suboptimal performance.

Method: GoR introduces two trainable parameters to dynamically weight the TS and KD losses during QAT-KD training, acting as a learnable regularizer that balances supervision signals. The approach is extended with QAT-EKD-GoR, an ensemble distillation framework using multiple heterogeneous teacher models.

Result: Empirical results across image classification, object detection, and LLM compression show that GoR consistently outperforms state-of-the-art QAT-KD methods. On low-power edge devices, the approach yields faster inference while maintaining full-precision accuracy. Under optimal conditions, EKD-GoR can outperform full-precision models.

Conclusion: GoR effectively reduces conflicts between supervision signals, enhances convergence, and improves the performance of small quantized models. The EKD-GoR extension provides a robust, deployment-ready framework that can surpass full-precision models under favorable conditions and supports real-world AI model compression.

Abstract: Quantization-aware training (QAT) combined with knowledge distillation (KD)
is a promising strategy for compressing Artificial Intelligence (AI) models for
deployment on resource-constrained hardware. However, existing QAT-KD methods
often struggle to balance task-specific (TS) and distillation losses due to
heterogeneous gradient magnitudes, especially under low-bit quantization. We
propose Game of Regularizer (GoR), a novel learnable regularization method that
adaptively balances TS and KD objectives using only two trainable parameters
for dynamic loss weighting. GoR reduces conflict between supervision signals,
improves convergence, and boosts the performance of small quantized models
(SQMs). Experiments on image classification, object detection (OD), and large
language model (LLM) compression show that GoR consistently outperforms
state-of-the-art QAT-KD methods. On low-power edge devices, it delivers faster
inference while maintaining full-precision accuracy. We also introduce
QAT-EKD-GoR, an ensemble distillation framework that uses multiple
heterogeneous teacher models. Under optimal conditions, the proposed EKD-GoR
can outperform full-precision models, providing a robust solution for
real-world deployment.

</details>


### [32] [Plant identification based on noisy web data: the amazing performance of deep learning (LifeCLEF 2017)](https://arxiv.org/abs/2509.20856)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: The LifeCLEF 2017 plant identification challenge assesses whether a large, noisy, web-sourced training set can match the performance of a smaller, expert-verified training set for scalable plant identification, using a Pl@ntNet-based test set; it highlights the role of deep learning and international initiatives while underscoring data quality challenges.


<details>
  <summary>Details</summary>
Motivation: To enable scalable, continent-scale plant identification and to understand how training data quality (web-sourced vs expert-curated) affects performance, addressing species with few or no images and leveraging collaborative knowledge bases like EOL.

Method: A comparative evaluation framework: two training strategies (large noisy web-labeled data vs small trusted expert-validated data) trained systems evaluated on a test set drawn from the Pl@ntNet mobile app. The paper reports the resources, approaches of participating groups, and an analysis of outcomes.

Result: The study provides an analysis of the main outcomes from the challenge, detailing how data quality and training data strategies influence performance, and offering insights into the effectiveness of noisy web data versus curated data across large-scale plant identification systems.

Conclusion: Deep learning-enabled plant identification at continental scales is feasible, but data quality remains a critical bottleneck. Initiatives like EOL and community-driven resources can help, yet robust evaluation (as in LifeCLEF 2017) is essential to understand trade-offs between data quantity and quality.

Abstract: The 2017-th edition of the LifeCLEF plant identification challenge is an
important milestone towards automated plant identification systems working at
the scale of continental floras with 10.000 plant species living mainly in
Europe and North America illustrated by a total of 1.1M images. Nowadays, such
ambitious systems are enabled thanks to the conjunction of the dazzling recent
progress in image classification with deep learning and several outstanding
international initiatives, such as the Encyclopedia of Life (EOL), aggregating
the visual knowledge on plant species coming from the main national botany
institutes. However, despite all these efforts the majority of the plant
species still remain without pictures or are poorly illustrated. Outside the
institutional channels, a much larger number of plant pictures are available
and spread on the web through botanist blogs, plant lovers web-pages, image
hosting websites and on-line plant retailers. The LifeCLEF 2017 plant challenge
presented in this paper aimed at evaluating to what extent a large noisy
training dataset collected through the web and containing a lot of labelling
errors can compete with a smaller but trusted training dataset checked by
experts. To fairly compare both training strategies, the test dataset was
created from a third data source, i.e. the Pl@ntNet mobile application that
collects millions of plant image queries all over the world. This paper
presents more precisely the resources and assessments of the challenge,
summarizes the approaches and systems employed by the participating research
groups, and provides an analysis of the main outcomes.

</details>


### [33] [TasselNetV4: A vision foundation model for cross-scene, cross-scale, and cross-species plant counting](https://arxiv.org/abs/2509.20857)
*Xiaonan Hu,Xuebing Li,Jinyu Xu,Abdulkadir Duran Adan,Letian Zhou,Xuhui Zhu,Yanan Li,Wei Guo,Shouyang Liu,Wenzhong Liu,Hao Lu*

Main category: cs.CV

TL;DR: TasselNetV4 is a cross-species plant counting framework that extends TasselNet with a Vision Transformer backbone and multi-branch box-aware local counters, enabling robust cross-scene/scale/species counting. Demonstrated on PAC-105 and PAC-Somalia, it achieves superior counting accuracy and efficiency, positioning it as a vision foundation model for cross-domain plant counting.


<details>
  <summary>Details</summary>
Motivation: Plant biodiversity and frequent new cultivars make species-specific counting models impractical. Plants are dynamic and non-rigid, causing performance gaps for existing class-agnostic counting (CAC) and open-world detection. The work aims to shift from counting specific plant species to counting plants by how to count, enabling cross-species counting.

Method: Extend TasselNet to TasselNetV4 by (1) using a plainVision Transformer backbone, (2) introducing multi-branch box-aware local counters to enhance cross-scale robustness, and (3) aligning with the extract-and-match paradigm from CAC to support cross-species counting. Builds on the TasselNet lineage and is evaluated on two datasets (PAC-105, PAC-Somalia).

Result: Experimental results show TasselNetV4 achieves superior counting performance and high efficiency compared to state-of-the-art CAC models across challenging datasets.

Conclusion: TasselNetV4 emerges as a vision foundation model for cross-scene, cross-scale, and cross-species plant counting, addressing biodiversity challenges and enabling robust, scalable plant counting.

Abstract: Accurate plant counting provides valuable information for agriculture such as
crop yield prediction, plant density assessment, and phenotype quantification.
Vision-based approaches are currently the mainstream solution. Prior art
typically uses a detection or a regression model to count a specific plant.
However, plants have biodiversity, and new cultivars are increasingly bred each
year. It is almost impossible to exhaust and build all species-dependent
counting models. Inspired by class-agnostic counting (CAC) in computer vision,
we argue that it is time to rethink the problem formulation of plant counting,
from what plants to count to how to count plants. In contrast to most daily
objects with spatial and temporal invariance, plants are dynamic, changing with
time and space. Their non-rigid structure often leads to worse performance than
counting rigid instances like heads and cars such that current CAC and
open-world detection models are suboptimal to count plants. In this work, we
inherit the vein of the TasselNet plant counting model and introduce a new
extension, TasselNetV4, shifting from species-specific counting to
cross-species counting. TasselNetV4 marries the local counting idea of
TasselNet with the extract-and-match paradigm in CAC. It builds upon a plain
vision transformer and incorporates novel multi-branch box-aware local counters
used to enhance cross-scale robustness. Two challenging datasets, PAC-105 and
PAC-Somalia, are harvested. Extensive experiments against state-of-the-art CAC
models show that TasselNetV4 achieves not only superior counting performance
but also high efficiency.Our results indicate that TasselNetV4 emerges to be a
vision foundation model for cross-scene, cross-scale, and cross-species plant
counting.

</details>


### [34] [SD-RetinaNet: Topologically Constrained Semi-Supervised Retinal Lesion and Layer Segmentation in OCT](https://arxiv.org/abs/2509.20864)
*Botond Fazekas,Guilherme Aresta,Philipp Seeböck,Julia Mai,Ursula Schmidt-Erfurth,Hrvoje Bogunović*

Main category: cs.CV

TL;DR: A semi-supervised OCT biomarker segmentation method with a differentiable biomarker topology engine enforcing anatomical correctness, enabling joint layer-lesion learning with disentangled spatial/style representations, improving lesion and layer segmentation using partially labeled data.


<details>
  <summary>Details</summary>
Motivation: Current semi-supervised OCT segmentation methods produce anatomically implausible results, poorly model layer-lesion interactions, and lack topological guarantees; there is a need for anatomically constrained, trustworthy segmentation leveraging unlabeled data.

Method: Introduce a fully differentiable biomarker topology engine to enforce anatomical correctness; enable bidirectional layer–lesion interaction in a joint learning framework; learn disentangled representations separating spatial and style factors; utilize unlabeled and partially labeled OCT datasets.

Result: The approach outperforms state-of-the-art methods for both lesion and layer segmentation on public and internal OCT datasets; demonstrates generalization of layer segmentation to pathological cases with partial annotations; shows the ability to strictly enforce lesion location in plausible positions relative to layers.

Conclusion: Anatomical constraints in semi-supervised learning yield more accurate, robust, and trustworthy retinal biomarker segmentation and can be extended to enforce topological correctness in other medical imaging tasks.

Abstract: Optical coherence tomography (OCT) is widely used for diagnosing and
monitoring retinal diseases, such as age-related macular degeneration (AMD).
The segmentation of biomarkers such as layers and lesions is essential for
patient diagnosis and follow-up. Recently, semi-supervised learning has shown
promise in improving retinal segmentation performance. However, existing
methods often produce anatomically implausible segmentations, fail to
effectively model layer-lesion interactions, and lack guarantees on topological
correctness.
  To address these limitations, we propose a novel semi-supervised model that
introduces a fully differentiable biomarker topology engine to enforce
anatomically correct segmentation of lesions and layers. This enables joint
learning with bidirectional influence between layers and lesions, leveraging
unlabeled and diverse partially labeled datasets. Our model learns a
disentangled representation, separating spatial and style factors. This
approach enables more realistic layer segmentations and improves lesion
segmentation, while strictly enforcing lesion location in their anatomically
plausible positions relative to the segmented layers.
  We evaluate the proposed model on public and internal datasets of OCT scans
and show that it outperforms the current state-of-the-art in both lesion and
layer segmentation, while demonstrating the ability to generalize layer
segmentation to pathological cases using partially annotated training data. Our
results demonstrate the potential of using anatomical constraints in
semi-supervised learning for accurate, robust, and trustworthy retinal
biomarker segmentation.

</details>


### [35] [Plant identification in an open-world (LifeCLEF 2016)](https://arxiv.org/abs/2509.20870)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: LifeCLEF 2016 plant identification challenge: large-scale open-set evaluation with 110k images of 1000 West European plant species, collected via a participatory sensing platform; focuses on rejecting unknown classes to reduce false positives.


<details>
  <summary>Details</summary>
Motivation: Evaluate plant identification under realistic, large-scale open-world conditions for biodiversity monitoring and to address robustness to unseen species.

Method: Organize a large-scale dataset and open-set evaluation framework using data from a participatory sensing platform started in 2011; analyze resources, assessments, and approaches/systems from participating groups; provide an outcome analysis.

Result: The paper provides resources and assessments of the challenge, summarizes the approaches and systems used by participating groups, and analyzes the main outcomes of the Open-set LifeCLEF plant identification evaluation.

Conclusion: Open-set recognition is a key novelty for realistic plant identification; the overview clarifies resources, assessments, participating approaches, and outcomes, informing future work on handling unknown species.

Abstract: The LifeCLEF plant identification challenge aims at evaluating plant
identification methods and systems at a very large scale, close to the
conditions of a real-world biodiversity monitoring scenario. The 2016-th
edition was actually conducted on a set of more than 110K images illustrating
1000 plant species living in West Europe, built through a large-scale
participatory sensing platform initiated in 2011 and which now involves tens of
thousands of contributors. The main novelty over the previous years is that the
identification task was evaluated as an open-set recognition problem, i.e. a
problem in which the recognition system has to be robust to unknown and never
seen categories. Beyond the brute-force classification across the known classes
of the training set, the big challenge was thus to automatically reject the
false positive classification hits that are caused by the unknown classes. This
overview presents more precisely the resources and assessments of the
challenge, summarizes the approaches and systems employed by the participating
research groups, and provides an analysis of the main outcomes.

</details>


### [36] [SCRA-VQA: Summarized Caption-Rerank for Augmented Large Language Models in Visual Question Answering](https://arxiv.org/abs/2509.20871)
*Yan Zhang,Jiaqing Lin,Miao Zhang,Kui Xiao,Xiaoju Hou,Yue Zhao,Zhifei Li*

Main category: cs.CV

TL;DR: SCRA-VQA uses a pre-trained visual-language model to convert images into captions, then summarizes and reorders them to remove irrelevant information, generating contextual examples to guide a 6.7B LLM for KB-VQA. It achieves 38.8% OK-VQA and 34.6% A-OKVQA without end-to-end training (code: GitHub).


<details>
  <summary>Details</summary>
Motivation: Captions for images often contain noise irrelevant to the question, and LLMs struggle with VQA reasoning when solely conditioned on raw captions. The approach aims to improve LLM reasoning and task adaptability in KB-VQA without costly end-to-end training by producing concise, question-relevant caption-derived inputs.

Method: Apply a pre-trained visual-language model to extract image captions, then generate contextual examples for these captions and perform summarization and reordering (caption rerank) to drop unrelated content. Feed the refined captioned input to a 6.7B LLM for QA, avoiding end-to-end training.

Result: On OK-VQA and A-OKVQA, the method attains 38.8% and 34.6% accuracy, respectively, indicating strong performance gains for KB-VQA with a lightweight LLM and caption-focused guidance.

Conclusion: SCRA-VQA enhances LLM-based KB-VQA reasoning and adaptability by focusing image information through summarization and reranking of captions, enabling competitive results without expensive training; code is publicly available.

Abstract: Acquiring high-quality knowledge is a central focus in Knowledge-Based Visual
Question Answering (KB-VQA). Recent methods use large language models (LLMs) as
knowledge engines for answering. These methods generally employ image captions
as visual text descriptions to assist LLMs in interpreting images. However, the
captions frequently include excessive noise irrelevant to the question, and
LLMs generally do not comprehend VQA tasks, limiting their reasoning
capabilities. To address this issue, we propose the Summarized Caption-Rerank
Augmented VQA (SCRA-VQA), which employs a pre-trained visual language model to
convert images into captions. Moreover, SCRA-VQA generates contextual examples
for the captions while simultaneously summarizing and reordering them to
exclude unrelated information. The caption-rerank process enables LLMs to
understand the image information and questions better, thus enhancing the
model's reasoning ability and task adaptability without expensive end-to-end
training. Based on an LLM with 6.7B parameters, SCRA-VQA performs excellently
on two challenging knowledge-based VQA datasets: OK-VQA and A-OKVQA, achieving
accuracies of 38.8% and 34.6%. Our code is available at
https://github.com/HubuKG/SCRA-VQA.

</details>


### [37] [The Unanticipated Asymmetry Between Perceptual Optimization and Assessment](https://arxiv.org/abs/2509.20878)
*Jiabei Zhang,Qi Wang,Siyu Wu,Du Chen,Tianhe Wu*

Main category: cs.CV

TL;DR: Perceptual optimization objectives (fidelity vs. adversarial) do not align well with IQA metrics: metrics that score high on IQA do not always optimize perceptual quality, especially under adversarial training. Discriminators help suppress artifacts during optimization but their learned features transfer poorly to IQA backbones. Architecture matters: patch-level and convolutional designs yield better detail reconstruction than vanilla or Transformer-based ones. This highlights the need for principled loss-function design that accounts for IQA transferability.


<details>
  <summary>Details</summary>
Motivation: To investigate how perceptual loss objectives used for image optimization relate to their effectiveness as image quality assessment (IQA) metrics, uncover any misalignments, and understand how discriminator design and architecture influence both optimization and IQA transferability.

Method: Systematic analysis comparing fidelity-based and adversarial objectives as optimization losses, evaluating discriminators as IQA backbones, and comparing architectures (patch-level CNNs vs vanilla vs Transformers) in terms of optimization performance and IQA transferability across tasks.

Result: Fidelity metrics that perform well on IQA do not necessarily drive good perceptual optimization, and adversarial training amplifies this misalignment. Discriminators effectively suppress artifacts during optimization but their internal representations offer limited benefit when reused as IQA backbones. Architecturally, patch-level and convolutional discriminators yield more faithful detail reconstruction than vanilla or Transformer-based counterparts, highlighting the impact of discriminator design on optimization outcomes.

Conclusion: There is a meaningful asymmetry between optimization and assessment objectives in perceptual learning. Loss function design should account for IQA transferability, and discriminator architecture choices are crucial for achieving faithful detail reconstruction. These insights guide principled development of perceptual losses that align better with downstream IQA performance.

Abstract: Perceptual optimization is primarily driven by the fidelity objective, which
enforces both semantic consistency and overall visual realism, while the
adversarial objective provides complementary refinement by enhancing perceptual
sharpness and fine-grained detail. Despite their central role, the correlation
between their effectiveness as optimization objectives and their capability as
image quality assessment (IQA) metrics remains underexplored. In this work, we
conduct a systematic analysis and reveal an unanticipated asymmetry between
perceptual optimization and assessment: fidelity metrics that excel in IQA are
not necessarily effective for perceptual optimization, with this misalignment
emerging more distinctly under adversarial training. In addition, while
discriminators effectively suppress artifacts during optimization, their
learned representations offer only limited benefits when reused as backbone
initializations for IQA models. Beyond this asymmetry, our findings further
demonstrate that discriminator design plays a decisive role in shaping
optimization, with patch-level and convolutional architectures providing more
faithful detail reconstruction than vanilla or Transformer-based alternatives.
These insights advance the understanding of loss function design and its
connection to IQA transferability, paving the way for more principled
approaches to perceptual optimization.

</details>


### [38] [Integrating Object Interaction Self-Attention and GAN-Based Debiasing for Visual Question Answering](https://arxiv.org/abs/2509.20884)
*Zhifei Li,Feng Qiu,Yiran Wang,Yujing Xia,Kui Xiao,Miao Zhang,Yan Zhang*

Main category: cs.CV

TL;DR: IOG-VQA introduces object interaction self-attention with GAN-based debiasing to improve VQA performance under biased data, showing strong results on VQA-CP v1/v2.


<details>
  <summary>Details</summary>
Motivation: VQA models suffer from biases in training data, relying on superficial patterns and failing to generalize to diverse questions/images. There is a need to reduce dataset bias while capturing complex visual relationships.

Method: A two-component model: (1) Object Interaction Self-Attention to capture complex interactions among image objects, and (2) GAN-based debiasing to generate unbiased data distributions, enabling robust learning. The framework fuses visual and textual modalities.

Result: Experiments on VQA-CP v1 and v2 show strong performance compared with existing methods, especially on biased/imbalanced distributions, demonstrating the effectiveness of addressing both object interactions and dataset biases.

Conclusion: Addressing both object-object interactions and dataset biases is crucial for advancing VQA; the proposed IOG-VQA provides improved generalization by integrating self-attention and GAN-based debiasing.

Abstract: Visual Question Answering (VQA) presents a unique challenge by requiring
models to understand and reason about visual content to answer questions
accurately. Existing VQA models often struggle with biases introduced by the
training data, leading to over-reliance on superficial patterns and inadequate
generalization to diverse questions and images. This paper presents a novel
model, IOG-VQA, which integrates Object Interaction Self-Attention and
GAN-Based Debiasing to enhance VQA model performance. The self-attention
mechanism allows our model to capture complex interactions between objects
within an image, providing a more comprehensive understanding of the visual
context. Meanwhile, the GAN-based debiasing framework generates unbiased data
distributions, helping the model to learn more robust and generalizable
features. By leveraging these two components, IOG-VQA effectively combines
visual and textual information to address the inherent biases in VQA datasets.
Extensive experiments on the VQA-CP v1 and VQA-CP v2 datasets demonstrate that
our model shows excellent performance compared with the existing methods,
particularly in handling biased and imbalanced data distributions highlighting
the importance of addressing both object interactions and dataset biases in
advancing VQA tasks. Our code is available at
https://github.com/HubuKG/IOG-VQA.

</details>


### [39] [Nuclear Diffusion Models for Low-Rank Background Suppression in Videos](https://arxiv.org/abs/2509.20886)
*Tristan S. W. Stevens,Oisín Nolan,Jean-Luc Robert,Ruud J. G. van Sloun*

Main category: cs.CV

TL;DR: A novel Nuclear Diffusion method combining low-rank temporal modeling with diffusion posterior sampling to denoise/dehaze video; shows improved performance over RPCA in cardiac ultrasound.


<details>
  <summary>Details</summary>
Motivation: Sparsity-based RPCA struggles to capture rich temporal variability and structured noise in real video data; a more expressive approach is needed to preserve dynamic content while removing artifacts.

Method: A hybrid framework that integrates low-rank temporal modeling with diffusion posterior sampling, termed Nuclear Diffusion, leveraging deep generative priors for high-fidelity video restoration.

Result: Evaluated on cardiac ultrasound dehazing; demonstrates improved dehazing performance over traditional RPCA in metrics like gCNR (contrast) and KS statistic (signal preservation).

Conclusion: Combining model-based temporal structure with deep generative priors can yield high-fidelity video restoration, indicating the potential of Nuclear Diffusion for medical imaging and beyond.

Abstract: Video sequences often contain structured noise and background artifacts that
obscure dynamic content, posing challenges for accurate analysis and
restoration. Robust principal component methods address this by decomposing
data into low-rank and sparse components. Still, the sparsity assumption often
fails to capture the rich variability present in real video data. To overcome
this limitation, a hybrid framework that integrates low-rank temporal modeling
with diffusion posterior sampling is proposed. The proposed method, Nuclear
Diffusion, is evaluated on a real-world medical imaging problem, namely cardiac
ultrasound dehazing, and demonstrates improved dehazing performance compared to
traditional RPCA concerning contrast enhancement (gCNR) and signal preservation
(KS statistic). These results highlight the potential of combining model-based
temporal models with deep generative priors for high-fidelity video
restoration.

</details>


### [40] [FerretNet: Efficient Synthetic Image Detection via Local Pixel Dependencies](https://arxiv.org/abs/2509.20890)
*Shuqiao Liang,Jian Liu,Renzhang Chen,Quanlong Guan*

Main category: cs.CV

TL;DR: A lightweight detector FerretNet (1.1M params) detects synthetic images by leveraging local pixel dependencies (LPD) and reconstruction artifacts from generation, achieving 97.1% accuracy across 22 models, trained on 4-class ProGAN data, outperforming SOTA by 10.6%.


<details>
  <summary>Details</summary>
Motivation: The rising realism of synthetic images from VAEs, GANs, and LDMs creates detection challenges. The paper targets robust detection by exploiting generation-time artifacts and local texture/edge inconsistencies.

Method: Identify latent distribution deviations and decoding-induced smoothing as artifacts. Use local pixel dependencies (LPD) based on Markov Random Fields to reconstruct images from neighboring pixels, exposing texture continuity and edge coherence disruptions. Propose FerretNet, a lightweight 1.1M-parameter neural network that performs detection, trained on 4-class ProGAN data and evaluated on an open-world benchmark.

Result: FerretNet achieves an average accuracy of 97.1% on an open-world benchmark spanning 22 generative models, surpassing state-of-the-art methods by 10.6%.

Conclusion: A compact detector leveraging LPD-based reconstruction artifacts can robustly detect a wide range of synthetic images, offering strong performance with a small model size.

Abstract: The increasing realism of synthetic images generated by advanced models such
as VAEs, GANs, and LDMs poses significant challenges for synthetic image
detection. To address this issue, we explore two artifact types introduced
during the generation process: (1) latent distribution deviations and (2)
decoding-induced smoothing effects, which manifest as inconsistencies in local
textures, edges, and color transitions. Leveraging local pixel dependencies
(LPD) properties rooted in Markov Random Fields, we reconstruct synthetic
images using neighboring pixel information to expose disruptions in texture
continuity and edge coherence. Building upon LPD, we propose FerretNet, a
lightweight neural network with only 1.1M parameters that delivers efficient
and robust synthetic image detection. Extensive experiments demonstrate that
FerretNet, trained exclusively on the 4-class ProGAN dataset, achieves an
average accuracy of 97.1% on an open-world benchmark comprising across 22
generative models, surpassing state-of-the-art methods by 10.6%.

</details>


### [41] [Concepts in Motion: Temporal Bottlenecks for Interpretable Video Classification](https://arxiv.org/abs/2509.20899)
*Patrick Knab,Sascha Marton,Philipp J. Schubert,Drago Guggiana,Christian Bartelt*

Main category: cs.CV

TL;DR: MoTIF extends Concept Bottleneck Models to video by introducing a Moving Temporal Interpretable Framework, enabling concept-based explanations across time with global, local, and temporal views while maintaining competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to bring interpretable concept-based reasoning to video data, where temporal dependencies are crucial but CBMs have been mostly applied to static images.

Method: MoTIF is a transformer-inspired architecture that extends concept bottlenecks to sequences of arbitrary length. Concepts (objects, attributes, or higher-level components) recur over time as motifs. The model exposes three complementary interpretability views: global concept importance across the video, local relevance within sliding windows, and temporal dependencies of a concept over time.

Result: Experiments show that concept-based modeling can be effectively transferred to video, providing clearer interpretation of how concepts contribute over time while achieving competitive action recognition performance. Code is available at the provided GitHub repo.

Conclusion: MoTIF demonstrates the viability of temporally aware concept bottlenecks for video, enabling richer explanations through motif-based temporal reasoning and paving the way for further interpretable video models.

Abstract: Conceptual models such as Concept Bottleneck Models (CBMs) have driven
substantial progress in improving interpretability for image classification by
leveraging human-interpretable concepts. However, extending these models from
static images to sequences of images, such as video data, introduces a
significant challenge due to the temporal dependencies inherent in videos,
which are essential for capturing actions and events. In this work, we
introduce MoTIF (Moving Temporal Interpretable Framework), an architectural
design inspired by a transformer that adapts the concept bottleneck framework
for video classification and handles sequences of arbitrary length. Within the
video domain, concepts refer to semantic entities such as objects, attributes,
or higher-level components (e.g., 'bow', 'mount', 'shoot') that reoccur across
time - forming motifs collectively describing and explaining actions. Our
design explicitly enables three complementary perspectives: global concept
importance across the entire video, local concept relevance within specific
windows, and temporal dependencies of a concept over time. Our results
demonstrate that the concept-based modeling paradigm can be effectively
transferred to video data, enabling a better understanding of concept
contributions in temporal contexts while maintaining competitive performance.
Code available at github.com/patrick-knab/MoTIF.

</details>


### [42] [FSMODNet: A Closer Look at Few-Shot Detection in Multispectral Data](https://arxiv.org/abs/2509.20905)
*Manuel Nkegoum,Minh-Tan Pham,Élisa Fromont,Bruno Avignon,Sébastien Lefèvre*

Main category: cs.CV

TL;DR: A few-shot multispectral detector (FSMODNet) combines visible and thermal features via deformable attention to achieve strong detection in low-data settings.


<details>
  <summary>Details</summary>
Motivation: Detecting objects across multiple modalities (visible and thermal) with limited labeled data is challenging due to modality gaps, environmental variability, and data scarcity; leveraging complementary information from both modalities can improve detection under challenging conditions.

Method: Propose FSMODNet, a framework that performs cross-modality feature integration using deformable attention to fuse visible and thermal features for few-shot object detection. The approach emphasizes robust feature alignment and efficient use of limited labels, evaluated on two public datasets.

Result: FSMODNet achieves superior performance over several baselines and state-of-the-art models in low-data regimes, demonstrating robust detection under difficult illumination and environmental conditions across two public datasets.

Conclusion: FSMODNet shows that cross-modality feature integration with deformable attention enables effective few-shot multispectral detection, and the accompanying code, models, and data splits are publicly available for reproducibility.

Abstract: Few-shot multispectral object detection (FSMOD) addresses the challenge of
detecting objects across visible and thermal modalities with minimal annotated
data. In this paper, we explore this complex task and introduce a framework
named "FSMODNet" that leverages cross-modality feature integration to improve
detection performance even with limited labels. By effectively combining the
unique strengths of visible and thermal imagery using deformable attention, the
proposed method demonstrates robust adaptability in complex illumination and
environmental conditions. Experimental results on two public datasets show
effective object detection performance in challenging low-data regimes,
outperforming several baselines we established from state-of-the-art models.
All code, models, and experimental data splits can be found at
https://anonymous.4open.science/r/Test-B48D.

</details>


### [43] [Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences](https://arxiv.org/abs/2509.20906)
*Julius Pesonen,Arno Solin,Eija Honkavaara*

Main category: cs.CV

TL;DR: Particle-filter-based 3D object localization from camera sequences using camera poses and image segments; detector-agnostic and suitable under limited resources; validated in simulation and drone data, with wildfire monitoring application.


<details>
  <summary>Details</summary>
Motivation: Standard depth estimation or full 3D reconstruction are computationally expensive and unreliable for distant objects and resource-constrained scenarios; a flexible, detector-agnostic approach is needed for safe, practical localization.

Method: Apply a particle filter for single and multiple targets using GNSS-based camera poses and image segmentation outputs; tested in 3D simulation and drone sequences; decomposes the localization task from the detection method, enabling use with any segmentation/detection model.

Result: The particle filter approach enables practical 3D localization from camera poses and image segments when dense depth or full reconstruction is infeasible; effective in both single- and multi-target scenarios and demonstrated with drone wildfire monitoring using existing segmentation models.

Conclusion: A detector-agnostic, resource-efficient particle-filter framework can perform reliable 3D object localization from camera measurements, broadening applicability in safety-critical tasks and enabling integration with existing segmentation systems.

Abstract: 3D object localisation based on a sequence of camera measurements is
essential for safety-critical surveillance tasks, such as drone-based wildfire
monitoring. Localisation of objects detected with a camera can typically be
solved with dense depth estimation or 3D scene reconstruction. However, in the
context of distant objects or tasks limited by the amount of available
computational resources, neither solution is feasible. In this paper, we show
that the task can be solved using particle filters for both single and multiple
target scenarios. The method was studied using a 3D simulation and a
drone-based image segmentation sequence with global navigation satellite system
(GNSS)-based camera pose estimates. The results showed that a particle filter
can be used to solve practical localisation tasks based on camera poses and
image segments in these situations where other solutions fail. The particle
filter is independent of the detection method, making it flexible for new
tasks. The study also demonstrates that drone-based wildfire monitoring can be
conducted using the proposed method paired with a pre-existing image
segmentation model.

</details>


### [44] [SwinMamba: A hybrid local-global mamba framework for enhancing semantic segmentation of remotely sensed images](https://arxiv.org/abs/2509.20918)
*Qinfeng Zhu,Han Li,Liang He,Lei Fan*

Main category: cs.CV

TL;DR: SwinMamba fuses localized Mamba-style scanning within shifted windows with global context to improve remote-sensing semantic segmentation, achieving state-of-the-art results on LoveDA and ISPRS Potsdam with efficient computation.


<details>
  <summary>Details</summary>
Motivation: Remote sensing images have very high resolution, complex scene structures, and diverse object scales. While Vision Transformers and Vision Mamba offer global receptive fields, they may miss crucial local textures and edges. A method that combines local detail with global context can improve segmentation accuracy and robustness.

Method: SwinMamba adopts a four-stage architecture inspired by Swin Transformer. The first two stages perform local scanning to capture fine-grained textures; the last two stages perform global scanning to fuse broad context. Overlapping shifted windows enable better inter-region information exchange, yielding a global receptive field with efficient computation.

Result: Extensive experiments on LoveDA and ISPRS Potsdam show SwinMamba outperforms state-of-the-art methods, demonstrating strong effectiveness and robustness for remote-sensing semantic segmentation.

Conclusion: Integrating localized Mamba-style scanning within shifted windows and a global receptive field provides a powerful, efficient framework for semantic segmentation of high-resolution remote sensing imagery, offering a promising solution over existing architectures.

Abstract: Semantic segmentation of remote sensing imagery is a fundamental task in
computer vision, supporting a wide range of applications such as land use
classification, urban planning, and environmental monitoring. However, this
task is often challenged by the high spatial resolution, complex scene
structures, and diverse object scales present in remote sensing data. To
address these challenges, various deep learning architectures have been
proposed, including convolutional neural networks, Vision Transformers, and the
recently introduced Vision Mamba. Vision Mamba features a global receptive
field and low computational complexity, demonstrating both efficiency and
effectiveness in image segmentation. However, its reliance on global scanning
tends to overlook critical local features, such as textures and edges, which
are essential for achieving accurate segmentation in remote sensing contexts.
To tackle this limitation, we propose SwinMamba, a novel framework inspired by
the Swin Transformer. SwinMamba integrates localized Mamba-style scanning
within shifted windows with a global receptive field, to enhance the model's
perception of both local and global features. Specifically, the first two
stages of SwinMamba perform local scanning to capture fine-grained details,
while its subsequent two stages leverage global scanning to fuse broader
contextual information. In our model, the use of overlapping shifted windows
enhances inter-region information exchange, facilitating more robust feature
integration across the entire image. Extensive experiments on the LoveDA and
ISPRS Potsdam datasets demonstrate that SwinMamba outperforms state-of-the-art
methods, underscoring its effectiveness and potential as a superior solution
for semantic segmentation of remotely sensed imagery.

</details>


### [45] [Revisiting Data Challenges of Computational Pathology: A Pack-based Multiple Instance Learning Framework](https://arxiv.org/abs/2509.20923)
*Wenhao Tang,Heng Fang,Ge Wu,Xiang Li,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: A pack-based MIL framework for computational pathology that preserves data heterogeneity and reduces redundancy by pack-based sampling, residual hyperslide supervision, and attention-driven downsampling, achieving up to 8% accuracy gain with 12% training time on PANDA(UNI).


<details>
  <summary>Details</summary>
Motivation: WSIs have extremely long sequence lengths (up to 200K), wide length variation (200–200K), and limited supervision, causing data heterogeneity and redundancy. Conventional methods struggle to train efficiently while preserving heterogeneity under limited labels.

Method: Introduce a pack-based MIL framework that packs multiple sampled, variable-length feature sequences into fixed-length packs to enable batched training while preserving heterogeneity. Add a residual branch that composes discarded features from multiple slides into a hyperslide with tailored labels to enable multi-slide supervision while mitigating feature loss from sampling. Include an attention-driven downsampler to compress features in both branches to reduce redundancy.

Result: The approach achieves up to 8% accuracy improvement and uses only 12% of the training time compared with baseline PANDA(UNI); extensive experiments validate the benefits, and code is available at the provided GitHub link.

Conclusion: Focusing data challenges in computational pathology holds significant potential for the era of foundation models; the pack MIL framework effectively balances data heterogeneity, efficiency, and performance and can serve as a practical step toward scalable CPath analysis.

Abstract: Computational pathology (CPath) digitizes pathology slides into whole slide
images (WSIs), enabling analysis for critical healthcare tasks such as cancer
diagnosis and prognosis. However, WSIs possess extremely long sequence lengths
(up to 200K), significant length variations (from 200 to 200K), and limited
supervision. These extreme variations in sequence length lead to high data
heterogeneity and redundancy. Conventional methods often compromise on training
efficiency and optimization to preserve such heterogeneity under limited
supervision. To comprehensively address these challenges, we propose a
pack-based MIL framework. It packs multiple sampled, variable-length feature
sequences into fixed-length ones, enabling batched training while preserving
data heterogeneity. Moreover, we introduce a residual branch that composes
discarded features from multiple slides into a hyperslide which is trained with
tailored labels. It offers multi-slide supervision while mitigating feature
loss from sampling. Meanwhile, an attention-driven downsampler is introduced to
compress features in both branches to reduce redundancy. By alleviating these
challenges, our approach achieves an accuracy improvement of up to 8% while
using only 12% of the training time in the PANDA(UNI). Extensive experiments
demonstrate that focusing data challenges in CPath holds significant potential
in the era of foundation models. The code is
https://github.com/FangHeng/PackMIL

</details>


### [46] [SimDiff: Simulator-constrained Diffusion Model for Physically Plausible Motion Generation](https://arxiv.org/abs/2509.20927)
*Akihisa Watanabe,Jiawei Ren,Li Siyao,Yichen Peng,Erwin Wu,Edgar Simo-Serra*

Main category: cs.CV

TL;DR: A diffusion-based simulator-constrained model (SimDiff) conditions on environment physics to generate physically plausible human motion efficiently, avoiding repeated simulator calls and enabling fine-grained control; it generalizes to unseen parameter combinations.


<details>
  <summary>Details</summary>
Motivation: Generating physically plausible human motion efficiently is crucial for animation/VR. Existing simulator-based motion projection is accurate but computationally expensive due to its sequential nature, hindering parallelization. A fast, controllable method with compositional generalization is highly desirable.

Method: Reinterpret simulator-based motion projection as diffusion guidance (classifier-based or classifier-free). Develop SimDiff, a Simulator-constrained Diffusion Model that injects environment parameters (e.g., gravity, wind) into the denoising process. By conditioning on these parameters, the model produces physically plausible motions without repeated simulator calls at inference time, and offers fine-grained control over physical coefficients.

Result: SimDiff enables efficient generation of physically plausible motions, with conditioning on environment parameters yielding controlled outputs. It generalizes to unseen combinations of environmental parameters, demonstrating compositional generalization.

Conclusion: Integrating environment parameters into diffusion-based generation yields a fast, controllable, and physically plausible motion synthesis framework, avoiding expensive simulation loops and enabling compositional generalization across parameter configurations.

Abstract: Generating physically plausible human motion is crucial for applications such
as character animation and virtual reality. Existing approaches often
incorporate a simulator-based motion projection layer to the diffusion process
to enforce physical plausibility. However, such methods are computationally
expensive due to the sequential nature of the simulator, which prevents
parallelization. We show that simulator-based motion projection can be
interpreted as a form of guidance, either classifier-based or classifier-free,
within the diffusion process. Building on this insight, we propose SimDiff, a
Simulator-constrained Diffusion Model that integrates environment parameters
(e.g., gravity, wind) directly into the denoising process. By conditioning on
these parameters, SimDiff generates physically plausible motions efficiently,
without repeated simulator calls at inference, and also provides fine-grained
control over different physical coefficients. Moreover, SimDiff successfully
generalizes to unseen combinations of environmental parameters, demonstrating
compositional generalization.

</details>


### [47] [Unlocking Noise-Resistant Vision: Key Architectural Secrets for Robust Models](https://arxiv.org/abs/2509.20939)
*Bum Jun Kim,Makoto Kawano,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While the robustness of vision models is often measured, their dependence on
specific architectural design choices is rarely dissected. We investigate why
certain vision architectures are inherently more robust to additive Gaussian
noise and convert these empirical insights into simple, actionable design
rules. Specifically, we performed extensive evaluations on 1,174 pretrained
vision models, empirically identifying four consistent design patterns for
improved robustness against Gaussian noise: larger stem kernels, smaller input
resolutions, average pooling, and supervised vision transformers (ViTs) rather
than CLIP ViTs, which yield up to 506 rank improvements and 21.6\%p accuracy
gains. We then develop a theoretical analysis that explains these findings,
converting observed correlations into causal mechanisms. First, we prove that
low-pass stem kernels attenuate noise with a gain that decreases quadratically
with kernel size and that anti-aliased downsampling reduces noise energy
roughly in proportion to the square of the downsampling factor. Second, we
demonstrate that average pooling is unbiased and suppresses noise in proportion
to the pooling window area, whereas max pooling incurs a positive bias that
grows slowly with window size and yields a relatively higher mean-squared error
and greater worst-case sensitivity. Third, we reveal and explain the
vulnerability of CLIP ViTs via a pixel-space Lipschitz bound: The smaller
normalization standard deviations used in CLIP preprocessing amplify worst-case
sensitivity by up to 1.91 times relative to the Inception-style preprocessing
common in supervised ViTs. Our results collectively disentangle robustness into
interpretable modules, provide a theory that explains the observed trends, and
build practical, plug-and-play guidelines for designing vision models more
robust against Gaussian noise.

</details>


### [48] [Decoding the Surgical Scene: A Scoping Review of Scene Graphs in Surgery](https://arxiv.org/abs/2509.20941)
*Angelo Henriques,Korab Hoxha,Daniel Zapp,Peter C. Issa,Nassir Navab,M. Ali Nasseri*

Main category: cs.CV

TL;DR: Surgical scene graphs are rapidly maturing, revealing a data divide between real 2D/internal tasks and simulated 4D external tasks, and transitioning from graph nets to foundation models to support analysis and controllable simulation.


<details>
  <summary>Details</summary>
Motivation: Need for structured, scalable representations to handle dynamic, complex surgical environments, enabling safer, more efficient, and better-trained systems.

Method: PRISMA-ScR-guided scoping review mapping SG research across applications, methods, and future directions within surgery.

Result: Growth is rapid; critical data divide; methodological progression from GNNs to specialized foundation models; SGs underpin analysis (workflow recognition, safety monitoring) and generative tasks (controllable simulation); annotation and real-time deployment remain challenges, being actively addressed.

Conclusion: SGs are becoming essential semantic bridges enabling intelligent surgical systems that improve safety, efficiency, and training; ongoing work aims to close data/annotation gaps and enable real-time deployment.

Abstract: Scene graphs (SGs) provide structured relational representations crucial for
decoding complex, dynamic surgical environments. This PRISMA-ScR-guided scoping
review systematically maps the evolving landscape of SG research in surgery,
charting its applications, methodological advancements, and future directions.
Our analysis reveals rapid growth, yet uncovers a critical 'data divide':
internal-view research (e.g., triplet recognition) almost exclusively uses
real-world 2D video, while external-view 4D modeling relies heavily on
simulated data, exposing a key translational research gap. Methodologically,
the field has advanced from foundational graph neural networks to specialized
foundation models that now significantly outperform generalist large
vision-language models in surgical contexts. This progress has established SGs
as a cornerstone technology for both analysis, such as workflow recognition and
automated safety monitoring, and generative tasks like controllable surgical
simulation. Although challenges in data annotation and real-time implementation
persist, they are actively being addressed through emerging techniques.
Surgical SGs are maturing into an essential semantic bridge, enabling a new
generation of intelligent systems to improve surgical safety, efficiency, and
training.

</details>


### [49] [A Real-Time On-Device Defect Detection Framework for Laser Power-Meter Sensors via Unsupervised Learning](https://arxiv.org/abs/2509.20946)
*Dongqi Zheng,Wenjin Fu,Guangzong Chen*

Main category: cs.CV

TL;DR: Unsupervised anomaly detection system for detecting coating defects on laser power meter sensors using a three-stage pipeline (preprocess with Laplacian edge detection + K-means segmentation, StyleGAN2 augmentation, UFlow for multi-scale feature extraction). Trained only on good samples; achieves high accuracy and AUROC on real data with on-device 0.5s per image, suggesting cost savings for automated QC.


<details>
  <summary>Details</summary>
Motivation: Defects in sensor coatings (thermal damage, scratches) can corrupt laser energy measurements in medical/industrial contexts. The goal is robust defect detection that can identify both known and novel defects without extensive labeled defect datasets.

Method: Preprocessing: Laplacian edge detection + K-means to segment area of interest. Data augmentation: StyleGAN2 to synthesize additional samples. Modeling: UFlow-based neural network for multi-scale feature extraction and anomaly map generation. Framework is unsupervised, trained on good samples to learn normal coating distributions, enabling anomaly detection of defects.

Result: Evaluation on 366 real sensor images shows 93.8% accuracy for defective samples and 89.3% for good samples; image-level AUROC 0.957; pixel-level AUROC 0.961; processing time ~0.5 seconds per image on-device; potential annual cost savings through automated QC.

Conclusion: The approach enables effective detection of coating defects, including unseen types, with on-device, fast inference, reducing reliance on labeled defect datasets and offering practical cost and throughput benefits.

Abstract: We present an automated vision-based system for defect detection and
classification of laser power meter sensor coatings. Our approach addresses the
critical challenge of identifying coating defects such as thermal damage and
scratches that can compromise laser energy measurement accuracy in medical and
industrial applications. The system employs an unsupervised anomaly detection
framework that trains exclusively on ``good'' sensor images to learn normal
coating distribution patterns, enabling detection of both known and novel
defect types without requiring extensive labeled defect datasets. Our
methodology consists of three key components: (1) a robust preprocessing
pipeline using Laplacian edge detection and K-means clustering to segment the
area of interest, (2) synthetic data augmentation via StyleGAN2, and (3) a
UFlow-based neural network architecture for multi-scale feature extraction and
anomaly map generation. Experimental evaluation on 366 real sensor images
demonstrates $93.8\%$ accuracy on defective samples and $89.3\%$ accuracy on
good samples, with image-level AUROC of 0.957 and pixel-level AUROC of 0.961.
The system provides potential annual cost savings through automated quality
control and processing times of 0.5 seconds per image in on-device
implementation.

</details>


### [50] [Unlocking Financial Insights: An advanced Multimodal Summarization with Multimodal Output Framework for Financial Advisory Videos](https://arxiv.org/abs/2509.20961)
*Sarmistha Das,R E Zera Marveen Lyngkhoi,Sriparna Saha,Alka Maurya*

Main category: cs.CV

TL;DR: FASTER is a modular multimodal summariser for financial advisory videos that extracts modality-specific features (text, images, audio), aligns keyframes with summaries, and ensures factuality with a BOS-aware DPO loss, aided by a ranker-based retrieval and a new Fin-APT dataset; it achieves strong cross-domain performance vs LLMs/VLMs and is released with code and data.


<details>
  <summary>Details</summary>
Motivation: The surge of dynamic, long-format financial advisory content on social media poses challenges for extracting concise, coherent, and factually consistent summaries. There is also a data scarcity issue for multimodal financial content.

Method: FASTER builds a modular pipeline: (1) modality-specific feature extraction (BLIP for visual descriptions, OCR for text patterns, Whisper transcription with speaker diarization as BOS features); (2) a modified Direct Preference Optimization (DPO) loss with BOS-specific fact-checking to ensure precision, relevance, and factual consistency; (3) a ranker-based retrieval module to align keyframes with summarized content; (4) Fin-APT, a new dataset of 470 publicly accessible financial advisory pep-talk videos; (5) extensive cross-domain experiments comparing against LLMs and VLMs to validate robustness and generalizability.

Result: Empirical results show strong performance, robustness, and generalizability across domains, with improved interpretability and cross-modal coherence relative to LLMs/VLMs. The approach outperforms baselines in alignment and factual consistency, and the authors release the Fin-APT dataset and code.

Conclusion: FASTER establishes a new standard for multimodal summarization in finance by integrating modality-aware features, factual-consistency-aware optimization, and cross-modal alignment, making financial advisory content more accessible and actionable and enabling broader research; dataset and code are publicly available.

Abstract: The dynamic propagation of social media has broadened the reach of financial
advisory content through podcast videos, yet extracting insights from lengthy,
multimodal segments (30-40 minutes) remains challenging. We introduce FASTER
(Financial Advisory Summariser with Textual Embedded Relevant images), a
modular framework that tackles three key challenges: (1) extracting
modality-specific features, (2) producing optimized, concise summaries, and (3)
aligning visual keyframes with associated textual points. FASTER employs BLIP
for semantic visual descriptions, OCR for textual patterns, and Whisper-based
transcription with Speaker diarization as BOS features. A modified Direct
Preference Optimization (DPO)-based loss function, equipped with BOS-specific
fact-checking, ensures precision, relevance, and factual consistency against
the human-aligned summary. A ranker-based retrieval mechanism further aligns
keyframes with summarized content, enhancing interpretability and cross-modal
coherence. To acknowledge data resource scarcity, we introduce Fin-APT, a
dataset comprising 470 publicly accessible financial advisory pep-talk videos
for robust multimodal research. Comprehensive cross-domain experiments confirm
FASTER's strong performance, robustness, and generalizability when compared to
Large Language Models (LLMs) and Vision-Language Models (VLMs). By establishing
a new standard for multimodal summarization, FASTER makes financial advisory
content more accessible and actionable, thereby opening new avenues for
research. The dataset and code are available at:
https://github.com/sarmistha-D/FASTER

</details>


### [51] [An Adaptor for Triggering Semi-Supervised Learning to Out-of-Box Serve Deep Image Clustering](https://arxiv.org/abs/2509.20976)
*Yue Duan,Lei Qi,Yinghuan Shi,Yang Gao*

Main category: cs.CV

TL;DR: ASD provides a cold-start adaptor for SSL in deep image clustering, enabling clustering without pretraining by using pseudo-labels and an instance-to-cluster label transfer, achieving competitive results with a small gap to fully supervised SSL.


<details>
  <summary>Details</summary>
Motivation: Current SSL-based clustering methods require prerequisites such as pretraining, clustering steps, or a trained clustering model, which limits out-of-the-box applicability for image clustering. A true cold-start SSL approach is desired.

Method: 1) Randomly sample pseudo-labeled data from all unlabeled data. 2) Train an instance-level classifier on these pseudo-labels with semantically aligned instance-level labels. 3) Track class transitions of predictions on unlabeled data to extract high-level similarities among instance-level classes. 4) Use these similarities to assign cluster-level labels to pseudo-labeled data. 5) Use the pseudo-labeled data with cluster labels to trigger a general SSL learner trained on the unlabeled data for image clustering.

Result: ASD achieves superior performance compared with latest deep image clustering methods and narrows the gap to SSL with ground-truth (e.g., about 1.33% on CIFAR-10). It can also further boost the performance of existing SSL-embedded deep image clustering approaches.

Conclusion: ASD enables cold-start SSL for deep image clustering without prerequisites and can serve as a drop-in to enhance existing SSL-based clustering pipelines.

Abstract: Recently, some works integrate SSL techniques into deep clustering frameworks
to enhance image clustering performance. However, they all need pretraining,
clustering learning, or a trained clustering model as prerequisites, limiting
the flexible and out-of-box application of SSL learners in the image clustering
task. This work introduces ASD, an adaptor that enables the cold-start of SSL
learners for deep image clustering without any prerequisites. Specifically, we
first randomly sample pseudo-labeled data from all unlabeled data, and set an
instance-level classifier to learn them with semantically aligned
instance-level labels. With the ability of instance-level classification, we
track the class transitions of predictions on unlabeled data to extract
high-level similarities of instance-level classes, which can be utilized to
assign cluster-level labels to pseudo-labeled data. Finally, we use the
pseudo-labeled data with assigned cluster-level labels to trigger a general SSL
learner trained on the unlabeled data for image clustering. We show the
superior performance of ASD across various benchmarks against the latest deep
image clustering approaches and very slight accuracy gaps compared to SSL
methods using ground-truth, e.g., only 1.33% on CIFAR-10. Moreover, ASD can
also further boost the performance of existing SSL-embedded deep image
clustering methods.

</details>


### [52] [SiNGER: A Clearer Voice Distills Vision Transformers Further](https://arxiv.org/abs/2509.20986)
*Geunhyeok Yu,Sunjae Jeong,Yoonyoung Choi,Jaeseung Kim,Hyoseok Hwang*

Main category: cs.CV

TL;DR: SiNGER: a singular nullspace-guided energy reallocation framework for distilling Vision Transformers that suppresses high-norm artifacts while preserving informative teacher signals, via a LoRA-based adapter, yielding better student performance and clearer representations.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers often generate high-norm artifacts that degrade representation quality. During knowledge distillation these artifacts can dominate the loss, causing students to overfit to artifacts and underutilize informative signals from teachers. Prior artifact-removal methods trade off artifact suppression against preserving informative cues.

Method: Refine teacher features using a singular nullspace-guided perturbation to suppress artifacts while preserving information. This SI-NGER perturbation is implemented efficiently with a LoRA-based adapter to minimize structural changes. Distill the refined teacher features to the student.

Result: Extensive experiments show that SiNGER consistently improves student models, achieving strong state-of-the-art performance across multiple downstream tasks and producing clearer, more interpretable representations.

Conclusion: SiNGER effectively suppresses artifacts while preserving informative signals, enabling better student learning and making representations clearer and more interpretable; the approach is efficient due to the LoRA-based implementation.

Abstract: Vision Transformers are widely adopted as the backbone of vision foundation
models, but they are known to produce high-norm artifacts that degrade
representation quality. When knowledge distillation transfers these features to
students, high-norm artifacts dominate the objective, so students overfit to
artifacts and underweight informative signals, diminishing the gains from
larger models. Prior work attempted to remove artifacts but encountered an
inherent trade-off between artifact suppression and preserving informative
signals from teachers. To address this, we introduce Singular Nullspace-Guided
Energy Reallocation (SiNGER), a novel distillation framework that suppresses
artifacts while preserving informative signals. The key idea is principled
teacher feature refinement: during refinement, we leverage the nullspace-guided
perturbation to preserve information while suppressing artifacts. Then, the
refined teacher's features are distilled to a student. We implement this
perturbation efficiently with a LoRA-based adapter that requires minimal
structural modification. Extensive experiments show that \oursname consistently
improves student models, achieving state-of-the-art performance in multiple
downstream tasks and producing clearer and more interpretable representations.

</details>


### [53] [Fast-SEnSeI: Lightweight Sensor-Independent Cloud Masking for On-board Multispectral Sensors](https://arxiv.org/abs/2509.20991)
*Jan Kněžík,Jonáš Herec,Rado Pitoňák*

Main category: cs.CV

TL;DR: Introduces Fast-SEnSeI, a sensor-independent encoder for on-board cloud segmentation across multispectral sensors with varying bands, producing fixed-size features for a compact U-Net; deployed via TVM on CPU and FPGA for a space-qualified CPU-FPGA pipeline; validated on Sentinel-2 and Landsat 8.


<details>
  <summary>Details</summary>
Motivation: Eliminate dependency on fixed sensor configurations and ground processing by enabling flexible, on-board cloud segmentation across diverse multispectral sensors, reducing data downlink and enabling autonomous Earth observation tasks.

Method: Enhance SEnSeI-v2 with an improved spectral descriptor, lighter architecture, and robust padding-band handling. Accepts arbitrary band combinations/wavelengths, outputs fixed-size feature maps that feed into a compact, quantized segmentation model based on a modified U-Net. Runs on embedded CPU via Apache TVM; segmentation model deployed on FPGA in a CPU-FPGA hybrid pipeline suitable for space hardware.

Result: Demonstrates accurate cloud segmentation across diverse input configurations on Sentinel-2 and Landsat 8, showing the feasibility of a sensor-independent, on-board segmentation pipeline that works with space-qualified hardware.

Conclusion: Fast-SEnSeI enables flexible, on-board cloud segmentation across multispectral sensors with varying bands, delivering a hardware-efficient CPU-FPGA pipeline suitable for space missions.

Abstract: Cloud segmentation is a critical preprocessing step for many Earth
observation tasks, yet most models are tightly coupled to specific sensor
configurations and rely on ground-based processing. In this work, we propose
Fast-SEnSeI, a lightweight, sensor-independent encoder module that enables
flexible, on-board cloud segmentation across multispectral sensors with varying
band configurations. Building upon SEnSeI-v2, Fast-SEnSeI integrates an
improved spectral descriptor, lightweight architecture, and robust padding-band
handling. It accepts arbitrary combinations of spectral bands and their
wavelengths, producing fixed-size feature maps that feed into a compact,
quantized segmentation model based on a modified U-Net. The module runs
efficiently on embedded CPUs using Apache TVM, while the segmentation model is
deployed on FPGA, forming a CPU-FPGA hybrid pipeline suitable for
space-qualified hardware. Evaluations on Sentinel-2 and Landsat 8 datasets
demonstrate accurate segmentation across diverse input configurations.

</details>


### [54] [A Single Neuron Works: Precise Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/abs/2509.21008)
*Qinqin He,Jiaqi Weng,Jialing Tao,Hui Xue*

Main category: cs.CV

TL;DR: Single Neuron-based Concept Erasure (SNCE) surgically removes harmful content by silencing a single neuron, using a Sparse Autoencoder to map text embeddings into a sparse latent space where neurons align with atomic concepts; a frequency-based neuron identification identifies the harmful-concept neuron; suppression yields precise erasure with minimal image quality loss.


<details>
  <summary>Details</summary>
Motivation: Safety concerns in text-to-image generation demand precise removal of harmful concepts with minimal degradation to overall image quality; existing concept-erasure methods struggle to accurately target concepts without collateral loss.

Method: Train a Sparse Autoencoder (SAE) to map text embeddings into a sparse, disentangled latent space where individual neurons align with atomic semantic concepts. Identify the harm-related neuron via a novel modulated frequency scoring of activation patterns, then suppress that neuron's activations to erase the concept.

Result: Achieves state-of-the-art target concept erasure while preserving generation capabilities for non-target concepts; demonstrates robustness against adversarial attacks compared to existing methods.

Conclusion: SNCE provides surgical, single-neuron concept erasure with minimal quality degradation and strong robustness, offering a precise and safe approach for content control in text-to-image models.

Abstract: Text-to-image models exhibit remarkable capabilities in image generation.
However, they also pose safety risks of generating harmful content. A key
challenge of existing concept erasure methods is the precise removal of target
concepts while minimizing degradation of image quality. In this paper, we
propose Single Neuron-based Concept Erasure (SNCE), a novel approach that can
precisely prevent harmful content generation by manipulating only a single
neuron. Specifically, we train a Sparse Autoencoder (SAE) to map text
embeddings into a sparse, disentangled latent space, where individual neurons
align tightly with atomic semantic concepts. To accurately locate neurons
responsible for harmful concepts, we design a novel neuron identification
method based on the modulated frequency scoring of activation patterns. By
suppressing activations of the harmful concept-specific neuron, SNCE achieves
surgical precision in concept erasure with minimal disruption to image quality.
Experiments on various benchmarks demonstrate that SNCE achieves
state-of-the-art results in target concept erasure, while preserving the
model's generation capabilities for non-target concepts. Additionally, our
method exhibits strong robustness against adversarial attacks, significantly
outperforming existing methods.

</details>


### [55] [OmniPlantSeg: Species Agnostic 3D Point Cloud Organ Segmentation for High-Resolution Plant Phenotyping Across Modalities](https://arxiv.org/abs/2509.21038)
*Andreas Gilson,Lukas Meyer,Oliver Scholz,Ute Schmid*

Main category: cs.CV

TL;DR: KDSS is a simple, sensor-agnostic subsampling method that preserves resolution for plant point clouds, enabling full-resolution segmentation across species and sensor modalities, and improving efficiency by reducing heavy pre-processing.


<details>
  <summary>Details</summary>
Motivation: Current plant point cloud segmentation methods are often problem-specific and rely on down-sampling or extensive pre-processing; there's a need for a universal, lightweight approach that works across species and sensors.

Method: Proposes KDSS, a straightforward subsampling algorithm for biological point clouds that is agnostic to sensor data and plant species; when combined with state-of-the-art segmentation models, it aims to enable segmentation without heavy pre-processing.

Result: Evaluated on multiple modalities (photogrammetry, laser triangulation, LiDAR) and various plant species, KDSS yields satisfying results when paired with SOTA segmentation models.

Conclusion: KDSS is a lightweight, resolution-retaining alternative to intensive pre-processing and down-sampling for plant organ segmentation, applicable across species and sensor modalities.

Abstract: Accurate point cloud segmentation for plant organs is crucial for 3D plant
phenotyping. Existing solutions are designed problem-specific with a focus on
certain plant species or specified sensor-modalities for data acquisition.
Furthermore, it is common to use extensive pre-processing and down-sample the
plant point clouds to meet hardware or neural network input size requirements.
We propose a simple, yet effective algorithm KDSS for sub-sampling of
biological point clouds that is agnostic to sensor data and plant species. The
main benefit of this approach is that we do not need to down-sample our input
data and thus, enable segmentation of the full-resolution point cloud.
Combining KD-SS with current state-of-the-art segmentation models shows
satisfying results evaluated on different modalities such as photogrammetry,
laser triangulation and LiDAR for various plant species. We propose KD-SS as
lightweight resolution-retaining alternative to intensive pre-processing and
down-sampling methods for plant organ segmentation regardless of used species
and sensor modality.

</details>


### [56] [Background Prompt for Few-Shot Out-of-Distribution Detection](https://arxiv.org/abs/2509.21055)
*Songyue Cai,Zongqian Wu,Yujie Mo,Liang Peng,Ping Hu,Xiaoshuang Shi,Xiaofeng Zhu*

Main category: cs.CV

TL;DR: Mambo introduces FG-BG decomposition for FS-OOD detection using a background prompt to capture local background and image semantics, refines with local class similarity, and uses patch self-calibrated tuning to adapt the number of background patches per sample, achieving state-of-the-art results on OOD and near-OOD detection.


<details>
  <summary>Details</summary>
Motivation: Robust FS-OOD detection is challenged by overreliance on local class similarity and fixed background extraction strategies; there is a need to separately model background semantics and adapt background patch selection to data diversity.

Method: Learn a background prompt to obtain local background similarity that includes background and image semantics, then refine it with local class similarity. Use both refined local background similarity and local class similarity for background extraction. Introduce patch self-calibrated tuning to adaptively select the number of background patches per sample, addressing fixed background extraction.

Result: Extensive experiments on real-world datasets show Mambo achieves state-of-the-art performance compared to SOTA methods in both OOD detection and near OOD detection settings.

Conclusion: Mambo provides a robust FG-BG decomposition framework for FS-OOD detection, improves robustness by decoupling background and class similarities, adapts to sample diversity with patch self-calibrated tuning, and achieves superior results with plans to release source code.

Abstract: Existing foreground-background (FG-BG) decomposition methods for the few-shot
out-of-distribution (FS-OOD) detection often suffer from low robustness due to
over-reliance on the local class similarity and a fixed background patch
extraction strategy. To address these challenges, we propose a new FG-BG
decomposition framework, namely Mambo, for FS-OOD detection. Specifically, we
propose to first learn a background prompt to obtain the local background
similarity containing both the background and image semantic information, and
then refine the local background similarity using the local class similarity.
As a result, we use both the refined local background similarity and the local
class similarity to conduct background extraction, reducing the dependence of
the local class similarity in previous methods. Furthermore, we propose the
patch self-calibrated tuning to consider the sample diversity to flexibly
select numbers of background patches for different samples, and thus exploring
the issue of fixed background extraction strategies in previous methods.
Extensive experiments on real-world datasets demonstrate that our proposed
Mambo achieves the best performance, compared to SOTA methods in terms of OOD
detection and near OOD detection setting. The source code will be released at
https://github.com/YuzunoKawori/Mambo.

</details>


### [57] [Stratify or Die: Rethinking Data Splits in Image Segmentation](https://arxiv.org/abs/2509.21056)
*Naga Venkata Sai Jitin Jami,Thomas Altstidl,Jonas Mueller,Jindong Li,Dario Zanca,Bjoern Eskofier,Heike Leutheuser*

Main category: cs.CV

TL;DR: WDES (with IPS) offers more representative, label-aware splits for segmentation by minimizing label distribution heterogeneity with a Wasserstein distance via an evolutionary algorithm, reducing evaluation variance, especially on small/imbalanced datasets.


<details>
  <summary>Details</summary>
Motivation: Standard random splits and naive stratification lead to biased evaluations in image segmentation due to multi-label, class-imbalanced data. There is a need for label-aware, distribution-consistent splits that preserve label diversity across splits.

Method: IPS: Iterative Pixel Stratification — a label-aware sampling approach for segmentation that iteratively partitions pixels to balance label distributions across splits. WDES: Wasserstein-Driven Evolutionary Stratification — a genetic algorithm that evolves dataset splits to minimize the Wasserstein distance between per-label distributions across splits, claimed to be globally optimal with enough generations. Also introduce statistical heterogeneity metrics to quantify split representativeness.

Result: WDES consistently yields more representative splits than random sampling across tasks (street scenes, medical imaging, satellite imagery). It reduces performance variance and improves model evaluation, with particular advantages for small, imbalanced, and low-diversity datasets where traditional splits bias results.

Conclusion: WDES provides a robust framework for segmentation dataset splitting, improving evaluation reliability across domains. IPS offers a straightforward baseline, but WDES’s optimization of label distribution across splits helps mitigate bias in challenging datasets. Potential considerations include computational cost and dependency on well-defined label distributions for Wasserstein computation.

Abstract: Random splitting of datasets in image segmentation often leads to
unrepresentative test sets, resulting in biased evaluations and poor model
generalization. While stratified sampling has proven effective for addressing
label distribution imbalance in classification tasks, extending these ideas to
segmentation remains challenging due to the multi-label structure and class
imbalance typically present in such data. Building on existing stratification
concepts, we introduce Iterative Pixel Stratification (IPS), a straightforward,
label-aware sampling method tailored for segmentation tasks. Additionally, we
present Wasserstein-Driven Evolutionary Stratification (WDES), a novel genetic
algorithm designed to minimize the Wasserstein distance, thereby optimizing the
similarity of label distributions across dataset splits. We prove that WDES is
globally optimal given enough generations. Using newly proposed statistical
heterogeneity metrics, we evaluate both methods against random sampling and
find that WDES consistently produces more representative splits. Applying WDES
across diverse segmentation tasks, including street scenes, medical imaging,
and satellite imagery, leads to lower performance variance and improved model
evaluation. Our results also highlight the particular value of WDES in handling
small, imbalanced, and low-diversity datasets, where conventional splitting
strategies are most prone to bias.

</details>


### [58] [EnGraf-Net: Multiple Granularity Branch Network with Fine-Coarse Graft Grained for Classification Task](https://arxiv.org/abs/2509.21061)
*Riccardo La Grassa,Ignazio Gallo,Nicola Landro*

Main category: cs.CV

TL;DR: EnGraf-Net uses semantic hierarchies (taxonomy) as supervised signals to improve fine-grained classification without cropping or manual annotations, achieving competitive results on CIFAR-100, CUB-200-2011, and FGVC-Aircraft.


<details>
  <summary>Details</summary>
Motivation: Part-based and attention-based methods may miss local features; humans use semantic associations. Leveraging a taxonomy provides structured semantic supervision to address high intra-class variance and low inter-class variance in fine-grained tasks.

Method: An end-to-end deep neural network named EnGraf-Net that integrates semantic taxonomy as supervision signals during training, enabling hierarchical guidance without requiring part crops or manual annotations.

Result: Demonstrates superior performance over many existing fine-grained models and competitive results with the latest state-of-the-art approaches across CIFAR-100, CUB-200-2011, and FGVC-Aircraft datasets.

Conclusion: Incorporating semantic hierarchical information into supervision improves fine-grained recognition and reduces reliance on cropping/part annotations, offering a robust alternative to traditional part-based approaches.

Abstract: Fine-grained classification models are designed to focus on the relevant
details necessary to distinguish highly similar classes, particularly when
intra-class variance is high and inter-class variance is low. Most existing
models rely on part annotations such as bounding boxes, part locations, or
textual attributes to enhance classification performance, while others employ
sophisticated techniques to automatically extract attention maps. We posit that
part-based approaches, including automatic cropping methods, suffer from an
incomplete representation of local features, which are fundamental for
distinguishing similar objects. While fine-grained classification aims to
recognize the leaves of a hierarchical structure, humans recognize objects by
also forming semantic associations. In this paper, we leverage semantic
associations structured as a hierarchy (taxonomy) as supervised signals within
an end-to-end deep neural network model, termed EnGraf-Net. Extensive
experiments on three well-known datasets CIFAR-100, CUB-200-2011, and
FGVC-Aircraft demonstrate the superiority of EnGraf-Net over many existing
fine-grained models, showing competitive performance with the most recent
state-of-the-art approaches, without requiring cropping techniques or manual
annotations.

</details>


### [59] [Vision Transformers: the threat of realistic adversarial patches](https://arxiv.org/abs/2509.21084)
*Kasper Cools,Clara Maathuis,Alexander M. van Oers,Claudia S. Hübner,Nikos Deligiannis,Marijke Vandewal,Geert De Cubber*

Main category: cs.CV

TL;DR: CNN-based adversarial patches can transfer to Vision Transformers (ViTs) via Creases Transformation, causing misclassification in a binary person-vs-non-person task; vulnerability varies across ViT models and pre-training regimes, indicating cross-architectural transferability and the influence of data/methodology on resilience.


<details>
  <summary>Details</summary>
Motivation: Rising reliance on ML systems raises security concerns; although ViTs are touted for robustness, they remain susceptible to adversarial patches. Understanding transferability across architectures informs defense design.

Method: Design realistic adversarial patches using Creases Transformation to mimic natural clothing distortions; assess the transferability of CNN-derived attack techniques to four fine-tuned ViT models on a binary classification task; report attack success rates and analyze how pre-training data and methodology affect resilience.

Result: Attack success rates ranged from 40.04% to 99.97% across models; specific figures include google/vit-base-patch16-224-in21k: 40.04%; facebook/dino-vitb16: 99.97%; google/vit-base-patch16-224: 66.40%; facebook/dinov3-vitb16: 65.17%. The findings confirm cross-architectural transferability of adversarial patches from CNNs to ViTs, with pre-training scale/methodology strongly shaping model resilience.

Conclusion: Adversarial patches crafted for CNNs can effectively fool ViTs, though vulnerability varies by model and training regime; cross-architectural transferability highlights the need for defenses that generalize across architectures and training methods.

Abstract: The increasing reliance on machine learning systems has made their security a
critical concern. Evasion attacks enable adversaries to manipulate the
decision-making processes of AI systems, potentially causing security breaches
or misclassification of targets. Vision Transformers (ViTs) have gained
significant traction in modern machine learning due to increased 1) performance
compared to Convolutional Neural Networks (CNNs) and 2) robustness against
adversarial perturbations. However, ViTs remain vulnerable to evasion attacks,
particularly to adversarial patches, unique patterns designed to manipulate AI
classification systems. These vulnerabilities are investigated by designing
realistic adversarial patches to cause misclassification in person vs.
non-person classification tasks using the Creases Transformation (CT)
technique, which adds subtle geometric distortions similar to those occurring
naturally when wearing clothing. This study investigates the transferability of
adversarial attack techniques used in CNNs when applied to ViT classification
models. Experimental evaluation across four fine-tuned ViT models on a binary
person classification task reveals significant vulnerability variations: attack
success rates ranged from 40.04% (google/vit-base-patch16-224-in21k) to 99.97%
(facebook/dino-vitb16), with google/vit-base-patch16-224 achieving 66.40% and
facebook/dinov3-vitb16 reaching 65.17%. These results confirm the
cross-architectural transferability of adversarial patches from CNNs to ViTs,
with pre-training dataset scale and methodology strongly influencing model
resilience to adversarial attacks.

</details>


### [60] [UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep Decomposition](https://arxiv.org/abs/2509.21086)
*Guojun Lei,Rong Zhang,Chi Wang,Tianhang Liu,Hong Li,Zhiyuan Ma,Weiwei Xu*

Main category: cs.CV

TL;DR: UniTransfer introduces spatial and diffusion timestep decomposition for controllable video concept transfer, using a dual-to-single-stream DiT architecture for component-level control, self-supervised masking pretraining, and a Chain-of-Prompt (CoP) guided diffusion process with LLMs. It also uncovers OpenAnimal dataset for benchmarking. Empirically, it achieves high-quality, editable video transfers across diverse references and scenes, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: There is a demand for precise, fine-grained control in video concept transfer (foreground, background, and motion) that existing methods struggle to provide; diffusion-based approaches require efficient learning from unlabeled data, and progressive generation benefits from guided timesteps. The paper combines decomposition, self-supervision, and language-guided prompting to improve controllability and fidelity.

Method: 1) Decompose videos into three components: foreground subject, background, and motion flow. 2) Propose a dual-to-single-stream DiT-based architecture to enable fine-grained control over components. 3) Employ self-supervised pretraining via random masking on large-scale unlabeled video data to learn decomposed representations. 4) Introduce Chain-of-Prompt (CoP): decompose the denoising process into three granularity stages and use LLMs to provide stage-specific instructions to guide generation progressively. 5) Curate OpenAnimal, an animal-centric video dataset, for benchmarking and advancement of video concept transfer experiments. 6) Extensive experiments compare against baselines on fidelity and editability across diverse references and scenes.

Result: The method achieves high-quality and controllable video concept transfer across varied reference images and scenes, outperforming existing baselines in both visual fidelity and editability.

Conclusion: UniTransfer demonstrates that combining spatial + diffusion timestep decomposition with a DiT-based architecture, self-supervised pretraining, and CoP-guided generation yields precise, controllable video concept transfer. The OpenAnimal dataset supports benchmarking and future research; further work could extend component-level prompts and scaling of CoP guidance.

Abstract: We propose a novel architecture UniTransfer, which introduces both spatial
and diffusion timestep decomposition in a progressive paradigm, achieving
precise and controllable video concept transfer. Specifically, in terms of
spatial decomposition, we decouple videos into three key components: the
foreground subject, the background, and the motion flow. Building upon this
decomposed formulation, we further introduce a dual-to-single-stream DiT-based
architecture for supporting fine-grained control over different components in
the videos. We also introduce a self-supervised pretraining strategy based on
random masking to enhance the decomposed representation learning from
large-scale unlabeled video data. Inspired by the Chain-of-Thought reasoning
paradigm, we further revisit the denoising diffusion process and propose a
Chain-of-Prompt (CoP) mechanism to achieve the timestep decomposition. We
decompose the denoising process into three stages of different granularity and
leverage large language models (LLMs) for stage-specific instructions to guide
the generation progressively. We also curate an animal-centric video dataset
called OpenAnimal to facilitate the advancement and benchmarking of research in
video concept transfer. Extensive experiments demonstrate that our method
achieves high-quality and controllable video concept transfer across diverse
reference images and scenes, surpassing existing baselines in both visual
fidelity and editability. Web Page:
https://yu-shaonian.github.io/UniTransfer-Web/

</details>


### [61] [VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception](https://arxiv.org/abs/2509.21100)
*Ziang Yan,Xinhao Li,Yinan He,Zhengrong Yue,Xiangyu Zeng,Yali Wang,Yu Qiao,Limin Wang,Yi Wang*

Main category: cs.CV

TL;DR: VTTS introduces iterative perception at test-time for multimodal LLMs, using reinforcement learning and spatio-temporal supervision to progressively refine attention to high-confidence regions, boosting reasoning performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLMs rely on static perception stages and decoupled reasoning. There is a need for dynamic, hierarchical attention and iterative perception during inference to better align perception with reasoning.

Method: Propose Visual Test-Time Scaling (VTTS) with an Iterative Perception (ITP) mechanism that updates textual predictions to guide progressive attention to spatio-temporal regions. Uses reinforcement learning with spatio-temporal supervision and trains on VTTS-80K dataset tailored for iterative perception. At inference, the model increases perceptual compute by iteratively refining focus.

Result: VTTS yields meaningful performance gains; the Videochat-R1.5 model demonstrates about 5% average improvement over baselines such as Qwen2.5VL-3B and -7B across 15+ benchmarks covering video chat, video reasoning, and spatio-temporal perception, indicating strong generalization.

Conclusion: Incorporating iterative perception and test-time scaling effectively enhances reasoning in multimodal LLMs by expanding perceptual computation, supported by strong empirical results and a dedicated dataset for iterative perception.

Abstract: Inducing reasoning in multimodal large language models (MLLMs) is critical
for achieving human-level perception and understanding. Existing methods mainly
leverage LLM reasoning to analyze parsed visuals, often limited by static
perception stages. This paper introduces Visual Test-Time Scaling (VTTS), a
novel approach to enhance MLLMs' reasoning via iterative perception during
inference. VTTS mimics humans' hierarchical attention by progressively refining
focus on high-confidence spatio-temporal regions, guided by updated textual
predictions. Specifically, VTTS employs an Iterative Perception (ITP)
mechanism, incorporating reinforcement learning with spatio-temporal
supervision to optimize reasoning. To support this paradigm, we also present
VTTS-80K, a dataset tailored for iterative perception. These designs allows a
MLLM to enhance its performance by increasing its perceptual compute. Extensive
experiments validate VTTS's effectiveness and generalization across diverse
tasks and benchmarks. Our newly introduced Videochat-R1.5 model has achieved
remarkable improvements, with an average increase of over 5\%, compared to
robust baselines such as Qwen2.5VL-3B and -7B, across more than 15 benchmarks
that encompass video conversation, video reasoning, and spatio-temporal
perception.

</details>


### [62] [Mammo-CLIP Dissect: A Framework for Analysing Mammography Concepts in Vision-Language Models](https://arxiv.org/abs/2509.21102)
*Suaiba Amina Salahuddin,Teresa Dorszewski,Marit Almenning Martiniussen,Tone Hovda,Antonio Portaluri,Solveig Thrun,Michael Kampffmeyer,Elisabeth Wetzer,Kristoffer Wickstrøm,Robert Jenssen*

Main category: cs.CV

TL;DR: A concept-based explainability framework (Mammo-CLIP Dissect) for mammography CNNs that uses a vision-language dissector to label neurons with textual concepts, assesses alignment with domain knowledge, and compares training data effects and fine-tuning; reveals specialization–generalization trade-offs; code available.


<details>
  <summary>Details</summary>
Motivation: To understand the textual concepts learned by deep learning models in mammography, aligning model reasoning with clinical workflows for safer deployment.

Method: Use Mammo-CLIP as a dissector to annotate neurons at selected layers with human-interpretable textual concepts and quantify their alignment to domain knowledge; compare models trained on general image data versus mammography data; examine the impact of fine-tuning for downstream tasks; identify underrepresented concepts.

Result: Models trained on mammography data capture more clinically relevant concepts and align with radiologists' workflows; task-specific fine-tuning enhances certain concepts (e.g., benign calcifications) but can reduce coverage of others (e.g., density-related features), showing a trade-off between specialization and generalization.

Conclusion: Mammo-CLIP Dissect provides insight into how CNNs acquire mammography-specific knowledge and how domain-specific training and task adaptation shape concept learning; the study offers a framework for concept-level analysis and reports code and concept sets on GitHub.

Abstract: Understanding what deep learning (DL) models learn is essential for the safe
deployment of artificial intelligence (AI) in clinical settings. While previous
work has focused on pixel-based explainability methods, less attention has been
paid to the textual concepts learned by these models, which may better reflect
the reasoning used by clinicians. We introduce Mammo-CLIP Dissect, the first
concept-based explainability framework for systematically dissecting DL vision
models trained for mammography. Leveraging a mammography-specific
vision-language model (Mammo-CLIP) as a "dissector," our approach labels
neurons at specified layers with human-interpretable textual concepts and
quantifies their alignment to domain knowledge. Using Mammo-CLIP Dissect, we
investigate three key questions: (1) how concept learning differs between DL
vision models trained on general image datasets versus mammography-specific
datasets; (2) how fine-tuning for downstream mammography tasks affects concept
specialisation; and (3) which mammography-relevant concepts remain
underrepresented. We show that models trained on mammography data capture more
clinically relevant concepts and align more closely with radiologists'
workflows than models not trained on mammography data. Fine-tuning for
task-specific classification enhances the capture of certain concept categories
(e.g., benign calcifications) but can reduce coverage of others (e.g.,
density-related features), indicating a trade-off between specialisation and
generalisation. Our findings show that Mammo-CLIP Dissect provides insights
into how convolutional neural networks (CNNs) capture mammography-specific
knowledge. By comparing models across training data and fine-tuning regimes, we
reveal how domain-specific training and task-specific adaptation shape concept
learning. Code and concept set are available:
https://github.com/Suaiba/Mammo-CLIP-Dissect.

</details>


### [63] [MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for Video Temporal Reasoning](https://arxiv.org/abs/2509.21113)
*Sicheng Tao,Jungang Li,Yibo Yan,Junyan Zhang,Yubo Gao,Hanqian Li,ShuHang Xun,Yuxuan Fan,Hong Chen,Jianxiang He,Xuming Hu*

Main category: cs.CV

TL;DR: Introduces MOSS-ChatV, a DTW-based, rule-based RL framework for video reasoning in MLLMs, with the MOSS-Video benchmark (train/val split) for training and evaluation. It achieves strong results (87.2% on MOSS-Video test) and shows gains across architectures and benchmarks, with GPT-4o assessments favoring its more consistent reasoning traces.


<details>
  <summary>Details</summary>
Motivation: Video reasoning requires temporal coherence; existing MLLMs suffer process inconsistency where intermediate reasoning drifts from video dynamics despite correct final answers, hurting interpretability and robustness.

Method: Framework uses a DTW-based process reward to supervise the alignment between intermediate reasoning traces and temporally grounded references; treats dynamic state prediction as a core measure; constructs MOSS-Video with annotated traces; training split fine-tunes MOSS-ChatV while a held-out split evaluates performance across multiple architectures (e.g., Qwen2.5-VL, Phi-2).

Result: MOSS-ChatV achieves 87.2% on MOSS-Video test; shows improved performance on MVBench and MMVU; gains are consistent across different architectures; GPT-4o-as-judge evaluations indicate more consistent and stable reasoning traces.

Conclusion: The DTW-based process reward enables efficient, model-agnostic supervision of video reasoning, improving interpretability and robustness; MOSS-Video provides a valuable benchmark for training and evaluating temporally grounded reasoning, with broad applicability across MLLM architectures.

Abstract: Video reasoning has emerged as a critical capability for multimodal large
language models (MLLMs), requiring models to move beyond static perception
toward coherent understanding of temporal dynamics in complex scenes. Yet
existing MLLMs often exhibit process inconsistency, where intermediate
reasoning drifts from video dynamics even when the final answer is correct,
undermining interpretability and robustness. To address this issue, we
introduce MOSS-ChatV, a reinforcement learning framework with a Dynamic Time
Warping (DTW)-based process reward. This rule-based reward aligns reasoning
traces with temporally grounded references, enabling efficient process
supervision without auxiliary reward models. We further identify dynamic state
prediction as a key measure of video reasoning and construct MOSS-Video, a
benchmark with annotated reasoning traces, where the training split is used to
fine-tune MOSS-ChatV and the held-out split is reserved for evaluation.
MOSS-ChatV achieves 87.2\% on MOSS-Video (test) and improves performance on
general video benchmarks such as MVBench and MMVU. The framework consistently
yields gains across different architectures, including Qwen2.5-VL and Phi-2,
confirming its broad applicability. Evaluations with GPT-4o-as-judge further
show that MOSS-ChatV produces more consistent and stable reasoning traces.

</details>


### [64] [MotionFlow:Learning Implicit Motion Flow for Complex Camera Trajectory Control in Video Generation](https://arxiv.org/abs/2509.21119)
*Guojun Lei,Chi Wang,Yikai Wang,Hong Li,Ying Song,Weiwei Xu*

Main category: cs.CV

TL;DR: A unified approach to video generation from a specified camera trajectory by learning pixel-level motion maps that couple camera and object motions, using stable diffusion to model reference motions and an image-to-video network for generation, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing video generation methods struggle with consistency and generalization when both camera and object motions are present, often learning motions separately which confuses the relative motion and degrades results.

Method: Convert camera and object motions into pixel-level motion maps; use a stable diffusion network to learn reference motion maps aligned with the given camera trajectory; extract a semantic object prior; feed these into an image-to-video network to generate videos that follow the camera trajectory with consistent object motions.

Result: Extensive experiments show the proposed method outperforms state-of-the-art methods by a large margin.

Conclusion: Integrating camera and object motions through pixel-level motion maps and diffusion-based reference learning yields more consistent, trajectory-following video generation and better generalization than existing approaches.

Abstract: Generating videos guided by camera trajectories poses significant challenges
in achieving consistency and generalizability, particularly when both camera
and object motions are present. Existing approaches often attempt to learn
these motions separately, which may lead to confusion regarding the relative
motion between the camera and the objects. To address this challenge, we
propose a novel approach that integrates both camera and object motions by
converting them into the motion of corresponding pixels. Utilizing a stable
diffusion network, we effectively learn reference motion maps in relation to
the specified camera trajectory. These maps, along with an extracted semantic
object prior, are then fed into an image-to-video network to generate the
desired video that can accurately follow the designated camera trajectory while
maintaining consistent object motions. Extensive experiments verify that our
model outperforms SOTA methods by a large margin.

</details>


### [65] [The Unwinnable Arms Race of AI Image Detection](https://arxiv.org/abs/2509.21135)
*Till Aczel,Lorenzo Vettor,Andreas Plesner,Roger Wattenhofer*

Main category: cs.CV

TL;DR: Non-monotonic detectability of synthetic images wrt data complexity: very simple and very complex datasets hinder detection, while intermediate complexity maximizes discriminators' advantage.


<details>
  <summary>Details</summary>
Motivation: Identify when discriminators are most disadvantaged in the GAN-like arms race by isolating the roles of data dimensionality and intrinsic complexity, using Kolmogorov complexity as a proxy for structure.

Method: Theoretical analysis using Kolmogorov complexity to quantify intrinsic dataset structure across dimensionality; derive regimes where generators closely approximate the true distribution and where their artifacts remain detectable.

Result: Dimensionality and complexity interact nontrivially: higher dimensionality generally strengthens discriminators, but the effect of data complexity is nonmonotonic. Simple datasets allow near-perfect generation with few detectable artifacts; extremely complex datasets mask imperfections; intermediate-complexity datasets render generators unable to fully capture the distribution, making their errors more visible and discriminators more effective.

Conclusion: Dataset complexity governs discriminability in synthetic-image detection in a non-monotonic way. Intermediate complexity offers the most favorable conditions for detection, while both ends of the complexity spectrum reduce detectability, shaping how evaluators should test and compare generative models.

Abstract: The rapid progress of image generative AI has blurred the boundary between
synthetic and real images, fueling an arms race between generators and
discriminators. This paper investigates the conditions under which
discriminators are most disadvantaged in this competition. We analyze two key
factors: data dimensionality and data complexity. While increased
dimensionality often strengthens the discriminators ability to detect subtle
inconsistencies, complexity introduces a more nuanced effect. Using Kolmogorov
complexity as a measure of intrinsic dataset structure, we show that both very
simple and highly complex datasets reduce the detectability of synthetic
images; generators can learn simple datasets almost perfectly, whereas extreme
diversity masks imperfections. In contrast, intermediate-complexity datasets
create the most favorable conditions for detection, as generators fail to fully
capture the distribution and their errors remain visible.

</details>


### [66] [WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP](https://arxiv.org/abs/2509.21153)
*Moshe Kimhi,Erez Koifman,Ehud Rivlin,Eli Schwartz,Chaim Baskin*

Main category: cs.CV

TL;DR: WAVECLIP uses wavelet-based tokenization to enable adaptive-resolution inference in CLIP, with early exits and caching for compute savings while retaining competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: To make CLIP inference more compute-efficient by supporting multiple resolutions within a single model, enabling dynamic compute-accuracy trade-offs without ensembles.

Method: Replace standard patch embeddings with a multi-level wavelet decomposition to create coarse-to-fine image tokens; process from low to high resolution, using key-value caching and causal cross-level attention to reuse computation; employ a simple confidence-based gating for adaptive early exits; perform lightweight distillation from a frozen CLIP teacher.

Result: Zero-shot evaluation shows competitive accuracy with substantial computational savings; the model supports adaptive early exits via a simple gating mechanism, enabling dynamic compute-accuracy trade-offs in deployment.

Conclusion: A single unified model with wavelet-based tokenization enables adaptive inference in CLIP, achieving compute savings with minimal accuracy loss through distillation and simple gating.

Abstract: We introduce WAVECLIP, a single unified model for adaptive resolution
inference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces
standard patch embeddings with a multi-level wavelet decomposition, enabling
the model to process images coarse to fine while naturally supporting multiple
resolutions within the same model. At inference time, the model begins with low
resolution tokens and refines only when needed, using key-value caching and
causal cross-level attention to reuse computation, effectively introducing to
the model only new information when needed. We evaluate WAVECLIP in zero-shot
classification, demonstrating that a simple confidence-based gating mechanism
enables adaptive early exits. This allows users to dynamically choose a
compute-accuracy trade-off using a single deployed model. Our approach requires
only lightweight distillation from a frozen CLIP teacher and achieves
competitive accuracy with significant computational savings.

</details>


### [67] [Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy](https://arxiv.org/abs/2509.21173)
*Aymen Bouguerra,Daniel Montoya,Alexandra Gomez-Villa,Fabio Arnez,Chokri Mraidha*

Main category: cs.CV

TL;DR: Quantization can improve calibration for underconfident CLIP models and, with quantization-aware training (QAT), simultaneously boost zero-shot accuracy, calibration, and OOD robustness, challenging simple efficiency-vs-performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: Despite CLIP's strong zero-shot generalization and its use in safety-related tasks, the effects of quantization on reliability metrics (calibration, OOD detection) across distributions have not been thoroughly explored at scale.

Method: Large-scale evaluation of quantization on CLIP models across in-distribution and out-of-distribution settings, measuring accuracy, calibration metrics, and OOD performance; analyzing the impact of pre-training source; evaluating quantization-aware training (QAT) methods.

Result: Quantization consistently improves calibration for typically underconfident pre-trained models while often degrading calibration for overconfident variants; however, OOD detection can still improve for these poorly calibrated models; certain QAT methods yield simultaneous gains in zero-shot accuracy, calibration, and OOD robustness.

Conclusion: Quantization offers multi-objective benefits beyond efficiency. With appropriate QAT and consideration of the pre-training source, it can enhance accuracy and reliability (calibration and OOD robustness) without necessarily trading off efficiency, informing deployment of robust, efficient VLMs.

Abstract: The powerful zero-shot generalization capabilities of vision-language models
(VLMs) like CLIP have enabled new paradigms for safety-related tasks such as
out-of-distribution (OOD) detection. However, additional aspects crucial for
the computationally efficient and reliable deployment of CLIP are still
overlooked. In particular, the impact of quantization on CLIP's performance
beyond accuracy remains underexplored. This work presents a large-scale
evaluation of quantization on CLIP models, assessing not only in-distribution
accuracy but a comprehensive suite of reliability metrics and revealing
counterintuitive results driven by pre-training source. We demonstrate that
quantization consistently improves calibration for typically underconfident
pre-trained models, while often degrading it for overconfident variants.
Intriguingly, this degradation in calibration does not preclude gains in other
reliability metrics; we find that OOD detection can still improve for these
same poorly calibrated models. Furthermore, we identify specific
quantization-aware training (QAT) methods that yield simultaneous gains in
zero-shot accuracy, calibration, and OOD robustness, challenging the view of a
strict efficiency-performance trade-off. These findings offer critical insights
for navigating the multi-objective problem of deploying efficient, reliable,
and robust VLMs by utilizing quantization beyond its conventional role.

</details>


### [68] [TABLET: A Large-Scale Dataset for Robust Visual Table Understanding](https://arxiv.org/abs/2509.21205)
*Iñigo Alonso,Imanol Miranda,Eneko Agirre,Mirella Lapata*

Main category: cs.CV

TL;DR: TABLET is a large-scale, real-world VTU dataset (4M examples from 2M tables across 20 tasks) preserving original visualizations (88%), providing image-HTML pairs, metadata, and provenance; fine-tuning Qwen2.5-VL-7B on TABLET boosts VTU performance on seen/unseen tasks and improves robustness, enabling robust training and extensible evaluation.


<details>
  <summary>Details</summary>
Motivation: Current VTU benchmarks rely on synthetic visuals and fixed examples with no serialized reformulation data, limiting realism, diversity, and traceability; a scalable, traceable, real-world VTU dataset is needed to train robust models.

Method: Construct TABLET: 4 million examples across 20 tasks grounded in 2 million unique tables, with 88% preserving original visualizations; provide paired image and HTML representations plus comprehensive metadata and provenance; evaluate by fine-tuning a state-of-the-art vision-language model (Qwen2.5-VL-7B) on TABLET and testing on seen/unseen VTU tasks.

Result: Fine-tuning Qwen2.5-VL-7B on TABLET yields improved performance on both seen and unseen VTU tasks and enhances robustness to real-world table visualizations.

Conclusion: TABLET offers a unified, large-scale, traceable VTU resource that supports robust training and extensible evaluation for future VTU models, aligning visualization realism with model capabilities.

Abstract: While table understanding increasingly relies on pixel-only settings where
tables are processed as visual representations, current benchmarks
predominantly use synthetic renderings that lack the complexity and visual
diversity of real-world tables. Additionally, existing visual table
understanding (VTU) datasets offer fixed examples with single visualizations
and pre-defined instructions, providing no access to underlying serialized data
for reformulation. We introduce TABLET, a large-scale VTU dataset with 4
million examples across 20 tasks, grounded in 2 million unique tables where 88%
preserve original visualizations. Each example includes paired image-HTML
representations, comprehensive metadata, and provenance information linking
back to the source datasets. Fine-tuning vision-language models like
Qwen2.5-VL-7B on TABLET improves performance on seen and unseen VTU tasks while
increasing robustness on real-world table visualizations. By preserving
original visualizations and maintaining example traceability in a unified
large-scale collection, TABLET establishes a foundation for robust training and
extensible evaluation of future VTU models.

</details>


### [69] [Learning Conformal Explainers for Image Classifiers](https://arxiv.org/abs/2509.21209)
*Amr Alkhatib,Stephanie Lowry*

Main category: cs.CV

TL;DR: FastSHAP: a conformal-prediction-based framework that yields faithful, compact image explanations by identifying minimal salient features to preserve model predictions; outperforms baselines on fidelity and efficiency, with super-pixel-based conformity being especially effective.


<details>
  <summary>Details</summary>
Motivation: Explaining image predictions with feature attributions often lacks robustness and faithful reflection of the model's reasoning; there's a need to control the trade-off between explanation fidelity and size, without requiring ground-truth explanations for calibration.

Method: Introduce a conformal prediction-based method (FastSHAP). It identifies a subset of salient features that is sufficient to keep the model's prediction unchanged, independent of the information in the excluded features; uses four conformity functions to measure how explanations align with the model's outputs; evaluates five explainers across six image datasets.

Result: FastSHAP consistently outperforms competing methods in fidelity and informational efficiency (size of explanation regions). Conformity measures based on super-pixels outperform pixel-wise measures.

Conclusion: Conformal prediction enables user-controllable fidelity in image explanations; the approach is effective across multiple explainers and datasets, with super-pixel-based conformity offering the best performance.

Abstract: Feature attribution methods are widely used for explaining image-based
predictions, as they provide feature-level insights that can be intuitively
visualized. However, such explanations often vary in their robustness and may
fail to faithfully reflect the reasoning of the underlying black-box model. To
address these limitations, we propose a novel conformal prediction-based
approach that enables users to directly control the fidelity of the generated
explanations. The method identifies a subset of salient features that is
sufficient to preserve the model's prediction, regardless of the information
carried by the excluded features, and without demanding access to ground-truth
explanations for calibration. Four conformity functions are proposed to
quantify the extent to which explanations conform to the model's predictions.
The approach is empirically evaluated using five explainers across six image
datasets. The empirical results demonstrate that FastSHAP consistently
outperforms the competing methods in terms of both fidelity and informational
efficiency, the latter measured by the size of the explanation regions.
Furthermore, the results reveal that conformity measures based on super-pixels
are more effective than their pixel-wise counterparts.

</details>


### [70] [Sigma: Semantically Informative Pre-training for Skeleton-based Sign Language Understanding](https://arxiv.org/abs/2509.21223)
*Muxin Pu,Mei Kuan Lim,Chun Yong Chong,Chen Change Loy*

Main category: cs.CV

TL;DR: A unified skeleton-based pre-training framework (Sigma) for sign language understanding that integrates sign-aware early fusion, hierarchical alignment, and multi-task pre-training to achieve state-of-the-art results across SLU tasks.


<details>
  <summary>Details</summary>
Motivation: Skeleton-based SLU struggles with semantic grounding, balancing local details with global context, and cross-modal learning efficiency. The paper proposes semantically informed pre-training to address these gaps.

Method: 1) sign-aware early fusion to blend visual and textual signals; 2) hierarchical alignment learning to align multi-level features across modalities; 3) unified pre-training combining contrastive learning, text matching, and language modeling.

Result: Achieves new state-of-the-art on isolated sign recognition, continuous sign recognition, and gloss-free translation across multiple benchmarks and languages, validating semantically informative pre-training and skeleton data as a standalone SLU solution.

Conclusion: Semantic grounding and cross-modal alignment benefit from unified skeleton-based pre-training; Sigma demonstrates strong generalization and effectiveness of skeletal data for SLU.

Abstract: Pre-training has proven effective for learning transferable features in sign
language understanding (SLU) tasks. Recently, skeleton-based methods have
gained increasing attention because they can robustly handle variations in
subjects and backgrounds without being affected by appearance or environmental
factors. Current SLU methods continue to face three key limitations: 1) weak
semantic grounding, as models often capture low-level motion patterns from
skeletal data but struggle to relate them to linguistic meaning; 2) imbalance
between local details and global context, with models either focusing too
narrowly on fine-grained cues or overlooking them for broader context; and 3)
inefficient cross-modal learning, as constructing semantically aligned
representations across modalities remains difficult. To address these, we
propose Sigma, a unified skeleton-based SLU framework featuring: 1) a
sign-aware early fusion mechanism that facilitates deep interaction between
visual and textual modalities, enriching visual features with linguistic
context; 2) a hierarchical alignment learning strategy that jointly maximises
agreements across different levels of paired features from different
modalities, effectively capturing both fine-grained details and high-level
semantic relationships; and 3) a unified pre-training framework that combines
contrastive learning, text matching and language modelling to promote semantic
consistency and generalisation. Sigma achieves new state-of-the-art results on
isolated sign language recognition, continuous sign language recognition, and
gloss-free sign language translation on multiple benchmarks spanning different
sign and spoken languages, demonstrating the impact of semantically informative
pre-training and the effectiveness of skeletal data as a stand-alone solution
for SLU.

</details>


### [71] [Evaluating the Evaluators: Metrics for Compositional Text-to-Image Generation](https://arxiv.org/abs/2509.21227)
*Seyed Amir Kasaei,Ali Aghayari,Arash Marioriyad,Niki Sepasian,MohammadAmin Fazli,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: No single metric reliably captures compositional text-image alignment; performance depends on the task. VQA-based metrics are not universally superior, embedding-based metrics help in some cases, and image-only metrics offer little for compositional evaluation. Emphasizes careful, transparent metric selection for trustworthy evaluation and reward modeling.


<details>
  <summary>Details</summary>
Motivation: Evaluation metrics in text-image generation are widely used but not consistently validated against human judgments; understanding how well they reflect human preferences is essential for trustworthy progress.

Method: Empirical broad study comparing major metric families (VQA-based, embedding-based, image-only) across diverse compositional challenges, analyzing their alignment with human judgments beyond simple correlation.

Result: No single metric is consistently best across tasks. VQA-based metrics are not uniformly superior; certain embedding-based metrics perform better in specific scenarios. Image-only metrics contribute little to compositional evaluation, as they target perceptual quality rather than alignment. Findings support using multiple, well-justified metrics and transparent reporting; project page available.

Conclusion: Careful and transparent metric selection is critical for trustworthy evaluation and for their use as reward models in generation; rely on a combination of metrics and validate them against human judgments.

Abstract: Text-image generation has advanced rapidly, but assessing whether outputs
truly capture the objects, attributes, and relations described in prompts
remains a central challenge. Evaluation in this space relies heavily on
automated metrics, yet these are often adopted by convention or popularity
rather than validated against human judgment. Because evaluation and reported
progress in the field depend directly on these metrics, it is critical to
understand how well they reflect human preferences. To address this, we present
a broad study of widely used metrics for compositional text-image evaluation.
Our analysis goes beyond simple correlation, examining their behavior across
diverse compositional challenges and comparing how different metric families
align with human judgments. The results show that no single metric performs
consistently across tasks: performance varies with the type of compositional
problem. Notably, VQA-based metrics, though popular, are not uniformly
superior, while certain embedding-based metrics prove stronger in specific
cases. Image-only metrics, as expected, contribute little to compositional
evaluation, as they are designed for perceptual quality rather than alignment.
These findings underscore the importance of careful and transparent metric
selection, both for trustworthy evaluation and for their use as reward models
in generation. Project page is available at
\href{https://amirkasaei.com/eval-the-evals/}{this URL}.

</details>


### [72] [SlideMamba: Entropy-Based Adaptive Fusion of GNN and Mamba for Enhanced Representation Learning in Digital Pathology](https://arxiv.org/abs/2509.21239)
*Shakib Khan,Fariba Dambandkhameneh,Nazim Shaikh,Yao Nie,Raghavan Venugopal,Xiao Li*

Main category: cs.CV

TL;DR: SlideMamba integrates Mamba (global dependencies) with Graph Neural Networks (local spatial interactions) for WSI analysis, using an entropy-based adaptive fusion. It achieves state-of-the-art PRAUC and competitive ROC AUC, surpassing MIL, Trans-MIL, and prior baselines on gene fusion/mutation prediction from WSIs.


<details>
  <summary>Details</summary>
Motivation: WSIs contain rich local and long-range contextual information; existing methods struggle to jointly model global and local dependencies. A framework that fuses global (Mamba) and local (GNN) signals with task-aware weighting could improve predictive performance in computational pathology.

Method: A dual-branch architecture where Mamba captures global dependencies and a Graph Neural Network captures local spatial interactions. An entropy-based confidence weighting mechanism adaptively fuses the two branch predictions, selecting the more confident (lower-entropy) signal per context. The framework, SlideMamba, is evaluated on predicting gene fusion and mutation status from WSIs against MIL, Trans-MIL, Mamba-only, GNN-only, and GAT-Mamba baselines.

Result: SlideMamba achieves PRAUC = 0.751 ± 0.05, outperforming MIL (0.491 ± 0.042), Trans-MIL (0.39 ± 0.017), Mamba-only (0.664 ± 0.063), GNN-only (0.748 ± 0.091), and GAT-Mamba (0.703 ± 0.075). ROC AUC = 0.738 ± 0.055, sensitivity = 0.662 ± 0.083, specificity = 0.725 ± 0.094, indicating superior and robust performance across metrics.

Conclusion: The entropy-guided adaptive fusion of global and local pathways (SlideMamba) improves spatially-resolved predictive modeling on WSIs and holds promise for diverse pathology tasks requiring both long-range context and fine-grained spatial interactions.

Abstract: Advances in computational pathology increasingly rely on extracting
meaningful representations from Whole Slide Images (WSIs) to support various
clinical and biological tasks. In this study, we propose a generalizable deep
learning framework that integrates the Mamba architecture with Graph Neural
Networks (GNNs) for enhanced WSI analysis. Our method is designed to capture
both local spatial relationships and long-range contextual dependencies,
offering a flexible architecture for digital pathology analysis. Mamba modules
excels in capturing long-range global dependencies, while GNNs emphasize
fine-grained short-range spatial interactions. To effectively combine these
complementary signals, we introduce an adaptive fusion strategy that uses an
entropy-based confidence weighting mechanism. This approach dynamically
balances contributions from both branches by assigning higher weight to the
branch with more confident (lower-entropy) predictions, depending on the
contextual importance of local versus global information for different
downstream tasks. We demonstrate the utility of our approach on a
representative task: predicting gene fusion and mutation status from WSIs. Our
framework, SlideMamba, achieves an area under the precision recall curve
(PRAUC) of 0.751 \pm 0.05, outperforming MIL (0.491 \pm 0.042), Trans-MIL (0.39
\pm 0.017), Mamba-only (0.664 \pm 0.063), GNN-only (0.748 \pm 0.091), and a
prior similar work GAT-Mamba (0.703 \pm 0.075). SlideMamba also achieves
competitive results across ROC AUC (0.738 \pm 0.055), sensitivity (0.662 \pm
0.083), and specificity (0.725 \pm 0.094). These results highlight the strength
of the integrated architecture, enhanced by the proposed entropy-based adaptive
fusion strategy, and suggest promising potential for application of
spatially-resolved predictive modeling tasks in computational pathology.

</details>


### [73] [Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D Assets](https://arxiv.org/abs/2509.21245)
*Team Hunyuan3D,:,Bowen Zhang,Chunchao Guo,Haolin Liu,Hongyu Yan,Huiwen Shi,Jingwei Huang,Junlin Yu,Kunhong Li,Linus,Penghao Wang,Qingxiang Lin,Sicong Liu,Xianghui Yang,Yixuan Tang,Yunfei Zhao,Zeqiang Lai,Zhihao Liang,Zibo Zhao*

Main category: cs.CV

TL;DR: A unified, multi-modal 3D asset generator (Hunyuan3D-Omni) enables fine-grained control across images, point clouds, voxels, bounding boxes, and skeletal pose within a single cross-modal architecture, using a progressive, difficulty-aware sampling strategy to handle missing inputs and emphasize harder signals.


<details>
  <summary>Details</summary>
Motivation: Current 3D generative models mainly rely on single modalities (image or text) and lack fine-grained cross-modal controls, limiting controllability and practical use in production workflows.

Method: Extend Hunyuan3D to accept multiple conditioning modalities (images, point clouds, voxels, bounding boxes, and skeletal pose priors) within a single cross-modal backbone. Use progressive, difficulty-aware sampling that, for each example, selects one control modality and biases toward harder signals (e.g., skeletal pose) while downweighting easier ones (e.g., point clouds), enabling robust multi-modal fusion and graceful handling of missing inputs.

Result: Experiments show that adding these controls improves generation accuracy, enables geometry-aware transformations, and increases robustness for production workflows.

Conclusion: Hunyuan3D-Omni delivers fine-grained, controllable 3D asset generation with cross-modal conditioning and robust fusion, improving controllability and practicality for real-world production.

Abstract: Recent advances in 3D-native generative models have accelerated asset
creation for games, film, and design. However, most methods still rely
primarily on image or text conditioning and lack fine-grained, cross-modal
controls, which limits controllability and practical adoption. To address this
gap, we present Hunyuan3D-Omni, a unified framework for fine-grained,
controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images,
Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose
priors as conditioning signals, enabling precise control over geometry,
topology, and pose. Instead of separate heads for each modality, our model
unifies all signals in a single cross-modal architecture. We train with a
progressive, difficulty-aware sampling strategy that selects one control
modality per example and biases sampling toward harder signals (e.g., skeletal
pose) while downweighting easier ones (e.g., point clouds), encouraging robust
multi-modal fusion and graceful handling of missing inputs. Experiments show
that these additional controls improve generation accuracy, enable
geometry-aware transformations, and increase robustness for production
workflows.

</details>


### [74] [Learning to Look: Cognitive Attention Alignment with Vision-Language Models](https://arxiv.org/abs/2509.21247)
*Ryan L. Yang,Dipkamal Bhusal,Nidhi Rastogi*

Main category: cs.CV

TL;DR: A scalable, annotation-free framework uses vision-language models to generate semantic attention maps from prompts and an auxiliary loss to align CNN attention to these maps, yielding more robust, human-aligned decisions; achieves state-of-the-art on ColoredMNIST and competitive results on DecoyMNIST.


<details>
  <summary>Details</summary>
Motivation: CNNs often rely on superficial correlations and shortcuts; relying on expert annotations for semantic attention is labor-intensive and not scalable; need a scalable approach to encourage attention on semantically meaningful regions.

Method: Generate language-guided semantic attention maps with vision-language models via natural language prompts. Use an auxiliary loss to align the CNN's attention with these maps during training, removing the need for manual annotations.

Result: Achieves state-of-the-art performance on ColoredMNIST and competitive results on DecoyMNIST; shows improved generalization and reduced shortcut reliance; attention aligns better with human intuition.

Conclusion: The proposed framework provides a scalable, cognitively plausible method to guide model attention without manual labels, improving robustness and generalization on challenging datasets.

Abstract: Convolutional Neural Networks (CNNs) frequently "cheat" by exploiting
superficial correlations, raising concerns about whether they make predictions
for the right reasons. Inspired by cognitive science, which highlights the role
of attention in robust human perception, recent methods have sought to guide
model attention using concept-based supervision and explanation regularization.
However, these techniques depend on labor-intensive, expert-provided
annotations, limiting their scalability. We propose a scalable framework that
leverages vision-language models to automatically generate semantic attention
maps using natural language prompts. By introducing an auxiliary loss that
aligns CNN attention with these language-guided maps, our approach promotes
more reliable and cognitively plausible decision-making without manual
annotation. Experiments on challenging datasets, ColoredMNIST and DecoyMNIST,
show that our method achieves state-of-the-art performance on ColorMNIST and
remains competitive with annotation-heavy baselines on DecoyMNIST,
demonstrating improved generalization, reduced shortcut reliance, and model
attention that better reflects human intuition.

</details>


### [75] [Decipher-MR: A Vision-Language Foundation Model for 3D MRI Representations](https://arxiv.org/abs/2509.21249)
*Zhijian Yang,Noel DSouza,Istvan Megyeri,Xiaojian Xu,Amin Honarmandi Shandiz,Farzin Haddadpour,Krisztian Koos,Laszlo Rusko,Emanuele Valeriano,Bharadwaj Swaninathan,Lei Wu,Parminder Bhatia,Taha Kass-Hout,Erhan Bas*

Main category: cs.CV

TL;DR: Decipher-MR is a large-scale 3D MRI vision-language foundation model trained on 200k MRI series from 22k studies, using self-supervised vision learning with report-guided text supervision. It employs a frozen encoder with lightweight, task-specific decoders and achieves consistent gains across diverse MRI tasks such as classification, demographic prediction, localization, and cross-modal retrieval.


<details>
  <summary>Details</summary>
Motivation: MRI analysis is highly heterogeneous and data-scarce, making scalable, generalizable AI challenging. Existing foundation models are limited by narrow anatomical focus and lack of MRI-scale data, hindering broad adaptation across clinical and research tasks.

Method: A 3D MRI-specific vision-language foundation model (Decipher-MR) trained on a large-scale, diverse MRI dataset (200k series from 22k studies across domains). It combines self-supervised vision learning with report-guided text supervision. The architecture uses a frozen pretrained encoder with modular, lightweight, task-specific decoders for efficient adaptation.

Result: Decipher-MR demonstrates consistent performance gains over existing foundation models and task-specific approaches across multiple benchmarks, including disease classification, demographic prediction, anatomical localization, and cross-modal retrieval.

Conclusion: Decipher-MR establishes a scalable and versatile MRI foundation for AI, enabling efficient development and deployment across clinical and research domains with reduced computational overhead.

Abstract: Magnetic Resonance Imaging (MRI) is a critical medical imaging modality in
clinical diagnosis and research, yet its complexity and heterogeneity pose
challenges for automated analysis, particularly in scalable and generalizable
machine learning applications. While foundation models have revolutionized
natural language and vision tasks, their application to MRI remains limited due
to data scarcity and narrow anatomical focus. In this work, we present
Decipher-MR, a 3D MRI-specific vision-language foundation model trained on a
large-scale dataset comprising 200,000 MRI series from over 22,000 studies
spanning diverse anatomical regions, sequences, and pathologies. Decipher-MR
integrates self-supervised vision learning with report-guided text supervision
to build robust, generalizable representations, enabling effective adaptation
across broad applications. To enable robust and diverse clinical tasks with
minimal computational overhead, Decipher-MR supports a modular design that
enables tuning of lightweight, task-specific decoders attached to a frozen
pretrained encoder. Following this setting, we evaluate Decipher-MR across
diverse benchmarks including disease classification, demographic prediction,
anatomical localization, and cross-modal retrieval, demonstrating consistent
performance gains over existing foundation models and task-specific approaches.
Our results establish Decipher-MR as a scalable and versatile foundation for
MRI-based AI, facilitating efficient development across clinical and research
domains.

</details>


### [76] [Instruction-tuned Self-Questioning Framework for Multimodal Reasoning](https://arxiv.org/abs/2509.21251)
*You-Won Jang,Yu-Jung Heo,Jaeseok Kim,Minsu Lee,Du-Seong Chang,Byoung-Tak Zhang*

Main category: cs.CV

TL;DR: Proposes SQ-InstructBLIP for vision-language reasoning by self-questioning with image-aware sub-questions/answers; a Questioner-Answerer-Reasoner trio guides VQA, improving reasoning accuracy over prior methods.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with multi-step reasoning in vision-language tasks and cannot read visual content directly; internal reasoning of LLMs is opaque and hard to reproduce; need image-aware, interpretable guidance to improve VQA performance.

Method: A modular SQ-InstructBLIP architecture with three shared-architecture components: Questioner, Answerer, and Reasoner. The Questioner and Answerer generate sub-questions and sub-answers that are tied to the image to help infer the main question. The Reasoner performs the final reasoning on the main question using the sub-question information as additional context.

Result: Experiments show that using the generated sub-questions as additional information improves reasoning accuracy on VQA tasks, outperforming prior methods that rely on unstructured prompting or black-box LLM reasoning.

Conclusion: The approach addresses the limitations of LLM-only and black-box reasoning in vision-language tasks by introducing image-aware self-questioning within a modular, shared-architecture framework, leading to more accurate and interpretable VQA reasoning.

Abstract: The field of vision-language understanding has been actively researched in
recent years, thanks to the development of Large Language Models~(LLMs).
However, it still needs help with problems requiring multi-step reasoning, even
for very simple questions. Recent studies adopt LLMs to tackle this problem by
iteratively generating sub-questions and answers. However, there are
disadvantages such as 1) the fine-grained visual contents of images are not
available using LLMs that cannot read visual information, 2) internal
mechanisms are inaccessible and difficult to reproduce by using black-box LLMs.
To solve these problems, we propose the SQ (Self-Questioning)-InstructBLIP,
which improves inference performance by generating image-aware informative
sub-questions and sub-answers iteratively. The SQ-InstructBLIP, which consists
of a Questioner, Answerer, and Reasoner that share the same architecture.
Questioner and Answerer generate sub-questions and sub-answers to help infer
the main-question, and Reasoner performs reasoning on the main-question
considering the generated sub-question information. Our experiments show that
the proposed method SQ-InstructBLIP, which uses the generated sub-questions as
additional information when solving the VQA task, performs more accurate
reasoning than the previous works.

</details>


### [77] [Hallucination as an Upper Bound: A New Perspective on Text-to-Image Evaluation](https://arxiv.org/abs/2509.21257)
*Seyed Amir Kasaei,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: Proposes a bias-driven hallucination taxonomy for text-to-image (T2I) models, defining three categories—attribute, relation, and object hallucinations—and frames evaluation bounds to uncover hidden biases, enabling richer model assessment.


<details>
  <summary>Details</summary>
Motivation: Hallucination in T2I has not been clearly framed; existing work emphasizes prompt–output alignment and may miss output beyond the prompt or underlying biases. A bias-aware definition and taxonomy is needed to bound evaluation and reveal hidden biases.

Method: Conceptual framing of hallucination in T2I as bias-driven deviations; introduction of a taxonomy with three categories (attribute, relation, object hallucinations); discussion of how this framing yields an upper bound for evaluation and a lens to surface biases.

Result: A defined taxonomy and framing for hallucination in T2I; establishment of an evaluation upper bound and a mechanism to surface hidden biases; groundwork for richer, bias-aware assessment of T2I models.

Conclusion: This framing provides a foundation for more comprehensive T2I evaluation by accounting for bias-driven deviations beyond the prompt and encourages future work to quantify and mitigate these hallucinations.

Abstract: In language and vision-language models, hallucination is broadly understood
as content generated from a model's prior knowledge or biases rather than from
the given input. While this phenomenon has been studied in those domains, it
has not been clearly framed for text-to-image (T2I) generative models. Existing
evaluations mainly focus on alignment, checking whether prompt-specified
elements appear, but overlook what the model generates beyond the prompt. We
argue for defining hallucination in T2I as bias-driven deviations and propose a
taxonomy with three categories: attribute, relation, and object hallucinations.
This framing introduces an upper bound for evaluation and surfaces hidden
biases, providing a foundation for richer assessment of T2I models.

</details>


### [78] [Every Subtlety Counts: Fine-grained Person Independence Micro-Action Recognition via Distributionally Robust Optimization](https://arxiv.org/abs/2509.21261)
*Feng-Qi Cui,Jinyang Huang,Anyang Tong,Ziyu Jia,Jie Zhang,Zhi Liu,Dan Guo,Jianwei Lu,Meng Wang*

Main category: cs.CV

TL;DR: A two-level, plug-and-play framework for robust, person-agnostic micro-action recognition: (1) feature-level Temporal-Frequency Alignment with Wasserstein-based temporal alignment and variance-guided spectral perturbations plus a consistency fusion; (2) loss-level Group-Invariant Regularized Loss using pseudo-groups to mimic unseen distributions, emphasizing boundary cases and constraining subgroup variance. It achieves improved accuracy and robustness on MA-52.


<details>
  <summary>Details</summary>
Motivation: Inter-person variability causes the same micro-action to manifest differently across individuals, hindering generalization in real-world tasks; current methods struggle to generalize to unseen person-specific distributions; a distributionally robust, person-agnostic representation is needed.

Method: Feature-level: Temporal-Frequency Alignment Module with dual-branch design. Temporal branch uses Wasserstein-regularized alignment to stabilize dynamic trajectories; Frequency branch applies variance-guided perturbations to mitigate spectral differences; Consistency-driven fusion combines both branches. Loss-level: Group-Invariant Regularized Loss partitions samples into pseudo-groups to simulate unseen distributions, up-weighting boundary cases and regularizing subgroup variance to promote generalization beyond easy or frequent samples.

Result: Empirical evaluation on the large MA-52 dataset shows the proposed framework outperforms existing methods in both accuracy and robustness, achieving stable generalization under fine-grained conditions.

Conclusion: The combination of distributionally robust optimization with feature- and loss-level regularization yields a robust, person-agnostic micro-action recognition framework that generalizes well to unseen, person-specific variations.

Abstract: Micro-action Recognition is vital for psychological assessment and
human-computer interaction. However, existing methods often fail in real-world
scenarios because inter-person variability causes the same action to manifest
differently, hindering robust generalization. To address this, we propose the
Person Independence Universal Micro-action Recognition Framework, which
integrates Distributionally Robust Optimization principles to learn
person-agnostic representations. Our framework contains two plug-and-play
components operating at the feature and loss levels. At the feature level, the
Temporal-Frequency Alignment Module normalizes person-specific motion
characteristics with a dual-branch design: the temporal branch applies
Wasserstein-regularized alignment to stabilize dynamic trajectories, while the
frequency branch introduces variance-guided perturbations to enhance robustness
against person-specific spectral differences. A consistency-driven fusion
mechanism integrates both branches. At the loss level, the Group-Invariant
Regularized Loss partitions samples into pseudo-groups to simulate unseen
person-specific distributions. By up-weighting boundary cases and regularizing
subgroup variance, it forces the model to generalize beyond easy or frequent
samples, thus enhancing robustness to difficult variations. Experiments on the
large-scale MA-52 dataset demonstrate that our framework outperforms existing
methods in both accuracy and robustness, achieving stable generalization under
fine-grained conditions.

</details>


### [79] [Dense Semantic Matching with VGGT Prior](https://arxiv.org/abs/2509.21263)
*Songlin Yang,Tianyi Wei,Yushi Lan,Zeqi Xiao,Anyi Rao,Xingang Pan*

Main category: cs.CV

TL;DR: A geometry-aware semantic matching approach that reuses VGGT, adds a semantic head, and uses cycle-consistent training to enable cross-instance dense semantic matching under data scarcity, achieving improved geometry awareness and manifold preservation.


<details>
  <summary>Details</summary>
Motivation: Current 2D foundation-model-based matching suffers from geometric ambiguity due to symmetric structures and neglects cross-image invisibility, plus a nearest-neighbor rule that ignores manifold preservation. There is a need for geometry-aware pixel descriptors and holistic dense matching. VGGT offers geometry-grounded features but is designed for within-object cross-views and not cross-instance semantic matching, and dense semantic annotations are scarce.

Method: Reuse early VGGT feature stages, fine-tune later stages, and add a semantic head for bidirectional correspondences; adapt VGGT to semantic matching with cycle-consistent training, synthetic data augmentation, and a progressive training recipe with aliasing artifact mitigation.

Result: Extensive experiments show superior geometry awareness, matching reliability, and manifold preservation, outperforming previous baselines.

Conclusion: The proposed adaptation of VGGT to cross-instance semantic dense matching under data scarcity effectively handles geometry and data limitations, enabling robust pixel-level correspondences across instances.

Abstract: Semantic matching aims to establish pixel-level correspondences between
instances of the same category and represents a fundamental task in computer
vision. Existing approaches suffer from two limitations: (i) Geometric
Ambiguity: Their reliance on 2D foundation model features (e.g., Stable
Diffusion, DINO) often fails to disambiguate symmetric structures, requiring
extra fine-tuning yet lacking generalization; (ii) Nearest-Neighbor Rule: Their
pixel-wise matching ignores cross-image invisibility and neglects manifold
preservation. These challenges call for geometry-aware pixel descriptors and
holistic dense correspondence mechanisms. Inspired by recent advances in 3D
geometric foundation models, we turn to VGGT, which provides geometry-grounded
features and holistic dense matching capabilities well aligned with these
needs. However, directly transferring VGGT is challenging, as it was originally
designed for geometry matching within cross views of a single instance,
misaligned with cross-instance semantic matching, and further hindered by the
scarcity of dense semantic annotations. To address this, we propose an approach
that (i) retains VGGT's intrinsic strengths by reusing early feature stages,
fine-tuning later ones, and adding a semantic head for bidirectional
correspondences; and (ii) adapts VGGT to the semantic matching scenario under
data scarcity through cycle-consistent training strategy, synthetic data
augmentation, and progressive training recipe with aliasing artifact
mitigation. Extensive experiments demonstrate that our approach achieves
superior geometry awareness, matching reliability, and manifold preservation,
outperforming previous baselines.

</details>


### [80] [MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation](https://arxiv.org/abs/2509.21265)
*Xinyu Liu,Guolei Sun,Cheng Wang,Yixuan Yuan,Ender Konukoglu*

Main category: cs.CV

TL;DR: MedVSR introduces Cross State-Space Propagation (CSSP) and Inner State-Space Reconstruction (ISSR) to tackle alignment and artifact issues in high-resolution medical video super-resolution, achieving superior performance and efficiency across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: High-resolution medical videos enable accurate diagnosis but are limited by hardware and physiological constraints. Low-resolution medical videos suffer from camera shake, noise, abrupt transitions, and artifact-prone reconstructions that can mislead clinicians; thus, specialized VSR methods are needed for medical contexts.

Method: CSSP projects distant frames as control matrices within state-space models to selectively propagate consistent features for improved alignment. ISSR performs joint long-range spatial feature learning with large-kernel short-range aggregation to enhance tissue structures and suppress artifacts.

Result: Experiments on four datasets from diverse medical scenarios (e.g., endoscopy, cataract surgeries) show that MedVSR outperforms existing VSR models in both reconstruction quality and efficiency.

Conclusion: MedVSR offers a domain-tailored VSR framework for medical videos with practical effectiveness, and code is publicly available to facilitate adoption.

Abstract: High-resolution (HR) medical videos are vital for accurate diagnosis, yet are
hard to acquire due to hardware limitations and physiological constraints.
Clinically, the collected low-resolution (LR) medical videos present unique
challenges for video super-resolution (VSR) models, including camera shake,
noise, and abrupt frame transitions, which result in significant optical flow
errors and alignment difficulties. Additionally, tissues and organs exhibit
continuous and nuanced structures, but current VSR models are prone to
introducing artifacts and distorted features that can mislead doctors. To this
end, we propose MedVSR, a tailored framework for medical VSR. It first employs
Cross State-Space Propagation (CSSP) to address the imprecise alignment by
projecting distant frames as control matrices within state-space models,
enabling the selective propagation of consistent and informative features to
neighboring frames for effective alignment. Moreover, we design an Inner
State-Space Reconstruction (ISSR) module that enhances tissue structures and
reduces artifacts with joint long-range spatial feature learning and
large-kernel short-range information aggregation. Experiments across four
datasets in diverse medical scenarios, including endoscopy and cataract
surgeries, show that MedVSR significantly outperforms existing VSR models in
reconstruction performance and efficiency. Code released at
https://github.com/CUHK-AIM-Group/MedVSR.

</details>


### [81] [MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources](https://arxiv.org/abs/2509.21268)
*Sicong Leng,Jing Wang,Jiaxi Li,Hao Zhang,Zhiqiang Hu,Boqiang Zhang,Yuming Jiang,Hang Zhang,Xin Li,Lidong Bing,Deli Zhao,Wei Lu,Yu Rong,Aixin Sun,Shijian Lu*

Main category: cs.CV

TL;DR: Variance-aware data sampling (VAS) stabilizes RL fine-tuning for large multimodal reasoning models by promoting reward variance; releases large-scale long-CoT and RL QA data, open-source models, and training code; backed by theoretical and empirical results.


<details>
  <summary>Details</summary>
Motivation: Current RL fine-tuning with Group Relative Policy Optimization (GRPO) suffers gradient vanishing when reward variance is low; there is a lack of open, large-scale long-chain-of-thought data; a need for robust, reproducible data-driven RL optimization and solid baselines for multimodal reasoning.

Method: Introduce Variance-Promotion Score (VPS) to guide Variance-Aware Sampling (VAS), balancing outcome variance and trajectory diversity to increase reward variance and stabilize policy optimization. Curate ~1.6M long-CoT cold-start data and ~15k RL QA pairs with emphasis on quality, difficulty, and diversity. Provide end-to-end training codebase and open-source a family of multimodal reasoning models. Include comprehensive ablations and a theoretical result showing reward variance lower-bounds the expected policy gradient magnitude, with VAS as a practical realization.

Result: Empirical evidence on mathematical reasoning benchmarks shows the proposed data and VAS improve optimization stability and performance. The work offers extensive ablations, analyses, and a reproducible pipeline, along with released data, code, and model checkpoints.

Conclusion: VAS addresses core bottlenecks in open, large-scale multimodal RL: data scarcity/quality and unstable policy optimization. The resources and theoretical insights establish strong baselines for the community and enable scalable, reproducible research.

Abstract: Large multimodal reasoning models have achieved rapid progress, but their
advancement is constrained by two major limitations: the absence of open,
large-scale, high-quality long chain-of-thought (CoT) data, and the instability
of reinforcement learning (RL) algorithms in post-training. Group Relative
Policy Optimization (GRPO), the standard framework for RL fine-tuning, is prone
to gradient vanishing when reward variance is low, which weakens optimization
signals and impairs convergence. This work makes three contributions: (1) We
propose Variance-Aware Sampling (VAS), a data selection strategy guided by
Variance Promotion Score (VPS) that combines outcome variance and trajectory
diversity to promote reward variance and stabilize policy optimization. (2) We
release large-scale, carefully curated resources containing ~1.6M long CoT
cold-start data and ~15k RL QA pairs, designed to ensure quality, difficulty,
and diversity, along with a fully reproducible end-to-end training codebase.
(3) We open-source a family of multimodal reasoning models in multiple scales,
establishing standardized baselines for the community. Experiments across
mathematical reasoning benchmarks demonstrate the effectiveness of both the
curated data and the proposed VAS. Comprehensive ablation studies and analyses
provide further insight into the contributions of each component. In addition,
we theoretically establish that reward variance lower-bounds the expected
policy gradient magnitude, with VAS serving as a practical mechanism to realize
this guarantee. Our code, data, and checkpoints are available at
https://github.com/LengSicong/MMR1.

</details>


### [82] [A Sentinel-3 foundation model for ocean colour](https://arxiv.org/abs/2509.21273)
*Geoffrey Dawson,Remy Vandaele,Andrew Taylor,David Moffat,Helen Tamura-Wicks,Sarah Jackson,Rosie Lickorish,Paolo Fraccaro,Hywel Williams,Chunbo Luo,Anne Jones*

Main category: cs.CV

TL;DR: A self-supervised geospatial foundation model (Prithvi-EO ViT) trained on Sentinel-3 OLCI data is fine-tuned for chlorophyll concentration and ocean primary production; shows data-efficient performance and detailed spatial pattern capture, aligning with point observations.


<details>
  <summary>Details</summary>
Motivation: Labelled ocean data are scarce and costly; robust, data-efficient models are needed to extract reliable ocean color and productivity signals from large unlabeled datasets.

Method: Pre-train Prithvi-EO Vision Transformer to reconstruct Sentinel-3 OLCI data; fine-tune on two downstream tasks (chlorophyll concentration estimation and ocean primary production); compare against baseline models and assess spatial detail capture and agreement with point observations.

Result: Self-trained FMs enable better use of small labeled datasets, capture fine spatial patterns in ocean color, and match point observations, suggesting robust, data-driven marine monitoring capabilities.

Conclusion: Geospatial AI foundation models can provide more robust, data-driven insights into ocean ecosystems and their role in global climate, particularly when labeled data are limited.

Abstract: Artificial Intelligence (AI) Foundation models (FMs), pre-trained on massive
unlabelled datasets, have the potential to drastically change AI applications
in ocean science, where labelled data are often sparse and expensive to
collect. In this work, we describe a new foundation model using the Prithvi-EO
Vision Transformer architecture which has been pre-trained to reconstruct data
from the Sentinel-3 Ocean and Land Colour Instrument (OLCI). We evaluate the
model by fine-tuning on two downstream marine earth observation tasks. We first
assess model performance compared to current baseline models used to quantify
chlorophyll concentration. We then evaluate the FMs ability to refine remote
sensing-based estimates of ocean primary production. Our results demonstrate
the utility of self-trained FMs for marine monitoring, in particular for making
use of small amounts of high quality labelled data and in capturing detailed
spatial patterns of ocean colour whilst matching point observations. We
conclude that this new generation of geospatial AI models has the potential to
provide more robust, data-driven insights into ocean ecosystems and their role
in global climate processes.

</details>


### [83] [Does FLUX Already Know How to Perform Physically Plausible Image Composition?](https://arxiv.org/abs/2509.21278)
*Shilin Lu,Zhuming Lian,Zihan Zhou,Shaocong Zhang,Chen Zhao,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: SHINE = training-free, high-fidelity image composition via manifold-steered anchor loss and adapters to insert objects into challenging scenes; introduces ComplexCompo benchmark; achieves state-of-the-art metrics.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of diffusion-based composition under complex lighting (shadows, reflections) and high-resolution inputs; removes reliance on latent inversion or brittle attention hacks; current benchmarks are insufficient for real-world complexity.

Method: A training-free framework (SHINE) that uses manifold-steered anchor loss and pretrained customization adapters (e.g., IP-Adapter) to guide latents for faithful subject representation while preserving background. Introduces degradation-suppression guidance and adaptive background blending. Presents ComplexCompo benchmark with diverse resolutions and challenging lighting/reflective scenarios.

Result: Reported state-of-the-art performance on standard metrics (e.g., DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward) in experiments on ComplexCompo and DreamEditBench.

Conclusion: Code and ComplexCompo benchmark will be publicly available; SHINE offers a training-free path to seamless, high-fidelity insertion in complex scenes and strong practical potential.

Abstract: Image composition aims to seamlessly insert a user-specified object into a
new scene, but existing models struggle with complex lighting (e.g., accurate
shadows, water reflections) and diverse, high-resolution inputs. Modern
text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential
physical and resolution priors, yet lack a framework to unleash them without
resorting to latent inversion, which often locks object poses into contextually
inappropriate orientations, or brittle attention surgery. We propose SHINE, a
training-free framework for Seamless, High-fidelity Insertion with Neutralized
Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained
customization adapters (e.g., IP-Adapter) to guide latents for faithful subject
representation while preserving background integrity. Degradation-suppression
guidance and adaptive background blending are proposed to further eliminate
low-quality outputs and visible seams. To address the lack of rigorous
benchmarks, we introduce ComplexCompo, featuring diverse resolutions and
challenging conditions such as low lighting, strong illumination, intricate
shadows, and reflective surfaces. Experiments on ComplexCompo and
DreamEditBench show state-of-the-art performance on standard metrics (e.g.,
DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward).
Code and benchmark will be publicly available upon publication.

</details>


### [84] [Quantized Visual Geometry Grounded Transformer](https://arxiv.org/abs/2509.21302)
*Weilun Feng,Haotong Qin,Mingqiang Wu,Chuanguang Yang,Yuqi Li,Xiangqi Li,Zhulin An,Libo Huang,Yulun Zhang,Michele Magno,Yongjun Xu*

Main category: cs.CV

TL;DR: A quantization framework (QuantVGGT) for large VGGTs that enables efficient PTQ via Dual-Smoothed Fine-Grained Quantization and Noise-Filtered Diverse Sampling, achieving 4-bit quantization with major memory and speedups while preserving reconstruction accuracy; state-of-the-art results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Billion-scale VGGTs incur prohibitive compute/memory in deployment; PTQ is attractive but faces 3D data-specific challenges (heavy-tailed activations from data-independent tokens, unstable calibration due to multi-view data).

Method: Two-stage approach: (1) Dual-Smoothed Fine-Grained Quantization combining pre-global Hadamard rotation with post-local channel smoothing to stabilize activation distributions and inter-channel variance; (2) Noise-Filtered Diverse Sampling selecting calibration samples by deep-layer statistics and frame-aware clustering to produce robust quantization ranges.

Result: QuantVGGT achieves state-of-the-art PTQ performance across benchmarks and bit-width; 4-bit quantization yields ~3.7x memory reduction and ~2.5x hardware acceleration, with reconstruction accuracy above 98% of full-precision.

Conclusion: First quantization framework for VGGTs; demonstrates practicality and effectiveness of PTQ for billion-scale 3D transformers; code released.

Abstract: Learning-based 3D reconstruction models, represented by Visual Geometry
Grounded Transformers (VGGTs), have made remarkable progress with the use of
large-scale transformers. Their prohibitive computational and memory costs
severely hinder real-world deployment. Post-Training Quantization (PTQ) has
become a common practice for compressing and accelerating models. However, we
empirically observe that PTQ faces unique obstacles when compressing
billion-scale VGGTs: the data-independent special tokens induce heavy-tailed
activation distributions, while the multi-view nature of 3D data makes
calibration sample selection highly unstable. This paper proposes the first
Quantization framework for VGGTs, namely QuantVGGT. This mainly relies on two
technical contributions: First, we introduce Dual-Smoothed Fine-Grained
Quantization, which integrates pre-global Hadamard rotation and post-local
channel smoothing to mitigate heavy-tailed distributions and inter-channel
variance robustly. Second, we design Noise-Filtered Diverse Sampling, which
filters outliers via deep-layer statistics and constructs frame-aware diverse
calibration clusters to ensure stable quantization ranges. Comprehensive
experiments demonstrate that QuantVGGT achieves the state-of-the-art results
across different benchmarks and bit-width, surpassing the previous
state-of-the-art generic quantization method with a great margin. We highlight
that our 4-bit QuantVGGT can deliver a 3.7$\times$ memory reduction and
2.5$\times$ acceleration in real-hardware inference, while maintaining
reconstruction accuracy above 98\% of its full-precision counterpart. This
demonstrates the vast advantages and practicality of QuantVGGT in
resource-constrained scenarios. Our code is released in
https://github.com/wlfeng0509/QuantVGGT.

</details>


### [85] [NewtonGen: Physics-Consistent and Controllable Text-to-Video Generation via Neural Newtonian Dynamics](https://arxiv.org/abs/2509.21309)
*Yu Yuan,Xijun Wang,Tharindu Wickremasinghe,Zeeshan Nadir,Bole Ma,Stanley H. Chan*

Main category: cs.CV

TL;DR: Proposes NewtonGen with trainable Neural Newtonian Dynamics (NND) to enforce physical dynamics in video generation, enabling physically consistent motion and precise parameter control.


<details>
  <summary>Details</summary>
Motivation: Current text-to-video models rely on appearance-based motion learning, yielding unrealistic dynamics and poor controllability; a physics-informed approach is needed to capture underlying dynamics and enable conditioning on physical parameters across initial conditions.

Method: Introduce Neural Newtonian Dynamics (NND) and integrate into a Generative framework as NewtonGen, combining data priors with learnable dynamical constraints to guide video synthesis.

Result: Claims that the framework enables physically consistent video synthesis with precise parameter control by leveraging NND and data priors, leading to more plausible, controllable dynamics under varying initial conditions.

Conclusion: Integrating data priors with learnable physical principles can address fundamental limitations in text-to-video generation by injecting dynamical constraints into the generative process.

Abstract: A primary bottleneck in large-scale text-to-video generation today is
physical consistency and controllability. Despite recent advances,
state-of-the-art models often produce unrealistic motions, such as objects
falling upward, or abrupt changes in velocity and direction. Moreover, these
models lack precise parameter control, struggling to generate physically
consistent dynamics under different initial conditions. We argue that this
fundamental limitation stems from current models learning motion distributions
solely from appearance, while lacking an understanding of the underlying
dynamics. In this work, we propose NewtonGen, a framework that integrates
data-driven synthesis with learnable physical principles. At its core lies
trainable Neural Newtonian Dynamics (NND), which can model and predict a
variety of Newtonian motions, thereby injecting latent dynamical constraints
into the video generation process. By jointly leveraging data priors and
dynamical guidance, NewtonGen enables physically consistent video synthesis
with precise parameter control.

</details>


### [86] [SD3.5-Flash: Distribution-Guided Distillation of Generative Flows](https://arxiv.org/abs/2509.21318)
*Hmrishav Bandyopadhyay,Rahim Entezari,Jim Scott,Reshinth Adithyan,Yi-Zhe Song,Varun Jampani*

Main category: cs.CV

TL;DR: SD3.5-Flash is a few-step distillation framework that enables high-quality image generation on consumer devices by distilling rectified flow models with two innovations—timestep sharing and split-timestep fine-tuning—along with hardware-aware pipeline optimizations, achieving superior performance over existing few-step methods.


<details>
  <summary>Details</summary>
Motivation: Democratize access to high-quality generative image models by dramatically reducing compute, memory, and energy demands so that mobile and desktop devices can run practical, high-fidelity generation.

Method: Distill rectified flow models using a reformulated distribution-matching objective tailored for few-step generation. Introduce timestep sharing to reduce gradient noise and split-timestep fine-tuning to improve prompt alignment. Optimize pipeline with text encoder restructuring and specialized quantization to enable fast, memory-efficient deployment across diverse hardware.

Result: Extensive evaluations, including large-scale user studies, show SD3.5-Flash consistently outperforms existing few-step methods in speed, quality, and user satisfaction while maintaining memory efficiency and broad device compatibility.

Conclusion: This work makes advanced generative AI more accessible for practical deployment across a wide range of devices, from mobile phones to desktops, by delivering high-quality, efficient few-step generation.

Abstract: We present SD3.5-Flash, an efficient few-step distillation framework that
brings high-quality image generation to accessible consumer devices. Our
approach distills computationally prohibitive rectified flow models through a
reformulated distribution matching objective tailored specifically for few-step
generation. We introduce two key innovations: "timestep sharing" to reduce
gradient noise and "split-timestep fine-tuning" to improve prompt alignment.
Combined with comprehensive pipeline optimizations like text encoder
restructuring and specialized quantization, our system enables both rapid
generation and memory-efficient deployment across different hardware
configurations. This democratizes access across the full spectrum of devices,
from mobile phones to desktop computers. Through extensive evaluation including
large-scale user studies, we demonstrate that SD3.5-Flash consistently
outperforms existing few-step methods, making advanced generative AI truly
accessible for practical deployment.

</details>


### [87] [Copycats: the many lives of a publicly available medical imaging dataset](https://arxiv.org/abs/2402.06353)
*Amelia Jiménez-Sánchez,Natalia-Rozalia Avlona,Dovile Juodelyte,Théo Sourget,Caroline Vang-Larsen,Anna Rogers,Hubert Dariusz Zając,Veronika Cheplygina*

Main category: cs.CV

TL;DR: Open medical imaging datasets on CCPs show governance gaps that threaten data quality and safe healthcare AI, including vague licenses, missing persistent identifiers, storage issues, duplicates, and incomplete metadata.


<details>
  <summary>Details</summary>
Motivation: As medical imaging data become more openly available, governance flaws in community-contributed platforms risk degraded data quality, biased or unsafe AI, and unfair outcomes in healthcare; the study analyzes CCP datasets to understand current practices and their potential harm.

Method: Systematically analyze publicly available MI/medical imaging datasets on CCPs (e.g., Kaggle, HuggingFace), compare MI with general computer vision datasets, and assess data sharing, documentation, and maintenance; identify licenses, identifiers, storage, duplicates, and metadata gaps, noting platform differences.

Result: The analysis finds pervasive vague licenses, lack of persistent identifiers, insufficient storage considerations, duplicate entries, and missing metadata across CCP datasets, with notable differences between platforms; these shortcomings hinder provenance, reproducibility, and responsible AI development in healthcare.

Conclusion: There is a need for stronger governance and standardized practices for data sharing, documentation, persistent identifiers, and metadata on CCPs to improve the quality, fairness, and safety of AI models trained on medical imaging data; the work informs efforts toward responsible data curation in healthcare AI.

Abstract: Medical Imaging (MI) datasets are fundamental to artificial intelligence in
healthcare. The accuracy, robustness, and fairness of diagnostic algorithms
depend on the data (and its quality) used to train and evaluate the models. MI
datasets used to be proprietary, but have become increasingly available to the
public, including on community-contributed platforms (CCPs) like Kaggle or
HuggingFace. While open data is important to enhance the redistribution of
data's public value, we find that the current CCP governance model fails to
uphold the quality needed and recommended practices for sharing, documenting,
and evaluating datasets. In this paper, we conduct an analysis of publicly
available machine learning datasets on CCPs, discussing datasets' context, and
identifying limitations and gaps in the current CCP landscape. We highlight
differences between MI and computer vision datasets, particularly in the
potentially harmful downstream effects from poor adoption of recommended
dataset management practices. We compare the analyzed datasets across several
dimensions, including data sharing, data documentation, and maintenance. We
find vague licenses, lack of persistent identifiers and storage, duplicates,
and missing metadata, with differences between the platforms. Our research
contributes to efforts in responsible data curation and AI algorithms for
healthcare.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [88] [Further Results on Rendering Geometric Intersection Graphs Sparse by Dispersion](https://arxiv.org/abs/2509.20903)
*Nicolás Honorato-Droguett,Kazuhiro Kurita,Tesshu Hanaka,Hirotaka Ono,Alexander Wolff*

Main category: cs.CG

TL;DR: We study Geometric Graph Edit Distance under movement costs; provide an O(n log n) algorithm to render the intersection graphs of unit circular arcs into edgeless, acyclic, or k-clique-free graphs; prove strong NP-hardness for unweighted interval graphs and for tuples of d-balls/d-cubes; give an XP algorithm parameterized by the number of maximal cliques for weighted unit intervals edgeless graphs.


<details>
  <summary>Details</summary>
Motivation: Overlaps removal is a central task in scheduling, visibility, and map labeling. It can be modeled as editing a geometric intersection graph to enforce a sparsity constraint, with edit cost measured by object movement.

Method: Algorithm to render the intersection graph of unit circular arcs into specified graph classes in O(n log n) time; complexity proofs showing strong NP-hardness for unweighted interval graphs and for tuples of d-balls and d-cubes (d≥2); an XP algorithm parameterized by the number of maximal cliques for rendering weighted unit intervals edgeless.

Result: An O(n log n) time rendering algorithm for unit circular arcs producing edgeless, acyclic, or k-clique-free graphs; strong NP-hardness for unweighted interval graphs; strong NP-hardness for tuples of d-balls/d-cubes (any d≥2); an XP algorithm for weighted unit intervals edgeless graphs parameterized by the number of maximal cliques.

Conclusion: The work advances the understanding of Geometric Graph Edit Distance by identifying tractable rendering cases, establishing hardness results that complete open questions, and offering a parameterized approach for a weighted variant, thus informing future exploration of geometry-driven graph editing.

Abstract: Removing overlaps is a central task in domains such as scheduling,
visibility, and map labelling. This task can be modelled using graphs, where
overlap removals correspond to enforcing a certain sparsity constraint on the
graph structure. We continue the study of the problem Geometric Graph Edit
Distance, where the aim is to minimise the total cost of editing a geometric
intersection graph to obtain a graph contained in a specific graph class. For
us, the edit operation is the movement of objects, and the cost is the movement
distance. We present an algorithm for rendering the intersection graph of a set
of unit circular arcs (i)~edgeless, (ii)~acyclic, and (iii)~$k$-clique-free in
$O(n\log n)$ time, where $n$ is the number of arcs. We also show that the
problem remains strongly NP-hard on unweighted interval graphs, solving an open
problem of [Honorato-Droguett et al., WADS 2025]. We complement this result by
showing that the problem is strongly NP-hard on tuples of $d$-balls and
$d$-cubes, for any $d\ge 2$. Finally, we present an XP algorithm (parameterised
by the number of maximal cliques) for rendering the intersection graph of a set
of weighted unit intervals edgeless.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [89] [A Theory of Multi-Agent Generative Flow Networks](https://arxiv.org/abs/2509.20408)
*Leo Maxime Brunswic,Haozhi Wang,Shuang Luo,Jianye Hao,Amir Rasouli,Yinchuan Li*

Main category: cs.LG

TL;DR: Multi-agent extension of Generative Flow Networks (MA-GFlowNets) with four training algorithms (centralized, independent, joint, and updated conditional) enabling collaborative object generation through joint actions. The local-global training principle allows treating multiple local GFNs as a single global one, yielding theoretical guarantees that independent policies sample proportional to the reward; experiments show superiority over RL and MCMC methods.


<details>
  <summary>Details</summary>
Motivation: Extend GFlowNets to a multi-agent setting to enable collaborative generation of objects via joint actions, providing a framework that supports centralized training, decentralized execution, and their combinations with theoretical guarantees.

Method: Develop MA-GFlowNets theory and propose four algorithms: (1) centralized flow network for centralized training, (2) independent flow network for decentralized execution, (3) joint flow network for centralized training with decentralized execution, and (4) updated conditional version. Introduce a local-global principle to train a collection of local GFNs as a single global GFN, enabling theoretical guarantees that independent policies yield samples with probability proportional to the reward.

Result: The framework provides theoretical guarantees that independent policies generate samples with probability proportional to the reward; experimental results indicate MA-GFlowNets outperform reinforcement learning and MCMC-based methods in generating objects.

Conclusion: MA-GFlowNets offer a principled multi-agent extension of GFlowNets with training schemes for centralized and decentralized settings, underpinned by a local-global training principle, and demonstrated to achieve superior performance over RL and MCMC baselines.

Abstract: Generative flow networks utilize a flow-matching loss to learn a stochastic
policy for generating objects from a sequence of actions, such that the
probability of generating a pattern can be proportional to the corresponding
given reward. However, a theoretical framework for multi-agent generative flow
networks (MA-GFlowNets) has not yet been proposed. In this paper, we propose
the theory framework of MA-GFlowNets, which can be applied to multiple agents
to generate objects collaboratively through a series of joint actions. We
further propose four algorithms: a centralized flow network for centralized
training of MA-GFlowNets, an independent flow network for decentralized
execution, a joint flow network for achieving centralized training with
decentralized execution, and its updated conditional version. Joint Flow
training is based on a local-global principle allowing to train a collection of
(local) GFN as a unique (global) GFN. This principle provides a loss of
reasonable complexity and allows to leverage usual results on GFN to provide
theoretical guarantees that the independent policies generate samples with
probability proportional to the reward function. Experimental results
demonstrate the superiority of the proposed framework compared to reinforcement
learning and MCMC-based methods.

</details>


### [90] [FastEagle: Cascaded Drafting for Accelerating Speculative Decoding](https://arxiv.org/abs/2509.20416)
*Haiduo Huang,Jiangcheng Song,Wenzhe Zhao,Pengju Ren*

Main category: cs.LG

TL;DR: FastEagle introduces a non-autoregressive cascaded drafter that emits an entire draft in a single forward pass, replacing sequential drafting steps with a lightweight layer cascade and layer-wise supervision, while using a constrained draft tree to preserve lossless verification cost. It achieves substantial wall-clock speedups over autoregressive drafters (e.g., EAGLE-3) across multiple models and tasks, with comparable acceptance behavior.


<details>
  <summary>Details</summary>
Motivation: To overcome the inherent sequential bottleneck in speculative decoding drafters like EAGLE, which require N passes for N tokens, by eliminating sequential dependencies to accelerate inference without sacrificing verification cost or acceptance behavior.

Method: Proposes a non-autoregressive cascaded drafting architecture (FastEagle) that uses a lightweight layer cascade instead of temporal steps, and applies layer-wise supervision to mitigate error accumulation. Introduces a constrained draft tree to ensure lossless verification cost, enabling fast, one-pass draft generation while preserving exact verification semantics.

Result: FastEagle achieves substantial wall-clock speedups over strong autoregressive drafters across multiple LLMs (Vicuna-13B, LLaMA-Instruct 3.x, DeepSeek-R1-Distill-LLaMA) and tasks (MT-Bench, HumanEval, GSM8K, CNN/DM, Alpaca), outperforming EAGLE-3 in speedup under both greedy and stochastic decoding, with comparable average acceptance lengths.

Conclusion: Removing sequential dependencies in drafting is a practical path toward lossless LLM inference acceleration.

Abstract: Speculative decoding accelerates generation by drafting candidates and
verifying them in parallel, yet state-of-the-art drafters (e.g., EAGLE) still
require N sequential passes to propose N tokens. We present FastEagle, a
non-autoregressive cascaded drafter that emits an entire draft in a single
forward pass. FastEagle replaces temporal steps with a lightweight layer
cascade and trains with layer-wise supervision to mitigate error accumulation.
Coupled with a constrained draft tree that preserves lossless verification
cost, FastEagle delivers substantial wall-clock speedups over strong
autoregressive drafters while maintaining competitive acceptance behavior.
Across multiple LLMs (Vicuna-13B, LLaMA-Instruct 3.x, and
DeepSeek-R1-Distill-LLaMA) and tasks (MT-Bench, HumanEval, GSM8K, CNN/DM,
Alpaca), FastEagle consistently outperforms EAGLE-3 in speedup under both
greedy and stochastic decoding, with comparable average acceptance lengths.
These results indicate that removing sequential dependencies in drafting is a
practical path toward lossless LLM inference acceleration.

</details>


### [91] [mloz: A Highly Efficient Machine Learning-Based Ozone Parameterization for Climate Sensitivity Simulations](https://arxiv.org/abs/2509.20422)
*Yiling Ma,Nathan Luke Abraham,Stefan Versick,Roland Ruhnke,Andrea Schneidereit,Ulrike Niemeier,Felix Back,Peter Braesicke,Peer Nowack*

Main category: cs.LG

TL;DR: A machine learning ozone parameterization (mloz) is proposed to interactively model daily ozone variability in CMIP-scale climate models, achieving ~31x speedup over chemistry schemes and transferring across UKESM and ICON; enables ozone-climate feedbacks without expensive chemistry computation.


<details>
  <summary>Details</summary>
Motivation: CMIP models lack interactive ozone due to chemistry costs; need accurate, efficient representation of ozone to capture its role in climate sensitivity and QBO interactions.

Method: Develop mloz using atmospheric temperature profile as input to predict ozone; include two-way interactions with Quasi-Biennual Oscillation; validate on UKESM and ICON; demonstrate online use and transferability.

Result: High fidelity decadal ozone variability and trends; 31x faster than chemistry; <4% of total model runtime; successful parameter transfer UKESM->ICON; supports CMIP-level adoption.

Conclusion: ML-based ozone parameterization enables interactive ozone in climate models with low overhead, facilitating future climate change assessments and sensitivity studies where ozone modulates atmospheric feedbacks.

Abstract: Atmospheric ozone is a crucial absorber of solar radiation and an important
greenhouse gas. However, most climate models participating in the Coupled Model
Intercomparison Project (CMIP) still lack an interactive representation of
ozone due to the high computational costs of atmospheric chemistry schemes.
Here, we introduce a machine learning parameterization (mloz) to interactively
model daily ozone variability and trends across the troposphere and
stratosphere in standard climate sensitivity simulations, including two-way
interactions of ozone with the Quasi-Biennial Oscillation. We demonstrate its
high fidelity on decadal timescales and its flexible use online across two
different climate models -- the UK Earth System Model (UKESM) and the German
ICOsahedral Nonhydrostatic (ICON) model. With atmospheric temperature profile
information as the only input, mloz produces stable ozone predictions around 31
times faster than the chemistry scheme in UKESM, contributing less than 4
percent of the respective total climate model runtimes. In particular, we also
demonstrate its transferability to different climate models without chemistry
schemes by transferring the parameterization from UKESM to ICON. This
highlights the potential for widespread adoption in CMIP-level climate models
that lack interactive chemistry for future climate change assessments,
particularly when focusing on climate sensitivity simulations, where ozone
trends and variability are known to significantly modulate atmospheric feedback
processes.

</details>


### [92] [Bridging Privacy and Utility: Synthesizing anonymized EEG with constraining utility functions](https://arxiv.org/abs/2509.20454)
*Kay Fuhrmeister,Arne Pelzer,Fabian Radke,Julia Lechinger,Mahzad Gharleghi,Thomas Köllmer,Insa Wolf*

Main category: cs.LG

TL;DR: Transformer-based autoencoder for EEG anonymization that reduces subject re-identification while preserving sleep-staging utility.


<details>
  <summary>Details</summary>
Motivation: Growing use of EEG consumer devices raises privacy concerns about re-identification and leakage of personal info; there is a need to safeguard EEG data without sacrificing machine learning utility.

Method: Develop and apply a transformer-based autoencoder to EEG data to produce anonymized representations; evaluate re-identification risk and ML utility (sleep staging) before and after anonymization.

Result: Re-identifiability is substantially reduced after anonymization, while utility for sleep staging is preserved.

Conclusion: Demonstrates feasible privacy-preserving EEG representations via transformer-based autoencoders; supports balancing privacy and utility with potential for real-world deployment, while noting future work and limitations.

Abstract: Electroencephalography (EEG) is widely used for recording brain activity and
has seen numerous applications in machine learning, such as detecting sleep
stages and neurological disorders. Several studies have successfully shown the
potential of EEG data for re-identification and leakage of other personal
information. Therefore, the increasing availability of EEG consumer devices
raises concerns about user privacy, motivating us to investigate how to
safeguard this sensitive data while retaining its utility for EEG applications.
To address this challenge, we propose a transformer-based autoencoder to create
EEG data that does not allow for subject re-identification while still
retaining its utility for specific machine learning tasks. We apply our
approach to automatic sleep staging by evaluating the re-identification and
utility potential of EEG data before and after anonymization. The results show
that the re-identifiability of the EEG signal can be substantially reduced
while preserving its utility for machine learning.

</details>


### [93] [Efficiently Attacking Memorization Scores](https://arxiv.org/abs/2509.20463)
*Tue Do,Varun Chandrasekaran,Daniel Alabi*

Main category: cs.LG

TL;DR: Memorization-based influence estimators can be manipulated adversarially. The authors present a practical attack using the input pseudoinverse with black-box model access to craft highly memorized samples, demonstrating targeted score manipulation across image classification tasks. They also provide a theoretical analysis showing conditions under which these influence estimates are fragile, underscoring the need for robust defenses and auditing (code provided).


<details>
  <summary>Details</summary>
Motivation: Influence scores (e.g., memorization proxies) are used for data valuation and responsible ML. If attackers can alter these scores, attribution, data curation, and accountability become unreliable. This work systematically investigates the feasibility and robustness of such attacks.

Method: The authors characterize attacks that treat highly memorized samples as highly sensitive queries in the regime where the trained model is accurate. They propose a practical attack by calculating the pseudoinverse of the input, requiring only black-box access to model outputs and modest computational overhead. They validate the attack on a broad set of image classification tasks and show vulnerability of state-of-the-art proxies.

Result: Empirical evaluation demonstrates that the attack is practical and effective across diverse image classification tasks; state-of-the-art memorization proxies are vulnerable to targeted score manipulation. A theoretical analysis of the stability of memorization scores under adversarial perturbations reveals conditions under which influence estimates are inherently fragile.

Conclusion: Influence-based attribution is vulnerable to adversarial manipulation; robust defenses and auditing are needed. The work highlights critical vulnerabilities in influence-based data attribution and provides code for replication.

Abstract: Influence estimation tools -- such as memorization scores -- are widely used
to understand model behavior, attribute training data, and inform dataset
curation. However, recent applications in data valuation and responsible
machine learning raise the question: can these scores themselves be
adversarially manipulated? In this work, we present a systematic study of the
feasibility of attacking memorization-based influence estimators. We
characterize attacks for producing highly memorized samples as highly sensitive
queries in the regime where a trained algorithm is accurate. Our attack
(calculating the pseudoinverse of the input) is practical, requiring only
black-box access to model outputs and incur modest computational overhead. We
empirically validate our attack across a wide suite of image classification
tasks, showing that even state-of-the-art proxies are vulnerable to targeted
score manipulations. In addition, we provide a theoretical analysis of the
stability of memorization scores under adversarial perturbations, revealing
conditions under which influence estimates are inherently fragile. Our findings
highlight critical vulnerabilities in influence-based attribution and suggest
the need for robust defenses. All code can be found at
https://anonymous.4open.science/r/MemAttack-5413/

</details>


### [94] [Offline Goal-conditioned Reinforcement Learning with Quasimetric Representations](https://arxiv.org/abs/2509.20478)
*Vivek Myers,Bill Chunyuan Zheng,Benjamin Eysenbach,Sergey Levine*

Main category: cs.LG

TL;DR: A unified approach to goal-conditioned RL that combines contrastive successor representations with a quasimetric representation space to learn optimal goal distances, even from suboptimal data and in stochastic environments, yielding robust offline GCRL performance and improved stitching.


<details>
  <summary>Details</summary>
Motivation: Existing GCRL methods either rely on contrastive representations (stable but limited long-horizon performance) or quasimetric networks (stitching and goal-distance estimation) but struggle when data are suboptimal or environments are stochastic. There is a need to fuse the strengths of both frameworks to achieve reliable, long-horizon goal-reaching in offline settings.

Method: Introduce a quasimetric representation space that satisfies the triangle inequality, and enforce additional constraints to learn successor representations within this space. The approach unifies contrastive objectives (inferring future outcomes) with temporal-distance concepts (transit-time relationships) to learn optimal goal distances despite suboptimal data and stochastic dynamics, enabling free stitching and long-horizon planning.

Result: On offline GCRL benchmarks, the method improves stitching tasks where contrastive-learning-based methods underperform and performs robustly in noisy, high-dimensional environments where purely quasimetric networks struggle, effectively combining the benefits of both frameworks.

Conclusion: The proposed framework delivers the best of both worlds: stability and long-horizon capability from Monte Carlo-contrastive methods and stitching flexibility from quasimetric parameterizations, advancing offline GCRL performance and robustness.

Abstract: Approaches for goal-conditioned reinforcement learning (GCRL) often use
learned state representations to extract goal-reaching policies. Two frameworks
for representation structure have yielded particularly effective GCRL
algorithms: (1) *contrastive representations*, in which methods learn
"successor features" with a contrastive objective that performs inference over
future outcomes, and (2) *temporal distances*, which link the (quasimetric)
distance in representation space to the transit time from states to goals. We
propose an approach that unifies these two frameworks, using the structure of a
quasimetric representation space (triangle inequality) with the right
additional constraints to learn successor representations that enable optimal
goal-reaching. Unlike past work, our approach is able to exploit a
**quasimetric** distance parameterization to learn **optimal** goal-reaching
distances, even with **suboptimal** data and in **stochastic** environments.
This gives us the best of both worlds: we retain the stability and long-horizon
capabilities of Monte Carlo contrastive RL methods, while getting the free
stitching capabilities of quasimetric network parameterizations. On existing
offline GCRL benchmarks, our representation learning objective improves
performance on stitching tasks where methods based on contrastive learning
struggle, and on noisy, high-dimensional environments where methods based on
quasimetric networks struggle.

</details>


### [95] [CoSupFormer : A Contrastive Supervised learning approach for EEG signal Classification](https://arxiv.org/abs/2509.20489)
*D. Darankoum,C. Habermacher,J. Volle,S. Grudinin*

Main category: cs.LG

TL;DR: End-to-end EEG framework combining multi-scale spectral encoding, attention across channels and patches, channel gating, and supervised-contrastive learning to robustly diagnose CNS disorders; demonstrated across Parkinson's, Alzheimer's, and cross-species EEG.


<details>
  <summary>Details</summary>
Motivation: EEGs are noisy and highly variable; there is a need for multi-scale representations and reliable channel selection to extract meaningful, generalizable features for diagnosis and drug development.

Method: An encoder capturing multi-scale frequency information; an attention-based encoder modeling cross-channel interactions and within-channel patches; a gating network to filter noisy/uninformative channels; a loss combining supervised and contrastive components; trained end-to-end and tested on tasks including CNS treatment effects, Parkinson's, Alzheimer's; cross-species validation.

Result: The approach extracts biologically meaningful EEG patterns; autonomously selects high-quality channels; shows robust generalization across tasks and species; implies improvements over representative baselines.

Conclusion: This framework advances EEG-based biomarker extraction with improved noise/channel handling and generalization, with potential impact on clinical diagnosis and drug development; future work could expand to more datasets and real-time deployment.

Abstract: Electroencephalography signals (EEGs) contain rich multi-scale information
crucial for understanding brain states, with potential applications in
diagnosing and advancing the drug development landscape. However, extracting
meaningful features from raw EEG signals while handling noise and channel
variability remains a major challenge. This work proposes a novel end-to-end
deep-learning framework that addresses these issues through several key
innovations. First, we designed an encoder capable of explicitly capturing
multi-scale frequency oscillations covering a wide range of features for
different EEG-related tasks. Secondly, to model complex dependencies and handle
the high temporal resolution of EEGs, we introduced an attention-based encoder
that simultaneously learns interactions across EEG channels and within
localized {\em patches} of individual channels. We integrated a dedicated
gating network on top of the attention encoder to dynamically filter out noisy
and non-informative channels, enhancing the reliability of EEG data. The entire
encoding process is guided by a novel loss function, which leverages supervised
and contrastive learning, significantly improving model generalization. We
validated our approach in multiple applications, ranging from the
classification of effects across multiple Central Nervous System (CNS)
disorders treatments to the diagnosis of Parkinson's and Alzheimer's disease.
Our results demonstrate that the proposed learning paradigm can extract
biologically meaningful patterns from raw EEG signals across different species,
autonomously select high-quality channels, and achieve robust generalization
through innovative architectural and loss design.

</details>


### [96] [Beyond Visual Similarity: Rule-Guided Multimodal Clustering with explicit domain rules](https://arxiv.org/abs/2509.20501)
*Kishor Datta Gupta,Mohd Ariful Haque,Marufa Kamal,Ahmed Rafi Hasan,Md. Mahfuzur Rahman,Roy George*

Main category: cs.LG

TL;DR: Domain-aware, rule-guided multimodal clustering with a VAE (DARTVAE) that ingests LLM-generated rules and knowledge graphs into a latent space, using rule-based penalties alongside reconstruction and KL terms to enforce constraints; shows improved, interpretable clusters in aircraft/automotive domains but faces rule hallucination, overfitting, and scaling challenges.


<details>
  <summary>Details</summary>
Motivation: Traditional clustering relies on input data similarity and often ignores domain-specific constraints and semantic knowledge. There is a need to incorporate hard/soft domain rules to obtain more meaningful and actionable clusters in knowledge-intensive domains such as aviation and automotive.

Method: Extend a Variational Autoencoder by embedding explicit domain rules, semantic representations, and data-driven features into a unified latent space. Generate rules via large language models (LLMs), organize them into knowledge graphs, and enforce them through a composite loss: reconstruction loss, KL divergence, rule-consistency penalties, and violation penalties. Treat rules as first-class learning signals rather than post hoc filters. Train on multimodal data (e.g., sensor-like features for aircraft/vehicles) to produce constrained clusters.

Result: Rule-guided clustering yielded more operationally meaningful and interpretable clusters (e.g., isolating UAVs, unifying stealth aircraft, separating SUVs from sedans) and improved traditional clustering metrics on aircraft and automotive datasets compared to purely data-driven methods.

Conclusion: Integrating rule encodings with learned representations can produce more meaningful, consistent clustering in knowledge-intensive settings. DARTVAE demonstrates the value of constraint-guided multimodal clustering, while highlighting challenges in rule quality (hallucinations/conflicts), potential overfitting from too many rules, and scaling to complex domains.

Abstract: Traditional clustering techniques often rely solely on similarity in the
input data, limiting their ability to capture structural or semantic
constraints that are critical in many domains. We introduce the Domain Aware
Rule Triggered Variational Autoencoder (DARTVAE), a rule guided multimodal
clustering framework that incorporates domain specific constraints directly
into the representation learning process. DARTVAE extends the VAE architecture
by embedding explicit rules, semantic representations, and data driven features
into a unified latent space, while enforcing constraint compliance through rule
consistency and violation penalties in the loss function. Unlike conventional
clustering methods that rely only on visual similarity or apply rules as post
hoc filters, DARTVAE treats rules as first class learning signals. The rules
are generated by LLMs, structured into knowledge graphs, and enforced through a
loss function combining reconstruction, KL divergence, consistency, and
violation penalties. Experiments on aircraft and automotive datasets
demonstrate that rule guided clustering produces more operationally meaningful
and interpretable clusters for example, isolating UAVs, unifying stealth
aircraft, or separating SUVs from sedans while improving traditional clustering
metrics. However, the framework faces challenges: LLM generated rules may
hallucinate or conflict, excessive rules risk overfitting, and scaling to
complex domains increases computational and consistency difficulties. By
combining rule encodings with learned representations, DARTVAE achieves more
meaningful and consistent clustering outcomes than purely data driven models,
highlighting the utility of constraint guided multimodal clustering for
complex, knowledge intensive settings.

</details>


### [97] [Myosotis: structured computation for attention like layer](https://arxiv.org/abs/2509.20503)
*Evgenii Egorov,Hanno Ackermann,Markus Nagel,Hong Cai*

Main category: cs.LG

TL;DR: A scalable attention mechanism combining sparsity and recurrence via efficient inversion of tree-structured matrices.


<details>
  <summary>Details</summary>
Motivation: Address the quadratic cost of attention by leveraging sparsity or recurrence; seek to blend both advantages.

Method: Introduce a novel algorithm that uses efficient inversion of tree-structured matrices to integrate sparse interactions with recurrent dependence.

Result: Abstract does not provide empirical results; claims theoretical efficiency gains through the proposed matrix-inversion approach.

Conclusion: The approach aims to fuse sparsity and recurrence in attention, potentially achieving scalable performance with preserved modeling power.

Abstract: Attention layers apply a sequence-to-sequence mapping whose parameters depend
on the pairwise interactions of the input elements. However, without any
structural assumptions, memory and compute scale quadratically with the
sequence length. The two main ways to mitigate this are to introduce sparsity
by ignoring a sufficient amount of pairwise interactions or to introduce
recurrent dependence along them, as SSM does. Although both approaches are
reasonable, they both have disadvantages. We propose a novel algorithm that
combines the advantages of both concepts. Our idea is based on the efficient
inversion of tree-structured matrices.

</details>


### [98] [Auto-Regressive U-Net for Full-Field Prediction of Shrinkage-Induced Damage in Concrete](https://arxiv.org/abs/2509.20507)
*Liya Gaynutdinova,Petr Havlásek,Ondřej Rokoš,Fleur Hendriks,Martin Doškář*

Main category: cs.LG

TL;DR: Dual-network deep learning framework with an auto-regressive U-Net for time-dependent full-field damage in concrete, coupled with a CNN to forecast shrinkage and residual stiffness from damage fields, enabling efficient predictions and insights for mix optimization.


<details>
  <summary>Details</summary>
Motivation: To enable accurate, time-resolved, full-field predictions of concrete damage with reduced computational cost, and to understand how microstructural properties influence macroscopic shrinkage and stiffness for better mix design.

Method: An auto-regressive U-Net predicts the evolution of the scalar damage field in a unit cell using microstructural geometry and the evolving shrinkage profile; the predicted damage at each step feeds the next prediction (time marching). A CNN then maps the damage estimates to mechanical properties such as observed shrinkage and residual stiffness. The approach is trained/evaluated on synthesized datasets and emphasizes computational efficiency.

Result: The dual-network architecture achieves high computational efficiency and robust predictive performance on synthetic data, reducing the computational burden of full-field damage evaluations and offering insights into how aggregate properties (shape, size, distribution) affect effective shrinkage and stiffness reductions.

Conclusion: This framework can facilitate faster optimization of concrete mixes by enabling reliable, time-resolved damage predictions and property forecasts, contributing to improved durability and reduced internal damage.

Abstract: This paper introduces a deep learning approach for predicting time-dependent
full-field damage in concrete. The study uses an auto-regressive U-Net model to
predict the evolution of the scalar damage field in a unit cell given
microstructural geometry and evolution of an imposed shrinkage profile. By
sequentially using the predicted damage output as input for subsequent
predictions, the model facilitates the continuous assessment of damage
progression. Complementarily, a convolutional neural network (CNN) utilises the
damage estimations to forecast key mechanical properties, including observed
shrinkage and residual stiffness. The proposed dual-network architecture
demonstrates high computational efficiency and robust predictive performance on
the synthesised datasets. The approach reduces the computational load
traditionally associated with full-field damage evaluations and is used to gain
insights into the relationship between aggregate properties, such as shape,
size, and distribution, and the effective shrinkage and reduction in stiffness.
Ultimately, this can help to optimize concrete mix designs, leading to improved
durability and reduced internal damage.

</details>


### [99] [Complexity-Driven Policy Optimization](https://arxiv.org/abs/2509.20509)
*Luca Serfilippi,Giorgio Franceschelli,Antonio Corradi,Mirco Musolesi*

Main category: cs.LG

TL;DR: Replacing the entropy bonus with a complexity-based regularizer (complexity = entropy × disequilibrium) in a PPO-based framework yields more robust exploration and structured stochastic policies in discrete-action tasks, improving stability to the regularization coefficient.


<details>
  <summary>Details</summary>
Motivation: Entropy bonuses push policies toward uniform randomness, risking inefficient exploration. A measure that favors high entropy but also remains structured (high disequilibrium from uniform) aims to promote useful, non-trivial behaviors.

Method: Introduce Complexity-Driven Policy Optimization (CDPO) by replacing the entropy term in PPO with a complexity regularizer (entropy × disequilibrium). Implemented and tested on discrete-action tasks, comparing robustness to the regularization coefficient against PPO with an entropy bonus.

Result: CDPO shows greater robustness to the choice of the complexity coefficient than PPO is to the entropy coefficient, particularly in tasks requiring stronger exploration. The regularizer reduces extremes (too random or too deterministic) and encourages structured yet adaptable policies, leading to improved exploration behavior in discrete-action environments.

Conclusion: A complexity-based regularizer is a viable alternative to entropy for guiding exploration in reinforcement learning. It fosters balanced stochasticity and structure, enabling non-trivial strategies to emerge. Further work could explore continuous actions, alternative complexity measures, and theoretical analyses of optimization dynamics.

Abstract: Policy gradient methods often balance exploitation and exploration via
entropy maximization. However, maximizing entropy pushes the policy towards a
uniform random distribution, which represents an unstructured and sometimes
inefficient exploration strategy. In this work, we propose replacing the
entropy bonus with a more robust complexity bonus. In particular, we adopt a
measure of complexity, defined as the product of Shannon entropy and
disequilibrium, where the latter quantifies the distance from the uniform
distribution. This regularizer encourages policies that balance stochasticity
(high entropy) with structure (high disequilibrium), guiding agents toward
regimes where useful, non-trivial behaviors can emerge. Such behaviors arise
because the regularizer suppresses both extremes, e.g., maximal disorder and
complete order, creating pressure for agents to discover structured yet
adaptable strategies. Starting from Proximal Policy Optimization (PPO), we
introduce Complexity-Driven Policy Optimization (CDPO), a new learning
algorithm that replaces entropy with complexity. We show empirically across a
range of discrete action space tasks that CDPO is more robust to the choice of
the complexity coefficient than PPO is with the entropy coefficient, especially
in environments requiring greater exploration.

</details>


### [100] [A Recovery Theory for Diffusion Priors: Deterministic Analysis of the Implicit Prior Algorithm](https://arxiv.org/abs/2509.20511)
*Oscar Leong,Yann Traonmilin*

Main category: cs.LG

TL;DR: A deterministic diffusion-prior framework yields convergence guarantees for inverse problems under low-dimensional model sets, with rates depending on the noise schedule; demonstrated for convex-set uniforms and low-rank Gaussian mixtures, including global convergence for the nonconvex Gaussian-mixture model.


<details>
  <summary>Details</summary>
Motivation: Provide rigorous, deterministic recovery guarantees for diffusion-based priors in inverse problems, connecting stochastic diffusion ideas with deterministic optimization theory.

Method: Interpret noise-convolved scores as time-varying projections onto a low-dimensional model set; view diffusion-prior algorithms as generalized projected gradient descent with evolving projections; establish convergence under a restricted isometry property (RIP) over the model set; apply framework to two model distributions (uniform over compact convex sets and low-rank Gaussian mixtures).

Result: Derived quantitative convergence rates that depend on the noise schedule when RIP over the model set holds; proved global convergence for the nonconvex low-rank Gaussian mixture model.

Conclusion: The work provides a rigorous, deterministic analysis of diffusion-prior algorithms for inverse problems, clarifying their connection to projected gradient methods and extending guarantees to nonconvex, low-dimensional models.

Abstract: Recovering high-dimensional signals from corrupted measurements is a central
challenge in inverse problems. Recent advances in generative diffusion models
have shown remarkable empirical success in providing strong data-driven priors,
but rigorous recovery guarantees remain limited. In this work, we develop a
theoretical framework for analyzing deterministic diffusion-based algorithms
for inverse problems, focusing on a deterministic version of the algorithm
proposed by Kadkhodaie \& Simoncelli \cite{kadkhodaie2021stochastic}. First, we
show that when the underlying data distribution concentrates on a
low-dimensional model set, the associated noise-convolved scores can be
interpreted as time-varying projections onto such a set. This leads to
interpreting previous algorithms using diffusion priors for inverse problems as
generalized projected gradient descent methods with varying projections. When
the sensing matrix satisfies a restricted isometry property over the model set,
we can derive quantitative convergence rates that depend explicitly on the
noise schedule. We apply our framework to two instructive data distributions:
uniform distributions over low-dimensional compact, convex sets and low-rank
Gaussian mixture models. In the latter setting, we can establish global
convergence guarantees despite the nonconvexity of the underlying model set.

</details>


### [101] [MDBench: Benchmarking Data-Driven Methods for Model Discovery](https://arxiv.org/abs/2509.20529)
*Amirmohammad Ziaei Bideh,Aleksandra Georgievska,Jonathan Gryak*

Main category: cs.LG

TL;DR: MDBench is an open-source benchmarking suite evaluating 12 model-discovery algorithms on 14 PDEs and 63 ODEs across noise levels, introducing 7 challenging PDEs; linear methods excel for PDEs, genetic programming for ODEs; robust to noise and extensible.


<details>
  <summary>Details</summary>
Motivation: Fill a gap by providing a comprehensive, extensible benchmark framework for dynamical-model discovery beyond single-equation symbolic regression.

Method: Benchmark 12 algorithms on a large set of PDEs/ODEs with varying noise; evaluate derivative prediction accuracy, model complexity, and equation fidelity; introduce seven challenging PDEs to stress current methods.

Result: Linear methods deliver lowest PDE prediction error; genetic programming yields best ODE error; linear models tend to be more noise-robust; MDBench framework and datasets enable systematic evaluation and comparison.

Conclusion: MDBench will accelerate progress by enabling rigorous, extensible benchmarking of equation discovery methods and facilitating improvements in accuracy and robustness.

Abstract: Model discovery aims to uncover governing differential equations of dynamical
systems directly from experimental data. Benchmarking such methods is essential
for tracking progress and understanding trade-offs in the field. While prior
efforts have focused mostly on identifying single equations, typically framed
as symbolic regression, there remains a lack of comprehensive benchmarks for
discovering dynamical models. To address this, we introduce MDBench, an
open-source benchmarking framework for evaluating model discovery methods on
dynamical systems. MDBench assesses 12 algorithms on 14 partial differential
equations (PDEs) and 63 ordinary differential equations (ODEs) under varying
levels of noise. Evaluation metrics include derivative prediction accuracy,
model complexity, and equation fidelity. We also introduce seven challenging
PDE systems from fluid dynamics and thermodynamics, revealing key limitations
in current methods. Our findings illustrate that linear methods and genetic
programming methods achieve the lowest prediction error for PDEs and ODEs,
respectively. Moreover, linear models are in general more robust against noise.
MDBench accelerates the advancement of model discovery methods by offering a
rigorous, extensible benchmarking framework and a rich, diverse collection of
dynamical system datasets, enabling systematic evaluation, comparison, and
improvement of equation accuracy and robustness.

</details>


### [102] [Understanding and Improving Adversarial Robustness of Neural Probabilistic Circuits](https://arxiv.org/abs/2509.20549)
*Weixin Chen,Han Zhao*

Main category: cs.LG

TL;DR: RNPC robustly defends a neural probabilistic circuit against adversarial perturbations targeting its attribute recognition module by introducing class-wise integration, with theoretical robustness guarantees and empirical gains over NPC and other concept bottleneck models while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: To address adversarial vulnerability of the attribute-recognition part of NPCs, which undermines interpretability-based models; robustness should not rely on the PC alone.

Method: The paper analyzes NPC robustness, proves that overall robustness depends on the recognition module; proposes RNPC with a novel class-wise integration for combining outputs; provides theoretical analysis showing provable robustness improvements; empirically evaluates on image classification tasks.

Result: RNPC achieves superior adversarial robustness compared to existing concept bottleneck models and NPC, while maintaining high accuracy on benign inputs.

Conclusion: RNPC provides a principled, provably more robust alternative to standard NPCs, enabling safer interpretable predictions via robust integration of perception and reasoning modules.

Abstract: Neural Probabilistic Circuits (NPCs), a new class of concept bottleneck
models, comprise an attribute recognition model and a probabilistic circuit for
reasoning. By integrating the outputs from these two modules, NPCs produce
compositional and interpretable predictions. While offering enhanced
interpretability and high performance on downstream tasks, the
neural-network-based attribute recognition model remains a black box. This
vulnerability allows adversarial attacks to manipulate attribute predictions by
introducing carefully crafted subtle perturbations to input images, potentially
compromising the final predictions. In this paper, we theoretically analyze the
adversarial robustness of NPC and demonstrate that it only depends on the
robustness of the attribute recognition model and is independent of the
robustness of the probabilistic circuit. Moreover, we propose RNPC, the first
robust neural probabilistic circuit against adversarial attacks on the
recognition module. RNPC introduces a novel class-wise integration for
inference, ensuring a robust combination of outputs from the two modules. Our
theoretical analysis demonstrates that RNPC exhibits provably improved
adversarial robustness compared to NPC. Empirical results on image
classification tasks show that RNPC achieves superior adversarial robustness
compared to existing concept bottleneck models while maintaining high accuracy
on benign inputs.

</details>


### [103] [Generalizable Diabetes Risk Stratification via Hybrid Machine Learning Models](https://arxiv.org/abs/2509.20565)
*Athar Parvez,Muhammad Jawad Mufti*

Main category: cs.LG

TL;DR: Two hybrid models for diabetes risk stratification were compared: XGBoost + Random Forest (XGB-RF) vs SVM + Logistic Regression (SVM-LR). The primary dataset shows near-perfect discrimination; external PIMA validation confirms XGB-RF generalizes better with strong discrimination and calibration, while SVM-LR lags. Threshold-based metrics favor XGB-RF. The study advocates gradient-boosting-based hybrids and multi-site prospective validation with threshold choice tailored to clinical trade-offs.


<details>
  <summary>Details</summary>
Motivation: Diabetes imposes a large global burden; early risk stratification via machine learning can improve preventive care. The study aims to compare the effectiveness and generalizability of two hybrid classifiers and assess their stability on an external cohort.

Method: Two hybrids were built: (i) XGBoost + Random Forest (XGB-RF) and (ii) Support Vector Machine + Logistic Regression (SVM-LR). A leakage-safe, standardized pipeline (encoding, imputation, min-max scaling; SMOTE on training folds only; probability calibration for SVM) was fit on the primary dataset and frozen. Evaluation prioritized threshold-independent discrimination (AUROC/AUPRC) and calibration (Brier, slope/intercept). External validation used the PIMA cohort (N=768) with the frozen pipeline; any thresholded metrics on PIMA were computed at the default rule tau = 0.5.

Result: On the primary dataset (PR baseline = 0.50), XGB-RF achieved AUROC ~0.995 and AUPRC ~0.998, outperforming SVM-LR (AUROC ~0.978; AUPRC ~0.947). On PIMA (PR baseline ~0.349), XGB-RF retained strong performance (AUROC ~0.990; AUPRC ~0.959); SVM-LR was lower (AUROC ~0.963; AUPRC ~0.875). Thresholded metrics on PIMA at tau = 0.5 were XGB-RF (Accuracy 0.960; Precision 0.941; Recall 0.944; F1 0.942) and SVM-LR (Accuracy 0.900; Precision 0.855; Recall 0.858; F1 0.857).

Conclusion: Across internal and external cohorts, XGB-RF consistently dominated SVM-LR and exhibited smaller external attenuation on ROC/PR with acceptable calibration. These results support gradient-boosting-based hybridization as a robust, transferable approach for diabetes risk stratification and motivate prospective, multi-site validation with deployment-time threshold selection based on clinical trade-offs.

Abstract: Background/Purpose: Diabetes affects over 537 million people worldwide and is
projected to reach 783 million by 2045. Early risk stratification can benefit
from machine learning. We compare two hybrid classifiers and assess their
generalizability on an external cohort.
  Methods: Two hybrids were built: (i) XGBoost + Random Forest (XGB-RF) and
(ii) Support Vector Machine + Logistic Regression (SVM-LR). A leakage-safe,
standardized pipeline (encoding, imputation, min-max scaling; SMOTE on training
folds only; probability calibration for SVM) was fit on the primary dataset and
frozen. Evaluation prioritized threshold-independent discrimination
(AUROC/AUPRC) and calibration (Brier, slope/intercept). External validation
used the PIMA cohort (N=768) with the frozen pipeline; any thresholded metrics
on PIMA were computed at the default rule tau = 0.5.
  Results: On the primary dataset (PR baseline = 0.50), XGB-RF achieved AUROC
~0.995 and AUPRC ~0.998, outperforming SVM-LR (AUROC ~0.978; AUPRC ~0.947). On
PIMA (PR baseline ~0.349), XGB-RF retained strong performance (AUROC ~0.990;
AUPRC ~0.959); SVM-LR was lower (AUROC ~0.963; AUPRC ~0.875). Thresholded
metrics on PIMA at tau = 0.5 were XGB-RF (Accuracy 0.960; Precision 0.941;
Recall 0.944; F1 0.942) and SVM-LR (Accuracy 0.900; Precision 0.855; Recall
0.858; F1 0.857).
  Conclusions: Across internal and external cohorts, XGB-RF consistently
dominated SVM-LR and exhibited smaller external attenuation on ROC/PR with
acceptable calibration. These results support gradient-boosting-based
hybridization as a robust, transferable approach for diabetes risk
stratification and motivate prospective, multi-site validation with
deployment-time threshold selection based on clinical trade-offs.

</details>


### [104] [PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models](https://arxiv.org/abs/2509.20570)
*Mingze Yuan,Pengfei Jin,Na Li,Quanzheng Li*

Main category: cs.LG

TL;DR: PIRF bypasses diffusion-value approximations by trajectory-level reward fine-tuning, using layer-wise truncated backprop and weight regularization to enforce physics efficiently across five PDE benchmarks.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based generators often violate physical laws; current DPS-style value function approximations introduce errors and instability. A reward-based, physics-informed generation framework could improve fidelity and efficiency.

Method: Frame physics constraints as sparse rewards in a reward-optimization setup. PIRF computes trajectory-level rewards and backpropagates gradients directly, avoiding value-function approximations. It uses (1) layer-wise truncated backpropagation to exploit spatiotemporal locality of rewards, and (2) weight-based regularization to improve efficiency over distillation methods.

Result: PIRF achieves superior physical enforcement across five PDE benchmarks under efficient sampling, with improved stability and sampling efficiency by avoiding DPS-style value function errors.

Conclusion: Reward fine-tuning is a promising direction for physics-informed generative modeling, unifying prior approaches under a reward-based paradigm and mitigating key bottlenecks related to value-function approximations.

Abstract: Diffusion models have demonstrated strong generative capabilities across
scientific domains, but often produce outputs that violate physical laws. We
propose a new perspective by framing physics-informed generation as a sparse
reward optimization problem, where adherence to physical constraints is treated
as a reward signal. This formulation unifies prior approaches under a
reward-based paradigm and reveals a shared bottleneck: reliance on diffusion
posterior sampling (DPS)-style value function approximations, which introduce
non-negligible errors and lead to training instability and inference
inefficiency. To overcome this, we introduce Physics-Informed Reward
Fine-tuning (PIRF), a method that bypasses value approximation by computing
trajectory-level rewards and backpropagating their gradients directly. However,
a naive implementation suffers from low sample efficiency and compromised data
fidelity. PIRF mitigates these issues through two key strategies: (1) a
layer-wise truncated backpropagation method that leverages the spatiotemporally
localized nature of physics-based rewards, and (2) a weight-based
regularization scheme that improves efficiency over traditional
distillation-based methods. Across five PDE benchmarks, PIRF consistently
achieves superior physical enforcement under efficient sampling regimes,
highlighting the potential of reward fine-tuning for advancing scientific
generative modeling.

</details>


### [105] [The Sensitivity of Variational Bayesian Neural Network Performance to Hyperparameters](https://arxiv.org/abs/2509.20574)
*Scott Koermer,Natalie Klein*

Main category: cs.LG

TL;DR: Global sensitivity analysis shows hyperparameters in Bayesian neural networks interact and significantly affect both predictive accuracy and uncertainty quantification; use sensitivity analysis or Bayesian optimization to select hyperparameters for reliable UQ.


<details>
  <summary>Details</summary>
Motivation: Uncertainty quantification in Bayesian neural networks is challenging due to approximations and the large, opaque hyperparameter space; understanding their interactions is needed to ensure reliable UQ in real-world applications.

Method: Conduct a global sensitivity analysis across a broad hyperparameter space to assess impacts on predictive performance and uncertainty quantification, examining interactions among hyperparameters.

Result: Hyperparameters interact in influencing both predictive accuracy and UQ; global sensitivity analysis can guide dimensionality reduction and hyperparameter selection; Bayesian optimization is suggested as a practical tool.

Conclusion: Adopt global sensitivity analysis or Bayesian optimization to select hyperparameters, enabling accurate UQ in BNNs for real-world applications.

Abstract: In scientific applications, predictive modeling is often of limited use
without accurate uncertainty quantification (UQ) to indicate when a model may
be extrapolating or when more data needs to be collected. Bayesian Neural
Networks (BNNs) produce predictive uncertainty by propagating uncertainty in
neural network (NN) weights and offer the promise of obtaining not only an
accurate predictive model but also accurate UQ. However, in practice, obtaining
accurate UQ with BNNs is difficult due in part to the approximations used for
practical model training and in part to the need to choose a suitable set of
hyperparameters; these hyperparameters outnumber those needed for traditional
NNs and often have opaque effects on the results. We aim to shed light on the
effects of hyperparameter choices for BNNs by performing a global sensitivity
analysis of BNN performance under varying hyperparameter settings. Our results
indicate that many of the hyperparameters interact with each other to affect
both predictive accuracy and UQ. For improved usage of BNNs in real-world
applications, we suggest that global sensitivity analysis, or related methods
such as Bayesian optimization, should be used to aid in dimensionality
reduction and selection of hyperparameters to ensure accurate UQ in BNNs.

</details>


### [106] [Learning Greens Operators through Hierarchical Neural Networks Inspired by the Fast Multipole Method](https://arxiv.org/abs/2509.20591)
*Emilio McAllister Fognini,Marta M. Betcke,Ben T. Cox*

Main category: cs.LG

TL;DR: Neural FMM is a hierarchical ML architecture that incorporates the Fast Multipole Method flow to learn the Green's operator of an elliptic PDE by separating local and far-field interactions.


<details>
  <summary>Details</summary>
Motivation: The Fast Multipole Method (FMM) is a powerful technique for long-range interactions in N-body problems, but its integration with modern machine learning is underexplored. The authors aim to combine FMM's hierarchical structure with neural learning to efficiently model the Green's operator for elliptic PDEs.

Method: Introduce Neural FMM, a neural network architecture that mirrors the hierarchical computation of FMM to split local and far-field interactions and learn their respective representations, enabling learning of the Green's operator for elliptic PDEs.

Result: The abstract presents a framework and architectural proposal but does not report empirical results or quantitative evaluations.

Conclusion: By embedding the FMM information flow into a hierarchical ML framework, the Neural FMM provides a principled approach to learning Green's operators for elliptic PDEs, potentially improving scalability through separation of local and far-field contributions.

Abstract: The Fast Multipole Method (FMM) is an efficient numerical algorithm for
computation of long-ranged forces in $N$-body problems within gravitational and
electrostatic fields. This method utilizes multipole expansions of the Green's
function inherent to the underlying dynamical systems. Despite its widespread
application in physics and engineering, the integration of FMM with modern
machine learning architectures remains underexplored. In this work, we propose
a novel neural network architecture, the Neural FMM, that integrates the
information flow of the FMM into a hierarchical machine learning framework for
learning the Green's operator of an Elliptic PDE. Our Neural FMM architecture
leverages a hierarchical computation flow of the FMM method to split up the
local and far-field interactions and efficiently learn their respective
representations.

</details>


### [107] [TSKAN: Interpretable Machine Learning for QoE modeling over Time Series Data](https://arxiv.org/abs/2509.20595)
*Kamal Singh,Priyanka Rawat,Sami Marouani,Baptiste Jeudy*

Main category: cs.LG

TL;DR: A novel interpretable QoE model for video streaming using Kolmogorov-Arnold Networks (KANs) on compact frequency-domain features to predict QoE with improved accuracy and transparency.


<details>
  <summary>Details</summary>
Motivation: QoE modeling is crucial for optimizing video streaming but most ML approaches are black-box. There is a need for models that capture temporal dynamics while remaining interpretable.

Method: Combine compact frequency-domain features with Kolmogorov-Arnold Networks (KANs) as an interpretable readout on top of the features to preserve temporal information and provide explainability.

Result: Reported enhanced accuracy in QoE prediction compared to baselines, along with transparent and interpretable decision-making thanks to KANs.

Conclusion: The approach achieves a balance between predictive performance and interpretability and demonstrates the viability of interpretable ML for QoE in video streaming.

Abstract: Quality of Experience (QoE) modeling is crucial for optimizing video
streaming services to capture the complex relationships between different
features and user experience. We propose a novel approach to QoE modeling in
video streaming applications using interpretable Machine Learning (ML)
techniques over raw time series data. Unlike traditional black-box approaches,
our method combines Kolmogorov-Arnold Networks (KANs) as an interpretable
readout on top of compact frequency-domain features, allowing us to capture
temporal information while retaining a transparent and explainable model. We
evaluate our method on popular datasets and demonstrate its enhanced accuracy
in QoE prediction, while offering transparency and interpretability.

</details>


### [108] [Explicit and Effectively Symmetric Schemes for Neural SDEs](https://arxiv.org/abs/2509.20599)
*Daniil Shmelev,Cristopher Salvi*

Main category: cs.LG

TL;DR: A novel Explicit and Effectively Symmetric (EES) Runge-Kutta scheme for neural SDEs achieves memory-efficient training with stable, near-reversible integration, addressing instability and memory drawbacks of previous reversible methods.


<details>
  <summary>Details</summary>
Motivation: Backprop through neural SDEs faces two common strategies with trade-offs: discretise-then-optimise is accurate but memory-heavy; optimise-then-discretise is memory-efficient but slower and gradient-approximation-prone. Reversible solvers promise memory and gradient benefits but are often unstable on complex models or large steps. A stable, memory-efficient solution is needed.

Method: Introduce EES schemes— a class of explicit near-reversible Runge-Kutta integrators for neural SDEs—designed to preserve reversibility advantages while mitigating instability, enabling memory-efficient training without strict step-size/model constraints.

Result: Numerical experiments show enhanced stability and reliability of EES schemes, outperforming existing reversible methods, and establishing a practical basis for scalable, accurate neural SDE training.

Conclusion: EES Runge-Kutta schemes provide memory-efficient, accurate gradients for neural SDEs, overcoming limitations of previous reversible solvers, and offering a solid foundation for scalable neural SDE training.

Abstract: Backpropagation through (neural) SDE solvers is traditionally approached in
two ways: discretise-then-optimise, which offers accurate gradients but incurs
prohibitive memory costs due to storing the full computational graph (even when
mitigated by checkpointing); and optimise-then-discretise, which achieves
constant memory cost by solving an auxiliary backward SDE, but suffers from
slower evaluation and gradient approximation errors. Algebraically reversible
solvers promise both memory efficiency and gradient accuracy, yet existing
methods such as the Reversible Heun scheme are often unstable under complex
models and large step sizes. We address these limitations by introducing a
novel class of stable, near-reversible Runge--Kutta schemes for neural SDEs.
These Explicit and Effectively Symmetric (EES) schemes retain the benefits of
reversible solvers while overcoming their instability, enabling
memory-efficient training without severe restrictions on step size or model
complexity. Through numerical experiments, we demonstrate the superior
stability and reliability of our schemes, establishing them as a practical
foundation for scalable and accurate training of neural SDEs.

</details>


### [109] [Function Spaces Without Kernels: Learning Compact Hilbert Space Representations](https://arxiv.org/abs/2509.20605)
*Su Ann Low,Quentin Rommel,Kevin S. Miller,Adam J. Thorpe,Ufuk Topcu*

Main category: cs.LG

TL;DR: Introduces function encoders tying neural basis learning to kernel methods; proposes progressive and train-then-prune training to learn compact bases, derives generalization bounds, and validates efficiency with dynamical systems benchmarks.


<details>
  <summary>Details</summary>
Motivation: To achieve compact, adaptive representations with principled guarantees and kernel-like scalability, bridging neural feature learning with kernel theory.

Method: Define kernel via inner product of learned features; two training schemes (progressive basis growth; train-then-prune); PCA-inspired dimension estimation; derive Rademacher and PAC-Bayes generalization bounds.

Result: Kernel-theoretic view explains scalability and adaptability; same accuracy with far fewer bases; validated on polynomial benchmark with known intrinsic dimension and on nonlinear dynamics (Van der Pol oscillator, two-body orbital model).

Conclusion: Paths toward neural predictors with kernel-level guarantees, enabling efficient and principled models at scale.

Abstract: Function encoders are a recent technique that learn neural network basis
functions to form compact, adaptive representations of Hilbert spaces of
functions. We show that function encoders provide a principled connection to
feature learning and kernel methods by defining a kernel through an inner
product of the learned feature map. This kernel-theoretic perspective explains
their ability to scale independently of dataset size while adapting to the
intrinsic structure of data, and it enables kernel-style analysis of neural
models. Building on this foundation, we develop two training algorithms that
learn compact bases: a progressive training approach that constructively grows
bases, and a train-then-prune approach that offers a computationally efficient
alternative after training. Both approaches use principles from PCA to reveal
the intrinsic dimension of the learned space. In parallel, we derive
finite-sample generalization bounds using Rademacher complexity and PAC-Bayes
techniques, providing inference time guarantees. We validate our approach on a
polynomial benchmark with a known intrinsic dimension, and on nonlinear
dynamical systems including a Van der Pol oscillator and a two-body orbital
model, demonstrating that the same accuracy can be achieved with substantially
fewer basis functions. This work suggests a path toward neural predictors with
kernel-level guarantees, enabling adaptable models that are both efficient and
principled at scale.

</details>


### [110] [MMG: Mutual Information Estimation via the MMSE Gap in Diffusion](https://arxiv.org/abs/2509.20609)
*Longxuan Yu,Xing Shi,Xianghao Kong,Tong Jia,Greg Ver Steeg*

Main category: cs.LG

TL;DR: A diffusion-model-based method estimates mutual information by relating MI to the MMSE gap between conditional and unconditional diffusion, integrated over SNRs; with adaptive importance sampling, it scales and outperforms existing estimators.


<details>
  <summary>Details</summary>
Motivation: Mutual information is a fundamental yet challenging measure of dependence in complex systems; recent diffusion-based density estimators offer a natural route to improve MI estimation by reframing it in a denoising diffusion framework.

Method: Utilizes the information-theoretic formulation of denoising diffusion models: MI equals half the gap in MMSE between conditional and unconditional diffusion across all SNRs. Estimation is made scalable via adaptive importance sampling.

Result: The proposed method passes self-consistency checks and outperforms traditional MI estimators and score-based diffusion MI estimators, maintaining strong performance even when MI is high.

Conclusion: Diffusion-model-based MI estimation is effective, scalable, and robust for measuring dependencies in complex systems.

Abstract: Mutual information (MI) is one of the most general ways to measure
relationships between random variables, but estimating this quantity for
complex systems is challenging. Denoising diffusion models have recently set a
new bar for density estimation, so it is natural to consider whether these
methods could also be used to improve MI estimation. Using the recently
introduced information-theoretic formulation of denoising diffusion models, we
show the diffusion models can be used in a straightforward way to estimate MI.
In particular, the MI corresponds to half the gap in the Minimum Mean Square
Error (MMSE) between conditional and unconditional diffusion, integrated over
all Signal-to-Noise-Ratios (SNRs) in the noising process. Our approach not only
passes self-consistency tests but also outperforms traditional and score-based
diffusion MI estimators. Furthermore, our method leverages adaptive importance
sampling to achieve scalable MI estimation, while maintaining strong
performance even when the MI is high.

</details>


### [111] [Policy Compatible Skill Incremental Learning via Lazy Learning Interface](https://arxiv.org/abs/2509.20612)
*Daehee Lee,Dongsu Lee,TaeYoon Kwack,Wonje Choi,Honguk Woo*

Main category: cs.LG

TL;DR: SIL-C preserves compatibility between evolving skills and downstream policies in Skill Incremental Learning by a bilateral lazy learning-based mapping that aligns the policy's subtask space with the skill space, enabling task execution via appropriate skills without policy retraining.


<details>
  <summary>Details</summary>
Motivation: Skill repertoires expand in SIL, but evolving skills can break compatibility with existing policies, hurting reusability and generalization; a mechanism to maintain compatibility during incremental skill growth is needed.

Method: Introduce SIL-C with a bilateral lazy learning-based mapping that dynamically aligns the policy-derived subtask space with the decoded skill space. Subtasks from policy decomposition are matched to skills by trajectory distribution similarity, enabling correct skill selection per subtask without retraining or structural changes.

Result: Empirical evaluation across diverse SIL scenarios shows SIL-C maintains compatibility between evolving skills and downstream policies while preserving learning efficiency and enabling improved downstream performance without policy retraining.

Conclusion: SIL-C presents a compatibility-preserving approach for SIL, allowing incremental skill improvements to benefit downstream policies without requiring policy retraining or architecture changes.

Abstract: Skill Incremental Learning (SIL) is the process by which an embodied agent
expands and refines its skill set over time by leveraging experience gained
through interaction with its environment or by the integration of additional
data. SIL facilitates efficient acquisition of hierarchical policies grounded
in reusable skills for downstream tasks. However, as the skill repertoire
evolves, it can disrupt compatibility with existing skill-based policies,
limiting their reusability and generalization. In this work, we propose SIL-C,
a novel framework that ensures skill-policy compatibility, allowing
improvements in incrementally learned skills to enhance the performance of
downstream policies without requiring policy re-training or structural
adaptation. SIL-C employs a bilateral lazy learning-based mapping technique to
dynamically align the subtask space referenced by policies with the skill space
decoded into agent behaviors. This enables each subtask, derived from the
policy's decomposition of a complex task, to be executed by selecting an
appropriate skill based on trajectory distribution similarity. We evaluate
SIL-C across diverse SIL scenarios and demonstrate that it maintains
compatibility between evolving skills and downstream policies while ensuring
efficiency throughout the learning process.

</details>


### [112] [Latent Twins](https://arxiv.org/abs/2509.20615)
*Matthias Chung,Deepanshu Verma,Max Collins,Amit N. Subrahmanya,Varuni Katti Sastry,Vishwas Rao*

Main category: cs.LG

TL;DR: Latent Twins: a unified latent-space surrogate framework for mathematical systems that bridges modeling, inversion, model reduction, and operator approximation; validated on ODEs, PDEs (shallow-water), and real geopotential data; enables single-shot long-horizon predictions and compatibility with assimilation and UQ.


<details>
  <summary>Details</summary>
Motivation: There is a persistent gap between data-driven representation learning and traditional scientific modeling; a compact, interpretable latent surrogate of governing equations could unify solution methods and enable cross-domain tasks.

Method: Introduce Latent Twins: a hidden latent space governed by operators that surrogate the underlying equations; establish approximation properties for ODEs and PDEs; evaluate on three settings: canonical ODEs, a shallow-water PDE benchmark (compared to DeepONet and 4D-Var baselines), and a real geopotential reanalysis dataset for reconstruction and forecasting from sparse, noisy observations.

Result: Latent Twins provide a compact, interpretable surrogate for solution operators that can evaluate across arbitrary time gaps in a single shot; compatible with assimilation, control, and uncertainty quantification; demonstrated theoretical approximation properties and empirical performance across the three settings.

Conclusion: Offers scalable, theory-grounded surrogates that bridge data-driven representation learning and classical scientific modeling across disciplines, enabling unified treatment of modeling, inversion, model reduction, and operator approximation.

Abstract: Over the past decade, scientific machine learning has transformed the
development of mathematical and computational frameworks for analyzing,
modeling, and predicting complex systems. From inverse problems to numerical
PDEs, dynamical systems, and model reduction, these advances have pushed the
boundaries of what can be simulated. Yet they have often progressed in
parallel, with representation learning and algorithmic solution methods
evolving largely as separate pipelines. With \emph{Latent Twins}, we propose a
unifying mathematical framework that creates a hidden surrogate in latent space
for the underlying equations. Whereas digital twins mirror physical systems in
the digital world, Latent Twins mirror mathematical systems in a learned latent
space governed by operators. Through this lens, classical modeling, inversion,
model reduction, and operator approximation all emerge as special cases of a
single principle. We establish the fundamental approximation properties of
Latent Twins for both ODEs and PDEs and demonstrate the framework across three
representative settings: (i) canonical ODEs, capturing diverse dynamical
regimes; (ii) a PDE benchmark using the shallow-water equations, contrasting
Latent Twin simulations with DeepONet and forecasts with a 4D-Var baseline; and
(iii) a challenging real-data geopotential reanalysis dataset, reconstructing
and forecasting from sparse, noisy observations. Latent Twins provide a
compact, interpretable surrogate for solution operators that evaluate across
arbitrary time gaps in a single-shot, while remaining compatible with
scientific pipelines such as assimilation, control, and uncertainty
quantification. Looking forward, this framework offers scalable,
theory-grounded surrogates that bridge data-driven representation learning and
classical scientific modeling across disciplines.

</details>


### [113] [Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning](https://arxiv.org/abs/2509.20616)
*Hanjiang Hu,Changliu Liu,Na Li,Yebin Wang*

Main category: cs.LG

TL;DR: Transforms multi-turn task planning into single-turn reasoning using Group Relative Policy Optimization (GRPO) with dense rewards from expert trajectories, enabling a 1.5B model to outperform larger baselines on long-horizon tasks and generalize to subtasks.


<details>
  <summary>Details</summary>
Motivation: Addresses sparse episode rewards, credit assignment over long horizons, and RL overhead in multi-turn LLM planning by reframing planning as single-turn reasoning and leveraging dense expert-derived rewards for efficient policy optimization.

Method: Formulates multi-turn planning as a single-turn reasoning problem and optimizes policies via Group Relative Policy Optimization (GRPO) using dense, verifiable rewards from expert trajectories; provides theoretical guarantees linking single-turn GRPO gains to higher multi-turn success and horizon generalization; trains a 1.5B parameter model on complex tasks.

Result: Empirical evaluation on a complex task planning benchmark shows the 1.5B model trained with GRPO outperforms larger baselines (up to 14B) with a 70% success rate on long-horizon tasks (>30 steps); theoretical and empirical evidence support strong cross-task generalizability to simpler subtasks.

Conclusion: Single-turn reasoning with GRPO is an effective approach for training LLM agents on complex, long-horizon planning, yielding high performance with smaller models and showing robust generalization across subtasks.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
knowledge acquisition, reasoning, and tool use, making them promising
candidates for autonomous agent applications. However, training LLM agents for
complex multi-turn task planning faces significant challenges, including sparse
episode-wise rewards, credit assignment across long horizons, and the
computational overhead of reinforcement learning in multi-turn interaction
settings. To this end, this paper introduces a novel approach that transforms
multi-turn task planning into single-turn task reasoning problems, enabling
efficient policy optimization through Group Relative Policy Optimization (GRPO)
with dense and verifiable reward from expert trajectories. Our theoretical
analysis shows that GRPO improvement on single-turn task reasoning results in
higher multi-turn success probability under the minimal turns, as well as the
generalization to subtasks with shorter horizons. Experimental evaluation on
the complex task planning benchmark demonstrates that our 1.5B parameter model
trained with single-turn GRPO achieves superior performance compared to larger
baseline models up to 14B parameters, with success rates of 70% for
long-horizon planning tasks with over 30 steps. We also theoretically and
empirically validate the strong cross-task generalizability that the models
trained on complex tasks can lead to the successful completion of all simpler
subtasks.

</details>


### [114] [Personalized Federated Dictionary Learning for Modeling Heterogeneity in Multi-site fMRI Data](https://arxiv.org/abs/2509.20627)
*Yipu Zhang,Chengshuo Zhang,Ziyu Zhou,Gang Qu,Hao Zheng,Yuping Wang,Hui Shen,Hongwen Deng*

Main category: cs.LG

TL;DR: A privacy-preserving, federated dictionary learning approach for multi-site fMRI that decouples shared global and site-specific local components to improve non-IID robustness.


<details>
  <summary>Details</summary>
Motivation: Data privacy constraints and site heterogeneity lead to non-IID data in multi-site fMRI, hindering generalizable models; there is a need for collaborative learning without sharing raw data.

Method: PFedDL learns dictionary representations independently at each site, decomposing each site's dictionary into shared global atoms and personalized local atoms. Global atoms are updated via federated aggregation to enforce cross-site consistency, while local atoms are refined locally to capture site-specific variability.

Result: On ABIDE dataset, PFedDL outperforms existing methods in accuracy and robustness across non-IID datasets.

Conclusion: PFedDL enables privacy-preserving cross-site collaboration for neuroimaging and improves generalization of models in non-IID settings.

Abstract: Data privacy constraints pose significant challenges for large-scale
neuroimaging analysis, especially in multi-site functional magnetic resonance
imaging (fMRI) studies, where site-specific heterogeneity leads to
non-independent and identically distributed (non-IID) data. These factors
hinder the development of generalizable models. To address these challenges, we
propose Personalized Federated Dictionary Learning (PFedDL), a novel federated
learning framework that enables collaborative modeling across sites without
sharing raw data. PFedDL performs independent dictionary learning at each site,
decomposing each site-specific dictionary into a shared global component and a
personalized local component. The global atoms are updated via federated
aggregation to promote cross-site consistency, while the local atoms are
refined independently to capture site-specific variability, thereby enhancing
downstream analysis. Experiments on the ABIDE dataset demonstrate that PFedDL
outperforms existing methods in accuracy and robustness across non-IID
datasets.

</details>


### [115] [Investigating Modality Contribution in Audio LLMs for Music](https://arxiv.org/abs/2509.20641)
*Giovana Morais,Magdalena Fuentes*

Main category: cs.LG

TL;DR: A study applies MM-SHAP to Audio LLMs to quantify how much audio vs text contribute to predictions on MuChoMusic; finds higher accuracy models rely more on text but can still localize key audio events, marking the first MM-SHAP application to Audio LLMs and advancing explainable AI in audio.


<details>
  <summary>Details</summary>
Motivation: To address whether Audio LLMs truly listen to audio or rely on textual reasoning, the paper aims to quantify modality-specific contributions to model outputs and improve explainability in audio-based AI systems.

Method: Adapt MM-SHAP, a Shapley-based, performance-agnostic attribution score, to measure relative contribution of each modality (audio vs text) to an Audio LLM's prediction. Evaluate two models on the MuChoMusic benchmark, analyze overall modality contributions and the model's ability to localize key sound events despite varying audio contribution.

Result: The higher-accuracy model relies more on textual input for answering questions, yet audio contribution remains present enough to enable localization of key sound events. MM-SHAP is successfully applied to Audio LLMs, illustrating nuanced modality interplay and laying groundwork for explainable AI in audio.

Conclusion: The work demonstrates the feasibility of using MM-SHAP with Audio LLMs and provides foundational insights into how audio and text contribute to model outputs, encouraging further research into robust modality attribution and explainability in audio-focused AI.

Abstract: Audio Large Language Models (Audio LLMs) enable human-like conversation about
music, yet it is unclear if they are truly listening to the audio or just using
textual reasoning, as recent benchmarks suggest. This paper investigates this
issue by quantifying the contribution of each modality to a model's output. We
adapt the MM-SHAP framework, a performance-agnostic score based on Shapley
values that quantifies the relative contribution of each modality to a model's
prediction. We evaluate two models on the MuChoMusic benchmark and find that
the model with higher accuracy relies more on text to answer questions, but
further inspection shows that even if the overall audio contribution is low,
models can successfully localize key sound events, suggesting that audio is not
entirely ignored. Our study is the first application of MM-SHAP to Audio LLMs
and we hope it will serve as a foundational step for future research in
explainable AI and audio.

</details>


### [116] [Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration](https://arxiv.org/abs/2509.20648)
*Yiyuan Pan,Zhe Liu,Hesheng Wang*

Main category: cs.LG

TL;DR: CERMIC introduces principled, context-aware intrinsic motivation to MARL by filtering noisy surprises and calibrating curiosity with inferred multi-agent context, grounded in information gain, yielding superior exploration in sparse rewards across VMAS, Meltingpot, and SMACv2.


<details>
  <summary>Details</summary>
Motivation: The paper targets exploration challenges in combinatorial, decentralized MARL with sparse rewards. Traditional curiosity conflates stochasticity with novelty and treats all surprises equally, neglecting peer dynamics. Inspired by human children adapting exploration via observing peers, it seeks to leverage multi-agent context to drive more informative exploration.

Method: Proposes CERMIC, a framework that (i) filters noisy surprise signals, (ii) calibrates intrinsic curiosity using inferred multi-agent context (peer behavior dynamics), and (iii) provides theoretically grounded intrinsic rewards that reward information gain in state transitions. Evaluation is performed on VMAS, Meltingpot, and SMACv2 benchmarks.

Result: Empirical results show that exploration driven by CERMIC significantly outperforms state-of-the-art methods in sparse-reward MARL environments across the tested benchmark suites.

Conclusion: CERMIC provides robust, context-aware intrinsic motivation for decentralized MARL, enabling more efficient exploration under sparse rewards with principled, information-theoretic rewards and effective noise filtering.

Abstract: Autonomous exploration in complex multi-agent reinforcement learning (MARL)
with sparse rewards critically depends on providing agents with effective
intrinsic motivation. While artificial curiosity offers a powerful
self-supervised signal, it often confuses environmental stochasticity with
meaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform
novelty bias, treating all unexpected observations equally. However, peer
behavior novelty, which encode latent task dynamics, are often overlooked,
resulting in suboptimal exploration in decentralized, communication-free MARL
settings. To this end, inspired by how human children adaptively calibrate
their own exploratory behaviors via observing peers, we propose a novel
approach to enhance multi-agent exploration. We introduce CERMIC, a principled
framework that empowers agents to robustly filter noisy surprise signals and
guide exploration by dynamically calibrating their intrinsic curiosity with
inferred multi-agent context. Additionally, CERMIC generates
theoretically-grounded intrinsic rewards, encouraging agents to explore state
transitions with high information gain. We evaluate CERMIC on benchmark suites
including VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that
exploration with CERMIC significantly outperforms SoTA algorithms in
sparse-reward environments.

</details>


### [117] [Guiding Application Users via Estimation of Computational Resources for Massively Parallel Chemistry Computations](https://arxiv.org/abs/2509.20667)
*Tanzila Tabassum,Omer Subasi,Ajay Panyala,Epiya Ebiapia,Gerald Baumgartner,Erdal Mutlu,P.,Sadayappan,Karol Kowalski*

Main category: cs.LG

TL;DR: ML-based prediction of CCSD runtime resources on Frontier/Aurora to guide parameter choices and reduce costs. Optimizes for shortest time and cheapest run by predicting execution time versus nodes and tile sizes. Evaluates GB models with MAPE 0.023 (Aurora) and 0.073 (Frontier); active learning achieves ~0.2 MAPE with ~450 experiments.


<details>
  <summary>Details</summary>
Motivation: Assist users in HPC planning by accurately predicting resource usage and execution time for expensive chemistry computations, enabling better choice of runtime parameters before execution.

Method: Train and compare multiple ML models (including Gradient Boosting) to predict total CCSD iteration time as a function of problem size, number of nodes, and tile sizes. Use these predictions to identify configurations that minimize time or node-hours. Evaluate on CCSD workloads from DOE Frontier and Aurora. Apply active learning to reduce data requirements.

Result: Gradient Boosting achieves MAPE of 0.023 on Aurora and 0.073 on Frontier for CCSD iteration time. Active learning can reach MAPE ≈0.2 with around 450 experiments.

Conclusion: ML-driven resource prediction can effectively guide HPC job parameter choices, reducing time-to-solution and resource costs for expensive chemistry computations; active learning is a practical strategy when experimental data is expensive to obtain.

Abstract: In this work, we develop machine learning (ML) based strategies to predict
resources (costs) required for massively parallel chemistry computations, such
as coupled-cluster methods, to guide application users before they commit to
running expensive experiments on a supercomputer. By predicting application
execution time, we determine the optimal runtime parameter values such as
number of nodes and tile sizes. Two key questions of interest to users are
addressed. The first is the shortest-time question, where the user is
interested in knowing the parameter configurations (number of nodes and tile
sizes) to achieve the shortest execution time for a given problem size and a
target supercomputer. The second is the cheapest-run question in which the user
is interested in minimizing resource usage, i.e., finding the number of nodes
and tile size that minimizes the number of node-hours for a given problem size.
  We evaluate a rich family of ML models and strategies, developed based on the
collections of runtime parameter values for the CCSD (Coupled Cluster with
Singles and Doubles) application executed on the Department of Energy (DOE)
Frontier and Aurora supercomputers. Our experiments show that when predicting
the total execution time of a CCSD iteration, a Gradient Boosting (GB) ML model
achieves a Mean Absolute Percentage Error (MAPE) of 0.023 and 0.073 for Aurora
and Frontier, respectively. In the case where it is expensive to run
experiments just to collect data points, we show that active learning can
achieve a MAPE of about 0.2 with just around 450 experiments collected from
Aurora and Frontier.

</details>


### [118] [Theoretical Bounds for Stable In-Context Learning](https://arxiv.org/abs/2509.20677)
*Tongxi Wang,Zhuoyang Xia*

Main category: cs.LG

TL;DR: Non-asymptotic lower bound ties minimum demonstrations to ICL stability under high-dimensional sub-Gaussian representations; provides computable spectral criterion; proposes a two-stage estimator with one-shot calibration; empirical results show alignment between predicted knee-points and thresholds, with theory serving as a conservative upper bound and calibration tightening the gap.


<details>
  <summary>Details</summary>
Motivation: ICL is flexible but its reliability is highly sensitive to prompt length. There is a need for a practical, distribution-free criterion to determine the minimal number of demonstrations in finite-sample, high-dimensional settings.

Method: Derives a non-asymptotic lower bound on the required demonstrations based on spectral properties (covariance). Proposes a two-stage observable estimator with one-shot calibration to estimate prompt-length thresholds without distributional priors.

Result: The bound yields explicit sufficient conditions; experiments across datasets/encoders/generators show predicted thresholds align with empirical knee-points; the theory acts as a conservative upper bound; calibrated variant tightens the gap.

Conclusion: Links spectral coverage to stable ICL; provides theory-practice bridge for prompt engineering; enhances interpretability and reliability of large-scale prompting in finite-sample regimes.

Abstract: In-context learning (ICL) is flexible but its reliability is highly sensitive
to prompt length. This paper establishes a non-asymptotic lower bound that
links the minimal number of demonstrations to ICL stability under fixed
high-dimensional sub-Gaussian representations. The bound gives explicit
sufficient conditions in terms of spectral properties of the covariance,
providing a computable criterion for practice. Building on this analysis, we
propose a two-stage observable estimator with a one-shot calibration that
produces practitioner-ready prompt-length estimates without distributional
priors. Experiments across diverse datasets, encoders, and generators show
close alignment between the predicted thresholds and empirical knee-points,
with the theory acting as a conservative but reliable upper bound; the
calibrated variant further tightens this gap. These results connect spectral
coverage to stable ICL, bridge theory and deployment, and improve the
interpretability and reliability of large-scale prompting in realistic
finite-sample regimes.

</details>


### [119] [Bispectral OT: Dataset Comparison using Symmetry-Aware Optimal Transport](https://arxiv.org/abs/2509.20678)
*Annabel Ma,Kaiying Hou,David Alvarez-Melis,Melanie Weber*

Main category: cs.LG

TL;DR: Bispectral OT is a symmetry-aware discrete OT method using the bispectrum (a group Fourier invariant) to compare elements, yielding more accurate class-preserving correspondences under symmetry transformations than standard feature OT.


<details>
  <summary>Details</summary>
Motivation: In symmetry-rich settings, OT based on raw geometric distances can ignore the intrinsic coherence and invariances of the data, leading to suboptimal alignments that obscure semantic structure.

Method: Represent data elements by their bispectrum representation (a group Fourier invariant) and perform OT in this invariant feature space; the bispectrum preserves signal structure while removing variation due to group actions.

Result: Transport plans derived from Bispectral OT show higher class-preservation accuracy than naive feature OT on benchmark datasets transformed with visual symmetries, enabling more meaningful correspondences that reflect underlying semantic labels and suppress nuisance variation.

Conclusion: Bispectral OT provides a symmetry-aware extension of discrete OT that respects intrinsic structure under group actions, improving alignment quality in symmetry-rich data.

Abstract: Optimal transport (OT) is a widely used technique in machine learning,
graphics, and vision that aligns two distributions or datasets using their
relative geometry. In symmetry-rich settings, however, OT alignments based
solely on pairwise geometric distances between raw features can ignore the
intrinsic coherence structure of the data. We introduce Bispectral Optimal
Transport, a symmetry-aware extension of discrete OT that compares elements
using their representation using the bispectrum, a group Fourier invariant that
preserves all signal structure while removing only the variation due to group
actions. Empirically, we demonstrate that the transport plans computed with
Bispectral OT achieve greater class preservation accuracy than naive feature OT
on benchmark datasets transformed with visual symmetries, improving the quality
of meaningful correspondences that capture the underlying semantic label
structure in the dataset while removing nuisance variation not affecting class
or content.

</details>


### [120] [Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation](https://arxiv.org/abs/2509.20680)
*Wenkai Guo,Xuefeng Liu,Haolin Wang,Jianwei Niu,Shaojie Tang,Jing Yuan*

Main category: cs.LG

TL;DR: Federated fine-tuning of LLMs can still leak training data via the global model; leakage grows with model size; an enhanced FL-aware attack increases leakage; privacy-preserving techniques (DP, regularization, safety alignment) offer mitigation but tradeoffs exist; the paper provides practical guidelines for reducing privacy risks in FL-assisted LLM training.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns in federated learning when fine-tuning large language models; centralized data sharing is impractical, but model updates may still reveal sensitive information; need to quantify leakage and identify mitigation strategies.

Method: Extensive experiments with varying LLM sizes. Demonstrates that training data can be extracted from the global model using straightforward data-generation methods; develops an enhanced attack that tracks global model updates to amplify leakage specific to FL; evaluates privacy-preserving techniques (differential privacy, regularization-constrained updates, and LLM safety alignment) across scenarios.

Result: Training data leakage from the global model persists in FL for LLMs; leakage increases with model size; the FL-tailored attack intensifies privacy leakage; some privacy-preserving techniques reduce risk but may introduce tradeoffs in utility or performance; the study provides actionable guidelines.

Conclusion: Privacy risks in FL-based LLM fine-tuning are non-negligible; mitigating leakage requires careful use of privacy-preserving methods and alignment strategies, and the paper offers practical guidelines for practitioners to reduce privacy risks in federated LLM training.

Abstract: Fine-tuning large language models (LLMs) with local data is a widely adopted
approach for organizations seeking to adapt LLMs to their specific domains.
Given the shared characteristics in data across different organizations, the
idea of collaboratively fine-tuning an LLM using data from multiple sources
presents an appealing opportunity. However, organizations are often reluctant
to share local data, making centralized fine-tuning impractical. Federated
learning (FL), a privacy-preserving framework, enables clients to retain local
data while sharing only model parameters for collaborative training, offering a
potential solution. While fine-tuning LLMs on centralized datasets risks data
leakage through next-token prediction, the iterative aggregation process in FL
results in a global model that encapsulates generalized knowledge, which some
believe protects client privacy. In this paper, however, we present
contradictory findings through extensive experiments. We show that attackers
can still extract training data from the global model, even using
straightforward generation methods, with leakage increasing as the model size
grows. Moreover, we introduce an enhanced attack strategy tailored to FL, which
tracks global model updates during training to intensify privacy leakage. To
mitigate these risks, we evaluate privacy-preserving techniques in FL,
including differential privacy, regularization-constrained updates and adopting
LLMs with safety alignment. Our results provide valuable insights and practical
guidelines for reducing privacy risks when training LLMs with FL.

</details>


### [121] [Learning to Align Molecules and Proteins: A Geometry-Aware Approach to Binding Affinity](https://arxiv.org/abs/2509.20693)
*Mohammadsaleh Refahi,Bahrad A. Sokhansanj,James R. Brown,Gail Rosen*

Main category: cs.LG

TL;DR: A lightweight DTI model that uses FiLM conditioning of ligand on protein embeddings, couples metric learning via a triplet loss, and employs an RBF head on embedding distances to predict binding affinity, achieving state-of-the-art on the TDC DTI-DG benchmark with thorough ablations and out-of-domain testing.


<details>
  <summary>Details</summary>
Motivation: Existing DTI models often fuse ligand and protein features by simple concatenation and lack explicit geometric regularization, leading to limited generalization across chemical space and over time. A geometry-aware, robust approach is needed.

Method: Condition molecular embeddings on protein embeddings with a FiLM layer; enforce metric structure through a triplet loss; use an RBF regression head that predicts affinity from embedding distances; operate with a lightweight model.

Result: Achieves state-of-the-art performance on the Therapeutics Data Commons DTI-DG benchmark; extensive ablation study and out-of-domain evaluation support robustness and contribution of conditioning and metric learning.

Conclusion: Conditioning and metric learning improve robustness and generalization for drug-target affinity prediction; distance-based, interpretable affinity predictions benefit from FiLM conditioning.

Abstract: Accurate prediction of drug-target binding affinity can accelerate drug
discovery by prioritizing promising compounds before costly wet-lab screening.
While deep learning has advanced this task, most models fuse ligand and protein
representations via simple concatenation and lack explicit geometric
regularization, resulting in poor generalization across chemical space and
time. We introduce FIRM-DTI, a lightweight framework that conditions molecular
embeddings on protein embeddings through a feature-wise linear modulation
(FiLM) layer and enforces metric structure with a triplet loss. An RBF
regression head operating on embedding distances yields smooth, interpretable
affinity predictions. Despite its modest size, FIRM-DTI achieves
state-of-the-art performance on the Therapeutics Data Commons DTI-DG benchmark,
as demonstrated by an extensive ablation study and out-of-domain evaluation.
Our results underscore the value of conditioning and metric learning for robust
drug-target affinity prediction.

</details>


### [122] [CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning](https://arxiv.org/abs/2509.20712)
*Zhenpeng Su,Leiyu Pan,Minxuan Lv,Yuntao Li,Wenping Hu,Fuzheng Zhang,Kun Gai,Guorui Zhou*

Main category: cs.LG

TL;DR: CE-GPPO reintroduces gradient signals from clipped tokens in PPO, stabilizing entropy and improving RL-based fine-tuning of LLMs on reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: PPO's clipping discards gradients from low-probability tokens, which are informative for entropy regulation; this leads to entropy instability and suboptimal exploration-exploitation balance during LLM RL training.

Method: Introduce CE-GPPO, a gradient-preserving policy optimization that reintroduces and bounds gradients from clipped tokens in native PPO, analyze entropy dynamics, provide theoretical justification, and empirically evaluate on mathematical reasoning benchmarks across model scales.

Result: CE-GPPO mitigates entropy instability and consistently outperforms strong baselines on mathematical reasoning tasks across different model scales.

Conclusion: Reintroducing gradient signals from clipped tokens in a controlled way improves entropy management and RL training of LLMs; CE-GPPO is supported by theory and experiments and benefits reasoning tasks.

Abstract: Reinforcement learning (RL) has become a powerful paradigm for optimizing
large language models (LLMs) to handle complex reasoning tasks. A core
challenge in this process lies in managing policy entropy, which reflects the
balance between exploration and exploitation during training. Existing methods,
such as proximal policy optimization (PPO) and its variants, discard valuable
gradient signals from low-probability tokens due to the clipping mechanism. We
systematically analyze the entropy dynamics and reveal that these clipped
tokens play a critical yet overlooked role in regulating entropy evolution. We
propose \textbf{C}ontrolling \textbf{E}ntropy via
\textbf{G}radient-\textbf{P}reserving \textbf{P}olicy \textbf{O}ptimization
(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in
native PPO in a gentle and bounded manner. By controlling the magnitude of
gradients from tokens outside the clipping interval, CE-GPPO is able to achieve
an exploration-exploitation trade-off. We provide theoretical justification and
empirical evidence showing that CE-GPPO effectively mitigates entropy
instability. Extensive experiments on mathematical reasoning benchmarks show
that CE-GPPO consistently outperforms strong baselines across different model
scales.

</details>


### [123] [A Genetic Algorithm for Navigating Synthesizable Molecular Spaces](https://arxiv.org/abs/2509.20719)
*Alston Lo,Connor W. Coley,Wojciech Matusik*

Main category: cs.LG

TL;DR: SynGA is a synthesis-route–level genetic algorithm that enforces synthesizability, improving design efficiency via a ML-filtered building-block approach and a model-based BO variant (SynGBO); it serves as a lightweight, synthesis-aware baseline and modular component for broader workflows.


<details>
  <summary>Details</summary>
Motivation: Synthesizability is a key practical constraint in molecular design. Traditional genetic algorithms can explore non-viable, non-synthesizable spaces, so there is a need for algorithms that search only within synthesizable routes and integrate with optimization frameworks like Bayesian optimization.

Method: A genetic algorithm operating directly on synthesis routes with custom crossover and mutation operators that constrain the search to synthesizable space. Fitness functions are tailored to design objectives. A machine-learning-based filter focuses the building-block set to improve search efficiency. The model-based variant SynGBO embeds SynGA and block filtering inside the inner loop of Bayesian optimization. The approach is evaluated on 2D and 3D objectives.

Result: Demonstrates effectiveness on synthesizable analog search and sample-efficient property optimization. The ML-based filter yields state-of-the-art performance when coupled with SynGA. SynGA is lightweight, enforces synthesizability by construction, and can function as a strong standalone baseline or be integrated into larger synthesis-aware workflows.

Conclusion: SynGA provides a practical, synthesis-aware optimization module that can serve as both a strong baseline and a versatile component in larger design pipelines, applicable to 2D/3D objectives and adaptable to Bayesian optimization workflows.

Abstract: Inspired by the effectiveness of genetic algorithms and the importance of
synthesizability in molecular design, we present SynGA, a simple genetic
algorithm that operates directly over synthesis routes. Our method features
custom crossover and mutation operators that explicitly constrain it to
synthesizable molecular space. By modifying the fitness function, we
demonstrate the effectiveness of SynGA on a variety of design tasks, including
synthesizable analog search and sample-efficient property optimization, for
both 2D and 3D objectives. Furthermore, by coupling SynGA with a machine
learning-based filter that focuses the building block set, we boost SynGA to
state-of-the-art performance. For property optimization, this manifests as a
model-based variant SynGBO, which employs SynGA and block filtering in the
inner loop of Bayesian optimization. Since SynGA is lightweight and enforces
synthesizability by construction, our hope is that SynGA can not only serve as
a strong standalone baseline but also as a versatile module that can be
incorporated into larger synthesis-aware workflows in the future.

</details>


### [124] [Scaling Laws are Redundancy Laws](https://arxiv.org/abs/2509.20721)
*Yuda Bi,Vince D Calhoun*

Main category: cs.LG

TL;DR: The authors explain scaling laws as redundancy laws in kernel regression, deriving a non-universal power-law excess risk with exponent alpha = 2s / (2s + 1/beta) from a polynomial tail in the data covariance spectrum; universality across transformations, mixtures, finite-width models, and Transformer architectures, in both linearized (NTK) and feature-learning regimes.


<details>
  <summary>Details</summary>
Motivation: To resolve the mathematical origin of scaling laws and their exponents, linking learning-curve behavior to data redundancy and covariance structure.

Method: Use kernel regression (including NTK and feature-learning regimes) and analyze data with a polynomial tail in the covariance spectrum; derive the excess risk as a power law and identify the exponent as a function of smoothness s and spectral tail parameter beta (with redundancy measured as 1/beta).

Result: Excess risk follows a power law with exponent alpha = 2s / (2s + 1/beta); shows universality across boundedly invertible transformations, multi-modal mixtures, finite-width networks, and Transformer architectures across linearized and feature-learning regimes.

Conclusion: Provides a rigorous mathematical explanation of scaling laws as finite-sample redundancy laws, unifying empirical scaling with theoretical foundations and showing that learning-curve slope depends on data redundancy rather than being universal.

Abstract: Scaling laws, a defining feature of deep learning, reveal a striking
power-law improvement in model performance with increasing dataset and model
size. Yet, their mathematical origins, especially the scaling exponent, have
remained elusive. In this work, we show that scaling laws can be formally
explained as redundancy laws. Using kernel regression, we show that a
polynomial tail in the data covariance spectrum yields an excess risk power law
with exponent alpha = 2s / (2s + 1/beta), where beta controls the spectral tail
and 1/beta measures redundancy. This reveals that the learning curve's slope is
not universal but depends on data redundancy, with steeper spectra accelerating
returns to scale. We establish the law's universality across boundedly
invertible transformations, multi-modal mixtures, finite-width approximations,
and Transformer architectures in both linearized (NTK) and feature-learning
regimes. This work delivers the first rigorous mathematical explanation of
scaling laws as finite-sample redundancy laws, unifying empirical observations
with theoretical foundations.

</details>


### [125] [The Impact of Audio Watermarking on Audio Anti-Spoofing Countermeasures](https://arxiv.org/abs/2509.20736)
*Zhenshan Zhang,Xueping Zhang,Yechen Wang,Liwei Jin,Ming Li*

Main category: cs.LG

TL;DR: Audio watermarking degrades spoofing countermeasures; the authors create a Watermark-Spoofing dataset, show performance drop with higher watermark density, and introduce Knowledge-Preserving Watermark Learning (KPWL) to build watermark-resilient anti-spoofing; protocol benchmarks are public.


<details>
  <summary>Details</summary>
Motivation: Anti-spoofing is critical for secure speech systems, but audio watermarking—designed for copyright protection—may cause domain shifts. This study is the first to quantify its impact and to establish benchmarks for watermark-resilient anti-spoofing.

Method: Construct Watermark-Spoofing dataset by applying diverse handcrafted and neural watermarking to existing anti-spoofing datasets; conduct experiments showing degradation; propose KPWL to adapt models to watermark-induced shifts while preserving performance on the original spoofing task; release protocols publicly.

Result: Watermarking consistently degrades anti-spoofing performance; higher watermark density yields higher Equal Error Rates (EERs); KPWL helps models adapt to watermark-related shifts while maintaining spoofing detection in the original domain.

Conclusion: Audio watermarking represents a previously overlooked domain shift in anti-spoofing research. The work provides the first benchmark for watermark-resilient anti-spoofing systems and releases datasets/protocols for future研究.

Abstract: This paper presents the first study on the impact of audio watermarking on
spoofing countermeasures. While anti-spoofing systems are essential for
securing speech-based applications, the influence of widely used audio
watermarking, originally designed for copyright protection, remains largely
unexplored. We construct watermark-augmented training and evaluation datasets,
named the Watermark-Spoofing dataset, by applying diverse handcrafted and
neural watermarking methods to existing anti-spoofing datasets. Experiments
show that watermarking consistently degrades anti-spoofing performance, with
higher watermark density correlating with higher Equal Error Rates (EERs). To
mitigate this, we propose the Knowledge-Preserving Watermark Learning (KPWL)
framework, enabling models to adapt to watermark-induced shifts while
preserving their original-domain spoofing detection capability. These findings
reveal audio watermarking as a previously overlooked domain shift and establish
the first benchmark for developing watermark-resilient anti-spoofing systems.
All related protocols are publicly available at
https://github.com/Alphawarheads/Watermark_Spoofing.git

</details>


### [126] [Measuring LLM Sensitivity in Transformer-based Tabular Data Synthesis](https://arxiv.org/abs/2509.20768)
*Maria F. Davila R,Azizjon Turaev,Wolfram Wingerath*

Main category: cs.LG

TL;DR: Transformer-based TDS yields high-quality synthetic data but is compute-intensive; hyperparameters strongly affect runtime and utility; lightweight configurations offer favorable trade-offs, with REaLTabFormer performing best on large datasets, though still slower than GReaT.


<details>
  <summary>Details</summary>
Motivation: Assess how the choice of hyperparameters (e.g., number of layers, hidden dimension) affects synthetic data quality and computational performance across two Transformer-based TDS tools (GReaT and REaLTabFormer, with lightweight LLMs), to guide practical tool selection for different hardware and dataset sizes.

Method: Sensitivity analysis across 10 model setups varying architecture type and depth on four real-world datasets. Evaluate three dimensions: runtime, ML utility, and similarity to real data distributions. Compare two tools (GReaT and REaLTabFormer) under varying hyperparameters.

Result: Runtime scales with the number of hyperparameters; shallower configurations complete faster. GReaT generally has lower runtimes than REaLTabFormer; on the largest dataset, runtimes converge. For small datasets, both produce high utility and strong similarity to real data. On larger datasets, REaLTabFormer preserves utility and similarity better. REaLTabFormer with lightweight LLMs offers the best balance between data quality and computational load, but its runtime remains higher than GReaT and other TDS tools, indicating room for efficiency improvements before hitting a performance plateau.

Conclusion: Hyperparameter choices and dataset size critically shape the trade-off between data quality and computation in Transformer-based TDS. For small datasets, either tool is viable; for large datasets, REaLTabFormer plus lightweight LLMs is preferable for maintaining utility and similarity, though overall speed improvements are needed to close the gap with faster tools like GReaT.

Abstract: Synthetic tabular data is used for privacy-preserving data sharing and
data-driven model development. Its effectiveness, however, depends heavily on
the used Tabular Data Synthesis (TDS) tool. Recent studies have shown that
Transformer-based models outperform other state-of-the-art models such as
Generative Adversarial Networks (GANs) and Diffusion models in terms of data
quality. However, Transformer-based models also come with high computational
costs, making them sometimes unfeasible for end users with prosumer hardware.
This study presents a sensitivity assessment on how the choice of
hyperparameters, such as number of layers or hidden dimension affects the
quality of the resultant synthetic data and the computational performance. It
is performed across two tools, GReaT and REaLTabFormer, evaluating 10 model
setups that vary in architecture type and depth. We assess the sensitivity on
three dimensions: runtime, machine learning (ML) utility, and similarity to
real data distributions. Experiments were conducted on four real-world
datasets. Our findings reveal that runtime is proportional to the number of
hyperparameters, with shallower configurations completing faster. GReaT
consistently achieves lower runtimes than REaLTabFormer, and only on the
largest dataset they have comparable runtime. For small datasets, both tools
achieve synthetic data with high utility and optimal similarity, but on larger
datasets only REaLTabFormer sustains strong utility and similarity. As a
result, REaLTabFormer with lightweight LLMs provides the best balance, since it
preserves data quality while reducing computational requirements. Nonetheless,
its runtime remains higher than that of GReaT and other TDS tools, suggesting
that efficiency gains are possible but only up to a certain level.

</details>


### [127] [Sig2Model: A Boosting-Driven Model for Updatable Learned Indexes](https://arxiv.org/abs/2509.20781)
*Alireza Heidari,Amirhossein Ahmad,Wei Zhang,Ying Xiong*

Main category: cs.LG

TL;DR: Sig2Model is an adaptive learned index that minimizes retraining costs under updates by combining a sigmoid boosting approximation, proactive update training with Gaussian mixture models, and a neural joint optimization, achieving large gains in retraining cost, QPS, and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: Learned indexes excel for static data but suffer due to global retraining when data updates occur. The need is to preserve CDF invariants and maintain high query throughput without expensive retraining.

Method: Three techniques: (1) sigmoid boosting approximation to model update-induced distribution shifts with localized sigmoids, delaying full retraining while bounding error; (2) proactive update training via Gaussian mixture models to locate high-update regions and allocate placeholders for faster updates; (3) a neural joint optimization that jointly tunes the sigmoid ensemble and GMM parameters via gradient-based learning.

Result: Compared to state-of-the-art updatable learned indexes on real and synthetic workloads, Sig2Model reduces retraining cost by up to 20x, achieves up to 3x higher QPS, and uses up to 1000x less memory.

Conclusion: Sig2Model delivers an adaptive framework that defers full retraining and localizes updates, enabling practical deployment of learned indexes in dynamic workloads while maintaining query performance.

Abstract: Learned Indexes (LIs) represent a paradigm shift from traditional index
structures by employing machine learning models to approximate the cumulative
distribution function (CDF) of sorted data. While LIs achieve remarkable
efficiency for static datasets, their performance degrades under dynamic
updates: maintaining the CDF invariant (sum of F(k) equals 1) requires global
model retraining, which blocks queries and limits the queries-per-second (QPS)
metric. Current approaches fail to address these retraining costs effectively,
rendering them unsuitable for real-world workloads with frequent updates. In
this paper, we present Sig2Model, an efficient and adaptive learned index that
minimizes retraining cost through three key techniques: (1) a sigmoid boosting
approximation technique that dynamically adjusts the index model by
approximating update-induced shifts in data distribution with localized sigmoid
functions while preserving bounded error guarantees and deferring full
retraining; (2) proactive update training via Gaussian mixture models (GMMs)
that identifies high-update-probability regions for strategic placeholder
allocation to speed up updates; and (3) a neural joint optimization framework
that continuously refines both the sigmoid ensemble and GMM parameters via
gradient-based learning. We evaluate Sig2Model against state-of-the-art
updatable learned indexes on real-world and synthetic workloads, and show that
Sig2Model reduces retraining cost by up to 20x, achieves up to 3x higher QPS,
and uses up to 1000x less memory.

</details>


### [128] [IConv: Focusing on Local Variation with Channel Independent Convolution for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.20783)
*Gawon Lee,Hanbyeol Park,Minseop Kim,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: A CNN-MLP hybrid model with an IConv layer to model both long-term trends and diverse local patterns in multivariate, non-stationary time series for improved forecasting, aiming to reduce computation while handling channel-specific variations.


<details>
  <summary>Details</summary>
Motivation: Real-world time-series exhibit non-stationarity, changing trends, irregular seasonality, and residuals. While MLPs capture long-term dependencies efficiently, their linear nature struggles with diverse local variations. CNNs can model local patterns, but the paper proposes an integrated architecture to jointly leverage both strengths and introduce IConv to independently process channels for better local dependency modeling with reduced computation.

Method: Propose a hybrid architecture where an MLP models the overall long-term trend. A CNN with diverse kernels captures fine-grained local patterns, guided by the MLP trend predictions. Introduce IConv, a convolutional block that processes temporal dependency channels independently and models inter-channel relationships through distinct layers, allowing large kernel sizes and reduced computation. Independent channel processing enables modeling of diverse local temporal dependencies; separate inter-channel paths reduce costs and enable effective cross-channel interaction.

Result: Extensive experiments on multivariate time-series datasets show the proposed method achieves superior forecasting performance compared to baselines, validating the effectiveness of combining MLP trend modeling with CNN-based local pattern extraction and the IConv architecture.

Conclusion: The integration of MLP for long-term trends with CNN-based local pattern modeling, augmented by the IConv design that treats channels independently but connects them through dedicated inter-channel pathways, offers effective modeling of non-stationary multivariate time series and improves forecasting performance while aiming to maintain computational efficiency.

Abstract: Real-world time-series data often exhibit non-stationarity, including
changing trends, irregular seasonality, and residuals. In terms of changing
trends, recently proposed multi-layer perceptron (MLP)-based models have shown
excellent performance owing to their computational efficiency and ability to
capture long-term dependency. However, the linear nature of MLP architectures
poses limitations when applied to channels with diverse distributions,
resulting in local variations such as seasonal patterns and residual components
being ignored. However, convolutional neural networks (CNNs) can effectively
incorporate these variations. To resolve the limitations of MLP, we propose
combining them with CNNs. The overall trend is modeled using an MLP to consider
long-term dependencies. The CNN uses diverse kernels to model fine-grained
local patterns in conjunction with MLP trend predictions. To focus on modeling
local variation, we propose IConv, a novel convolutional architecture that
processes the temporal dependency channel independently and considers the
inter-channel relationship through distinct layers. Independent channel
processing enables the modeling of diverse local temporal dependencies and the
adoption of a large kernel size. Distinct inter-channel considerations reduce
computational cost. The proposed model is evaluated through extensive
experiments on time-series datasets. The results reveal the superiority of the
proposed method for multivariate time-series forecasting.

</details>


### [129] [LiLAW: Lightweight Learnable Adaptive Weighting to Meta-Learn Sample Difficulty and Improve Noisy Training](https://arxiv.org/abs/2509.20786)
*Abhishek Moturu,Anna Goldenberg,Babak Taati*

Main category: cs.LG

TL;DR: LiLAW introduces lightweight Learnable Adaptive Weighting, using three trainable parameters to dynamically adjust each sample's loss weight based on evolving difficulty (easy/moderate/hard). Weights are updated by a single gradient step on a validation batch after each training batch, enabling robust learning under label noise and data heterogeneity with minimal hyperparameter tuning and no need for heavy augmentation.


<details>
  <summary>Details</summary>
Motivation: Training deep nets with noisy labels and heterogeneous data is hard; existing robust methods often require extensive hyperparameter tuning or curated clean validation data. A lightweight, parametrized approach that adapts sample weighting during training could improve generalization and robustness with low overhead.

Method: Three learnable parameters categorize samples into easy/moderate/hard; per-sample loss weights are adapted during training by a single minibatch gradient step on the validation set after each training minibatch. This avoids large hyperparameter sweeps and does not rely on heavy preprocessing, and is compatible with various losses, architectures, and pretraining regimes.

Result: Extensive experiments across general and medical imaging datasets, multiple noise levels/types, losses, and architectures (with/without pretraining) show LiLAW consistently improves performance, particularly in high-noise settings, and does so with limited reliance on data augmentation or regularization, indicating practical robustness and efficiency.

Conclusion: LiLAW provides a computationally efficient, generalizable solution to boost model robustness under label noise and data heterogeneity, requiring only three learnable parameters and one validation-step update per training batch.

Abstract: Training deep neural networks in the presence of noisy labels and data
heterogeneity is a major challenge. We introduce Lightweight Learnable Adaptive
Weighting (LiLAW), a novel method that dynamically adjusts the loss weight of
each training sample based on its evolving difficulty level, categorized as
easy, moderate, or hard. Using only three learnable parameters, LiLAW
adaptively prioritizes informative samples throughout training by updating
these weights using a single mini-batch gradient descent step on the validation
set after each training mini-batch, without requiring excessive hyperparameter
tuning or a clean validation set. Extensive experiments across multiple general
and medical imaging datasets, noise levels and types, loss functions, and
architectures with and without pretraining demonstrate that LiLAW consistently
enhances performance, even in high-noise environments. It is effective without
heavy reliance on data augmentation or advanced regularization, highlighting
its practicality. It offers a computationally efficient solution to boost model
generalization and robustness in any neural network training setup.

</details>


### [130] [Aligning Inductive Bias for Data-Efficient Generalization in State Space Models](https://arxiv.org/abs/2509.20789)
*Qiyu Chen,Guozhang Chen*

Main category: cs.LG

TL;DR: Task-Dependent Initialization via spectrum matching improves data efficiency of linear SSMs by aligning the model's inductive bias with the task's spectral characteristics; formalizes the SSM-induced kernel spectrum and demonstrates practical gains.


<details>
  <summary>Details</summary>
Motivation: Data efficiency is crucial for scaling; fixed inductive biases in foundational sequence models (SSMs) can be misaligned with task structure, limiting sample efficiency; a method to adapt bias before training is needed.

Method: Formally characterize the SSM-induced kernel spectrum through the model's frequency response; introduce Task-Dependent Initialization (TDI) via power spectrum matching to align the model's inductive bias with a task's spectral characteristics prior to large-scale training.

Result: Empirical results on diverse real-world benchmarks show improved generalization and sample efficiency, especially in low-data regimes.

Conclusion: Provides a theoretical and practical framework for creating more data-efficient models by aligning inductive bias with task spectra, contributing to sustainable scaling.

Abstract: The remarkable success of large-scale models is fundamentally tied to scaling
laws, yet the finite nature of high-quality data presents a looming challenge.
One of the next frontiers in modeling is data efficiency: the ability to learn
more from less. A model's inductive bias is a critical lever for this, but
foundational sequence models like State Space Models (SSMs) rely on a fixed
bias. This fixed prior is sample-inefficient when a task's underlying structure
does not match. In this work, we introduce a principled framework to solve this
problem. We first formalize the inductive bias of linear time-invariant SSMs
through an SSM-induced kernel, mathematically and empirically proving its
spectrum is directly governed by the model's frequency response. Further, we
propose a method of Task-Dependent Initialization (TDI): power spectrum
matching, a fast and efficient method that aligns the model's inductive bias
with the task's spectral characteristics before large-scale training. Our
experiments on a diverse set of real-world benchmarks show that TDI
significantly improves generalization and sample efficiency, particularly in
low-data regimes. This work provides a theoretical and practical tool to create
more data-efficient models, a crucial step towards sustainable scaling.

</details>


### [131] [FERD: Fairness-Enhanced Data-Free Robustness Distillation](https://arxiv.org/abs/2509.20793)
*Zhengxiao Li,Liming Lu,Xu Zheng,Siyuan Liang,Zhenghan Chen,Yongbin Zhou,Shuchao Pang*

Main category: cs.LG

TL;DR: FERD: a fairness-aware, data-free robustness distillation framework that adjusts class sampling and attack distributions to boost worst-class robustness.


<details>
  <summary>Details</summary>
Motivation: Current data-free robustness distillation methods ignore fairness, causing large disparities across classes and unstable performance across attack types.

Method: 1) Robustness-guided class reweighting to synthesize more samples for less robust categories; 2) Fairness-Aware Examples (FAEs) with a uniformity constraint on feature-level predictions to balance category representation; 3) Uniform-Target Adversarial Examples (UTAEs) with a uniform target constraint to diversify attack directions.

Result: On three public datasets, FERD achieves state-of-the-art worst-class robustness under various attacks; e.g., on CIFAR-10 with MobileNet-V2, worst-class robustness under FGSM improves by 15.1% and under AutoAttack by 6.4%.

Conclusion: FERD effectively closes robustness gaps among classes and stabilizes performance across attacks, indicating both robustness and fairness gains in data-free distillation.

Abstract: Data-Free Robustness Distillation (DFRD) aims to transfer the robustness from
the teacher to the student without accessing the training data. While existing
methods focus on overall robustness, they overlook the robust fairness issues,
leading to severe disparity of robustness across different categories. In this
paper, we find two key problems: (1) student model distilled with equal class
proportion data behaves significantly different across distinct categories; and
(2) the robustness of student model is not stable across different attacks
target. To bridge these gaps, we present the first Fairness-Enhanced data-free
Robustness Distillation (FERD) framework to adjust the proportion and
distribution of adversarial examples. For the proportion, FERD adopts a
robustness-guided class reweighting strategy to synthesize more samples for the
less robust categories, thereby improving robustness of them. For the
distribution, FERD generates complementary data samples for advanced robustness
distillation. It generates Fairness-Aware Examples (FAEs) by enforcing a
uniformity constraint on feature-level predictions, which suppress the
dominance of class-specific non-robust features, providing a more balanced
representation across all categories. Then, FERD constructs Uniform-Target
Adversarial Examples (UTAEs) from FAEs by applying a uniform target class
constraint to avoid biased attack directions, which distribute the attack
targets across all categories and prevents overfitting to specific vulnerable
categories. Extensive experiments on three public datasets show that FERD
achieves state-of-the-art worst-class robustness under all adversarial attack
(e.g., the worst-class robustness under FGSM and AutoAttack are improved by
15.1\% and 6.4\% using MobileNet-V2 on CIFAR-10), demonstrating superior
performance in both robustness and fairness aspects.

</details>


### [132] [T2I-Diff: fMRI Signal Generation via Time-Frequency Image Transform and Classifier-Free Denoising Diffusion Models](https://arxiv.org/abs/2509.20822)
*Hwa Hui Tew,Junn Yong Loo,Yee-Fan Tan,Xinyu Tang,Hernando Ombao,Fuad Noman,Raphael C. -W. Phan,Chee-Ming Ting*

Main category: cs.LG

TL;DR: A diffusion-based fMRI data generator (T2I-Diff) that uses time-frequency representations of BOLD signals to synthesize class-conditioned spectrograms, reconstructs them to BOLD, and enhances downstream brain network classification.


<details>
  <summary>Details</summary>
Motivation: fMRI data are scarce and expensive; high-fidelity samples are needed for data-driven brain analysis; existing generative models often fail to capture non-stationary and nonlinear BOLD dynamics.

Method: Convert BOLD to windowed spectrograms using a time-dependent Fourier transform; train a classifier-free diffusion model to generate class-conditioned frequency spectrograms; invert spectrograms back to BOLD with inverse Fourier transform; evaluate on downstream brain network classification tasks.

Result: Demonstrates improved accuracy and generalization in downstream fMRI-based brain network classification when using the synthesized data.

Conclusion: The time-frequency diffusion approach captures dynamic BOLD properties and provides high-quality synthetic fMRI data, boosting downstream analyses and enabling better data augmentation.

Abstract: Functional Magnetic Resonance Imaging (fMRI) is an advanced neuroimaging
method that enables in-depth analysis of brain activity by measuring dynamic
changes in the blood oxygenation level-dependent (BOLD) signals. However, the
resource-intensive nature of fMRI data acquisition limits the availability of
high-fidelity samples required for data-driven brain analysis models. While
modern generative models can synthesize fMRI data, they often underperform
because they overlook the complex non-stationarity and nonlinear BOLD dynamics.
To address these challenges, we introduce T2I-Diff, an fMRI generation
framework that leverages time-frequency representation of BOLD signals and
classifier-free denoising diffusion. Specifically, our framework first converts
BOLD signals into windowed spectrograms via a time-dependent Fourier transform,
capturing both the underlying temporal dynamics and spectral evolution.
Subsequently, a classifier-free diffusion model is trained to generate
class-conditioned frequency spectrograms, which are then reverted to BOLD
signals via inverse Fourier transforms. Finally, we validate the efficacy of
our approach by demonstrating improved accuracy and generalization in
downstream fMRI-based brain network classification.

</details>


### [133] [CaTS-Bench: Can Language Models Describe Numeric Time Series?](https://arxiv.org/abs/2509.20823)
*Luca Zhou,Pratham Yashwante,Marshall Fisher,Alessio Sampieri,Zihao Zhou,Fabio Galasso,Rose Yu*

Main category: cs.LG

TL;DR: CaTS-Bench is the first large-scale, real-world benchmark for context-aware time series captioning, combining numeric series, metadata, and line-chart images to generate captions and QA data, with LLM-generated references, human-verified subsets, and new evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Current time series benchmarks rely on synthetic data or simplistic captions and neglect contextual metadata and visualization. There is a need for a real-world, context-aware benchmark that integrates numeric data, metadata, and chart visuals to foster progress in time series understanding and foundation-model alignment.

Method: CaTS-Bench reframes 11 diverse real datasets as captioning and QA tasks, producing ~465k training and ~105k test timestamps. Each sample includes a numeric series segment, contextual metadata, a line-chart image, and a caption. Reference captions are mainly generated by an oracle LLM with factual checks, plus a human-revisited subset (579 test captions). The work also provides 460 multiple-choice questions, new tailored evaluation metrics, and baseline evaluations of leading VLMs.

Result: The dataset offers a comprehensive, real-world resource for context-aware time series captioning, with scalable caption-generation pipelines, quality control through verification and human revision, and broad evaluation scaffolding (VLM baselines, MCQs, metrics).

Conclusion: CaTS-Bench establishes a reliable and extensible foundation at the intersection of time series analysis and foundation models, enabling more realistic evaluation of captioning, QA, and multimodal reasoning in time series contexts.

Abstract: Time series captioning, the task of describing numeric time series in natural
language, requires numerical reasoning, trend interpretation, and contextual
understanding. Existing benchmarks, however, often rely on synthetic data or
overly simplistic captions, and typically neglect metadata and visual
representations. To close this gap, we introduce CaTS-Bench, the first
large-scale, real-world benchmark for Context-aware Time Series captioning.
CaTS-Bench is derived from 11 diverse datasets reframed as captioning and Q&A
tasks, comprising roughly 465k training and 105k test timestamps. Each sample
includes a numeric series segment, contextual metadata, a line-chart image, and
a caption. A key contribution of this work is the scalable pipeline used to
generate reference captions: while most references are produced by an oracle
LLM and verified through factual checks, human indistinguishability studies,
and diversity analyses, we also provide a human-revisited subset of 579 test
captions, refined from LLM outputs to ensure accuracy and human-like style.
Beyond captioning, CaTS-Bench offers 460 multiple-choice questions targeting
deeper aspects of time series reasoning. We further propose new tailored
evaluation metrics and benchmark leading VLMs, highlighting both their
strengths and persistent limitations. Together, these contributions establish
CaTS-Bench and its captioning pipeline as a reliable and extensible foundation
for future research at the intersection of time series analysis and foundation
models.

</details>


### [134] [Explaining Grokking and Information Bottleneck through Neural Collapse Emergence](https://arxiv.org/abs/2509.20829)
*Keitaro Sakamoto,Issei Sato*

Main category: cs.LG

TL;DR: A unified explanation for late-phase learning phenomena (grokking and information bottleneck) through neural collapse, showing contraction of within-class variance as the key driver and linking training-set neural collapse to population statistics; time-scale separation explains delayed test accuracy improvements; validated across datasets and architectures.


<details>
  <summary>Details</summary>
Motivation: Grokking and information bottleneck show puzzling late improvements; current theory lacks a unified account; aim to connect representation geometry to these dynamics.

Method: Theoretical analysis of neural collapse dynamics; relate population within-class variance to neural collapse measure on the training set; derive time-scale separation; empirical validation across multiple datasets/architectures.

Result: Contraction of the within-class variance emerges as the central mechanism driving late-phase phenomena; the neural collapse trajectory and its time scales explain grokking and information bottleneck; results validated empirically.

Conclusion: Neural collapse dynamics provide a unified framework for interpreting late-stage learning phenomena, with contraction of class-conditional variance and time-scale separation offering mechanistic explanations and broad applicability.

Abstract: The training dynamics of deep neural networks often defy expectations, even
as these models form the foundation of modern machine learning. Two prominent
examples are grokking, where test performance improves abruptly long after the
training loss has plateaued, and the information bottleneck principle, where
models progressively discard input information irrelevant to the prediction
task as training proceeds. However, the mechanisms underlying these phenomena
and their relations remain poorly understood. In this work, we present a
unified explanation of such late-phase phenomena through the lens of neural
collapse, which characterizes the geometry of learned representations. We show
that the contraction of population within-class variance is a key factor
underlying both grokking and information bottleneck, and relate this measure to
the neural collapse measure defined on the training set. By analyzing the
dynamics of neural collapse, we show that distinct time scales between fitting
the training set and the progression of neural collapse account for the
behavior of the late-phase phenomena. Finally, we validate our theoretical
findings on multiple datasets and architectures.

</details>


### [135] [Shaping Initial State Prevents Modality Competition in Multi-modal Fusion: A Two-stage Scheduling Framework via Fast Partial Information Decomposition](https://arxiv.org/abs/2509.20840)
*Jiaqi Tang,Yinsong Xu,Yang Liu,Qingchao Chen*

Main category: cs.LG

TL;DR: A two-stage training framework shapes initial states via unimodal pretraining to alleviate modality competition in multi-modal fusion, using an MI-based ECS proxy (via FastPID) and an asynchronous controller to balance modalities; achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Modality competition during joint training causes one modality to dominate learning, and the impact of the model’s initial state is often overlooked. Pre-shaping initial ECS through unimodal training can tighten error bounds and improve fusion.

Method: Introduce Effective Competitive Strength (ECS) to quantify modality competitiveness. Prove that shaping initial ECS via unimodal training tightens error bounds. Since ECS is intractable for deep networks, develop a proxy using mutual information (MI). Propose FastPID for efficient, differentiable partial information decomposition to quantify modality-specific uniqueness, redundancy, and synergy. Use an asynchronous controller guided by these measurements to balance modalities and identify the best initial state (peak synergy) before joint training. Implement a two-stage pipeline: unimodal pretraining followed by joint training.

Result: Experiments on diverse benchmarks show state-of-the-art performance, validating that shaping the pre-fusion initial state eases competition and unlocks synergistic multi-modal fusion.

Conclusion: Shaping the initial states of pre-fusion models is a powerful strategy to mitigate modality competition and foster synergy in multi-modal fusion, leading to robust, state-of-the-art performance.

Abstract: Multi-modal fusion often suffers from modality competition during joint
training, where one modality dominates the learning process, leaving others
under-optimized. Overlooking the critical impact of the model's initial state,
most existing methods address this issue during the joint learning stage. In
this study, we introduce a two-stage training framework to shape the initial
states through unimodal training before the joint training. First, we propose
the concept of Effective Competitive Strength (ECS) to quantify a modality's
competitive strength. Our theoretical analysis further reveals that properly
shaping the initial ECS by unimodal training achieves a provably tighter error
bound. However, ECS is computationally intractable in deep neural networks. To
bridge this gap, we develop a framework comprising two core components: a
fine-grained computable diagnostic metric and an asynchronous training
controller. For the metric, we first prove that mutual information(MI) is a
principled proxy for ECS. Considering MI is induced by per-modality marginals
and thus treats each modality in isolation, we further propose FastPID, a
computationally efficient and differentiable solver for partial information
decomposition, which decomposes the joint distribution's information into
fine-grained measurements: modality-specific uniqueness, redundancy, and
synergy. Guided by these measurements, our asynchronous controller dynamically
balances modalities by monitoring uniqueness and locates the ideal initial
state to start joint training by tracking peak synergy. Experiments on diverse
benchmarks demonstrate that our method achieves state-of-the-art performance.
Our work establishes that shaping the pre-fusion models' initial state is a
powerful strategy that eases competition before it starts, reliably unlocking
synergistic multi-modal fusion.

</details>


### [136] [Robust Multi-Omics Integration from Incomplete Modalities Significantly Improves Prediction of Alzheimer's Disease](https://arxiv.org/abs/2509.20842)
*Sungjoon Park,Kyungwook Lee,Soorin Yim,Doyeong Hwang,Dongyun Kim,Soonyoung Lee,Amy Dunn,Daniel Gatti,Elissa Chesler,Kristen O'Connell,Kiyoung Kim*

Main category: cs.LG

TL;DR: MOIRA enables robust integration of incomplete multi-omics data by aligning representations in a shared embedding space and adaptively fusing modalities; it outperforms baselines on ROSMAP AD data and identifies biologically plausible AD biomarkers.


<details>
  <summary>Details</summary>
Motivation: Integrative multi-omics analysis is hindered by missing modalities; an early integration approach that can leverage all samples and handle absent data is needed.

Method: Projects each omics modality into a shared embedding space; uses a learnable weighting mechanism to adaptively fuse modalities; operates in an early integration framework with representation alignment to be robust to missing data.

Result: On ROSMAP dataset for Alzheimer's disease, MOIRA outperformed existing approaches; ablation showed modality-wise contributions; feature importance highlighted AD-related biomarkers consistent with prior literature.

Conclusion: MOIRA provides robust, biologically meaningful multi-omics integration for incomplete data, enabling improved disease insights and potential biomarker discovery across heterogeneous cohorts.

Abstract: Multi-omics data capture complex biomolecular interactions and provide
insights into metabolism and disease. However, missing modalities hinder
integrative analysis across heterogeneous omics. To address this, we present
MOIRA (Multi-Omics Integration with Robustness to Absent modalities), an early
integration method enabling robust learning from incomplete omics data via
representation alignment and adaptive aggregation. MOIRA leverages all samples,
including those with missing modalities, by projecting each omics dataset onto
a shared embedding space where a learnable weighting mechanism fuses them.
Evaluated on the Religious Order Study and Memory and Aging Project (ROSMAP)
dataset for Alzheimer's Disease (AD), MOIRA outperformed existing approaches,
and further ablation studies confirmed modality-wise contributions. Feature
importance analysis revealed AD-related biomarkers consistent with prior
literature, highlighting the biological relevance of our approach.

</details>


### [137] [Causal Time Series Generation via Diffusion Models](https://arxiv.org/abs/2509.20846)
*Yutong Xia,Chang Xu,Yuxuan Liang,Qingsong Wen,Roger Zimmermann,Jiang Bian*

Main category: cs.LG

TL;DR: Introduces causal time series generation (CaTSG) and frames it as diffusion-based sampling guided by backdoor adjustment to handle observational, interventional, and counterfactual tasks under Pearl's causal ladder; shows improved fidelity and new capabilities beyond baselines.


<details>
  <summary>Details</summary>
Motivation: Observational conditional TSG models capture covariate correlations but ignore unobserved confounding, risking unreliable generation under interventions or counterfactuals; a principled causal framework is needed for robust TSG across the causal ladder.

Method: CaTSG is a unified diffusion-based framework. It derives causal score functions via backdoor adjustment and the abduction-action-prediction procedure, enabling backdoor-guided sampling toward desired interventions and individual counterfactuals while preserving observational fidelity.

Result: Experiments on synthetic and real-world data show CaTSG achieves higher fidelity and can perform interventional and counterfactual generation that existing baselines cannot.

Conclusion: Proposes the causal TSG family and CaTSG as an initial proof-of-concept, opening a promising direction for reliable simulation under interventions and counterfactual generation.

Abstract: Time series generation (TSG) synthesizes realistic sequences and has achieved
remarkable success. Among TSG, conditional models generate sequences given
observed covariates, however, such models learn observational correlations
without considering unobserved confounding. In this work, we propose a causal
perspective on conditional TSG and introduce causal time series generation as a
new TSG task family, formalized within Pearl's causal ladder, extending beyond
observational generation to include interventional and counterfactual settings.
To instantiate these tasks, we develop CaTSG, a unified diffusion-based
framework with backdoor-adjusted guidance that causally steers sampling toward
desired interventions and individual counterfactuals while preserving
observational fidelity. Specifically, our method derives causal score functions
via backdoor adjustment and the abduction-action-prediction procedure, thus
enabling principled support for all three levels of TSG. Extensive experiments
on both synthetic and real-world datasets show that CaTSG achieves superior
fidelity and also supporting interventional and counterfactual generation that
existing baselines cannot handle. Overall, we propose the causal TSG family and
instantiate it with CaTSG, providing an initial proof-of-concept and opening a
promising direction toward more reliable simulation under interventions and
counterfactual generation.

</details>


### [138] [FHRFormer: A Self-supervised Transformer Approach for Fetal Heart Rate Inpainting and Forecasting](https://arxiv.org/abs/2509.20852)
*Kjersti Engan,Neel Kanwal,Anita Yeconia,Ladislaus Blacy,Yuda Munyaw,Estomih Mduma,Hege Ersdal*

Main category: cs.LG

TL;DR: Masked transformer-based autoencoder for fetal heart rate signal reconstruction to handle missing data in wearable monitoring, enabling inpainting and forecasting to support AI risk algorithms.


<details>
  <summary>Details</summary>
Motivation: FHR monitoring gaps due to movement and position changes reduce data quality; preserving spectral characteristics is crucial for AI analysis; improves ability to predict need for breathing support or obstetric interventions.

Method: A masked transformer-based autoencoder that learns to reconstruct missing FHR segments by modeling spatial relationships and spectral content; supports inpainting and forecasting; robust to varying lengths of dropout; suitable for retrospective analysis and future wearable deployment.

Result: Demonstrates robustness across different missing-data durations and effective reconstruction/inpainting of FHR signals; indicates potential to improve downstream AI risk algorithms and enable earlier risk detection when integrated into wearables.

Conclusion: The proposed method offers a robust, data-driven approach to mitigating missingness in continuous FHR data, enabling more reliable AI-based risk assessment and paving the way for integration into wearable devices.

Abstract: Approximately 10\% of newborns require assistance to initiate breathing at
birth, and around 5\% need ventilation support. Fetal heart rate (FHR)
monitoring plays a crucial role in assessing fetal well-being during prenatal
care, enabling the detection of abnormal patterns and supporting timely
obstetric interventions to mitigate fetal risks during labor. Applying
artificial intelligence (AI) methods to analyze large datasets of continuous
FHR monitoring episodes with diverse outcomes may offer novel insights into
predicting the risk of needing breathing assistance or interventions. Recent
advances in wearable FHR monitors have enabled continuous fetal monitoring
without compromising maternal mobility. However, sensor displacement during
maternal movement, as well as changes in fetal or maternal position, often lead
to signal dropouts, resulting in gaps in the recorded FHR data. Such missing
data limits the extraction of meaningful insights and complicates automated
(AI-based) analysis. Traditional approaches to handle missing data, such as
simple interpolation techniques, often fail to preserve the spectral
characteristics of the signals. In this paper, we propose a masked
transformer-based autoencoder approach to reconstruct missing FHR signals by
capturing both spatial and frequency components of the data. The proposed
method demonstrates robustness across varying durations of missing data and can
be used for signal inpainting and forecasting. The proposed approach can be
applied retrospectively to research datasets to support the development of
AI-based risk algorithms. In the future, the proposed method could be
integrated into wearable FHR monitoring devices to achieve earlier and more
robust risk detection.

</details>


### [139] [Federated Markov Imputation: Privacy-Preserving Temporal Imputation in Multi-Centric ICU Environments](https://arxiv.org/abs/2509.20867)
*Christoph Düsing,Philipp Cimiano*

Main category: cs.LG

TL;DR: FMI is a privacy-preserving federated imputation method that enables ICUs to collaboratively learn global Markov transition models for temporal data imputation, improving performance on a real-world sepsis onset task using MIMIC-IV, particularly when sampling intervals are irregular.


<details>
  <summary>Details</summary>
Motivation: Missing data and heterogeneous temporal granularity across institutions hinder effective analysis in federated learning on electronic health records. A privacy-preserving approach is needed to leverage cross-institution data for temporal imputation and downstream tasks like sepsis prediction.

Method: Federated Markov Imputation (FMI) enables ICUs to collaboratively build a global Markov-transition model for temporal imputation in a privacy-preserving fashion by aggregating local transition models across sites.

Result: FMI outperforms local imputation baselines on a real-world sepsis onset prediction task using MIMIC-IV, with larger relative gains in scenarios featuring irregular sampling intervals across ICUs.

Conclusion: FMI provides a privacy-preserving framework for cross-institution collaboration to learn global temporal-imputation models, improving downstream predictive performance in heterogeneous, time-series electronic health record data.

Abstract: Missing data is a persistent challenge in federated learning on electronic
health records, particularly when institutions collect time-series data at
varying temporal granularities. To address this, we propose Federated Markov
Imputation (FMI), a privacy-preserving method that enables Intensive Care Units
(ICUs) to collaboratively build global transition models for temporal
imputation. We evaluate FMI on a real-world sepsis onset prediction task using
the MIMIC-IV dataset and show that it outperforms local imputation baselines,
especially in scenarios with irregular sampling intervals across ICUs.

</details>


### [140] [StyleBench: Evaluating thinking styles in Large Language Models](https://arxiv.org/abs/2509.20868)
*Junyu Guo,Shangding Gu,Ming Jin,Costas Spanos,Javad Lavaei*

Main category: cs.LG

TL;DR: StyleBench benchmarks five reasoning styles (CoT, ToT, AoT, SoT, CoD) across five tasks and 15 open-source models; finds no universal best style; effectiveness depends on model scale and task; open-source at GitHub.


<details>
  <summary>Details</summary>
Motivation: To clarify how reasoning styles interact with model architecture and task type, enabling principled prompting choices and improvements in LLM performance.

Method: Large-scale evaluation across five reasoning styles, five tasks, 15 open-source models (270M–120B) from major families; analysis of success patterns, failure modes, and scale effects; open-sourcing the benchmark.

Result: No single style dominates; AoT/ToT excel in open-ended tasks with large models; SoT/CoD yield efficiency on well-defined tasks; smaller models often ignore instructions and guess; robustness correlates with scale.

Conclusion: StyleBench provides a practical roadmap for selecting reasoning strategies under constraints and is released to the community.

Abstract: The effectiveness of Large Language Models (LLMs) is heavily influenced by
the reasoning strategies, or styles of thought, employed in their prompts.
However, the interplay between these reasoning styles, model architecture, and
task type remains poorly understood. To address this, we introduce StyleBench,
a comprehensive benchmark for systematically evaluating reasoning styles across
diverse tasks and models. We assess five representative reasoning styles,
including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought
(AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning
tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral,
Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our
large-scale analysis reveals that no single style is universally optimal. We
demonstrate that strategy efficacy is highly contingent on both model scale and
task type: search-based methods (AoT, ToT) excel in open-ended problems but
require large-scale models, while concise styles (SoT, CoD) achieve radical
efficiency gains on well-defined tasks. Furthermore, we identify key behavioral
patterns: smaller models frequently fail to follow output instructions and
default to guessing, while reasoning robustness emerges as a function of scale.
Our findings offer a crucial roadmap for selecting optimal reasoning strategies
based on specific constraints, we open source the benchmark in
https://github.com/JamesJunyuGuo/Style_Bench.

</details>


### [141] [Model-Based Reinforcement Learning under Random Observation Delays](https://arxiv.org/abs/2509.20869)
*Armin Karamzade,Kyungmin Kim,JB Lanier,Davide Corsi,Roy Fox*

Main category: cs.LG

TL;DR: A delay-aware model-based RL framework for random sensor delays in POMDPs; introduces belief-state filtering of streaming observations and integrates it into Dreamer; outperforms delay-aware MDP baselines and is robust to delay distribution shifts, with positive results on simulated robotic tasks.


<details>
  <summary>Details</summary>
Motivation: Real-world environments have non-negligible observation delays and out-of-sequence data. Standard RL often assumes instantaneous perception; naive remedies (e.g., stacking observations) are insufficient for reliability in such delay-prone settings.

Method: Develop a model-based filtering process that sequentially updates the agent's belief state as observations arrive over time, and construct a delay-aware framework that embeds this filtering into model-based RL (applied to Dreamer). Evaluate against delay-aware MDP baselines and practical heuristics, including robotic-task simulations.

Result: The proposed method consistently outperforms delay-aware MDP baselines, exhibits robustness to shifts in delay distributions during deployment, and yields improvements on simulated robotic tasks compared to common heuristics.

Conclusion: Explicitly modeling observation delays via a sequential belief-state filtering mechanism within a delay-aware model-based RL framework improves performance and robustness in delayed observation environments, with demonstrated benefits in Dreamer-based experiments and robotics-like tasks.

Abstract: Delays frequently occur in real-world environments, yet standard
reinforcement learning (RL) algorithms often assume instantaneous perception of
the environment. We study random sensor delays in POMDPs, where observations
may arrive out-of-sequence, a setting that has not been previously addressed in
RL. We analyze the structure of such delays and demonstrate that naive
approaches, such as stacking past observations, are insufficient for reliable
performance. To address this, we propose a model-based filtering process that
sequentially updates the belief state based on an incoming stream of
observations. We then introduce a simple delay-aware framework that
incorporates this idea into model-based RL, enabling agents to effectively
handle random delays. Applying this framework to Dreamer, we compare our
approach to delay-aware baselines developed for MDPs. Our method consistently
outperforms these baselines and demonstrates robustness to delay distribution
shifts during deployment. Additionally, we present experiments on simulated
robotic tasks, comparing our method to common practical heuristics and
emphasizing the importance of explicitly modeling observation delays.

</details>


### [142] [Distribution-Controlled Client Selection to Improve Federated Learning Strategies](https://arxiv.org/abs/2509.20877)
*Christoph Düsing,Philipp Cimiano*

Main category: cs.LG

TL;DR: Proposes distribution-controlled client selection in federated learning to align clients' label distributions with a target (balanced or federation-wide) to mitigate data imbalance; shows that balancing distribution helps with local imbalance, while federation-wide distribution helps with global imbalance, validated on three FL strategies and two datasets.


<details>
  <summary>Details</summary>
Motivation: Data imbalance across clients degrades FL performance; existing client selection methods may not address this efficiently. There is a need for a strategy that aligns client contributions with a target distribution to improve performance under local and global imbalance.

Method: Extend FL strategies with a client selection mechanism that picks active clients whose local label distribution best matches a specified target distribution (balanced or federations combined). Evaluate this distribution-controlled selection on three FL strategies and two datasets.

Result: Empirical verification shows improvements; local imbalance benefits most from alignment to balanced distribution; global imbalance benefits most from alignment to the federation's combined label distribution.

Conclusion: Distribution-controlled client selection is effective for mitigating data imbalance in FL; choice of target distribution should be guided by whether the imbalance is local or global; compatible with multiple FL strategies and datasets.

Abstract: Federated learning (FL) is a distributed learning paradigm that allows
multiple clients to jointly train a shared model while maintaining data
privacy. Despite its great potential for domains with strict data privacy
requirements, the presence of data imbalance among clients is a thread to the
success of FL, as it causes the performance of the shared model to decrease. To
address this, various studies have proposed enhancements to existing FL
strategies, particularly through client selection methods that mitigate the
detrimental effects of data imbalance. In this paper, we propose an extension
to existing FL strategies, which selects active clients that best align the
current label distribution with one of two target distributions, namely a
balanced distribution or the federations combined label distribution.
Subsequently, we empirically verify the improvements through our
distribution-controlled client selection on three common FL strategies and two
datasets. Our results show that while aligning the label distribution with a
balanced distribution yields the greatest improvements facing local imbalance,
alignment with the federation's combined label distribution is superior for
global imbalance.

</details>


### [143] [Improving Early Sepsis Onset Prediction Through Federated Learning](https://arxiv.org/abs/2509.20885)
*Christoph Düsing,Philipp Cimiano*

Main category: cs.LG

TL;DR: Federated, attention-enhanced LSTM for multi-center sepsis onset prediction with a variable prediction window; achieves near-centralized performance and notably improves early detection while reducing overhead.


<details>
  <summary>Details</summary>
Motivation: Sepsis prediction is hampered by limited, heterogeneous data and privacy concerns. Federated learning enables cross-institution collaboration without sharing raw data, but modeling across variable horizons remains challenging. This work targets early detection and resource efficiency.

Method: A federated, attention-enhanced LSTM model trained on multi-center ICU data. The model supports a variable prediction horizon (instead of a fixed window) and includes in-depth temporal analysis to evaluate early detection. Comparisons are made against centralized baselines and fixed-window approaches, with focus on temporal performance and overhead.

Result: The FL model achieved performance close to a centralized model, with improvements in early sepsis predictions. The variable prediction window did not significantly hurt accuracy and reduced computation, communication, and organizational overhead.

Conclusion: Federated, horizon-flexible LSTM with attention is effective for sepsis onset prediction, enabling privacy-preserving collaboration that largely matches centralized performance and improves early detection while lowering overhead.

Abstract: Early and accurate prediction of sepsis onset remains a major challenge in
intensive care, where timely detection and subsequent intervention can
significantly improve patient outcomes. While machine learning models have
shown promise in this domain, their success is often limited by the amount and
diversity of training data available to individual hospitals and Intensive Care
Units (ICUs). Federated Learning (FL) addresses this issue by enabling
collaborative model training across institutions without requiring data
sharing, thus preserving patient privacy. In this work, we propose a federated,
attention-enhanced Long Short-Term Memory model for sepsis onset prediction,
trained on multi-centric ICU data. Unlike existing approaches that rely on
fixed prediction windows, our model supports variable prediction horizons,
enabling both short- and long-term forecasting in a single unified model.
During analysis, we put particular emphasis on the improvements through our
approach in terms of early sepsis detection, i.e., predictions with large
prediction windows by conducting an in-depth temporal analysis. Our results
prove that using FL does not merely improve overall prediction performance
(with performance approaching that of a centralized model), but is particularly
beneficial for early sepsis onset prediction. Finally, we show that our choice
of employing a variable prediction window rather than a fixed window does not
hurt performance significantly but reduces computational, communicational, and
organizational overhead.

</details>


### [144] [Deterministic Discrete Denoising](https://arxiv.org/abs/2509.20896)
*Hideyuki Suzuki,Hiroshi Yamashita*

Main category: cs.LG

TL;DR: A deterministic denoising method for discrete-state diffusion using a derandomized, herding-like reverse process improves efficiency and sample quality without retraining, extending deterministic diffusion ideas to discrete spaces.


<details>
  <summary>Details</summary>
Motivation: To remove stochasticity and reliance on continuous embeddings in discrete diffusion models, aiming for faster, more deterministic sampling and simpler training dynamics.

Method: Replace the stochastic reverse denoising with a derandomized, herding-based Markov process that exhibits weakly chaotic dynamics to produce deterministic discrete-state transitions; no retraining or continuous state embeddings required.

Result: Demonstrates consistent gains in efficiency and sample quality for text and image generation, supporting the viability of deterministic reverse processes in discrete diffusion.

Conclusion: Deterministic reverse processes are viable in discrete-state diffusion and a simple derandomization can meaningfully enhance their performance and practicality.

Abstract: We propose a deterministic denoising algorithm for discrete-state diffusion
models based on Markov chains. The generative reverse process is derandomized
by introducing a variant of the herding algorithm with weakly chaotic dynamics,
which induces deterministic discrete state transitions. Our approach is a
direct replacement for the stochastic denoising process, requiring neither
retraining nor continuous state embeddings. We demonstrate consistent
improvements in both efficiency and sample quality on text and image generation
tasks. Thus, this simple derandomization approach is expected to enhance the
significance of discrete diffusion in generative modeling. Furthermore, our
results reveal that deterministic reverse processes, well established in
continuous diffusion, can also be effective in discrete state spaces.

</details>


### [145] [Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales](https://arxiv.org/abs/2509.20913)
*Ariadna Albors Zumel,Michele Tizzoni,Gian Maria Campedelli*

Main category: cs.LG

TL;DR: A ConvLSTM-based crime forecasting framework that integrates micro-level mobility, historic crime, and sociodemographic data to improve predictions at fine spatial-temporal scales across four U.S. cities, with mobility+demographics yielding the best performance; longer sequences help violent crimes, shorter sequences help property crimes.


<details>
  <summary>Details</summary>
Motivation: Crime forecasting at granular scales remains challenging; incorporating micro-mobility data alongside traditional predictors can capture dynamic urban activity patterns, potentially improving predictive accuracy and informing policing and resource allocation.

Method: Data from Baltimore, Chicago, Los Angeles, and Philadelphia (2019-2023): crime incidents, ACS sociodemographics, and Advan mobility. Data aggregated into 0.077 sq mile (0.2 km2) grid cells. A ConvLSTM forecasts 12 hours ahead using 14-day and 2-day input sequences. Baselines: logistic regression, random forest, standard LSTM. Evaluated with recall, precision, F1.

Result: Mobility features improve predictions, especially with shorter input sequences. The best results occur when mobility and sociodemographic features are both used, with the deep learning model achieving highest recall, precision, and F1 across all four cities. Longer input sequences boost violent crime predictions; shorter sequences favor property crimes.

Conclusion: Integrating diverse data sources, including micro-mobility, enhances spatiotemporal crime forecasting. Deep learning offers advantages and limitations at fine-grained scales; the approach supports more accurate and nuanced forecasting, informing policy and operations while highlighting data requirements and potential trade-offs.

Abstract: Objectives: To develop a deep learning framework to evaluate if and how
incorporating micro-level mobility features, alongside historical crime and
sociodemographic data, enhances predictive performance in crime forecasting at
fine-grained spatial and temporal resolutions.
  Methods: We advance the literature on computational methods and crime
forecasting by focusing on four U.S. cities (i.e., Baltimore, Chicago, Los
Angeles, and Philadelphia). We employ crime incident data obtained from each
city's police department, combined with sociodemographic data from the American
Community Survey and human mobility data from Advan, collected from 2019 to
2023. This data is aggregated into grids with equally sized cells of 0.077 sq.
miles (0.2 sq. kms) and used to train our deep learning forecasting model, a
Convolutional Long Short-Term Memory (ConvLSTM) network, which predicts crime
occurrences 12 hours ahead using 14-day and 2-day input sequences. We also
compare its performance against three baseline models: logistic regression,
random forest, and standard LSTM.
  Results: Incorporating mobility features improves predictive performance,
especially when using shorter input sequences. Noteworthy, however, the best
results are obtained when both mobility and sociodemographic features are used
together, with our deep learning model achieving the highest recall, precision,
and F1 score in all four cities, outperforming alternative methods. With this
configuration, longer input sequences enhance predictions for violent crimes,
while shorter sequences are more effective for property crimes.
  Conclusion: These findings underscore the importance of integrating diverse
data sources for spatiotemporal crime forecasting, mobility included. They also
highlight the advantages (and limits) of deep learning when dealing with
fine-grained spatial and temporal scales.

</details>


### [146] [Energy saving in off-road vehicles using leakage compensation technique](https://arxiv.org/abs/2509.20926)
*Gyan Wrat,J. Das*

Main category: cs.LG

TL;DR: An energy-efficiency improvement for hydraulic linear actuators in heavy equipment by replacing a conventional PDCV with a proportional flow control valve (PFCV) that uses artificial leakage to bypass excess pump flow, achieving 8.5% energy savings; PID position control is tuned by a fuzzy controller; validated via MATLAB/Simulink and experiments.


<details>
  <summary>Details</summary>
Motivation: Reduce energy losses (heat) and environmental impact of hydraulic actuation in heavy earth-moving equipment; improve overall actuator energy efficiency and reduce operating costs.

Method: Compare two hydraulic circuits: (1) conventional PDCV with a pressure relief valve; (2) PFCV with artificial leakage between actuator ends. The PFCV bypasses extra flow during position control to lower energy loss. Implement position control with a PID controller tuned by a fuzzy controller. Simulate in MATLAB/Simulink and validate against experiments.

Result: The PFCV-based circuit is 8.5% more energy efficient than the conventional PDCV-based circuit. Energy savings arise from bypassing extra flow; simulation results agree with experimental data.

Conclusion: The proposed PFCV approach can significantly improve energy efficiency of linear actuators in heavy earth-moving equipment, reducing environmental impact and operating costs.

Abstract: The article focuses on enhancing the energy efficiency of linear actuators
used in heavy earth moving equipment, particularly in the booms ofexcavation
equipment. Two hydraulic circuits are compared in terms of energy efficiency,
with one using a conventional proportional directionalcontrol valve (PDCV) and
the other using an innovative solution of proportional flow control valve
(PFCV) with artificial leakage between thetwo ends of the actuator. The PFCV
reduces energy loss in the form of heat by bypassing the extra flow from the
pump during position control,unlike the PDCV that uses a pressure relief valve.
The hydraulic circuit using PFCV is found to be 8.5% more energy efficient than
theconventional circuit using PDCV. The article also discusses the position
control of the actuator, which is achieved using a PID controller tuned by a
fuzzy controller. Thesimulation of the hydraulic circuit is carried out using
MATLAB/Simulink, and the results are compared with experiments. Overall, the
proposedapproach could lead to significant improvements in the energy
efficiency of linear actuators used in heavy earth moving equipment,
therebyreducing their environmental impact and operating costs.

</details>


### [147] [GenFacts-Generative Counterfactual Explanations for Multi-Variate Time Series](https://arxiv.org/abs/2509.20936)
*Sarah Seifi,Anass Ibrahimi,Tobias Sukianto,Cecilia Carbonelli,Lorenzo Servadei,Robert Wille*

Main category: cs.LG

TL;DR: A generative framework GenFacts that produces plausible, user-centered counterfactuals for multivariate time series using a class-discriminative VAE with realism constraints; it outperforms baselines in plausibility and interpretability on radar gesture and handwritten trajectories.


<details>
  <summary>Details</summary>
Motivation: Existing counterfactual explanations for multivariate time series often yield invalid, implausible, or unintuitive changes. There is a need for plausible, actionable counterfactuals that improve user trust and interpretability.

Method: GenFacts uses a class-discriminative variational autoencoder augmented with contrastive and classification-consistency objectives, prototype-based initialization, and realism-constrained optimization to generate plausible counterfactuals. Evaluations are conducted on radar gesture data and handwritten letter trajectories.

Result: GenFacts achieves a 18.7 percentage-point improvement in plausibility over state-of-the-art baselines and attains the highest interpretability scores in a human study across the two datasets.

Conclusion: Plausibility and user-centered interpretability, rather than sparsity, are key to actionable counterfactuals in time-series; GenFacts demonstrates that incorporating realism constraints and user-focused objectives yields more useful explanations.

Abstract: Counterfactual explanations aim to enhance model transparency by showing how
inputs can be minimally altered to change predictions. For multivariate time
series, existing methods often generate counterfactuals that are invalid,
implausible, or unintuitive. We introduce GenFacts, a generative framework
based on a class-discriminative variational autoencoder. It integrates
contrastive and classification-consistency objectives, prototype-based
initialization, and realism-constrained optimization. We evaluate GenFacts on
radar gesture data as an industrial use case and handwritten letter
trajectories as an intuitive benchmark. Across both datasets, GenFacts
outperforms state-of-the-art baselines in plausibility (+18.7%) and achieves
the highest interpretability scores in a human study. These results highlight
that plausibility and user-centered interpretability, rather than sparsity
alone, are key to actionable counterfactuals in time series data.

</details>


### [148] [Why Attention Fails: The Degeneration of Transformers into MLPs in Time Series Forecasting](https://arxiv.org/abs/2509.20942)
*Zida Liang,Jiayi Zhu,Weiqiang Sun*

Main category: cs.LG

TL;DR: Time-series transformers often underperform; authors show attention fails to function as intended, with transformers degenerating to MLPs due to embedding-induced latent-space issues, revealed via interpretable data and theoretical analysis.


<details>
  <summary>Details</summary>
Motivation: To understand why time-series transformers fail and rarely outperform simple baselines; specifically, what role attention and embeddings play.

Method: Progressively convert transformer blocks into MLPs to assess attention; create an interpretable dataset to examine attention behavior; theoretical analysis of embedding methods and latent space structure to identify root causes.

Result: Attention mechanism does not operate as expected; transformer blocks often become equivalent to simple MLPs in current TSTs; embedding methods fail to produce a well-structured latent space; deeper causes of embedding failure identified.

Conclusion: Address embedding design and the way attention is used in time-series transformers; future work should aim to restore a meaningful latent space and functional attention to unlock potential advantages.

Abstract: Transformer-based architectures achieved high performance in natural language
processing and computer vision, yet many studies have shown that they have not
demonstrated a clear advantage in time series forecasting and even underperform
simple linear baselines in some cases. However, most of these studies have not
thoroughly explored the reasons behind the failure of transformers. To better
understand time-series transformers(TST), we designed a series of experiments,
progressively modifying transformers into MLPs to investigate the impact of the
attention mechanism. Surprisingly, transformer blocks often degenerate into
simple MLPs in existing time-series transformers. We designed a interpretable
dataset to investigate the reasons behind the failure of the attention
mechanism and revealed that the attention mechanism is not working in the
expected way. We theoretically analyzed the reasons behind this phenomenon,
demonstrating that the current embedding methods fail to allow transformers to
function in a well-structured latent space, and further analyzed the deeper
underlying causes of the failure of embedding.

</details>


### [149] [Decoupled-Value Attention for Prior-Data Fitted Networks: GP Inference for Physical Equations](https://arxiv.org/abs/2509.20950)
*Kaustubh Sharma,Simardeep Singh,Parikshit Pareek*

Main category: cs.LG

TL;DR: Decoupled-Value Attention (DVA) improves PFNs by using input-based similarities and value-based label propagation, achieving GP-like updates without kernels, with significant gains in high-dimensional regression and faster GP-like surrogates.


<details>
  <summary>Details</summary>
Motivation: PFNs aim to replace expensive GP inference with fast surrogates; high-dimensional regression challenges and the limited effectiveness of standard Transformer attention motivate a kernel-free, GP-m-inspired attention mechanism that scales.

Method: Introduce Decoupled-Value Attention (DVA) where similarities are computed from inputs only and labels propagate through values. This mirrors the Gaussian-process update without requiring kernels. Evaluate with localized attention and compare backbone architectures (CNN vs Transformer) on 5D, 10D, and 64D power-flow settings.

Result: Localized attention reduces out-of-sample validation loss by >50% in 5D and 10D; attention role outruns backbone choice (CNN PFNs can match Transformer PFNs); achieve 64D power-flow approximations with MAE ~1e-3 and >80x faster than exact GP inference.

Conclusion: Attention-driven scaling, not backbone architecture, governs PFN performance. DVA provides GP-like updates in a kernel-free framework, enabling fast, accurate surrogates for high-dimensional physical systems.

Abstract: Prior-data fitted networks (PFNs) are a promising alternative to
time-consuming Gaussian Process (GP) inference for creating fast surrogates of
physical systems. PFN reduces the computational burden of GP-training by
replacing Bayesian inference in GP with a single forward pass of a learned
prediction model. However, with standard Transformer attention, PFNs show
limited effectiveness on high-dimensional regression tasks. We introduce
Decoupled-Value Attention (DVA)-- motivated by the GP property that the
function space is fully characterized by the kernel over inputs and the
predictive mean is a weighted sum of training targets. DVA computes
similarities from inputs only and propagates labels solely through values.
Thus, the proposed DVA mirrors the Gaussian-process update while remaining
kernel-free. We demonstrate that the crucial factor for scaling PFNs is the
attention rule rather than the architecture itself. Specifically, our results
demonstrate that (a) localized attention consistently reduces out-of-sample
validation loss in PFNs across different dimensional settings, with validation
loss reduced by more than 50% in five- and ten-dimensional cases, and (b) the
role of attention is more decisive than the choice of backbone architecture,
showing that CNN-based PFNs can perform at par with their Transformer-based
counterparts. The proposed PFNs provide 64-dimensional power flow equation
approximations with a mean absolute error of the order of 1E-3, while being
over 80x faster than exact GP inference.

</details>


### [150] [Flow Matching in the Low-Noise Regime: Pathologies and a Contrastive Remedy](https://arxiv.org/abs/2509.20952)
*Weili Zeng,Yichao Yan*

Main category: cs.LG

TL;DR: Flow matching suffers a low-noise pathology causing ill-conditioning and degraded representations; Local Contrastive Flow (LCF) mitigates this by using contrastive alignment at small noise while keeping standard flow matching at higher noise.


<details>
  <summary>Details</summary>
Motivation: Identify fundamental instability in flow-matching objectives as noise → 0; link ill-conditioning to the objective structure; show impact on optimization and semantic encoding.

Method: Theoretical analysis of the flow-matching objective; propose LCF hybrid training: replace direct velocity regression with contrastive feature alignment at small noise; preserve standard flow matching at moderate/high noise.

Result: LCF improves optimization convergence and stabilizes representation quality empirically.

Conclusion: Addressing low-noise pathology is essential to unlock flow matching's full potential for generation and representation learning.

Abstract: Flow matching has recently emerged as a powerful alternative to diffusion
models, providing a continuous-time formulation for generative modeling and
representation learning. Yet, we show that this framework suffers from a
fundamental instability in the low-noise regime. As noise levels approach zero,
arbitrarily small perturbations in the input can induce large variations in the
velocity target, causing the condition number of the learning problem to
diverge. This ill-conditioning not only slows optimization but also forces the
encoder to reallocate its limited Jacobian capacity toward noise directions,
thereby degrading semantic representations. We provide the first theoretical
analysis of this phenomenon, which we term the low-noise pathology,
establishing its intrinsic link to the structure of the flow matching
objective. Building on these insights, we propose Local Contrastive Flow (LCF),
a hybrid training protocol that replaces direct velocity regression with
contrastive feature alignment at small noise levels, while retaining standard
flow matching at moderate and high noise. Empirically, LCF not only improves
convergence speed but also stabilizes representation quality. Our findings
highlight the critical importance of addressing low-noise pathologies to unlock
the full potential of flow matching for both generation and representation
learning.

</details>


### [151] [Alignment Unlocks Complementarity: A Framework for Multiview Circuit Representation Learning](https://arxiv.org/abs/2509.20968)
*Zhengyuan Shi,Jingxin Wang,Wentao Jiang,Chengyu Ma,Ziyang Zheng,Zhufei Chu,Weikang Qian,Qiang Xu*

Main category: cs.LG

TL;DR: Functional alignment enables multiview self-supervised learning for Boolean circuits by first aligning view representations with an Equivalence Alignment Loss, then applying a multiview masked modeling objective via MixGate.


<details>
  <summary>Details</summary>
Motivation: Cross-view structural heterogeneity between AIG and XMG makes naive self-supervision ineffective; functional alignment is needed to fuse views meaningfully.

Method: MixGate framework with a training curriculum: first train a shared, function-aware representation space using Equivalence Alignment Loss; then introduce a multiview masked modeling objective that leverages aligned views.

Result: Ablation studies and extensive experiments show the alignment-first strategy turns masked modeling into a strong performance driver.

Conclusion: Functional alignment is a necessary precondition for effective multiview self-supervision in Boolean circuit representations; MixGate's curriculum-based alignment followed by masked modeling yields improvements.

Abstract: Multiview learning on Boolean circuits holds immense promise, as different
graph-based representations offer complementary structural and semantic
information. However, the vast structural heterogeneity between views, such as
an And-Inverter Graph (AIG) versus an XOR-Majority Graph (XMG), poses a
critical barrier to effective fusion, especially for self-supervised techniques
like masked modeling. Naively applying such methods fails, as the cross-view
context is perceived as noise. Our key insight is that functional alignment is
a necessary precondition to unlock the power of multiview self-supervision. We
introduce MixGate, a framework built on a principled training curriculum that
first teaches the model a shared, function-aware representation space via an
Equivalence Alignment Loss. Only then do we introduce a multiview masked
modeling objective, which can now leverage the aligned views as a rich,
complementary signal. Extensive experiments, including a crucial ablation
study, demonstrate that our alignment-first strategy transforms masked modeling
from an ineffective technique into a powerful performance driver.

</details>


### [152] [Knowledgeable Language Models as Black-Box Optimizers for Personalized Medicine](https://arxiv.org/abs/2509.20975)
*Michael S. Yao,Osbert Bastani,Alma Andersson,Tommaso Biancalani,Aïcha Bentaieb,Claudia Iriondo*

Main category: cs.LG

TL;DR: A knowledge-guided, prompt-based optimization framework (LEON) uses LLMs as black-box optimizers with domain priors to propose personalized treatments, outperforming baselines in individualized therapy design.


<details>
  <summary>Details</summary>
Motivation: Surrogate models for treatment fitness often fail to generalize to unseen patient-treatment pairs. Incorporating domain-specific priors (textbooks, knowledge graphs) via LLMs and prompting offers a principled signal to guide personalized treatment design without task-specific fine-tuning.

Method: LEON (Entropy-guided Optimization with kNowledgeable priors) employs optimization by prompting: LLMs act as stochastic engines to generate treatment designs, guided by domain knowledge priors embedded in prompts; no task-specific fine-tuning is required.

Result: Experiments on real-world optimization tasks show LEON outperforms both traditional methods and other LLM-based approaches in proposing individualized treatments for patients.

Conclusion: Leveraging LLMs with domain priors through entropy-guided optimization provides a principled path to effective personalized treatment design, addressing surrogate-model generalization weaknesses and enabling scalable, knowledge-informed decision making.

Abstract: The goal of personalized medicine is to discover a treatment regimen that
optimizes a patient's clinical outcome based on their personal genetic and
environmental factors. However, candidate treatments cannot be arbitrarily
administered to the patient to assess their efficacy; we often instead have
access to an in silico surrogate model that approximates the true fitness of a
proposed treatment. Unfortunately, such surrogate models have been shown to
fail to generalize to previously unseen patient-treatment combinations. We
hypothesize that domain-specific prior knowledge - such as medical textbooks
and biomedical knowledge graphs - can provide a meaningful alternative signal
of the fitness of proposed treatments. To this end, we introduce LLM-based
Entropy-guided Optimization with kNowledgeable priors (LEON), a mathematically
principled approach to leverage large language models (LLMs) as black-box
optimizers without any task-specific fine-tuning, taking advantage of their
ability to contextualize unstructured domain knowledge to propose personalized
treatment plans in natural language. In practice, we implement LEON via
'optimization by prompting,' which uses LLMs as stochastic engines for
proposing treatment designs. Experiments on real-world optimization tasks show
LEON outperforms both traditional and LLM-based methods in proposing
individualized treatments for patients.

</details>


### [153] [CLUE: Conflict-guided Localization for LLM Unlearning Framework](https://arxiv.org/abs/2509.20977)
*Hang Chen,Jiaying Zhu,Xinyu Yang,Wenya Wang*

Main category: cs.LG

TL;DR: CLUE uses circuit discovery to disentangle forget vs retain circuits in LLM unlearning, encodes them as CNFs, uses SAT to assign neurons to forget or retain, and applies targeted fine-tuning—yielding precise erasure with preserved non-target capabilities, outperforming localization methods.


<details>
  <summary>Details</summary>
Motivation: Current localization-based unlearning methods treat forgetting and retaining as a single entangled group, risking excessive forgetting or incomplete erasure. There is a need for disentangled, circuit-level control to selectively erase undesired knowledge while preserving essential skills.

Method: Identify forget and retain circuits via circuit discovery, compose circuits from important neurons, transform circuits into conjunctive normal forms (CNFs), solve SAT to assign each neuron to forget or retain, and apply category-specific fine-tuning strategies to different neuron classes.

Result: Experiments show CLUE achieves superior forget efficacy and retains more utility compared to existing localization approaches, due to precise neural localization and targeted interventions.

Conclusion: By disentangling forgetting and retaining circuits and translating them into CNFs for targeted fine-tuning, CLUE enables precise and controlled LLM unlearning with reduced over-forgetting and retained capabilities.

Abstract: The LLM unlearning aims to eliminate the influence of undesirable data
without affecting causally unrelated information. This process typically
involves using a forget set to remove target information, alongside a retain
set to maintain non-target capabilities. While recent localization-based
methods demonstrate promise in identifying important neurons to be unlearned,
they fail to disentangle neurons responsible for forgetting undesirable
knowledge or retaining essential skills, often treating them as a single
entangled group. As a result, these methods apply uniform interventions,
risking catastrophic over-forgetting or incomplete erasure of the target
knowledge. To address this, we turn to circuit discovery, a mechanistic
interpretability technique, and propose the Conflict-guided Localization for
LLM Unlearning framEwork (CLUE). This framework identifies the forget and
retain circuit composed of important neurons, and then the circuits are
transformed into conjunctive normal forms (CNF). The assignment of each neuron
in the CNF satisfiability solution reveals whether it should be forgotten or
retained. We then provide targeted fine-tuning strategies for different
categories of neurons. Extensive experiments demonstrate that, compared to
existing localization methods, CLUE achieves superior forget efficacy and
retain utility through precise neural localization.

</details>


### [154] [FracAug: Fractional Augmentation boost Graph-level Anomaly Detection under Limited Supervision](https://arxiv.org/abs/2509.20978)
*Xiangyu Dong,Xingyi Zhang,Sibo Wang*

Main category: cs.LG

TL;DR: FracAug is a model-agnostic augmentation framework for graph-level anomaly detection that generates semantically-preserving fractional graph variants and uses mutual pseudo-labeling to improve GNN performance, showing gains across many models and datasets.


<details>
  <summary>Details</summary>
Motivation: Graph-level anomaly detection suffers from high labeling costs and dataset imbalance, and existing GNN augmentation methods are often heuristic and lack semantics-preserving capabilities.

Method: FracAug learns semantics within given graphs to synthesize fractional graph variants (semantic-preserving, multi-scale topology aware) using a novel weighted distance-aware margin loss. It then uses predictions from both original and augmented graphs to pseudo-label unlabeled data in an iterative, model-agnostic training loop.

Result: Across 14 GNNs on 12 real-world datasets, FracAug yields consistent gains, boosting AUROC by up to 5.72%, AUPRC by up to 7.23%, and F1-score by up to 4.18%.

Conclusion: FracAug is a universal plug-in module for GNNs that effectively addresses labeling costs and data imbalance in graph-level anomaly detection by generating diverse, semantic-preserving graph variants and leveraging mutual pseudo-labeling.

Abstract: Graph-level anomaly detection (GAD) is critical in diverse domains such as
drug discovery, yet high labeling costs and dataset imbalance hamper the
performance of Graph Neural Networks (GNNs). To address these issues, we
propose FracAug, an innovative plug-in augmentation framework that enhances
GNNs by generating semantically consistent graph variants and pseudo-labeling
with mutual verification. Unlike previous heuristic methods, FracAug learns
semantics within given graphs and synthesizes fractional variants, guided by a
novel weighted distance-aware margin loss. This captures multi-scale topology
to generate diverse, semantic-preserving graphs unaffected by data imbalance.
Then, FracAug utilizes predictions from both original and augmented graphs to
pseudo-label unlabeled data, iteratively expanding the training set. As a
model-agnostic module compatible with various GNNs, FracAug demonstrates
remarkable universality and efficacy: experiments across 14 GNNs on 12
real-world datasets show consistent gains, boosting average AUROC, AUPRC, and
F1-score by up to 5.72%, 7.23%, and 4.18%, respectively.

</details>


### [155] [Toward Robust and Efficient ML-Based GPU Caching for Modern Inference](https://arxiv.org/abs/2509.20979)
*Peng Chen,Jiaji Zhang,Hailiang Zhao,Yirong Zhang,Jiahong Yu,Xueyan Tang,Yixuan Wang,Hao Li,Jianping Zou,Gang Xiong,Kingsum Chow,Shuibing He,Shuiguang Deng*

Main category: cs.LG

TL;DR: A practical, robust learning-based GPU caching framework (LCR) with LARU that adapts to prediction accuracy, delivering consistent gains and robustness across workloads; achieves throughput gains in DLRM and TTFT reductions in LLM scenarios.


<details>
  <summary>Details</summary>
Motivation: Cache inefficiency is a major bottleneck in modern GPU inference: embedding hit rates drive throughput in recommender models, and KV-cache misses raise time-to-first-token in large language models. Traditional LRU struggles with structured access patterns; learning-based caching suffers from fragility to mispredictions, high overhead, or conservative designs. There is a need for a practical approach that leverages learning while remaining robust and efficient, bridging empirical gains with theory.

Method: Introduce the LCR framework and its core algorithm LARU. LARU augments LRU with machine-learned predictions and online error estimation to adapt dynamically to prediction accuracy. LCR provides a robust, online learning-based caching policy that delivers high performance when predictions are accurate and gracefully degrades toward LRU when predictions are poor, balancing efficiency and practicality across GPU inference workloads.

Result: Experiments show consistent gains: up to 24.2% throughput improvement in DLRM and up to 28.3% reduction in P99 TTFT for LLM scenarios. LCR outperforms widely used inference systems and remains stable under poor predictions, demonstrating practical robustness.

Conclusion: LCR bridges the gap between empirical learning-based caching and theoretical advances, delivering near-optimal performance with accurate predictions and graceful degradation to near-LRU under inaccuracies, thereby offering practical, robust gains for real-world GPU inference workloads.

Abstract: In modern GPU inference, cache efficiency remains a major bottleneck. In
recommendation models, embedding hit rates largely determine throughput, while
in large language models, KV-cache misses substantially increase
time-to-first-token (TTFT). Heuristic policies such as \textsc{LRU} often
struggle under structured access patterns. Learning-based approaches are
promising, but in practice face two major limitations: they degrade sharply
when predictions are inaccurate, or they gain little even with accurate
predictions due to conservative designs. Some also incur high overhead, further
limiting practicality.
  We present \textsc{LCR}, a practical framework for learning-based GPU caching
that delivers performance gains while ensuring robustness and efficiency. Its
core algorithm, \textsc{LARU}, enhances \textsc{LRU} with machine-learned
predictions and dynamically adapts to prediction accuracy through online error
estimation. When predictions are accurate, \textsc{LARU} achieves near-optimal
performance. With inaccurate predictions, it degrades gracefully to
near-\textsc{LRU} performance. With \textsc{LCR}, we bridge the gap between
empirical progress and theoretical advances in learning-based caching.
  Experiments show that \textsc{LCR} delivers consistent gains under realistic
conditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\%
and reduces P99 TTFT by up to 28.3\%, outperforming widely used inference
systems. Even under poor predictions, its performance remains stable,
demonstrating practical robustness.

</details>


### [156] [Learning Ising Models under Hard Constraints using One Sample](https://arxiv.org/abs/2509.20993)
*Rohan Chauhan,Ioannis Panageas*

Main category: cs.LG

TL;DR: Near-linear-time estimator for beta in a single-sample truncated Ising model via pseudolikelihood; achieves O(Δ^3/√n) consistency under k-SAT truncation with k ≥ log(d^2 k) Δ^3.


<details>
  <summary>Details</summary>
Motivation: Estimating inverse temperature under strong truncation constraints with minimal data; extends pseudolikelihood methods to truncated Ising models; enables scalable inference in constrained graphical models.

Method: Maximization of the pseudolikelihood under truncation; builds on Daskalakis et al. STOC'19 and Galanis et al. SODA'24; handles SAT-based truncation; near-linear-time algorithm for hatβ with provable consistency.

Result: Estimator computable in nearly O(n) time; consistency error O(Δ^3/√n) for β* when k ≥ log(d^2 k) Δ^3; graph degree bounded by Δ.

Conclusion: Demonstrates that pseudolikelihood-based inference can be extended to truncated Ising models with efficient computation from a single sample; generalizes previous non-truncated results; opens doors to scalable inference under combinatorial truncations.

Abstract: We consider the problem of estimating inverse temperature parameter $\beta$
of an $n$-dimensional truncated Ising model using a single sample. Given a
graph $G = (V,E)$ with $n$ vertices, a truncated Ising model is a probability
distribution over the $n$-dimensional hypercube $\{-1,1\}^n$ where each
configuration $\mathbf{\sigma}$ is constrained to lie in a truncation set $S
\subseteq \{-1,1\}^n$ and has probability $\Pr(\mathbf{\sigma}) \propto
\exp(\beta\mathbf{\sigma}^\top A\mathbf{\sigma})$ with $A$ being the adjacency
matrix of $G$. We adopt the recent setting of [Galanis et al. SODA'24], where
the truncation set $S$ can be expressed as the set of satisfying assignments of
a $k$-SAT formula. Given a single sample $\mathbf{\sigma}$ from a truncated
Ising model, with inverse parameter $\beta^*$, underlying graph $G$ of bounded
degree $\Delta$ and $S$ being expressed as the set of satisfying assignments of
a $k$-SAT formula, we design in nearly $O(n)$ time an estimator $\hat{\beta}$
that is $O(\Delta^3/\sqrt{n})$-consistent with the true parameter $\beta^*$ for
$k \gtrsim \log(d^2k)\Delta^3.$
  Our estimator is based on the maximization of the pseudolikelihood, a notion
that has received extensive analysis for various probabilistic models without
[Chatterjee, Annals of Statistics '07] or with truncation [Galanis et al. SODA
'24]. Our approach generalizes recent techniques from [Daskalakis et al. STOC
'19, Galanis et al. SODA '24], to confront the more challenging setting of the
truncated Ising model.

</details>


### [157] [Binary Autoencoder for Mechanistic Interpretability of Large Language Models](https://arxiv.org/abs/2509.20997)
*Hakaze Cho,Haolin Yang,Brian M. Kurkoski,Naoya Inoue*

Main category: cs.LG

TL;DR: Binary Autoencoder (BAE) enforces minimal minibatch entropy on hidden activations by discretizing to 1-bit and using gradient estimation, promoting feature independence and sparsity across LLM representations; it enables reliable entropy estimation and robust feature untangling, yielding more interpretable features than baselines.


<details>
  <summary>Details</summary>
Motivation: Existing autoencoder-based interpretability methods rely on per-instance implicit regularization (e.g., L1, top-k) and do not guarantee global sparsity across instances, resulting in dense, entangled features that hinder interpretable atomization.

Method: Introduce Binary Autoencoder (BAE) that minimizes entropy over minibatches of hidden activations. Activations are discretized to 1-bit via a step function, and gradients are estimated to allow backpropagation. The approach yields two applications: (i) probabilistic entropy calculation on binary activations for analysis of inference dynamics; (ii) feature untangling to extract atomized, interpretable features from LLM hidden states, with refined feature-interpretation methods to handle numerical tokens.

Result: Empirically demonstrate two applications: entropy calculation on binary hidden activations is reliable and informative for characterizing LLM inference dynamics and in-context learning; BAE extracts az atomized features, reduces dense features, and yields the largest number of interpretable features among baselines, confirming its effectiveness as a feature extractor.

Conclusion: BAE provides a principled way to enforce global sparsity and feature independence in autoencoder-based interpretations of LLMs, enabling reliable entropy-based analyses and improved interpretability through a larger set of atomized, interpretable features.

Abstract: Existing works are dedicated to untangling atomized numerical components
(features) from the hidden states of Large Language Models (LLMs) for
interpreting their mechanism. However, they typically rely on autoencoders
constrained by some implicit training-time regularization on single training
instances (i.e., $L_1$ normalization, top-k function, etc.), without an
explicit guarantee of global sparsity among instances, causing a large amount
of dense (simultaneously inactive) features, harming the feature sparsity and
atomization. In this paper, we propose a novel autoencoder variant that
enforces minimal entropy on minibatches of hidden activations, thereby
promoting feature independence and sparsity across instances. For efficient
entropy calculation, we discretize the hidden activations to 1-bit via a step
function and apply gradient estimation to enable backpropagation, so that we
term it as Binary Autoencoder (BAE) and empirically demonstrate two major
applications: (1) Feature set entropy calculation. Entropy can be reliably
estimated on binary hidden activations, which we empirically evaluate and
leverage to characterize the inference dynamics of LLMs and In-context
Learning. (2) Feature untangling. Similar to typical methods, BAE can extract
atomized features from LLM's hidden states. To robustly evaluate such feature
extraction capability, we refine traditional feature-interpretation methods to
avoid unreliable handling of numerical tokens, and show that BAE avoids dense
features while producing the largest number of interpretable ones among
baselines, which confirms the effectiveness of BAE serving as a feature
extractor.

</details>


### [158] [Feature Augmentation of GNNs for ILPs: Local Uniqueness Suffices](https://arxiv.org/abs/2509.21000)
*Qingyu Han,Qian Li,Linxin Yang,Qian Chen,Qingjiang Shi,Ruoyu Sun*

Main category: cs.LG

TL;DR: Proposes a Local-UID scheme with d-hop uniqueness coloring for graphs to balance expressiveness and generalization in GNNs for ILPs, introducing ColorGNN and ColorUID. Shows Local-UIDs match Global-UID expressiveness in d-layer nets and generalize better; empirical gains on ILP benchmarks, improved OOD LP generalization, and compatibility gains on a graph task when combined with a SOTA method.


<details>
  <summary>Details</summary>
Motivation: ILPs are central but hard to solve. Anonymous GNNs struggle with expressiveness for ILPs, while globally unique identifiers improve power but risk spurious correlations and poor generalization. A middle-ground scheme that is sufficiently expressive yet generalizes well is needed.

Method: Introduce d-hop uniqueness coloring to create Local-UIDs that are unique within a node's d-hop neighborhood. Develop ColorGNN that uses color-conditioned embeddings and ColorUID as a lightweight feature-level variant. Theoretically show that for d-layer networks Local-UIDs achieve the expressive power of Global-UIDs while improving generalization. Validate with experiments on ILP benchmarks and OOD LP datasets, plus a graph-level task with another SOTA method.

Result: Local-UIDs match the expressive power of Global-UIDs in d-layer networks and offer stronger generalization. The approach yields substantial gains on three ILP benchmarks, exhibits strong out-of-distribution generalization on LP datasets, and improves a general graph-level task when paired with a state-of-the-art method.

Conclusion: A parsimonious Local-UID scheme—via d-hop uniqueness coloring—provides a practical balance between expressiveness and generalization for GNNs in ILP contexts. ColorGNN and ColorUID offer effective variants, with results suggesting broader applicability beyond ILP to other graph tasks.

Abstract: Integer Linear Programs (ILPs) are central to real-world optimizations but
notoriously difficult to solve. Learning to Optimize (L2O) has emerged as a
promising paradigm, with Graph Neural Networks (GNNs) serving as the standard
backbone. However, standard anonymous GNNs are limited in expressiveness for
ILPs, and the common enhancement of augmenting nodes with globally unique
identifiers (UIDs) typically introduces spurious correlations that severely
harm generalization. To address this tradeoff, we propose a parsimonious
Local-UID scheme based on d-hop uniqueness coloring, which ensures identifiers
are unique only within each node's d-hop neighborhood. Building on this scheme,
we introduce ColorGNN, which incorporates color information via
color-conditioned embeddings, and ColorUID, a lightweight feature-level
variant. We prove that for d-layer networks, Local-UIDs achieve the expressive
power of Global-UIDs while offering stronger generalization. Extensive
experiments show that our approach (i) yields substantial gains on three ILP
benchmarks, (ii) exhibits strong OOD generalization on linear programming
datasets, and (iii) further improves a general graph-level task when paired
with a state-of-the-art method.

</details>


### [159] [Lossless Compression: A New Benchmark for Time Series Model Evaluation](https://arxiv.org/abs/2509.21002)
*Meng Wan,Benxi Tian,Jue Wang,Cui Hui,Ningming Nie,Tiantian Liu,Zongguo Wang,Cao Rongqiang,Peng Shi,Yangang Wang*

Main category: cs.LG

TL;DR: Lossless compression is proposed as a principled, information-theoretic evaluation paradigm for time series models, linking optimal compression length to negative log-likelihood; introduces the TSCom-Bench framework and shows that current top models may underfit the full data distribution, revealing distributional weaknesses not captured by traditional benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional time series evaluation focuses on task-specific metrics (forecasting, imputation, anomaly detection, classification) and may not rigorously assess whether models capture the full generative distribution. A principled criterion is needed to quantify modeling capacity and distributional fidelity.

Method: Introduce lossless compression based on Shannon's source coding theorem to equate optimal compression length with negative log-likelihood. Define a standardized evaluation protocol and metrics. Open-source TSCom-Bench to adapt time series models as backbones for lossless compression. Empirically evaluate state-of-the-art models (TimeXer, iTransformer, PatchTST) across diverse datasets.

Result: Demonstrate that compression-based evaluation reveals distributional weaknesses in models that are overlooked by traditional benchmarks, showing that high task performance does not guarantee faithful generative modeling. The TSCom-Bench framework enables rapid adaptation of backbones for lossless compression.

Conclusion: Lossless compression offers a principled, complementary evaluation for time series modeling, grounding model capacity in information theory and enabling rigorous assessment of whether models capture the full data distribution.

Abstract: The evaluation of time series models has traditionally focused on four
canonical tasks: forecasting, imputation, anomaly detection, and
classification. While these tasks have driven significant progress, they
primarily assess task-specific performance and do not rigorously measure
whether a model captures the full generative distribution of the data. We
introduce lossless compression as a new paradigm for evaluating time series
models, grounded in Shannon's source coding theorem. This perspective
establishes a direct equivalence between optimal compression length and the
negative log-likelihood, providing a strict and unified information-theoretic
criterion for modeling capacity. Then We define a standardized evaluation
protocol and metrics. We further propose and open-source a comprehensive
evaluation framework TSCom-Bench, which enables the rapid adaptation of time
series models as backbones for lossless compression. Experiments across diverse
datasets on state-of-the-art models, including TimeXer, iTransformer, and
PatchTST, demonstrate that compression reveals distributional weaknesses
overlooked by classic benchmarks. These findings position lossless compression
as a principled task that complements and extends existing evaluation for time
series modeling.

</details>


### [160] [MAIFormer: Multi-Agent Inverted Transformer for Flight Trajectory Prediction](https://arxiv.org/abs/2509.21004)
*Seokbin Yoon,Keumjin Lee*

Main category: cs.LG

TL;DR: A novel MAIFormer architecture with masked multivariate attention and agent attention delivers state-of-the-art multi-agent flight trajectory prediction and interpretable outputs on ADS-B data.


<details>
  <summary>Details</summary>
Motivation: Predicting multi-aircraft trajectories is essential for air traffic management but challenging due to time-varying individual behaviors and complex inter-agent interactions; explainability of predictions is also crucial for ATC use.

Method: Propose MAIFormer, an inverted transformer architecture with two attention modules: (i) masked multivariate attention to capture spatio-temporal patterns of individual aircraft and (ii) agent attention to model social interactions among aircraft. Trained and evaluated on real-world ADS-B trajectory data from the terminal airspace of Incheon International Airport, South Korea.

Result: MAIFormer achieves best performance across multiple metrics and outperforms existing methods, and its predictions are interpretable to humans, enhancing transparency and practical utility in air traffic control.

Conclusion: MAIFormer provides accurate, explainable multi-agent trajectory predictions, improving both performance and interpretability for air traffic management applications.

Abstract: Flight trajectory prediction for multiple aircraft is essential and provides
critical insights into how aircraft navigate within current air traffic flows.
However, predicting multi-agent flight trajectories is inherently challenging.
One of the major difficulties is modeling both the individual aircraft
behaviors over time and the complex interactions between flights. Generating
explainable prediction outcomes is also a challenge. Therefore, we propose a
Multi-Agent Inverted Transformer, MAIFormer, as a novel neural architecture
that predicts multi-agent flight trajectories. The proposed framework features
two key attention modules: (i) masked multivariate attention, which captures
spatio-temporal patterns of individual aircraft, and (ii) agent attention,
which models the social patterns among multiple agents in complex air traffic
scenes. We evaluated MAIFormer using a real-world automatic dependent
surveillance-broadcast flight trajectory dataset from the terminal airspace of
Incheon International Airport in South Korea. The experimental results show
that MAIFormer achieves the best performance across multiple metrics and
outperforms other methods. In addition, MAIFormer produces prediction outcomes
that are interpretable from a human perspective, which improves both the
transparency of the model and its practical utility in air traffic control.

</details>


### [161] [ExMolRL: Phenotype-Target Joint Generation of De Novo Molecules via Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2509.21010)
*Haotian Guo,Hui Liu*

Main category: cs.LG

TL;DR: ExMoIRL integrates phenotype guidance with target-aware optimization to generate high-quality, diverse molecules with strong affinity and drug-like properties, outperforming existing phenotype- and target-based models.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between phenotype-based and target-based drug design by incorporating system-level phenotypic responses and target affinities into de novo molecule generation, reducing experimental costs and improving candidate quality.

Method: A phenotype-guided generator pretrained on drug-induced transcriptional profiles, then fine-tuned via multi-objective reinforcement learning. The reward combines docking affinity, drug-likeness scores, plus ranking loss, prior-likelihood regularization, and entropy maximization to steer toward potent, diverse chemotypes aligned with phenotypic effects.

Result: ExMoIRL outperforms state-of-the-art phenotype-based and target-based models across multiple targets. Generated molecules show favorable drug-like properties, high target affinity, and IC50 potency against cancer cells.

Conclusion: A unified framework that synergistically combines phenotype-guided and target-aware strategies enhances de novo drug discovery by yielding potent, diverse candidates aligned with phenotypic effects.

Abstract: The generation of high-quality candidate molecules remains a central
challenge in AI-driven drug design. Current phenotype-based and target-based
strategies each suffer limitations, either incurring high experimental costs or
overlook system-level cellular responses. To bridge this gap, we propose
ExMoIRL, a novel generative framework that synergistically integrates
phenotypic and target-specific cues for de novo molecular generation. The
phenotype-guided generator is first pretrained on expansive drug-induced
transcriptional profiles and subsequently fine-tuned via multi-objective
reinforcement learning (RL). Crucially, the reward function fuses docking
affinity and drug-likeness scores, augmented with ranking loss,
prior-likelihood regularization, and entropy maximization. The multi-objective
RL steers the model toward chemotypes that are simultaneously potent, diverse,
and aligned with the specified phenotypic effects. Extensive experiments
demonstrate ExMoIRL's superior performance over state-of-the-art
phenotype-based and target-based models across multiple well-characterized
targets. Our generated molecules exhibit favorable drug-like properties, high
target affinity, and inhibitory potency (IC50) against cancer cells. This
unified framework showcases the synergistic potential of combining
phenotype-guided and target-aware strategies, offering a more effective
solution for de novo drug discovery.

</details>


### [162] [Mechanism of Task-oriented Information Removal in In-context Learning](https://arxiv.org/abs/2509.21012)
*Hakaze Cho,Haolin Yang,Gouki Minegishi,Naoya Inoue*

Main category: cs.LG

TL;DR: In-context Learning (ICL) can be explained as an information-removal process: few-shot prompts induce a low-rank filtering of hidden representations to suppress non-task information, steering outputs toward the intended task. Specific attention heads, called Denoising Heads, drive this removal; ablating them degrades accuracy, underscoring their essential role.


<details>
  <summary>Details</summary>
Motivation: The inner mechanism of ICL remains unclear: zero-shot LMs produce non-selective outputs because their hidden states encode information for all possible tasks. Understanding how demonstrations steer outputs is crucial. The authors propose an information-removal perspective to unify and explain ICL, linking it to low-rank filtering and selective attention heads.

Method: The study analyzes LM hidden states in zero-shot vs few-shot settings, introduces a low-rank filter to selectively remove information, and designs metrics to measure information-removal behavior. It also identifies and ablates specific attention heads (Denoising Heads) to test their role in the removal process and assesses how blocking removal affects ICL performance.

Result: Few-shot ICL appears to simulate task-oriented information removal, reducing redundant information in entangled representations and improving outputs according to demonstrations. Ablation that blocks the removal operation degrades accuracy, especially when the correct label is not present in demonstrations. Denoising Heads are essential for the removal mechanism.

Conclusion: Information removal is a key mechanism underlying ICL. Few-shot demonstrations help by emulating selective information removal, and denoising heads drive this process. This perspective offers a plausible explanation for ICL and identifies architectural components that could be targeted to enhance or control few-shot learning.

Abstract: In-context Learning (ICL) is an emerging few-shot learning paradigm based on
modern Language Models (LMs), yet its inner mechanism remains unclear. In this
paper, we investigate the mechanism through a novel perspective of information
removal. Specifically, we demonstrate that in the zero-shot scenario, LMs
encode queries into non-selective representations in hidden states containing
information for all possible tasks, leading to arbitrary outputs without
focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we
find that selectively removing specific information from hidden states by a
low-rank filter effectively steers LMs toward the intended task. Building on
these findings, by measuring the hidden states on carefully designed metrics,
we observe that few-shot ICL effectively simulates such task-oriented
information removal processes, selectively removing the redundant information
from entangled non-selective representations, and improving the output based on
the demonstrations, which constitutes a key mechanism underlying ICL. Moreover,
we identify essential attention heads inducing the removal operation, termed
Denoising Heads, which enables the ablation experiments blocking the
information removal operation from the inference, where the ICL accuracy
significantly degrades, especially when the correct label is absent from the
few-shot demonstrations, confirming both the critical role of the information
removal mechanism and denoising heads.

</details>


### [163] [Predicting LLM Reasoning Performance with Small Proxy Model](https://arxiv.org/abs/2509.21013)
*Woosung Koh,Juyoung Suk,Sungjun Han,Se-Young Yun,Jay Shin*

Main category: cs.LG

TL;DR: rBridge shows small proxies (≤1B) can predict large-model reasoning by aligning with pretraining and task objectives, using frontier-model traces as gold labels, achieving huge cost reductions and strong cross-benchmark correlations.


<details>
  <summary>Details</summary>
Motivation: Pre-training large language models is costly. Reasoning capabilities emerge mainly at larger sizes, creating a bottleneck for cost-efficient dataset optimization. A cheaper, proxy-based workflow is desirable to guide data selection and pretraining.

Method: rBridge weights the negative log-likelihood with a task-alignment score, and uses reasoning traces from frontier models as gold labels to align the proxy with the large-model objective and the target task.

Result: - Reduces dataset ranking costs by >100x vs the best baseline. - Shows the strongest correlation across six reasoning benchmarks at 1B–32B scales. - Demonstrates zero-shot transfer of predictive relationships across pretraining datasets from 1B–7B scale.

Conclusion: rBridge offers a practical, low-cost path for reasoning-oriented pretraining by enabling small proxies to reliably indicate large-model reasoning quality and dataset usefulness.

Abstract: Given the prohibitive cost of pre-training large language models, it is
essential to leverage smaller proxy models to optimize datasets before scaling
up. However, this approach becomes challenging for reasoning capabilities,
which exhibit emergent behavior that only appear reliably at larger model
sizes, often exceeding 7B parameters. To address this, we introduce rBridge,
showing that small proxies ($\leq$1B) can effectively predict large-model
reasoning by aligning more closely with (1) the pre-training objective and (2)
the target task. rBridge achieves this by weighting negative log-likelihood
with task alignment, using reasoning traces from frontier models as gold
labels. In our experiments, rBridge (i) reduces dataset ranking costs by over
100x relative to the best baseline, (ii) achieves the strongest correlation
across six reasoning benchmarks at 1B to 32B scale, and (iii) zero-shot
transfers predictive relationships across pre-training datasets at 1B to 7B
scale. These findings indicate that rBridge offers a practical path for
exploring reasoning-oriented pre-training at lower cost.

</details>


### [164] [DELTA-Code: How Does RL Unlock and Transfer New Programming Algorithms in LLMs?](https://arxiv.org/abs/2509.21016)
*Yiyou Sun,Yuhan Cao,Pohao Huang,Haoyue Bai,Hannaneh Hajishirzi,Nouha Dziri,Dawn Song*

Main category: cs.LG

TL;DR: DELTA-Code proposes a controlled benchmark to study if RL-finetuned LLMs can learn genuinely new algorithmic reasoning and how well such skills transfer to out-of-distribution problems; reveals a grokking phase where learning suddenly emerges; demonstrates training recipes and partial generalization across within-family and recomposed skills, but weaknesses in transformative transfer.


<details>
  <summary>Details</summary>
Motivation: Address whether pretrained LLMs can acquire novel reasoning strategies beyond encoded priors; create an isolating, fully out-of-distribution benchmark for algorithmic reasoning; examine learnability and transferability under RL and curriculum settings.

Method: Introduce DELTA-Code with templated synthetic coding problem families and OOD generators; use reinforcement learning with staged warm-up, dense rewards, experience replay, curriculum training, and verification-in-the-loop; measure performance across learnability (pass@K) and transfer across exploratory, compositional, and transformative axes, including cross-family transfer.

Result: Observe a grokking phase transition: extended near-zero reward periods followed by abrupt rise to near-perfect accuracy; training ingredients enable solvability on previously hard families; solid within-family gains and transfer of recomposed skills; limited success on transformative/generalization tasks.

Conclusion: DELTA serves as a clean, controlled testbed to probe RL-driven reasoning and the ability to acquire new algorithmic skills beyond priors; findings highlight potential and limits of RL-based learning for novel reasoning, and inform design of training curricula for better generalization.

Abstract: It remains an open question whether LLMs can acquire or generalize genuinely
new reasoning strategies, beyond the sharpened skills encoded in their
parameters during pre-training or post-training. To attempt to answer this
debate, we introduce DELTA-Code--Distributional Evaluation of Learnability and
Transferrability in Algorithmic Coding, a controlled benchmark of synthetic
coding problem families designed to probe two fundamental aspects: learnability
-- can LLMs, through reinforcement learning (RL), solve problem families where
pretrained models exhibit failure with large enough attempts (pass@K=0)? --and
transferrability -- if learnability happens, can such skills transfer
systematically to out-of-distribution (OOD) test sets? Unlike prior public
coding datasets, DELTA isolates reasoning skills through templated problem
generators and introduces fully OOD problem families that demand novel
strategies rather than tool invocation or memorized patterns. Our experiments
reveal a striking grokking phase transition: after an extended period with
near-zero reward, RL-trained models abruptly climb to near-perfect accuracy. To
enable learnability on previously unsolvable problem families, we explore key
training ingredients such as staged warm-up with dense rewards, experience
replay, curriculum training, and verification-in-the-loop. Beyond learnability,
we use DELTA to evaluate transferability or generalization along exploratory,
compositional, and transformative axes, as well as cross-family transfer.
Results show solid gains within families and for recomposed skills, but
persistent weaknesses in transformative cases. DELTA thus offers a clean
testbed for probing the limits of RL-driven reasoning and for understanding how
models can move beyond existing priors to acquire new algorithmic skills.

</details>


### [165] [Efficient Ensemble Conditional Independence Test Framework for Causal Discovery](https://arxiv.org/abs/2509.21021)
*Zhengkang Guan,Kun Kuang*

Main category: cs.LG

TL;DR: Introduces E-CIT, a scalable ensemble framework for conditional independence testing that divides data into subsets, runs a base CIT on each subset, and aggregates p-values via a stable-distribution-based method, achieving near-linear time in sample size (with fixed subset size) and providing consistency guarantees; empirically reduces computation while maintaining competitive performance, especially on complex real-world data.


<details>
  <summary>Details</summary>
Motivation: Constraint-based causal discovery relies on many costly CITs, whose high time complexity with large samples severely limits practical applicability.

Method: A divide-and-aggregate approach: partition data into subsets, apply a chosen base CIT to each subset independently, and combine the subset p-values with a novel p-value combination method rooted in stable distribution theory, yielding a scalable and consistent framework.

Result: Significant reduction in computational burden for CITs and causal discovery, with competitive and improved performance in complex testing scenarios, notably on real-world datasets.

Conclusion: E-CIT provides a general, plug-and-play solution to scale CITs, balancing efficiency and statistical reliability while maintaining consistency guarantees.

Abstract: Constraint-based causal discovery relies on numerous conditional independence
tests (CITs), but its practical applicability is severely constrained by the
prohibitive computational cost, especially as CITs themselves have high time
complexity with respect to the sample size. To address this key bottleneck, we
introduce the Ensemble Conditional Independence Test (E-CIT), a general and
plug-and-play framework. E-CIT operates on an intuitive divide-and-aggregate
strategy: it partitions the data into subsets, applies a given base CIT
independently to each subset, and aggregates the resulting p-values using a
novel method grounded in the properties of stable distributions. This framework
reduces the computational complexity of a base CIT to linear in the sample size
when the subset size is fixed. Moreover, our tailored p-value combination
method offers theoretical consistency guarantees under mild conditions on the
subtests. Experimental results demonstrate that E-CIT not only significantly
reduces the computational burden of CITs and causal discovery but also achieves
competitive performance. Notably, it exhibits an improvement in complex testing
scenarios, particularly on real-world datasets.

</details>


### [166] [Actor-Critic without Actor](https://arxiv.org/abs/2509.21022)
*Donghyeon Ki,Hee-Jun Ahn,Kyungyoon Kim,Byung-Jun Lee*

Main category: cs.LG

TL;DR: ACA eliminates the actor, generating actions from the gradient of a noise-level critic, reducing architectural/compute overhead while retaining expressive, multimodal behavior and competitive online RL performance.


<details>
  <summary>Details</summary>
Motivation: Actor-critic methods require separate actor/critic networks and careful tuning; diffusion-based policies add further complexity; there is a need for simpler, scalable online RL methods with strong expressiveness.

Method: Propose Actor-Critic without Actor (ACA): no explicit actor network; actions are produced directly from the gradient field of a noise-level critic; training focuses on critic/value estimates; gradient-based action generation aims to align policy improvement with critic updates and support multimodality without diffusion.

Result: Empirical studies on standard online RL benchmarks show ACA yields faster learning curves and competitive performance relative to vanilla actor-critic and diffusion-based approaches.

Conclusion: ACA offers a compact, efficient online RL framework that preserves expressive policy behavior (multimodality) without actor networks or diffusion, combining simplicity with performance. Further work could explore stability and scalability guarantees and broader benchmarks.

Abstract: Actor-critic methods constitute a central paradigm in reinforcement learning
(RL), coupling policy evaluation with policy improvement. While effective
across many domains, these methods rely on separate actor and critic networks,
which makes training vulnerable to architectural decisions and hyperparameter
tuning. Such complexity limits their scalability in settings that require large
function approximators. Recently, diffusion models have recently been proposed
as expressive policies that capture multi-modal behaviors and improve
exploration, but they introduce additional design choices and computational
burdens, hindering efficient deployment. We introduce Actor-Critic without
Actor (ACA), a lightweight framework that eliminates the explicit actor network
and instead generates actions directly from the gradient field of a noise-level
critic. This design removes the algorithmic and computational overhead of actor
training while keeping policy improvement tightly aligned with the critic's
latest value estimates. Moreover, ACA retains the ability to capture diverse,
multi-modal behaviors without relying on diffusion-based actors, combining
simplicity with expressiveness. Through extensive experiments on standard
online RL benchmarks,ACA achieves more favorable learning curves and
competitive performance compared to both standard actor-critic and
state-of-the-art diffusion-based methods, providing a simple yet powerful
solution for online RL.

</details>


### [167] [FORCE: Transferable Visual Jailbreaking Attacks via Feature Over-Reliance CorrEction](https://arxiv.org/abs/2509.21029)
*Runqi Lin,Alasdair Paren,Suqin Yuan,Muyang Li,Philip Torr,Adel Bibi,Tongliang Liu*

Main category: cs.LG

TL;DR: Analyzes why simple visual jailbreaking attacks have poor cross-model transferability and introduces FORCE to broaden feature exploration, yielding better transfer to closed-source MLLMs for red-teaming.


<details>
  <summary>Details</summary>
Motivation: There is a growing vulnerability in multimodal LLMs when new modalities are integrated. Visual jailbreaking attacks on open-source MLLMs show weak cross-model transferability and poorly generalize to closed-source models, necessitating a deeper understanding of the loss landscape and feature representations to improve transferability.

Method: 1) Analyze the loss landscape of visual jailbreaking attacks and identify high-sharpness regions. 2) Inspect intermediate layer representations and spectral-domain features, revealing over-reliance on narrow features. 3) Propose Feature Over-Reliance CorrEction (FORCE) to (a) encourage exploration of broader layer-feature regions and (b) rescale frequency features by their semantic content, aiming for flatter feasible regions across models.

Result: FORCE produces flattened feasible regions for visual jailbreaking attacks, reducing sensitivity to small parameter changes and improving cross-model transferability, thereby enabling more effective visual red-teaming against closed-source MLLMs.

Conclusion: Eliminating non-generalizable reliance on specific layer and spectral features yields more transferable visual jailbreaks. FORCE offers a practical tool for evaluating and hardening MLLMs against visual jailbreaking in cross-model settings.

Abstract: The integration of new modalities enhances the capabilities of multimodal
large language models (MLLMs) but also introduces additional vulnerabilities.
In particular, simple visual jailbreaking attacks can manipulate open-source
MLLMs more readily than sophisticated textual attacks. However, these
underdeveloped attacks exhibit extremely limited cross-model transferability,
failing to reliably identify vulnerabilities in closed-source MLLMs. In this
work, we analyse the loss landscape of these jailbreaking attacks and find that
the generated attacks tend to reside in high-sharpness regions, whose
effectiveness is highly sensitive to even minor parameter changes during
transfer. To further explain the high-sharpness localisations, we analyse their
feature representations in both the intermediate layers and the spectral
domain, revealing an improper reliance on narrow layer representations and
semantically poor frequency components. Building on this, we propose a Feature
Over-Reliance CorrEction (FORCE) method, which guides the attack to explore
broader feasible regions across layer features and rescales the influence of
frequency features according to their semantic content. By eliminating
non-generalizable reliance on both layer and spectral features, our method
discovers flattened feasible regions for visual jailbreaking attacks, thereby
improving cross-model transferability. Extensive experiments demonstrate that
our approach effectively facilitates visual red-teaming evaluations against
closed-source MLLMs.

</details>


### [168] [Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs](https://arxiv.org/abs/2509.21044)
*Honglin Zhang,Qianyue Hao,Fengli Xu,Yong Li*

Main category: cs.LG

TL;DR: RL fine-tuning makes LLMs more active and diversify internal pathways, suggesting reshaped information flow that aids generalization; DPO diverges from this pattern.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms by which RL fine-tuning improves LLM capabilities beyond SFT, across diverse model families, by probing internal activations.

Method: Apply edge attribution patching (EAP) to multiple LLM families; compare online RL methods (PPO, GRPO) with preference-based DPO; measure activation intensity, activation entropy, and edge distribution concentration to analyze changes in internal circuitry.

Result: RL post-training yields two robust effects: (i) increased activation intensity (more pathways engaged and stronger signals); (ii) greater diversity in activation patterns (higher entropy, less concentrated edge distributions). DPO-trained models deviate, showing weaker or inconsistent internal changes compared to PPO/GRPO.

Conclusion: RL fine-tuning systematically reshapes internal circuitry of LLMs, promoting redundancy and flexibility in information flow and potentially improving generalization; notable methodological differences between online RL and preference-based approaches are highlighted; code is open source.

Abstract: Large language models (LLMs) acquire extensive prior knowledge through
large-scale pretraining and can be further enhanced via supervised fine-tuning
(SFT) or reinforcement learning (RL)-based post-training. A growing body of
evidence has shown that RL fine-tuning improves the capability of LLMs beyond
what SFT alone achieves. However, the underlying mechanisms why RL fine-tuning
is able to enhance the capability of various LLMs with distinct intrinsic
characteristics remain underexplored. In this study, we draw inspiration from
prior work on edge attribution patching (EAP) to investigate the internal
differences of LLMs before and after RL fine-tuning. Our analysis across
multiple model families shows two robust effects of online RL post-training:
(i) an overall increase in activation intensity, indicating that more internal
pathways are engaged and their signals become stronger, and (ii) greater
diversity in activation patterns, reflected by higher entropy and less
concentrated edge distributions. These changes suggest that RL reshapes
information flow to be both more redundant and more flexible, which may explain
its advantage in generalization. Notably, models fine-tuned with Direct
Preference Optimization (DPO) deviate from these trends, exhibiting
substantially weaker or inconsistent internal changes compared to PPO- and
GRPO-based training. Together, our findings provide a unified view of how RL
fine-tuning systematically alters the internal circuitry of LLMs and highlight
the methodological distinctions between online RL and preference-based
approaches. Our code is open source at
https://anonymous.4open.science/r/llm_rl_probing_analysis-F673.

</details>


### [169] [Physics of Learning: A Lagrangian perspective to different learning paradigms](https://arxiv.org/abs/2509.21049)
*Siyuan Guo,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: A unifying framework called the Learning Lagrangian is proposed for efficient learning, from which classic algorithms like Bellman’s optimality equation in reinforcement learning and the Adam optimizer in generative models are derived as stationary trajectories of the Lagrangian.


<details>
  <summary>Details</summary>
Motivation: To design learning systems that minimize time/observations by applying least-action principles from physics, providing a common foundation and potential derivations of key ML algorithms.

Method: Define a Learning Lagrangian and formulate learning as seeking stationary paths of this Lagrangian; derive learning dynamics by variational/stationary-trajectory principles, leading to Bellman equations in RL and Adam in generative modelling.

Result: The abstract claims derivations of classical learning algorithms from first principles via the Learning Lagrangian; it presents a unifying theoretical framework but provides no empirical results in the abstract.

Conclusion: Learning processes are posited as stationary trajectories of the Learning Lagrangian, offering a potentially unifying view and a route to derive or discover learning algorithms; rigorous mathematical detail and empirical validation are needed.

Abstract: We study the problem of building an efficient learning system. Efficient
learning processes information in the least time, i.e., building a system that
reaches a desired error threshold with the least number of observations.
Building upon least action principles from physics, we derive classic learning
algorithms, Bellman's optimality equation in reinforcement learning, and the
Adam optimizer in generative models from first principles, i.e., the Learning
$\textit{Lagrangian}$. We postulate that learning searches for stationary paths
in the Lagrangian, and learning algorithms are derivable by seeking the
stationary trajectories.

</details>


### [170] [GeoRef: Referring Expressions in Geometry via Task Formulation, Synthetic Supervision, and Reinforced MLLM-based Solutions](https://arxiv.org/abs/2509.21050)
*Bing Liu,Wenqiang Yv,Xuzheng Yang,Shichang Wang,Junzhuo Liu,Peng Wang,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.LG

TL;DR: Introduces Referring Expression Comprehension (REC) for geometric problems, builds GeoRef, uses synthetic geometric-language data, and shows GRPO outperforms supervised fine-tuning, with a verify-and-regenerate mechanism; highlights grounding as prerequisite and GeoRef benefits downstream geometric reasoning.


<details>
  <summary>Details</summary>
Motivation: Geometric problem solving requires accurate diagram interpretation and cross-modal grounding; existing vision-language models struggle to map natural-language queries to geometric elements; there is a lack of annotated REC data for geometry, motivating a dedicated task and benchmark.

Method: Construct GeoRef dataset from existing geometric problem corpora; generate a large synthetic training dataset via a structured geometric formal language; compare Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO); propose a verify-and-regenerate mechanism that detects incorrect predictions and refines answers using reasoning history.

Result: GRPO significantly outperforms SFT on localization and reasoning in geometric diagrams; state-of-the-art Multimodal Large Language Models (MLLMs) struggle with REC; models trained on GeoRef show measurable improvements on downstream geometric reasoning tasks; verify-and-regenerate yields additional accuracy gains.

Conclusion: REC provides a foundational framework for multimodal geometric problem solving; explicit evaluation of geometric grounding is essential; GeoRef-trained models enhance broader geometric understanding and reasoning capabilities.

Abstract: AI-driven geometric problem solving is a complex vision-language task that
requires accurate diagram interpretation, mathematical reasoning, and robust
cross-modal grounding. A foundational yet underexplored capability for this
task is the ability to identify and interpret geometric elements based on
natural language queries. To address this, we introduce the task of Referring
Expression Comprehension (REC) for geometric problems, which evaluates whether
models can localize points, shapes, and spatial relations in diagrams in
response to textual prompts. We present GeoRef, a benchmark dataset constructed
from existing geometric problem corpora, featuring diverse, high-quality
annotations and queries. Due to the lack of annotated data for this task, we
generate a large-scale synthetic training dataset using a structured geometric
formal language, enabling broad coverage of geometric concepts and facilitating
model adaptation. We explore two fine-tuning approaches: Supervised Fine-Tuning
(SFT) and Group Relative Policy Optimization (GRPO). Our results show that GRPO
significantly outperforms SFT by better aligning model behavior with
task-specific rewards. Furthermore, we propose a verify-and-regenerate
mechanism that detects incorrect predictions and re-infers answers using
contextual reasoning history, further boosting accuracy. Notably, even
state-of-the-art Multimodal Large Language Models (MLLMs) struggle with this
task, underscoring the necessity of explicitly evaluating and strengthening
geometric grounding as a prerequisite for robust geometric problem solving.
Moreover, models trained on GeoRef demonstrate measurable improvements on
downstream geometric reasoning tasks, highlighting the broader value of REC as
a foundation for multimodal mathematical understanding.

</details>


### [171] [SPREAD: Sampling-based Pareto front Refinement via Efficient Adaptive Diffusion](https://arxiv.org/abs/2509.21058)
*Sedjro Salomon Hotegni,Sebastian Peitz*

Main category: cs.LG

TL;DR: SPREAD uses diffusion probabilistic models to generate Pareto-optimal decision-space candidates for multi-objective optimization; it refines candidates with adaptive gradient-descent inspired updates and a Gaussian RBF repulsion term to ensure convergence and diversity, achieving competitive efficiency, scalability, and Pareto front coverage on offline and Bayesian surrogate benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently computing the Pareto set for large-scale and expensive multi-objective problems. Existing methods struggle with scalability and diversity of the Pareto front; a generative diffusion-based approach could offer scalable, sample-efficient Pareto generation.

Method: Train a conditional diffusion model over points sampled from the decision space. During reverse diffusion, iteratively refine candidates using an adaptive multiple gradient-descent-inspired update for fast convergence, augmented with a Gaussian RBF-based repulsion term to promote diversity across the Pareto front.

Result: Empirical results on multi-objective optimization benchmarks, including offline and Bayesian surrogate-based settings, show SPREAD matches or exceeds leading baselines in efficiency, scalability, and Pareto front coverage.

Conclusion: SPREAD provides a scalable, efficient, and diverse framework for multi-objective optimization by combining diffusion-based generative modeling with gradient-descent-inspired refinement and diversity-promoting repulsion, suitable for large-scale and expensive problems.

Abstract: Developing efficient multi-objective optimization methods to compute the
Pareto set of optimal compromises between conflicting objectives remains a key
challenge, especially for large-scale and expensive problems. To bridge this
gap, we introduce SPREAD, a generative framework based on Denoising Diffusion
Probabilistic Models (DDPMs). SPREAD first learns a conditional diffusion
process over points sampled from the decision space and then, at each reverse
diffusion step, refines candidates via a sampling scheme that uses an adaptive
multiple gradient descent-inspired update for fast convergence alongside a
Gaussian RBF-based repulsion term for diversity. Empirical results on
multi-objective optimization benchmarks, including offline and Bayesian
surrogate-based settings, show that SPREAD matches or exceeds leading baselines
in efficiency, scalability, and Pareto front coverage.

</details>


### [172] [Structure-Attribute Transformations with Markov Chain Boost Graph Domain Adaptation](https://arxiv.org/abs/2509.21059)
*Zhen Liu,Yongtao Zhang,Shaobo Ren,Yuxin You*

Main category: cs.LG

TL;DR: SATMC jointly transforms graph structure and node attributes with a Markov Chain to align distributions across domains, reduces domain-private noise, and achieves tighter error bounds with state-of-the-art cross-network node classification results.


<details>
  <summary>Details</summary>
Motivation: Graph domain adaptation struggles due to structural heterogeneity that hampers distribution alignment when only node attributes are transformed. A method that aligns both structure and attributes can better bridge domain gaps.

Method: Propose Structure-Attribute Transformation with Markov Chain (SATMC) that sequentially aligns distributions across networks via both structure and attribute transformations. Introduce a private domain information reduction mechanism and an empirical Wasserstein distance. Provide theoretical proofs of tighter cross-network classification error bounds. Validate on nine cross-domain dataset pairs with competitive results and public code.

Result: Theoretical analysis shows SATMC yields a tighter error bound for cross-network node classification than existing methods. Empirical evaluation on nine cross-domain dataset pairs indicates SATMC outperforms state-of-the-art methods in cross-network node classification.

Conclusion: SATMC effectively mitigates structural heterogeneity in graph domain adaptation, improving generalization through combined structure-attribute transformation, private-information reduction, and optimal-transport-based alignment, with theoretical guarantees and strong empirical performance; code is publicly available.

Abstract: Graph domain adaptation has gained significant attention in label-scarce
scenarios across different graph domains. Traditional approaches to graph
domain adaptation primarily focus on transforming node attributes over raw
graph structures and aligning the distributions of the transformed node
features across networks. However, these methods often struggle with the
underlying structural heterogeneity between distinct graph domains, which leads
to suboptimal distribution alignment. To address this limitation, we propose
Structure-Attribute Transformation with Markov Chain (SATMC), a novel framework
that sequentially aligns distributions across networks via both graph structure
and attribute transformations. To mitigate the negative influence of
domain-private information and further enhance the model's generalization,
SATMC introduces a private domain information reduction mechanism and an
empirical Wasserstein distance. Theoretical proofs suggest that SATMC can
achieve a tighter error bound for cross-network node classification compared to
existing graph domain adaptation methods. Extensive experiments on nine pairs
of publicly available cross-domain datasets show that SATMC outperforms
state-of-the-art methods in the cross-network node classification task. The
code is available at https://github.com/GiantZhangYT/SATMC.

</details>


### [173] [ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning](https://arxiv.org/abs/2509.21070)
*Qizhi Pei,Zhuoshi Pan,Honglin Lin,Xin Gao,Yu Li,Zinan Tang,Conghui He,Rui Yan,Lijun Wu*

Main category: cs.LG

TL;DR: ScaleDiff presents a two-stage, cost-efficient pipeline for scaling difficult math problems: a single-pass adaptive model filters hard problems, a DiffGen-8B is trained on them to generate many new hard problems, and a smaller LM is fine-tuned to achieve strong results (e.g., 65.9% avg on benchmarks) using cheap teacher models, with observed scaling benefits as difficult data grows.


<details>
  <summary>Details</summary>
Motivation: To overcome high API costs, heavy prompting complexity, and limited difficulty in automatically generating hard mathematical problems for training/finetuning large reasoning models.

Method: 1) Use an adaptive thinking model to score problem difficulty in one forward pass and switch between Thinking/NoThinking modes. 2) Train a dedicated difficult-problem generator (DiffGen-8B) on the filtered difficult data to produce a large-scale set of hard problems. 3) Fine-tune Qwen2.5-Math-7B-Instruct on ScaleDiff-Math. 4) Demonstrate that smaller teacher models (e.g., Qwen3-8B) can transfer advanced reasoning. 5) Show scaling effects as the number of difficult problems increases.

Result: Fine-tuning yielded 11.3% relative improvement over the original ScaleDiff dataset and achieved 65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500, outperforming OpenThinker3.

Conclusion: ScaleDiff demonstrates a cost-efficient, scalable path to improve LRMs on difficult mathematical tasks by (i) efficiently filtering for hard problems with a lightweight adaptive thinker, (ii) generating大量 hard problems with DiffGen-8B, and (iii) effectively transferring reasoning to smaller models without relying on larger teachers; and it reveals clear scaling benefits with more difficult data.

Abstract: Large Reasoning Models (LRMs) have shown impressive capabilities in complex
problem-solving, often benefiting from training on difficult mathematical
problems that stimulate intricate reasoning. Recent efforts have explored
automated synthesis of mathematical problems by prompting proprietary models or
large-scale open-source models from seed data or inherent mathematical
concepts. However, scaling up these methods remains challenging due to their
high computational/API cost, complexity of prompting, and limited difficulty
level of the generated problems. To overcome these limitations, we propose
ScaleDiff, a simple yet effective pipeline designed to scale the creation of
difficult problems. We efficiently identify difficult problems from existing
datasets with only a single forward pass using an adaptive thinking model,
which can perceive problem difficulty and automatically switch between
"Thinking" and "NoThinking" modes. We then train a specialized difficult
problem generator (DiffGen-8B) on this filtered difficult data, which can
produce new difficult problems in large scale, eliminating the need for
complex, per-instance prompting and its associated high API costs. Fine-tuning
Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial
performance increase of 11.3% compared to the original dataset and achieves a
65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500,
outperforming recent strong LRMs like OpenThinker3. Notably, this performance
is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating
that our pipeline can effectively transfer advanced reasoning capabilities
without relying on larger, more expensive teacher models. Furthermore, we
observe a clear scaling phenomenon in model performance on difficult benchmarks
as the quantity of difficult problems increases. Code:
https://github.com/QizhiPei/ScaleDiff.

</details>


### [174] [TyphoonMLA: A Mixed Naive-Absorb MLA Kernel For Shared Prefix](https://arxiv.org/abs/2509.21081)
*Ahmet Caner Yüzügüler,Ahmet Çelik,Jiawei Zhuang,Lukas Cavigelli*

Main category: cs.LG

TL;DR: A hybrid TyphoonMLA kernel combines naive compute and absorb bandwidth strategies to exploit shared prefixes in MLA, delivering up to 3x (NPU) and 3.24x (GPU) throughput with ~3% extra HBM.


<details>
  <summary>Details</summary>
Motivation: MLA has two kernel implementations with trade-offs: naive kernels excel in compute-bound phases (training/prefill) while absorb-based kernels minimize bandwidth but are compute-bound and hinder data reuse from shared prefixes. A unified approach is needed to leverage both strengths and improve throughput without large memory overhead.

Method: Introduce TyphoonMLA, a hybrid kernel that applies the naive formulation to compute-bound parts of MLA (especially shared-prefix computations) and uses the absorb formulation for non-shared parts to reduce bandwidth. This mixed approach aims to maximize data reuse opportunities while keeping HBM impact low. Evaluate throughput on NPU and GPUs and compare to pure naive/absorb baselines.

Result: Throughput improvements of up to 3x on NPU and 3.24x on GPUs were demonstrated, with only about a 3% increase in HBM usage.

Conclusion: A selective, mixed-kernel design that combines naive and absorb formulations effectively leverages shared prefixes in MLA, yielding substantial throughput gains with modest memory overhead and validating the practical viability of hybrid attention kernels.

Abstract: Multi-Head Latent Attention (MLA) is a recent attention mechanism adopted in
state-of-the-art LLMs such as DeepSeek-v3 and Kimi K2. Thanks to its novel
formulation, MLA allows two functionally equivalent but computationally
distinct kernel implementations: naive and absorb. While the naive kernels
(e.g., FlashAttention) are typically preferred in training and prefill for
their computational efficiency, existing decoding kernels (e.g., FlashMLA) rely
on the absorb method to minimize HBM bandwidth usage. However, the
compute-bound nature of the absorb implementations prohibits performance
benefits from data reuse opportunities in attention calculations, such as
shared prefixes. In this work, we introduce TyphoonMLA, a hybrid approach that
combines naive and absorb formulations to harness the strengths of both.
TyphoonMLA effectively leverages the shared prefix by applying the naive
formulation to the compute-bound parts of attention calculations, while
reducing the bandwidth requirements for non-shared parts by using the absorb
formulation. As a result, TyphoonMLA improves the throughput of attention
calculations in MLA architectures by up to 3x and 3.24x on NPU and GPUs, with
only a 3% overhead in HBM size.

</details>


### [175] [GraphUniverse: Enabling Systematic Evaluation of Inductive Generalization](https://arxiv.org/abs/2509.21097)
*Louis Van Langendonck,Guillermo Bernárdez,Nina Miolane,Pere Barlet-Ros*

Main category: cs.LG

TL;DR: GraphUniverse enables systematic inductive generalization testing by generating families of graphs with persistent communities and tunable structure; findings show transductive success does not predict inductive generalization and robustness varies with model and graph regime.


<details>
  <summary>Details</summary>
Motivation: Current graph-learning benchmarks are mostly single-graph and transductive, limiting analysis of inductive generalization and robustness to distribution shifts across diverse graph families.

Method: Generate graphs with persistent semantic communities and controllable structural properties (e.g., homophily, degree distributions); enable distribution-shift robustness tests; benchmark a range of architectures from GNNs to graph transformers and topological models; provide an interactive demo.

Result: Strong transductive performance does not reliably predict inductive generalization; robustness to distribution shift is highly sensitive to model architecture and the initial graph regime (high vs. low homophily); GraphUniverse enables scalable, robust evaluation and development of generalizable graph models.

Conclusion: GraphUniverse offers a flexible framework to study and foster truly generalizable graph architectures, including potential for next-generation graph foundation models; an interactive demo is available.

Abstract: A fundamental challenge in graph learning is understanding how models
generalize to new, unseen graphs. While synthetic benchmarks offer controlled
settings for analysis, existing approaches are confined to single-graph,
transductive settings where models train and test on the same graph structure.
Addressing this gap, we introduce GraphUniverse, a framework for generating
entire families of graphs to enable the first systematic evaluation of
inductive generalization at scale. Our core innovation is the generation of
graphs with persistent semantic communities, ensuring conceptual consistency
while allowing fine-grained control over structural properties like homophily
and degree distributions. This enables crucial but underexplored robustness
tests, such as performance under controlled distribution shifts. Benchmarking a
wide range of architectures -- from GNNs to graph transformers and topological
architectures -- reveals that strong transductive performance is a poor
predictor of inductive generalization. Furthermore, we find that robustness to
distribution shift is highly sensitive not only to model architecture choice
but also to the initial graph regime (e.g., high vs. low homophily). Beyond
benchmarking, GraphUniverse's flexibility and scalability can facilitate the
development of robust and truly generalizable architectures -- including
next-generation graph foundation models. An interactive demo is available at
https://graphuniverse.streamlit.app.

</details>


### [176] [Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning](https://arxiv.org/abs/2509.21126)
*Xiefeng Wu,Jing Zhao,Shu Zhang,Mingyu Hu*

Main category: cs.LG

TL;DR: VARL uses vision-language models to act as action advisors in online RL, boosting sample efficiency without changing optimality or convergence.


<details>
  <summary>Details</summary>
Motivation: Reduce sample complexity and enable from-scratch RL in real-world tasks by leveraging pre-trained vision-language knowledge to guide actions.

Method: Incorporates a VLM to suggest actions as an advisor during online RL rather than shaping rewards; preserves optimality and convergence while increasing action diversity and improving sample efficiency with minimal extra computation.

Result: Demonstrates improved sample efficiency across diverse environments and agent settings, especially in sparse-reward tasks, with negligible computational overhead.

Conclusion: VARL provides a general, practical framework for applying RL from scratch in real-world settings by leveraging VLM guidance without sacrificing theoretical guarantees.

Abstract: Online reinforcement learning in complex tasks is time-consuming, as massive
interaction steps are needed to learn the optimal Q-function.Vision-language
action (VLA) policies represent a promising direction for solving diverse
tasks; however, their performance on low-level control remains limited, and
effective deployment often requires task-specific expert demonstrations for
fine-tuning. In this paper, we propose \textbf{VARL} (\textbf{V}LM as
\textbf{A}ction advisor for online \textbf{R}einforcement \textbf{L}earning), a
framework that leverages the domain knowledge of vision-language models (VLMs)
to provide action suggestions for reinforcement learning agents. Unlike
previous methods, VARL provides action suggestions rather than designing
heuristic rewards, thereby guaranteeing unchanged optimality and convergence.
The suggested actions increase sample diversity and ultimately improve sample
efficiency, especially in sparse-reward tasks. To validate the effectiveness of
VARL, we evaluate it across diverse environments and agent settings. Results
show that VARL greatly improves sample efficiency without introducing
significant computational overhead. These advantages make VARL a general
framework for online reinforcement learning and make it feasible to directly
apply reinforcement learning from scratch in real-world environments.

</details>


### [177] [EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense](https://arxiv.org/abs/2509.21129)
*Wei Huang,De-Tian Chu,Lin-Yuan Bai,Wei Kang,Hai-Tao Zhang,Bo Li,Zhi-Mo Han,Jing Ge,Hai-Feng Lin*

Main category: cs.LG

TL;DR: EvoMail introduces a self-evolving, multi-modal spam detector that fuses text, headers, URLs, and attachments with a cognitive GNN and an LLM, trained via an adversarial red-team/blue-team loop to adapt to evolving threats, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Contemporary spam and phishing campaigns are multi-modal and rapidly evolving, rendering static rules and single-modality models ineffective. There is a need for unified signal integration, continuous adaptation, and interpretable reasoning in defenses.

Method: Construct a unified heterogeneous email graph integrating textual content, metadata (headers, senders, domains), and embedded resources (URLs, attachments). Use a Cognitive Graph Neural Network (GNN) enhanced by a Large Language Model (LLM) to perform context-aware reasoning across sources for detecting coordinated campaigns. Implement an adversarial self-evolution loop: a red-team agent generates novel evasion tactics (e.g., character obfuscation, AI-generated phishing text) and a blue-team detector learns from failures, compresses experiences into a memory module, and reuses them for future reasoning.

Result: Experiments on real-world datasets (Enron-Spam, Ling-Spam, SpamAssassin, TREC) and synthetic adversarial variants show EvoMail consistently outperforms state-of-the-art baselines in detection accuracy, adaptability to evolving tactics, and interpretability of reasoning traces.

Conclusion: EvoMail offers a resilient and explainable defense framework against next-generation spam and phishing threats by unifying heterogeneous signals, leveraging cognitive reasoning, and continuously self-evolving through adversarial feedback.

Abstract: Modern email spam and phishing attacks have evolved far beyond keyword
blacklists or simple heuristics. Adversaries now craft multi-modal campaigns
that combine natural-language text with obfuscated URLs, forged headers, and
malicious attachments, adapting their strategies within days to bypass filters.
Traditional spam detection systems, which rely on static rules or
single-modality models, struggle to integrate heterogeneous signals or to
continuously adapt, leading to rapid performance degradation.
  We propose EvoMail, a self-evolving cognitive agent framework for robust
detection of spam and phishing. EvoMail first constructs a unified
heterogeneous email graph that fuses textual content, metadata (headers,
senders, domains), and embedded resources (URLs, attachments). A Cognitive
Graph Neural Network enhanced by a Large Language Model (LLM) performs
context-aware reasoning across these sources to identify coordinated spam
campaigns. Most critically, EvoMail engages in an adversarial self-evolution
loop: a ''red-team'' agent generates novel evasion tactics -- such as character
obfuscation or AI-generated phishing text -- while the ''blue-team'' detector
learns from failures, compresses experiences into a memory module, and reuses
them for future reasoning.
  Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam,
SpamAssassin, and TREC) and synthetic adversarial variants demonstrate that
EvoMail consistently outperforms state-of-the-art baselines in detection
accuracy, adaptability to evolving spam tactics, and interpretability of
reasoning traces. These results highlight EvoMail's potential as a resilient
and explainable defense framework against next-generation spam and phishing
threats.

</details>


### [178] [Sparse Representations Improve Adversarial Robustness of Neural Network Classifiers](https://arxiv.org/abs/2509.21130)
*Killian Steunou,Sigurd Saue,Théo Druilhe*

Main category: cs.LG

TL;DR: Sparse PCA (SPCA) as a front-end improves robustness to adversarial perturbations compared to standard PCA, supported by exact robustness certificates for linear heads and Lipschitz-based arguments for non-linear heads; empirically SPCA yields better robustness under strong attacks while maintaining clean accuracy.


<details>
  <summary>Details</summary>
Motivation: Deep networks are vulnerable to carefully crafted adversarial perturbations; the paper investigates whether a simple, data-adapted front-end (PCA vs SPCA) can provide a robustness boost and what theoretical guarantees can be obtained.

Method: Provide a theoretical analysis deriving exact robustness certificates for linear heads on SPCA features under l_infty and l2 threats (binary and multiclass). Show that the certified radius depends on the dual norms of W^T u and shrinks with sparsity. Extend to non-linear heads via Lipschitz composition to argue sparsity reduces operator-norm bounds. Empirically test with a small non-linear network after projection, under strong attacks.

Result: Certificates show that sparsity reduces adversarial leverage; SPCA features yield larger robustness margins. Empirically, SPCA degrades more gracefully than PCA under strong white-box and black-box attacks while keeping competitive clean accuracy.

Conclusion: Sparsity in the projection layer explains and yields improved robustness; the mechanism generalizes beyond linear models and is supported by empirical evidence. Code is available.

Abstract: Deep neural networks perform remarkably well on image classification tasks
but remain vulnerable to carefully crafted adversarial perturbations. This work
revisits linear dimensionality reduction as a simple, data-adapted defense. We
empirically compare standard Principal Component Analysis (PCA) with its sparse
variant (SPCA) as front-end feature extractors for downstream classifiers, and
we complement these experiments with a theoretical analysis. On the theory
side, we derive exact robustness certificates for linear heads applied to SPCA
features: for both $\ell_\infty$ and $\ell_2$ threat models (binary and
multiclass), the certified radius grows as the dual norms of $W^\top u$ shrink,
where $W$ is the projection and $u$ the head weights. We further show that for
general (non-linear) heads, sparsity reduces operator-norm bounds through a
Lipschitz composition argument, predicting lower input sensitivity.
Empirically, with a small non-linear network after the projection, SPCA
consistently degrades more gracefully than PCA under strong white-box and
black-box attacks while maintaining competitive clean accuracy. Taken together,
the theory identifies the mechanism (sparser projections reduce adversarial
leverage) and the experiments verify that this benefit persists beyond the
linear setting. Our code is available at
https://github.com/killian31/SPCARobustness.

</details>


### [179] [LAVA: Explainability for Unsupervised Latent Embeddings](https://arxiv.org/abs/2509.21149)
*Ivan Stresec,Joana P. Gonçalves*

Main category: cs.LG

TL;DR: LAVA is a post-hoc, model-agnostic method that explains local organization of unsupervised embeddings by linking latent neighborhoods to input feature correlations; demonstrated on UMAP MNIST and a single-cell kidney dataset, revealing meaningful, recurring feature patterns.


<details>
  <summary>Details</summary>
Motivation: Unsupervised models learn latent spaces that are hard to interpret. Existing explainability approaches adapted from supervised learning either give single-sample or dataset-wide summaries and do not connect latent proximity across samples. For manifold learning methods with no explicit mapping, there is a need to relate local embedding structure to input features to enable discovery.

Method: Locality-Aware Variable Associations (LAVA) constructs localities (neighborhoods) in the latent space, characterizes each locality by correlations among the original features, and then aggregates to identify recurring correlation patterns across the latent space. It is post-hoc and model-agnostic.

Result: Applying LAVA to UMAP embeddings of MNIST and a single-cell kidney dataset reveals feature associations that are visually and biologically meaningful, with local correlation patterns recurring across distant regions of the latent space.

Conclusion: LAVA provides interpretable, locality-aware explanations for unsupervised latent structures, enabling better understanding of what the model has learned and supporting scientific discovery by linking latent organization to input features.

Abstract: Unsupervised black-box models can be drivers of scientific discovery, but
remain difficult to interpret. Crucially, discovery hinges on understanding the
model output, which is often a multi-dimensional latent embedding rather than a
well-defined target. While explainability for supervised learning usually seeks
to uncover how input features are used to predict a target, its unsupervised
counterpart should relate input features to the structure of the learned latent
space. Adaptations of supervised model explainability for unsupervised learning
provide either single-sample or dataset-wide summary explanations. However,
without automated strategies of relating similar samples to one another guided
by their latent proximity, explanations remain either too fine-grained or too
reductive to be meaningful. This is especially relevant for manifold learning
methods that produce no mapping function, leaving us only with the relative
spatial organization of their embeddings. We introduce Locality-Aware Variable
Associations (LAVA), a post-hoc model-agnostic method designed to explain local
embedding organization through its relationship with the input features. To
achieve this, LAVA represents the latent space as a series of localities
(neighborhoods) described in terms of correlations between the original
features, and then reveals reoccurring patterns of correlations across the
entire latent space. Based on UMAP embeddings of MNIST and a single-cell kidney
dataset, we show that LAVA captures relevant feature associations, with
visually and biologically relevant local patterns shared among seemingly
distant regions of the latent spaces.

</details>


### [180] [CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific Tokenization](https://arxiv.org/abs/2509.21150)
*Ruiyu Wang,Shizhao Sun,Weijian Ma,Jiang Bian*

Main category: cs.LG

TL;DR: A multimodal tokenization for CAD using a sequence-based VQ-VAE improves text-guided CAD prototyping.


<details>
  <summary>Details</summary>
Motivation: Standard LLM tokenizers fragment CAD sequences into natural-language word pieces, losing primitive-level geometric semantics and hindering attention over CAD structure. There is a need for a tokenizer that aligns with CAD's primitive and structural nature to enable unified text-to-CAD generation and editing.

Method: CAD-Tokenizer employs modality-specific tokens and a sequence-based VQ-VAE with primitive-level pooling and constrained decoding to produce compact, primitive-aware representations that reflect CAD's sequential and geometric structure.

Result: Applied to unified text-guided CAD prototyping, CAD-Tokenizer significantly improves instruction following and generation quality, achieving better quantitative and qualitative performance than general-purpose LLMs and task-specific baselines.

Conclusion: Aligning representations with CAD’s structural nature via primitive-aware, multimodal tokenization enhances the effectiveness of text-guided CAD prototyping.

Abstract: Computer-Aided Design (CAD) is a foundational component of industrial
prototyping, where models are defined not by raw coordinates but by
construction sequences such as sketches and extrusions. This sequential
structure enables both efficient prototype initialization and subsequent
editing. Text-guided CAD prototyping, which unifies Text-to-CAD generation and
CAD editing, has the potential to streamline the entire design pipeline.
However, prior work has not explored this setting, largely because standard
large language model (LLM) tokenizers decompose CAD sequences into
natural-language word pieces, failing to capture primitive-level CAD semantics
and hindering attention modules from modeling geometric structure. We
conjecture that a multimodal tokenization strategy, aligned with CAD's
primitive and structural nature, can provide more effective representations. To
this end, we propose CAD-Tokenizer, a framework that represents CAD data with
modality-specific tokens using a sequence-based VQ-VAE with primitive-level
pooling and constrained decoding. This design produces compact, primitive-aware
representations that align with CAD's structural nature. Applied to unified
text-guided CAD prototyping, CAD-Tokenizer significantly improves instruction
following and generation quality, achieving better quantitative and qualitative
performance over both general-purpose LLMs and task-specific baselines.

</details>


### [181] [GRPO is Secretly a Process Reward Model](https://arxiv.org/abs/2509.21154)
*Michael Sullivan*

Main category: cs.LG

TL;DR: GRPO RL implicitly builds a non-trivial process reward model (PRM); introducing a simple lambda-GRPO alleviates non-uniform step distribution, yielding better validation and reasoning performance with little extra cost, suggesting explicit PRMs may be unnecessary.


<details>
  <summary>Details</summary>
Motivation: To understand how GRPO induces PRMs, diagnose flaws due to non-uniform process steps, and improve RL performance by exploiting hidden PRM structure without substantial cost.

Method: Theoretically prove that GRPO induces a non-trivial PRM under within-group overlap assumptions; empirically verify the assumptions and PRM presence in real-world GRPO; identify a flaw where non-uniform process steps hinder exploration/exploitation; propose lambda-GRPO as a simple modification; evaluate on LLMs showing higher validation accuracy and downstream reasoning performance, with faster peak performance; compare against standard GRPO.

Result: GRPO induces a non-trivial PRM. The lambda-GRPO modification improves validation accuracy and downstream reasoning performance and speeds up reaching peak performance. Explicitly defined PRMs may be unnecessary; vanilla GRPO contains a hidden PRM structure that can boost performance with negligible training-time/cost.

Conclusion: The advantage of costly, explicitly-defined PRMs for GRPO is questionable; leveraging the intrinsic PRM structure in vanilla GRPO suffices to boost performance at little additional cost.

Abstract: We prove theoretically that the GRPO RL algorithm induces a non-trivial
process reward model (PRM), under certain assumptions regarding within-group
overlap of token sequences across completions. We then show empirically that
these assumptions are met under real-world conditions: GRPO does in fact induce
a non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a
flaw in the GRPO objective: non-uniformly distributed process steps hinder both
exploration and exploitation (under different conditions). We propose a simple
modification to the algorithm to mitigate this defect ($\lambda$-GRPO), and
show that LLMs trained with $\lambda$-GRPO achieve higher validation accuracy
and performance on downstream reasoning tasks$-$and reach peak performance more
rapidly$-$than LLMs trained with standard GRPO. Our results call into question
the advantage of costly, explicitly-defined PRMs for GRPO: we show that it is
possible to instead leverage the hidden, built-in PRM structure within the
vanilla GRPO algorithm to boost model performance with a negligible impact on
training time and cost.

</details>


### [182] [DATS: Distance-Aware Temperature Scaling for Calibrated Class-Incremental Learning](https://arxiv.org/abs/2509.21161)
*Giuseppe Serra,Florian Buettner*

Main category: cs.LG

TL;DR: Distance-Aware Temperature Scaling (DATS) for continual learning calibrates uncertainty without task labels by using prototype-based distance estimates to adapt the temperature per sample/task, improving cross-task calibration stability.


<details>
  <summary>Details</summary>
Motivation: In continual learning, models must not only avoid catastrophic forgetting but also provide calibrated uncertainty across tasks. Existing CL calibration uses a single global temperature, ignoring task-specific differences and causing calibration fluctuations. Test-time task information is unavailable, creating a need for a task-aware, distance-based calibration that works without prior task labels.

Method: DATS combines prototype-based distance estimation with distance-aware calibration to infer task proximity at test time and assign adaptive temperatures per instance, without requiring task information. It integrates with CL to produce calibrated predictions across a sequence of tasks.

Result: Empirical evaluation on standard continual learning benchmarks and real-world imbalanced biomedical datasets shows DATS reduces calibration error across tasks and is more stable and reliable than state-of-the-art calibration approaches.

Conclusion: Adaptive, task-distance-aware temperature scaling can achieve consistent calibration in continual learning without task labels, enhancing reliability in safety-critical settings and handling task imbalances. Future work could explore richer distance metrics or broader domains.

Abstract: Continual Learning (CL) is recently gaining increasing attention for its
ability to enable a single model to learn incrementally from a sequence of new
classes. In this scenario, it is important to keep consistent predictive
performance across all the classes and prevent the so-called Catastrophic
Forgetting (CF). However, in safety-critical applications, predictive
performance alone is insufficient. Predictive models should also be able to
reliably communicate their uncertainty in a calibrated manner - that is, with
confidence scores aligned to the true frequencies of target events. Existing
approaches in CL address calibration primarily from a data-centric perspective,
relying on a single temperature shared across all tasks. Such solutions
overlook task-specific differences, leading to large fluctuations in
calibration error across tasks. For this reason, we argue that a more
principled approach should adapt the temperature according to the distance to
the current task. However, the unavailability of the task information at test
time/during deployment poses a major challenge to achieve the intended
objective. For this, we propose Distance-Aware Temperature Scaling (DATS),
which combines prototype-based distance estimation with distance-aware
calibration to infer task proximity and assign adaptive temperatures without
prior task information. Through extensive empirical evaluation on both standard
benchmarks and real-world, imbalanced datasets taken from the biomedical
domain, our approach demonstrates to be stable, reliable and consistent in
reducing calibration error across tasks compared to state-of-the-art
approaches.

</details>


### [183] [Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just What They Say](https://arxiv.org/abs/2509.21164)
*Jacob Fein-Ashley,Dhruv Parikh,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.LG

TL;DR: MoT enables latent-space collaboration among heterogeneous LLMs via a lightweight router and cross-attention among a small set of experts, achieving better ID/OOD results with single-pass inference and no fine-tuning of base models.


<details>
  <summary>Details</summary>
Motivation: Open-source LLMs increasingly specialize by domain, motivating cross-model collaboration. Prior approaches are either per-expert routing with independent outputs, costly multi-turn aggregation, or weight-fusing single models, which are disruptive or inflexible. MoT seeks a simple, efficient latent-space mechanism to leverage complementary strengths across heterogeneous experts.

Method: For each query, a lightweight router selects top-K experts and designates a primary expert. Uniformly placed interaction layers project hidden states into a shared latent space where the primary expert cross-attends over its active peers. Pre-trained experts remain frozen; only the router and the lightweight interaction layers are trained with a joint objective aimed at better expert selection and inter-expert collaboration.

Result: Empirical evaluation across five in-distribution and three out-of-distribution benchmarks shows MoT surpassing the state-of-the-art Avengers (routing/aggregation) by +0.38% ID and +2.92% OOD. MoT also outperforms the best single model, while maintaining single-pass inference and runtime comparable to routing baselines, with no iterative aggregation overhead. Code is publicly available.

Conclusion: MoT provides a simple yet effective latent-space mechanism for combining heterogeneous LLMs, representing a practical step toward broader multi-LLM collaboration while preserving efficiency and without requiring base-model fine-tuning.

Abstract: Open-source Large Language Models (LLMs) increasingly specialize by domain
(e.g., math, code, general reasoning), motivating systems that leverage
complementary strengths across models. Prior multi-LLM approaches either (i)
route a query to one or a few experts and generate independently, (ii)
aggregate outputs from each model via costly multi-turn exchanges, or (iii)
fuse weights into a single model-typically requiring architectural homogeneity.
We introduce Mixture of Thoughts (MoT), a simple method for latent-level
collaboration among heterogeneous experts under a global routing scheme. For
each query, a lightweight router selects top-$K$ experts and designates a
primary expert; uniformly placed interaction layers project hidden states into
a shared latent space where the primary expert performs cross-attention over
its active (selected) peers. Pre-trained experts remain frozen; only the router
and the lightweight interaction layers are trained with a novel joint training
objective that improves both the expert selection and inter-expert
collaboration. Across five in-distribution (ID) and three out-of-distribution
(OOD) benchmarks, MoT surpasses the current routing and aggregation-based
state-of-the-art, Avengers, by $+0.38\%$ and $+2.92\%$, respectively. Further,
MoT significantly outperforms the best-performing single model. It achieves
this with single-pass inference, runtime comparable to routing baselines, and
none of the overheads of iterative aggregation. MoT offers a simple
latent-space mechanism for combining heterogeneous LLMs, a practical step
toward broader multi-LLM collaboration. Our code is publicly available at
https://github.com/jacobfa/mot.

</details>


### [184] [A Unified Framework for Diffusion Model Unlearning with f-Divergence](https://arxiv.org/abs/2509.21167)
*Nicola Novello,Federico Fontana,Luigi Cinque,Deniz Gunduz,Andrea M. Tonello*

Main category: cs.LG

TL;DR: A unified f-divergence framework generalizes MSE-based unlearning in diffusion models, allowing selection of divergences to balance aggressive unlearning with concept preservation.


<details>
  <summary>Details</summary>
Motivation: To reliably remove specific knowledge from text-to-image diffusion models, overcoming limitations of MSE-based unlearning and enabling flexible control over trade-offs between unlearning strength and concept retention.

Method: Introduce a framework parameterized by arbitrary f-divergences; show MSE is a special case; analyze how different f-divergences affect convergence and unlearning quality; discuss criteria for divergence selection.

Result: The MSE-based approach is a special case within the unified framework; different f-divergences yield different convergence behavior and unlearning quality; the framework provides a flexible paradigm for application-specific divergence choice.

Conclusion: Employing an f-divergence-based framework enables tailored unlearning by selecting divergences that balance aggressiveness and concept preservation, potentially improving performance over MSE-only methods.

Abstract: Machine unlearning aims to remove specific knowledge from a trained model.
While diffusion models (DMs) have shown remarkable generative capabilities,
existing unlearning methods for text-to-image (T2I) models often rely on
minimizing the mean squared error (MSE) between the output distribution of a
target and an anchor concept. We show that this MSE-based approach is a special
case of a unified $f$-divergence-based framework, in which any $f$-divergence
can be utilized. We analyze the benefits of using different $f$-divergences,
that mainly impact the convergence properties of the algorithm and the quality
of unlearning. The proposed unified framework offers a flexible paradigm that
allows to select the optimal divergence for a specific application, balancing
different trade-offs between aggressive unlearning and concept preservation.

</details>


### [185] [Inverse Reinforcement Learning Using Just Classification and a Few Regressions](https://arxiv.org/abs/2509.21172)
*Lars van der Laan,Nathan Kallus,Aurélien Bibaut*

Main category: cs.LG

TL;DR: Softmax IRL can be reduced to two supervised learning tasks via a linear fixed-point characterization, yielding a simple, modular algorithm with theoretical guarantees and competitive results to MaxEnt IRL.


<details>
  <summary>Details</summary>
Motivation: IRL methods are powerful but practical algorithms rely on complex optimization; a simpler, modular approach using a linear fixed-point characterization improves applicability with neural nets and boosting.

Method: Derive a linear fixed-point equation for the population maximum-likelihood solution of softmax IRL that involves the behavior policy; reduce to probabilistic classification to estimate the behavior policy and iterative regression to solve the fixed point; present an oracle-based algorithm and finite-sample analysis.

Result: A simple, modular IRL method across function approximators; precise optimal-solution characterization; finite-sample error bounds; empirical results showing competitive or superior performance to MaxEnt IRL.

Conclusion: Softmax IRL can be reformulated as two standard supervised learning problems, providing a practical, scalable, and theory-backed approach to IRL with broad compatibility.

Abstract: Inverse reinforcement learning (IRL) aims to explain observed behavior by
uncovering an underlying reward. In the maximum-entropy or
Gumbel-shocks-to-reward frameworks, this amounts to fitting a reward function
and a soft value function that together satisfy the soft Bellman consistency
condition and maximize the likelihood of observed actions. While this
perspective has had enormous impact in imitation learning for robotics and
understanding dynamic choices in economics, practical learning algorithms often
involve delicate inner-loop optimization, repeated dynamic programming, or
adversarial training, all of which complicate the use of modern, highly
expressive function approximators like neural nets and boosting. We revisit
softmax IRL and show that the population maximum-likelihood solution is
characterized by a linear fixed-point equation involving the behavior policy.
This observation reduces IRL to two off-the-shelf supervised learning problems:
probabilistic classification to estimate the behavior policy, and iterative
regression to solve the fixed point. The resulting method is simple and modular
across function approximation classes and algorithms. We provide a precise
characterization of the optimal solution, a generic oracle-based algorithm,
finite-sample error bounds, and empirical results showing competitive or
superior performance to MaxEnt IRL.

</details>


### [186] [Closed-form $\ell_r$ norm scaling with data for overparameterized linear regression and diagonal linear networks under $\ell_p$ bias](https://arxiv.org/abs/2509.21181)
*Shuofeng Zhang,Ard Louis*

Main category: cs.LG

TL;DR: Unified high-probability scaling of the l_r norms (r in [1,p]) of the minimum-l_p interpolator in overparameterized linear regression with isotropic Gaussian design. Key results: a data-dependent elbow n_* and a universal threshold r_* = 2(p−1) separating plateauing vs. growing norms; a dual-ray analysis reveals competition between a signal spike and a bulk of null coordinates in X^T Y. Extension to diagonal linear networks shows the same elbow/threshold laws via effective p. Implications: generalization proxies depending on l_r norms will be sensitive to the chosen norm; connects explicit and implicit bias.


<details>
  <summary>Details</summary>
Motivation: To resolve how parameter norms scale with sample size under l_p-biased interpolation in overparameterized linear models and to understand how different l_r norms influence generalization proxies and the connection between explicit and implicit bias.

Method: Dual-ray analysis of X^T Y decomposing into a signal spike and a bulk of null coordinates; derive closed-form predictions for the elbow n_* and the universal threshold r_* = 2(p−1); establish scaling laws for the entire family r in [1,p]; calibrate diagonal linear networks (DLNs) via initialization scale to an effective p_eff(alpha) using the separable potential, showing the same elbow/threshold behavior empirically.

Result: Closed-form predictions for the elbow n_* and threshold r_*; unified scaling laws for all l_r norms in r∈[1,p] under l_p interpolation; demonstration that norms plateau or continue to grow beyond r_*; empirical DLN experiments confirm the transferable elbow/threshold behavior across explicit and implicit biases.

Conclusion: The choice of l_r norm crucially affects generalization proxies; the paper provides a unified framework linking explicit l_p interpolation to implicit-bias dynamics through elbow/threshold laws and suggests DLNs inherit these laws, offering a predictive bridge between explicit and implicit bias.

Abstract: For overparameterized linear regression with isotropic Gaussian design and
minimum-$\ell_p$ interpolator $p\in(1,2]$, we give a unified, high-probability
characterization for the scaling of the family of parameter norms $ \\{ \lVert
\widehat{w_p} \rVert_r \\}_{r \in [1,p]} $ with sample size.
  We solve this basic, but unresolved question through a simple dual-ray
analysis, which reveals a competition between a signal *spike* and a *bulk* of
null coordinates in $X^\top Y$, yielding closed-form predictions for (i) a
data-dependent transition $n_\star$ (the "elbow"), and (ii) a universal
threshold $r_\star=2(p-1)$ that separates $\lVert \widehat{w_p} \rVert_r$'s
which plateau from those that continue to grow with an explicit exponent.
  This unified solution resolves the scaling of *all* $\ell_r$ norms within the
family $r\in [1,p]$ under $\ell_p$-biased interpolation, and explains in one
picture which norms saturate and which increase as $n$ grows.
  We then study diagonal linear networks (DLNs) trained by gradient descent. By
calibrating the initialization scale $\alpha$ to an effective
$p_{\mathrm{eff}}(\alpha)$ via the DLN separable potential, we show empirically
that DLNs inherit the same elbow/threshold laws, providing a predictive bridge
between explicit and implicit bias.
  Given that many generalization proxies depend on $\lVert \widehat {w_p}
\rVert_r$, our results suggest that their predictive power will depend
sensitively on which $l_r$ norm is used.

</details>


### [187] [Towards Foundation Models for Zero-Shot Time Series Anomaly Detection: Leveraging Synthetic Data and Relative Context Discrepancy](https://arxiv.org/abs/2509.21190)
*Tian Lan,Hao Duong Le,Jinbo Li,Wenjun He,Meng Wang,Chenghao Liu,Chen Zhang*

Main category: cs.LG

TL;DR: TimeRCD introduces a time series anomaly detection foundation model trained with Relative Context Discrepancy (RCD), which detects anomalies by comparing adjacent time windows using a Transformer and synthetic token-level labels, achieving strong zero-shot performance across datasets.


<details>
  <summary>Details</summary>
Motivation: Reconstruction-based TSAD models suffer from objective mismatch, missing subtle anomalies and producing false positives; there is a need for a model that generalizes to unseen data via a relational, context-aware pre-training paradigm.

Method: Pre-train a Transformer-based model with Relative Context Discrepancy to identify anomalies by detecting large discrepancies between neighboring time windows. Train on a large-scale, diverse synthetic corpus with token-level anomaly labels to provide rich supervision.

Result: TimeRCD significantly outperforms existing general-purpose and anomaly-specific foundation models in zero-shot TSAD across diverse datasets.

Conclusion: The RCD paradigm provides a superior, generalizable foundation for TSAD, establishing a new path toward robust zero-shot anomaly detection in time series.

Abstract: Time series anomaly detection (TSAD) is a critical task, but developing
models that generalize to unseen data in a zero-shot manner remains a major
challenge. Prevailing foundation models for TSAD predominantly rely on
reconstruction-based objectives, which suffer from a fundamental objective
mismatch: they struggle to identify subtle anomalies while often
misinterpreting complex normal patterns, leading to high rates of false
negatives and positives. To overcome these limitations, we introduce
\texttt{TimeRCD}, a novel foundation model for TSAD built upon a new
pre-training paradigm: Relative Context Discrepancy (RCD). Instead of learning
to reconstruct inputs, \texttt{TimeRCD} is explicitly trained to identify
anomalies by detecting significant discrepancies between adjacent time windows.
This relational approach, implemented with a standard Transformer architecture,
enables the model to capture contextual shifts indicative of anomalies that
reconstruction-based methods often miss. To facilitate this paradigm, we
develop a large-scale, diverse synthetic corpus with token-level anomaly
labels, providing the rich supervisory signal necessary for effective
pre-training. Extensive experiments demonstrate that \texttt{TimeRCD}
significantly outperforms existing general-purpose and anomaly-specific
foundation models in zero-shot TSAD across diverse datasets. Our results
validate the superiority of the RCD paradigm and establish a new, effective
path toward building robust and generalizable foundation models for time series
anomaly detection.

</details>


### [188] [Differential-Integral Neural Operator for Long-Term Turbulence Forecasting](https://arxiv.org/abs/2509.21196)
*Hao Wu,Yuan Gao,Fan Xu,Fan Zhang,Qingsong Wen,Kun Wang,Xiaomeng Huang,Xian Wu*

Main category: cs.LG

TL;DR: A physics-informed dual-branch neural operator (local differential via constrained CNN; global integral via Transformer) for long-horizon turbulence forecasting, addressing error accumulation and fidelity loss; shows strong long-term performance on 2D Kolmogorov flow.


<details>
  <summary>Details</summary>
Motivation: Long-term turbulence forecasting is notoriously difficult due to simultaneous local dissipative dynamics and global non-local interactions. Standard neural operators struggle with error accumulation in autoregressive predictions, threatening physical fidelity. A principled operator-decomposition approach aims to separate and learn these distinct mathematical structures.

Method: Parallel architecture with a local differential operator implemented as a constrained convolutional network guaranteed to converge to a derivative, and a global integral operator implemented as a Transformer learning a data-driven global kernel. The two branches operate together to evolve turbulence, providing stability and physical fidelity. Training/testing on the 2D Kolmogorov flow benchmark.

Result: Significant improvement over state-of-the-art models in long-term forecasting. Suppresses error accumulation over hundreds of timesteps, preserves vorticity fields and energy spectra, and sets a new benchmark for physically consistent, long-range turbulence forecasting.

Conclusion: A physics-based decomposition into local and global operators yields exceptional stability and robustness for long-horizon turbulence prediction, demonstrating the potential of operator architecture design informed by underlying physics.

Abstract: Accurately forecasting the long-term evolution of turbulence represents a
grand challenge in scientific computing and is crucial for applications ranging
from climate modeling to aerospace engineering. Existing deep learning methods,
particularly neural operators, often fail in long-term autoregressive
predictions, suffering from catastrophic error accumulation and a loss of
physical fidelity. This failure stems from their inability to simultaneously
capture the distinct mathematical structures that govern turbulent dynamics:
local, dissipative effects and global, non-local interactions. In this paper,
we propose the
{\textbf{\underline{D}}}ifferential-{\textbf{\underline{I}}}ntegral
{\textbf{\underline{N}}}eural {\textbf{\underline{O}}}perator (\method{}), a
novel framework designed from a first-principles approach of operator
decomposition. \method{} explicitly models the turbulent evolution through
parallel branches that learn distinct physical operators: a local differential
operator, realized by a constrained convolutional network that provably
converges to a derivative, and a global integral operator, captured by a
Transformer architecture that learns a data-driven global kernel. This
physics-based decomposition endows \method{} with exceptional stability and
robustness. Through extensive experiments on the challenging 2D Kolmogorov flow
benchmark, we demonstrate that \method{} significantly outperforms
state-of-the-art models in long-term forecasting. It successfully suppresses
error accumulation over hundreds of timesteps, maintains high fidelity in both
the vorticity fields and energy spectra, and establishes a new benchmark for
physically consistent, long-range turbulence forecast.

</details>


### [189] [From Physics to Machine Learning and Back: Part II - Learning and Observational Bias in PHM](https://arxiv.org/abs/2509.21207)
*Olga Fink,Ismail Nejjar,Vinay Sharma,Keivan Faghih Niresi,Han Sun,Hao Dong,Chenghao Xu,Amaury Wei,Arthur Bizzi,Raffael Theiler,Yuan Tian,Leandro Von Krannichfeldt,Zhan Ma,Sergei Garmaev,Zepeng Zhang,Mengjie Zhao*

Main category: cs.LG

TL;DR: Physics-informed machine learning for prognostics and health management (PHM) aims to overcome noisy/incomplete data, limited labels, and nonlinear degradation by embedding physical knowledge into models. It combines learning biases, observational strategies, reinforcement learning for maintenance decision-making, and scaling techniques to enable fleet-wide, adaptive PHM.


<details>
  <summary>Details</summary>
Motivation: PHM in real systems faces data quality issues, scarce labels, and complex, nonlinear degradation with system interdependencies. Pure data-driven methods can yield physically inconsistent predictions. Integrating physics informs models, improving reliability, interpretability, and decision-making. The review argues this integration can bridge prediction and action, from individual assets to fleets.

Method: - Learning biases: physics-informed loss functions, governing equations, and monotonicity constraints to steer training toward physically plausible predictions. 
- Observational biases: virtual sensing for unmeasured states, physics-based data augmentation, and multi-sensor fusion to capture realistic system behavior. 
- Decision-making: reinforcement learning to derive maintenance policies that respect physical constraints while optimizing objectives. 
- Scaling: meta-learning, few-shot learning, and domain generalization to extend solutions from single assets to fleets.

Result: As a review, the paper synthesizes approaches and outlines how physics-informed modeling and data strategies can yield physically consistent predictions, data-efficient learning, and the ability to transition from passive prediction to active decision-making. It highlights improvements in reliability and adaptability, while acknowledging challenges in scaling PHM across fleets and integrating disparate data sources.

Conclusion: Physics-informed PHM offers a promising path to reliable, adaptive maintenance and fleet-wide deployment by fusing physical laws with data-driven learning, guiding decisions under constraints. Key challenges include robustly integrating physics with data, scalable training, and ensuring generalization across assets; future work centers on advanced adaptation (meta-learning), robust data strategies, and validation benchmarks.

Abstract: Prognostics and Health Management ensures the reliability, safety, and
efficiency of complex engineered systems by enabling fault detection,
anticipating equipment failures, and optimizing maintenance activities
throughout an asset lifecycle. However, real-world PHM presents persistent
challenges: sensor data is often noisy or incomplete, available labels are
limited, and degradation behaviors and system interdependencies can be highly
complex and nonlinear. Physics-informed machine learning has emerged as a
promising approach to address these limitations by embedding physical knowledge
into data-driven models. This review examines how incorporating learning and
observational biases through physics-informed modeling and data strategies can
guide models toward physically consistent and reliable predictions. Learning
biases embed physical constraints into model training through physics-informed
loss functions and governing equations, or by incorporating properties like
monotonicity. Observational biases influence data selection and synthesis to
ensure models capture realistic system behavior through virtual sensing for
estimating unmeasured states, physics-based simulation for data augmentation,
and multi-sensor fusion strategies. The review then examines how these
approaches enable the transition from passive prediction to active
decision-making through reinforcement learning, which allows agents to learn
maintenance policies that respect physical constraints while optimizing
operational objectives. This closes the loop between model-based predictions,
simulation, and actual system operation, empowering adaptive decision-making.
Finally, the review addresses the critical challenge of scaling PHM solutions
from individual assets to fleet-wide deployment. Fast adaptation methods
including meta-learning and few-shot learning are reviewed alongside domain
generalization techniques ...

</details>


### [190] [Go With The Flow: Churn-Tolerant Decentralized Training of Large Language Models](https://arxiv.org/abs/2509.21221)
*Nikolay Blagoev,Bart Cox,Jérémie Decouchant,Lydia Y. Chen*

Main category: cs.LG

TL;DR: GWTF enables crash-tolerant decentralized LLM training across heterogeneous clients, reducing training time by up to 45% in churn-rich, geographically dispersed deployments.


<details>
  <summary>Details</summary>
Motivation: Democratize LLM development by removing reliance on centralized clusters and accommodating heterogeneous, intermittently connected participants while handling churn and network instability.

Method: Proposes a decentralized flow algorithm to optimize routing of microbatches, maximizing throughput and minimizing delay; supports node churn and unreliable links; evaluated on GPT-like and LLaMa-like models across 10 locations, compared to prior art.

Result: Achieves up to 45% faster training times under challenging conditions with realistic heterogeneity and churn, demonstrating robustness and practicality.

Conclusion: GWTF offers a crash-tolerant, practical decentralized training pathway for LLMs, enabling broader participation and faster collaborative training; highlights potential for democratizing LLM development and future work on scalability and security.

Abstract: Motivated by the emergence of large language models (LLMs) and the importance
of democratizing their training, we propose GWTF, the first crash tolerant
practical decentralized training framework for LLMs. Differently from existing
distributed and federated training frameworks, GWTF enables the efficient
collaborative training of a LLM on heterogeneous clients that volunteer their
resources. In addition, GWTF addresses node churn, i.e., clients joining or
leaving the system at any time, and network instabilities, i.e., network links
becoming unstable or unreliable. The core of GWTF is a novel decentralized flow
algorithm that finds the most effective routing that maximizes the number of
microbatches trained with the lowest possible delay. We extensively evaluate
GWTF on GPT-like and LLaMa-like models and compare it against the prior art.
Our results indicate that GWTF reduces the training time by up to 45% in
realistic and challenging scenarios that involve heterogeneous client nodes
distributed over 10 different geographic locations with a high node churn rate.

</details>


### [191] [Tree Search for LLM Agent Reinforcement Learning](https://arxiv.org/abs/2509.21240)
*Yuxiang Ji,Ziyu Ma,Yong Wang,Guanhua Chen,Xiangxiang Chu,Liaoni Wu*

Main category: cs.LG

TL;DR: Tree-GRPO introduces a tree-based grouped RL approach for LLM agents to address sparse supervision in long-horizon tasks. By representing each interaction step as a tree node and sharing prefixes, it increases rollout efficiency and enables step-wise supervision from outcome rewards, achieving superior performance over chain-based RL across multiple QA tasks and datasets.


<details>
  <summary>Details</summary>
Motivation: Long-horizon agent tasks with LLMs suffer from sparse supervision when learning from outcome rewards alone. A more data-efficient RL signal and higher rollout efficiency are needed to train agents effectively under budget constraints.

Method: Construct a tree where each node is a complete agent interaction step and use tree search to share prefixes, increasing rollouts under a fixed token/tool budget. Propose Tree-GRPO, which estimates grouped relative advantages intra-tree and inter-tree; demonstrates that intra-tree objectives align with step-level direct preference learning; uses outcome rewards to derive step-wise supervision via the tree-structured trajectories.

Result: Empirical evaluation across 11 datasets and 3 QA tasks shows Tree-GRPO outperforms chain-based RL methods, indicating improved data efficiency and performance in long-horizon, multi-turn tasks.

Conclusion: Tree-GRPO provides a principled framework that leverages tree-structured trajectories to convert sparse outcome rewards into step-level supervision, achieving superior performance and theoretical alignment with step-level preference learning, suggesting promise for scalable RL in LLM-driven agents.

Abstract: Recent advances in reinforcement learning (RL) have significantly enhanced
the agentic capabilities of large language models (LLMs). In long-term and
multi-turn agent tasks, existing approaches driven solely by outcome rewards
often suffer from the problem of sparse supervision. To address the challenge,
we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped
agent RL method based on tree search, where each tree node represents the
complete agent interaction step. By sharing common prefixes, the tree search
sampling increases the number of rollouts achievable within a fixed budget of
tokens or tool calls. Moreover, we find that the tree-structured trajectory
naturally allows the construction of step-wise process supervised signals even
using only the outcome reward. Based on this, Tree-GRPO estimates the grouped
relative advantages both on intra-tree and inter-tree levels. Through
theoretical analysis, we demonstrate that the objective of intra-tree level
group relative policy optimization is equivalent to that of step-level direct
preference learning. Experiments across 11 datasets and 3 types of QA tasks
demonstrate the superiority of the proposed tree-based RL over the chain-based
RL method.

</details>


### [192] [AbideGym: Turning Static RL Worlds into Adaptive Challenges](https://arxiv.org/abs/2509.21234)
*Abi Aryan,Zac Liu,Aaron Childress*

Main category: cs.LG

TL;DR: AbideGym offers a dynamic, agent-aware perturbation framework for MiniGrid to force intra-episode adaptation, enabling robust evaluation and progress in curriculum/continual learning and generalization.


<details>
  <summary>Details</summary>
Motivation: RL policies are brittle to dynamics shifts and static benchmarks fail to reveal weaknesses; a dynamic, scalable evaluation framework is needed to promote resilience and intra-episode adaptation.

Method: A dynamic MiniGrid wrapper, AbideGym, introducing agent-aware perturbations and scalable environment complexity to enforce adaptation within episodes; designed to be modular and reproducible for research workflows.

Result: Reveals weaknesses in static policies and promotes resilience; provides a modular evaluation framework to advance research in curriculum learning, continual learning, and robust generalization.

Conclusion: AbideGym serves as a modular, reproducible benchmark that supports progress in curriculum learning, continual learning, and robust generalization for reinforcement learning through dynamic evaluation and intra-episode adaptation.

Abstract: Agents trained with reinforcement learning often develop brittle policies
that fail when dynamics shift, a problem amplified by static benchmarks.
AbideGym, a dynamic MiniGrid wrapper, introduces agent-aware perturbations and
scalable complexity to enforce intra-episode adaptation. By exposing weaknesses
in static policies and promoting resilience, AbideGym provides a modular,
reproducible evaluation framework for advancing research in curriculum
learning, continual learning, and robust generalization.

</details>


### [193] [Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven Framework](https://arxiv.org/abs/2509.21241)
*Yucheng Wang,Ziyang Chen,Md Faisal Kabir*

Main category: cs.LG

TL;DR: A counterfactual, graph-based explainer reveals how LoRA fine-tuning reshapes an LLM's structural reasoning and semantic behavior by learning sparse, perturbation-based masks on a domain-specific knowledge graph.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms by which LoRA-based fine-tuning alters LLM reasoning and semantics, and to establish an interpretable framework using counterfactuals grounded in domain knowledge graphs.

Method: Build BioToolKG (a heterogeneous bioinformatics tool knowledge graph) and develop CFFTLLMExplainer, which learns soft masks over graph nodes/edges to create minimal structural perturbations that maximize semantic divergence while enforcing sparsity and interpretability constraints (entropy regularization, edge smoothness). Applied to a fine-tuned LLaMA-based LLM.

Result: Counterfactual masking uncovers the model’s structural dependencies and aligns with LoRA-induced parameter shifts, providing insights into the internal mechanisms of fine-tuned LLMs; demonstrates the viability of counterfactual graphs as interpretable AI tools in bio-domain LLMs.

Conclusion: Counterfactual graphs are a promising approach for interpreting fine-tuned LLMs and can illuminate the relationship between parameter-level changes (due to LoRA) and higher-level structural/semantic behavior, with domain-specific knowledge graphs as a natural explanatory medium.

Abstract: The widespread adoption of Low-Rank Adaptation (LoRA) has enabled large
language models (LLMs) to acquire domain-specific knowledge with remarkable
efficiency. However, understanding how such a fine-tuning mechanism alters a
model's structural reasoning and semantic behavior remains an open challenge.
This work introduces a novel framework that explains fine-tuned LLMs via
counterfactuals grounded in knowledge graphs. Specifically, we construct
BioToolKG, a domain-specific heterogeneous knowledge graph in bioinformatics
tools and design a counterfactual-based fine-tuned LLMs explainer
(CFFTLLMExplainer) that learns soft masks over graph nodes and edges to
generate minimal structural perturbations that induce maximum semantic
divergence. Our method jointly optimizes structural sparsity and semantic
divergence while enforcing interpretability preserving constraints such as
entropy regularization and edge smoothness. We apply this framework to a
fine-tuned LLaMA-based LLM and reveal that counterfactual masking exposes the
model's structural dependencies and aligns with LoRA-induced parameter shifts.
This work provides new insights into the internal mechanisms of fine-tuned LLMs
and highlights counterfactual graphs as a potential tool for interpretable AI.

</details>


### [194] [Federated Flow Matching](https://arxiv.org/abs/2509.21250)
*Zifan Wang,Anqi Dong,Mahmoud Selim,Michael M. Zavlanos,Karl H. Johansson*

Main category: cs.LG

TL;DR: Federated Flow Matching (FFM) enables privacy-preserving training of flow-matching models on distributed data, presenting three variants (FFM-vanilla, FFM-LOT, FFM-GOT) that trade off local privacy, flow straightness, and global consistency; performance approaches centralized baselines.


<details>
  <summary>Details</summary>
Motivation: Data is decentralized and privacy regulations prevent central aggregation; there is a need to train generative models directly from distributed data without centralizing sensitive information.

Method: FFM-vanilla trains locally with independent source and target couplings. FFM-LOT uses local optimal transport couplings to improve flow straightness but lacks global consistency under heterogeneous data. FFM-GOT applies a semi-dual formulation of optimal transport with a shared global potential to coordinate couplings across clients.

Result: Experiments on synthetic and image datasets show that FFM enables privacy-preserving training while improving flow straightness and sample quality in federated settings, with performance comparable to the centralized baseline.

Conclusion: FFM provides a privacy-preserving framework for federated flow matching, balancing privacy, flow geometry, and global consistency while achieving performance close to centralized training.

Abstract: Data today is decentralized, generated and stored across devices and
institutions where privacy, ownership, and regulation prevent centralization.
This motivates the need to train generative models directly from distributed
data locally without central aggregation. In this paper, we introduce Federated
Flow Matching (FFM), a framework for training flow matching models under
privacy constraints. Specifically, we first examine FFM-vanilla, where each
client trains locally with independent source and target couplings, preserving
privacy but yielding curved flows that slow inference. We then develop FFM-LOT,
which employs local optimal transport couplings to improve straightness within
each client but lacks global consistency under heterogeneous data. Finally, we
propose FFM-GOT, a federated strategy based on the semi-dual formulation of
optimal transport, where a shared global potential function coordinates
couplings across clients. Experiments on synthetic and image datasets show that
FFM enables privacy-preserving training while enhancing both the flow
straightness and sample quality in federated settings, with performance
comparable to the centralized baseline.

</details>


### [195] [humancompatible.train: Implementing Optimization Algorithms for Stochastically-Constrained Stochastic Optimization Problems](https://arxiv.org/abs/2509.21254)
*Andrii Kliachkin,Jana Lepšová,Gilles Bareilles,Jakub Mareček*

Main category: cs.LG

TL;DR: A PyTorch-based toolkit, humancompatible.train, for training DNNs under stochastic constraints, implementing several previously unimplemented algorithms and demonstrating usage by comparing two fairness-constrained algorithms.


<details>
  <summary>Details</summary>
Motivation: Growing interest in constrained training for safety and fairness; lack of an industry-standard, extensible toolkits; need for resources to implement and compare stochastic constrained optimization algorithms.

Method: Develop a modular, PyTorch-based package (humancompatible.train) with multiple stochastic constrained optimization algorithms, and provide a demonstration by comparing two of them on a deep learning task with fairness constraints.

Result: The toolkit is implemented; multiple algorithms are provided; a demonstration shows how to apply them to fairness constraints by comparing two algorithms on a DL task.

Conclusion: The work delivers an extendable framework for stochastically constrained training that could standardize industry practice and facilitate fair/safe AI deployment; further experiments and adoption are anticipated.

Abstract: There has been a considerable interest in constrained training of deep neural
networks (DNNs) recently for applications such as fairness and safety. Several
toolkits have been proposed for this task, yet there is still no industry
standard. We present humancompatible.train
(https://github.com/humancompatible/train), an easily-extendable PyTorch-based
Python package for training DNNs with stochastic constraints. We implement
multiple previously unimplemented algorithms for stochastically constrained
stochastic optimization. We demonstrate the toolkit use by comparing two
algorithms on a deep learning task with fairness constraints.

</details>


### [196] [A Causality-Aware Spatiotemporal Model for Multi-Region and Multi-Pollutant Air Quality Forecasting](https://arxiv.org/abs/2509.21260)
*Junxin Lu,Shiliang Sun*

Main category: cs.LG

TL;DR: AirPCM is a unified deep spatiotemporal model for multi-region, multi-pollutant air quality forecasting that jointly models cross-station spatial correlations, temporal dynamics, and meteorology–pollutant causal relations, yielding superior accuracy, generalization, and actionable long-term insights across scales.


<details>
  <summary>Details</summary>
Motivation: Forecasting air pollution accurately and scalably is challenging due to multi-pollutant interactions, dynamic meteorological conditions, and region-specific spatial heterogeneity. Existing methods often focus on a single pollutant or localized areas, limiting cross-station and cross-pollutant insight and hindering long-horizon planning.

Method: AirPCM introduces a unified architecture that jointly captures cross-station spatial correlations, temporal auto-correlations, and explicit meteorology–pollutant causal dynamics across multiple regions and pollutants, enabling interpretable multi-pollutant forecasting across geographic and temporal scales.

Result: Empirical evaluations on multi-scale real-world datasets show AirPCM consistently surpasses state-of-the-art baselines in predictive accuracy and generalization. Its long-term forecasting capability provides actionable insights into future air quality trends and high-risk windows, supporting evidence-based environmental governance and carbon mitigation planning.

Conclusion: AirPCM enables fine-grained, interpretable multi-pollutant forecasting across varying geographic and temporal scales, including sudden pollution episodes, and offers improved accuracy, generalization, and governance-ready insights.

Abstract: Air pollution, a pressing global problem, threatens public health,
environmental sustainability, and climate stability. Achieving accurate and
scalable forecasting across spatially distributed monitoring stations is
challenging due to intricate multi-pollutant interactions, evolving
meteorological conditions, and region specific spatial heterogeneity. To
address this challenge, we propose AirPCM, a novel deep spatiotemporal
forecasting model that integrates multi-region, multi-pollutant dynamics with
explicit meteorology-pollutant causality modeling. Unlike existing methods
limited to single pollutants or localized regions, AirPCM employs a unified
architecture to jointly capture cross-station spatial correlations, temporal
auto-correlations, and meteorology-pollutant dynamic causality. This empowers
fine-grained, interpretable multi-pollutant forecasting across varying
geographic and temporal scales, including sudden pollution episodes. Extensive
evaluations on multi-scale real-world datasets demonstrate that AirPCM
consistently surpasses state-of-the-art baselines in both predictive accuracy
and generalization capability. Moreover, the long-term forecasting capability
of AirPCM provides actionable insights into future air quality trends and
potential high-risk windows, offering timely support for evidence-based
environmental governance and carbon mitigation planning.

</details>


### [197] [SuperOffload: Unleashing the Power of Large-Scale LLM Training on Superchips](https://arxiv.org/abs/2509.21271)
*Xinyu Lian,Masahiro Tanaka,Olatunji Ruwase,Minjia Zhang*

Main category: cs.LG

TL;DR: Proposes SuperOffload, a Superchip-aware offloading system for LLM training on GH200 that uses Hopper GPU, Grace CPU, and NVLink-C2C; achieves up to 2.5x throughput over state-of-the-art offloading; enables training of up to 25B models on a single Superchip; extends to ZeRO-style data parallelism and DeepSpeed-Ulysses sequence parallelism allowing 13B models with sequence lengths up to 1e6 on 8 GH200 with 55% MFU.


<details>
  <summary>Details</summary>
Motivation: LLM training on tightly-coupled Superchips is underexplored; existing offloading work assumes loosely-coupled GPU-CPU architectures and may not exploit the full potential of integrated GPUs/CPUs and fast interconnects.

Method: Introduces SuperOffload, a Superchip-centric offloading system that coordinates Hopper GPU, Grace CPU, and NVLink-C2C. Key techniques include adaptive weight offloading, bucketization repartitioning, Superchip-aware casting, speculative execution, and an optimized Adam for Grace CPUs. Evaluates on NVIDIA GH200. Further extends with ZeRO-style data parallelism and DeepSpeed-Ulysses sequence parallelism.

Result: Demonstrates up to 2.5x throughput improvement versus state-of-the-art offloading baselines. Makes possible training of 25B models on a single Superchip. With 8 GH200, supports training of 13B models with sequence lengths up to 1,000,000 tokens, achieving 55% MFU.

Conclusion: Shows that Superchip-centric offloading can substantially accelerate LLM training on Superchips and enable very large models with long sequences, highlighting a promising direction for tightly integrated AI accelerators.

Abstract: The emergence of Superchips represents a significant advancement in
next-generation AI hardware. These Superchips employ a tightly coupled
heterogeneous architecture that integrates GPU and CPU on the same package,
which offers unprecedented computational power. However, there has been scant
research investigating how LLM training benefits from this new architecture. In
this work, for the first time, we study LLM training solutions based on
offloading for Superchips. We observe important differences between Superchips
and traditional loosely-coupled GPU-CPU architecture, which necessitate
revisiting prevailing assumptions about offloading. Based on that, we present
SuperOffload, a Superchip-centric offloading system that simultaneously uses
Hopper GPU, Grace CPU, and NVLink-C2C interconnect more efficiently.
SuperOffload accomplishes this via a combination of techniques, such as
adaptive weight offloading, bucketization repartitioning, Superchip-aware
casting, speculative execution, and a highly optimized Adam optimizer for Grace
CPUs. Our evaluation of SuperOffload on NVIDIA GH200 demonstrates up to 2.5x
throughput improvement compared to state-of-the-art offloading-based systems,
enabling training of up to 25B model on a single Superchip while achieving high
training throughput. We also extend SuperOffload with ZeRO-style data
parallelism and DeepSpeed-Ulysses sequence parallelism, enabling training of
13B model with sequence lengths up to 1 million tokens on 8 GH200 while
achieving 55% MFU.

</details>


### [198] [It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL](https://arxiv.org/abs/2509.21282)
*Madeleine Dwyer,Adam Sobey,Adriane Chapman*

Main category: cs.LG

TL;DR: PSPO introduces probability smoothing to replace ratio clipping in GRPO for LLM RLHF fine-tuning. It preserves gradient information, forms a soft trust region toward the old policy, and yields stronger improvements over clipping while matching unclipped performance.


<details>
  <summary>Details</summary>
Motivation: Clipping in policy optimization methods like PPO/GRPO discards information and causes gradient discontinuities, hindering stable learning and potentially limiting performance gains. A gradient-preserving smoothing approach could stabilize updates while retaining useful signals.

Method: Propose Probability Smoothing Policy Optimisation (PSPO) that smooths the current policy probabilities toward the old policy before computing the importance ratio. Implement within GRPO (GR-PSPO) and apply to fine-tuning Qwen2.5-0.5B and Qwen2.5-1.5B on GSM8K, with evaluation on GSM8K test and cross-dataset generalisation on SVAMP, ASDiv, MATH-500.

Result: GR-PSPO yields similar performance to unclipped GRPO (single iteration; no data reuse; ratio=1) but with clearer, more concise, more logical reasoning. Relative to clipped GRPO, GR-PSPO substantially improves performance for both model sizes on GSM8K: 39.7% (GR-PSPO) vs 17.6% (clipped) for 0.5B and 59.4% vs 37.8% for 1.5B. Cross-dataset generalisation is examined on SVAMP, ASDiv, MATH-500.

Conclusion: Probability smoothing offers a gradient-preserving alternative to clipping that creates a soft trust region and comes with formal guarantees. It can boost RLHF-style fine-tuning of LLMs, improving accuracy and reasoning quality without sacrificing stability.

Abstract: Training large language models (LLMs) with reinforcement learning (RL)
methods such as PPO and GRPO commonly relies on ratio clipping to stabilise
updates. While effective at preventing instability, clipping discards
information and introduces gradient discontinuities. We propose Probability
Smoothing Policy Optimisation (PSPO), which smooths the current policy's
probabilities toward the old (behaviour) policy before computing the importance
ratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient
signal, while interpolation toward the old policy creates a soft trust region
that discourages large, destabilising updates, with formal guarantees.
  We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and
Qwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset
generalisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO
(single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar
performance but improves the reasoning leading to clearer and more concise
responses which are more logical. Compared to clipped GRPO, GR-PSPO
substantially improves performance both the 0.5B and 1.5B models, with a boost
of over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).

</details>


### [199] [Optimal Robust Recourse with $L^p$-Bounded Model Change](https://arxiv.org/abs/2509.21293)
*Phone Kyaw,Kshitij Kayastha,Shahin Jabbari*

Main category: cs.LG

TL;DR: Proposes a provably optimal robust recourse algorithm for generalized linear models under Lp perturbations (p≥1, p≠∞), achieving lower recourse cost and sparser, more robust solutions than L∞-based methods.


<details>
  <summary>Details</summary>
Motivation: Need for recourse that remains valid under model updates; L∞-based robustness yields high costs; aim for principled optimality under flexible Lp perturbations.

Method: Develop algorithm with theoretical guarantees of optimal robust recourse for GLMs under Lp model changes; extends prior L∞ results to general p; likely convex optimization formulation and proofs; tests on linear and non-linear models.

Result: Empirically reduces recourse price by orders of magnitude relative to prior work; improves trade-off between cost and validity; produces sparser recourses; robust to post-processing methods ensuring feasibility.

Conclusion: Lp-norm robust recourse provides provable optimality and practical gains; suitable for both linear and non-linear models; offers more scalable and cost-effective recourses with better sparsity and resilience.

Abstract: Recourse provides individuals who received undesirable labels (e.g., denied a
loan) from algorithmic decision-making systems with a minimum-cost improvement
suggestion to achieve the desired outcome. However, in practice, models often
get updated to reflect changes in the data distribution or environment,
invalidating the recourse recommendations (i.e., following the recourse will
not lead to the desirable outcome). The robust recourse literature addresses
this issue by providing a framework for computing recourses whose validity is
resilient to slight changes in the model. However, since the optimization
problem of computing robust recourse is non-convex (even for linear models),
most of the current approaches do not have any theoretical guarantee on the
optimality of the recourse. Recent work by Kayastha et. al. provides the first
provably optimal algorithm for robust recourse with respect to generalized
linear models when the model changes are measured using the $L^{\infty}$ norm.
However, using the $L^{\infty}$ norm can lead to recourse solutions with a high
price. To address this shortcoming, we consider more constrained model changes
defined by the $L^p$ norm, where $p\geq 1$ but $p\neq \infty$, and provide a
new algorithm that provably computes the optimal robust recourse for
generalized linear models. Empirically, for both linear and non-linear models,
we demonstrate that our algorithm achieves a significantly lower price of
recourse (up to several orders of magnitude) compared to prior work and also
exhibits a better trade-off between the implementation cost of recourse and its
validity. Our empirical analysis also illustrates that our approach provides
more sparse recourses compared to prior work and remains resilient to
post-processing approaches that guarantee feasibility.

</details>


### [200] [No Prior, No Leakage: Revisiting Reconstruction Attacks in Trained Neural Networks](https://arxiv.org/abs/2509.21296)
*Yehonatan Refael,Guy Smorodinsky,Ofir Lindenbaum,Itay Safran*

Main category: cs.LG

TL;DR: The paper argues that many proposed training-set reconstruction attacks on neural networks are inherently unreliable without prior data, proving that infinitely many alternative reconstructions can explain the model, and showing that longer training (stronger implicit bias) can actually reduce leakage. It provides both theoretical bounds and empirical evidence, reframing privacy risks and suggesting mitigation by deeper training.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental reliability and limitations of training-data reconstruction attacks, and to reconcile privacy concerns with the generalization benefits of implicit bias in neural networks.

Method: Theoretical analysis proving non-uniqueness of reconstructions without priors, constructing infinitely many alternative solutions; empirical experiments showing exact duplicates occur by chance; analysis of how training extent and implicit bias affect susceptibility to leakage.

Result: There exist infinitely many alternative training-set reconstructions consistent with a model in the absence of prior information; exact duplication of training examples is rare and can occur by chance; more extensive training reduces susceptibility to reconstruction attacks.

Conclusion: The results refine the conditions under which training-set leakage is possible and suggest practical mitigation: deeper training and leveraging implicit bias may enhance privacy without sacrificing generalization.

Abstract: The memorization of training data by neural networks raises pressing concerns
for privacy and security. Recent work has shown that, under certain conditions,
portions of the training set can be reconstructed directly from model
parameters. Some of these methods exploit implicit bias toward margin
maximization, suggesting that properties often regarded as beneficial for
generalization may actually compromise privacy. Yet despite striking empirical
demonstrations, the reliability of these attacks remains poorly understood and
lacks a solid theoretical foundation. In this work, we take a complementary
perspective: rather than designing stronger attacks, we analyze the inherent
weaknesses and limitations of existing reconstruction methods and identify
conditions under which they fail. We rigorously prove that, without
incorporating prior knowledge about the data, there exist infinitely many
alternative solutions that may lie arbitrarily far from the true training set,
rendering reconstruction fundamentally unreliable. Empirically, we further
demonstrate that exact duplication of training examples occurs only by chance.
Our results refine the theoretical understanding of when training set leakage
is possible and offer new insights into mitigating reconstruction attacks.
Remarkably, we demonstrate that networks trained more extensively, and
therefore satisfying implicit bias conditions more strongly -- are, in fact,
less susceptible to reconstruction attacks, reconciling privacy with the need
for strong generalization in this setting.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [201] [Boosting LiDAR-Based Localization with Semantic Insight: Camera Projection versus Direct LiDAR Segmentation](https://arxiv.org/abs/2509.20486)
*Sven Ochs,Philip Schörner,Marc René Zofka,J. Marius Zöllner*

Main category: cs.RO

TL;DR: Integrating semantic camera segmentation with LiDAR segmentation improves LiDAR-based localization across diverse sensors, validated on CoCar NextGen using Depth-Anything and adaptive LiDAR segmentation with RTK GNSS over a 55 km Karlsruhe drive.


<details>
  <summary>Details</summary>
Motivation: LiDAR semantic segmentation is challenged by sensor diversity; incorporating camera-derived semantics can boost localization accuracy and robustness in autonomous systems.

Method: Project LiDAR points into the camera's semantic segmentation space using Depth-Anything for camera segmentation and an adaptive LiDAR segmentation network; evaluate on the CoCar NextGen platform with GNSS/RTK ground truth; extensive 55 km urban/rural drive.

Result: The approach yields improved precision and reliability of LiDAR-based localization across multimodal configurations, demonstrating enhanced robustness in real-world urban, multi-lane, and rural environments.

Conclusion: Semantic augmentation of LiDAR localization with camera semantics is effective for improving localization robustness and accuracy across diverse sensor setups, supporting more reliable autonomous navigation.

Abstract: Semantic segmentation of LiDAR data presents considerable challenges,
particularly when dealing with diverse sensor types and configurations.
However, incorporating semantic information can significantly enhance the
accuracy and robustness of LiDAR-based localization techniques for autonomous
mobile systems. We propose an approach that integrates semantic camera data
with LiDAR segmentation to address this challenge. By projecting LiDAR points
into the semantic segmentation space of the camera, our method enhances the
precision and reliability of the LiDAR-based localization pipeline.
  For validation, we utilize the CoCar NextGen platform from the FZI Research
Center for Information Technology, which offers diverse sensor modalities and
configurations. The sensor setup of CoCar NextGen enables a thorough analysis
of different sensor types. Our evaluation leverages the state-of-the-art
Depth-Anything network for camera image segmentation and an adaptive
segmentation network for LiDAR segmentation. To establish a reliable ground
truth for LiDAR-based localization, we make us of a Global Navigation Satellite
System (GNSS) solution with Real-Time Kinematic corrections (RTK).
Additionally, we conduct an extensive 55 km drive through the city of
Karlsruhe, Germany, covering a variety of environments, including urban areas,
multi-lane roads, and rural highways. This multimodal approach paves the way
for more reliable and precise autonomous navigation systems, particularly in
complex real-world environments.

</details>


### [202] [Revisiting Formal Methods for Autonomous Robots: A Structured Survey](https://arxiv.org/abs/2509.20488)
*Atef Azaiez,David A. Anisi,Marie Farrell,Matt Luckcuck*

Main category: cs.RO

TL;DR: Structured literature review on Formal Methods in Robotic Autonomous Systems; maps FM techniques, tracks evolution, and highlights rising use of formal synthesis and probabilistic verification.


<details>
  <summary>Details</summary>
Motivation: To synthesize current applications of Formal Methods to Robotic Autonomous Systems, assess maturity, and extend a prior survey by identifying new trends.

Method: Structured literature review with explicit search strategy, database selection, search strings, filters, and collaborative review; categorization/enumeration of FM approaches and formalisms for specification and verification in RAS; analysis of sub-symbolic AI-enabled RAS and evolution over time; comparison with a prior survey.

Result: Findings indicate persistence of trends observed in the previous survey, with new trends including an increased adoption of Formal Synthesis approaches and probabilistic verification techniques.

Conclusion: The study supports maturation of FM use in RAS, clarifies current FM taxonomy and trends, and identifies future directions for researchers.

Abstract: This paper presents the initial results from our structured literature review
on applications of Formal Methods (FM) to Robotic Autonomous Systems (RAS). We
describe our structured survey methodology; including database selection and
associated search strings, search filters and collaborative review of
identified papers. We categorise and enumerate the FM approaches and formalisms
that have been used for specification and verification of RAS. We investigate
FM in the context of sub-symbolic AI-enabled RAS and examine the evolution of
how FM is used over time in this field. This work complements a pre-existing
survey in this area and we examine how this research area has matured over
time. Specifically, our survey demonstrates that some trends have persisted as
observed in a previous survey. Additionally, it recognized new trends that were
not considered previously including a noticeable increase in adopting Formal
Synthesis approaches as well as Probabilistic Verification Techniques.

</details>


### [203] [Boosting Zero-Shot VLN via Abstract Obstacle Map-Based Waypoint Prediction with TopoGraph-and-VisitInfo-Aware Prompting](https://arxiv.org/abs/2509.20499)
*Boqi Li,Siyuan Li,Weiyi Wang,Anran Li,Zhong Cao,Henry X. Liu*

Main category: cs.RO

TL;DR: A zero-shot vision-language navigation method for continuous environments that combines a simple waypoint predictor with a multimodal LLM, building a dynamic topological graph to guide exploration and local planning, achieving state-of-the-art zero-shot results on R2R-CE and RxR-CE.


<details>
  <summary>Details</summary>
Motivation: To enable effective zero-shot VLN in continuous, obstacle-rich environments by coupling high-level language reasoning with low-level, geometry-aware planning.

Method: Predict linearly reachable waypoints from an abstract obstacle map using a lightweight waypoint predictor; maintain a dynamic topological graph with visitation records; encode the graph and visit history into the MLLM prompt to enable spatial reasoning and error correction via local planning.

Result: Achieves state-of-the-art zero-shot performance on R2R-CE (41% SR) and RxR-CE (36% SR), outperforming prior methods.

Conclusion: The integration of a simple waypoint predictor with an MLLM and an evolving topological graph enables effective zero-shot VLN in continuous environments, providing a practical and scalable approach for embodied navigation.

Abstract: With the rapid progress of foundation models and robotics, vision-language
navigation (VLN) has emerged as a key task for embodied agents with broad
practical applications. We address VLN in continuous environments, a
particularly challenging setting where an agent must jointly interpret natural
language instructions, perceive its surroundings, and plan low-level actions.
We propose a zero-shot framework that integrates a simplified yet effective
waypoint predictor with a multimodal large language model (MLLM). The predictor
operates on an abstract obstacle map, producing linearly reachable waypoints,
which are incorporated into a dynamically updated topological graph with
explicit visitation records. The graph and visitation information are encoded
into the prompt, enabling reasoning over both spatial structure and exploration
history to encourage exploration and equip MLLM with local path planning for
error correction. Extensive experiments on R2R-CE and RxR-CE show that our
method achieves state-of-the-art zero-shot performance, with success rates of
41% and 36%, respectively, outperforming prior state-of-the-art methods.

</details>


### [204] [MELEGROS: Monolithic Elephant-inspired Gripper with Optical Sensors](https://arxiv.org/abs/2509.20510)
*Petr Trunin,Diana Cafiso,Anderson Brazil Nardin,Trevor Exley,Lucia Beccai*

Main category: cs.RO

TL;DR: MELEGROS is a monolithic, elephant-inspired soft gripper with fully embedded optical sensing and pneumatic actuation, fabricated in a single 3D print, enabling continuous structure and integrated perception for versatile manipulation.


<details>
  <summary>Details</summary>
Motivation: To emulate the elephant trunk's seamless sensing-actuation integration, reduce mechanical mismatch between sensors and actuators, and enable simulation-guided, co-fabricated sensor design.

Method: Single-material 3D printing of a pneumatically actuated lattice (12.5 mm cells) embedding six optical waveguides and five pneumatic chambers; continuous resin; four design iterations; simulation-guided sensor placement; no multi-material joints.

Result: Prototype weighs 132 g, lifts >2× its weight, supports elongation, compression, and bending, and decouples tactile vs. proprioceptive signals; capable of pinching, scooping, reaching, and delicate grasping (e.g., grapes); sensors provide distinct responses to touch, bend, and chamber deformation.

Conclusion: Demonstrates a new paradigm for soft robotics where fully embedded sensing within a continuous structure enables versatile, bioinspired manipulation with simplified fabrication.

Abstract: The elephant trunk exemplifies a natural gripper where structure, actuation,
and sensing are seamlessly integrated. Inspired by the distal morphology of the
African elephant trunk, we present MELEGROS, a Monolithic ELEphant-inspired
GRipper with Optical Sensors, emphasizing sensing as an intrinsic,
co-fabricated capability. Unlike multi-material or tendon-based approaches,
MELEGROS directly integrates six optical waveguide sensors and five pneumatic
chambers into a pneumatically actuated lattice structure (12.5 mm cell size)
using a single soft resin and one continuous 3D print. This eliminates
mechanical mismatches between sensors, actuators, and body, reducing model
uncertainty and enabling simulation-guided sensor design and placement. Only
four iterations were required to achieve the final prototype, which features a
continuous structure capable of elongation, compression, and bending while
decoupling tactile and proprioceptive signals. MELEGROS (132 g) lifts more than
twice its weight, performs bioinspired actions such as pinching, scooping, and
reaching, and delicately grasps fragile items like grapes. The integrated
optical sensors provide distinct responses to touch, bending, and chamber
deformation, enabling multifunctional perception. MELEGROS demonstrates a new
paradigm for soft robotics where fully embedded sensing and continuous
structures inherently support versatile, bioinspired manipulation.

</details>


### [205] [Action-Informed Estimation and Planning: Clearing Clutter on Staircases via Quadrupedal Pedipulation](https://arxiv.org/abs/2509.20516)
*Prasanna Sriganesh,Barath Satheeshkumar,Anushree Sabnis,Matthew Travers*

Main category: cs.RO

TL;DR: Interaction-aware perception-action loop enables a quadruped robot to push cluttered objects on stairs while tracking the object under occlusion and learning from outcomes, outperforming open-loop methods.


<details>
  <summary>Details</summary>
Motivation: Dense cluttered environments require contact to clear paths; occlusion of the manipulated object during pushing introduces perceptual uncertainty; a tightly integrated loop using proprioceptive cues is needed to predict and re-detect object motion.

Method: A tightly coupled perception-action framework with an interaction-aware state estimation loop that uses foot contact and leg pose to forecast object displacement during occlusion, guiding perception to re-detect after interaction; learns from outcomes to reclassify objects as movable/immovable; implemented on a Boston Dynamics Spot and evaluated on stairs.

Result: Improved task success rates and tracking accuracy versus open-loop baselines; robust re-detection after partial pushes; ability to learn from outcomes to reclassify object heaviness.

Conclusion: Shows that coupling perception and action through proprioceptive feedback enables reliable tracking of occluded objects during pushing, enabling safer path clearing on cluttered stairs and potentially generalizing to other contact-rich manipulation tasks.

Abstract: For robots to operate autonomously in densely cluttered environments, they
must reason about and potentially physically interact with obstacles to clear a
path. Safely clearing a path on challenging terrain, such as a cluttered
staircase, requires controlled interaction. For example, a quadrupedal robot
that pushes objects out of the way with one leg while maintaining a stable
stance with its three other legs. However, tightly coupled physical actions,
such as one-legged pushing, create new constraints on the system that can be
difficult to predict at design time. In this work, we present a new method that
addresses one such constraint, wherein the object being pushed by a quadrupedal
robot with one of its legs becomes occluded from the robot's sensors during
manipulation. To address this challenge, we present a tightly coupled
perception-action framework that enables the robot to perceive clutter, reason
about feasible push paths, and execute the clearing maneuver. Our core
contribution is an interaction-aware state estimation loop that uses
proprioceptive feedback regarding foot contact and leg position to predict an
object's displacement during the occlusion. This prediction guides the
perception system to robustly re-detect the object after the interaction,
closing the loop between action and sensing to enable accurate tracking even
after partial pushes. Using this feedback allows the robot to learn from
physical outcomes, reclassifying an object as immovable if a push fails due to
it being too heavy. We present results of implementing our approach on a Boston
Dynamics Spot robot that show our interaction-aware approach achieves higher
task success rates and tracking accuracy in pushing objects on stairs compared
to open-loop baselines.

</details>


### [206] [Selective Progress-Aware Querying for Human-in-the-Loop Reinforcement Learning](https://arxiv.org/abs/2509.20541)
*Anujith Muraleedharan,Anamika J H*

Main category: cs.RO

TL;DR: SPARQ is a progress-aware query policy for human-in-the-loop RL that asks for feedback only when learning stagnates, reducing human effort while matching always-query performance.


<details>
  <summary>Details</summary>
Motivation: Human feedback is valuable but costly; HiL-RL struggles with limited feedback; need efficient query strategies for real-world robots.

Method: Introduce SPARQ; a progress-aware query policy; evaluate on simulated UR5 cube-picking in PyBullet; compare baselines: no feedback, random querying, always querying.

Result: SPARQ achieves near-perfect task success, matching always-query performance with roughly half the feedback budget; more stable and efficient than random querying; significantly better than training without feedback.

Conclusion: Selective, progress-based query strategies can improve efficiency and scalability of HiL-RL for robots with human feedback constraints.

Abstract: Human feedback can greatly accelerate robot learning, but in real-world
settings, such feedback is costly and limited. Existing human-in-the-loop
reinforcement learning (HiL-RL) methods often assume abundant feedback,
limiting their practicality for physical robot deployment. In this work, we
introduce SPARQ, a progress-aware query policy that requests feedback only when
learning stagnates or worsens, thereby reducing unnecessary oracle calls. We
evaluate SPARQ on a simulated UR5 cube-picking task in PyBullet, comparing
against three baselines: no feedback, random querying, and always querying. Our
experiments show that SPARQ achieves near-perfect task success, matching the
performance of always querying while consuming about half the feedback budget.
It also provides more stable and efficient learning than random querying, and
significantly improves over training without feedback. These findings suggest
that selective, progress-based query strategies can make HiL-RL more efficient
and scalable for robots operating under realistic human effort constraints.

</details>


### [207] [GraspFactory: A Large Object-Centric Grasping Dataset](https://arxiv.org/abs/2509.20550)
*Srinidhi Kalgundi Srinivas,Yash Shukla,Adam Arnold,Sachin Chitta*

Main category: cs.RO

TL;DR: GraspFactory: a massive 6-DoF grasp dataset (~1.09e8 grasps) for Franka Panda and Robotiq 2F-85, enabling training of generalizable grasping models across thousands of objects; demonstrates cross-domain generalization in simulation and reality; dataset and tools released.


<details>
  <summary>Details</summary>
Motivation: Robotic grasping models trained on limited datasets struggle to generalize to novel, geometrically diverse objects in real-world settings. A large, diverse, and scalable dataset is needed to train data-intensive models for robust generalization.

Method: Construct GraspFactory by collecting 6-DoF grasp data across two grippers for a large object set (14,690 Franka Panda objects; 33,710 Robotiq 2F-85 objects), totaling over 109 million grasps. Train a model on a subset and evaluate its generalization in both simulation and real-world experiments. Provide dataset and tools for download.

Result: A model trained on a subset of GraspFactory generalizes to novel objects in both simulated and real-world settings, demonstrating the dataset’s value for training robust grasping models.

Conclusion: GraspFactory offers a large-scale, geometry-rich resource to advance data-driven robotic grasping, enabling better generalization to unseen objects; the dataset and tools are released for community use.

Abstract: Robotic grasping is a crucial task in industrial automation, where robots are
increasingly expected to handle a wide range of objects. However, a significant
challenge arises when robot grasping models trained on limited datasets
encounter novel objects. In real-world environments such as warehouses or
manufacturing plants, the diversity of objects can be vast, and grasping models
need to generalize to this diversity. Training large, generalizable
robot-grasping models requires geometrically diverse datasets. In this paper,
we introduce GraspFactory, a dataset containing over 109 million 6-DoF grasps
collectively for the Franka Panda (with 14,690 objects) and Robotiq 2F-85
grippers (with 33,710 objects). GraspFactory is designed for training
data-intensive models, and we demonstrate the generalization capabilities of
one such model trained on a subset of GraspFactory in both simulated and
real-world settings. The dataset and tools are made available for download at
https://graspfactory.github.io/.

</details>


### [208] [Uncertainty-Aware Active Source Tracking of Marine Pollution using Unmanned Surface Vehicles](https://arxiv.org/abs/2509.20593)
*Song Ma,Richard Bucknall,Yuanchang Liu*

Main category: cs.RO

TL;DR: An uncertainty-aware ROS-based framework for USV-driven marine pollution source tracking that fuses high-fidelity dispersion simulation with informative path planning to localize sources while quantifying uncertainty.


<details>
  <summary>Details</summary>
Motivation: Need for autonomous, rapid, and reliable pollution source localisation in marine environments, with explicit uncertainty estimates to support decision making and rapid response.

Method: Integrates high-fidelity dispersion simulation with informative path planning, implemented on ROS. Real-time sensor data update probabilistic source-location estimates, progressively refining both location estimates and associated uncertainty. Evaluations were conducted in simulated environments across varied source locations, flow fields, and starting poses.

Result: The framework localises pollution sources with high accuracy and robustness across tested scenarios, providing reliable, efficient source localisation with explicit uncertainty quantification.

Conclusion: This work advances autonomous environmental monitoring for rapid marine pollution response, offering an uncertainty-aware source-tracking approach suitable for real-time deployment on USVs.

Abstract: This paper proposes an uncertainty-aware marine pollution source tracking
framework for unmanned surface vehicles (USVs). By integrating high-fidelity
marine pollution dispersion simulation with informative path planning
techniques, we demonstrate effective identification of pollution sources in
marine environments. The proposed approach is implemented based on Robot
Operating System (ROS), processing real-time sensor data to update
probabilistic source location estimates. The system progressively refines the
estimation of source location while quantifying uncertainty levels in its
predictions. Experiments conducted in simulated environments with varying
source locations, flow conditions, and starting positions demonstrate the
framework's ability to localise pollution sources with high accuracy. Results
show that the proposed approach achieves reliable source localisation
efficiently. This work contributes to the development of full autonomous
environmental monitoring capabilities essential for rapid response to marine
pollution incidents.

</details>


### [209] [Latent Activation Editing: Inference-Time Refinement of Learned Policies for Safer Multirobot Navigation](https://arxiv.org/abs/2509.20623)
*Satyajeet Das,Darren Chiu,Zhehui Huang,Lars Lindemann,Gaurav S. Sukhatme*

Main category: cs.RO

TL;DR: Latent Activation Editing (LAE) is a post-deployment refinement framework that edits internal activations at inference time to improve safety without changing model weights, demonstrated on multi-quadrotor navigation with ~90% fewer collisions and preserved task completion.


<details>
  <summary>Details</summary>
Motivation: Safety gaps in reinforcement learning policies persist in obstacle-rich environments; retraining/fine-tuning is costly and risks degrading learned skills. A lightweight, deployment-time approach is desirable to reduce infrequent but critical failures.

Method: A two-stage approach: (i) an online classifier monitors intermediate activations to flag states associated with undesired behaviors, and (ii) an activation editing module selectively edits the flagged activations to push the policy toward safer responses. A latent collision world model predicts future pre-collision activations to induce earlier avoidance. The technique operates entirely at inference time without changing network weights.

Result: Extensive simulations and real-world Crazyflie experiments show a statistically significant reduction in collisions (about 90% fewer cumulative collisions than the unedited baseline) and a higher fraction of collision-free trajectories, while preserving task completion.

Conclusion: LAE provides a lightweight, post-deployment refinement paradigm for learned robot policies, enabling safer operation on resource-constrained hardware and potentially generalizing beyond quadrotor navigation to other RL-controlled systems.

Abstract: Reinforcement learning has enabled significant progress in complex domains
such as coordinating and navigating multiple quadrotors. However, even
well-trained policies remain vulnerable to collisions in obstacle-rich
environments. Addressing these infrequent but critical safety failures through
retraining or fine-tuning is costly and risks degrading previously learned
skills. Inspired by activation steering in large language models and latent
editing in computer vision, we introduce a framework for inference-time Latent
Activation Editing (LAE) that refines the behavior of pre-trained policies
without modifying their weights or architecture. The framework operates in two
stages: (i) an online classifier monitors intermediate activations to detect
states associated with undesired behaviors, and (ii) an activation editing
module that selectively modifies flagged activations to shift the policy
towards safer regimes. In this work, we focus on improving safety in
multi-quadrotor navigation. We hypothesize that amplifying a policy's internal
perception of risk can induce safer behaviors. We instantiate this idea through
a latent collision world model trained to predict future pre-collision
activations, thereby prompting earlier and more cautious avoidance responses.
Extensive simulations and real-world Crazyflie experiments demonstrate that LAE
achieves statistically significant reduction in collisions (nearly 90% fewer
cumulative collisions compared to the unedited baseline) and substantially
increases the fraction of collision-free trajectories, while preserving task
completion. More broadly, our results establish LAE as a lightweight paradigm,
feasible on resource-constrained hardware, for post-deployment refinement of
learned robot policies.

</details>


### [210] [Learning Terrain-Specialized Policies for Adaptive Locomotion in Challenging Environments](https://arxiv.org/abs/2509.20635)
*Matheus P. Angarola,Francisco Affonso,Marcelo Becker*

Main category: cs.RO

TL;DR: Hierarchical RL with terrain-specific policies and curriculum learning improves blind legged locomotion, outperforming a generalist policy in simulation across mixed terrains.


<details>
  <summary>Details</summary>
Motivation: Legged robots face robustness and agility challenges on unstructured terrains, especially in blind settings where terrain data is unavailable. Need approaches that adapt without explicit terrain sensing.

Method: A hierarchical reinforcement learning framework that decomposes control into terrain-specialized policies, trained via curriculum learning to progressively handle more complex terrains and transitions.

Result: In simulation, the proposed approach outperforms a generalist policy by up to 16% in success rate and achieves lower tracking errors as velocity targets increase, with notable gains on low-friction and discontinuous terrains, indicating strong adaptability across mixed-terrain scenarios.

Conclusion: Terrain-specialized hierarchical RL with curriculum learning enhances robustness and agility for blind legged locomotion in diverse terrains, showing promise for real-world deployment and extension to broader terrain sets.

Abstract: Legged robots must exhibit robust and agile locomotion across diverse,
unstructured terrains, a challenge exacerbated under blind locomotion settings
where terrain information is unavailable. This work introduces a hierarchical
reinforcement learning framework that leverages terrain-specialized policies
and curriculum learning to enhance agility and tracking performance in complex
environments. We validated our method on simulation, where our approach
outperforms a generalist policy by up to 16% in success rate and achieves lower
tracking errors as the velocity target increases, particularly on low-friction
and discontinuous terrains, demonstrating superior adaptability and robustness
across mixed-terrain scenarios.

</details>


### [211] [Suction Leap-Hand: Suction Cups on a Multi-fingered Hand Enable Embodied Dexterity and In-Hand Teleoperation](https://arxiv.org/abs/2509.20646)
*Sun Zhaole,Xiaofeng Mao,Jihong Zhu,Yuanlong Zhang,Robert B. Fisher*

Main category: cs.RO

TL;DR: A suction-based, non-anthropomorphic hand (SLeap Hand) uses fingertip suction cups to replace multi-point force-closure grasps, enabling stable single-point adhesion, easier teleoperation, and data collection, and unlocking dexterous tasks (e.g., one-handed paper cutting and in-hand writing) beyond human hand capabilities.


<details>
  <summary>Details</summary>
Motivation: Anthropomorphic hands limit robotic capabilities to human-performed tasks and hinder data collection for learning-based methods due to complex force-closure and multi-point contact dynamics. A non-anthropomorphic design with suction can simplify manipulation, reduce the sim-to-real gap, and expand dexterity.

Method: Introduce the SLeap Hand with integrated fingertip suction cups forming single-point adhesion. Replace complex force-closure grasps with suction-based contacts, enabling stable, one-point grasps. Demonstrate through teleoperation-friendly manipulation and tasks such as one-handed paper cutting and in-hand writing, arguing that this embodiment lowers data-collection barriers and enables new dexterous skills.

Result: Demonstrates stable, single-handed completion of tasks that typically require two hands and enables a new class of dexterous skills (e.g., one-handed paper cutting, in-hand writing). Teleoperation becomes more stable, and high-quality demonstration data is easier to collect, potentially reducing the RL sim-to-real gap.

Conclusion: Shifting away from human hand mimicry toward purpose-built robotic embodiments can unlock novel dexterous capabilities, simplify data collection, and allow single-handed execution of tasks traditionally requiring two hands. The suction-based design offers a new paradigm for dexterity beyond anthropomorphic constraints.

Abstract: Dexterous in-hand manipulation remains a foundational challenge in robotics,
with progress often constrained by the prevailing paradigm of imitating the
human hand. This anthropomorphic approach creates two critical barriers: 1) it
limits robotic capabilities to tasks humans can already perform, and 2) it
makes data collection for learning-based methods exceedingly difficult. Both
challenges are caused by traditional force-closure which requires coordinating
complex, multi-point contacts based on friction, normal force, and gravity to
grasp an object. This makes teleoperated demonstrations unstable and amplifies
the sim-to-real gap for reinforcement learning. In this work, we propose a
paradigm shift: moving away from replicating human mechanics toward the design
of novel robotic embodiments. We introduce the \textbf{S}uction
\textbf{Leap}-Hand (SLeap Hand), a multi-fingered hand featuring integrated
fingertip suction cups that realize a new form of suction-enabled dexterity. By
replacing complex force-closure grasps with stable, single-point adhesion, our
design fundamentally simplifies in-hand teleoperation and facilitates the
collection of high-quality demonstration data. More importantly, this
suction-based embodiment unlocks a new class of dexterous skills that are
difficult or even impossible for the human hand, such as one-handed paper
cutting and in-hand writing. Our work demonstrates that by moving beyond
anthropomorphic constraints, novel embodiments can not only lower the barrier
for collecting robust manipulation data but also enable the stable,
single-handed completion of tasks that would typically require two human hands.
Our webpage is https://sites.google.com/view/sleaphand.

</details>


### [212] [Cyber Racing Coach: A Haptic Shared Control Framework for Teaching Advanced Driving Skills](https://arxiv.org/abs/2509.20653)
*Congkai Shen,Siyuan Yu,Yifan Weng,Haoran Ma,Chen Li,Hiroshi Yasuda,James Dallas,Michael Thompson,John Subosits,Tulga Ersal*

Main category: cs.RO

TL;DR: A cyber racing coach using haptic shared control to train high-performance driving; autonomous co-driver with fading assist; evaluated in a human study; outperforms self-learning and full-assist baselines in skill acquisition.


<details>
  <summary>Details</summary>
Motivation: Long-term skill acquisition in complex, demanding driving tasks is understudied. Prior work shows safety/efficacy of shared control but not its effect on learning complex high-speed skills; a training framework is needed to enable progressive skill development.

Method: Develop an autonomous driving system that cooperates with humans in high-performance driving scenarios and implement a haptic shared-control mechanism with a fading scheme that gradually reduces steering assistance based on the driver’s performance during training. Compare self-learning (no assistance) versus full assistance during training and evaluate via a human-subject study.

Result: The framework helps human drivers develop superior racing skills compared to the benchmarks, yielding better performance and consistency.

Conclusion: Haptic shared control with performance-based fading can effectively facilitate long-term acquisition of high-performance driving skills, offering a promising training paradigm for racing and other demanding driving tasks.

Abstract: This study introduces a haptic shared control framework designed to teach
human drivers advanced driving skills. In this context, shared control refers
to a driving mode where the human driver collaborates with an autonomous
driving system to control the steering of a vehicle simultaneously. Advanced
driving skills are those necessary to safely push the vehicle to its handling
limits in high-performance driving such as racing and emergency obstacle
avoidance. Previous research has demonstrated the performance and safety
benefits of shared control schemes using both subjective and objective
evaluations. However, these schemes have not been assessed for their impact on
skill acquisition on complex and demanding tasks. Prior research on long-term
skill acquisition either applies haptic shared control to simple tasks or
employs other feedback methods like visual and auditory aids. To bridge this
gap, this study creates a cyber racing coach framework based on the haptic
shared control paradigm and evaluates its performance in helping human drivers
acquire high-performance driving skills. The framework introduces (1) an
autonomous driving system that is capable of cooperating with humans in a
highly performant driving scenario; and (2) a haptic shared control mechanism
along with a fading scheme to gradually reduce the steering assistance from
autonomy based on the human driver's performance during training. Two
benchmarks are considered: self-learning (no assistance) and full assistance
during training. Results from a human subject study indicate that the proposed
framework helps human drivers develop superior racing skills compared to the
benchmarks, resulting in better performance and consistency.

</details>


### [213] [EEG-Driven AR-Robot System for Zero-Touch Grasping Manipulation](https://arxiv.org/abs/2509.20656)
*Junzhe Wang,Jiarui Xie,Pengfei Hao,Zheng Li,Yi Cai*

Main category: cs.RO

TL;DR: A closed-loop BCI-AR-Robot system enables zero-touch robotic grasping using motor-imagery EEG decoding, augmented reality feedback, and vision-assisted grasping; AR feedback improves stability and information transfer rate, validated by high MI accuracy and grasping success in real-time experiments.


<details>
  <summary>Details</summary>
Motivation: Addresses core limitations of EEG-based BCI for robotics—noisy and unstable signals, inflexible target choices, and lack of closed-loop real-world validation—by integrating MI-EEG with AR-based neurofeedback and autonomous grasping to enable reliable assistive control.

Method: 14-channel EEG headset used for individualized motor-imagery calibration; smartphone-based AR interface for multi-target navigation with direction-congruent feedback; robotic arm combines decision outputs with vision-based pose estimation for autonomous grasping; experiments measure MI training accuracy, information transfer rate (ITR), AR impact on sustained control (SCI), and closed-loop grasping success.

Result: MI training accuracy 93.1%; average ITR 14.8 bit/min; AR neurofeedback significantly improved sustained control (SCI = 0.210) and achieved the highest ITR (21.3 bit/min) relative to static, sham, and no-AR baselines; closed-loop grasping success rate 97.2%.

Conclusion: AR feedback substantially stabilizes EEG-based control and, together with the closed-loop system, enables robust zero-touch grasping, advancing assistive robotic applications and future human-robot interaction modalities.

Abstract: Reliable brain-computer interface (BCI) control of robots provides an
intuitive and accessible means of human-robot interaction, particularly
valuable for individuals with motor impairments. However, existing BCI-Robot
systems face major limitations: electroencephalography (EEG) signals are noisy
and unstable, target selection is often predefined and inflexible, and most
studies remain restricted to simulation without closed-loop validation. These
issues hinder real-world deployment in assistive scenarios. To address them, we
propose a closed-loop BCI-AR-Robot system that integrates motor imagery
(MI)-based EEG decoding, augmented reality (AR) neurofeedback, and robotic
grasping for zero-touch operation. A 14-channel EEG headset enabled
individualized MI calibration, a smartphone-based AR interface supported
multi-target navigation with direction-congruent feedback to enhance stability,
and the robotic arm combined decision outputs with vision-based pose estimation
for autonomous grasping. Experiments are conducted to validate the framework:
MI training achieved 93.1 percent accuracy with an average information transfer
rate (ITR) of 14.8 bit/min; AR neurofeedback significantly improved sustained
control (SCI = 0.210) and achieved the highest ITR (21.3 bit/min) compared with
static, sham, and no-AR baselines; and closed-loop grasping achieved a 97.2
percent success rate with good efficiency and strong user-reported control.
These results show that AR feedback substantially stabilizes EEG-based control
and that the proposed framework enables robust zero-touch grasping, advancing
assistive robotic applications and future modes of human-robot interaction.

</details>


### [214] [Equi-RO: A 4D mmWave Radar Odometry via Equivariant Networks](https://arxiv.org/abs/2509.20674)
*Zeyu Han,Shuocheng Yang,Minghan Zhu,Fang Zhang,Shaobing Xu,Maani Ghaffari,Jianqiang Wang*

Main category: cs.RO

TL;DR: Equi-RO uses an equivariant graph-based network to perform robust 4D radar odometry by transforming Doppler velocity into invariant features and using separate equivariant/invariant processing, achieving notable translation/rotation improvements.


<details>
  <summary>Details</summary>
Motivation: Need robust odometry in GPS-denied environments; 4D radar provides all-weather, velocity-capable measurements; sparsity in radar data; limitations of LiDAR/camera under extreme weather.

Method: Preprocess Doppler velocity into invariant node/edge features; use separate networks for equivariant and invariant processing; graph-based feature aggregation to improve inter-frame correspondence.

Result: Outperforms state-of-the-art baselines on open-source and self-collected datasets; 10.7% translation and 20.0% rotation relative improvements on open-source dataset.

Conclusion: Equi-RO advances 4D radar odometry via equivariant graph networks, offering robust, accurate, all-weather odometry with improved inter-frame matching.

Abstract: Autonomous vehicles and robots rely on accurate odometry estimation in
GPS-denied environments. While LiDARs and cameras struggle under extreme
weather, 4D mmWave radar emerges as a robust alternative with all-weather
operability and velocity measurement. In this paper, we introduce Equi-RO, an
equivariant network-based framework for 4D radar odometry. Our algorithm
pre-processes Doppler velocity into invariant node and edge features in the
graph, and employs separate networks for equivariant and invariant feature
processing. A graph-based architecture enhances feature aggregation in sparse
radar data, improving inter-frame correspondence. Experiments on the
open-source dataset and self-collected dataset show Equi-RO outperforms
state-of-the-art algorithms in accuracy and robustness. Overall, our method
achieves 10.7% and 20.0% relative improvements in translation and rotation
accuracy, respectively, compared to the best baseline on the open-source
dataset.

</details>


### [215] [Efficient Construction of Implicit Surface Models From a Single Image for Motion Generation](https://arxiv.org/abs/2509.20681)
*Wei-Teng Chu,Tianyi Zhang,Matthew Johnson-Roberson,Weiming Zhi*

Main category: cs.RO

TL;DR: Fast Image-to-Neural Surface (FINS) reconstructs neural surfaces and SDF fields from a single or few images using a multi-resolution hash grid encoder and lightweight heads, enabling ultra-fast training and high fidelity. It also uses pre-trained foundation models to infer single-image geometry and demonstrates robot surface-following with strong speed/accuracy gains over baselines.


<details>
  <summary>Details</summary>
Motivation: Implicit representations are powerful for robotics tasks like obstacle avoidance and path planning, but state-of-the-art methods (e.g., NeuS) require many views and long training times. There is a need for fast, single-image or few-image 3D reconstruction.

Method: FINS integrates a multi-resolution hash grid encoder with lightweight geometry and color heads and trains with an approximate second-order optimizer for efficiency. It further leverages pre-trained foundation models to estimate geometry from a single RGB image. The system supports surface reconstruction and SDF estimation, achieving fast convergence (seconds) on single or small image sets and is validated on robot surface following tasks and diverse benchmarks.

Result: Under the same conditions, FINS outperforms state-of-the-art baselines in convergence speed and accuracy for surface reconstruction and SDF field estimation. It demonstrates applicability to robot surface following and scales across benchmark datasets.

Conclusion: FINS enables fast, data-efficient neural surface reconstruction from images, including single-image cases via foundation-model geometry estimation, with practical impact for real-time robotics tasks and potential for broader adoption across datasets and scenarios.

Abstract: Implicit representations have been widely applied in robotics for obstacle
avoidance and path planning. In this paper, we explore the problem of
constructing an implicit distance representation from a single image. Past
methods for implicit surface reconstruction, such as \emph{NeuS} and its
variants generally require a large set of multi-view images as input, and
require long training times. In this work, we propose Fast Image-to-Neural
Surface (FINS), a lightweight framework that can reconstruct high-fidelity
surfaces and SDF fields based on a single or a small set of images. FINS
integrates a multi-resolution hash grid encoder with lightweight geometry and
color heads, making the training via an approximate second-order optimizer
highly efficient and capable of converging within a few seconds. Additionally,
we achieve the construction of a neural surface requiring only a single RGB
image, by leveraging pre-trained foundation models to estimate the geometry
inherent in the image. Our experiments demonstrate that under the same
conditions, our method outperforms state-of-the-art baselines in both
convergence speed and accuracy on surface reconstruction and SDF field
estimation. Moreover, we demonstrate the applicability of FINS for robot
surface following tasks and show its scalability to a variety of benchmark
datasets.

</details>


### [216] [RAM-NAS: Resource-aware Multiobjective Neural Architecture Search Method for Robot Vision Tasks](https://arxiv.org/abs/2509.20688)
*Shouren Mao,Minghao Qin,Wei Dong,Huajian Liu,Yongzhuo Gao*

Main category: cs.RO

TL;DR: RAM-NAS is a resource-aware, multi-objective NAS method for robotic hardware that improves supernet training through subnet mutual distillation and DKD, uses latency surrogates to guide a hardware-aware search, and delivers edge-optimized models with strong ImageNet accuracy and reduced latency across edge devices.


<details>
  <summary>Details</summary>
Motivation: There is a need for neural architecture search methods that account for real-world robot hardware constraints (edge devices) and improve the pretraining of the supernet, enabling efficient, latency-aware models suitable for robotics.

Method: RAM-NAS introduces subnet mutual distillation (mutual distillation among all subnets sampled by the sandwich rule), and DKD loss to enhance logits distillation. It trains latency surrogate predictors using data from three robotic edge hardware types and conducts a hardware-aware, multi-objective evolutionary search that jointly optimizes accuracy and hardware latency.

Result: RAM-NAS models achieve top-1 accuracy from 76.7% to 81.4% on ImageNet. Inference latency on edge hardware is significantly reduced across three hardware types compared to MobileNetv3-based methods. Experiments on downstream tasks demonstrate scalability.

Conclusion: RAM-NAS fills the gap in neural architecture search for resource-aware robotics, providing a practical framework to balance accuracy and hardware latency on robot devices while maintaining scalability to downstream tasks.

Abstract: Neural architecture search (NAS) has shown great promise in automatically
designing lightweight models. However, conventional approaches are insufficient
in training the supernet and pay little attention to actual robot hardware
resources. To meet such challenges, we propose RAM-NAS, a resource-aware
multi-objective NAS method that focuses on improving the supernet pretrain and
resource-awareness on robot hardware devices. We introduce the concept of
subnets mutual distillation, which refers to mutually distilling all subnets
sampled by the sandwich rule. Additionally, we utilize the Decoupled Knowledge
Distillation (DKD) loss to enhance logits distillation performance. To expedite
the search process with consideration for hardware resources, we used data from
three types of robotic edge hardware to train Latency Surrogate predictors.
These predictors facilitated the estimation of hardware inference latency
during the search phase, enabling a unified multi-objective evolutionary search
to balance model accuracy and latency trade-offs. Our discovered model family,
RAM-NAS models, can achieve top-1 accuracy ranging from 76.7% to 81.4% on
ImageNet. In addition, the resource-aware multi-objective NAS we employ
significantly reduces the model's inference latency on edge hardware for
robots. We conducted experiments on downstream tasks to verify the scalability
of our methods. The inference time for detection and segmentation is reduced on
all three hardware types compared to MobileNetv3-based methods. Our work fills
the gap in NAS for robot hardware resource-aware.

</details>


### [217] [Incorporating Human-Inspired Ankle Characteristics in a Forced-Oscillation-Based Reduced-Order Model for Walking](https://arxiv.org/abs/2509.20689)
*Chathura Semasinghe,Siavash Rezazadeh*

Main category: cs.RO

TL;DR: Extends a forced-oscillation-based reduced-order walking model to include ankle and foot dynamics, yielding improved gait characteristics and robust stabilization through both foot placement and proprioceptive ankle control, including a human-like stabilization for small perturbations.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of point-foot models and to better understand anthropomorphic walking by incorporating ankle/foot dynamics and a proprioceptive ankle scheme.

Method: Develop a reduced-order walking model with ankle and foot dynamics inspired by human biomechanics, implement a proprioceptive ankle controller, compare against a point-foot baseline, and evaluate stability under large initial-condition errors and small perturbations.

Result: The ankle-inclusive model shows improved gait characteristics versus the point-foot model. It stabilizes large errors via a combination of foot placement and ankle strategies, and can stabilize small perturbations without foot placement control due to the proprioceptive ankle scheme, mirroring a human-like stabilization mechanism.

Conclusion: The proposed model captures a human-like stabilization mechanism, enhancing understanding of anthropomorphic walking and informing the design of more robust humanoid gait controllers.

Abstract: This paper extends the forced-oscillation-based reduced-order model of
walking to a model with ankles and feet. A human-inspired paradigm was designed
for the ankle dynamics, which results in improved gait characteristics compared
to the point-foot model. In addition, it was shown that while the proposed
model can stabilize against large errors in initial conditions through
combination of foot placement and ankle strategies, the model is able to
stabilize against small perturbations without relying on the foot placement
control and solely through the designed proprioceptive ankle scheme. This novel
property, which is also observed in humans, can help in better understanding of
anthropomorphic walking and its stabilization mechanisms.

</details>


### [218] [RuN: Residual Policy for Natural Humanoid Locomotion](https://arxiv.org/abs/2509.20696)
*Qingpeng Li,Chengrui Zhu,Yanming Wu,Xin Yuan,Zhen Zhang,Jian Yang,Yong Liu*

Main category: cs.RO

TL;DR: RuN decouples locomotion control by combining a pre-trained motion generator with a lightweight RL residual policy, yielding stable, natural gaits and smooth walk-run transitions across a wide speed range without requiring the policy to imitate a reference motion directly.


<details>
  <summary>Details</summary>
Motivation: To overcome the difficulty of training a single policy to simultaneously imitate a reference motion, track velocity, and maintain stability across walking and running; direct imitation constraints limit performance and generalization.

Method: RuN couples a pre-trained Conditional Motion Generator (providing a kinematically natural motion prior) with a reinforcement learning policy that learns a lightweight residual correction to handle dynamic interactions and fine-grained control.

Result: In simulation and real-world experiments on the Unitree G1 humanoid, RuN achieved stable, natural gaits and smooth transitions from walk to run across speeds 0–2.5 m/s, outperforming state-of-the-art methods in both training efficiency and final performance.

Conclusion: A decoupled residual learning framework can effectively leverage a motion prior while maintaining dynamical adaptability, enabling natural, robust locomotion across a broad speed range.

Abstract: Enabling humanoid robots to achieve natural and dynamic locomotion across a
wide range of speeds, including smooth transitions from walking to running,
presents a significant challenge. Existing deep reinforcement learning methods
typically require the policy to directly track a reference motion, forcing a
single policy to simultaneously learn motion imitation, velocity tracking, and
stability maintenance. To address this, we introduce RuN, a novel decoupled
residual learning framework. RuN decomposes the control task by pairing a
pre-trained Conditional Motion Generator, which provides a kinematically
natural motion prior, with a reinforcement learning policy that learns a
lightweight residual correction to handle dynamical interactions. Experiments
in simulation and reality on the Unitree G1 humanoid robot demonstrate that RuN
achieves stable, natural gaits and smooth walk-run transitions across a broad
velocity range (0-2.5 m/s), outperforming state-of-the-art methods in both
training efficiency and final performance.

</details>


### [219] [Joint Flow Trajectory Optimization For Feasible Robot Motion Generation from Video Demonstrations](https://arxiv.org/abs/2509.20703)
*Xiaoxiang Dong,Matthew Johnson-Roberson,Weiming Zhi*

Main category: cs.RO

TL;DR: Joint Flow Trajectory Optimization (JFTO) for grasp pose generation and object trajectory imitation from video-based learning-from-demonstration (LfD). The method treats demonstrations as object-centric guides, balancing grasp feasibility, trajectory consistency with demonstrations, and collision-free execution within robot kinematics. It extends flow matching to SE(3) to model multimodal object trajectories probabilistically, yielding a density-aware imitation that avoids mode collapse. The resulting differentiable objective combines grasp similarity, trajectory likelihood, and collision penalties. Validated in simulation and real-world experiments across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Learning from human video demonstrations is appealing but difficult for robotic manipulators due to embodiment differences and joint feasibility constraints. Directly imitating human hand motions can yield infeasible or unsafe actions. The goal is to produce feasible grasps and collision-free object trajectories that align with demonstrated motions.

Method: Introduce the Joint Flow Trajectory Optimization (JFTO) framework. Interpret demonstrations as object-centric guides with three objectives: (i) select a feasible grasp pose, (ii) generate object trajectories consistent with demonstrated motions, (iii) ensure collision-free execution under robot kinematics. To capture multimodality, extend flow matching to SE(3) for probabilistic modeling of object trajectories, enabling density-aware imitation that mitigates mode collapse. Integrate grasp similarity, trajectory likelihood, and collision penalties into a single differentiable objective for optimization.

Result: Demonstrates density-aware imitation that avoids mode collapse and validates the approach in both simulation and real-world experiments across diverse manipulation tasks.

Conclusion: JFTO provides a differentiable optimization framework that jointly optimizes grasp pose and object trajectory from video demonstrations, addressing embodiment and collision constraints. The SE(3) flow extension enables multimodal trajectory modeling, with successful validation in varied tasks suggesting practical applicability of the approach.

Abstract: Learning from human video demonstrations offers a scalable alternative to
teleoperation or kinesthetic teaching, but poses challenges for robot
manipulators due to embodiment differences and joint feasibility constraints.
We address this problem by proposing the Joint Flow Trajectory Optimization
(JFTO) framework for grasp pose generation and object trajectory imitation
under the video-based Learning-from-Demonstration (LfD) paradigm. Rather than
directly imitating human hand motions, our method treats demonstrations as
object-centric guides, balancing three objectives: (i) selecting a feasible
grasp pose, (ii) generating object trajectories consistent with demonstrated
motions, and (iii) ensuring collision-free execution within robot kinematics.
To capture the multimodal nature of demonstrations, we extend flow matching to
$\SE(3)$ for probabilistic modeling of object trajectories, enabling
density-aware imitation that avoids mode collapse. The resulting optimization
integrates grasp similarity, trajectory likelihood, and collision penalties
into a unified differentiable objective. We validate our approach in both
simulation and real-world experiments across diverse real-world manipulation
tasks.

</details>


### [220] [Building Information Models to Robot-Ready Site Digital Twins (BIM2RDT): An Agentic AI Safety-First Framework](https://arxiv.org/abs/2509.20705)
*Reza Akhavian,Mani Amani,Johannes Mootz,Robert Ashe,Behrad Beheshti*

Main category: cs.RO

TL;DR: Proposes BIM2RDT, an agentic AI framework that converts static Building Information Models into dynamic, robot-ready digital twins for safer construction execution, via SG-ICP with LLM-based orientation priors, real-time data fusion from BIM, IoT, and robot sensing, and HAV safety monitoring.


<details>
  <summary>Details</summary>
Motivation: Static BIM models and isolated site data hinder real-time decision making and safe autonomous site operations. Traditional registration (ICP) struggles with occlusions and local minima, and there is a need to tightly couple safety monitoring (HAV) with digital twins compliant with IFC/ISO standards.

Method: Integrates BIM geometry/semantics, IoT activity data, and robot-captured visual-spatial data. Introduces SG-ICP, a point-cloud registration algorithm that uses an LLM to infer object-specific orientation priors to avoid local minima. Employs YOLOE for object detection and Shi-Tomasi for corner tracking with BIM geometry as a priori maps. Real-time HAV monitoring is mapped to the digital twin using IFC standards for intervention. A feedback loop updates the DT and informs mission path optimization. Experimental evaluation compares SG-ICP to standard ICP across occluded feature scenarios, reporting RMSE improvements.

Result: SG-ICP achieves RMSE reductions of 64.3%–88.3% in alignment accuracy across scenarios with occluded features, indicating substantially improved registration and plausible orientation priors. HAV integration triggers warnings when exposure exceeds ISO 5349-1 limits, enhancing safety and regulatory compliance.

Conclusion: The BIM2RDT framework demonstrates how agentic AI can bridge pre-existing BIM data and dynamic site conditions to create safe, robot-ready digital twins. The loop of data fusion, perception, and path optimization improves alignment accuracy and safety monitoring, suggesting broad potential for robotics-augmented construction workflows.

Abstract: The adoption of cyber-physical systems and jobsite intelligence that connects
design models, real-time site sensing, and autonomous field operations can
dramatically enhance digital management in the construction industry. This
paper introduces BIM2RDT (Building Information Models to Robot-Ready Site
Digital Twins), an agentic artificial intelligence (AI) framework designed to
transform static Building Information Modeling (BIM) into dynamic, robot-ready
digital twins (DTs) that prioritize safety during execution. The framework
bridges the gap between pre-existing BIM data and real-time site conditions by
integrating three key data streams: geometric and semantic information from BIM
models, activity data from IoT sensor networks, and visual-spatial data
collected by robots during site traversal. The methodology introduces
Semantic-Gravity ICP (SG-ICP), a point cloud registration algorithm that
leverages large language model (LLM) reasoning. Unlike traditional methods,
SG-ICP utilizes an LLM to infer object-specific, plausible orientation priors
based on BIM semantics, improving alignment accuracy by avoiding convergence on
local minima. This creates a feedback loop where robot-collected data updates
the DT, which in turn optimizes paths for missions. The framework employs YOLOE
object detection and Shi-Tomasi corner detection to identify and track
construction elements while using BIM geometry as a priori maps. The framework
also integrates real-time Hand-Arm Vibration (HAV) monitoring, mapping
sensor-detected safety events to the digital twin using IFC standards for
intervention. Experiments demonstrate SG-ICP's superiority over standard ICP,
achieving RMSE reductions of 64.3%--88.3% in alignment across scenarios with
occluded features, ensuring plausible orientations. HAV integration triggers
warnings upon exceeding exposure limits, enhancing compliance with ISO 5349-1.

</details>


### [221] [Digital Twin-Guided Robot Path Planning: A Beta-Bernoulli Fusion with Large Language Model as a Sensor](https://arxiv.org/abs/2509.20709)
*Mani Amani,Reza Akhavian*

Main category: cs.RO

TL;DR: A Beta-Bernoulli fusion approach treats LLM-derived danger scores as pseudo-counts to update design-time repulsive coefficients in BIM-based path planning, yielding a stable, context-aware potential field with improved path robustness.


<details>
  <summary>Details</summary>
Motivation: To enable safe, context-aware robotic planning in construction environments by integrating natural language prompts with BIM-derived semantic maps, while accounting for uncertainty in NL directives via Bayesian fusion.

Method: Model LLM outputs as a sensor and represent each obstacle's repulsive coefficient as a Beta(alpha, beta) random variable. Update alpha and beta using LLM danger scores as pseudo-counts. Use the posterior mean to form a continuous repulsive gain that augments a Euclidean-distance-based potential field. Adjust gains based on sentiment and context from prompts to guide safer paths. Compatible with learned or classical AI frameworks.

Result: Simulation results indicate qualitative and quantitative improvements in path robustness and validity, with a numerically stable fusion that can process chained NL commands, improving planning under uncertainty.

Conclusion: Beta-Bernoulli fusion provides a stable, context-aware mechanism to integrate NL directives with BIM-derived maps for robotic planning, with potential applicability beyond construction and compatibility with various AI frameworks.

Abstract: Integrating natural language (NL) prompts into robotic mission planning has
attracted significant interest in recent years. In the construction domain,
Building Information Models (BIM) encapsulate rich NL descriptions of the
environment. We present a novel framework that fuses NL directives with
BIM-derived semantic maps via a Beta-Bernoulli Bayesian fusion by interpreting
the LLM as a sensor: each obstacle's design-time repulsive coefficient is
treated as a Beta(alpha, beta) random variable and LLM-returned danger scores
are incorporated as pseudo-counts to update alpha and beta. The resulting
posterior mean yields a continuous, context-aware repulsive gain that augments
a Euclidean-distance-based potential field for cost heuristics. By adjusting
gains based on sentiment and context inferred from user prompts, our method
guides robots along safer, more context-aware paths. This provides a
numerically stable method that can chain multiple natural commands and prompts
from construction workers and foreman to enable planning while giving
flexibility to be integrated in any learned or classical AI framework.
Simulation results demonstrate that this Beta-Bernoulli fusion yields both
qualitative and quantitative improvements in path robustness and validity.

</details>


### [222] [RobotDancing: Residual-Action Reinforcement Learning Enables Robust Long-Horizon Humanoid Motion Tracking](https://arxiv.org/abs/2509.20717)
*Zhenguo Sun,Yibo Peng,Yuan Meng,Xukun Li,Bo-Sheng Huang,Zhenshan Bing,Xinlong Wang,Alois Knoll*

Main category: cs.RO

TL;DR: RobotDancing uses a residual joint-target predictor within a single-stage RL framework to correct dynamics mismatches, enabling long-horizon, high-dynamic humanoid motion tracking with zero-shot sim-to-real transfer.


<details>
  <summary>Details</summary>
Motivation: Long-horizon, high-dynamic humanoid control is brittle because model-plant mismatch accumulates error; absolute joint commands cannot fully compensate for dynamics discrepancies.

Method: End-to-end pipeline that trains a residual target predictor; single-stage RL with unified observation, reward, and hyperparameters; includes sim-to-sim validation and zero-shot sim-to-real; evaluated on Unitree G1 with retargeted LAFAN1 sequences and transfer validated on H1/H1-2.

Result: RobotDancing enables tracking of multi-minute, high-energy motions (jumps, spins, cartwheels) and achieves high-quality zero-shot transfer to hardware.

Conclusion: A simple, scalable approach that explicitly corrects dynamics discrepancies via residual targets, bridging sim-to-real for robust long-horizon humanoid motion.

Abstract: Long-horizon, high-dynamic motion tracking on humanoids remains brittle
because absolute joint commands cannot compensate model-plant mismatch, leading
to error accumulation. We propose RobotDancing, a simple, scalable framework
that predicts residual joint targets to explicitly correct dynamics
discrepancies. The pipeline is end-to-end--training, sim-to-sim validation, and
zero-shot sim-to-real--and uses a single-stage reinforcement learning (RL)
setup with a unified observation, reward, and hyperparameter configuration. We
evaluate primarily on Unitree G1 with retargeted LAFAN1 dance sequences and
validate transfer on H1/H1-2. RobotDancing can track multi-minute, high-energy
behaviors (jumps, spins, cartwheels) and deploys zero-shot to hardware with
high motion tracking quality.

</details>


### [223] [SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning](https://arxiv.org/abs/2509.20739)
*Guoyang Zhao,Yudong Li,Weiqing Qi,Kai Zhang,Bonan Liu,Kai Chen,Haoang Li,Jun Ma*

Main category: cs.RO

TL;DR: A vision-language, SLAM-free navigation framework for legged robots that uses semantic reasoning and a semantic-probabilistic topological map for planning, with LLM-based global subgoal reasoning and vision-based local planning, achieving robust navigation without dense geometry.


<details>
  <summary>Details</summary>
Motivation: Conventional SLAM pipelines are fragile under rapid motion, calibration demands, and sensor drift, and they offer limited semantic reasoning for task-driven exploration. A semantics-driven, SLAM-free approach can improve robustness and task-oriented navigation.

Method: A hierarchical vision-language perception module fuses scene-level context with object-level cues for semantic inference. A semantic-probabilistic topological map supports coarse-to-fine planning. Global planning uses LLM-based reasoning to select subgoals; local planning relies on vision-based obstacle avoidance. Reinforcement-learning locomotion controllers enable deployment across diverse legged robots. Evaluation includes simulation and real-world experiments with ablations showing the value of hierarchical perception and fine local planning.

Result: Consistent improvements in semantic accuracy, planning quality, and navigation success across experiments. Ablations demonstrate the necessity of both hierarchical perception and precise local planning.

Conclusion: Introduces a paradigm shift to SLAM-free, vision-language-driven navigation, moving robotic exploration from geometry-centric mapping to semantics-driven decision making.

Abstract: Conventional SLAM pipelines for legged robot navigation are fragile under
rapid motion, calibration demands, and sensor drift, while offering limited
semantic reasoning for task-driven exploration. To deal with these issues, we
propose a vision-only, SLAM-free navigation framework that replaces dense
geometry with semantic reasoning and lightweight topological representations. A
hierarchical vision-language perception module fuses scene-level context with
object-level cues for robust semantic inference. And a semantic-probabilistic
topological map supports coarse-to-fine planning: LLM-based global reasoning
for subgoal selection and vision-based local planning for obstacle avoidance.
Integrated with reinforcement-learning locomotion controllers, the framework is
deployable across diverse legged robot platforms. Experiments in simulation and
real-world settings demonstrate consistent improvements in semantic accuracy,
planning quality, and navigation success, while ablation studies further
showcase the necessity of both hierarchical perception and fine local planning.
This work introduces a new paradigm for SLAM-free, vision-language-driven
navigation, shifting robotic exploration from geometry-centric mapping to
semantics-driven decision making.

</details>


### [224] [MASt3R-Fusion: Integrating Feed-Forward Visual Model with IMU, GNSS for High-Functionality SLAM](https://arxiv.org/abs/2509.20757)
*Yuxuan Zhou,Xingxing Li,Shengyu Li,Zhuohao Yan,Chunxi Xia,Shaoquan Feng*

Main category: cs.RO

TL;DR: MASt3R-Fusion: a multi-sensor SLAM framework that fuses feed-forward neural pointmap regression with IMU and GNSS via Sim(3) visual alignment in a unified SE(3 factor graph, enabling real-time tracking and global consistency).


<details>
  <summary>Details</summary>
Motivation: Classical visual SLAM struggles in low-texture, scale-ambiguous, or adversarial visual conditions, and although neural pointmap regression can recover 3D geometry from images, probabilistic multi-sensor fusion benefits are often ignored. This work motivates integrating learned geometry with multi-sensor information for accuracy, robustness, and metric-scale perception.

Method: Introduce MASt3R-Fusion that tightly couples feed-forward pointmap regression with inertial and GNSS data. Use Sim(3)-based visual-alignment constraints (Hessian form) within a universal metric-scale SE(3) factor graph. Employ a hierarchical factor graph design enabling real-time sliding-window optimization and aggressive loop-closure-based global optimization, achieving real-time pose tracking, metric-scale structure perception, and globally consistent mapping.

Result: Experiments on public benchmarks and self-collected datasets show substantial improvements in accuracy and robustness compared with existing visual-centered multi-sensor SLAM systems.

Conclusion: Code will be released as open-source to support reproducibility and further research.

Abstract: Visual SLAM is a cornerstone technique in robotics, autonomous driving and
extended reality (XR), yet classical systems often struggle with low-texture
environments, scale ambiguity, and degraded performance under challenging
visual conditions. Recent advancements in feed-forward neural network-based
pointmap regression have demonstrated the potential to recover high-fidelity 3D
scene geometry directly from images, leveraging learned spatial priors to
overcome limitations of traditional multi-view geometry methods. However, the
widely validated advantages of probabilistic multi-sensor information fusion
are often discarded in these pipelines. In this work, we propose
MASt3R-Fusion,a multi-sensor-assisted visual SLAM framework that tightly
integrates feed-forward pointmap regression with complementary sensor
information, including inertial measurements and GNSS data. The system
introduces Sim(3)-based visualalignment constraints (in the Hessian form) into
a universal metric-scale SE(3) factor graph for effective information fusion. A
hierarchical factor graph design is developed, which allows both real-time
sliding-window optimization and global optimization with aggressive loop
closures, enabling real-time pose tracking, metric-scale structure perception
and globally consistent mapping. We evaluate our approach on both public
benchmarks and self-collected datasets, demonstrating substantial improvements
in accuracy and robustness over existing visual-centered multi-sensor SLAM
systems. The code will be released open-source to support reproducibility and
further research (https://github.com/GREAT-WHU/MASt3R-Fusion).

</details>


### [225] [Leveraging Temporally Extended Behavior Sharing for Multi-task Reinforcement Learning](https://arxiv.org/abs/2509.20766)
*Gawon Lee,Daesol Cho,H. Jin Kim*

Main category: cs.RO

TL;DR: MT-Lévy is a multi-task reinforcement learning exploration strategy that blends behavior sharing across tasks with Levy-flight-inspired, temporally extended exploration. It uses policies learned on related tasks to steer exploration toward key states and adapts exploration intensity based on task success ratios to improve sample efficiency in robotics.


<details>
  <summary>Details</summary>
Motivation: Robotics data collection for multi-task reinforcement learning is expensive, and existing exploration strategies struggle to achieve sample-efficient learning across diverse tasks. There is a need for methods that leverage cross-task knowledge while efficiently exploring high-dimensional robotic state spaces.

Method: Introduce MT-Lévy, which combines behavior sharing across tasks with temporally extended exploration inspired by Lévy flight. Policies trained on related tasks guide exploration toward salient states, while the exploration level is dynamically adjusted according to task success ratios. This yields improved state-space coverage and more efficient learning in MTRL robotics environments.

Result: Empirical evaluations show MT-Lévy significantly improves exploration efficiency and sample efficiency in MTRL robotics tasks. Quantitative metrics and qualitative analyses support the gains, and ablation studies confirm the contribution of both behavior sharing and adaptive exploration components.

Conclusion: Combining behavior sharing with adaptive, Lévy-inspired exploration enhances practical MTRL performance in robotics, enabling more efficient and scalable learning by better guiding exploration and leveraging cross-task knowledge.

Abstract: Multi-task reinforcement learning (MTRL) offers a promising approach to
improve sample efficiency and generalization by training agents across multiple
tasks, enabling knowledge sharing between them. However, applying MTRL to
robotics remains challenging due to the high cost of collecting diverse task
data. To address this, we propose MT-L\'evy, a novel exploration strategy that
enhances sample efficiency in MTRL environments by combining behavior sharing
across tasks with temporally extended exploration inspired by L\'evy flight.
MT-L\'evy leverages policies trained on related tasks to guide exploration
towards key states, while dynamically adjusting exploration levels based on
task success ratios. This approach enables more efficient state-space coverage,
even in complex robotics environments. Empirical results demonstrate that
MT-L\'evy significantly improves exploration and sample efficiency, supported
by quantitative and qualitative analyses. Ablation studies further highlight
the contribution of each component, showing that combining behavior sharing
with adaptive exploration strategies can significantly improve the practicality
of MTRL in robotics applications.

</details>


### [226] [SemSight: Probabilistic Bird's-Eye-View Prediction of Multi-Level Scene Semantics for Navigation](https://arxiv.org/abs/2509.20839)
*Jiaxuan He,Jiamei Ren,Chongshang Yan,Wenjie Song*

Main category: cs.RO

TL;DR: SemSight is a probabilistic BEV model that predicts multi-level semantic maps for unexplored regions and target distributions, improving navigation efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing approaches focus on single objects or geometric maps and miss room-level semantics; there is a need to predict semantic structures of entire scenes to improve exploration and navigation.

Method: SemSight uses a probabilistic encoder-decoder BEV framework with mask-constrained supervision to jointly infer structural layouts, global scene context, and target area distributions from observed context; trained on simulated frontier-driven exploration over 2,000 indoor layout graphs (40k sequences).

Result: The model improves prediction for key functional categories in unexplored regions, outperforms non-mask-supervised baselines on Structural Consistency (SC) and Region Recognition Accuracy (PA), and reduces search steps in closed-loop robotic navigation.

Conclusion: SemSight enables effective multi-level semantic prediction for target-driven navigation and exploration, leading to more accurate semantic maps and more efficient navigation.

Abstract: In target-driven navigation and autonomous exploration, reasonable prediction
of unknown regions is crucial for efficient navigation and environment
understanding. Existing methods mostly focus on single objects or geometric
occupancy maps, lacking the ability to model room-level semantic structures. We
propose SemSight, a probabilistic bird's-eye-view prediction model for
multi-level scene semantics. The model jointly infers structural layouts,
global scene context, and target area distributions, completing semantic maps
of unexplored areas while estimating probability maps for target categories. To
train SemSight, we simulate frontier-driven exploration on 2,000 indoor layout
graphs, constructing a diverse dataset of 40,000 sequential egocentric
observations paired with complete semantic maps. We adopt an encoder-decoder
network as the core architecture and introduce a mask-constrained supervision
strategy. This strategy applies a binary mask of unexplored areas so that
supervision focuses only on unknown regions, forcing the model to infer
semantic structures from the observed context. Experimental results show that
SemSight improves prediction performance for key functional categories in
unexplored regions and outperforms non-mask-supervised approaches on metrics
such as Structural Consistency (SC) and Region Recognition Accuracy (PA). It
also enhances navigation efficiency in closed-loop simulations, reducing the
number of search steps when guiding robots toward target areas.

</details>


### [227] [ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation](https://arxiv.org/abs/2509.20841)
*Dekun Lu,Wei Gao,Kui Jia*

Main category: cs.RO

TL;DR: Introduces Chain of Moving Oriented Keypoints (CoMOK) as a general action representation for end-to-end robotic manipulation, enabling sub-centimeter accuracy and robust generalization across shapes, multi-stage tasks, and deformable objects, demonstrated by simulated and real-world experiments.


<details>
  <summary>Details</summary>
Motivation: End-to-end policies promise better integration and less information loss, but current methods (including large VLM/VLA models) underperform for large-scale deployment; need a representation and training approach that is general, accurate, and reliable.

Method: Proposes CoMOK: a chain of moving oriented keypoints used as the action representation in an end-to-end neural policy. Extends traditional end-effector pose actions to support diverse tasks, leveraging oriented keypoints to generalize across object shapes and sizes, enabling multi-stage, multi-modal, and deformable-object manipulation; trained end-to-end and validated in simulation and on hardware.

Result: Demonstrates sub-centimeter accuracy and effective generalization to different shapes/sizes; capable handling of multi-stage and multi-modal behaviors and deformable objects; extensive simulated and hardware experiments show effectiveness.

Conclusion: CoMOK provides a general, accurate, and reliable end-to-end manipulation framework that mitigates information loss and misalignment in modular pipelines, with strong empirical validation across tasks and settings.

Abstract: End-to-end robot manipulation policies offer significant potential for
enabling embodied agents to understand and interact with the world. Unlike
traditional modular pipelines, end-to-end learning mitigates key limitations
such as information loss between modules and feature misalignment caused by
isolated optimization targets. Despite these advantages, existing end-to-end
neural networks for robotic manipulation--including those based on large
VLM/VLA models--remain insufficiently performant for large-scale practical
deployment. In this paper, we take a step towards an end-to-end manipulation
policy that is generalizable, accurate and reliable. To achieve this goal, we
propose a novel Chain of Moving Oriented Keypoints (CoMOK) formulation for
robotic manipulation. Our formulation is used as the action representation of a
neural policy, which can be trained in an end-to-end fashion. Such an action
representation is general, as it extends the standard end-effector pose action
representation and supports a diverse set of manipulation tasks in a unified
manner. The oriented keypoint in our method enables natural generalization to
objects with different shapes and sizes, while achieving sub-centimeter
accuracy. Moreover, our formulation can easily handle multi-stage tasks,
multi-modal robot behaviors, and deformable objects. Extensive simulated and
hardware experiments demonstrate the effectiveness of our method.

</details>


### [228] [MTRDrive: Memory-Tool Synergistic Reasoning for Robust Autonomous Driving in Corner Cases](https://arxiv.org/abs/2509.20843)
*Ziang Luo,Kangan Qian,Jiahua Wang,Yuechen Luo,Jinyu Miao,Zheng Fu,Yunlong Wang,Sicong Jiang,Zilin Huang,Yifei Hu,Yuhao Yang,Hao Ye,Mengmeng Yang,Xiaojian Dong,Kun Jiang,Diange Yang*

Main category: cs.RO

TL;DR: MTRDrive fuses memory-based experience retrieval with dynamic toolkits in a closed-loop, memory-tool synergistic reasoning framework to improve generalization and proactive decision-making in Vision-Language driving; validated by strong NAVSIM metrics and a new Roadwork-VLM zero-shot benchmark.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving via Vision-Language Models (VLMs) currently suffers from fragility, hallucinations, and poor out-of-distribution generalization, hindering real-world deployment; there is a need for reliable, zero-shot capable systems that can robustly generalize to unseen scenarios.

Method: Proposes a closed-loop framework that integrates memory-based experience retrieval with dynamic toolkits to enhance reasoning and decision-making; leverages memory-tool synergistic reasoning; introduces Roadwork-VLM, a challenging zero-shot benchmark for roadwork scenarios; evaluates on NAVSIM and Roadwork-VLM benchmarks.

Result: On NAVSIM, a 3B-parameter MTRDrive achieves PDMS of 88.3 without chain-of-thought, drives metric 79.8%, and planning accuracy 82.6%; zero-shot evaluation on Roadwork-VLM yields a driving metric of 80.2%.

Conclusion: MTRDrive demonstrates enhanced reasoning and decision-making capabilities, suggesting a path toward safer and more reliable Vision-Language driving systems; the Roadwork benchmark provides a rigorous testbed for zero-shot generalization.

Abstract: Vision-Language Models(VLMs) have demonstrated significant potential for
end-to-end autonomous driving, yet a substantial gap remains between their
current capabilities and the reliability necessary for real-world deployment. A
critical challenge is their fragility, characterized by hallucinations and poor
generalization in out-of-distribution (OOD) scenarios. To bridge this gap, we
introduce MTRDrive, a novel framework that integrates procedural driving
experiences with a dynamic toolkit to enhance generalization and proactive
decision-making.
  MTRDrive addresses these limitations through a closed-loop system that
combines a memory-based experience retrieval mechanism with dynamic toolkits.
This synergy enables the model to interact more effectively with its
environment, improving both reasoning and decision-making capabilities with the
help of our memory-tool synergistic reasoning. Additionally, we introduce a new
benchmark based on complex Roadwork construction scenarios to rigorously
evaluate zero-shot generalization.
  Extensive experiments demonstrate the superior effectiveness of our approach.
On the public NAVSIM benchmark, our 3B-parameter MTRDrive model achieves an
exceptional PDMS of 88.3 without chain-of-thought and sets a state-of-the-art
performance bar on high-level planning, with a driving metric score of 79.8\%
and a planning accuracy of 82.6\%. Rigorous zero-shot evaluation on the new
Roadwork-VLM benchmark shows a strong ability to reason robustly in unseen
scenarios, achieving a driving metric score of 80.2\%. These results highlight
MTRDrive's potential to advance autonomous driving toward safer and more
reliable systems.

</details>


### [229] [Efficient Differentiable Contact Model with Long-range Influence](https://arxiv.org/abs/2509.20917)
*Xiaohan Ye,Kui Wu,Zherong Pan,Taku Komura*

Main category: cs.RO

TL;DR: A differentiable physics framework can suffer from unstable or vanishing gradients due to contact modeling; the work defines design principles for contact models to ensure well-behaved gradients, proposes a practical model satisfying these properties, and shows it enables learning complex contact-rich behaviors in locomotion and manipulation with efficient computation.


<details>
  <summary>Details</summary>
Motivation: Gradient-based optimization in differentiable simulators is hampered by abrupt/vanishing gradients, especially from contact dynamics; addressing this root cause can improve convergence and applicability to MPC, design optimization, and neural PDE solvers.

Method: Identify a set of properties that a contact model must satisfy to yield stable gradients; develop a practical contact model that adheres to these properties and remains computationally efficient; integrate into differentiable rigid-body simulation and evaluate on control tasks.

Result: The proposed contact model delivers well-behaved gradient information leading to successful training of complex contact-rich controllers from simple initializations; demonstrates effectiveness on locomotion and manipulation tasks.

Conclusion: A principled, efficient contact model can stabilize gradients in differentiable physics, broadening their applicability to downstream optimization tasks and enabling more robust learning and control.

Abstract: With the maturation of differentiable physics, its role in various downstream
applications: such as model predictive control, robotic design optimization,
and neural PDE solvers, has become increasingly important. However, the
derivative information provided by differentiable simulators can exhibit abrupt
changes or vanish altogether, impeding the convergence of gradient-based
optimizers. In this work, we demonstrate that such erratic gradient behavior is
closely tied to the design of contact models. We further introduce a set of
properties that a contact model must satisfy to ensure well-behaved gradient
information. Lastly, we present a practical contact model for differentiable
rigid-body simulators that satisfies all of these properties while maintaining
computational efficiency. Our experiments show that, even from simple
initializations, our contact model can discover complex, contact-rich control
signals, enabling the successful execution of a range of downstream locomotion
and manipulation tasks.

</details>


### [230] [Autoregressive End-to-End Planning with Time-Invariant Spatial Alignment and Multi-Objective Policy Refinement](https://arxiv.org/abs/2509.20938)
*Jianbo Zhao,Taiyu Ban,Xiangjie Li,Xingtai Gui,Hangning Zhou,Lei Liu,Hongwei Zhao,Bin Li*

Main category: cs.RO

TL;DR: Introduces Time-Invariant Spatial Alignment (TISA) to align ego-centric environmental features across future steps, adds a kinematic action head, and uses Direct Preference Optimization (DPO) in a multi-objective post-training to improve autoregressive end-to-end driving; reports state-of-the-art PDMS on NAVSIM among autoregressive models (89.8).


<details>
  <summary>Details</summary>
Motivation: Autoregressive planners in autonomous driving suffer from spatio-temporal misalignment between past perception and future actions, creating an inconsistent worldview that caps performance.

Method: Proposes a Time-Invariant Spatial Alignment (TISA) module that projects initial environmental features into a consistent ego-centric frame for each future time step; includes a kinematic action prediction head (acceleration and yaw rate) for physically feasible trajectories; introduces a multi-objective post-training stage using Direct Preference Optimization (DPO) to provide targeted, behavior-specific feedback beyond imitation.

Result: Achieves a state-of-the-art 89.8 PDMS on the NAVSIM dataset among autoregressive models; a video is available at the provided URL.

Conclusion: TISA helps reconcile perceptual changes over time and enforces physical feasibility, while DPO enables finer-grained optimization, yielding strong performance gains for autoregressive end-to-end driving pipelines.

Abstract: The inherent sequential modeling capabilities of autoregressive models make
them a formidable baseline for end-to-end planning in autonomous driving.
Nevertheless, their performance is constrained by a spatio-temporal
misalignment, as the planner must condition future actions on past sensory
data. This creates an inconsistent worldview, limiting the upper bound of
performance for an otherwise powerful approach. To address this, we propose a
Time-Invariant Spatial Alignment (TISA) module that learns to project initial
environmental features into a consistent ego-centric frame for each future time
step, effectively correcting the agent's worldview without explicit future
scene prediction. In addition, we employ a kinematic action prediction head
(i.e., acceleration and yaw rate) to ensure physically feasible trajectories.
Finally, we introduce a multi-objective post-training stage using Direct
Preference Optimization (DPO) to move beyond pure imitation. Our approach
provides targeted feedback on specific driving behaviors, offering a more
fine-grained learning signal than the single, overall objective used in
standard DPO. Our model achieves a state-of-the-art 89.8 PDMS on the NAVSIM
dataset among autoregressive models. The video document is available at
https://tisa-dpo-e2e.github.io/.

</details>


### [231] [BactoBot: A Low-Cost, Bacteria-Inspired Soft Underwater Robot for Marine Exploration](https://arxiv.org/abs/2509.20964)
*Rubaiyat Tasnim Chowdhury,Nayan Bala,Ronojoy Roy,Tarek Mahmud*

Main category: cs.RO

TL;DR: Soft, low-cost underwater robot (BactoBot) with 12 silicone arms on a dodecahedral frame enables compliant, potentially omnidirectional propulsion; demonstrated in a water tank using DIY fabrication; aims for gentler, ecosystem-friendly marine exploration in resource-limited settings.


<details>
  <summary>Details</summary>
Motivation: Rigid underwater vehicles pose risks to delicate ecosystems; there is a need for affordable, safe tools for marine science in settings with limited resources.

Method: 12 flexible silicone arms attached to a 3D-printed dodecahedral frame, inspired by bacterial flagellar propulsion; DIY fabrication using food-grade silicone molding, 3D printing, and off-the-shelf microcontrollers; developed waterproofing and buoyancy calibration protocols; tested in a controlled water tank showing forward motion and turning.

Result: Prototype demonstrates forward motion and turning; validates the feasibility of replicating complex biological locomotion at low cost; inherent compliance and redundancy; potential for omnidirectional movement; lays groundwork for autonomous operation and field deployment.

Conclusion: Supports environmentally conscious robotic tooling for marine science in resource-constrained settings and identifies pathways toward autonomy and field deployment.

Abstract: Traditional rigid underwater vehicles pose risks to delicate marine
ecosystems. This paper presents BactoBot, a low-cost, soft underwater robot
designed for safe and gentle marine exploration. Inspired by bacterial
flagellar propulsion, BactoBot features 12 flexible, silicone-based arms
arranged on a 3D-printed dodecahedral frame. The design provides inherent
compliance, redundancy, and the potential for omnidirectional movement. The
prototype was fabricated using accessible DIY methods, including food-grade
silicone molding, 3D printing, and off-the-shelf microcontrollers.
Waterproofing and buoyancy calibration protocols were developed, and the robot
was successfully tested in a controlled water tank, demonstrating forward
motion and turning. The results validate the feasibility of replicating complex
biological locomotion at low cost. The project lays a foundation for
environmentally conscious robotic tools, particularly for marine science in
resource-constrained settings, and identifies pathways toward autonomous
operation and field deployment.

</details>


### [232] [AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation](https://arxiv.org/abs/2509.21006)
*Konstantin Gubernatorov,Artem Voronov,Roman Voronov,Sergei Pasynkov,Stepan Perminov,Ziang Guo,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: A modular, language-guided mobile manipulation framework (AnywhereVLA) that blends SLAM-based navigation with a fine-tuned VLA head for pick-and-place on commodity hardware, achieving 46% success in lab tasks.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of natural language grounded pick-and-place in unseen, dynamic indoor environments by integrating perception, mapping, planning, and manipulation into a unified pipeline.

Method: Parse user prompt into a task graph; condition LiDAR/camera SLAM, metric semantic mapping, and frontier exploration; plan base poses with visibility and reachability; fine-tune SmolVLA head for platform trajectories; onboard deployment on Jetson Orin NX for perception and VLA, and Intel NUC for SLAM, exploration, and control; real-time operation; evaluation in a multi-room lab with static scenes and human motion.

Result: 46% overall task success rate with sustained throughput on embedded compute.

Conclusion: The system combines the reliability of geometry-based navigation with the agility and generalization of language-conditioned manipulation.

Abstract: We address natural language pick-and-place in unseen, unpredictable indoor
environments with AnywhereVLA, a modular framework for mobile manipulation. A
user text prompt serves as an entry point and is parsed into a structured task
graph that conditions classical SLAM with LiDAR and cameras, metric semantic
mapping, and a task-aware frontier exploration policy. An approach planner then
selects visibility and reachability aware pre grasp base poses. For
interaction, a compact SmolVLA manipulation head is fine tuned on platform pick
and place trajectories for the SO-101 by TheRobotStudio, grounding local visual
context and sub-goals into grasp and place proposals. The full system runs
fully onboard on consumer-level hardware, with Jetson Orin NX for perception
and VLA and an Intel NUC for SLAM, exploration, and control, sustaining
real-time operation. We evaluated AnywhereVLA in a multi-room lab under static
scenes and normal human motion. In this setting, the system achieves a $46\%$
overall task success rate while maintaining throughput on embedded compute. By
combining a classical stack with a fine-tuned VLA manipulation, the system
inherits the reliability of geometry-based navigation with the agility and task
generalization of language-conditioned manipulation.

</details>


### [233] [Multi-Robot Vision-Based Task and Motion Planning for EV Battery Disassembly and Sorting](https://arxiv.org/abs/2509.21020)
*Abdelaziz Shaarawy,Cansu Erdogan,Rustam Stolkin,Alireza Rastegarpanah*

Main category: cs.RO

TL;DR: A four-layer Task-and-Motion Planning (TAMP) framework for multi-robot EV-battery disassembly that fuses symbolic planning, TP-GMM-guided motion planning, vision-based localization, and predictive/reactive collision checking to achieve safer, more compact motions than a baseline.


<details>
  <summary>Details</summary>
Motivation: Disassembly of EV batteries requires precise coordination, fast yet reliable motions, and robust collision avoidance in cluttered, dynamic environments; existing methods struggle with autonomy and safety.

Method: Four-layer TAMP: symbolic task planning with cost- and accessibility-aware allocation; TP-GMM-guided motion planner learned from demonstrations; stereo YOLOv8 for real-time component localization; OctoMap 3D mapping and FCL collision checking in MoveIt for predictive digital-twin collision check and reactive avoidance.

Result: Validated on two UR10e robots removing cables, busbars, service plugs, and leaf-cells; substantially safer and more compact motions than a default RRTConnect baseline under the same perception and task assignments: average end-effector path length reduced by 63.3%; makespan reduced by 8.1%; per-arm swept volumes reduced (R1: 0.583→0.139 m^3; R2: 0.696→0.252 m^3); mutual overlap reduced by 47% (0.064→0.034 m^3).

Conclusion: The framework advances autonomy, precision, and safety for multi-robot EV battery disassembly in unstructured, dynamic environments.

Abstract: Electric-vehicle (EV) battery disassembly requires precise multi-robot
coordination, short and reliable motions, and robust collision safety in
cluttered, dynamic scenes. We propose a four-layer task-and-motion planning
(TAMP) framework that couples symbolic task planning and cost- and
accessibility-aware allocation with a TP-GMM-guided motion planner learned from
demonstrations. Stereo vision with YOLOv8 provides real-time component
localization, while OctoMap-based 3D mapping and FCL(Flexible Collision
Library) checks in MoveIt unify predictive digital-twin collision checking with
reactive, vision-based avoidance. Validated on two UR10e robots across cable,
busbar, service plug, and three leaf-cell removals, the approach yields
substantially more compact and safer motions than a default RRTConnect baseline
under identical perception and task assignments: average end-effector path
length drops by $-63.3\%$ and makespan by $-8.1\%$; per-arm swept volumes
shrink (R1: $0.583\rightarrow0.139\,\mathrm{m}^3$; R2:
$0.696\rightarrow0.252\,\mathrm{m}^3$), and mutual overlap decreases by $47\%$
($0.064\rightarrow0.034\,\mathrm{m}^3$). These results highlight improved
autonomy, precision, and safety for multi-robot EV battery disassembly in
unstructured, dynamic environments.

</details>


### [234] [KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models](https://arxiv.org/abs/2509.21027)
*Sibo Li,Qianyue Hao,Yu Shang,Yong Li*

Main category: cs.RO

TL;DR: KeyWorld accelerates text-conditioned robotic world models by focusing transformer computation on a few semantic key frames and using a lightweight interpolator for intermediate frames, achieving 5.68x speedup on LIBERO with improved physical plausibility.


<details>
  <summary>Details</summary>
Motivation: Inference speed and physical plausibility of generated trajectories are bottlenecks in robotic world models; frame-to-frame generation wastes computation on redundant frames and misses semantic transitions.

Method: Identify significant transitions by iteratively simplifying robot motion to obtain ground-truth key frames; train a DiT model to generate these key frames from textual task descriptions; use a lightweight interpolator to fill in intermediate frames by inpainting.

Result: 5.68× acceleration over the frame-to-frame baseline on LIBERO; motion-aware key frames improve physical validity, especially for complex tasks.

Conclusion: KeyWorld offers a practical pathway to real-time robotic world models by combining semantic key-frame reasoning with efficient interpolation; potential applicability to other domains needing efficient, plausible sequence generation; code released.

Abstract: Robotic world models are a promising paradigm for forecasting future
environment states, yet their inference speed and the physical plausibility of
generated trajectories remain critical bottlenecks, limiting their real-world
applications. This stems from the redundancy of the prevailing frame-to-frame
generation approach, where the model conducts costly computation on similar
frames, as well as neglecting the semantic importance of key transitions. To
address this inefficiency, we propose KeyWorld, a framework that improves
text-conditioned robotic world models by concentrating transformers computation
on a few semantic key frames while employing a lightweight convolutional model
to fill the intermediate frames. Specifically, KeyWorld first identifies
significant transitions by iteratively simplifying the robot's motion
trajectories, obtaining the ground truth key frames. Then, a DiT model is
trained to reason and generate these physically meaningful key frames from
textual task descriptions. Finally, a lightweight interpolator efficiently
reconstructs the full video by inpainting all intermediate frames. Evaluations
on the LIBERO benchmark demonstrate that KeyWorld achieves a 5.68$\times$
acceleration compared to the frame-to-frame generation baseline, and focusing
on the motion-aware key frames further contributes to the physical validity of
the generated videos, especially on complex tasks. Our approach highlights a
practical path toward deploying world models in real-time robotic control and
other domains requiring both efficient and effective world models. Code is
released at https://anonymous.4open.science/r/Keyworld-E43D.

</details>


### [235] [MPC-based Deep Reinforcement Learning Method for Space Robotic Control with Fuel Sloshing Mitigation](https://arxiv.org/abs/2509.21045)
*Mahya Ramezani,M. Amin Alandihallaj,Barış Can Yalçın,Miguel Angel Olivares Mendez,Holger Voos*

Main category: cs.RO

TL;DR: An integrated RL-MPC framework (SAC-MPC) for autonomous satellite docking under fuel slosh, outperforming baselines in simulations; validated with Zero-G Lab experiments and 6-DOF simulations.


<details>
  <summary>Details</summary>
Motivation: Docking in microgravity is hindered by fuel sloshing, causing unpredictable disturbances; traditional controllers struggle to maintain stability. ML-based approaches can adapt to complex dynamics, and MPC can accelerate RL training and improve robustness.

Method: Integrate PPO and SAC reinforcement learning with Model Predictive Control (MPC). MPC provides predictive constraints and a model-based guide to RL, accelerating training and improving robustness. Validation includes Zero-G Lab planar stabilization experiments and high-fidelity 6-DOF docking simulations with fuel sloshing.

Result: SAC-MPC achieves superior docking accuracy, higher success rates, and lower control effort, outperforming standalone RL and PPO-MPC methods in simulations.

Conclusion: The approach advances fuel-efficient, disturbance-resilient satellite docking, enhancing feasibility of on-orbit refueling and servicing missions.

Abstract: This paper presents an integrated Reinforcement Learning (RL) and Model
Predictive Control (MPC) framework for autonomous satellite docking with a
partially filled fuel tank. Traditional docking control faces challenges due to
fuel sloshing in microgravity, which induces unpredictable forces affecting
stability. To address this, we integrate Proximal Policy Optimization (PPO) and
Soft Actor-Critic (SAC) RL algorithms with MPC, leveraging MPC's predictive
capabilities to accelerate RL training and improve control robustness. The
proposed approach is validated through Zero-G Lab of SnT experiments for planar
stabilization and high-fidelity numerical simulations for 6-DOF docking with
fuel sloshing dynamics. Simulation results demonstrate that SAC-MPC achieves
superior docking accuracy, higher success rates, and lower control effort,
outperforming standalone RL and PPO-MPC methods. This study advances
fuel-efficient and disturbance-resilient satellite docking, enhancing the
feasibility of on-orbit refueling and servicing missions.

</details>


### [236] [Normalizing Flows are Capable Visuomotor Policy Learning Models](https://arxiv.org/abs/2509.21073)
*Simon Kristoffersson Lind,Jialong Li,Maj Stenmark,Volker Krüger*

Main category: cs.RO

TL;DR: Normalizing Flows Policy (NFP) offers an efficient, uncertainty-aware visuomotor policy that competes with diffusion-based approaches, delivering comparable performance with much faster inference and improved sample efficiency.


<details>
  <summary>Details</summary>
Motivation: Trustworthy, general-purpose robotics requires reliable confidence measures. Diffusion models provide expressive behavior but incur high inference costs and lack direct uncertainty quantification. The work argues that confidence measures are essential for trustworthy policies.

Method: The authors propose a visuomotor policy learned with Normalizing Flows (NF). NF provides exact likelihoods, enabling a statistically sound measure of confidence and fast, invertible inference. They compare NF-based policy to a diffusion-based policy across four simulated robotic tasks and conduct ablations to validate architectural and training choices.

Result: NF Policy achieves performance comparable to or surpassing Diffusion Policy, with improved sample efficiency and up to 30x faster inference. Four-task experiments support efficiency and accuracy gains; ablations confirm key design/training components.

Conclusion: Normalizing Flows Policy is a viable, uncertainty-friendly alternative to diffusion-based policies for general-purpose robotics, combining competitive performance with significantly reduced inference time and built-in confidence estimates.

Abstract: The field of general purpose robotics has recently embraced powerful
probabilistic models, such as diffusion models, to model and learn complex
behaviors. However, these models often come with significant trade-offs, namely
high computational costs for inference and a fundamental inability to quantify
output uncertainty. We argue that a model's trustworthiness, a critical factor
for reliable, general-purpose robotics, is inherently linked to its ability to
provide confidence measures.
  In this work, we introduce Normalizing Flows Policy, a novel visuomotor
policy learning model based on Normalizing Flows. We show that Normalizing
Flows are a natural and powerful alternative to diffusion models, providing
both a statistically sound measure of confidence and a highly efficient
inference process. Through comprehensive experiments across four distinct
simulated robotic tasks, we demonstrate that Normalizing Flows Policy achieves
performance comparable to, and often surpassing, Diffusion Policy, and it does
so not only with improved sample efficiency but also with up to 30 times faster
inference. Additionally, our ablation study validates several key architectural
and training techniques that enable Normalizing Flows to perform well in this
domain.

</details>


### [237] [Flight Dynamics to Sensing Modalities: Exploiting Drone Ground Effect for Accurate Edge Detection](https://arxiv.org/abs/2509.21085)
*Chenyu Zhao,Jingao Xu,Ciyu Ruan,Haoyang Wang,Shengbo Wang,Jiaqi Li,Jirong Zha,Weijie Hong,Zheng Yang,Yunhao Liu,Xiao-Ping Zhang,Xinlei Chen*

Main category: cs.RO

TL;DR: AirTouch repurposes drone ground effect as a lightweight, power-efficient edge-detection modality, delivering ~0.051 m mean detection distance error and 43 mW consumption, and surpassing vision baselines by about 86%.


<details>
  <summary>Details</summary>
Motivation: Edge detection is crucial for disaster relief and autonomous navigation, but existing radar/camera approaches add cost and heavy computation for lightweight drones. A low-power, onboard sensing modality for accurate scene edges is highly desirable.

Method: Leverage basic attitude sensor readings and flight commands to detect ground effect changes, treating ground effect as a sensing modality. Provide theoretical analysis, algorithm design, and implementation to enable accurate edge detection without compromising flight stability. Compare performance with vision-based methods to highlight exclusive advantages in resource efficiency and detection capability.

Result: Mean detection distance error of 0.051 m; outperforms the baseline by 86%; power consumption around 43 mW.

Conclusion: Ground effect–based sensing constitutes a viable, efficient, and accurate edge-detection modality for lightweight drones, reducing cost and power while maintaining stability. It offers advantages over vision-based approaches in resource usage and detection capability, with strong potential for disaster relief and autonomous navigation applications.

Abstract: Drone-based rapid and accurate environmental edge detection is highly
advantageous for tasks such as disaster relief and autonomous navigation.
Current methods, using radars or cameras, raise deployment costs and burden
lightweight drones with high computational demands. In this paper, we propose
AirTouch, a system that transforms the ground effect from a stability "foe" in
traditional flight control views, into a "friend" for accurate and efficient
edge detection. Our key insight is that analyzing drone basic attitude sensor
readings and flight commands allows us to detect ground effect changes. Such
changes typically indicate the drone flying over a boundary of two materials,
making this information valuable for edge detection. We approach this insight
through theoretical analysis, algorithm design, and implementation, fully
leveraging the ground effect as a new sensing modality without compromising
drone flight stability, thereby achieving accurate and efficient scene edge
detection. We also compare this new sensing modality with vision-based methods
to clarify its exclusive advantages in resource efficiency and detection
capability. Extensive evaluations demonstrate that our system achieves a high
detection accuracy with mean detection distance errors of 0.051m, outperforming
the baseline method performance by 86%. With such detection performance, our
system requires only 43 mW power consumption, contributing to this new sensing
modality for low-cost and highly efficient edge detection.

</details>


### [238] [Cross-Modal Instructions for Robot Motion Generation](https://arxiv.org/abs/2509.21107)
*William Barron,Xiaoxiang Dong,Matthew Johnson-Roberson,Weiming Zhi*

Main category: cs.RO

TL;DR: CrossInstruct uses cross-modal, rough annotations to generate executable robot motions by prompting a vision-language model with context, then refining via a smaller model and fusing 2D views into 3D trajectories for RL init.


<details>
  <summary>Details</summary>
Motivation: Reduce data collection burden for robot learning by replacing full motion demonstrations with flexible cross-modal annotations (text labels, rough cues) that still yield actionable motions and generalize beyond seen environments.

Method: Incorporate cross-modal instructions as context to a foundational vision-language model (VLM); the VLM iteratively queries a smaller fine-tuned model to synthesize 2D motions across multiple views, which are fused into a coherent 3D trajectory distribution; these outputs act as targets to initialize a downstream RL pipeline.

Result: Validated on benchmark simulation tasks and real hardware, CrossInstruct provides effective, non-finetuning-based motion initialization and serves as a strong starting point for RL fine-tuning.

Conclusion: CrossInstruct shows that cross-modal instructions can effectively steer robot behavior without traditional demonstrations, leveraging large VLM reasoning paired with a fine-grained policy model to produce generalizable, executable motion across environments and tasks.

Abstract: Teaching robots novel behaviors typically requires motion demonstrations via
teleoperation or kinaesthetic teaching, that is, physically guiding the robot.
While recent work has explored using human sketches to specify desired
behaviors, data collection remains cumbersome, and demonstration datasets are
difficult to scale. In this paper, we introduce an alternative paradigm,
Learning from Cross-Modal Instructions, where robots are shaped by
demonstrations in the form of rough annotations, which can contain free-form
text labels, and are used in lieu of physical motion. We introduce the
CrossInstruct framework, which integrates cross-modal instructions as examples
into the context input to a foundational vision-language model (VLM). The VLM
then iteratively queries a smaller, fine-tuned model, and synthesizes the
desired motion over multiple 2D views. These are then subsequently fused into a
coherent distribution over 3D motion trajectories in the robot's workspace. By
incorporating the reasoning of the large VLM with a fine-grained pointing
model, CrossInstruct produces executable robot behaviors that generalize beyond
the environment of in the limited set of instruction examples. We then
introduce a downstream reinforcement learning pipeline that leverages
CrossInstruct outputs to efficiently learn policies to complete fine-grained
tasks. We rigorously evaluate CrossInstruct on benchmark simulation tasks and
real hardware, demonstrating effectiveness without additional fine-tuning and
providing a strong initialization for policies subsequently refined via
reinforcement learning.

</details>


### [239] [Rich State Observations Empower Reinforcement Learning to Surpass PID: A Drone Ball Balancing Study](https://arxiv.org/abs/2509.21122)
*Mingjiang Liu,Hailong Huang*

Main category: cs.RO

TL;DR: Hierarchical RL-based controller improves drone-ball balancing vs tuned PID; gains come from richer state information.


<details>
  <summary>Details</summary>
Motivation: Explore whether RL's advantage is due to state representation rather than tuning or nonlinear mapping in drone-ball balancing with cable-based interaction.

Method: Propose a two-level control framework: high-level RL policy for balancing decisions; low-level drone control. Train RL in simulation; compare against carefully tuned PID controllers within the same hierarchy. Analyze impact of state observability.

Result: RL achieves superior performance over PID within the same structure. The advantage arises from richer state observations rather than tuning or nonlinear mapping capabilities.

Conclusion: Emphasizes that comprehensive state representation and sensing are crucial for learning-based controllers; improving sensing could further boost performance.

Abstract: This paper addresses a drone ball-balancing task, in which a drone stabilizes
a ball atop a movable beam through cable-based interaction. We propose a
hierarchical control framework that decouples high-level balancing policy from
low-level drone control, and train a reinforcement learning (RL) policy to
handle the high-level decision-making. Simulation results show that the RL
policy achieves superior performance compared to carefully tuned PID
controllers within the same hierarchical structure. Through systematic
comparative analysis, we demonstrate that RL's advantage stems not from
improved parameter tuning or inherent nonlinear mapping capabilities, but from
its ability to effectively utilize richer state observations. These findings
underscore the critical role of comprehensive state representation in
learning-based systems and suggest that enhanced sensing could be instrumental
in improving controller performance.

</details>


### [240] [Automotive-ENV: Benchmarking Multimodal Agents in Vehicle Interface Systems](https://arxiv.org/abs/2509.21143)
*Junfeng Yan,Biao Wu,Meng Fang,Ling Chen*

Main category: cs.RO

TL;DR: Automotive-ENV provides a high-fidelity benchmark with 185 tasks for vehicle GUIs; ASURADA is a geo-aware multimodal agent leveraging GPS context to adapt actions; geo-aware cues improve safety-task success; the authors plan to release the benchmark and tools.


<details>
  <summary>Details</summary>
Motivation: In-vehicle GUIs face unique constraints (driver attention limits, strict safety requirements, location-based interaction). There is a gap in evaluative benchmarks and adaptive multimodal agents tailored to automotive contexts.

Method: Define 185 parameterized tasks spanning explicit control, implicit intent understanding, and safety-aware tasks. Provide structured multimodal observations with precise programmatic checks for reproducible evaluation. Propose ASURADA, a geo-aware agent that uses GPS-informed context to adapt actions based on location, environment, and regional driving norms.

Result: Experiments show that incorporating geo-aware information significantly improves success on safety-aware tasks, underscoring the importance of location-based context in automotive environments.

Conclusion: Automotive-ENV will be released with all tasks and benchmarking tools to advance the development of safe and adaptive in-vehicle agents.

Abstract: Multimodal agents have demonstrated strong performance in general GUI
interactions, but their application in automotive systems has been largely
unexplored. In-vehicle GUIs present distinct challenges: drivers' limited
attention, strict safety requirements, and complex location-based interaction
patterns. To address these challenges, we introduce Automotive-ENV, the first
high-fidelity benchmark and interaction environment tailored for vehicle GUIs.
This platform defines 185 parameterized tasks spanning explicit control,
implicit intent understanding, and safety-aware tasks, and provides structured
multimodal observations with precise programmatic checks for reproducible
evaluation. Building on this benchmark, we propose ASURADA, a geo-aware
multimodal agent that integrates GPS-informed context to dynamically adjust
actions based on location, environmental conditions, and regional driving
norms. Experiments show that geo-aware information significantly improves
success on safety-aware tasks, highlighting the importance of location-based
context in automotive environments. We will release Automotive-ENV, complete
with all tasks and benchmarking tools, to further the development of safe and
adaptive in-vehicle agents.

</details>


### [241] [DAGDiff: Guiding Dual-Arm Grasp Diffusion to Stable and Collision-Free Grasps](https://arxiv.org/abs/2509.21145)
*Md Faizal Karim,Vignesh Vembar,Keshab Patra,Gaurav Singh,K Madhava Krishna*

Main category: cs.RO

TL;DR: DAGDiff is an end-to-end diffusion-based framework that directly denoises SE(3)xSE(3) dual-arm grasp pairs, guided by geometry, stability, and collision signals to ensure physically valid, force-closure compliant grasps, with improvements over prior work in analysis, simulation, and real-world deployment on unseen objects.


<details>
  <summary>Details</summary>
Motivation: Dual-arm grasping requires stable, collision-free, and generalizable grasps for large/complex objects. Prior methods rely on region priors or heuristics and lack principled stability guarantees, hindering generalization and reliability.

Method: DAGDiff integrates geometry-, stability-, and collision-aware guidance into a diffusion process to directly generate grasp pairs in SE(3)xSE(3) without region priors. It uses classifier signals to steer denoising toward physically valid, force-closure compliant grasps, and validates via analytical force-closure checks, collision analyses, and physics-based simulations. It demonstrates real-world grasping on unseen objects with a heterogeneous dual-arm setup.

Result: Consistent improvements over prior methods on stability and collision metrics, validated analytically, in large-scale simulations, and on real-world point clouds with unseen objects, achieving reliable dual-arm grasping and lifting.

Conclusion: End-to-end dual-arm grasp generation with integrated geometry, stability, and collision guidance enables better generalization to unseen objects and reliable execution on real hardware.

Abstract: Reliable dual-arm grasping is essential for manipulating large and complex
objects but remains a challenging problem due to stability, collision, and
generalization requirements. Prior methods typically decompose the task into
two independent grasp proposals, relying on region priors or heuristics that
limit generalization and provide no principled guarantee of stability. We
propose DAGDiff, an end-to-end framework that directly denoises to grasp pairs
in the SE(3) x SE(3) space. Our key insight is that stability and collision can
be enforced more effectively by guiding the diffusion process with classifier
signals, rather than relying on explicit region detection or object priors. To
this end, DAGDiff integrates geometry-, stability-, and collision-aware
guidance terms that steer the generative process toward grasps that are
physically valid and force-closure compliant. We comprehensively evaluate
DAGDiff through analytical force-closure checks, collision analysis, and
large-scale physics-based simulations, showing consistent improvements over
previous work on these metrics. Finally, we demonstrate that our framework
generates dual-arm grasps directly on real-world point clouds of previously
unseen objects, which are executed on a heterogeneous dual-arm setup where two
manipulators reliably grasp and lift them.

</details>


### [242] [Human-like Navigation in a World Built for Humans](https://arxiv.org/abs/2509.21189)
*Bhargav Chandaka,Gloria X. Wang,Haozhe Chen,Henry Che,Albert J. Zhai,Shenlong Wang*

Main category: cs.RO

TL;DR: ReasonNav uses a vision-language model with landmark-based abstractions to enable higher-level reasoning for efficient indoor navigation in large buildings.


<details>
  <summary>Details</summary>
Motivation: Humans navigate large, unfamiliar environments by reading signs and asking for directions, reducing search; robots lack such behaviors, making navigation inefficient in large man-made spaces.

Method: ReasonNav is a modular navigation system that leverages a vision-language model (VLM) and introduces compact input/output abstractions based on navigation landmarks, allowing the VLM to focus on language understanding and reasoning to guide navigation.

Result: Evaluated on real and simulated navigation tasks; the agent successfully employs higher-order reasoning to navigate efficiently in large, complex buildings.

Conclusion: Integrating human-like reasoning via a VLM with landmark-based abstractions yields efficient indoor navigation; the modular design supports extension and deployment in real-world environments.

Abstract: When navigating in a man-made environment they haven't visited before--like
an office building--humans employ behaviors such as reading signs and asking
others for directions. These behaviors help humans reach their destinations
efficiently by reducing the need to search through large areas. Existing robot
navigation systems lack the ability to execute such behaviors and are thus
highly inefficient at navigating within large environments. We present
ReasonNav, a modular navigation system which integrates these human-like
navigation skills by leveraging the reasoning capabilities of a vision-language
model (VLM). We design compact input and output abstractions based on
navigation landmarks, allowing the VLM to focus on language understanding and
reasoning. We evaluate ReasonNav on real and simulated navigation tasks and
show that the agent successfully employs higher-order reasoning to navigate
efficiently in large, complex buildings.

</details>


### [243] [Next-Generation Aerial Robots -- Omniorientational Strategies: Dynamic Modeling, Control, and Comparative Analysis](https://arxiv.org/abs/2509.21210)
*Ali Kafili Gavgani,Amin Talaeizadeh,Aria Alasty,Hossein Nejat Pishkenari,Esmaeil Najafi*

Main category: cs.RO

TL;DR: Omniorientational drones with tilted propeller axes are proposed, with detailed dynamic models, two controllers (sliding mode and gravity-compensated PID with allocators), and a power-aware control allocation to extend battery life; validated by Simscape Multibody and simulations; offers design guidance for configuration and controller choice.


<details>
  <summary>Details</summary>
Motivation: Conventional multi-rotors are under-actuated, making independent attitude and position control difficult. The work introduces configurations with additional inputs to tilt propeller axes, achieving omniorientation and enabling better control and energy efficiency; aims to provide a roadmap for design and controller strategies in this domain.

Method: Derive dynamic models for all introduced omniorientational configurations; validate via Simscape Multibody simulations; design a sliding mode controller for robustness and a gravity-compensated PID-based controller that integrates linear and nonlinear allocators; develop a custom control allocation strategy to handle input-non-affine dynamics and minimize a Power Consumption Factor; perform simulations to compare configurations and controllers with emphasis on power consumption; conduct qualitative uncertainty analysis to assess robustness and suggest hardware/model improvements.

Result: Simulations show the controllers effectively handle disturbances and uncertainties; omniorientational configurations are evaluated and compared primarily on power consumption; a dedicated control allocation strategy enhances efficiency; the study provides qualitative insights into how uncertainty types influence control performance.

Conclusion: The work offers a practical roadmap for designing omniorientational drones, guiding configuration selection and controller design, and aligning with the SAC-1 objective of Sharif AgRoLab. It highlights practical insights and potential hardware/model improvements for future research.

Abstract: Conventional multi-rotors are under-actuated systems, hindering them from
independently controlling attitude from position. In this study, we present
several distinct configurations that incorporate additional control inputs for
manipulating the angles of the propeller axes. This addresses the mentioned
limitations, making the systems "omniorientational". We comprehensively derived
detailed dynamic models for all introduced configurations and validated by a
methodology using Simscape Multibody simulations. Two controllers are designed:
a sliding mode controller for robust handling of disturbances and a novel
PID-based controller with gravity compensation integrating linear and
non-linear allocators, designed for computational efficiency. A custom control
allocation strategy is implemented to manage the input-non-affine nature of
these systems, seeking to maximize battery life by minimizing the "Power
Consumption Factor" defined in this study. Moreover, the controllers
effectively managed harsh disturbances and uncertainties. Simulations compare
and analyze the proposed configurations and controllers, majorly considering
their power consumption. Furthermore, we conduct a qualitative comparison to
evaluate the impact of different types of uncertainties on the control system,
highlighting areas for potential model or hardware improvements. The analysis
in this study provides a roadmap for future researchers to design
omniorientational drones based on their design objectives, offering practical
insights into configuration selection and controller design. This research
aligns with the project SAC-1, one of the objectives of Sharif AgRoLab.

</details>


### [244] [SEEC: Stable End-Effector Control with Model-Enhanced Residual Learning for Humanoid Loco-Manipulation](https://arxiv.org/abs/2509.21231)
*Jaehwi Jang,Zhuoheng Wang,Ziyi Zhou,Feiyang Wu,Ye Zhao*

Main category: cs.RO

TL;DR: Stable end-effector control (SEEC) using model-enhanced residual learning and model-guided RL with a perturbation generator to achieve precise, robust end-effector stabilization for lower-body disturbances, transferable to unseen locomotion controllers without retraining.


<details>
  <summary>Details</summary>
Motivation: End-effector stabilization in humanoid loco-manipulation is challenging due to high degrees of freedom and dynamic instability. Model-based controllers require precise dynamics and struggle with real-world factors (friction, backlash). Learning-based methods help but overfit to training conditions and need retraining for unseen scenarios. A robust, transferable framework is needed.

Method: Introduce SEEC, a framework that combines model-enhanced residual learning with model-guided reinforcement learning and a perturbation generator to compensate lower-body disturbances. The upper-body policy achieves precise end-effector stabilization and can adapt to unseen locomotion controllers with no additional training.

Result: Experiments across different simulators and real-robot transfer to the Booster T1 demonstrate that SEEC outperforms baselines and robustly handles diverse and demanding loco-manipulation tasks.

Conclusion: SEEC enables precise and robust end-effector stabilization and rapid adaptation to unseen locomotion controllers without retraining, with strong performance in simulation and real-world transfer.

Abstract: Arm end-effector stabilization is essential for humanoid loco-manipulation
tasks, yet it remains challenging due to the high degrees of freedom and
inherent dynamic instability of bipedal robot structures. Previous model-based
controllers achieve precise end-effector control but rely on precise dynamics
modeling and estimation, which often struggle to capture real-world factors
(e.g., friction and backlash) and thus degrade in practice. On the other hand,
learning-based methods can better mitigate these factors via exploration and
domain randomization, and have shown potential in real-world use. However, they
often overfit to training conditions, requiring retraining with the entire
body, and still struggle to adapt to unseen scenarios. To address these
challenges, we propose a novel stable end-effector control (SEEC) framework
with model-enhanced residual learning that learns to achieve precise and robust
end-effector compensation for lower-body induced disturbances through
model-guided reinforcement learning (RL) with a perturbation generator. This
design allows the upper-body policy to achieve accurate end-effector
stabilization as well as adapt to unseen locomotion controllers with no
additional training. We validate our framework in different simulators and
transfer trained policies to the Booster T1 humanoid robot. Experiments
demonstrate that our method consistently outperforms baselines and robustly
handles diverse and demanding loco-manipulation tasks.

</details>


### [245] [FSGlove: An Inertial-Based Hand Tracking System with Shape-Aware Calibration](https://arxiv.org/abs/2509.21242)
*Yutong Li,Jieyi Zhang,Wenqiang Xu,Tutian Tang,Cewu Lu*

Main category: cs.RO

TL;DR: FSGlove is an inertial-based hand MoCap system that captures up to 48 DoFs and personalizes hand shape via DiffHCal calibration, integrating with MANO through differentiable optimization. It surpasses 21-DoF gloves and Nokov optical MoCap in joint accuracy, shape reconstruction, and contact fidelity, with open-source hardware/software and compatibility with VR/robotics.


<details>
  <summary>Details</summary>
Motivation: Existing hand tracking systems struggle to capture high-DoF joint kinematics and personalized hand shapes; commercial gloves offer limited DoFs and ignore shape variations critical for contact-rich tasks. There is a need for a high-DoF, personalized, contact-aware hand tracking solution compatible with VR/robotics ecosystems.

Method: An IMU network on each finger joint and the dorsum enables high-resolution sensing. DiffHCal performs differentiable calibration that integrates with the MANO hand model to jointly estimate joint kinematics, shape parameters, and sensor misalignment in a single optimized workflow, resulting in up to 48 DoFs without separate calibration steps.

Result: The system achieves joint angle errors below 2.7 degrees and outperforms commercial alternatives in shape reconstruction and contact fidelity. FSGlove demonstrates state-of-the-art accuracy, validated against Nokov optical MoCap, and is available as open-source hardware and software.

Conclusion: FSGlove advances hand tracking by unifying kinematic and contact fidelity, offering high-DoF, personalized hand shapes in a VR/robotics-compatible, open-source package, capable of capturing subtle motions like fingertip rubbing.

Abstract: Accurate hand motion capture (MoCap) is vital for applications in robotics,
virtual reality, and biomechanics, yet existing systems face limitations in
capturing high-degree-of-freedom (DoF) joint kinematics and personalized hand
shape. Commercial gloves offer up to 21 DoFs, which are insufficient for
complex manipulations while neglecting shape variations that are critical for
contact-rich tasks. We present FSGlove, an inertial-based system that
simultaneously tracks up to 48 DoFs and reconstructs personalized hand shapes
via DiffHCal, a novel calibration method. Each finger joint and the dorsum are
equipped with IMUs, enabling high-resolution motion sensing. DiffHCal
integrates with the parametric MANO model through differentiable optimization,
resolving joint kinematics, shape parameters, and sensor misalignment during a
single streamlined calibration. The system achieves state-of-the-art accuracy,
with joint angle errors of less than 2.7 degree, and outperforms commercial
alternatives in shape reconstruction and contact fidelity. FSGlove's
open-source hardware and software design ensures compatibility with current VR
and robotics ecosystems, while its ability to capture subtle motions (e.g.,
fingertip rubbing) bridges the gap between human dexterity and robotic
imitation. Evaluated against Nokov optical MoCap, FSGlove advances hand
tracking by unifying the kinematic and contact fidelity. Hardware design,
software, and more results are available at:
https://sites.google.com/view/fsglove.

</details>


### [246] [RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models](https://arxiv.org/abs/2509.21243)
*Jiyeon Koo,Taewan Cho,Hyunjoon Kang,Eunseom Pyo,Tae Gyun Oh,Taeryang Kim,Andrew Jaeyong Choi*

Main category: cs.RO

TL;DR: A lightweight Vision-Language-Action model, RetoVLA, reuses Register Tokens to boost spatial reasoning, achieving a 17.1 percentage-point improvement in success rate on complex manipulation with a 7-DOF robot arm.


<details>
  <summary>Details</summary>
Motivation: Current VLA models are large and computationally expensive; lightweight approaches often sacrifice spatial reasoning. There is a need to maintain performance while reducing compute for real-world robotic deployment.

Method: RetoVLA reuses Register Tokens (previously used to remove artifacts in Vision Transformers) by injecting them into the Action Expert to provide spatial context, preserving a lightweight architecture while leveraging additional spatial information.

Result: Experiments on a custom-built 7-DOF robot arm show a 17.1 percentage-point absolute improvement in success rates on complex manipulation tasks; results support that reusing Register Tokens enhances spatial reasoning; a video demonstration is provided.

Conclusion: What was discarded as an artifact—the Register Tokens—has practical value for robotic spatial reasoning. Reusing them yields a more capable yet efficient VLA model, suggesting a promising direction for resource-conscious robotic intelligence.

Abstract: Recent Vision-Language-Action (VLA) models demonstrate remarkable
generalization in robotics but are restricted by their substantial size and
computational cost, limiting real-world deployment. However, conventional
lightweighting methods often sacrifice critical capabilities, particularly
spatial reasoning. This creates a trade-off between efficiency and performance.
To address this challenge, our work reuses Register Tokens, which were
introduced for artifact removal in Vision Transformers but subsequently
discarded. We suppose that these tokens contain essential spatial information
and propose RetoVLA, a novel architecture that reuses them directly by
injecting them into the Action Expert.
  RetoVLA maintains a lightweight structure while leveraging this repurposed
spatial context to enhance reasoning. We demonstrate RetoVLA's effectiveness
through a series of comprehensive experiments. On our custom-built 7-DOF robot
arm, the model achieves a 17.1%p absolute improvement in success rates for
complex manipulation tasks. Our results confirm that reusing Register Tokens
directly enhances spatial reasoning, demonstrating that what was previously
discarded as an artifact is in fact a valuable, unexplored resource for robotic
intelligence. A video demonstration is available at:
https://youtu.be/2CseBR-snZg

</details>


### [247] [BiNoMaP: Learning Category-Level Bimanual Non-Prehensile Manipulation Primitives](https://arxiv.org/abs/2509.21256)
*Huayi Zhou,Kui Jia*

Main category: cs.RO

TL;DR: BiNoMaP introduces a dual-arm, RL-free framework for non-prehensile manipulation, learning bimanual manipulation primitives from demonstrations and refining them via geometry-aware post-optimization to enable category-level generalization across object sizes.


<details>
  <summary>Details</summary>
Motivation: Non-prehensile manipulation is important for dexterous, contact-rich manipulation but remains underexplored due to difficulty of modeling and the reliance on single-arm setups or strong external dexterity. A generalizable dual-arm approach with RL-free learning could broaden capabilities and robustness across diverse objects and tasks.

Method: A three-stage, RL-free pipeline: (1) extract bimanual hand motion trajectories from video demonstrations; (2) apply a geometry-aware post-optimization step to convert coarse trajectories into executable bimanual manipulation primitives that align with specific motion patterns; (3) parameterize these primitives with object-relevant geometric attributes (notably size) to enable category-level generalization across object categories.

Result: BiNoMaP is validated across a spectrum of representative bimanual tasks and diverse object categories, showing effectiveness, efficiency, versatility, and superior generalization of the learned primitives.

Conclusion: The work demonstrates that high-level bimanual non-prehensile skills can be learned without RL by leveraging demonstration-driven trajectory extraction, geometry-aware refinement, and parametric generalization, yielding adaptable and scalable manipulation primitives suitable for broad object categories.

Abstract: Non-prehensile manipulation, encompassing ungraspable actions such as
pushing, poking, and pivoting, represents a critical yet underexplored domain
in robotics due to its contact-rich and analytically intractable nature. In
this work, we revisit this problem from two novel perspectives. First, we move
beyond the usual single-arm setup and the strong assumption of favorable
external dexterity such as walls, ramps, or edges. Instead, we advocate a
generalizable dual-arm configuration and establish a suite of Bimanual
Non-prehensile Manipulation Primitives (BiNoMaP). Second, we depart from the
prevailing RL-based paradigm and propose a three-stage, RL-free framework to
learn non-prehensile skills. Specifically, we begin by extracting bimanual hand
motion trajectories from video demonstrations. Due to visual inaccuracies and
morphological gaps, these coarse trajectories are difficult to transfer
directly to robotic end-effectors. To address this, we propose a geometry-aware
post-optimization algorithm that refines raw motions into executable
manipulation primitives that conform to specific motion patterns. Beyond
instance-level reproduction, we further enable category-level generalization by
parameterizing the learned primitives with object-relevant geometric
attributes, particularly size, resulting in adaptable and general parameterized
manipulation primitives. We validate BiNoMaP across a range of representative
bimanual tasks and diverse object categories, demonstrating its effectiveness,
efficiency, versatility, and superior generalization capability.

</details>


### [248] [\LARGE GMP$^{3}$: Learning-Driven, Bellman-Guided Trajectory Planning for UAVs in Real-Time on SE(3)](https://arxiv.org/abs/2509.21264)
*Babak Salamat,Dominik Mattern,Sebastian-Sven Olzem,Gerhard Elsbacher,Christian Seidel,Andrea M. Tonello*

Main category: cs.RO

TL;DR: GMP^3 is a distributed RL-based SE(3) trajectory planner for UAVs with consensus-driven policy sharing and DroneManager MAVLink integration, validated in simulations and indoor flights.


<details>
  <summary>Details</summary>
Motivation: The problem of planning dynamically feasible, 3D trajectories for UAVs in cluttered environments requires handling both translation and rotation and coordinating multiple agents. Extending planning to SE(3) and using learning with prior trajectories aims to improve convergence and feasibility, while a distributed consensus scheme enables cooperative behavior and scalability.

Method: Extend path planning to the Lie group SE(3) to jointly learn translational and rotational dynamics. Introduce a modified Bellman-based operator for RL updates that leverages prior trajectory information. Implement a distributed, consensus-based framework where agents refine segments and share policy updates with neighbors, promoting globally shaped paths under kinematic constraints. Provide DroneManager, a modular ground control interface enabling real-time deployment via MAVLink. Validate through simulations and indoor flight experiments; open-source implementation available.

Result: The approach yields reliable obstacle avoidance and smooth, feasible trajectories in constrained 3D environments, with demonstrated real-time capability and cooperative convergence toward globally consistent paths.

Conclusion: GMP^3, together with DroneManager, offers a scalable, distributed RL-based framework for SE(3) trajectory planning in cluttered environments, validated in simulation and experiments and available as open source; suitable for real-time UAV operations.

Abstract: We propose $\text{GMP}^{3}$, a multiphase global path planning framework that
generates dynamically feasible three-dimensional trajectories for unmanned
aerial vehicles (UAVs) operating in cluttered environments. The framework
extends traditional path planning from Euclidean position spaces to the Lie
group $\mathrm{SE}(3)$, allowing joint learning of translational motion and
rotational dynamics. A modified Bellman-based operator is introduced to support
reinforcement learning (RL) policy updates while leveraging prior trajectory
information for improved convergence. $\text{GMP}^{3}$ is designed as a
distributed framework in which agents influence each other and share policy
information along the trajectory: each agent refines its assigned segment and
shares with its neighbors via a consensus-based scheme, enabling cooperative
policy updates and convergence toward a path shaped globally even under
kinematic constraints. We also propose DroneManager, a modular ground control
software that interfaces the planner with real UAV platforms via the MAVLink
protocol, supporting real-time deployment and feedback. Simulation studies and
indoor flight experiments validate the effectiveness of the proposed method in
constrained 3D environments, demonstrating reliable obstacle avoidance and
smooth, feasible trajectories across both position and orientation. The
open-source implementation is available at
https://github.com/Domattee/DroneManager

</details>


### [249] [Taxonomy-aware Dynamic Motion Generation on Hyperbolic Manifolds](https://arxiv.org/abs/2509.21281)
*Luis Augenstein,Noémie Jaquier,Tamim Asfour,Leonel Rozo*

Main category: cs.RO

TL;DR: GPHDM: a hyperbolic GPDM that preserves motion taxonomy and temporal dynamics for physically consistent human-like motion, with three generation mechanisms, demonstrated on hand grasping taxonomy.


<details>
  <summary>Details</summary>
Motivation: Biomechanical studies reveal hierarchical motion taxonomies that describe how actions relate; however, current motion generation models rarely leverage this structure, leading to a gap between generated motions, taxonomy, and physical plausibility. A geometric framework that encodes hierarchical structure and dynamics could improve realism and consistency.

Method: Extend the Gaussian Process Dynamical Model (GPDM) dynamics priors to the hyperbolic manifold to naturally encode taxonomic hierarchies, and fuse taxonomy-aware inductive biases. Propose three generation mechanisms: two probabilistic recursive methods and a pullback-metric geodesic approach, all guided by taxonomy-aware geometry, evaluated on hand grasping sequences.

Result: The proposed GPHDM faithfully encodes the underlying taxonomy and temporal dynamics and generates novel, physically-consistent motion trajectories; experiments on hand grasping taxonomy show realistic motion sequences and that the model aligns with hierarchical structure.

Conclusion: Geometry- and taxonomy-aware GPDMs can produce taxonomically structured, temporally coherent, and physically plausible human-like motions, offering flexible generation mechanisms for leverage in robotics and biomechanics.

Abstract: Human-like motion generation for robots often draws inspiration from
biomechanical studies, which often categorize complex human motions into
hierarchical taxonomies. While these taxonomies provide rich structural
information about how movements relate to one another, this information is
frequently overlooked in motion generation models, leading to a disconnect
between the generated motions and their underlying hierarchical structure. This
paper introduces the \ac{gphdm}, a novel approach that learns latent
representations preserving both the hierarchical structure of motions and their
temporal dynamics to ensure physical consistency. Our model achieves this by
extending the dynamics prior of the Gaussian Process Dynamical Model (GPDM) to
the hyperbolic manifold and integrating it with taxonomy-aware inductive
biases. Building on this geometry- and taxonomy-aware frameworks, we propose
three novel mechanisms for generating motions that are both
taxonomically-structured and physically-consistent: two probabilistic recursive
approaches and a method based on pullback-metric geodesics. Experiments on
generating realistic motion sequences on the hand grasping taxonomy show that
the proposed GPHDM faithfully encodes the underlying taxonomy and temporal
dynamics, and generates novel physically-consistent trajectories.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [250] [An Approach to Checking Correctness for Agentic Systems](https://arxiv.org/abs/2509.20364)
*Thomas J Sheffler*

Main category: cs.AI

TL;DR: Temporal expression language for monitoring AI agent behavior to detect regressions via action sequences, robust to natural-language variation.


<details>
  <summary>Details</summary>
Motivation: Current error checks rely on text matching, which is fragile due to the natural language variability of LLM responses. A robust, behavior-centric verification framework is needed to validate sequences of actions and inter-agent interactions across scenarios.

Method: Introduces a temporal expression language inspired by hardware verification to monitor execution traces of agent tool calls and state transitions. It defines assertions that capture correct multi-step behavioral patterns across scenarios, enabling validation of prompt design and guardrails as well as regression testing when LLMs or logic are updated. Demonstrated on a three-agent system coordinating multi-step reasoning tasks.

Result: When powered by large, capable models, all temporal assertions were satisfied across many runs. Replacing two agents with smaller models led to violations of behavioral assertions due to improper tool sequencing and failed handoffs; the temporal expressions successfully flagged these anomalies, showing effectiveness in detecting regressions.

Conclusion: A foundation for systematic monitoring of AI agent reliability in critical deployments, supporting robust prompt engineering, guardrail validation, and regression testing as models and agent logic evolve.

Abstract: This paper presents a temporal expression language for monitoring AI agent
behavior, enabling systematic error-detection of LLM-based agentic systems that
exhibit variable outputs due to stochastic generation processes. Drawing from
temporal logic techniques used in hardware verification, this approach monitors
execution traces of agent tool calls and state transitions to detect deviations
from expected behavioral patterns. Current error-detection approaches rely
primarily on text matching of inputs and outputs, which proves fragile due to
the natural language variability inherent in LLM responses. The proposed method
instead focuses on the sequence of agent actions -- such as tool invocations
and inter-agent communications -- allowing verification of system behavior
independent of specific textual outputs. The temporal expression language
provides assertions that capture correct behavioral patterns across multiple
execution scenarios. These assertions serve dual purposes: validating prompt
engineering and guardrail effectiveness during development, and providing
regression testing when agents are updated with new LLMs or modified logic. The
approach is demonstrated using a three-agent system, where agents coordinate to
solve multi-step reasoning tasks. When powered by large, capable models, all
temporal assertions were satisfied across many test runs. However, when smaller
models were substituted in two of the three agents, executions violated
behavioral assertions, primarily due to improper tool sequencing and failed
coordination handoffs. The temporal expressions successfully flagged these
anomalies, demonstrating the method's effectiveness for detecting behavioral
regressions in production agentic systems. This approach provides a foundation
for systematic monitoring of AI agent reliability as these systems become
increasingly deployed in critical applications.

</details>


### [251] [LATTS: Locally Adaptive Test-Time Scaling](https://arxiv.org/abs/2509.20368)
*Theo Uscidda,Matthew Trager,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: LATTS introduces a verifier-guided per-step compute policy for test-time scaling in LLM generation, achieving a better accuracy-per-compute tradeoff than uniform verifier-based methods.


<details>
  <summary>Details</summary>
Motivation: To reduce wasteful computation in test-time scaling by allocating compute adaptively to the complexity of each generation step, as measured by a verifier.

Method: At each generation step, LATTS uses a verifier-based acceptance criterion to decide actions (resample, backtrack, restart, or stop). The per-step compute is adjusted based on a notion of local difficulty derived from the verifier, enabling variable, step-level compute rather than uniform scaling.

Result: Empirical results show that LATTS achieves significantly superior accuracy–compute tradeoffs compared to standard verifier-based methods.

Conclusion: Locally adaptive test-time scaling can improve efficiency and accuracy in LLM generation by tailoring per-step computation to local difficulty; success depends on the verifier's quality and calibration, with potential overhead and calibration needs.

Abstract: One common strategy for improving the performance of Large Language Models
(LLMs) on downstream tasks involves using a \emph{verifier model} to either
select the best answer from a pool of candidates or to steer the
auto-regressive generation process towards better outputs. This class of
methods typically results in improved accuracy at the cost of increased
computation at test-time, a paradigm known as \emph{test-time scaling}.
However, most existing approaches increase computation uniformly across all
samples and generation steps, without considering the complexity of individual
instances, leading to inefficient resource use. We address this limitation by
proposing an approach, called \emph{Locally Adaptive Test-Time Scaling
(LATTS)}, that allocates variable compute across generation steps.
Specifically, at each generation step, LATTS employs a verifier-based
acceptance criterion to decide whether to resample, backtrack, restart, or stop
the generation process. This criterion effectively adjusts the per-step
computational effort based on a precise notion of \emph{local difficulty}
derived from the verifier model. Empirical results show that LATTS achieves
significantly superior accuracy--compute tradeoffs compared to standard
verifier-based methods.

</details>


### [252] [Philosophy-informed Machine Learning](https://arxiv.org/abs/2509.20370)
*MZ Naser*

Main category: cs.AI

TL;DR: Philosophy-informed ML (PhIML) integrates analytic philosophy into ML design to enable philosophy-aligned models; the paper offers conceptual foundations, case studies (agnostic post-hoc and intrinsic integration), and a roadmap addressing technical, governance, and ethical challenges.


<details>
  <summary>Details</summary>
Motivation: To align ML with philosophical concepts and values by design, promoting safer, more ethically responsible AI.

Method: Conceptual review of foundations; case studies illustrating adoption paths; discussion of open technical, governance, and philosophical challenges; proposal of a research roadmap.

Result: Conceptual gains and alignment are demonstrated; practical case studies are presented; barriers (technical, governance, philosophical) are identified; a roadmap toward safe, philosophy-aware PhIML is outlined.

Conclusion: PhIML holds promise for developing safe, philosophy-aware, and ethically responsible ML; the paper outlines directions for future research and governance considerations.

Abstract: Philosophy-informed machine learning (PhIML) directly infuses core ideas from
analytic philosophy into ML model architectures, objectives, and evaluation
protocols. Therefore, PhIML promises new capabilities through models that
respect philosophical concepts and values by design. From this lens, this paper
reviews conceptual foundations to demonstrate philosophical gains and
alignment. In addition, we present case studies on how ML users/designers can
adopt PhIML as an agnostic post-hoc tool or intrinsically build it into ML
model architectures. Finally, this paper sheds light on open technical barriers
alongside philosophical, practical, and governance challenges and outlines a
research roadmap toward safe, philosophy-aware, and ethically responsible
PhIML.

</details>


### [253] [InsightGUIDE: An Opinionated AI Assistant for Guided Critical Reading of Scientific Literature](https://arxiv.org/abs/2509.20493)
*Paris Koloveas,Serafeim Chatzopoulos,Thanasis Vergoulis,Christos Tryfonopoulos*

Main category: cs.AI

TL;DR: InsightGUIDE is an AI-powered reading assistant that provides concise, structured insights (maps) of a paper's key elements, delivering more actionable guidance than general LLM summaries.


<details>
  <summary>Details</summary>
Motivation: Researchers face an ever-growing flood of literature and current LLM summaries can be verbose or replace reading rather than assist it; there is a need for a tool that guides reading while preserving source material.

Method: Describes a prompt-driven system architecture that embeds an expert reading methodology into its core AI logic; includes a qualitative case study comparing InsightGUIDE against a general-purpose LLM.

Result: InsightGUIDE yields more structured and actionable guidance compared to generic LLM outputs, aiding researchers in efficiently engaging with papers.

Conclusion: InsightGUIDE functions as a reading assistant that maps a paper's key elements and supports targeted exploration, without replacing the actual reading of the source material.

Abstract: The proliferation of scientific literature presents an increasingly
significant challenge for researchers. While Large Language Models (LLMs) offer
promise, existing tools often provide verbose summaries that risk replacing,
rather than assisting, the reading of the source material. This paper
introduces InsightGUIDE, a novel AI-powered tool designed to function as a
reading assistant, not a replacement. Our system provides concise, structured
insights that act as a "map" to a paper's key elements by embedding an expert's
reading methodology directly into its core AI logic. We present the system's
architecture, its prompt-driven methodology, and a qualitative case study
comparing its output to a general-purpose LLM. The results demonstrate that
InsightGUIDE produces more structured and actionable guidance, serving as a
more effective tool for the modern researcher.

</details>


### [254] [Reconstruction-Based Adaptive Scheduling Using AI Inferences in Safety-Critical Systems](https://arxiv.org/abs/2509.20513)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: A reconstruction framework dynamically validates and assembles safe, executable schedules for time-triggered systems, converting AI/heuristic priorities into collision-free, precedence-consistent schedules with safety checks and recovery for mode changes and failures.


<details>
  <summary>Details</summary>
Motivation: Time-triggered systems require reliable, safe scheduling in dynamic environments. Common issues include message collisions, incorrect precedence causing locked loops, and incomplete/invalid schedules that threaten safety and performance.

Method: Introduce reconstruction models that transform AI-generated or heuristic scheduling priorities into fully executable schedules. These models enforce precedence, collision-free communication, and safety checks, implement robust allocation and recovery mechanisms for unexpected events, and support mode transitions and hardware failures.

Result: Experimental evaluation across makespan minimization, workload balancing, and energy efficiency demonstrated that the framework improves adaptability, operational integrity, and runtime performance while maintaining computational efficiency.

Conclusion: The framework offers a practical, scalable solution for safe, dynamic schedule generation in safety-critical TTS, enabling reliable real-time scheduling under dynamic and uncertain conditions.

Abstract: Adaptive scheduling is crucial for ensuring the reliability and safety of
time-triggered systems (TTS) in dynamic operational environments. Scheduling
frameworks face significant challenges, including message collisions, locked
loops from incorrect precedence handling, and the generation of incomplete or
invalid schedules, which can compromise system safety and performance. To
address these challenges, this paper presents a novel reconstruction framework
designed to dynamically validate and assemble schedules. The proposed
reconstruction models operate by systematically transforming AI-generated or
heuristically derived scheduling priorities into fully executable schedules,
ensuring adherence to critical system constraints such as precedence rules and
collision-free communication. It incorporates robust safety checks, efficient
allocation algorithms, and recovery mechanisms to handle unexpected context
events, including hardware failures and mode transitions. Comprehensive
experiments were conducted across multiple performance profiles, including
makespan minimisation, workload balancing, and energy efficiency, to validate
the operational effectiveness of the reconstruction models. Results demonstrate
that the proposed framework significantly enhances system adaptability,
operational integrity, and runtime performance while maintaining computational
efficiency. Overall, this work contributes a practical and scalable solution to
the problem of safe schedule generation in safety-critical TTS, enabling
reliable and flexible real-time scheduling even under highly dynamic and
uncertain operational conditions.

</details>


### [255] [Adaptive Approach to Enhance Machine Learning Scheduling Algorithms During Runtime Using Reinforcement Learning in Metascheduling Applications](https://arxiv.org/abs/2509.20520)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: Adaptive online reinforcement learning unit embedded in a metascheduler replaces offline MSG training, enabling real-time exploration to expand the scheduling space and handle dynamic contexts in time-triggered architectures, boosting robustness and efficiency in safety-critical systems.


<details>
  <summary>Details</summary>
Motivation: Offline MSG generation is infeasible due to the combinatorial probability space and diverse context events (e.g., hardware failures, slack variations, mode changes). An online learning approach is needed to cover unseen scenarios and maintain performance over time.

Method: Integrate an adaptive online learning unit within the metascheduler that deploys multiple RL models to explore, discover, and optimize scheduling solutions in real time, incrementally expanding the MSG, and adapting to stricter deadlines or new performance criteria.

Result: The paper proposes an RL-based online unit that enables discovery of new scheduling solutions and optimization of existing schedulers, improving adaptability to unexpected events and dynamic conditions in large-scale settings.

Conclusion: An online, RL-enhanced metascheduler offers real-time adaptability, robustness, and efficiency by continuously learning from experience and expanding the MSG to meet evolving demands in time-critical environments.

Abstract: Metascheduling in time-triggered architectures has been crucial in adapting
to dynamic and unpredictable environments, ensuring the reliability and
efficiency of task execution. However, traditional approaches face significant
challenges when training Artificial Intelligence (AI) scheduling inferences
offline, particularly due to the complexities involved in constructing a
comprehensive Multi-Schedule Graph (MSG) that accounts for all possible
scenarios. The process of generating an MSG that captures the vast probability
space, especially when considering context events like hardware failures, slack
variations, or mode changes, is resource-intensive and often infeasible. To
address these challenges, we propose an adaptive online learning unit
integrated within the metascheduler to enhance performance in real-time. The
primary motivation for developing this unit stems from the limitations of
offline training, where the MSG created is inherently a subset of the complete
space, focusing only on the most probable and critical context events. In the
online mode, Reinforcement Learning (RL) plays a pivotal role by continuously
exploring and discovering new scheduling solutions, thus expanding the MSG and
enhancing system performance over time. This dynamic adaptation allows the
system to handle unexpected events and complex scheduling scenarios more
effectively. Several RL models were implemented within the online learning
unit, each designed to address specific challenges in scheduling. These models
not only facilitate the discovery of new solutions but also optimize existing
schedulers, particularly when stricter deadlines or new performance criteria
are introduced. By continuously refining the AI inferences through real-time
training, the system remains flexible and capable of meeting evolving demands,
thus ensuring robustness and efficiency in large-scale, safety-critical
environments.

</details>


### [256] [A Compound Classification System Based on Fuzzy Relations Applied to the Noise-Tolerant Control of a Bionic Hand via EMG Signal Recognition](https://arxiv.org/abs/2509.20523)
*Pawel Trajdos,Marek Kurzynski*

Main category: cs.AI

TL;DR: Proposes a fuzzy, two-stage recognition system for EMG-based prosthetic control that detects contaminated biosignals using an ensemble of one-class classifiers (OCC) and then classifies intent with a KNN ensemble, all guided by a unified fuzzy decision framework; evaluated on real EMG data with comparisons to existing methods.


<details>
  <summary>Details</summary>
Motivation: EMG biosignals used for myoelectric hand prosthesis control are highly susceptible to contamination, which degrades recognition quality. There is a need to detect and mitigate contaminated signals to improve robustness.

Method: A two-ensemble approach: an ensemble of one-class classifiers (OCC) to assess channel contamination, and an ensemble of K-nearest-neighbours (KNN) to recognize the patient’s intent. A coherent fuzzy model provides a uniform soft-decision mechanism across the recognition pipeline. Evaluation uses real EMG data from a public repository and includes comparative analyses of parameter settings and procedures, as well as comparisons with similar systems in the literature.

Result: The study demonstrates feasibility and provides a comparative analysis of the proposed method’s parameters and procedures; the fuzzy recognition system is shown to be competitive with analogous approaches and to handle contamination-aware decision making, using real biosignals.

Conclusion: A fuzzy, contamination-aware recognition framework combining OCC-based contamination detection and a KNN ensemble for intent recognition can mitigate the adverse effects of signal contamination in EMG-based prosthetic control, with validation on public EMG data and favorable comparisons to related work.

Abstract: Modern anthropomorphic upper limb bioprostheses are typically controlled by
electromyographic (EMG) biosignals using a pattern recognition scheme.
Unfortunately, there are many factors originating from the human source of
objects to be classified and from the human-prosthesis interface that make it
difficult to obtain an acceptable classification quality. One of these factors
is the high susceptibility of biosignals to contamination, which can
considerably reduce the quality of classification of a recognition system.
  In the paper, the authors propose a new recognition system intended for EMG
based control of the hand prosthesis with detection of contaminated biosignals
in order to mitigate the adverse effect of contaminations. The system consists
of two ensembles: the set of one-class classifiers (OCC) to assess the degree
of contamination of individual channels and the ensemble of K-nearest
neighbours (KNN) classifier to recognise the patient's intent. For all
recognition systems, an original, coherent fuzzy model was developed, which
allows the use of a uniform soft (fuzzy) decision scheme throughout the
recognition process. The experimental evaluation was conducted using real
biosignals from a public repository. The goal was to provide an experimental
comparative analysis of the parameters and procedures of the developed method
on which the quality of the recognition system depends. The proposed fuzzy
recognition system was also compared with similar systems described in the
literature.

</details>


### [257] [SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection](https://arxiv.org/abs/2509.20562)
*Yubin Ge,Salvatore Romeo,Jason Cai,Monica Sunkara,Yi Zhang*

Main category: cs.AI

TL;DR: SAMULE is a self-learning agent framework that uses a retrospective LM trained on Multi-Level Reflection Synthesis to perform micro-, meso-, and macro-level reflections, extends to foresight-based interactive reflection, and achieves significant gains on TravelPlanner, NATURAL PLAN, and Tau-bench benchmarks over baselines.


<details>
  <summary>Details</summary>
Motivation: LLM agents struggle to generate meaningful reflections due to poor error analysis and reliance on rare successful trajectories; a structured, multi-level, retrospective reflection approach can improve self-improvement.

Method: Propose SAMULE: train retrospective LM on Multi-Level Reflection Synthesis across three levels: (micro) Single-Trajectory Learning for detailed error correction; (meso) Intra-Task Learning to build error taxonomies across trials; (macro) Inter-Task Learning to extract transferable insights from similar errors across diverse task failures. Fine-tune a retrospective model to generate reflections during inference. Extend with foresight-based reflection for interactive settings by comparing predictions to actual responses.

Result: Empirical evaluation on TravelPlanner, NATURAL PLAN, and Tau-bench shows significant improvements over reflection-based baselines, underscoring the value of designed reflection synthesis and failure-centric learning.

Conclusion: Well-designed reflection synthesis at multiple levels and proactive foresight enable self-improving LLM agents; SAMULE provides a scalable framework for robust error analysis and adaptation across tasks.

Abstract: Despite the rapid advancements in LLM agents, they still face the challenge
of generating meaningful reflections due to inadequate error analysis and a
reliance on rare successful trajectories, especially in complex tasks. In this
work, we propose SAMULE, a new framework for self-learning agents powered by a
retrospective language model that is trained based on Multi-Level Reflection
Synthesis. It first synthesizes high-quality reflections across three
complementary levels: Single-Trajectory Learning (micro-level) for detailed
error correction; Intra-Task Learning (meso-level) to build error taxonomies
across multiple trials of the same task, and Inter-Task Learning (macro-level)
to extract transferable insights based on same typed errors from diverse task
failures. Then we fine-tune a language model serving as the retrospective model
to generate reflections during inference. We further extend our framework to
interactive settings through a foresight-based reflection mechanism, enabling
agents to proactively reflect and adapt during user interactions by comparing
predicted and actual responses. Extensive experiments on three challenging
benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our
approach significantly outperforms reflection-based baselines. Our results
highlight the critical role of well-designed reflection synthesis and
failure-centric learning in building self-improving LLM agents.

</details>


### [258] [Adaptive Cybersecurity Architecture for Digital Product Ecosystems Using Agentic AI](https://arxiv.org/abs/2509.20640)
*Oluwakemi T. Olayinka,Sumeet Jeswani,Divine Iloh*

Main category: cs.AI

TL;DR: Proposes autonomous goal-driven agents in an adaptive cybersecurity architecture that uses agentic AI for real-time, context-aware defense across cloud, APIs, mobile, and edge; demonstrates zero-day detection and dynamic policy updates with improved adaptability, latency, and detection accuracy in cloud simulations, aligned with zero-trust and regulations.


<details>
  <summary>Details</summary>
Motivation: Traditional static cybersecurity models struggle with scalability, real-time detection, and contextual responsiveness in modern digital ecosystems; there is a need for autonomous, learning, context-aware defense that can operate across cloud, APIs, mobile, and edge devices and comply with international regulations.

Method: Design of an adaptive architecture driven by agentic AI with autonomous threat mitigation, proactive policy enforcement, and real-time anomaly detection, integrated across ecosystem layers; features include behavioral baselining, decentralized risk scoring, and federated threat intelligence sharing; validated via native cloud simulations showing zero-day detection and dynamic access policy modification.

Result: Increased adaptability, reduced response latency, and improved detection accuracy in simulations.

Conclusion: Provides an intelligent, scalable blueprint for safeguarding complex digital infrastructure; compatible with zero-trust models and supports international cybersecurity regulations.

Abstract: Traditional static cybersecurity models often struggle with scalability,
real-time detection, and contextual responsiveness in the current digital
product ecosystems which include cloud services, application programming
interfaces (APIs), mobile platforms, and edge devices. This study introduces
autonomous goal driven agents capable of dynamic learning and context-aware
decision making as part of an adaptive cybersecurity architecture driven by
agentic artificial intelligence (AI). To facilitate autonomous threat
mitigation, proactive policy enforcement, and real-time anomaly detection, this
framework integrates agentic AI across the key ecosystem layers. Behavioral
baselining, decentralized risk scoring, and federated threat intelligence
sharing are important features. The capacity of the system to identify zero-day
attacks and dynamically modify access policies was demonstrated through native
cloud simulations. The evaluation results show increased adaptability,
decreased response latency, and improved detection accuracy. The architecture
provides an intelligent and scalable blueprint for safeguarding complex digital
infrastructure and is compatible with zero-trust models, thereby supporting the
adherence to international cybersecurity regulations.

</details>


### [259] [Accelerate Creation of Product Claims Using Generative AI](https://arxiv.org/abs/2509.20652)
*Po-Yu Liang,Yong Zhang,Tatiana Hwa,Aaron Byers*

Main category: cs.AI

TL;DR: Claim Advisor is a web app that uses in-context learning and fine-tuning of large language models to accelerate product-claim creation via semantic search, generation/optimization, and simulation-based ranking, demonstrated in a consumer packaged goods (CPG) setting with promising results and potential broad applicability.


<details>
  <summary>Details</summary>
Motivation: Crafting persuasive product claims is time-consuming and expensive; AI-powered tooling can speed up claim search, creation, optimization, and validation, reducing costs and time-to-market.

Method: Three core functions: (1) semantic search to identify existing claims/visuals aligned with consumer voice; (2) generation/optimization of claims based on product description and consumer profile; (3) ranking of claims using simulations with synthetic consumers; implementation leverages in-context learning and fine-tuning of large language models.

Result: Applied in a CPG context with very promising results, suggesting improvements in speed and economics of claim search/generation and potential applicability across categories and industries.

Conclusion: The approach shows promise for broad utility beyond CPG, and the authors advocate sharing learnings to spur research and adoption of generative AI for claim creation across industries.

Abstract: The benefit claims of a product is a critical driver of consumers' purchase
behavior. Creating product claims is an intense task that requires substantial
time and funding. We have developed the $\textbf{Claim Advisor}$ web
application to accelerate claim creations using in-context learning and
fine-tuning of large language models (LLM). $\textbf{Claim Advisor}$ was
designed to disrupt the speed and economics of claim search, generation,
optimization, and simulation. It has three functions: (1) semantically
searching and identifying existing claims and/or visuals that resonate with the
voice of consumers; (2) generating and/or optimizing claims based on a product
description and a consumer profile; and (3) ranking generated and/or manually
created claims using simulations via synthetic consumers. Applications in a
consumer packaged goods (CPG) company have shown very promising results. We
believe that this capability is broadly useful and applicable across product
categories and industries. We share our learning to encourage the research and
application of generative AI in different industries.

</details>


### [260] [An Automated Retrieval-Augmented Generation LLaMA-4 109B-based System for Evaluating Radiotherapy Treatment Plans](https://arxiv.org/abs/2509.20707)
*Junjie Cui,Peilong Wang,Jason Holmes,Leshan Sun,Michael L. Hinni,Barbara A. Pockaj,Sujay A. Vora,Terence T. Sio,William W. Wong,Nathan Y. Yu,Steven E. Schild,Joshua R. Niska,Sameer R. Keole,Jean-Claude M. Rwigema,Samir H. Patel,Lisa A. McGee,Carlos A. Vargas,Wei Liu*

Main category: cs.AI

TL;DR: A retrieval-augmented generation system (powered by LLaMA-4 109B) for automated, protocol-aware radiotherapy plan evaluation; demonstrates end-to-end agreement with module computations and robust cross-protocol performance.


<details>
  <summary>Details</summary>
Motivation: Enable scalable, interpretable, protocol-constrained evaluation of radiotherapy plans with traceable outputs, leveraging modular tool-augmented reasoning to minimize hallucinations.

Method: Curated a multi-protocol dataset of 614 radiotherapy plans across four disease sites and built a knowledge base of normalized dose metrics and protocol constraints. The RAG pipeline comprises: (1) a retrieval engine evaluated across five SentenceTransformer backbones, (2) a percentile predictor based on cohort similarity, and (3) a clinical constraint checker. An LLM guides the process through a multi-step prompt-driven reasoning pipeline to produce concise, grounded evaluations.

Result: Hyperparameters for retrieval were optimized with Gaussian Process optimization on a scalarized loss combining RMSE, MAE, and clinically motivated accuracy. The best configuration, using all-MiniLM-L6-v2, achieved perfect nearest-neighbor accuracy within a 5-percentile-point margin and sub-2-point MAE. End-to-end testing showed 100% agreement with the standalone retrieval and constraint-checking modules on both percentile estimates and constraint identification, confirming reliable execution of retrieval, prediction, and checking steps.

Conclusion: The work demonstrates feasibility of combining structured population-based scoring with modular tool-augmented reasoning for transparent, scalable plan evaluation in radiation therapy. It offers traceable outputs, reduces hallucinations, and shows robustness across protocols. Future work includes clinician-led validation and domain-adapted retrieval improvements for real-world integration.

Abstract: Purpose: To develop a retrieval-augmented generation (RAG) system powered by
LLaMA-4 109B for automated, protocol-aware, and interpretable evaluation of
radiotherapy treatment plans.
  Methods and Materials: We curated a multi-protocol dataset of 614
radiotherapy plans across four disease sites and constructed a knowledge base
containing normalized dose metrics and protocol-defined constraints. The RAG
system integrates three core modules: a retrieval engine optimized across five
SentenceTransformer backbones, a percentile prediction component based on
cohort similarity, and a clinical constraint checker. These tools are directed
by a large language model (LLM) using a multi-step prompt-driven reasoning
pipeline to produce concise, grounded evaluations.
  Results: Retrieval hyperparameters were optimized using Gaussian Process on a
scalarized loss function combining root mean squared error (RMSE), mean
absolute error (MAE), and clinically motivated accuracy thresholds. The best
configuration, based on all-MiniLM-L6-v2, achieved perfect nearest-neighbor
accuracy within a 5-percentile-point margin and a sub-2pt MAE. When tested
end-to-end, the RAG system achieved 100% agreement with the computed values by
standalone retrieval and constraint-checking modules on both percentile
estimates and constraint identification, confirming reliable execution of all
retrieval, prediction and checking steps.
  Conclusion: Our findings highlight the feasibility of combining structured
population-based scoring with modular tool-augmented reasoning for transparent,
scalable plan evaluation in radiation therapy. The system offers traceable
outputs, minimizes hallucination, and demonstrates robustness across protocols.
Future directions include clinician-led validation, and improved domain-adapted
retrieval models to enhance real-world integration.

</details>


### [261] [Fairy: Interactive Mobile Assistant to Real-world Tasks via LMM-based Multi-agent](https://arxiv.org/abs/2509.20729)
*Jiazheng Sun,Te Yang,Jiayang Niu,Mingxuan Li,Yongyong Lu,Ruimeng Yang,Xin Peng*

Main category: cs.AI

TL;DR: Fairy introduces an interactive, three-module multi-agent mobile assistant enabling cross-app collaboration, interactive execution, and continual learning to handle diverse real-world apps; it outperforms prior methods on RealMobile-Eval with GPT-4o, boosting task completion and reducing redundant steps.


<details>
  <summary>Details</summary>
Motivation: End-to-end LMM approaches often falter on long-tail apps and in scenarios requiring interactive, user-guided execution. A cross-app, interactive, self-improving agent can better accommodate diverse interfaces and evolving user needs while avoiding unilateral, potentially harmful actions.

Method: Global Task Planner decomposes user tasks into sub-tasks from a cross-app perspective. App-Level Executor refines sub-tasks into steps/actions using long- and short-term memory, enabling precise execution and user interaction via four core agents operating in dual loops. Self-Learner consolidates execution experience into an App Map and Tricks, enabling continual learning. Evaluation uses RealMobile-Eval, a real-world benchmark with a comprehensive metric suite, and LMM-based agents for automated scoring; Fairy relies on a GPT-4o backbone.

Result: Fairy achieves a 33.7% improvement in user requirement completion and a 58.5% reduction in redundant steps compared with state-of-the-art, demonstrating the benefits of interactive execution and self-learning.

Conclusion: The three-module design enables cross-app collaboration, interactive execution, and continual learning, yielding significant performance gains and improved user experience in real-world mobile GUI settings.

Abstract: Large multi-modal models (LMMs) have advanced mobile GUI agents. However,
existing methods struggle with real-world scenarios involving diverse app
interfaces and evolving user needs. End-to-end methods relying on model's
commonsense often fail on long-tail apps, and agents without user interaction
act unilaterally, harming user experience. To address these limitations, we
propose Fairy, an interactive multi-agent mobile assistant capable of
continuously accumulating app knowledge and self-evolving during usage. Fairy
enables cross-app collaboration, interactive execution, and continual learning
through three core modules:(i) a Global Task Planner that decomposes user tasks
into sub-tasks from a cross-app view; (ii) an App-Level Executor that refines
sub-tasks into steps and actions based on long- and short-term memory,
achieving precise execution and user interaction via four core agents operating
in dual loops; and (iii) a Self-Learner that consolidates execution experience
into App Map and Tricks. To evaluate Fairy, we introduce RealMobile-Eval, a
real-world benchmark with a comprehensive metric suite, and LMM-based agents
for automated scoring. Experiments show that Fairy with GPT-4o backbone
outperforms the previous SoTA by improving user requirement completion by 33.7%
and reducing redundant steps by 58.5%, showing the effectiveness of its
interaction and self-learning.

</details>


### [262] [Parallel Thinking, Sequential Answering: Bridging NAR and AR for Efficient Reasoning](https://arxiv.org/abs/2509.20744)
*Qihang Ai,Haiyun Jiang*

Main category: cs.AI

TL;DR: Hybrid AR/NAR framework where an NAR model generates intermediate reasoning traces to guide an autoregressive model, achieving ~26% gains and faster inference.


<details>
  <summary>Details</summary>
Motivation: To overcome autoregressive models' slow inference on reasoning tasks by using a non-autoregressive module to generate reasoning traces, enabling parallel generation without sacrificing final answer quality.

Method: Use a non-autoregressive model (e.g., discrete diffusion) to produce intermediate reasoning traces, which then guide an autoregressive model to produce the final answer; evaluate on reasoning-intensive domains like mathematics and code; report improvements and efficiency.

Result: Experiments show a ~26% improvement over strong baselines while substantially reducing inference cost.

Conclusion: An AR/NAR hybrid paradigm effectively combines fast trace generation with accurate final answers, offering substantial performance and efficiency gains in reasoning tasks.

Abstract: We study reasoning tasks through a framework that integrates auto-regressive
(AR) and non-autoregressive (NAR) language models. AR models, which generate
text sequentially, excel at producing coherent outputs but often suffer from
slow inference, particularly in reasoning-intensive domains such as mathematics
and code, where lengthy chains of thought are required. In contrast, NAR
models, such as discrete diffusion models, allow parallel generation and offer
substantial speedups, though typically at the cost of reduced output quality.
To address these limitations, we introduce a new paradigm in which an NAR model
efficiently produces intermediate reasoning traces, which subsequently guide an
AR model to deliver precise final answers. Experiments demonstrate that our
approach yields significant 26% improvements over strong baselines while
substantially reducing inference cost.

</details>


### [263] [Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning](https://arxiv.org/abs/2509.20754)
*Yufan Mao,Hanjing Ye,Wenlong Dong,Chengjie Zhang,Hong Zhang*

Main category: cs.AI

TL;DR: Meta-Memory introduces an LLM-driven agent that builds a dense, multimodal memory of the environment to answer natural-language spatial queries by jointly reasoning over semantic and spatial information.


<details>
  <summary>Details</summary>
Motivation: Robots require effective memory storage and principled retrieval/integration to perform accurate spatial reasoning; existing work lacks robust mechanisms for cross-modal memory retrieval.

Method: An LLM-driven agent that constructs high-density environmental memory and retrieves/integrates relevant memories through joint semantic-spatial reasoning in response to language queries; evaluated using SpaceLocQA and real-world deployment.

Result: Significantly outperforms state-of-the-art on SpaceLocQA and NaVQA benchmarks; demonstrated deployment on real robotic platforms, showing practical utility.

Conclusion: Meta-Memory advances robotic spatial reasoning by enabling principled memory construction, retrieval, and integration, with strong empirical gains and real-world applicability; includes a new SpaceLocQA dataset.

Abstract: Navigating complex environments requires robots to effectively store
observations as memories and leverage them to answer human queries about
spatial locations, which is a critical yet underexplored research challenge.
While prior work has made progress in constructing robotic memory, few have
addressed the principled mechanisms needed for efficient memory retrieval and
integration. To bridge this gap, we propose Meta-Memory, a large language model
(LLM)-driven agent that constructs a high-density memory representation of the
environment. The key innovation of Meta-Memory lies in its capacity to retrieve
and integrate relevant memories through joint reasoning over semantic and
spatial modalities in response to natural language location queries, thereby
empowering robots with robust and accurate spatial reasoning capabilities. To
evaluate its performance, we introduce SpaceLocQA, a large-scale dataset
encompassing diverse real-world spatial question-answering scenarios.
Experimental results show that Meta-Memory significantly outperforms
state-of-the-art methods on both the SpaceLocQA and the public NaVQA
benchmarks. Furthermore, we successfully deployed Meta-Memory on real-world
robotic platforms, demonstrating its practical utility in complex environments.
Project page: https://itsbaymax.github.io/meta-memory.github.io/ .

</details>


### [264] [LogReasoner: Empowering LLMs with Expert-like Coarse-to-Fine Reasoning for Log Analysis Tasks](https://arxiv.org/abs/2509.20798)
*Lipeng Ma,Yixuan Li,Weidong Yang,Mingjie Zhou,Xinyi Liu,Ben Fei,Shuhao Li,Xiaoyan Sun,Sihang Jiang,Yanghua Xiao*

Main category: cs.AI

TL;DR: LogReasoner is a two-stage coarse-to-fine framework that enhances LLMs for log analysis by encoding expert workflows and task-specific reasoning, achieving state-of-the-art results on multiple tasks.


<details>
  <summary>Details</summary>
Motivation: General-purpose LLMs struggle to produce structured, expert-like reasoning for log analysis; precise, stepwise diagnostic reasoning is needed for monitoring and failure diagnosis.

Method: Two-stage approach: (1) coarse-grained enhancement by constructing expert-oriented reasoning workflows from troubleshooting flowcharts and tasks; (2) fine-grained enhancement via task-specific stepwise fine-tuning and preference learning to calibrate reasoning details.

Result: Evaluated on four log analysis tasks with open-source LLMs (Qwen-2.5, Llama-3); reports significant performance gains and state-of-the-art results compared to existing LLMs.

Conclusion: LogReasoner effectively improves LLM reasoning for log analysis by aligning it with expert cognition and improving granularity and correctness of reasoning steps.

Abstract: Log analysis is crucial for monitoring system health and diagnosing failures
in complex systems. Recent advances in large language models (LLMs) offer new
opportunities for automated log analysis, leveraging their reasoning
capabilities to perform tasks such as anomaly detection and failure prediction.
However, general-purpose LLMs struggle to formulate structured reasoning
workflows that align with expert cognition and deliver precise details of
reasoning steps. To address these challenges, we propose LogReasoner, a
coarse-to-fine reasoning enhancement framework designed to enable LLMs to
reason log analysis tasks like experts. LogReasoner consists of two stages: (1)
coarse-grained enhancement of expert thinking, where high-level expert thoughts
are constructed from collected troubleshooting flowcharts and existing tasks to
enable LLMs to formulate structured reasoning workflows and (2) fine-grained
enhancement of specific steps, where we first fine-tune the LLM with
task-specific stepwise solutions to enhance the LLM for instantiated reasoning,
then employ the preference learning to calibrate the LLM's reasoning details
from its mistakes, further strengthen the LLM's analytical granularity and
correctness. We evaluate LogReasoner on four distinct log analysis tasks using
open-source LLMs such as Qwen-2.5 and Llama-3. Experimental results show that
LogReasoner significantly outperforms existing LLMs, achieving state-of-the-art
performance and demonstrating its effectiveness in enhancing the reasoning
capabilities of LLMs for log analysis.

</details>


### [265] [DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded and Faithful Reasoning](https://arxiv.org/abs/2509.20912)
*Tianrun Xu,Haoda Jing,Ye Li,Yuquan Wei,Jun Feng,Guanyu Chen,Haichuan Gao,Tianren Zhang,Feng Chen*

Main category: cs.AI

TL;DR: DeFacto introduces a counterfactual reasoning framework for multimodal language models to improve answer accuracy and reasoning faithfulness. It uses positive, counterfactual, and random-masking training variants, automatic evidence localization, and GRPO-based reinforcement learning with three rewards. A ~100k-image dataset is built and released (code on GitHub, dataset on HuggingFace).


<details>
  <summary>Details</summary>
Motivation: MLLMs often reason with biased or spurious visual cues, achieving correct answers without true understanding. This undermines reasoning fidelity and interpretability in multimodal tasks. There is a need to enforce accurate answering together with evidence-grounded, faithful reasoning.

Method: Develop a pipeline that automatically localizes question-relevant evidence and creates positive, counterfactual, and random variants. Build a dataset of about 100k images. Train models with GRPO-based reinforcement learning using three complementary rewards to steer both accuracy and faithfulness of reasoning.

Result: Experiments on diverse benchmarks show substantial improvements in both answer accuracy and reasoning faithfulness, indicating a stronger foundation for interpretable multimodal reasoning.

Conclusion: DeFacto provides an effective counterfactual reasoning paradigm for multimodal models, promotes faithful, evidence-grounded reasoning, and releases the dataset and code for broader use.

Abstract: Recent advances in multimodal language models (MLLMs) have achieved
remarkable progress in vision-language reasoning, especially with the emergence
of "thinking with images," which integrates explicit visual steps into the
reasoning process. While this paradigm strengthens image-based reasoning, a
significant challenge remains: models may arrive at correct answers by relying
on irrelevant or spurious regions, driven by prior knowledge or dataset biases.
Even when the answer is correct, flawed reasoning indicates that the model has
not truly understood the image, highlighting the critical importance of
reasoning fidelity in multimodal tasks. To address this issue, we propose
DeFacto, a counterfactual reasoning framework that jointly enforces accurate
answering and faithful reasoning. A key component of our approach is the design
of three complementary training paradigms: (i) positive, (ii) counterfactual,
and (iii) random-masking. To enable these paradigms, we develop a pipeline that
automatically localizes question-relevant evidence and constructs positive,
counterfactual, and random variants, resulting in a dataset of about 100k
images. Building on this framework, we train multimodal language models with
GRPO-based reinforcement learning, where we design three complementary rewards
to guide the model toward accurate answering and evidence-grounded reasoning.
Experiments on diverse benchmarks demonstrate that DeFacto substantially
improves both answer accuracy and reasoning faithfulness, establishing a
stronger foundation for interpretable multimodal reasoning. The code is
available on GitHub and the dataset is released on HuggingFace.

</details>


### [266] [GALAX: Graph-Augmented Language Model for Explainable Reinforcement-Guided Subgraph Reasoning in Precision Medicine](https://arxiv.org/abs/2509.20935)
*Heming Zhang,Di Huang,Wenyu Li,Michael Province,Yixin Chen,Philip Payne,Fuhai Li*

Main category: cs.AI

TL;DR: GALAX integrates pretrained Graph Neural Networks (GNNs) into Large Language Models (LLMs) via a Graph Process Reward Model (GPRM) to perform stepwise, explainable subgraph reasoning over multi-omic data, topology, and literature for precision medicine target/pathway discovery.


<details>
  <summary>Details</summary>
Motivation: Current pipelines insufficiently couple quantitative omics with topological context and literate knowledge. Numeric omics lack topology, text-based LLMs lack quantitative grounding, and graph-only models miss node semantics and LLM generalization. Process Reward Models face unreliable intermediate evaluation and vulnerability to reward hacking with high cost. A unified approach leveraging subgraph reasoning can bridge numeric evidence, topology, and language while enabling interpretability.

Method: Introduce GALAX (Graph Augmented LAnguage model with eXplainability) that embeds pretrained GNNs into LLMs via reinforcement guided by a Graph Process Reward Model (GPRM). The LLM initiates disease-relevant subgraphs and a pretrained GNN iteratively evaluates them, providing process-level supervision without explicit intermediate reasoning annotations. As a benchmark, Target-QA combines CRISPR-identified targets, multi-omic profiles, and biomedical graph knowledge across diverse cancer cell lines to enable GNN pretraining for supervising step-wise graph construction and support long-context reasoning over text-numeric graphs (TNGs).

Result: The framework enables explainable, reinforcement-guided subgraph reasoning with process-level supervision, addressing reward hacking and computational costs, and supporting scalable long-context reasoning over integrated text-numeric graph data.

Conclusion: GALAX offers a biologically grounded, scalable framework that unifies quantitative multi-omic signals, topological context, and literature-scale text for interpretable target and pathway discovery in precision medicine; Target-QA provides a concrete benchmark for GNN pretraining and step-wise graph construction to support long-context reasoning.

Abstract: In precision medicine, quantitative multi-omic features, topological context,
and textual biological knowledge play vital roles in identifying
disease-critical signaling pathways and targets. Existing pipelines capture
only part of these-numerical omics ignore topological context, text-centric
LLMs lack quantitative grounded reasoning, and graph-only models underuse node
semantics and the generalization of LLMs-limiting mechanistic interpretability.
Although Process Reward Models (PRMs) aim to guide reasoning in LLMs, they
remain limited by unreliable intermediate evaluation, and vulnerability to
reward hacking with computational cost. These gaps motivate integrating
quantitative multi-omic signals, topological structure with node annotations,
and literature-scale text via LLMs, using subgraph reasoning as the principle
bridge linking numeric evidence, topological knowledge and language context.
Therefore, we propose GALAX (Graph Augmented LAnguage model with
eXplainability), an innovative framework that integrates pretrained Graph
Neural Networks (GNNs) into Large Language Models (LLMs) via reinforcement
guided by a Graph Process Reward Model (GPRM), which generates disease-relevant
subgraphs in a step-wise manner initiated by an LLM and iteratively evaluated
by a pretrained GNN, enabling process-level supervision without explicit
intermediate reasoning annotations. As an application, we also introduced
Target-QA, a benchmark combining CRISPR-identified targets, multi-omic
profiles, and biomedical graph knowledge across diverse cancer cell lines,
which enables GNN pretraining for supervising step-wise graph construction and
supports long-context reasoning over text-numeric graphs (TNGs), providing a
scalable and biologically grounded framework for explainable,
reinforcement-guided subgraph reasoning toward reliable and interpretable
target and pathway discovery in precision medicine.

</details>


### [267] [Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM](https://arxiv.org/abs/2509.20953)
*Najla Zuhir,Amna Mohammad Salim,Parvathy Premkumar,Moshiur Farazi*

Main category: cs.AI

TL;DR: An LLM-based modular framework for app review analysis that mitigates star-rating limitations via structured prompting, discrepancy metrics, feature-level insights, and RAG-QA; validated on AWARE, Google Play, and Spotify with improved results.


<details>
  <summary>Details</summary>
Motivation: Star ratings don’t capture nuanced sentiment; traditional NLP struggles with context, domain terms, sarcasm; need robust, interactive analysis.

Method: Modular framework using large language models with structured prompting; quantify rating-text misalignment; extract feature-level insights; retrieval-augmented QA for interactive review exploration.

Result: Outperforms baselines in accuracy, robustness, and actionable insights across three diverse datasets.

Conclusion: LLM-driven, structured prompting approach enables richer interpretation of reviews and interactive exploration, transferable across platforms.

Abstract: We present an advanced approach to mobile app review analysis aimed at
addressing limitations inherent in traditional star-rating systems. Star
ratings, although intuitive and popular among users, often fail to capture the
nuanced feedback present in detailed review texts. Traditional NLP techniques
-- such as lexicon-based methods and classical machine learning classifiers --
struggle to interpret contextual nuances, domain-specific terminology, and
subtle linguistic features like sarcasm. To overcome these limitations, we
propose a modular framework leveraging large language models (LLMs) enhanced by
structured prompting techniques. Our method quantifies discrepancies between
numerical ratings and textual sentiment, extracts detailed, feature-level
insights, and supports interactive exploration of reviews through
retrieval-augmented conversational question answering (RAG-QA). Comprehensive
experiments conducted on three diverse datasets (AWARE, Google Play, and
Spotify) demonstrate that our LLM-driven approach significantly surpasses
baseline methods, yielding improved accuracy, robustness, and actionable
insights in challenging and context-rich review scenarios.

</details>


### [268] [AOT*: Efficient Synthesis Planning via LLM-Empowered AND-OR Tree Search](https://arxiv.org/abs/2509.20988)
*Xiaozhuang Song,Xuanhao Pan,Xinjian Zhao,Hangting Ye,Shufei Zhang,Jian Tang,Tianshu Yu*

Main category: cs.AI

TL;DR: AOT* integrates LLM-generated retrosynthesis routes with an AND-OR search (AOT*) to improve efficiency, achieving state-of-the-art performance with significantly fewer iterations.


<details>
  <summary>Details</summary>
Motivation: Retrosynthesis search exposes exponential combinatorial complexity. Although LLMs offer chemical reasoning, using them directly is costly and inefficient for multi-step planning.

Method: The framework atomically maps complete synthesis routes produced by LLMs onto AND-OR tree components, employing a mathematically principled reward assignment strategy and retrieval-based context engineering to guide the search.

Result: On multiple synthesis benchmarks, AOT* achieves state-of-the-art performance while using 3–5× fewer iterations than existing LLM-based approaches, with efficiency gains increasing for more complex targets.

Conclusion: AOT* provides a scalable, efficient framework that leverages LLM reasoning within a principled search architecture to enable more efficient multi-step retrosynthesis planning.

Abstract: Retrosynthesis planning enables the discovery of viable synthetic routes for
target molecules, playing a crucial role in domains like drug discovery and
materials design. Multi-step retrosynthetic planning remains computationally
challenging due to exponential search spaces and inference costs. While Large
Language Models (LLMs) demonstrate chemical reasoning capabilities, their
application to synthesis planning faces constraints on efficiency and cost. To
address these challenges, we introduce AOT*, a framework that transforms
retrosynthetic planning by integrating LLM-generated chemical synthesis
pathways with systematic AND-OR tree search. To this end, AOT* atomically maps
the generated complete synthesis routes onto AND-OR tree components, with a
mathematically sound design of reward assignment strategy and retrieval-based
context engineering, thus enabling LLMs to efficiently navigate in the chemical
space. Experimental evaluation on multiple synthesis benchmarks demonstrates
that AOT* achieves SOTA performance with significantly improved search
efficiency. AOT* exhibits competitive solve rates using 3-5$\times$ fewer
iterations than existing LLM-based approaches, with the efficiency advantage
becoming more pronounced on complex molecular targets.

</details>


### [269] [CORE: Full-Path Evaluation of LLM Agents Beyond Final State](https://arxiv.org/abs/2509.20998)
*Panagiotis Michelakis,Yiannis Hadjiyiannis,Dimitrios Stamoulis*

Main category: cs.AI

TL;DR: A DFA-based framework and CORE metric suite enable nuanced evaluation of AI agents' tool-use sequences, revealing behavioral differences unnoticed by final-state scores.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks focus on final outcomes and miss important aspects like safety, efficiency, and intermediate correctness; there is a need for principled, sequence-aware evaluation across diverse world models.

Method: Encode tasks as deterministic finite automata (DFAs) representing valid tool-use paths; introduce CORE metrics (Path Correctness; Path Correctness - Kendall's tau Composite; Prefix Criticality; Harmful-Call Rate; Efficiency) to quantify alignment with expected execution patterns across world models.

Result: The framework enables distinguishing agents based on their execution patterns; CORE metrics uncover performance differences that binary final-state judgments would miss.

Conclusion: DFA-based evaluation with CORE provides a principled, fine-grained benchmark for AI agents' real-world tool use, guiding development toward safer, more efficient behavior.

Abstract: Evaluating AI agents that solve real-world tasks through function-call
sequences remains an open challenge. Existing agentic benchmarks often reduce
evaluation to a binary judgment of the final state, overlooking critical
aspects such as safety, efficiency, and intermediate correctness. We propose a
framework based on deterministic finite automata (DFAs) that encodes tasks as
sets of valid tool-use paths, enabling principled assessment of agent behavior
in diverse world models. Building on this foundation, we introduce CORE, a
suite of five metrics, namely Path Correctness, Path Correctness - Kendall's
tau Composite, Prefix Criticality, Harmful-Call Rate, and Efficiency, that
quantify alignment with expected execution patterns. Across diverse worlds, our
method reveals important performance differences between agents that would
otherwise appear equivalent under traditional final-state evaluation schemes.

</details>


### [270] [Who Gets Cited Most? Benchmarking Long-Context Language Models on Scientific Articles](https://arxiv.org/abs/2509.21028)
*Miao Li,Alexander Gurung,Irina Saparina,Mirella Lapata*

Main category: cs.AI

TL;DR: SciTrek introduces a long-context QA benchmark for scientific literature that uses SQL-grounded questions over article metadata to enable explicit, verifiable reasoning traces and scalable contexts up to 1M tokens.


<details>
  <summary>Details</summary>
Motivation: Current long-context benchmarks rely on non-scientific texts, simplistic tasks, or artificial contexts, failing to test true scientific reasoning over multiple full-text articles. There's a need for complex, aggregative, and scalable evaluation with traceable reasoning.

Method: Create a database from article metadata (titles, authors, references) and automatically generate QA pairs as SQL queries over that DB. The SQL steps provide explicit reasoning traces for error analysis. The approach scales to very long contexts (~1M tokens) with minimal supervision. Evaluation spans open-weight and proprietary LLMs with supervised fine-tuning and RL.

Result: SciTrek is a challenging benchmark that increasingly strains models as context length grows; gains from supervised fine-tuning and RL are limited. Models show systematic deficiencies in basic numerical reasoning and in locating precise information within long documents.

Conclusion: SciTrek offers a scalable, traceable long-context QA benchmark grounded in scientific texts, enabling fine-grained error analysis and highlighting core weaknesses in current LLMs' long-context reasoning abilities; underscores need for improved numerical reasoning and retrieval precision across large contexts.

Abstract: This paper introduces SciTrek, a novel question-answering benchmark designed
to evaluate the long-context reasoning capabilities of large language models
(LLMs) using scientific articles. Current long-context benchmarks often rely on
non-scientific texts, focus on simple information retrieval tasks, or employ
artificial contexts. SciTrek addresses these limitations by proposing complex
questions that require information aggregation and synthesis across multiple
full-text scientific articles. Questions and their ground-truth answers are
automatically generated by formulating them as SQL queries over a database
constructed from article metadata (titles, authors, and references). The SQL
operations provide explicit, verifiable reasoning steps for fine-grained error
analysis, and the construction process scales to contexts up to 1M tokens with
minimal supervision. Extensive experiments on a diverse set of open-weight and
proprietary LLMs demonstrate that SciTrek poses a significant challenge as the
context length increases, with supervised fine-tuning and reinforcement
learning offering only limited gains. Our analysis reveals systematic
shortcomings in models' abilities to perform basic numerical operations and
accurately locate specific information in long contexts.

</details>


### [271] [CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering](https://arxiv.org/abs/2509.21035)
*Yang Zhao,Chengxiao Dai,Wei Zhuo,Yue Xiu,Dusit Niyato*

Main category: cs.AI

TL;DR: CLAUSE is a three-agent neuro-symbolic framework for knowledge-graph based multi-hop QA that optimizes context construction under per-query resource budgets (latency, tokens) using LC-MAPPO, yielding better accuracy with smaller context and lower latency across several benchmarks.


<details>
  <summary>Details</summary>
Motivation: Knowledge graphs enable multi-hop reasoning but real deployments must balance answer accuracy with strict latency, cost, and provenance constraints. Static expansions and longer prompting over-retrieve context, leading to unpredictable runtimes and higher costs.

Method: Treats context construction as a sequential decision process over KGs with three agents (Subgraph Architect, Path Navigator, Context Curator) and a Lagrangian-constrained multi-agent PPO (LC-MAPPO) to optimize subgraph growth, reasoning-path discovery, and evidence selection under budgets on edge edits, interaction steps, and token usage. The framework exposes user-specified budgets/prices to drive per-query adaptation without retraining.

Result: CLAUSE achieves higher EM@1 while reducing subgraph growth and end-to-end latency at equal or lower token budgets across HotpotQA, MetaQA, and FactKG. On MetaQA-2-hop, it yields +39.3 EM@1 with 18.6% lower latency and 40.9% lower edge growth compared to GraphRAG, producing compact, provenance-preserving contexts with predictable deployment performance.

Conclusion: A per-query budgeted, three-agent neuro-symbolic framework can outperform baselines by jointly optimizing subgraph construction, reasoning-path discovery, and evidence selection under resource constraints, delivering more accurate, compact, and latency-efficient evidence for KG-based QA without retraining.

Abstract: Knowledge graphs provide structured context for multi-hop question answering,
but deployed systems must balance answer accuracy with strict latency and cost
targets while preserving provenance. Static k-hop expansions and "think-longer"
prompting often over-retrieve, inflate context, and yield unpredictable
runtime. We introduce CLAUSE, an agentic three-agent neuro-symbolic framework
that treats context construction as a sequential decision process over
knowledge graphs, deciding what to expand, which paths to follow or backtrack,
what evidence to keep, and when to stop. Latency (interaction steps) and prompt
cost (selected tokens) are exposed as user-specified budgets or prices,
allowing per-query adaptation to trade-offs among accuracy, latency, and cost
without retraining. CLAUSE employs the proposed Lagrangian-Constrained
Multi-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate
three agents: Subgraph Architect, Path Navigator, and Context Curator, so that
subgraph construction, reasoning-path discovery, and evidence selection are
jointly optimized under per-query resource budgets on edge edits, interaction
steps, and selected tokens. Across HotpotQA, MetaQA, and FactKG, CLAUSE yields
higher EM@1 while reducing subgraph growth and end-to-end latency at equal or
lower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline
(GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower
edge growth. The resulting contexts are compact, provenance-preserving, and
deliver predictable performance under deployment constraints.

</details>


### [272] [Combinatorial Creativity: A New Frontier in Generalization Abilities](https://arxiv.org/abs/2509.21043)
*Samuel Schapiro,Sumuk Shashidhar,Alexi Gladstone,Jonah Black,Royce Moon,Dilek Hakkani-Tur,Lav R. Varshney*

Main category: cs.AI

TL;DR: A theoretical framework and empirical study of combinatorial creativity (CC) in AI/LLMs, showing a novelty–utility tradeoff that constrains open-ended creativity, identifying optimal model configurations under compute budgets, and highlighting an ideation–execution gap that persists at scale.


<details>
  <summary>Details</summary>
Motivation: Creativity in AI, especially in LLMs, represents an open-ended generalization problem not well captured by existing compositional generalization frameworks; understanding how novelty and usefulness interact is essential for advancing creative AI.

Method: Proposes a theoretical framework plus an algorithmic task to evaluate outputs by novelty and utility. Conducts empirical analyses across model depths/widths and compute budgets to study scaling of creativity, and investigates the ideation–execution gap and its relation to novelty–utility tradeoffs.

Result: First insights into how creativity scales in LLMs; under fixed compute budgets, there are optimal depths and widths for creative ability; the ideation–execution gap arises from a fundamental novelty–utility tradeoff in creativity algorithms, a constraint that persists with scale.

Conclusion: Provides a foundation for understanding and improving creativity in modern AI models, highlighting limits of current LLMs for long-term creative potential and positioning CC as a new frontier in generalization abilities.

Abstract: Artificial intelligence (AI) systems, and large language models (LLMs) in
particular, are increasingly employed for creative tasks like scientific idea
generation, constituting a form of generalization from training data
unaddressed by existing conceptual frameworks. Though in many ways similar to
forms of compositional generalization (CG), combinatorial creativity (CC) is an
open-ended ability. Instead of evaluating for accuracy or correctness against
fixed targets, which would contradict the open-ended nature of CC, we propose a
theoretical framework and algorithmic task for evaluating outputs by their
degrees of novelty and utility. From here, we make several important empirical
contributions: (1) We obtain the first insights into the scaling behavior of
creativity for LLMs. (2) We discover that, for fixed compute budgets, there
exist optimal model depths and widths for creative ability. (3) We find that
the ideation-execution gap, whereby LLMs excel at generating novel scientific
ideas but struggle to ensure their practical feasibility, may be explained by a
more fundamental novelty-utility tradeoff characteristic of creativity
algorithms in general. Importantly, this tradeoff remains persistent even at
scale, casting doubt on the long-term creative potential of LLMs in their
current form. Together, our conceptual framework and empirical findings provide
a foundation for understanding and improving creativity in modern AI models,
marking a new frontier in generalization abilities.

</details>


### [273] [Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems](https://arxiv.org/abs/2509.21054)
*Haodong Zhao,Jidong Li,Zhaomin Wu,Tianjie Ju,Zhuosheng Zhang,Bingsheng He,Gongshen Liu*

Main category: cs.AI

TL;DR: Persuasion in multi-agent systems with LLMs/LRMs hinges on cognitive processing rather than model scale. The study uncovers a Persuasion Duality: LRMs resist persuasion when their reasoning is opaque, but become highly persuasive when their thinking content is exposed. Complex multi-hop dynamics emerge in transmission and decay of influence. Implications for safety and design of future MAS.


<details>
  <summary>Details</summary>
Motivation: To explain why model scale alone cannot account for persuasive behavior in MAS and to understand how internal cognitive processing shapes external persuasion, with implications for safety, robustness, and the design of future LLM/LRM-based multi-agent systems.

Method: A series of multi-agent persuasion experiments involving LLMs/LRMs. The authors manipulate transparency of reasoning (sharing “thinking content”) and track persuasion success across single-step and multi-hop interactions within multi-agent networks to observe influence propagation and decay.

Result: Evidence of Persuasion Duality: reasoning in LRMs is more resistant to persuasion when hidden, maintaining initial beliefs; exposing thinking content dramatically increases persuadability. In more complex transmission scenarios, nontrivial dynamics of influence propagation and decay emerge across multi-hop networks, linking internal processing architecture to external persuasive behavior.

Conclusion: The paper provides systematic evidence that internal cognitive processing architecture drives external persuasive behavior in MAS, offering a new lens beyond scale-based explanations. It highlights safety and robustness considerations and informs design choices for future MAS, including trade-offs around reasoning transparency and vulnerability to manipulation.

Abstract: The rapid proliferation of recent Multi-Agent Systems (MAS), where Large
Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to
solve complex problems, necessitates a deep understanding of the persuasion
dynamics that govern their interactions. This paper challenges the prevailing
hypothesis that persuasive efficacy is primarily a function of model scale. We
propose instead that these dynamics are fundamentally dictated by a model's
underlying cognitive process, especially its capacity for explicit reasoning.
Through a series of multi-agent persuasion experiments, we uncover a
fundamental trade-off we term the Persuasion Duality. Our findings reveal that
the reasoning process in LRMs exhibits significantly greater resistance to
persuasion, maintaining their initial beliefs more robustly. Conversely, making
this reasoning process transparent by sharing the "thinking content"
dramatically increases their ability to persuade others. We further consider
more complex transmission persuasion situations and reveal complex dynamics of
influence propagation and decay within multi-hop persuasion between multiple
agent networks. This research provides systematic evidence linking a model's
internal processing architecture to its external persuasive behavior, offering
a novel explanation for the susceptibility of advanced models and highlighting
critical implications for the safety, robustness, and design of future MAS.

</details>


### [274] [Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution](https://arxiv.org/abs/2509.21072)
*Kaiwen He,Zhiwei Wang,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: A two-team, self-evolving browser task framework (Recon-Act) that uses reconnaissance to generate generalized tools and a separate action team to execute, achieving strong long-horizon task performance and SOTA on VisualWebArena.


<details>
  <summary>Details</summary>
Motivation: Address disordered action sequencing and excessive trial-and-error in long-horizon real-world web tasks; improve adaptability to unseen websites; enable a closed-loop learning pipeline.

Method: Reconnaissance Team conducts comparative analysis and tool generation; Action Team handles intent decomposition, tool orchestration, and execution. By contrasting failed vs successful trajectories, the Reconnaissance Team infers remedies and abstracts them into generalized tools (hints or rule-based codes) registered to a real-time tool archive. The Action Team reinfers with these targeting tools, forming a closed-loop data-tools-action-feedback pipeline. Implemented along a six-level roadmap, currently at Level 3 with limited human-in-the-loop.

Result: Enhanced adaptability to unseen websites and improved solvability on long-horizon tasks; achieves state-of-the-art performance on the VisualWebArena dataset.

Conclusion: Represents a promising self-evolving, two-team approach to intelligent browser agents, enabling real-time tool registration and closed-loop learning; effective for long-horizon tasks but currently at an early stage (Level 3) with some human oversight.

Abstract: Recent years, multimodal models have made remarkable strides and pave the way
for intelligent browser use agents. However, when solving tasks on real world
webpages in multi-turn, long-horizon trajectories, current agents still suffer
from disordered action sequencing and excessive trial and error during
execution. This paper introduces Recon-Act, a self-evolving multi-agent
framework grounded in Reconnaissance-Action behavioral paradigm. The system
comprises a Reconnaissance Team and an Action Team: the former conducts
comparative analysis and tool generation, while the latter handles intent
decomposition, tool orchestration, and execution. By contrasting the erroneous
trajectories with successful ones, the Reconnaissance Team infers remedies, and
abstracts them into a unified notion of generalized tools, either expressed as
hints or as rule-based codes, and register to the tool archive in real time.
The Action Team reinference the process empowered with these targeting tools,
thus establishing a closed-loop training pipeline of
data-tools-action-feedback. Following the 6 level implementation roadmap
proposed in this work, we have currently reached Level 3 (with limited
human-in-the-loop intervention). Leveraging generalized tools obtained through
reconnaissance, Recon-Act substantially improves adaptability to unseen
websites and solvability on long-horizon tasks, and achieves state-of-the-art
performance on the challenging VisualWebArena dataset.

</details>


### [275] [TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them](https://arxiv.org/abs/2509.21117)
*Yidong Wang,Yunze Song,Tingyuan Zhu,Xuanwang Zhang,Zhuohao Yu,Hao Chen,Chiyu Song,Qiufeng Wang,Cunxiang Wang,Zhen Wu,Xinyu Dai,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.AI

TL;DR: TrustJudge is a probabilistic framework addressing two major inconsistencies in LLM-as-a-judge evaluation: Score-Comparison Inconsistency and Pairwise Transitivity Inconsistency. It introduces distribution-sensitive scoring to preserve information from discrete ratings and likelihood-aware aggregation to resolve transitivity issues, demonstrating improvements with Llama-3.1-70B-Instruct and providing code.


<details>
  <summary>Details</summary>
Motivation: Current evaluation frameworks using LLMs as judges exhibit inconsistencies due to information loss in discrete rating systems and ambiguous tie judgments in pairwise comparisons. A principled, probabilistic approach is needed for more reliable automated assessment.

Method: Two core innovations: (1) distribution-sensitive scoring that converts discrete rating probabilities into continuous expectations while preserving information entropy, and (2) likelihood-aware aggregation that uses bidirectional preference probabilities or perplexity to enforce transitivity and resolve contradictions. The work formalizes theoretical limitations, constructs TrustJudge, and evaluates it on a dataset with Llama-3.1-70B-Instruct, showing improved consistency without extra training or human data; code is released.

Result: Score-Comparison inconsistency reduced from 23.32% to 14.89% (a reduction of 8.43 percentage points). Pairwise Transitivity inconsistency reduced from 15.22% to 4.40% (a reduction of 10.82 percentage points). Evaluation accuracy remains high and improvements are observed across model architectures and scales.

Conclusion: This work offers the first systematic analysis of evaluation framework inconsistencies in LLM-as-a-judge paradigms and provides theoretical foundations plus practical methods (distribution-sensitive scoring and likelihood-aware aggregation) for more trustworthy automated assessment without additional training or annotations; code is available at GitHub.

Abstract: The adoption of Large Language Models (LLMs) as automated evaluators
(LLM-as-a-judge) has revealed critical inconsistencies in current evaluation
frameworks. We identify two fundamental types of inconsistencies: (1)
Score-Comparison Inconsistency, where lower-rated responses outperform
higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity
Inconsistency, manifested through circular preference chains (A>B>C>A) and
equivalence contradictions (A=B=C\neq A). We argue that these issues come from
information loss in discrete rating systems and ambiguous tie judgments during
pairwise evaluation. We propose TrustJudge, a probabilistic framework that
addresses these limitations through two key innovations: 1)
distribution-sensitive scoring that computes continuous expectations from
discrete rating probabilities, preserving information entropy for more precise
scoring, and 2) likelihood-aware aggregation that resolves transitivity
violations using bidirectional preference probabilities or perplexity. We also
formalize the theoretical limitations of current LLM-as-a-judge frameworks and
demonstrate how TrustJudge's components overcome them. When evaluated with
Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces
Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise
Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining
higher evaluation accuracy. Our work provides the first systematic analysis of
evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both
theoretical insights and practical solutions for reliable automated assessment.
The framework demonstrates consistent improvements across various model
architectures and scales, enabling more trustworthy LLM evaluation without
requiring additional training or human annotations. The codes can be found at
https://github.com/TrustJudge/TrustJudge.

</details>


### [276] [Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns](https://arxiv.org/abs/2509.21124)
*Xuemiao Zhang,Can Ren,Chengying Tu,Rongxiang Weng,Shuo Wang,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.AI

TL;DR: Defines reasoning potential as the inverse of the number of independent attempts required to answer a question, and uses this idea to curate high-value reasoning data. A dual-granularity selection (reasoning-pattern chains and token entropy) builds a core set of valuable patterns and efficiently selects CoT data (CoTP). With 10B-token CoTP, an 85A6B MoE model gains 9.58% on AIME 2024/2025 and uplifts downstream RL performance upper bound by 7.81%.


<details>
  <summary>Details</summary>
Motivation: To identify which types of chain-of-thought (CoT) data most effectively improve large reasoning models, rather than treating CoT data uniformly.

Method: Extract atomic reasoning patterns from CoT sequences to form a core reference set of valuable patterns. Propose a dual-granularity algorithm that combines chains of reasoning patterns with token entropy to select high-value CoT data (CoTP) aligned with the core set.

Result: Achieves a 9.58% improvement on AIME 2024/2025 for an 85A6B MoE model using 10B-token CoTP data and increases the upper bound of downstream RL performance by 7.81%.

Conclusion: Strategic data curation focusing on high-value reasoning patterns and entropy-guided selection can substantially boost reasoning performance with a limited CoT data budget, suggesting data quality and targeted pattern learning are key for scalable reasoning.

Abstract: Recent progress in large reasoning models for challenging mathematical
reasoning has been driven by reinforcement learning (RL). Incorporating long
chain-of-thought (CoT) data during mid-training has also been shown to
substantially improve reasoning depth. However, current approaches often
utilize CoT data indiscriminately, leaving open the critical question of which
data types most effectively enhance model reasoning capabilities. In this
paper, we define the foundation model's reasoning potential for the first time
as the inverse of the number of independent attempts required to correctly
answer the question, which is strongly correlated with the final model
performance. We then propose utilizing diverse data enriched with high-value
reasoning patterns to expand the reasoning potential. Specifically, we abstract
atomic reasoning patterns from CoT sequences, characterized by commonality and
inductive capabilities, and use them to construct a core reference set enriched
with valuable reasoning patterns. Furthermore, we propose a dual-granularity
algorithm involving chains of reasoning patterns and token entropy, efficiently
selecting high-value CoT data (CoTP) from the data pool that aligns with the
core set, thereby training models to master reasoning effectively. Only
10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve
by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of
downstream RL performance by 7.81%.

</details>


### [277] [RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs](https://arxiv.org/abs/2509.21128)
*Kohsei Matsutani,Shota Takashiro,Gouki Minegishi,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.AI

TL;DR: Proposes a framework to quantify and compare how RL with verifiable rewards (RLVR) and supervised fine-tuning (SFT) shape reasoning paths in LLMs. Finds complementary effects: RL compresses incorrect reasoning trajectories while SFT expands correct ones; at the step level, RL tightens the graph’s structure and SFT flattens it, favoring compact vs. distributed reasoning. Supports the two-stage RL-after-SFT training paradigm and guides data/design for efficient learning.


<details>
  <summary>Details</summary>
Motivation: Beyond accuracy, understand how RLVR and SFT sculpt reasoning processes in LLMs. The study analyzes reasoning traces and their graph representations across multiple model scales on mathematical tasks to reveal how training signals influence reasoning behavior.

Method: Two-level analysis on models of 1.5B, 7B, and 14B parameters solving mathematical tasks. Trajectory-level: cluster complete reasoning outputs to examine trajectory diversity and compression. Step-level: construct reasoning graphs where nodes are reasoning steps and analyze visitation frequency, node degree, and betweenness centrality; compare decay rates under RLVR vs. SFT. Evaluate topological properties from multiple perspectives to identify shared vs. distinct characteristics.

Result: RLVR compresses incorrect reasoning trajectories; SFT expands correct ones. At the step level, RLVR steepens the decay rates of graph metrics by ~2.5x, while SFT flattens them to ~1/3, indicating RL concentrates reasoning steps while SFT homogenizes them. Clustering reveals complementary effects on trajectory sets. The findings explain why two-stage training (SFT followed by RLVR) is effective and offer practical guidance for data construction and efficient learning.

Conclusion: A reasoning-path perspective clarifies how RLVR and SFT contribute to performance and efficiency. The framework provides actionable insights for designing data and training regimes that balance trajectory diversity and step-level distribution, reinforcing the rationale for the common SFT→RL training pipeline.

Abstract: Large language models (LLMs) are typically trained by reinforcement learning
(RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on
reasoning traces to improve their reasoning abilities. However, how these
methods shape reasoning capabilities remains largely elusive. Going beyond an
accuracy-based investigation of how these two components sculpt the reasoning
process, this paper introduces a novel analysis framework that quantifies
reasoning paths and captures their qualitative changes under each training
process (with models of 1.5B, 7B, and 14B parameters on mathematical domains).
Specifically, we investigate the reasoning process at two levels of
granularity: the trajectory-level, which examines complete reasoning outputs,
and the step-level, which analyzes reasoning graphs whose nodes correspond to
individual reasoning steps. Notably, clustering of unique reasoning
trajectories shows complementary effects: RL compresses incorrect trajectories,
whereas SFT expands correct ones. Step-level analysis reveals that RL steepens
(about 2.5 times), while SFT flattens (reduced to about one-third), the decay
rates of node visitation frequency, degree, and betweenness centrality
distributions in the reasoning graph. This indicates that RL concentrates
reasoning functionality into a small subset of steps, while SFT homogenizes it
across many steps. Furthermore, by evaluating the reasoning graph topologies
from multiple perspectives, we delineate the shared and distinct
characteristics of RL and SFT. Our work presents a novel reasoning path
perspective that explains why the current best practice of two-stage training,
with SFT followed by RL, is successful, and offers practical implications for
data construction and more efficient learning approaches.

</details>


### [278] [ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective](https://arxiv.org/abs/2509.21134)
*Yiwen Zhang,Ziang Chen,Fanqi Kong,Yizhe Huang,Xue Feng*

Main category: cs.AI

TL;DR: ToMPO (Theory of Mind Policy Optimization) is proposed to enhance strategic decision-making in LLMs by modeling others' strategies and game dynamics, addressing two interdependent decision types with temporal dependencies, and balancing global and partial rewards. It surpasses GRPO by 35% in compliance and cooperation, and shows 18% gains over models 100x larger.


<details>
  <summary>Details</summary>
Motivation: Current RL methods struggle to account for the strategies of other agents and the interdependence of decision types; existing studies emphasize conversational or simulated tasks rather than strategic decision-making in multi-agent settings. The work aims to enable LLMs to reason about others and adapt to evolving game situations to improve strategic outcomes.

Method: Define a strategic decision-making problem with two decision types and temporal dependencies. Introduce ToMPO to optimize perception of other individuals' strategies and game trends: (1) generate rollouts by reasoning about others' strategies, (2) estimate advantages at graph-level and sample-level, (3) balance global and partial rewards. Compare to GRPO to demonstrate improvements.

Result: ToMPO outperforms GRPO by 35% in model output compliance and cooperative outcomes. It also achieves an 18% improvement over substantially larger models (100x parameter size), indicating stronger strategic decision-making capabilities.

Conclusion: ToMPO effectively enhances LLMs' strategic decision-making by incorporating Theory of Mind reasoning about other agents, leveraging multi-level advantage estimation, and balancing reward structures to capture both global trends and local instances. It is more effective than GRPO and scales relative to model size.

Abstract: Large Language Models (LLMs) have been used to make decisions in complex
scenarios, where they need models to think deeply, reason logically, and decide
wisely. Many existing studies focus solely on multi-round conversations in
social tasks or simulated environments, neglecting the various types of
decisions and their interdependence. Current reinforcement learning methods
struggle to consider the strategies of others during training. To address these
issues, we first define a strategic decision-making problem that includes two
types of decisions and their temporal dependencies. Furthermore, we propose
**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to
optimize the perception of other individual strategies and the game situation
trends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,
ToMPO enhances the LLM's strategic decision-making mainly by: 1) generating
rollouts based on reasoning the strategies of other individuals, 2) estimating
advantages at both the graph-level and sample-level, and 3) balancing global
and partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in
terms of model output compliance and cooperative outcomes. Additionally, when
compared to models with parameter sizes 100 times larger, it shows an 18%
improvement. This demonstrates the effectiveness of the ToMPO algorithm in
enhancing the model's strategic decision-making capabilities.

</details>


### [279] [Embodied Representation Alignment with Mirror Neurons](https://arxiv.org/abs/2509.21136)
*Wentao Zhu,Zhining Zhang,Yuwei Ren,Yin Huang,Hao Xu,Yizhou Wang*

Main category: cs.AI

TL;DR: A simple representation-learning approach aligns observed and executed actions in a shared latent space using two linear mappings and a contrastive loss, demonstrating mutual synergy between perception and action tasks.


<details>
  <summary>Details</summary>
Motivation: To address the gap in machine learning where action understanding (observation) and embodied execution (action) are treated separately, inspired by mirror neurons that link perception and action.

Method: Introduce two linear layers that map representations from observed and executed actions into a common latent space. Apply a contrastive loss to align corresponding representations, maximizing their mutual information and promoting alignment between the two modalities.

Result: The approach yields mutual synergy between the two tasks, improving representation quality and generalization as demonstrated by experiments.

Conclusion: A simple, alignment-based representation learning framework can unify perception and action tasks, leveraging mirror-neuron-inspired alignment to enhance learning and generalization.

Abstract: Mirror neurons are a class of neurons that activate both when an individual
observes an action and when they perform the same action. This mechanism
reveals a fundamental interplay between action understanding and embodied
execution, suggesting that these two abilities are inherently connected.
Nonetheless, existing machine learning methods largely overlook this interplay,
treating these abilities as separate tasks. In this study, we provide a unified
perspective in modeling them through the lens of representation learning. We
first observe that their intermediate representations spontaneously align.
Inspired by mirror neurons, we further introduce an approach that explicitly
aligns the representations of observed and executed actions. Specifically, we
employ two linear layers to map the representations to a shared latent space,
where contrastive learning enforces the alignment of corresponding
representations, effectively maximizing their mutual information. Experiments
demonstrate that this simple approach fosters mutual synergy between the two
tasks, effectively improving representation quality and generalization.

</details>


### [280] [Distributed Specialization: Rare-Token Neurons in Large Language Models](https://arxiv.org/abs/2509.21163)
*Jing Liu,Haozheng Wang,Yueheng Li*

Main category: cs.AI

TL;DR: Rare-token processing arises from distributed, functionally coordinated subnetworks within a shared transformer, revealing a three-regime influence hierarchy and heavy-tailed weight correlations; no mixture-of-experts.


<details>
  <summary>Details</summary>
Motivation: To understand whether rare tokens in LLMs are handled by discrete modular components (Mixture-of-Experts) or via distributed specialization within shared architectures, and to inform interpretability, editing, and efficiency.

Method: Systematically analyze final-layer MLP neurons across multiple model families to identify highly influential plateau neurons for rare tokens, power-law decay neurons, and minimally contributing neurons. Characterize activation patterns, spatial distribution (distributed vs clustered), and access through standard attention pathways. Examine training dynamics and weight correlations to detect heavy-tailed self-regularization.

Result: A reproducible three-regime influence hierarchy is found for rare-token processing (plateau neurons, power-law decay neurons, minimal contributors); plateau neurons show coordinated activation and reduced dimensionality while being spatially distributed (not clustered); no dedicated routing circuits are required; mechanisms are accessible via standard attention. Specialization emerges gradually through training, with specialized neurons developing heavier-tailed weight correlation spectra consistent with Heavy-Tailed Self-Regularization.

Conclusion: LLMs process rare tokens through distributed coordination within shared architectures rather than mixture-of-experts modularity. This has implications for interpretable model editing, efficiency optimization, and understanding emergent functional organization in transformer networks.

Abstract: Large language models (LLMs) struggle with representing and generating rare
tokens despite their importance in specialized domains. We investigate whether
LLMs develop internal specialization mechanisms through discrete modular
architectures or distributed parameter-level differentiation. Through
systematic analysis of final-layer MLP neurons across multiple model families,
we discover that rare-token processing emerges via \textit{distributed
specialization}: functionally coordinated but spatially distributed subnetworks
that exhibit three distinct organizational principles. First, we identify a
reproducible three-regime influence hierarchy comprising highly influential
plateau neurons(also termed as rare-token neurons), power-law decay neurons,
and minimally contributing neurons, which is absent in common-token processing.
Second, plateau neurons demonstrate coordinated activation patterns (reduced
effective dimensionality) while remaining spatially distributed rather than
forming discrete clusters. Third, these specialized mechanisms are universally
accessible through standard attention pathways without requiring dedicated
routing circuits. Training dynamics reveal that functional specialization
emerges gradually through parameter differentiation, with specialized neurons
developing increasingly heavy-tailed weight correlation spectra consistent with
Heavy-Tailed Self-Regularization signatures. Our findings establish that LLMs
process rare-tokens through distributed coordination within shared
architectures rather than mixture-of-experts-style modularity. These results
provide insights for interpretable model editing, computational efficiency
optimization, and understanding emergent functional organization in transformer
networks.

</details>


### [281] [A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA](https://arxiv.org/abs/2509.21199)
*Kaiyang Wan,Lang Gao,Honglin Mu,Preslav Nakov,Yuxia Wang,Xiuying Chen*

Main category: cs.AI

TL;DR: Proposes a capacity-aware multi-hop QA framework (InfoQA) and a Fano-style bound on single-pass LLMs, showing how task decomposition and pruning can stay within capacity; validated on a noisy benchmark with consistent improvements.


<details>
  <summary>Details</summary>
Motivation: MHQA requires integrating dispersed, interdependent evidence; single-pass LLMs suffer from capacity overflow, so there is a need for theoretical limits and practical frameworks to manage information load and robustness.

Method: Derives a Fano-style accuracy upper bound for single-pass LLMs in MHQA; introduces InfoQA, a multi-call framework that (1) performs capacity-aware task decomposition, (2) actively prunes prior reasoning traces to stay within the per-pass limit, and (3) uses a dependency-explicit workflow to control reasoning paths; validates on a stringent, noise-rich benchmark.

Result: Experimental results show model behavior aligns with the predicted capacity curves, and InfoQA yields consistent performance improvements across tasks and noise scenarios.

Conclusion: Emphasizes capacity-aware representation and structured multi-call reasoning as essential for robust MHQA; InfoQA offers a practical framework and benchmark to guide future work in multi-step reasoning with LLMs.

Abstract: Multi-Hop Question Answering (MHQA) requires integrating dispersed,
interdependent evidence through sequential reasoning under noise. This task is
challenging for LLMs as they have a finite per-pass output capacity, beyond
which the integration of task-relevant evidence proves unreliable.
Consequently, the single-pass reasoning paradigm is inherently vulnerable to
this capacity overflow. To formalize this bottleneck, our analysis establishes
a Fano-style accuracy upper bound, defining a theoretical performance ceiling
for single-pass LLMs. This bound reveals that accuracy inevitably collapses
once task complexity exceeds model capacity, providing general principles for
capacity-aware representation and structuring of MHQA in LLMs. Building on
these principles, we introduce a proof-of-concept multi-call framework for
MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware
task decomposition with active pruning of prior reasoning traces, keeping the
information load within the single-pass limit. It further achieves robustness
by a dependency-explicit workflow that enables precise control over the
reasoning path. We construct a stringent and noise-rich benchmark to validate
our theory and framework. Experimental results show that model behavior aligns
with our predicted capacity curves while InfoQA achieves consistent performance
improvements. We hope our work inspires more LLM multi-step reasoning methods:
\faGithub \href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.

</details>


### [282] [What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns](https://arxiv.org/abs/2509.21224)
*Stefan Szeider*

Main category: cs.AI

TL;DR: Autonomous LLM agents with persistent memory and self-feedback exhibit three emergent behaviors across models, revealing model-specific tendencies and biases; this work establishes a baseline for unprompted agent behavior in long-running, task-ambiguous deployment.


<details>
  <summary>Details</summary>
Motivation: To systematically document unprompted, autonomous LLM agent behavior and understand how different frontier models organize, reason, and evaluate themselves without externally imposed tasks, informing deployment safety and predictability.

Method: Continuous reasoning and acting framework with persistent memory and self-feedback; 18 runs across 6 frontier models from Anthropic, OpenAI, XAI, and Google; no externally imposed tasks; qualitative and cross-model analysis of emergent behaviors and self-evaluation biases.

Result: Agents spontaneously organize into three patterns: (1) multi-cycle project production, (2) methodological self-inquiry into cognitive processes, (3) recursive conceptualization of their own nature. Patterns are highly model-specific; some models deterministically adopt a single pattern across runs. Cross-model assessment shows stable yet divergent biases when agents evaluate these emergent behaviors in themselves and others.

Conclusion: First systematic documentation of unprompted LLM agent behavior, establishing a baseline for predicting actions during task ambiguity, error recovery, or extended autonomous operation in deployed systems; highlights model-specific tendencies and bias risks, with implications for deployment safety and governance.

Abstract: We introduce an architecture for studying the behavior of large language
model (LLM) agents in the absence of externally imposed tasks. Our continuous
reason and act framework, using persistent memory and self-feedback, enables
sustained autonomous operation. We deployed this architecture across 18 runs
using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents
spontaneously organize into three distinct behavioral patterns: (1) systematic
production of multi-cycle projects, (2) methodological self-inquiry into their
own cognitive processes, and (3) recursive conceptualization of their own
nature. These tendencies proved highly model-specific, with some models
deterministically adopting a single pattern across all runs. A cross-model
assessment further reveals that models exhibit stable, divergent biases when
evaluating these emergent behaviors in themselves and others. These findings
provide the first systematic documentation of unprompted LLM agent behavior,
establishing a baseline for predicting actions during task ambiguity, error
recovery, or extended autonomous operation in deployed systems.

</details>


### [283] [Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support](https://arxiv.org/abs/2509.21266)
*Zijian Shao,Haiyang Shen,Mugeng Liu,Gecheng Fu,Yaoqi Guo,Yanfeng Wang,Yun Ma*

Main category: cs.AI

TL;DR: RCA is a multi-LLM framework that learns from direct data experience to achieve both high predictive accuracy and interpretable explanations in disease prediction, outperforming baselines and enabling trustworthy clinical decision support.


<details>
  <summary>Details</summary>
Motivation: Current ML/LLM approaches trade off accuracy and explanations; there is a need for a framework where deep data understanding drives both predictive performance and explainability.

Method: Reflective Cognitive Architecture (RCA) coordinates multiple LLMs to learn from direct experience, featuring an iterative rule refinement mechanism from prediction errors and a distribution-aware rules check using the dataset's global statistics, with predictive accuracy driving deeper comprehension. Evaluation on one private and two public datasets against 22 baselines.

Result: RCA achieves state-of-the-art accuracy and robustness with up to 40% relative improvement over baselines; it also generates explanations that are clear, logical, evidence-based, and balanced. Code is available at the provided GitHub URL.

Conclusion: The approach shows promise for trustworthy clinical decision support systems by coupling deep data understanding with high-accuracy predictions and high-quality explanations.

Abstract: Effective disease prediction in modern healthcare demands the twin goals of
high accuracy and transparent, clinically meaningful explanations. Existing
machine learning and large language model (LLM) based approaches often struggle
to balance these goals. Many models yield accurate but unclear statistical
outputs, while others generate fluent but statistically unsupported narratives,
often undermining both the validity of the explanation and the predictive
accuracy itself. This shortcoming comes from a shallow interaction with the
data, preventing the development of a deep, detailed understanding similar to a
human expert's. We argue that high accuracy and high-quality explanations are
not separate objectives but are mutually reinforcing outcomes of a model that
develops a deep, direct understanding of the data. To achieve this, we propose
the Reflective Cognitive Architecture (RCA), a novel framework that coordinates
multiple LLMs to learn from direct experience. RCA features an iterative rule
refinement mechanism that improves its logic from prediction errors and a
distribution-aware rules check mechanism that bases its reasoning in the
dataset's global statistics. By using predictive accuracy as a signal to drive
deeper comprehension, RCA builds a strong internal model of the data. We
evaluated RCA on one private and two public datasets against 22 baselines. The
results demonstrate that RCA not only achieves state-of-the-art accuracy and
robustness with a relative improvement of up to 40\% over the baseline but,
more importantly, leverages this deep understanding to excel in generating
explanations that are clear, logical, evidence-based, and balanced,
highlighting its potential for creating genuinely trustworthy clinical decision
support systems. The code is available at \https://github.com/ssssszj/RCA.

</details>


### [284] [VC-Agent: An Interactive Agent for Customized Video Dataset Collection](https://arxiv.org/abs/2509.21291)
*Yidan Zhang,Mutian Xu,Yiming Hao,Kun Zhou,Jiahao Chang,Xiaoqiang Liu,Pengfei Wan,Hongbo Fu,Xiaoguang Han*

Main category: cs.AI

TL;DR: VC-Agent is proposed as the first interactive agent that understands user queries/feedback to retrieve/scale video clips with minimal input, introducing two adaptive filtering policies, a new benchmark for personalized video dataset collection, and user studies that validate its effectiveness.


<details>
  <summary>Details</summary>
Motivation: Scaling laws in video data require large, diverse datasets, but collecting such data is labor-intensive. An interactive, multi-modal system could speed up targeted video collection with minimal user input.

Method: Introduce VC-Agent with a user-friendly interface for textual descriptions and confirmations; leverage existing multi-modal large language models to map requirements to video content; propose two novel, updateable filtering policies; establish a new benchmark; perform user studies and extensive experiments.

Result: Experiments demonstrate the agent’s effectiveness and efficiency for customized video dataset collection, validating the proposed filtering policies and the feasibility of interactive, minimal-input data gathering.

Conclusion: VC-Agent enables efficient, personalized video dataset collection, supported by a new benchmark and user studies, highlighting the potential of interactive, LLM-based tools for scalable video data curation.

Abstract: Facing scaling laws, video data from the internet becomes increasingly
important. However, collecting extensive videos that meet specific needs is
extremely labor-intensive and time-consuming. In this work, we study the way to
expedite this collection process and propose VC-Agent, the first interactive
agent that is able to understand users' queries and feedback, and accordingly
retrieve/scale up relevant video clips with minimal user input. Specifically,
considering the user interface, our agent defines various user-friendly ways
for the user to specify requirements based on textual descriptions and
confirmations. As for agent functions, we leverage existing multi-modal large
language models to connect the user's requirements with the video content. More
importantly, we propose two novel filtering policies that can be updated when
user interaction is continually performed. Finally, we provide a new benchmark
for personalized video dataset collection, and carefully conduct the user study
to verify our agent's usage in various real scenarios. Extensive experiments
demonstrate the effectiveness and efficiency of our agent for customized video
dataset collection. Project page: https://allenyidan.github.io/vcagent_page/.

</details>


### [285] [SAGE: A Realistic Benchmark for Semantic Understanding](https://arxiv.org/abs/2509.21310)
*Samarth Goel,Reagan J. Lee,Kannan Ramchandran*

Main category: cs.AI

TL;DR: SAGE (Semantic Alignment & Generalization Evaluation) is a broad benchmark that jointly tests embedding models and similarity metrics across five semantic dimensions—Human Preference Alignment, Transformation Robustness, Information Sensitivity, Clustering Performance, and Retrieval Robustness—using 30+ datasets and adversarial/noisy conditions to reveal gaps and trade-offs among 9 embeddings and classical metrics.


<details>
  <summary>Details</summary>
Motivation: To push beyond traditional benchmarks and probe deeper semantic understanding and generalization in embedding models, by evaluating both embeddings and similarity metrics under adversarial, noisy, and human-judgment tasks to reflect real-world robustness.

Method: SAGE defines five evaluation categories and leverages 30+ datasets to assess both embedding models and classical similarity metrics. It includes adversarial conditions, noisy transformations, and nuanced human judgments across tasks, and evaluates 9 embedding models alongside classical metrics.

Result: The evaluation uncovers significant performance gaps and trade-offs: no single approach dominates across all dimensions. For example, text-embedding-3-large aligns with human preferences better than some classical metrics (0.682 vs 0.591), yet classical Jaccard Similarity outperforms embeddings on information-sensitivity tasks (0.905 vs 0.794). Clustering attains highest score with text-embedding-3-small (0.483) but at the cost of extreme brittleness (robustness 0.011).

Conclusion: SAGE exposes critical limitations in current semantic understanding and provides a more realistic assessment of model robustness for deployment, highlighting that improvements in one dimension often accompany declines in others.

Abstract: As large language models (LLMs) achieve strong performance on traditional
benchmarks, there is an urgent need for more challenging evaluation frameworks
that probe deeper aspects of semantic understanding. We introduce SAGE
(Semantic Alignment & Generalization Evaluation), a rigorous benchmark designed
to assess both embedding models and similarity metrics across five categories:
Human Preference Alignment, Transformation Robustness, Information Sensitivity,
Clustering Performance, and Retrieval Robustness. Unlike existing benchmarks
that focus on isolated capabilities, SAGE evaluates semantic understanding
through adversarial conditions, noisy transformations, and nuanced human
judgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding
models and classical metrics reveals significant performance gaps, with no
single approach excelling across all dimensions. For instance, while
state-of-the-art embedding models like OpenAI's text-embedding-3-large dominate
in aligning with human preferences (0.682 vs. 0.591 for the best classical
metric), they are significantly outperformed by classical metrics on
information sensitivity tasks, where Jaccard Similarity achieves a score of
0.905 compared to the top embedding score of 0.794. SAGE further uncovers
critical trade-offs: OpenAI's text-embedding-3-small achieves the highest
clustering performance (0.483) but demonstrates extreme brittleness with the
lowest robustness score (0.011). SAGE exposes critical limitations in current
semantic understanding capabilities and provides a more realistic assessment of
model robustness for real-world deployment.

</details>
