<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 183]
- [cs.LG](#cs.LG) [Total: 184]
- [cs.AI](#cs.AI) [Total: 55]
- [cs.RO](#cs.RO) [Total: 62]
- [cs.CG](#cs.CG) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Generative human motion mimicking through feature extraction in denoising diffusion settings](https://arxiv.org/abs/2511.00011)
*Alexander Okupnik,Johannes Schneider,Kyriakos Flouris*

Main category: cs.CV

TL;DR: A diffusion-based MoCap dance system that partly mimics and creatively enhances an incoming movement sequence using single-person data and high-level features, achieving temporally coherent, diverse, and realistic motion without low-level joint-level human-human data.


<details>
  <summary>Details</summary>
Motivation: To explore embodied human-AI interaction through dance by combining motion capture data with diffusion-based generation, addressing the lack of embodied interaction in large language models and enabling a responsive AI dance partner.

Method: A dual-diffusion-model framework that integrates motion inpainting and motion style transfer. It uses single-person MoCap data with high-level features to generate temporally coherent motion that partially mimics and creatively enhances an incoming sequence, without relying on low-level human-human interaction data.

Result: Quantitative evaluation shows convergence of the generated motion feature distribution to that of a test set simulating a human performer. The generated dances are diverse and realistic, representing initial steps toward AI-assisted creative dancing.

Conclusion: The work demonstrates feasibility of using diffusion-based generation on single-person MoCap data for embodied, responsive AI dance, offering a foundation for more sophisticated human-AI dance collaboration with creative potential.

Abstract: Recent success with large language models has sparked a new wave of verbal
human-AI interaction. While such models support users in a variety of creative
tasks, they lack the embodied nature of human interaction. Dance, as a primal
form of human expression, is predestined to complement this experience. To
explore creative human-AI interaction exemplified by dance, we build an
interactive model based on motion capture (MoCap) data. It generates an
artificial other by partially mimicking and also "creatively" enhancing an
incoming sequence of movement data. It is the first model, which leverages
single-person motion data and high level features in order to do so and, thus,
it does not rely on low level human-human interaction data. It combines ideas
of two diffusion models, motion inpainting, and motion style transfer to
generate movement representations that are both temporally coherent and
responsive to a chosen movement reference. The success of the model is
demonstrated by quantitatively assessing the convergence of the feature
distribution of the generated samples and the test set which serves as
simulating the human performer. We show that our generations are first steps to
creative dancing with AI as they are both diverse showing various deviations
from the human partner while appearing realistic.

</details>


### [2] [Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets](https://arxiv.org/abs/2511.00021)
*Julio Jerison E. Macrohon,Gordon Hung*

Main category: cs.CV

TL;DR: CNN-based coral bleaching classifier on a diverse global dataset achieved 88% accuracy, outperforming ResNet and ViT; demonstrates potential for autonomous coral monitoring.


<details>
  <summary>Details</summary>
Motivation: Coral reefs are threatened by pollution, ocean acidification, and warming; there is urgent need for efficient, scalable monitoring and protection, motivating automated ML approaches for bleaching detection.

Method: Assemble a diverse global image dataset with healthy and bleached corals under various environments (deep sea, marshes, coastal zones). Benchmark three state-of-the-art models: ResNet, Vision Transformer (ViT), and CNN; perform hyperparameter tuning; compare performance.

Result: CNN achieved the highest accuracy at 88%, outperforming ResNet and ViT and existing benchmarks.

Conclusion: Demonstrates the viability of CNN-based autonomous coral bleaching classification; contributes a comparative analysis of popular CV models and informs deployment for automated coral monitoring.

Abstract: Coral reefs support numerous marine organisms and are an important source of
coastal protection from storms and floods, representing a major part of marine
ecosystems. However coral reefs face increasing threats from pollution, ocean
acidification, and sea temperature anomalies, making efficient protection and
monitoring heavily urgent. Therefore, this study presents a novel
machine-learning-based coral bleaching classification system based on a diverse
global dataset with samples of healthy and bleached corals under varying
environmental conditions, including deep seas, marshes, and coastal zones. We
benchmarked and compared three state-of-the-art models: Residual Neural Network
(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).
After comprehensive hyperparameter tuning, the CNN model achieved the highest
accuracy of 88%, outperforming existing benchmarks. Our findings offer
important insights into autonomous coral monitoring and present a comprehensive
analysis of the most widely used computer vision models.

</details>


### [3] [Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline](https://arxiv.org/abs/2511.00022)
*Jules Gerard,Leandro Di Bella,Filip Huyghe,Marc Kochzius*

Main category: cs.CV

TL;DR: YOLOv8-based pipeline for automating family-level reef fish IDs from Western Indian Ocean transects; benchmark on 24 families; best mAP@0.5 0.52; scalable approach with limitations on rare taxa.


<details>
  <summary>Details</summary>
Motivation: Labor-intensive underwater visual censuses limit coral reef monitoring; need scalable, region-specific tools and benchmarks for the Western Indian Ocean.

Method: Curated video transect dataset from Kenya and Tanzania covering 24 fish families; evaluated a YOLOv8-based detection pipeline under multiple configurations; assessed performance using mAP@0.5.

Result: Best model achieved mAP@0.5 of 0.52; high accuracy for abundant families but weaker detection of rare or complex taxa.

Conclusion: Deep learning can be a scalable complement to traditional reef-monitoring methods, with strong potential in the Western Indian Ocean; performance is limited by rare/complex taxa and may benefit from more diverse data and taxa-specific tuning.

Abstract: Coral reef monitoring in the Western Indian Ocean is limited by the labor
demands of underwater visual censuses. This work evaluates a YOLOv8-based deep
learning pipeline for automating family-level fish identification from video
transects collected in Kenya and Tanzania. A curated dataset of 24 families was
tested under different configurations, providing the first region-specific
benchmark for automated reef fish monitoring in the Western Indian Ocean. The
best model achieved mAP@0.5 of 0.52, with high accuracy for abundant families
but weaker detection of rare or complex taxa. Results demonstrate the potential
of deep learning as a scalable complement to traditional monitoring methods.

</details>


### [4] [Mutual Information guided Visual Contrastive Learning](https://arxiv.org/abs/2511.00028)
*Hanyang Chen,Yanchao Yang*

Main category: cs.CV

TL;DR: Mutual-information-informed data selection for contrastive learning; uses MI under natural perturbations to pick high-value patches as positives, improving generalization in open environments.


<details>
  <summary>Details</summary>
Motivation: Current data selection and augmentation rely on human hypotheses; leveraging real-world mutual information under perturbations could yield more robust representations for open-world deployment.

Method: Compute mutual information for scene patches under natural perturbations (e.g., color changes, motion); select patches with high MI as positive samples for contrastive learning, evaluated across multiple SOTA frameworks and benchmarks.

Result: Across benchmarks and frameworks, MI-informed data augmentation is effective, improving representation learning and generalization; demonstrated as a promising research direction.

Conclusion: MI-informed data selection is a promising approach to enhance contrastive learning generalization; future work could optimize MI estimation, scale, and integration with diverse training objectives.

Abstract: Representation learning methods utilizing the InfoNCE loss have demonstrated
considerable capacity in reducing human annotation effort by training invariant
neural feature extractors. Although different variants of the training
objective adhere to the information maximization principle between the data and
learned features, data selection and augmentation still rely on human
hypotheses or engineering, which may be suboptimal. For instance, data
augmentation in contrastive learning primarily focuses on color jittering,
aiming to emulate real-world illumination changes. In this work, we investigate
the potential of selecting training data based on their mutual information
computed from real-world distributions, which, in principle, should endow the
learned features with better generalization when applied in open environments.
Specifically, we consider patches attached to scenes that exhibit high mutual
information under natural perturbations, such as color changes and motion, as
positive samples for learning with contrastive loss. We evaluate the proposed
mutual-information-informed data augmentation method on several benchmarks
across multiple state-of-the-art representation learning frameworks,
demonstrating its effectiveness and establishing it as a promising direction
for future research.

</details>


### [5] [Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra](https://arxiv.org/abs/2511.00037)
*Riya Gupta,Alexander Chowdhury,Sahil Nalawade*

Main category: cs.CV

TL;DR: A comparative study benchmarks NVIDIA FLARE, Flower, and Owkin Substra for federated learning in medical imaging using PathMNIST, assessing performance, convergence, communication, scalability, and developer experience. It concludes each framework excels in different use-cases: FLARE for production scalability, Flower for prototyping/academia, and Substra for privacy/compliance.


<details>
  <summary>Details</summary>
Motivation: To understand how leading federated learning frameworks perform in realistic medical imaging deployment, balancing model quality, training efficiency, operational overhead, and regulatory/privacy considerations across institutions.

Method: Benchmark three FL frameworks (NVIDIA FLARE, Flower, Owkin Substra) on the PathMNIST dataset, evaluating model performance, convergence speed, communication overhead, scalability, and developer experience to reflect real-world healthcare deployments.

Result: NVIDIA FLARE shows superior production scalability; Flower offers flexibility for prototyping/academic research; Owkin Substra emphasizes privacy and compliance. Each framework demonstrates strengths aligned with specific use-case requirements.

Conclusion: The choice of FL framework should be guided by the deployment context in healthcare: production-scale deployments may favor FLARE, exploratory and academic work may prefer Flower, and privacy/compliance-centric settings may opt for Owkin Substra. Overall, the study highlights that different ecosystems cater to distinct practical deployment needs in healthcare.

Abstract: Federated Learning (FL) has emerged as a transformative paradigm in medical
AI, enabling collaborative model training across institutions without direct
data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE,
Flower, and Owkin Substra to evaluate their suitability for medical imaging
applications in real-world settings. Using the PathMNIST dataset, we assess
model performance, convergence efficiency, communication overhead, scalability,
and developer experience. Results indicate that NVIDIA FLARE offers superior
production scalability, Flower provides flexibility for prototyping and
academic research, and Owkin Substra demonstrates exceptional privacy and
compliance features. Each framework exhibits strengths optimized for distinct
use cases, emphasizing their relevance to practical deployment in healthcare
environments.

</details>


### [6] [Enhancing rice leaf images: An overview of image denoising techniques](https://arxiv.org/abs/2511.00046)
*Rupjyoti Chutia,Dibya Jyoti Bora*

Main category: cs.CV

TL;DR: A comparative study of denoising methods combined with CLAHE for rice-leaf image enhancement, evaluating how denoising plus contrast enhancement affects image quality for downstream analysis.


<details>
  <summary>Details</summary>
Motivation: Improve rice leaf analysis (disease/detection, nutrient assessment, growth) by reliable preprocessing that enhances quality for segmentation, feature extraction, and classification.

Method: Apply several well-known denoising filters (e.g., Gaussian, median, bilateral, non-local means, wavelet-based approaches) in combination with CLAHE on a rice-leaf image dataset; assess using multiple quantitative metrics (PSNR, SSIM, contrast measures) to evaluate enhancement quality and potential impact on downstream tasks.

Result: Certain denoising+CLAHE combinations yield superior results across metrics, indicating favorable preservation of details while boosting contrast; findings are dataset-specific but provide a robust baseline for agricultural image processing.

Conclusion: Denoising followed by CLAHE is an effective baseline for rice-leaf image enhancement, offering practical guidance for agricultural research and a foundation for future adaptations to related domains.

Abstract: Digital image processing involves the systematic handling of images using
advanced computer algorithms, and has gained significant attention in both
academic and practical fields. Image enhancement is a crucial preprocessing
stage in the image-processing chain, improving image quality and emphasizing
features. This makes subsequent tasks (segmentation, feature extraction,
classification) more reliable. Image enhancement is essential for rice leaf
analysis, aiding in disease detection, nutrient deficiency evaluation, and
growth analysis. Denoising followed by contrast enhancement are the primary
steps. Image filters, generally employed for denoising, transform or enhance
visual characteristics like brightness, contrast, and sharpness, playing a
crucial role in improving overall image quality and enabling the extraction of
useful information. This work provides an extensive comparative study of
well-known image-denoising methods combined with CLAHE (Contrast Limited
Adaptive Histogram Equalization) for efficient denoising of rice leaf images.
The experiments were performed on a rice leaf image dataset to ensure the data
is relevant and representative. Results were examined using various metrics to
comprehensively test enhancement methods. This approach provides a strong basis
for assessing the effectiveness of methodologies in digital image processing
and reveals insights useful for future adaptation in agricultural research and
other domains.

</details>


### [7] [Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?](https://arxiv.org/abs/2511.00060)
*Zhiqi Qi,Runxin Zhao,Hanyang Zhuang,Chunxiang Wang,Ming Yang*

Main category: cs.CV

TL;DR: InfraLiDARs' Benchmark in CARLA compares non-repetitive (prism-based) and 128-line repetitive LiDARs for roadside perception, finding comparable detection performance despite range differences, and highlights cost-effective options; dataset released for further research.


<details>
  <summary>Details</summary>
Motivation: Roadside LiDAR scanning patterns can critically affect perception quality, yet most work focuses on placement or individual sensors. Understanding how scanning paradigms influence detection performance is essential for cost-efficient, robust ITS deployments.

Method: The authors create InfraLiDARs' Benchmark in the CARLA simulator with concurrently operating infrastructure-based LiDARs representing both scanning paradigms. They perform a statistical analysis of scanning distributions and evaluate multiple leading 3D object detection algorithms on the benchmark data.

Result: Non-repetitive scanning LiDAR and 128-line repetitive LiDAR achieve comparable detection performance across diverse scenarios. The non-repetitive system has a shorter perception range but is cheaper, offering a favorable cost-performance trade-off.

Conclusion: The study offers practical guidance for configuring roadside perception systems with appropriate scanning patterns and compatible algorithms, and publicly releases the InfraLiDARs' Benchmark dataset to spur further research.

Abstract: LiDAR-based roadside perception is a cornerstone of advanced Intelligent
Transportation Systems (ITS). While considerable research has addressed optimal
LiDAR placement for infrastructure, the profound impact of differing LiDAR
scanning patterns on perceptual performance remains comparatively
under-investigated. The inherent nature of various scanning modes - such as
traditional repetitive (mechanical/solid-state) versus emerging non-repetitive
(e.g. prism-based) systems - leads to distinct point cloud distributions at
varying distances, critically dictating the efficacy of object detection and
overall environmental understanding. To systematically investigate these
differences in infrastructure-based contexts, we introduce the "InfraLiDARs'
Benchmark," a novel dataset meticulously collected in the CARLA simulation
environment using concurrently operating infrastructure-based LiDARs exhibiting
both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive
statistical analysis of the respective LiDAR scanning abilities and evaluate
the impact of these distinct patterns on the performance of various leading 3D
object detection algorithms. Our findings reveal that non-repetitive scanning
LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable
detection performance across various scenarios. Despite non-repetitive LiDAR's
limited perception range, it's a cost-effective option considering its low
price. Ultimately, this study provides insights for setting up roadside
perception system with optimal LiDAR scanning patterns and compatible
algorithms for diverse roadside applications, and publicly releases the
"InfraLiDARs' Benchmark" dataset to foster further research.

</details>


### [8] [World Simulation with Video Foundation Models for Physical AI](https://arxiv.org/abs/2511.00062)
*NVIDIA,:,Arslan Ali,Junjie Bai,Maciej Bala,Yogesh Balaji,Aaron Blakeman,Tiffany Cai,Jiaxin Cao,Tianshi Cao,Elizabeth Cha,Yu-Wei Chao,Prithvijit Chattopadhyay,Mike Chen,Yongxin Chen,Yu Chen,Shuai Cheng,Yin Cui,Jenna Diamond,Yifan Ding,Jiaojiao Fan,Linxi Fan,Liang Feng,Francesco Ferroni,Sanja Fidler,Xiao Fu,Ruiyuan Gao,Yunhao Ge,Jinwei Gu,Aryaman Gupta,Siddharth Gururani,Imad El Hanafi,Ali Hassani,Zekun Hao,Jacob Huffman,Joel Jang,Pooya Jannaty,Jan Kautz,Grace Lam,Xuan Li,Zhaoshuo Li,Maosheng Liao,Chen-Hsuan Lin,Tsung-Yi Lin,Yen-Chen Lin,Huan Ling,Ming-Yu Liu,Xian Liu,Yifan Lu,Alice Luo,Qianli Ma,Hanzi Mao,Kaichun Mo,Seungjun Nah,Yashraj Narang,Abhijeet Panaskar,Lindsey Pavao,Trung Pham,Morteza Ramezanali,Fitsum Reda,Scott Reed,Xuanchi Ren,Haonan Shao,Yue Shen,Stella Shi,Shuran Song,Bartosz Stefaniak,Shangkun Sun,Shitao Tang,Sameena Tasmeen,Lyne Tchapmi,Wei-Cheng Tseng,Jibin Varghese,Andrew Z. Wang,Hao Wang,Haoxiang Wang,Heng Wang,Ting-Chun Wang,Fangyin Wei,Jiashu Xu,Dinghao Yang,Xiaodong Yang,Haotian Ye,Seonghyeon Ye,Xiaohui Zeng,Jing Zhang,Qinsheng Zhang,Kaiwen Zheng,Andrew Zhu,Yuke Zhu*

Main category: cs.CV

TL;DR: A unified, flow-based embodied AI stack (Cosmos-Predict2.5) with stronger grounding and RL fine-tuning, plus a smaller, high-fidelity Cosmos-Transfer2.5 for Sim2Real/Real2Real, all open-sourced to advance embodied intelligence.


<details>
  <summary>Details</summary>
Motivation: Need to unify Text2World, Image2World, and Video2World generation with reliable grounding and control, enabling synthetic data, policy evaluation, and closed-loop simulation for robotics/autonomy; lower barriers to adoption.

Method: Proposes a flow-based architecture unifying multiple world-generation modalities; leverages Cosmos-Reason1 vision-language grounding for_Text/ground; trained on 200M curated video clips; reinforced-learning-based post-training; released models at 2B and 14B scales; introduces Cosmos-Transfer2.5 as a control-net style framework for Sim2Real/Real2Real.

Result: Cosmos-Predict2.5 shows substantial improvements over Cosmos-Predict1 in video quality and instruction alignment; opensource release under NVIDIA Open Model License; Cosmos-Transfer2.5 achieves higher fidelity and robust long-horizon video generation despite being 3.5x smaller than Cosmos-Transfer1.

Conclusion: Positions Cosmos-Predict2.5 and Cosmos-Transfer2.5 as versatile, scalable tools for embodied intelligence, enabling better synthetic data, policy evaluation, and real-world translation; aims to accelerate research and deployment.

Abstract: We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World
Foundation Models for Physical AI. Built on a flow-based architecture,
[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation
in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language
model, to provide richer text grounding and finer control of world simulation.
Trained on 200M curated video clips and refined with reinforcement
learning-based post-training, [Cosmos-Predict2.5] achieves substantial
improvements over [Cosmos-Predict1] in video quality and instruction alignment,
with models released at 2B and 14B scales. These capabilities enable more
reliable synthetic data generation, policy evaluation, and closed-loop
simulation for robotics and autonomous systems. We further extend the family
with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and
Real2Real world translation. Despite being 3.5$\times$ smaller than
[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video
generation. Together, these advances establish [Cosmos-Predict2.5] and
[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To
accelerate research and deployment in Physical AI, we release source code,
pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model
License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and
https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open
resources lower the barrier to adoption and foster innovation in building the
next generation of embodied intelligence.

</details>


### [9] [Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures](https://arxiv.org/abs/2511.00073)
*Harald Kristen,Daniel Kulmer,Manuela Hirschmugl*

Main category: cs.CV

TL;DR: A comparative study on change detection in alpine habitats using two paradigms: post-classification with geospatial foundation models (GFMs) and direct CD with transformer models, against U-Net baselines. Finds moderate accuracy in complex alpine landscapes, with LiDAR and cross-temporal robustness aiding performance; results vary by multi-class vs binary change detection.


<details>
  <summary>Details</summary>
Motivation: Rapid climate change and disturbances require frequent, fine-resolution habitat monitoring in alpine ecosystems. Manual mapping is costly and slow, creating a need for scalable automated methods. The paper investigates whether geospatial foundation models (GFMs) can be effectively applied to complex natural environments and how different CD paradigms perform.

Method: Two CD paradigms are compared. (1) Post-classification CD: GFMs Prithvi-EO-2.0 and Clay v1.0 are evaluated alongside U-Net CNNs for multi-class habitat change detection (and binary change). (2) Direct CD: Transformer ChangeViT is evaluated against U-Net baselines. Data modalities include RGB, NIR, LiDAR, and terrain attributes. The dataset comprises 4,480 documented changes over 15.3 km2. Evaluation metrics include overall accuracy (OA), IoU for binary and multi-class, and cross-temporal performance (2020 data).

Result: Clay v1.0 achieves ~51% OA vs ~41% for U-Net on multi-class change detection; both reach ~67% OA for binary change. Direct CD (ChangeViT) yields higher IoU (0.53 vs 0.35) for binary change but only 28% accuracy for multi-class detection. Cross-temporal results show Clay maintaining ~33% accuracy on 2020 data vs U-Net ~23%. Adding LiDAR improves semantic segmentation from ~30% to ~50% accuracy. Overall, accuracies are lower than in homogeneous landscapes, reflecting the complexity of alpine habitats.

Conclusion: GFMs demonstrate robustness to temporal shifts in alpine environments, and LiDAR integration notably boosts performance. However, the task remains challenging due to fuzzy class boundaries and severe class imbalance. The study suggests that combining object-based post-processing and physical constraints could enhance practical applicability in high-complexity landscapes.

Abstract: Rapid climate change and other disturbances in alpine ecosystems demand
frequent habitat monitoring, yet manual mapping remains prohibitively expensive
for the required temporal resolution. We employ deep learning for change
detection using long-term alpine habitat data from Gesaeuse National Park,
Austria, addressing a major gap in applying geospatial foundation models (GFMs)
to complex natural environments with fuzzy class boundaries and highly
imbalanced classes. We compare two paradigms: post-classification change
detection (CD) versus direct CD. For post-classification CD, we evaluate GFMs
Prithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the
transformer ChangeViT against U-Net baselines. Using high-resolution multimodal
data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes
over 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus
U-Net's 41% for multi-class habitat change, while both reach 67% for binary
change detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but
only 28% accuracy for multi-class detection. Cross-temporal evaluation reveals
GFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net's
23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy.
Although overall accuracies are lower than in more homogeneous landscapes, they
reflect realistic performance for complex alpine habitats. Future work will
integrate object-based post-processing and physical constraints to enhance
applicability.

</details>


### [10] [LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation](https://arxiv.org/abs/2511.00090)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Chao Tan,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: Training-free diffusion video acceleration LeMiCa uses error-weighted directed-graph cache scheduling with Lexicographic Minimax Path Optimization to bound worst-case errors, improving speed and global consistency.


<details>
  <summary>Details</summary>
Motivation: Global error accumulation in caching-based acceleration degrades content consistency across frames; a bound on worst-case path error is needed for reliable video quality.

Method: Model cache scheduling as a directed graph with error-weighted edges; apply Lexicographic Minimax Path Optimization to explicitly bound the worst-case path error, reducing global content/style degradation without training.

Result: Achieves dual improvements in speed and quality: 2.9x speedup on Latte and LPIPS of 0.05 on Open-Sora, outperforming prior caching methods with minimal perceptual degradation.

Conclusion: LeMiCa is a robust, generalizable, training-free framework for accelerating diffusion-based video generation and can serve as a foundation for future efficient and reliable video synthesis; code available at the provided URL.

Abstract: We present LeMiCa, a training-free and efficient acceleration framework for
diffusion-based video generation. While existing caching strategies primarily
focus on reducing local heuristic errors, they often overlook the accumulation
of global errors, leading to noticeable content degradation between accelerated
and original videos. To address this issue, we formulate cache scheduling as a
directed graph with error-weighted edges and introduce a Lexicographic Minimax
Path Optimization strategy that explicitly bounds the worst-case path error.
This approach substantially improves the consistency of global content and
style across generated frames. Extensive experiments on multiple text-to-video
benchmarks demonstrate that LeMiCa delivers dual improvements in both inference
speed and generation quality. Notably, our method achieves a 2.9x speedup on
the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming
prior caching techniques. Importantly, these gains come with minimal perceptual
quality degradation, making LeMiCa a robust and generalizable paradigm for
accelerating diffusion-based video generation. We believe this approach can
serve as a strong foundation for future research on efficient and reliable
video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa

</details>


### [11] [Self-Improving Vision-Language-Action Models with Data Generation via Residual RL](https://arxiv.org/abs/2511.00091)
*Wenli Xiao,Haotian Lin,Andy Peng,Haoru Xue,Tairan He,Yuqi Xie,Fengyuan Hu,Jimmy Wu,Zhengyi Luo,Linxi "Jim" Fan,Guanya Shi,Yuke Zhu*

Main category: cs.CV

TL;DR: PLD is a three-stage plug-and-play framework that enhances vision-language-action models via residual reinforcement learning and deployment-aware data collection, followed by distillation back into the base model. It yields strong cross-task gains and demonstrates scalability for self-improving VLA systems.


<details>
  <summary>Details</summary>
Motivation: Supervised fine-tuning (SFT) relies on costly human demonstrations, which limits scalability and generalization. The paper seeks a scalable, self-improving approach to improve VLA models beyond static SFT.

Method: Stage 1: train lightweight residual actors to probe failure regions of the generalist VLA. Stage 2: employ a hybrid rollout that aligns collected trajectories with the deployment distribution while capturing recovery behaviors. Stage 3: distill the curated trajectories back into the generalist via standard SFT.

Result: PLD achieves near-saturated 99% task success on LIBERO, over 50% gains on SimplerEnv, and 100% success on real-world Franka and YAM arm manipulation tasks. Ablations indicate residual probing and distribution-aware replay are key to collecting deployment-aligned data that improves both seen and unseen tasks.

Conclusion: Residual probing plus deployment-aligned data collection enable scalable self-improvement for VLA models by collecting and distilling targeted experiences back into the generalist, improving performance on both seen and unseen tasks.

Abstract: Supervised fine-tuning (SFT) has become the de facto post-training strategy
for large vision-language-action (VLA) models, but its reliance on costly human
demonstrations limits scalability and generalization. We propose Probe, Learn,
Distill (PLD), a three-stage plug-and-play framework that improves VLAs through
residual reinforcement learning (RL) and distribution-aware data collection. In
Stage 1, we train lightweight residual actors to probe failure regions of the
VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns
collected trajectories with the generalist's deployment distribution while
capturing recovery behaviors. In Stage 3, we distill the curated trajectories
back into the generalist with standard SFT. PLD achieves near-saturated 99%
task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on
real-world Franka and YAM arm manipulation tasks. Ablations show that residual
probing and distribution-aware replay are key to collecting deployment-aligned
data that improves both seen and unseen tasks, offering a scalable path toward
self-improving VLA models.

</details>


### [12] [SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation](https://arxiv.org/abs/2511.00095)
*Jiaming Liu,Dingwei Fan,Junyong Zhao,Chunlin Li,Haipeng Si,Liang Sun*

Main category: cs.CV

TL;DR: SpinalSAM-R1 integrates a fine-tuned Segment Anything Model (SAM) with DeepSeek-R1 to deliver multimodal, interactive spine CT segmentation, featuring anatomy-guided attention and semantics-driven language guidance, improved by Low-Rank Adaptation (LoRA). A PyQt5-based GUI supports 11 clinical operations with fast responses.


<details>
  <summary>Details</summary>
Motivation: CT spine segmentation is hindered by low image contrast and complex vertebral boundaries, while existing SAM-based methods require heavy annotation and suffer domain adaptation gaps. There is a need for an efficient, interactive, and accurate tool that fits clinical workflows.

Method: Fine-tune SAM for spine CT with Low-Rank Adaptation (LoRA); incorporate an anatomy-guided attention mechanism to enhance spine segmentation; integrate a semantics-driven interaction protocol via DeepSeek-R1 for natural language-guided refinement; develop a PyQt5-based interactive software supporting point, box, and text prompts (11 operations) with 94.3% parsing accuracy and sub-800 ms latency.

Result: Experimental results indicate superior spine CT segmentation performance of SpinalSAM-R1. The interactive software achieves 94.3% parsing accuracy across 11 clinical operations with sub-800 ms response times.

Conclusion: SpinalSAM-R1 offers an effective, efficient, and interactive solution for spine CT segmentation, combining anatomy-aware segmentation with language-guided refinement, and provides a usable software tool for clinical workflows.

Abstract: The anatomical structure segmentation of the spine and adjacent structures
from computed tomography (CT) images is a key step for spinal disease diagnosis
and treatment. However, the segmentation of CT images is impeded by low
contrast and complex vertebral boundaries. Although advanced models such as the
Segment Anything Model (SAM) have shown promise in various segmentation tasks,
their performance in spinal CT imaging is limited by high annotation
requirements and poor domain adaptability. To address these limitations, we
propose SpinalSAM-R1, a multimodal vision-language interactive system that
integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation.
Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism
to improve spine segmentation performance, and a semantics-driven interaction
protocol powered by DeepSeek-R1, enabling natural language-guided refinement.
The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient
adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with
CT images. Experimental results suggest that our method achieves superior
segmentation performance. Meanwhile, we develop a PyQt5-based interactive
software, which supports point, box, and text-based prompts. The system
supports 11 clinical operations with 94.3\% parsing accuracy and sub-800 ms
response times. The software is released on
https://github.com/6jm233333/spinalsam-r1.

</details>


### [13] [A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning](https://arxiv.org/abs/2511.00098)
*Nils Porsche,Flurin MÃ¼ller-Diesing,Sweta Banerjee,Miguel Goncalves,Marc Aubreville*

Main category: cs.CV

TL;DR: Self-supervised pretraining on filtered CLE video data improves SSL performance and efficiency for CLE-based diagnostics, achieving 67.48% and 73.52% accuracy and 67% faster training compared to non-SSL baselines.


<details>
  <summary>Details</summary>
Motivation: CLE images are hard to interpret for non-experts; limited labeled data with histopathology-correlated CLE; high inter-frame correlation in CLE videos leads to non-stratified SSL data and overfitting; need to exploit unlabeled data efficiently.

Method: Introduce a video filter to reduce redundancy in CLE video sequences for SSL training. Evaluate four baseline networks plus a SSL teacher-student model with a ViT-S backbone on two datasets (sinonasal tumor, skin squamous cell carcinoma). Compare SSL-pretrained vs non-SSL baselines and report accuracy and training-time reductions.

Result: Filtered SSL-pretrained models achieve the highest test accuracy: 67.48% (sinonasal) and 73.52% (skin SCC), outperforming non-SSL baselines. SSL improves performance and the CLE video filter reduces training time by 67%.

Conclusion: SSL is effective for CLE pretraining when paired with the proposed video-filter to mitigate inter-frame redundancy, yielding better diagnostic accuracy and substantially faster training, suggesting broad utility for CLE-based ML.

Abstract: Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging
modality that can be used for in-situ, in-vivo imaging and the microstructural
analysis of mucous structures. The diagnosis using CLE is, however, complicated
by images being hard to interpret for non-experienced physicians. Utilizing
machine learning as an augmentative tool would hence be beneficial, but is
complicated by the shortage of histopathology-correlated CLE imaging sequences
with respect to the plurality of patterns in this domain, leading to
overfitting of machine learning models. To overcome this, self-supervised
learning (SSL) can be employed on larger unlabeled datasets. CLE is a
video-based modality with high inter-frame correlation, leading to a
non-stratified data distribution for SSL training. In this work, we propose a
filter functionality on CLE video sequences to reduce the dataset redundancy in
SSL training and improve SSL training convergence and training efficiency. We
use four state-of-the-art baseline networks and a SSL teacher-student network
with a vision transformer small backbone for the evaluation. These networks
were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous
cell carcinoma of the skin dataset. On both datasets, we found the highest test
accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both
considerably outperforming their non-SSL baselines. Our results show that SSL
is an effective method for CLE pretraining. Further, we show that our proposed
CLE video filter can be utilized to improve training efficiency in
self-supervised scenarios, resulting in a reduction of 67% in training time.

</details>


### [14] [FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video](https://arxiv.org/abs/2511.00103)
*Rotem Ezra,Hedi Zisling,Nimrod Berman,Ilan Naiman,Alexey Gorkor,Liran Nochumsohn,Eliya Nachmani,Omri Azencot*

Main category: cs.CV

TL;DR: FreeSliders enables training-free, modality-agnostic, fine-grained controllable generation across diffusion-model modalities by partially estimating the concept-sliders formula at inference; extends the CS benchmark to video and audio; introduces a two-stage saturation-aware reparameterization and new evaluation metrics, achieving plug-and-play improvements over baselines.


<details>
  <summary>Details</summary>
Motivation: Controllable generation with diffusion models across multiple modalities remains challenging; existing Concept Sliders (CS) require per-concept training and architecture-specific fine-tuning, hindering scalability to new modalities like video and audio. A training-free, modality-agnostic approach is highly desirable.

Method: At inference time, partially estimate the CS formula to enable concept sliders without per-concept training. Extend the CS benchmark to video and audio. Propose three evaluation properties and new metrics. Introduce a two-stage procedure to automatically detect saturation points and reparameterize traversal for perceptually uniform, semantically meaningful edits.

Result: Empirically, FreeSliders provides plug-and-play, training-free concept control across multiple modalities, outperforming existing baselines. It also expands the CS benchmark to cover video and audio and establishes tools and metrics for principled evaluation.

Conclusion: FreeSliders offers a simple, training-free, modality-agnostic route to fine-grained controllable generation with diffusion models. It identifies scale selection and non-linear traversals as open problems and presents a two-stage saturation-aware reparameterization along with a multimodal benchmark and evaluation toolkit.

Abstract: Diffusion models have become state-of-the-art generative models for images,
audio, and video, yet enabling fine-grained controllable generation, i.e.,
continuously steering specific concepts without disturbing unrelated content,
remains challenging. Concept Sliders (CS) offer a promising direction by
discovering semantic directions through textual contrasts, but they require
per-concept training and architecture-specific fine-tuning (e.g., LoRA),
limiting scalability to new modalities. In this work we introduce FreeSliders,
a simple yet effective approach that is fully training-free and
modality-agnostic, achieved by partially estimating the CS formula during
inference. To support modality-agnostic evaluation, we extend the CS benchmark
to include both video and audio, establishing the first suite for fine-grained
concept generation control with multiple modalities. We further propose three
evaluation properties along with new metrics to improve evaluation quality.
Finally, we identify an open problem of scale selection and non-linear
traversals and introduce a two-stage procedure that automatically detects
saturation points and reparameterizes traversal for perceptually uniform,
semantically meaningful edits. Extensive experiments demonstrate that our
method enables plug-and-play, training-free concept control across modalities,
improves over existing baselines, and establishes new tools for principled
controllable generation. An interactive presentation of our benchmark and
method is available at: https://azencot-group.github.io/FreeSliders/

</details>


### [15] [AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency](https://arxiv.org/abs/2511.00107)
*Piyushkumar Patel*

Main category: cs.CV

TL;DR: MOVAI presents a hierarchical, multimodal approach to text-to-video with CSP, TSAM, and PVR, achieving state-of-the-art results on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: Text-to-video generation struggles with temporal coherence, compositional understanding, and controllability; a framework that integrates scene parsing with temporal diffusion is needed.

Method: Three innovations: (1) Compositional Scene Parser (CSP) for hierarchical, temporally annotated scene graphs; (2) Temporal-Spatial Attention Mechanism (TSAM) for coherent motion and spatial detail; (3) Progressive Video Refinement (PVR) for multi-scale temporal refinement.

Result: Empirical results on standard benchmarks show improvements: 15.3% in LPIPS, 12.7% in FVD, and 18.9% in user preference studies over existing methods.

Conclusion: MOVAI delivers state-of-the-art video quality and strong performance in complex multi-object scenes with realistic temporal dynamics and fine-grained semantic control.

Abstract: Text to video generation has emerged as a critical frontier in generative
artificial intelligence, yet existing approaches struggle with maintaining
temporal consistency, compositional understanding, and fine grained control
over visual narratives. We present MOVAI (Multimodal Original Video AI), a
novel hierarchical framework that integrates compositional scene understanding
with temporal aware diffusion models for high fidelity text to video synthesis.
Our approach introduces three key innovations: (1) a Compositional Scene Parser
(CSP) that decomposes textual descriptions into hierarchical scene graphs with
temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that
ensures coherent motion dynamics across frames while preserving spatial
details, and (3) a Progressive Video Refinement (PVR) module that iteratively
enhances video quality through multi-scale temporal reasoning. Extensive
experiments on standard benchmarks demonstrate that MOVAI achieves
state-of-the-art performance, improving video quality metrics by 15.3% in
LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing
methods. Our framework shows particular strength in generating complex
multi-object scenes with realistic temporal dynamics and fine-grained semantic
control.

</details>


### [16] [Chain of Time: In-Context Physical Simulation with Image Generation Models](https://arxiv.org/abs/2511.00110)
*YingQiao Wang,Eric Bigelow,Boyi Li,Tomer Ullman*

Main category: cs.CV

TL;DR: Introduce Chain of Time, a cognitively inspired, inference-time method that generates a sequence of intermediate frames to simulate physics, boosting and interpreting physical reasoning in vision-language models without fine-tuning, tested on 2D graphics and real 3D videos across velocity, acceleration, fluid dynamics, and momentum.


<details>
  <summary>Details</summary>
Motivation: Improve physical reasoning and interpretability in vision-language models by emulating human mental simulation and in-context reasoning; provide temporal insights into the model's internal dynamics.

Method: Chain of Time generates intermediate frames during inference (no fine-tuning) to simulate a temporal trajectory. It is applied to both synthetic 2D graphics and real 3D videos to probe properties like velocity, acceleration, fluid dynamics, and momentum; analyzes the world states at each time step to understand dynamics and identify failures.

Result: Yields substantial improvement in a state-of-the-art image-generation model. Provides insights into dynamics (e.g., velocity, gravity, collisions) that traditional evaluations miss. Reveals cases where the model can simulate time-evolving physical properties and where it struggles to infer certain physical parameters.

Conclusion: A simple, inference-time augmentation that enhances physical simulation and interpretability in vision-language models. It reveals temporal dynamics and can guide future model improvements, while also exposing specific failure modes in parameter inference.

Abstract: We propose a novel cognitively-inspired method to improve and interpret
physical simulation in vision-language models. Our ``Chain of Time" method
involves generating a series of intermediate images during a simulation, and it
is motivated by in-context reasoning in machine learning, as well as mental
simulation in humans. Chain of Time is used at inference time, and requires no
additional fine-tuning. We apply the Chain-of-Time method to synthetic and
real-world domains, including 2-D graphics simulations and natural 3-D videos.
These domains test a variety of particular physical properties, including
velocity, acceleration, fluid dynamics, and conservation of momentum. We found
that using Chain-of-Time simulation substantially improves the performance of a
state-of-the-art image generation model. Beyond examining performance, we also
analyzed the specific states of the world simulated by an image model at each
time step, which sheds light on the dynamics underlying these simulations. This
analysis reveals insights that are hidden from traditional evaluations of
physical reasoning, including cases where an image generation model is able to
simulate physical properties that unfold over time, such as velocity, gravity,
and collisions. Our analysis also highlights particular cases where the image
generation model struggles to infer particular physical parameters from input
images, despite being capable of simulating relevant physical processes.

</details>


### [17] [End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning](https://arxiv.org/abs/2511.00114)
*Hanae Elmekki,Amanda Spilkin,Ehsan Zakeri,Antonela Mariel Zanuttini,Ahmed Alagha,Hani Sami,Jamal Bentahar,Lyes Kadem,Wen-Fang Xie,Philippe Pibarot,Rabeb Mizouni,Hadi Otrok,Azzam Mourad,Sami Muhaidat*

Main category: cs.CV

TL;DR: An end-to-end framework combining a GAN+VAE-based conditional simulator with a DRL agent to autonomously and reproducibly perform cardiac ultrasound scanning, with a public dataset and validated results.


<details>
  <summary>Details</summary>
Motivation: Cardiac ultrasound is highly operator-dependent, time-constrained, and scarce in remote areas. Prior DRL approaches lack reproducibility, rely on proprietary data, and use simplified models. There is a need for automated, reproducible, accessible ultrasound scanning.

Method: Develop a two-part system: (1) a conditional generative simulator (GAN + VAE) that models the cardiac US environment and produces action-conditioned, realistic images; (2) a DRL module trained in this simulator to learn autonomous, accurate scanning policies. The framework includes expert-validated models for image-type classification and quality assessment, conditional generation of realistic US images, and release of a real cardiac US dataset to enable reproducibility.

Result: The framework delivers AI-driven guidance for cardiac US scanning, supports classification of image type and assessment of image quality, enables conditional generation of realistic US images, and provides a reproducible foundation. The VAE-GAN is benchmarked against existing GAN variants, with qualitative and quantitative evaluation, while the DRL-based scanning system is tested under varying configurations to demonstrate effectiveness.

Conclusion: This work presents the first end-to-end, reproducible framework combining generative AI and DRL for autonomous cardiac US scanning, with a publicly released real US dataset, and a foundation extendable to imaging other organs.

Abstract: Cardiac ultrasound (US) is among the most widely used diagnostic tools in
cardiology for assessing heart health, but its effectiveness is limited by
operator dependence, time constraints, and human error. The shortage of trained
professionals, especially in remote areas, further restricts access. These
issues underscore the need for automated solutions that can ensure consistent,
and accessible cardiac imaging regardless of operator skill or location. Recent
progress in artificial intelligence (AI), especially in deep reinforcement
learning (DRL), has gained attention for enabling autonomous decision-making.
However, existing DRL-based approaches to cardiac US scanning lack
reproducibility, rely on proprietary data, and use simplified models. Motivated
by these gaps, we present the first end-to-end framework that integrates
generative AI and DRL to enable autonomous and reproducible cardiac US
scanning. The framework comprises two components: (i) a conditional generative
simulator combining Generative Adversarial Networks (GANs) with Variational
Autoencoders (VAEs), that models the cardiac US environment producing realistic
action-conditioned images; and (ii) a DRL module that leverages this simulator
to learn autonomous, accurate scanning policies. The proposed framework
delivers AI-driven guidance through expert-validated models that classify image
type and assess quality, supports conditional generation of realistic US
images, and establishes a reproducible foundation extendable to other organs.
To ensure reproducibility, a publicly available dataset of real cardiac US
scans is released. The solution is validated through several experiments. The
VAE-GAN is benchmarked against existing GAN variants, with performance assessed
using qualitative and quantitative approaches, while the DRL-based scanning
system is evaluated under varying configurations to demonstrate effectiveness.

</details>


### [18] [VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images](https://arxiv.org/abs/2511.00120)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: VLM6D introduces a dual-stream RGB-D pose estimator that fuses DINOv2-based RGB features with PointNet++ 3D geometry, achieving state-of-the-art results on Occluded-LineMOD.


<details>
  <summary>Details</summary>
Motivation: 6D pose estimation is fragile when generalizing from synthetic data to real-world conditions with varying lighting, textureless objects, and heavy occlusions; leveraging complementary visual and geometric cues can improve robustness and accuracy.

Method: A dual-stream architecture where RGB is processed by a self-supervised Vision Transformer (DINOv2) and 3D points (from depth) are processed by PointNet++; features are fused in a multi-task prediction head for pose estimation, with emphasis on robustness to occlusion and lighting variations.

Result: The method achieves new state-of-the-art performance on Occluded-LineMOD, demonstrating superior robustness and accuracy in challenging scenarios.

Conclusion: Combining rich visual representations with geometric reasoning via a dual-stream fusion yields robust 6D pose estimation under real-world challenges and establishes a strong benchmark for occluded scenes.

Abstract: The primary challenge in computer vision is precisely calculating the pose of
6D objects, however many current approaches are still fragile and have trouble
generalizing from synthetic data to real-world situations with fluctuating
lighting, textureless objects, and significant occlusions. To address these
limitations, VLM6D, a novel dual-stream architecture that leverages the
distinct strengths of visual and geometric data from RGB-D input for robust and
precise pose estimation. Our framework uniquely integrates two specialized
encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the
RGB modality, harnessing its rich, pre-trained understanding of visual grammar
to achieve remarkable resilience against texture and lighting variations.
Concurrently, a PointNet++ encoder processes the 3D point cloud derived from
depth data, enabling robust geometric reasoning that excels even with the
sparse, fragmented data typical of severe occlusion. These complementary
feature streams are effectively fused to inform a multi task prediction head.
We demonstrate through comprehensive experiments that VLM6D obtained new SOTA
performance on the challenging Occluded-LineMOD, validating its superior
robustness and accuracy.

</details>


### [19] [Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation](https://arxiv.org/abs/2511.00123)
*Gaby Maroun,Salah Eddine Bekhouche,Fadi Dornaika*

Main category: cs.CV

TL;DR: A ConvNeXt-ViT hybrid for age estimation yields state-of-the-art MAE on MORPH II, CACD, and AFAD by combining CNN local features with Transformer global attention; ablation studies emphasize adapted CNN attention and training strategies.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of single-model age estimation approaches by leveraging the complementary strengths of ConvNeXt (CNN) for local feature extraction and Vision Transformers for global attention, enabling improved focus on age-related facial features.

Method: Propose a ConvNeXt-ViT hybrid architecture, use pre-trained weights, add linear layers and regularization, and perform comprehensive ablation studies. Evaluate on MORPH II, CACD, and AFAD with MAE as the metric.

Result: The hybrid achieves superior mean absolute error (MAE) across the benchmark datasets compared to traditional methods, with ablations highlighting key components and training strategies, including the importance of adapted attention mechanisms within the CNN framework.

Conclusion: Hybrid CNN-Transformer architectures are a promising direction for age estimation and related visual tasks, offering robustness and potential for future advances by integrating CNNs and transformers.

Abstract: Age estimation from facial images is a complex and multifaceted challenge in
computer vision. In this study, we present a novel hybrid architecture that
combines ConvNeXt, a state-of-the-art advancement of convolutional neural
networks (CNNs), with Vision Transformers (ViT). While each model independently
delivers excellent performance on a variety of tasks, their integration
leverages the complementary strengths of the CNNs localized feature extraction
capabilities and the Transformers global attention mechanisms. Our proposed
ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age
estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior
performance in terms of mean absolute error (MAE). To address computational
constraints, we leverage pre-trained models and systematically explore
different configurations, using linear layers and advanced regularization
techniques to optimize the architecture. Comprehensive ablation studies
highlight the critical role of individual components and training strategies,
and in particular emphasize the importance of adapted attention mechanisms
within the CNN framework to improve the model focus on age-relevant facial
features. The results show that the ConvNeXt-ViT hybrid not only outperforms
traditional methods, but also provides a robust foundation for future advances
in age estimation and related visual tasks. This work underscores the
transformative potential of hybrid architectures and represents a promising
direction for the seamless integration of CNNs and transformers to address
complex computer vision challenges.

</details>


### [20] [FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding](https://arxiv.org/abs/2511.00141)
*Janghoon Cho,Jungsoo Lee,Munawar Hayat,Kyuwoong Hwang,Fatih Porikli,Sungha Choi*

Main category: cs.CV

TL;DR: FLoC is a training-free, model-agnostic token compression framework for long video understanding. It uses a facility location function to select a compact, diverse token subset within a fixed budget, optimized by lazy greedy, yielding near-optimal performance with high efficiency across various video-LMMs.


<details>
  <summary>Details</summary>
Motivation: Long video understanding suffers from an explosion of visual tokens produced by extended sequences, taxing model capacity and speed. Existing compression methods often require training, model-specific design, or fail to balance representation quality with efficiency.

Method: Propose FLoC, which formulates token selection as maximizing a facility location function under a token budget. It employs the lazy greedy algorithm for efficient near-optimal selection. The method is training-free, model-agnostic, and query-agnostic, enabling seamless integration with diverse video-LLMs and workflows.

Result: Empirical evaluations on Video-MME, MLVU, and LongVideoBench show that FLoC consistently surpasses recent compression techniques in both effectiveness and processing speed, indicating robust scalability and robustness for long video understanding.

Conclusion: FLoC provides a versatile, efficient, training-free token compression solution that addresses key scalability challenges in long video understanding and can be readily integrated into existing video-LLM pipelines.

Abstract: Recent studies in long video understanding have harnessed the advanced
visual-language reasoning capabilities of Large Multimodal Models (LMMs),
driving the evolution of video-LMMs specialized for processing extended video
sequences. However, the scalability of these models is severely limited by the
overwhelming volume of visual tokens generated from extended video sequences.
To address this challenge, this paper proposes FLoC, an efficient visual token
compression framework based on the facility location function, a principled
approach that swiftly selects a compact yet highly representative and diverse
subset of visual tokens within a predefined budget on the number of visual
tokens. By integrating the lazy greedy algorithm, our method achieves
remarkable efficiency gains by swiftly selecting a compact subset of tokens,
drastically reducing the number of visual tokens while guaranteeing
near-optimal performance. Notably, our approach is training-free,
model-agnostic, and query-agnostic, providing a versatile solution that
seamlessly integrates with diverse video-LLMs and existing workflows. Extensive
evaluations on large-scale benchmarks, such as Video-MME, MLVU, and
LongVideoBench, demonstrate that our framework consistently surpasses recent
compression techniques, highlighting not only its effectiveness and robustness
in addressing the critical challenges of long video understanding, but also its
efficiency in processing speed.

</details>


### [21] [BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing](https://arxiv.org/abs/2511.00143)
*Jinsu Kim,Yunhun Nam,Minseon Kim,Sangpil Kim,Jongheon Jeong*

Main category: cs.CV

TL;DR: Adaptive per-region Gaussian blur on protective adversarial noise yields more irreversible, robust image protection against reversal edits, preserving perceptual quality; named BlurGuard; code at GitHub.


<details>
  <summary>Details</summary>
Motivation: Protect against malicious misuse of text-to-image editing by irreversible, hard-to-remove protections; prior protective noises were easily undone by simple transformations like JPEG, undermining practicality.

Method: Apply an adaptive Gaussian blur to the protective noise on a per-region basis to reshape the noise frequency spectrum and hinder reversal attempts; region-aware blur.

Result: Improved worst-case protection across various reversal techniques and editing scenarios; reduced perceptual degradation relative to prior approaches.

Conclusion: The proposed BlurGuard approach enhances robustness of image protections, offering practical effectiveness; code available.

Abstract: Recent advances in text-to-image models have increased the exposure of
powerful image editing techniques as a tool, raising concerns about their
potential for malicious use. An emerging line of research to address such
threats focuses on implanting "protective" adversarial noise into images before
their public release, so future attempts to edit them using text-to-image
models can be impeded. However, subsequent works have shown that these
adversarial noises are often easily "reversed," e.g., with techniques as simple
as JPEG compression, casting doubt on the practicality of the approach. In this
paper, we argue that adversarial noise for image protection should not only be
imperceptible, as has been a primary focus of prior work, but also
irreversible, viz., it should be difficult to detect as noise provided that the
original image is hidden. We propose a surprisingly simple method to enhance
the robustness of image protection methods against noise reversal techniques.
Specifically, it applies an adaptive per-region Gaussian blur on the noise to
adjust the overall frequency spectrum. Through extensive experiments, we show
that our method consistently improves the per-sample worst-case protection
performance of existing methods against a wide range of reversal techniques on
diverse image editing scenarios, while also reducing quality degradation due to
noise in terms of perceptual metrics. Code is available at
https://github.com/jsu-kim/BlurGuard.

</details>


### [22] [CompAgent: An Agentic Framework for Visual Compliance Verification](https://arxiv.org/abs/2511.00171)
*Rahul Ghosh,Baishali Chaudhury,Hari Prasanna Das,Meghana Ashok,Ryan Razkenari,Sungmin Hong,Chun-Hao Liu*

Main category: cs.CV

TL;DR: Proposes CompAgent, an agentic framework that augments MLLMs with visual tools and planning to verify visual compliance; achieves strong performance on UnsafeBench.


<details>
  <summary>Details</summary>
Motivation: Visual compliance verification is important in media/advertising but underexplored; existing methods lack generalizability; MLLMs alone struggle with fine-grained rules.

Method: Introduce planning agent to select visual tools (object detectors, face analyzers, NSFW detectors, captioning); verification agent integrates image, outputs, policy context for multi-modal reasoning.

Result: Outperforms specialized classifiers, direct MLLM prompting, routing baselines; up to 76% F1; +10% SOTA on UnsafeBench.

Conclusion: Agentic planning + tool-augmented reasoning enables scalable, accurate, adaptable visual compliance verification.

Abstract: Visual compliance verification is a critical yet underexplored problem in
computer vision, especially in domains such as media, entertainment, and
advertising where content must adhere to complex and evolving policy rules.
Existing methods often rely on task-specific deep learning models trained on
manually labeled datasets, which are costly to build and limited in
generalizability. While recent multi-modal large language models (MLLMs) offer
broad real-world knowledge and policy understanding, they struggle to reason
over fine-grained visual details and apply structured compliance rules
effectively on their own. In this paper, we propose CompAgent, the first
agentic framework for visual compliance verification. CompAgent augments MLLMs
with a suite of visual tools - such as object detectors, face analyzers, NSFW
detectors, and captioning models - and introduces a planning agent that
dynamically selects appropriate tools based on the compliance policy. A
verification agent then integrates image, tool outputs, and policy context to
perform multi-modal reasoning. Experiments on public benchmarks show that
CompAgent outperforms specialized classifiers, direct MLLM prompting, and
curated routing baselines, achieving up to 76% F1 score and a 10% improvement
over the state-of-the-art on the UnsafeBench dataset. Our results demonstrate
the effectiveness of agentic planning and tool-augmented reasoning for
scalable, accurate, and adaptable visual compliance verification.

</details>


### [23] [From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection](https://arxiv.org/abs/2511.00181)
*Mengfei Liang,Yiting Qu,Yukun Jiang,Michael Backes,Yang Zhang*

Main category: cs.CV

TL;DR: AIFo is a training-free, agent-based forensics framework that uses multi-agent collaboration and cross-source evidence (reverse image search, metadata, classifiers, VLMs) guided by LLMs, with memory-augmented reasoning and a debate mechanism, achieving 97.05% accuracy on 6,000 images and outperforming traditional classifiers and VLMs.


<details>
  <summary>Details</summary>
Motivation: The rapid rise of AI-generated images threatens information integrity and media authenticity. Existing detectors struggle with generalization, interpretability, and cross-model robustness. There is a need for robust, interpretable, and adaptable detection that leverages multi-source evidence.

Method: AIFo deploys a coordinated set of forensic tools (reverse image search, metadata extraction, pre-trained classifiers, VLM analysis) managed by specialized LLM-based agents. When evidence conflicts, agents engage in structured multi-agent debates to reach reliable conclusions. A memory-augmented reasoning module learns from historical cases to improve future detection. The approach is training-free and evaluated on 6,000 images from controlled and real-world sources, including modern generative platforms.

Result: Achieves 97.05% accuracy, substantially outperforming traditional classifiers and state-of-the-art VLMs. Demonstrates that agent-based procedural reasoning yields more robust, interpretable, and adaptable AI-generated image detection.

Conclusion: Agent-based procedural reasoning offers a new paradigm for AI-generated image detection, providing interpretable, robust, and adaptable detection through cross-source evidence synthesis and evolving memory-informed reasoning.

Abstract: The rapid evolution of AI-generated images poses unprecedented challenges to
information integrity and media authenticity. Existing detection approaches
suffer from fundamental limitations: traditional classifiers lack
interpretability and fail to generalize across evolving generative models,
while vision-language models (VLMs), despite their promise, remain constrained
to single-shot analysis and pixel-level reasoning. To address these challenges,
we introduce AIFo (Agent-based Image Forensics), a novel training-free
framework that emulates human forensic investigation through multi-agent
collaboration. Unlike conventional methods, our framework employs a set of
forensic tools, including reverse image search, metadata extraction,
pre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based
agents that collect, synthesize, and reason over cross-source evidence. When
evidence is conflicting or insufficient, a structured multi-agent debate
mechanism allows agents to exchange arguments and reach a reliable conclusion.
Furthermore, we enhance the framework with a memory-augmented reasoning module
that learns from historical cases to improve future detection accuracy. Our
comprehensive evaluation spans 6,000 images across both controlled laboratory
settings and challenging real-world scenarios, including images from modern
generative platforms and diverse online sources. AIFo achieves 97.05% accuracy,
substantially outperforming traditional classifiers and state-of-the-art VLMs.
These results demonstrate that agent-based procedural reasoning offers a new
paradigm for more robust, interpretable, and adaptable AI-generated image
detection.

</details>


### [24] [A Retrospect to Multi-prompt Learning across Vision and Language](https://arxiv.org/abs/2511.00191)
*Ziliang Chen,Xin Huang,Quanlong Guan,Liang Lin,Weiqi Luo*

Main category: cs.CV

TL;DR: Energy-based Multi-prompt Learning (EMPL) for vision-language models samples multiple prompts from an energy-based distribution defined by the model, enabling parameter-efficient, diverse prompts and improved open-vocabulary generalization. The work provides theoretical justification and empirical validation for multi-prompt augmentation.


<details>
  <summary>Details</summary>
Motivation: VLMs enable rapid adaptation via prompts, but most work centers on a single prompt. Extending to learnable, multi-prompt prompts and leveraging their augmentation potential can enhance cross-domain transfer and generalization. The paper extends the constant modality gap phenomenon to learnable prompts and argues that multi-prompt sampling is beneficial.

Method: Introduce EMPL: learnable prompts augmented by sampling multiple prompt embeddings from an energy-based distribution implicitly defined by the VLM. This yields a parameter-efficient multi-prompt framework that aims to balance in-domain and out-of-domain open-vocabulary generalization.

Result: Comprehensive experiments validate that EMPL is parameter-efficient and improves open-vocabulary generalization, outperforming single-prompt baselines and showcasing the effectiveness of multi-prompt augmentation.

Conclusion: EMPL provides a principled, efficient approach to vision-language multi-prompt learning, extends the constant modality gap to learnable prompts, and demonstrates the benefits of energy-based, multi-prompt augmentation for VLM adaptation.

Abstract: The vision community is undergoing the unprecedented progress with the
emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays
as the holy grail of accessing VLMs since it enables their fast adaptation to
downstream tasks with limited resources. Whereas existing researches milling
around single-prompt paradigms, rarely investigate the technical potential
behind their multi-prompt learning counterparts. This paper aims to provide a
principled retrospect for vision-language multi-prompt learning. We extend the
recent constant modality gap phenomenon to learnable prompts and then, justify
the superiority of vision-language transfer with multi-prompt augmentation,
empirically and theoretically. In terms of this observation, we propose an
Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt
embeddings by drawing instances from an energy-based distribution, which is
implicitly defined by VLMs. So our EMPL is not only parameter-efficient but
also rigorously lead to the balance between in-domain and out-of-domain
open-vocabulary generalization. Comprehensive experiments have been conducted
to justify our claims and the excellence of EMPL.

</details>


### [25] [An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals](https://arxiv.org/abs/2511.00211)
*Wenxuan Zhang,Peng Hu*

Main category: cs.CV

TL;DR: A transfer learning-based approach enables local ground-terminal weather-condition detection for satellite LEO networks, outperforming standard detectors and generalizing across scenarios.


<details>
  <summary>Details</summary>
Motivation: Weather-induced impairments threaten space-ground link reliability. Fine-grained, locally adaptable detection on ground terminals is needed for fault diagnostics and mitigation, which existing DL methods fail to provide in practice.

Method: An efficient transfer learning framework that customizes a base detector to ground-terminal imagery to recognize weather-related conditions (e.g., snow, wet) with lightweight adaptation, comparing against YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO.

Result: The proposed TL method achieves superior detection performance relative to standard DL baselines and demonstrates strong generalization to various weather scenarios and deployment conditions.

Conclusion: TL-based ground-terminal weather-condition detection can enhance reliability of satellite Internet in adverse weather by enabling timely diagnostics and mitigation through local, adaptable models.

Abstract: The increasing adoption of satellite Internet with low-Earth-orbit (LEO)
satellites in mega-constellations allows ubiquitous connectivity to rural and
remote areas. However, weather events have a significant impact on the
performance and reliability of satellite Internet. Adverse weather events such
as snow and rain can disturb the performance and operations of satellite
Internet's essential ground terminal components, such as satellite antennas,
significantly disrupting the space-ground link conditions between LEO
satellites and ground stations. This challenge calls for not only region-based
weather forecasts but also fine-grained detection capability on ground terminal
components of fine-grained weather conditions. Such a capability can assist in
fault diagnostics and mitigation for reliable satellite Internet, but its
solutions are lacking, not to mention the effectiveness and generalization that
are essential in real-world deployments. This paper discusses an efficient
transfer learning (TL) method that can enable a ground component to locally
detect representative weather-related conditions. The proposed method can
detect snow, wet, and other conditions resulting from adverse and typical
weather events and shows superior performance compared to the typical deep
learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL
method also shows the advantage of being generalizable to various scenarios.

</details>


### [26] [DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy](https://arxiv.org/abs/2511.00218)
*Rajatsubhra Chakraborty,Ana Espinosa-Momox,Riley Haskin,Depeng Xu,Rosario Porras-Aguilar*

Main category: cs.CV

TL;DR: Dual-encoder multimodal network DM-QPMNet for ssQPM cell segmentation; separate encoders for polarized intensity and phase maps with multi-head attention-based fusion achieve better segmentation than simple concatenation.


<details>
  <summary>Details</summary>
Motivation: Challenges: thresholding is noise- and density-sensitive; simple channel concatenation fails to exploit complementary information between polarization and phase data; need principled multi-modal integration for robust segmentation.

Method: DM-QPMNet uses two encoders for the two modalities, intermediate-depth feature fusion via multi-head attention, per-modality normalization, and dual-source skip connections to maintain training stability with minimal overhead.

Result: Outperforms monolithic concatenation and single-modality baselines, demonstrating effective integration of polarized edge/texture cues with phase information for robust ssQPM cell segmentation.

Conclusion: Modality-specific encoding with learnable fusion is effective for multimodal ssQPM segmentation and can generalize to other multi-modal microscopy tasks.

Abstract: Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces
challenges from traditional thresholding methods that are sensitive to noise
and cell density, while deep learning approaches using simple channel
concatenation fail to exploit the complementary nature of polarized intensity
images and phase maps. We introduce DM-QPMNet, a dual-encoder network that
treats these as distinct modalities with separate encoding streams. Our
architecture fuses modality-specific features at intermediate depth via
multi-head attention, enabling polarized edge and texture representations to
selectively integrate complementary phase information. This content-aware
fusion preserves training stability while adding principled multi-modal
integration through dual-source skip connections and per-modality normalization
at minimal overhead. Our approach demonstrates substantial improvements over
monolithic concatenation and single-modality baselines, showing that
modality-specific encoding with learnable fusion effectively exploits ssQPM's
simultaneous capture of complementary illumination and phase cues for robust
cell segmentation.

</details>


### [27] [Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior](https://arxiv.org/abs/2511.00231)
*Fuming Yang,Yicong Li,Hanspeter Pfister,Jeff W. Lichtman,Yaron Meirovitch*

Main category: cs.CV

TL;DR: A VQ-VAE-based compression framework for petascale EM data enabling extreme compression (16xâ1024x) with pay-as-you-decode decoding; includes an optional Transformer prior to predict bottom tokens to restore texture via FiLM/concatenation, and an ROI-driven workflow for selective high-resolution reconstruction from 1024x latents where needed.


<details>
  <summary>Details</summary>
Motivation: Petascale electron microscopy datasets challenge storage, transfer, and downstream analysis. There is a need for scalable, flexible compression that preserves essential texture while enabling selective high-resolution reconstructions to focus compute where it matters.

Method: Use a vector-quantized variational autoencoder (VQ-VAE) for lossy compression. Support pay-as-you-decode: top-only decoding for extreme compression. Include an optional Transformer prior that predicts bottom tokens without changing the compression ratio to restore texture via feature-wise linear modulation (FiLM) and concatenation. Introduce an ROI-driven workflow that performs selective high-resolution reconstruction from 1024x-compressed latents only where needed.

Result: The framework provides 16xâ1024x compression with pay-as-you-decode capability, an optional Transformer-based texture restoration pathway, and an ROI-driven workflow enabling selective high-resolution reconstruction from highly compressed latents.

Conclusion: This approach offers scalable, ROI-aware compression for petascale EM data, balancing storage/transfer efficiency with the ability to selectively restore high-resolution texture where it matters, potentially reducing decode costs and enabling targeted analyses.

Abstract: Petascale electron microscopy (EM) datasets push storage, transfer, and
downstream analysis toward their current limits. We present a vector-quantized
variational autoencoder-based (VQ-VAE) compression framework for EM that spans
16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme
compression, with an optional Transformer prior that predicts bottom tokens
(without changing the compression ratio) to restore texture via feature-wise
linear modulation (FiLM) and concatenation; we further introduce an ROI-driven
workflow that performs selective high-resolution reconstruction from
1024x-compressed latents only where needed.

</details>


### [28] [Hyperbolic Optimal Transport](https://arxiv.org/abs/2511.00244)
*Yan Bin Ng,Xianfeng Gu*

Main category: cs.CV

TL;DR: Efficient hyperbolic OT map via geometric variational approach, extending Euclidean/spherical methods; tested on synthetic data and multi-genus surfaces.


<details>
  <summary>Details</summary>
Motivation: OT is fundamental for comparing distributions; existing methods focus on Euclidean spaces and the sphere. Hyperbolic space naturally captures hierarchical/network data and multi-genus surfaces, motivating development of OT in hyperbolic geometry.

Method: Introduce a novel geometric variational framework to compute OT maps in hyperbolic space by adapting variational principles from Euclidean/spherical OT to hyperbolic geometry, leveraging hyperbolic distance, geodesics, and appropriate discretization for optimization.

Result: Empirical validation on synthetic data and multi-genus surface models demonstrates the method's effectiveness and efficiency in computing OT maps in hyperbolic space.

Conclusion: The work extends optimal transport to hyperbolic geometry with a practical algorithm, enabling applications to hierarchical and network-structured data and complex surfaces; future work includes theoretical guarantees, scalability, and broader experiments.

Abstract: The optimal transport (OT) problem aims to find the most efficient mapping
between two probability distributions under a given cost function, and has
diverse applications in many fields such as machine learning, computer vision
and computer graphics. However, existing methods for computing optimal
transport maps are primarily developed for Euclidean spaces and the sphere. In
this paper, we explore the problem of computing the optimal transport map in
hyperbolic space, which naturally arises in contexts involving hierarchical
data, networks, and multi-genus Riemann surfaces. We propose a novel and
efficient algorithm for computing the optimal transport map in hyperbolic space
using a geometric variational technique by extending methods for Euclidean and
spherical geometry to the hyperbolic setting. We also perform experiments on
synthetic data and multi-genus surface models to validate the efficacy of the
proposed method.

</details>


### [29] [Object-Aware 4D Human Motion Generation](https://arxiv.org/abs/2511.00248)
*Shurui Gui,Deep Anil Patel,Xiner Li,Martin Renqiang Min*

Main category: cs.CV

TL;DR: Zero-shot, object-aware 4D human motion generation using 3D Gaussian representations, diffusion priors, and MSDS guided by LLMs to produce physically plausible motions without retraining.


<details>
  <summary>Details</summary>
Motivation: Video diffusion models struggle to enforce 3D physical priors and object interactions, leading to deformations and semantic/physical violations. A scalable, zero-shot approach with 3D priors and object-awareness is needed.

Method: Pre-generated 3D humans and objects are used. The Motion Score Distilled Interaction (MSDI) framework combines spatial and prompt semantic information from LLMs with motion priors via Motion Diffusion Score Distillation Sampling (MSDS). It distills score gradients from pre-trained motion diffusion models to optimize motion while honoring object and semantic constraints, without joint retraining.

Result: The method yields natural, physically plausible human motions that respect 3D spatial context and object constraints, demonstrating a scalable solution for realistic 4D generation and generalization to out-of-distribution object-aware motions.

Conclusion: The paper presents a scalable zero-shot framework for object-aware 4D human motion generation that enforces 3D spatial and semantic constraints without retraining, offering a practical path toward realistic video diffusion-based 4D content.

Abstract: Recent advances in video diffusion models have enabled the generation of
high-quality videos. However, these videos still suffer from unrealistic
deformations, semantic violations, and physical inconsistencies that are
largely rooted in the absence of 3D physical priors. To address these
challenges, we propose an object-aware 4D human motion generation framework
grounded in 3D Gaussian representations and motion diffusion priors. With
pre-generated 3D humans and objects, our method, Motion Score Distilled
Interaction (MSDI), employs the spatial and prompt semantic information in
large language models (LLMs) and motion priors through the proposed Motion
Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs
enables our spatial-aware motion optimization, which distills score gradients
from pre-trained motion diffusion models, to refine human motion while
respecting object and semantic constraints. Unlike prior methods requiring
joint training on limited interaction datasets, our zero-shot approach avoids
retraining and generalizes to out-of-distribution object aware human motions.
Experiments demonstrate that our framework produces natural and physically
plausible human motions that respect 3D spatial context, offering a scalable
solution for realistic 4D generation.

</details>


### [30] [Merlin L48 Spectrogram Dataset](https://arxiv.org/abs/2511.00252)
*Aaron Sun,Subhransu Maji,Grant Van Horn*

Main category: cs.CV

TL;DR: Introduces L48, a real-world, fine-grained SPML dataset based on bird sounds, showing gaps between SPML methods and full supervision; includes two extended settings with domain priors for extra negative labels; benchmarks reveal weaknesses of existing SPML methods and the necessity for more realistic benchmarks.


<details>
  <summary>Details</summary>
Motivation: Bridge the performance gap between single-positive multi-label learning and fully supervised learning using realistic, difficult benchmarks. Synthetic SPML datasets fail to capture fine-grained misclassifications seen in real-world data.

Method: Construct L48 from bird sound recordings with single-positive annotations; present two extended settings enabling access to additional negative labels via domain priors; benchmark existing SPML methods on L48 and analyze weaknesses and performance differences versus synthetic datasets.

Result: Benchmark results on L48 show significant performance differences from synthetic SPML datasets and reveal weaknesses of current methods in realistic, fine-grained settings.

Conclusion: L48 provides a realistic, challenging benchmark for SPML, highlighting the need for methods that generalize to real-world data and offering extended settings with domain priors to further probe model behavior.

Abstract: In the single-positive multi-label (SPML) setting, each image in a dataset is
labeled with the presence of a single class, while the true presence of other
classes remains unknown. The challenge is to narrow the performance gap between
this partially-labeled setting and fully-supervised learning, which often
requires a significant annotation budget. Prior SPML methods were developed and
benchmarked on synthetic datasets created by randomly sampling single positive
labels from fully-annotated datasets like Pascal VOC, COCO, NUS-WIDE, and
CUB200. However, this synthetic approach does not reflect real-world scenarios
and fails to capture the fine-grained complexities that can lead to difficult
misclassifications. In this work, we introduce the L48 dataset, a fine-grained,
real-world multi-label dataset derived from recordings of bird sounds. L48
provides a natural SPML setting with single-positive annotations on a
challenging, fine-grained domain, as well as two extended settings in which
domain priors give access to additional negative labels. We benchmark existing
SPML methods on L48 and observe significant performance differences compared to
synthetic datasets and analyze method weaknesses, underscoring the need for
more realistic and difficult benchmarks.

</details>


### [31] [BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing](https://arxiv.org/abs/2511.00255)
*Fangxun Liu,S M Rayeed,Samuel Stevens,Alyson East,Cheng Hsuan Chiang,Colin Lee,Daniel Yi,Junke Yang,Tejas Naik,Ziyi Wang,Connor Kilrain,Elijah H Buckwalter,Jiacheng Hou,Saul Ibaven Bueno,Shuheng Wang,Xinyue Ma,Yifan Liu,Zhiyuan Tao,Ziheng Zhang,Eric Sokol,Michael Belitz,Sydne Record,Charles V. Stewart,Wei-Lun Chao*

Main category: cs.CV

TL;DR: A three-stage pipeline for automated detection, cropping, and fine-grained segmentation of beetles on trays, combining a transformer-based open-vocabulary detector with a vision-language model and fine-tuned transformer segmentation trained on 670 labeled images.


<details>
  <summary>Details</summary>
Motivation: Process thousands of tray images efficiently for entomology/ecology research by automating beetle detection and segmentation.

Method: Stage 1 iterative detection using a transformer-based open-vocabulary detector and a vision-language model; Stage 2 sort and crop per-beetle images; Stage 3 morphological segmentation by fine-tuning two transformer-based segmentation models on 670 labeled beetle images.

Result: The pipeline achieves relatively high accuracy for fine-grained segmentation and substantially improves efficiency in processing large-scale beetle datasets.

Conclusion: The specialized, multi-model pipeline effectively processes large-scale beetle imagery and can accelerate biological research.

Abstract: In entomology and ecology research, biologists often need to collect a large
number of insects, among which beetles are the most common species. A common
practice for biologists to organize beetles is to place them on trays and take
a picture of each tray. Given the images of thousands of such trays, it is
important to have an automated pipeline to process the large-scale data for
further research. Therefore, we develop a 3-stage pipeline to detect all the
beetles on each tray, sort and crop the image of each beetle, and do
morphological segmentation on the cropped beetles. For detection, we design an
iterative process utilizing a transformer-based open-vocabulary object detector
and a vision-language model. For segmentation, we manually labeled 670 beetle
images and fine-tuned two variants of a transformer-based segmentation model to
achieve fine-grained segmentation of beetles with relatively high accuracy. The
pipeline integrates multiple deep learning methods and is specialized for
beetle image processing, which can greatly improve the efficiency to process
large-scale beetle data and accelerate biological research.

</details>


### [32] [MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba](https://arxiv.org/abs/2511.00260)
*Linzhe Jiang,Jiayuan Huang,Sophia Bano,Matthew J. Clarkson,Zhehua Mao,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: A novel 3D registration framework for endoscopic navigation, MambaNetLK, uses a Mamba State Space Model to extract cross-modal features within PointNetLK, enabling efficient, long-range dependency capture; evaluated on a large clinical dataset (C3VD-Raycasting-10k) with strong accuracy and robustness, suitable for colonoscopy guidance.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D point cloud registration is critical for image-guided colonoscopy, but tissue textures cause feature degeneracy and domain shifts between pre-operative and intra-operative anatomy, undermining alignment stability. A scalable, robust method and benchmarking dataset are needed for reliable surgical navigation.

Method: Proposes MambaNetLK, a correspondence-free registration framework that enhances PointNetLK with a Mamba State Space Model as a cross-modal feature extractor to capture long-range dependencies with linear-time complexity. Alignment is performed iteratively via the Lucas-Kanade algorithm. Evaluation is conducted on a large clinical dataset for colonoscopy.

Result: On C3VD-Raycasting-10k, MambaNetLK achieves state-of-the-art performance, reducing median rotation error by 56.04% and RMSE translation error by 26.19% over the second-best method. The model generalizes to ModelNet40 and shows robustness to initial pose perturbations.

Conclusion: The global expressiveness of the SSM-based feature extractor combined with a large-scale clinical dataset provides a robust foundation for 3D registration in surgical navigation, enabling more accurate and reliable guidance in minimally invasive procedures like colonoscopy.

Abstract: Accurate 3D point cloud registration underpins reliable image-guided
colonoscopy, directly affecting lesion localization, margin assessment, and
navigation safety. However, biological tissue exhibits repetitive textures and
locally homogeneous geometry that cause feature degeneracy, while substantial
domain shifts between pre-operative anatomy and intra-operative observations
further degrade alignment stability. To address these clinically critical
challenges, we introduce a novel 3D registration method tailored for endoscopic
navigation and a high-quality, clinically grounded dataset to support rigorous
and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale
benchmark dataset with 10,014 geometrically aligned point cloud pairs derived
from clinical CT data. We propose MambaNetLK, a novel correspondence-free
registration framework, which enhances the PointNetLK architecture by
integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor.
As a result, the proposed framework efficiently captures long-range
dependencies with linear-time complexity. The alignment is achieved iteratively
using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k,
MambaNetLK achieves the best performance compared with the state-of-the-art
methods, reducing median rotation error by 56.04% and RMSE translation error by
26.19% over the second-best method. The model also demonstrates strong
generalization on ModelNet40 and superior robustness to initial pose
perturbations. MambaNetLK provides a robust foundation for 3D registration in
surgical navigation. The combination of a globally expressive SSM-based feature
extractor and a large-scale clinical dataset enables more accurate and reliable
guidance systems in minimally invasive procedures like colonoscopy.

</details>


### [33] [Spot The Ball: A Benchmark for Visual Social Inference](https://arxiv.org/abs/2511.00261)
*Neha Balamurugan,Sarah Wu,Adam Chun,Gabe Gaw,Cristobal Eyzaguirre,Tobias Gerstenberg*

Main category: cs.CV

TL;DR: Humans outperform vision-language models on the Spot The Ball benchmark; models rely on simple heuristics while humans exploit social cues.


<details>
  <summary>Details</summary>
Motivation: Visual social inference is a core component of human-like AI reasoning; current vision-language models struggle to infer hidden social elements in scenes and need architectures that encode structured behavioral cues.

Method: Introduce Spot The Ball dataset: a benchmark to localize a removed sports ball in soccer, basketball, and volleyball images. Curated evaluation set with human baselines and a scalable pipeline to generate more items. Evaluate four VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting strategies.

Result: Humans achieve 20â34% accuracy, while models achieve â¤17% across all sports; humans are consistently 2â3x more accurate than models. Analyses show models rely on superficial spatial heuristics (e.g., center of image or proximity to players) whereas humans leverage social cues like gaze direction and body pose.

Conclusion: There remains a persistent gap between human and model visual social reasoning. Advancing toward human-like inference will require architectures that explicitly encode and exploit structured behavioral cues such as gaze and pose.

Abstract: Humans excel at visual social inference, the ability to infer hidden elements
of a scene from subtle behavioral cues such as other people's gaze, pose, and
orientation. This ability drives everyday social reasoning in humans and is
critical for developing more human-like AI agents. We introduce Spot The Ball,
a challenging benchmark for evaluating visual social inference in
vision-language models (VLMs) using sports as a test domain. The task is to
localize a removed sports ball from soccer, basketball, and volleyball images.
We present a curated evaluation set with human baselines and a scalable
pipeline for generating additional test items. We evaluate four
state-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting
strategies, finding that humans are consistently two to three times more
accurate (20-34%) than models ($\leq$ 17%) across all sports. Our analyses show
that models rely on superficial spatial heuristics--such as guessing near the
image center or nearby players--while humans leverage social cues like gaze
direction and body pose. These findings reveal a persistent human-model gap in
visual social reasoning and underscore the need for architectures that
explicitly encode structured behavioral cues to achieve robust, human-like
inference.

</details>


### [34] [FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture](https://arxiv.org/abs/2511.00269)
*Long Li,Jiajia Li,Dong Chen,Lina Pu,Haibo Yao,Yanbo Huang*

Main category: cs.CV

TL;DR: A privacy-preserving federated learning approach for agricultural classification using frozen CLIP ViT features with a small shared feature subset, achieving 86.6% accuracy and over 4x improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: Address privacy and data leakage concerns in centralized training; reduce communication costs and improve performance of federated learning on non-IID agricultural data.

Method: Freeze a CLIP Vision Transformer as a fixed feature extractor and train a lightweight transformer classifier on the client; limit federated updates to a compact classifier; share a small subset (1%) of CLIP-extracted features across clients to align class representations while preserving privacy.

Result: On agricultural classification tasks, achieves 86.6% accuracy, more than 4x higher than baseline federated learning approaches.

Conclusion: Integrating vision-language pre-trained features with federated learning yields privacy-preserving, scalable, and efficient agricultural intelligence with strong performance.

Abstract: Accurate classification plays a pivotal role in smart agriculture, enabling
applications such as crop monitoring, fruit recognition, and pest detection.
However, conventional centralized training often requires large-scale data
collection, which raises privacy concerns, while standard federated learning
struggles with non-independent and identically distributed (non-IID) data and
incurs high communication costs. To address these challenges, we propose a
federated learning framework that integrates a frozen Contrastive
Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight
transformer classifier. By leveraging the strong feature extraction capability
of the pre-trained CLIP ViT, the framework avoids training large-scale models
from scratch and restricts federated updates to a compact classifier, thereby
reducing transmission overhead significantly. Furthermore, to mitigate
performance degradation caused by non-IID data distribution, a small subset
(1%) of CLIP-extracted feature representations from all classes is shared
across clients. These shared features are non-reversible to raw images,
ensuring privacy preservation while aligning class representation across
participants. Experimental results on agricultural classification tasks show
that the proposed method achieve 86.6% accuracy, which is more than 4 times
higher compared to baseline federated learning approaches. This demonstrates
the effectiveness and efficiency of combining vision-language model features
with federated learning for privacy-preserving and scalable agricultural
intelligence.

</details>


### [35] [Multi-View Consistent Human Image Customization via In-Context Learning](https://arxiv.org/abs/2511.00293)
*Hengjia Li,Jianjin Xu,Keli Cheng,Lei Wang,Ning Bi,Boxi Wu,Fernando De la Torre,Deng Cai*

Main category: cs.CV

TL;DR: PersonalView is a lightweight adaptation that enables an existing personalized diffusion model to generate multi-view (viewpoint-controlled) identity-consistent images using as few as 100 training samples, outperforming baselines trained on large multi-view datasets.


<details>
  <summary>Details</summary>
Motivation: Current personalized generative models struggle to control the viewpoint and produce consistent multiple views of the same identity, making true multi-view identity editing data-inefficient. A data-efficient, plug-in method is needed to equip models with multi-view generation while preserving original capabilities.

Method: A two-part approach: (1) a conditioning architecture that leverages the in-context learning ability of the pre-trained diffusion transformer to incorporate viewpoint control; (2) a Semantic Correspondence Alignment Loss to preserve the original generative ability while enabling multi-view consistency. Training requires as few as 100 samples, and evaluation covers multi-view consistency, text alignment, identity similarity, and visual quality, compared against baselines.

Result: PersonalView significantly outperforms baselines trained on large multi-view data in terms of multi-view consistency, text alignment, identity similarity, and visual quality, despite using only 100 training samples.

Conclusion: PersonalView offers a data-efficient, plug-in adaptation that endows existing personalized diffusion models with robust, identity-consistent multi-view generation, enabling scalable multi-view customization with minimal data.

Abstract: Recent advances in personalized generative models demonstrate impressive
results in creating identity-consistent images of the same person under diverse
settings. Yet, we note that most methods cannot control the viewpoint of the
generated image, nor generate consistent multiple views of the person. To
address this problem, we propose a lightweight adaptation method, PersonalView,
capable of enabling an existing model to acquire multi-view generation
capability with as few as 100 training samples. PersonalView consists of two
key components: First, we design a conditioning architecture to take advantage
of the in-context learning ability of the pre-trained diffusion transformer.
Second, we preserve the original generative ability of the pretrained model
with a new Semantic Correspondence Alignment Loss. We evaluate the multi-view
consistency, text alignment, identity similarity, and visual quality of
PersonalView and compare it to recent baselines with potential capability of
multi-view customization. PersonalView significantly outperforms baselines
trained on a large corpus of multi-view data with only 100 training samples.

</details>


### [36] [Towards Automated Petrography](https://arxiv.org/abs/2511.00328)
*Isai Daniel ChacÃ³n,Paola Ruiz Puentes,Jillian Pearse,Pablo ArbelÃ¡ez*

Main category: cs.CV

TL;DR: A large-scale, public petrography benchmark called LITHOS is introduced, featuring over 211k polarized RGB patches and 105k expert-annotated mineral grains across 25 classes. The paper proposes a dual-encoder transformer that fuses two polarization modalities for mineral classification, showing improved performance over single-modality models. The dataset, code, and pretrained models are released to enable reproducibility and further research in automated petrography.


<details>
  <summary>Details</summary>
Motivation: Petrography is labor-intensive, requiring expert visual examination under polarized light microscopes. There is a need for scalable, automated approaches and large, diverse public datasets to advance automated petrographic analysis.

Method: The authors present LITHOS, a large-scale imaging and thin-section optical-polarization dataset with 211,604 high-resolution RGB patches and 105,802 expert-annotated grains in 25 mineral categories. Each annotation includes mineral class, spatial coordinates, and grain geometry/orientation. They evaluate multiple deep learning models for mineral classification and introduce a dual-encoder transformer that fuses two polarization modalities as a strong baseline.

Result: The dual-encoder transformer leveraging dual polarization modalities outperforms single-polarization models, demonstrating the value of polarization synergy for mineral classification. The LITHOS benchmark and pretrained models outperform baselines and are publicly released to support replication and further research.

Conclusion: LITHOS provides a comprehensive, public resource for automated petrographic analysis, enabling reproducible research and setting a strong baseline for multi-modal polarization-based mineral classification. The dataset, code, and pretrained models are released to accelerate progress in automated petrography.

Abstract: Petrography is a branch of geology that analyzes the mineralogical
composition of rocks from microscopical thin section samples. It is essential
for understanding rock properties across geology, archaeology, engineering,
mineral exploration, and the oil industry. However, petrography is a
labor-intensive task requiring experts to conduct detailed visual examinations
of thin section samples through optical polarization microscopes, thus
hampering scalability and highlighting the need for automated techniques. To
address this challenge, we introduce the Large-scale Imaging and Thin section
Optical-polarization Set (LITHOS), the largest and most diverse publicly
available experimental framework for automated petrography. LITHOS includes
211,604 high-resolution RGB patches of polarized light and 105,802
expert-annotated grains across 25 mineral categories. Each annotation consists
of the mineral class, spatial coordinates, and expert-defined major and minor
axes represented as intersecting vector paths, capturing grain geometry and
orientation. We evaluate multiple deep learning techniques for mineral
classification in LITHOS and propose a dual-encoder transformer architecture
that integrates both polarization modalities as a strong baseline for future
reference. Our method consistently outperforms single-polarization models,
demonstrating the value of polarization synergy in mineral classification. We
have made the LITHOS Benchmark publicly available, comprising our dataset,
code, and pretrained models, to foster reproducibility and further research in
automated petrographic analysis.

</details>


### [37] [Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models](https://arxiv.org/abs/2511.00335)
*Weidong Zhang,Pak Lun Kevin Ding,Huan Liu*

Main category: cs.CV

TL;DR: Cross-dataset evaluation shows ImageNet accuracy is not a reliable predictor of cross-domain generalization for lightweight vision models. The xScore metric quantifies robustness across domains across 11 models and 7 datasets. Isotropic convolutions with higher spatial resolution and channel-wise attention improve generalization, while Transformer blocks add little benefit for resource-constrained settings.


<details>
  <summary>Details</summary>
Motivation: To quantify and understand how well lightweight vision models generalize across diverse visual domains beyond ImageNet, addressing deployment reliability for mobile/embedded systems.

Method: Systematic evaluation of 11 lightweight models (~2.5M parameters) trained for 100 epochs across 7 diverse datasets. Introduction of Cross-Dataset Score (xScore) as a unified robustness metric. Analysis of architectural components (isotropic convolutions, spatial resolution, channel-wise attention) and comparison with Transformer-based blocks.

Result: ImageNet accuracy does not reliably predict performance on fine-grained or medical datasets. xScore can be estimated from as few as four datasets, enabling scalable cross-domain evaluation. Architectural componentsâisotropic convolutions with higher spatial resolution and channel-wise attentionâenhance cross-domain generalization; Transformer-based blocks offer little extra benefit and incur higher parameter costs.

Conclusion: Provides a reproducible framework for evaluating lightweight vision models beyond ImageNet, identifies design principles for mobile-friendly architectures, and guides development toward models with robust cross-domain generalization.

Abstract: Lightweight vision classification models such as MobileNet, ShuffleNet, and
EfficientNet are increasingly deployed in mobile and embedded systems, yet
their performance has been predominantly benchmarked on ImageNet. This raises
critical questions: Do models that excel on ImageNet also generalize across
other domains? How can cross-dataset robustness be systematically quantified?
And which architectural elements consistently drive generalization under tight
resource constraints? Here, we present the first systematic evaluation of 11
lightweight vision models (2.5M parameters), trained under a fixed 100-epoch
schedule across 7 diverse datasets. We introduce the Cross-Dataset Score
(xScore), a unified metric that quantifies the consistency and robustness of
model performance across diverse visual domains. Our results show that (1)
ImageNet accuracy does not reliably predict performance on fine-grained or
medical datasets, (2) xScore provides a scalable predictor of mobile model
performance that can be estimated from just four datasets, and (3) certain
architectural components--such as isotropic convolutions with higher spatial
resolution and channel-wise attention--promote broader generalization, while
Transformer-based blocks yield little additional benefit, despite incurring
higher parameter overhead. This study provides a reproducible framework for
evaluating lightweight vision models beyond ImageNet, highlights key design
principles for mobile-friendly architectures, and guides the development of
future models that generalize robustly across diverse application domains.

</details>


### [38] [A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction](https://arxiv.org/abs/2511.00338)
*Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: A hybrid DeepONet-NTK framework for inverse problems that uses physics-informed loss to handle nonlinearity, sparsity, and noise, demonstrated on Navier-Stokes-based source localization and image reconstruction with synthetic and real data.


<details>
  <summary>Details</summary>
Motivation: Inverse problems in physics and imaging are highly nonlinear and data-sparse/noisy; there is a need for scalable, physics-consistent operator-learning methods that can leverage both neural operators and kernel-based regularization.

Method: Integrate DeepONet (operator learning) with Neural Tangent Kernel (NTK) to impose kernel-based regularization and stability. Embed physics-informed constraints (e.g., NavierâStokes equations) and task-specific regularization into the training loss to enforce physical consistency and improve accuracy for inverse mappings.

Result: The approach shows robustness to nonlinearity, sparsity, and noise, with accurate source localization and high-quality image reconstruction across synthetic and real datasets, indicating strong scalability and precision.

Conclusion: The hybrid DeepONet-NTK framework provides a versatile, physics-informed tool for challenging inverse problems in computational physics and imaging, with potential for extension to additional PDEs, applications, and more complex datasets.

Abstract: This work presents a novel hybrid approach that integrates Deep Operator
Networks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex
inverse problem. The method effectively addresses tasks such as source
localization governed by the Navier-Stokes equations and image reconstruction,
overcoming challenges related to nonlinearity, sparsity, and noisy data. By
incorporating physics-informed constraints and task-specific regularization
into the loss function, the framework ensures solutions that are both
physically consistent and accurate. Validation on diverse synthetic and real
datasets demonstrates its robustness, scalability, and precision, showcasing
its broad potential applications in computational physics and imaging sciences.

</details>


### [39] [Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities](https://arxiv.org/abs/2511.00344)
*Xihang Qiu,Jiarong Cheng,Yuhao Fang,Wanpeng Zhang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: FedDISC introduces federated diffusion-based missing-modality recovery for multimodal emotion recognition in conversations, achieving semantic-consistent recovery and superior performance when modalities are missing.


<details>
  <summary>Details</summary>
Motivation: To address unpredictable modality absence in multimodal emotion recognition and overcome semantic distortion and single-client reliance of full-data-trained missing-modality recovery methods.

Method: Federated aggregation of modality-specific diffusion models trained on clients, broadcasting to clients missing corresponding modalities. The DISC-Diffusion module enforces context, speaker identity, and semantic alignment via a Dialogue Graph Network and a Semantic Conditioning Network. An Alternating Frozen Aggregation strategy cycles freezing of recovery and classifier modules to facilitate collaborative optimization.

Result: Experiments on IEMOCAP, CMUMOSI, and CMUMOSEI show FedDISC achieves superior emotion classification performance across diverse missing modality patterns, outperforming existing approaches.

Conclusion: FedDISC provides robust, semantically consistent missing-modality recovery in a federated setting, enabling collaborative optimization and improved MERC performance when modalities are incomplete.

Abstract: Multimodal Emotion Recognition in Conversations (MERC) enhances emotional
understanding through the fusion of multimodal signals. However, unpredictable
modality absence in real-world scenarios significantly degrades the performance
of existing methods. Conventional missing-modality recovery approaches, which
depend on training with complete multimodal data, often suffer from semantic
distortion under extreme data distributions, such as fixed-modality absence. To
address this, we propose the Federated Dialogue-guided and Semantic-Consistent
Diffusion (FedDISC) framework, pioneering the integration of federated learning
into missing-modality recovery. By federated aggregation of modality-specific
diffusion models trained on clients and broadcasting them to clients missing
corresponding modalities, FedDISC overcomes single-client reliance on modality
completeness. Additionally, the DISC-Diffusion module ensures consistency in
context, speaker identity, and semantics between recovered and available
modalities, using a Dialogue Graph Network to capture conversational
dependencies and a Semantic Conditioning Network to enforce semantic alignment.
We further introduce a novel Alternating Frozen Aggregation strategy, which
cyclically freezes recovery and classifier modules to facilitate collaborative
optimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI
datasets demonstrate that FedDISC achieves superior emotion classification
performance across diverse missing modality patterns, outperforming existing
approaches.

</details>


### [40] [OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data](https://arxiv.org/abs/2511.00345)
*Amir Ziashahabi,Narges Ghasemi,Sajjad Shahabi,John Krumm,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: Generates realistic satellite imagery from raw OSM data and produces consistent before-after pairs for urban-change simulation; supports data augmentation and planning previews; open-source.


<details>
  <summary>Details</summary>
Motivation: Scarcity of curated, up-to-date urban datasets hinders automated urban monitoring; need controllable synthetic data and a way to visualize edits.

Method: OSMGen consumes full OSM JSON (vector geometries, semantic tags, location, time) to render imagery; unlike raster-tile methods; translates user edits in OSM into visual changes; emits paired JSON+image data for static and changed states.

Result: Realistic, controllable imagery generation with consistent before-after pairs, enabling targeted data generation and planning previews; supports a potential closed-loop with imagery driving OSM updates.

Conclusion: Code released at GitHub; framework addresses data scarcity and imbalance in urban monitoring and supports broader planning and environmental applications.

Abstract: Accurate and up-to-date geospatial data are essential for urban planning,
infrastructure monitoring, and environmental management. Yet, automating urban
monitoring remains difficult because curated datasets of specific urban
features and their changes are scarce. We introduce OSMGen, a generative
framework that creates realistic satellite imagery directly from raw
OpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen
uses the full richness of OSM JSON, including vector geometries, semantic tags,
location, and time, giving fine-grained control over how scenes are generated.
A central feature of the framework is the ability to produce consistent
before-after image pairs: user edits to OSM inputs translate into targeted
visual changes, while the rest of the scene is preserved. This makes it
possible to generate training data that addresses scarcity and class imbalance,
and to give planners a simple way to preview proposed interventions by editing
map data. More broadly, OSMGen produces paired (JSON, image) data for both
static and changed states, paving the way toward a closed-loop system where
satellite imagery can automatically drive structured OSM updates. Source code
is available at https://github.com/amir-zsh/OSMGen.

</details>


### [41] [Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach](https://arxiv.org/abs/2511.00352)
*Mohd Ruhul Ameen,Akif Islam*

Main category: cs.CV

TL;DR: A diffusion-based forensic method, diffusion snap-back, analyzes image reconstruction dynamics across multiple noise strengths to distinguish real and AI-generated images; achieves near-perfect AUROC and shows robustness and interpretability.


<details>
  <summary>Details</summary>
Motivation: As generative diffusion models become dominant, distinguishing authentic visual content from synthetic imagery becomes increasingly challenging for traditional detectors, which often rely on artifact patterns that diffusion models can eliminate. There is a need for a model-agnostic, interpretable, and scalable forensic approach that leverages the diffusion process itself.

Method: Utilizes multi-strength diffusion-based image reconstruction dynamics. Tracks reconstruction metrics (LPIPS, SSIM, PSNR) as noise strength varies, extracting manifold-based features that differentiate real from synthetic images. Evaluated with Stable Diffusion v1.5 as the backbone on a balanced dataset of 4,000 images, employing cross-validation.

Result: Achieves 0.993 AUROC under cross-validation on the 4,000-image dataset. Demonstrates robustness to common distortions such as compression and noise. Uses limited data and a single diffusion backbone yet shows strong generalization and interpretability, suggesting potential for scalable, model-agnostic synthetic media forensics.

Conclusion: Diffusion-based reconstruction dynamics offer a strong, interpretable basis for synthetic media forensics, enabling high-performance detection with limited data and supporting a scalable, model-agnostic forensic framework.

Abstract: The rapid rise of generative diffusion models has made distinguishing
authentic visual content from synthetic imagery increasingly challenging.
Traditional deepfake detection methods, which rely on frequency or pixel-level
artifacts, fail against modern text-to-image systems such as Stable Diffusion
and DALL-E that produce photorealistic and artifact-free results. This paper
introduces a diffusion-based forensic framework that leverages multi-strength
image reconstruction dynamics, termed diffusion snap-back, to identify
AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and
PSNR) evolve across varying noise strengths, we extract interpretable
manifold-based features that differentiate real and synthetic images. Evaluated
on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under
cross-validation and remains robust to common distortions such as compression
and noise. Despite using limited data and a single diffusion backbone (Stable
Diffusion v1.5), the proposed method demonstrates strong generalization and
interpretability, offering a foundation for scalable, model-agnostic synthetic
media forensics.

</details>


### [42] [Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation](https://arxiv.org/abs/2511.00357)
*Niklas WÃ¶lki,Lukas Kondmann,Christian MolliÃ¨re,Martin Langer,Julia Gottfriedsen,Martin Werner*

Main category: cs.CV

TL;DR: Transfer-learning-based thermal cloud segmentation for CubeSats using a lightweight UNet with MobileNet encoder. Pretrained on Landsat-7 Cloud Cover Assessment, fine-tuned with limited mission data, achieving macro F1 improvement (0.850 -> 0.877). TensorRT conversion enables on-device inference <5s on Jetson Nano.


<details>
  <summary>Details</summary>
Motivation: Address onboard cloud masking for thermal EO on CubeSats with limited hardware and labeled data. Leverage public datasets and lightweight architectures to enable real-time, thermal-only cloud segmentation.

Method: UNet with MobileNet encoder; pretrain on Landsat-7 Cloud Cover Assessment dataset; fine-tune with a small FOREST-2-specific sample set in a joint-training setup; convert to TensorRT engine for full-image inference on an NVIDIA Jetson Nano.

Result: Macro F1 improved from 0.850 (FOREST-2 baseline) to 0.877; on-device inference under 5 seconds for full-image inference on Jetson Nano.

Conclusion: Public datasets and lightweight architectures enable accurate, efficient thermal-only cloud masking on-orbit, supporting real-time decision-making in data-limited EO missions.

Abstract: Onboard cloud segmentation is a critical yet underexplored task in thermal
Earth observation (EO), particularly for CubeSat missions constrained by
limited hardware and spectral information. CubeSats often rely on a single
thermal band and lack sufficient labeled data, making conventional cloud
masking techniques infeasible. This work addresses these challenges by applying
transfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, using
a UNet with a lightweight MobileNet encoder. We pretrain the model on the
public Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a small
set of mission-specific samples in a joint-training setup, improving the macro
F1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to a
TensorRT engine and demonstrate full-image inference in under 5 seconds on an
NVIDIA Jetson Nano. These results show that leveraging public datasets and
lightweight architectures can enable accurate, efficient thermal-only cloud
masking on-orbit, supporting real-time decision-making in data-limited EO
missions.

</details>


### [43] [Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery](https://arxiv.org/abs/2511.00362)
*Momen Khandoker Ope,Akif Islam,Mohd Ruhul Ameen,Abu Saleh Musa Miah,Md Rashedul Islam,Jungpil Shin*

Main category: cs.CV

TL;DR: Democratizes 3D cultural heritage preservation in Bangladesh by leveraging free AI tools and publicly available imagery to generate fast, photorealistic 3D models with minimal hardware/expertise.


<details>
  <summary>Details</summary>
Motivation: Resource-limited contexts face limited access to traditional 3D digitization. The work aims to safeguard Bangladesh's architectural heritage (e.g., Paharpur, Ahsan Manzil) by lowering economic/technical barriers and enabling community-driven digital preservation.

Method: A two-stage pipeline: (1) multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture synthesis; (2) neural image-to-3D geometry recovery with Hexagen, using publicly available Google Street View imagery to reconstruct metrically coherent 3D models; no specialized hardware or expert supervision required.

Result: Produces photorealistic, metrically coherent reconstructions in seconds; scalable speedups compared to traditional Structure-from-Motion; preserves visual and structural fidelity for landmarks like Ahsan Manzil, Choto Sona Mosque, Paharpur; democratizes access to digital heritage.

Conclusion: Reconception of preservation as a community-driven, AI-assisted practice, enabling resource-limited nations to digitally preserve and share cultural heritage using open imagery.

Abstract: Cultural heritage restoration in Bangladesh faces a dual challenge of limited
resources and scarce technical expertise. Traditional 3D digitization methods,
such as photogrammetry or LiDAR scanning, require expensive hardware, expert
operators, and extensive on-site access, which are often infeasible in
developing contexts. As a result, many of Bangladesh's architectural treasures,
from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to
decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a
cost-free generative AI framework that democratizes 3D cultural preservation.
By using publicly available Google Street View imagery, Oitijjo-3D reconstructs
faithful 3D models of heritage structures through a two-stage pipeline -
multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture
synthesis, and neural image-to-3D generation through Hexagen for geometry
recovery. The system produces photorealistic, metrically coherent
reconstructions in seconds, achieving significant speedups compared to
conventional Structure-from-Motion pipelines, without requiring any specialized
hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,
Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both
visual and structural fidelity while drastically lowering economic and
technical barriers. By turning open imagery into digital heritage, this work
reframes preservation as a community-driven, AI-assisted act of cultural
continuity for resource-limited nations.

</details>


### [44] [Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict](https://arxiv.org/abs/2511.00370)
*Chaochen Wu,Guan Luo,Meiyun Zuo,Zhitao Fan*

Main category: cs.CV

TL;DR: A reinforcement learningâbased video moment retrieval framework with a multi-agent evidential system that resolves conflicts among localizations and can detect out-of-scope queries without extra training, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods ignore conflicts between modelsâ localization outputs and struggle with queries that have no corresponding moment; there is a need for a model that can scan efficiently and handle uncertain results in real-world settings.

Method: A reinforcement learning model that scans the entire video once to locate moment boundaries and provide localization evidence; a multi-agent framework with evidential learning to resolve conflicts among agentsâ outputs; the system can also determine when a query is out-of-scope.

Result: Experiments on benchmark datasets show the approach is effective compared to state-of-the-art methods; modeling competition and conflict among agents improves RL performance; evidential learning plays a new role in the multi-agent framework.

Conclusion: The approach offers a conflict-aware RL solution for video moment retrieval, enabling out-of-scope detection without extra training and improving retrieval accuracy by leveraging multi-agent competition and evidential reasoning.

Abstract: Video moment retrieval uses a text query to locate a moment from a given
untrimmed video reference. Locating corresponding video moments with text
queries helps people interact with videos efficiently. Current solutions for
this task have not considered conflict within location results from different
models, so various models cannot integrate correctly to produce better results.
This study introduces a reinforcement learning-based video moment retrieval
model that can scan the whole video once to find the moment's boundary while
producing its locational evidence. Moreover, we proposed a multi-agent system
framework that can use evidential learning to resolve conflicts between agents'
localization output. As a side product of observing and dealing with conflicts
between agents, we can decide whether a query has no corresponding moment in a
video (out-of-scope) without additional training, which is suitable for
real-world applications. Extensive experiments on benchmark datasets show the
effectiveness of our proposed methods compared with state-of-the-art
approaches. Furthermore, the results of our study reveal that modeling
competition and conflict of the multi-agent system is an effective way to
improve RL performance in moment retrieval and show the new role of evidential
learning in the multi-agent framework.

</details>


### [45] [VisionCAD: An Integration-Free Radiology Copilot Framework](https://arxiv.org/abs/2511.00381)
*Jiaming Li,Junlei Wu,Sheng Wang,Honglin Xiong,Jiangdong Cai,Zihao Zhao,Yitao Zhu,Yuan Yin,Dinggang Shen,Qian Wang*

Main category: cs.CV

TL;DR: VisionCAD enables AI-assisted diagnosis by capturing on-screen medical images via a camera, transforming them into diagnostic-quality data without changing hospital IT infrastructure; shows near-parity with conventional CAD across tasks (F1 < 2% degradation; NLP report metrics within 1%), using a modular pipeline and standard hardware.


<details>
  <summary>Details</summary>
Motivation: Integrating CAD systems into hospital IT is a major barrier; many clinics lack the infrastructure for direct digital image access. A camera-based capture approach offers a practical, low-barrier alternative for deploying AI diagnostics.

Method: An automated pipeline that detects, restores, and analyzes on-screen medical images captured from displays by a camera system. The approach converts camera-captured visuals into diagnostic-quality images and reports, and employs a modular architecture to plug in state-of-the-art diagnostic models for task-specific analyses.

Result: Across diverse medical imaging datasets, VisionCAD achieved diagnostic performance comparable to CAD systems operating on original digital images, with F1-score degradation typically less than 2% across classification tasks. Automated report generation NLP metrics remained within 1% of those derived from original images.

Conclusion: VisionCAD provides an accessible, infrastructure-light path to AI-assisted diagnosis by leveraging off-the-shelf camera hardware and standard computing resources, enabling deployment of diagnostic capabilities in diverse clinical settings without modifications to existing IT infrastructure.

Abstract: Widespread clinical deployment of computer-aided diagnosis (CAD) systems is
hindered by the challenge of integrating with existing hospital IT
infrastructure. Here, we introduce VisionCAD, a vision-based radiological
assistance framework that circumvents this barrier by capturing medical images
directly from displays using a camera system. The framework operates through an
automated pipeline that detects, restores, and analyzes on-screen medical
images, transforming camera-captured visual data into diagnostic-quality images
suitable for automated analysis and report generation. We validated VisionCAD
across diverse medical imaging datasets, demonstrating that our modular
architecture can flexibly utilize state-of-the-art diagnostic models for
specific tasks. The system achieves diagnostic performance comparable to
conventional CAD systems operating on original digital images, with an F1-score
degradation typically less than 2\% across classification tasks, while natural
language generation metrics for automated reports remain within 1\% of those
derived from original images. By requiring only a camera device and standard
computing resources, VisionCAD offers an accessible approach for AI-assisted
diagnosis, enabling the deployment of diagnostic capabilities in diverse
clinical settings without modifications to existing infrastructure.

</details>


### [46] [Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond](https://arxiv.org/abs/2511.00389)
*Fan Zhang,Haoxuan Li,Shengju Qian,Xin Wang,Zheng Lian,Hao Wu,Zhihong Zhu,Yuan Gao,Qiankun Li,Yefeng Zheng,Zhouchen Lin,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: FERBench benchmarks 20 MLLMs on 4 FER datasets; UniFER-7B, built with UniFER-CoT-230K and UniFER-RLVR-360K, achieves strong FER performance and interpretability, surpassing many generalist MLLMs.


<details>
  <summary>Details</summary>
Motivation: Bridge facial expression recognition (FER) with multimodal large language models (MLLMs), aiming to unify FER tasks, improve reasoning and interpretability, and leverage VQA-style data conversion and post-training.

Method: Convert FER datasets into visual question-answering formats to evaluate 20 state-of-the-art MLLMs across four FER datasets (FERBench). Introduce two post-training strategies: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K for reinforcement learning with verifiable rewards. Build UniFER-7B as a unified FER foundation model.

Result: MLLMs show good classification performance but have significant limitations in reasoning and interpretability on FER tasks. UniFER-7B outperforms many open-sourced and closed-source generalist MLLMs (e.g., Gemini-2.5-Pro, Qwen2.5-VL-72B).

Conclusion: A unified, interpretable FER foundation model can be built by curated data and RL-based post-training; UniFER-7B demonstrates strong performance and sets a new benchmark for MLLMs in FER tasks.

Abstract: Multimodal Large Language Models (MLLMs) have revolutionized numerous
research fields, including computer vision and affective computing. As a
pivotal challenge in this interdisciplinary domain, facial expression
recognition (FER) has evolved from separate, domain-specific models to more
unified approaches. One promising avenue to unify FER tasks is converting
conventional FER datasets into visual question-answering (VQA) formats,
enabling the direct application of powerful generalist MLLMs for inference.
However, despite the success of cutting-edge MLLMs in various tasks, their
performance on FER tasks remains largely unexplored. To address this gap, we
provide FERBench, a systematic benchmark that incorporates 20 state-of-the-art
MLLMs across four widely used FER datasets. Our results reveal that, while
MLLMs exhibit good classification performance, they still face significant
limitations in reasoning and interpretability. To this end, we introduce
post-training strategies aimed at enhancing the facial expression reasoning
capabilities of MLLMs. Specifically, we curate two high-quality and large-scale
datasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K
for reinforcement learning with verifiable rewards (RLVR), respectively.
Building upon them, we develop a unified and interpretable FER foundation model
termed UniFER-7B, which outperforms many open-sourced and closed-source
generalist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).

</details>


### [47] [VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning](https://arxiv.org/abs/2511.00391)
*Xuanle Zhao,Deyang Jiang,Zhixiong Zeng,Lei Chen,Haibo Qiu,Jing Huang,Yufeng Zhong,Liming Zheng,Yilin Cao,Lin Ma*

Main category: cs.CV

TL;DR: VinciCoder is a unified multimodal code-generation model trained via a two-stage pipeline: large-scale supervised finetuning on 1.6M image-code pairs and a Visual Reinforcement Learning (ViRL) phase with coarse-to-fine visual rewards to boost image-code alignment and visual fidelity, achieving state-of-the-art results on multimodal code-generation benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models trained on single tasks struggle to generalize across diverse multimodal coding tasks (image-to-code, code refinement from visuals). There is a need for a unified, generalizable VisionâCode Intelligence that can handle varied multimodal programming tasks.

Method: 1) Build a large SFT corpus of 1.6 million image-code pairs for direct code generation and visual-based code refinement. 2) Apply a Visual Reinforcement Learning (ViRL) phase with a coarse-to-fine reward that measures visual similarity at both local and global image patches to improve visual fidelity and alignment with code generation.

Result: The approach yields state-of-the-art performance on multiple multimodal code-generation benchmarks, demonstrating the efficacy of the coarse-to-fine ViRL strategy in enhancing both accuracy and visual alignment.

Conclusion: A two-stage training framework combining large-scale supervised finetuning with a coarse-to-fine visual RL strategy can produce a unified VisionâCode Intelligence, reinforcing the potential of ViRL for improving multimodal code-generation systems; code and model release planned.

Abstract: Multimodal code generation has garnered significant interest within the
research community. Despite the notable success of recent vision-language
models (VLMs) on specialized tasks like Chart-to-code generation, their
reliance on single-task training regimens fosters a narrow paradigm that
hinders the development of generalized \textbf{VI}sio\textbf{N} \textbf{C}ode
\textbf{I}ntelligence. In this work, we introduce \textbf{VinciCoder}, a
unified multimodal code generation model that addresses this limitation via a
two-stage training framework. We begin by constructing a large-scale Supervised
Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving
direct code generation and visual-based code refinement. Subsequently, we
introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a
coarse-to-fine reward mechanism to improve visual fidelity by calculating
visual similarity across local and global image patches. Extensive experiments
on various multimodal code generation benchmarks demonstrate that VinciCoder
achieves state-of-the-art performance, underscoring the effectiveness of our
coarse-to-fine ViRL strategy. The code and model will be available at
https://github.com/DocTron-hub/VinciCoder.

</details>


### [48] [CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks](https://arxiv.org/abs/2511.00396)
*Long Li,Shuichen Ji,Ziyang Luo,Nian Liu,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: A unified CoT-based framework for SOD, CoSOD, and SIS using a Vision-Language Model with two-stage training (SFT + RL) and a Confidence-Guided Policy Optimization (CGPO); achieves state-of-the-art results (e.g., CoCA CoSOD S-measure 0.899) with less data.


<details>
  <summary>Details</summary>
Motivation: Bridge heterogeneous saliency tasks by casting them as Chain-of-Thought reasoning within a Vision-Language Model, enabling a single framework to handle SOD, CoSOD, and SIS and overcome heterogeneity in objectives and outputs.

Method: Two-stage CoT training: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). Introduce Confidence-Guided Policy Optimization (CGPO): a lightweight, single-sample RL update leveraging rewardâconfidence discrepancy as a per-sample advantage; eliminate group sampling to reduce overhead. Add an 'output-to-reasoning' strategy to generate high-fidelity SFT data aligned with ground-truth masks.

Result: Matches or surpasses specialized SOTA across all three tasks; notably, CoSOD on CoCA achieves S-measure 0.899, beating the previous best by 8.0 percentage points, with much less training data.

Conclusion: A data-efficient, unified CoT-based approach for heterogeneous saliency tasks that combines CGPO-based RL with coherent SFT data to achieve strong, generalizable performance across SOD, CoSOD, and SIS.

Abstract: We present the first unified framework that jointly handles three
operationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting
each as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model
(VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm:
Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT
quality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a
lightweight single-sample algorithm that leverages the discrepancy between
reward and model confidence as a per-sample advantage signal. This design
naturally focuses updates on informative responses while eliminating group
sampling, thereby addressing GRPO's key limitations: confidence-agnostic
learning, signal dilution, and prohibitive computational overhead. We also
introduce an "output-to-reasoning" strategy to construct high-fidelity SFT data
that ensures logical consistency with ground-truth masks. Experiments show our
model matches or outperforms specialized SOTA methods and strong closed-source
VLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for
CoSOD, surpassing the prior best by 8.0 percentage points, despite using far
less training data.

</details>


### [49] [LGCA: Enhancing Semantic Representation via Progressive Expansion](https://arxiv.org/abs/2511.00419)
*Thanh Hieu Cao,Trung Khang Tran,Gia Thinh Pham,Tuong Nghiem Diep,Thanh Binh Nguyen*

Main category: cs.CV

TL;DR: Localized-Globalized Cross-Alignment (LGCA) improves zero-shot image classification by expanding and integrating salient local regions with the original image, achieving better performance without increasing time complexity.


<details>
  <summary>Details</summary>
Motivation: To mitigate misinformation and bias from random small crops in CLIP-like models and to robustly capture both local and global image features efficiently.

Method: LGCA builds a cross-alignment framework that (1) extracts local image features, (2) repeatedly selects salient regions and expands them, and (3) uses a similarity score that combines original and expanded images to align with text. The approach preserves the original model's time complexity prior to expansion, ensuring efficiency and scalability.

Result: LGCA yields substantial improvements in zero-shot performance across diverse datasets and outperforms state-of-the-art baselines.

Conclusion: LGCA offers an efficient, scalable solution for robust cross-modal alignment by fusing local and global cues while mitigating misinformation from small crops.

Abstract: Recent advancements in large-scale pretraining in natural language processing
have enabled pretrained vision-language models such as CLIP to effectively
align images and text, significantly improving performance in zero-shot image
classification tasks. Subsequent studies have further demonstrated that
cropping images into smaller regions and using large language models to
generate multiple descriptions for each caption can further enhance model
performance. However, due to the inherent sensitivity of CLIP, random image
crops can introduce misinformation and bias, as many images share similar
features at small scales. To address this issue, we propose
Localized-Globalized Cross-Alignment (LGCA), a framework that first captures
the local features of an image and then repeatedly selects the most salient
regions and expands them. The similarity score is designed to incorporate both
the original and expanded images, enabling the model to capture both local and
global features while minimizing misinformation. Additionally, we provide a
theoretical analysis demonstrating that the time complexity of LGCA remains the
same as that of the original model prior to the repeated expansion process,
highlighting its efficiency and scalability. Extensive experiments demonstrate
that our method substantially improves zero-shot performance across diverse
datasets, outperforming state-of-the-art baselines.

</details>


### [50] [Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection](https://arxiv.org/abs/2511.00427)
*Daichi Zhang,Tong Zhang,Jianmin Bao,Shiming Ge,Sabine SÃ¼sstrunk*

Main category: cs.CV

TL;DR: Multi-modal detector ITEM leverages image-text misalignment in CLIP space to detect AI-generated images, using a hierarchical scheme (global image and per-object semantics) to improve generalization to unseen generators.


<details>
  <summary>Details</summary>
Motivation: Binary visual detectors overfit to known models; need robust generalization; observation that fake images misalign with captions compared to real images in CLIP space.

Method: Compute image-caption misalignment in CLIP space; train an MLP head for detection; hierarchical scheme: global image alignment first, then per-caption semantic object alignment; joint visual-language space used for discriminative cues.

Result: Outperforms state-of-the-art; strong generalization and robustness across different generative models.

Conclusion: ITEM demonstrates that image-text misalignment in a CLIP-based space is a powerful cue for detecting synthetic images, with global and local semantic alignment cues enhancing robustness to unseen generators.

Abstract: With the rapid development of generative models, detecting generated fake
images to prevent their malicious use has become a critical issue recently.
Existing methods frame this challenge as a naive binary image classification
task. However, such methods focus only on visual clues, yielding trained
detectors susceptible to overfitting specific image patterns and incapable of
generalizing to unseen models. In this paper, we address this issue from a
multi-modal perspective and find that fake images cannot be properly aligned
with corresponding captions compared to real images. Upon this observation, we
propose a simple yet effective detector termed ITEM by leveraging the
image-text misalignment in a joint visual-language space as discriminative
clues. Specifically, we first measure the misalignment of the images and
captions in pre-trained CLIP's space, and then tune a MLP head to perform the
usual detection task. Furthermore, we propose a hierarchical misalignment
scheme that first focuses on the whole image and then each semantic object
described in the caption, which can explore both global and fine-grained local
semantic misalignment as clues. Extensive experiments demonstrate the
superiority of our method against other state-of-the-art competitors with
impressive generalization and robustness on various recent generative models.

</details>


### [51] [Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection](https://arxiv.org/abs/2511.00429)
*Daichi Zhang,Tong Zhang,Shiming Ge,Sabine SÃ¼sstrunk*

Main category: cs.CV

TL;DR: A diffusion-image detector enhances a frequency-domain cue (F^2C) by applying a frequency-selective weighting to the Fourier spectrum, improving detection of unseen diffusion models and robustness to perturbations.


<details>
  <summary>Details</summary>
Motivation: Detectors struggle to generalize across different diffusion models and perturbations; diffusion-generated images diverge more from natural images in high-frequency bands, so a robust detector should leverage discriminative information across all frequencies.

Method: Introduce a frequency-selective function that acts as a weighted filter on the Fourier spectrum to enhance informative frequency bands and suppress less discriminative ones, thereby strengthening the Frequency Forgery Clue (F^2C) across all frequencies.

Result: Extensive experiments on datasets of diffusion-generated images show the proposed approach outperforms state-of-the-art detectors, with superior generalization to unseen models and robustness to perturbations.

Conclusion: A simple, frequency-domain approach using F^2C and a frequency-selective filter yields strong generalization and robustness for detecting diffusion-generated images across unseen models.

Abstract: Diffusion models have achieved remarkable success in image synthesis, but the
generated high-quality images raise concerns about potential malicious use.
Existing detectors often struggle to capture discriminative clues across
different models and settings, limiting their generalization to unseen
diffusion models and robustness to various perturbations. To address this
issue, we observe that diffusion-generated images exhibit progressively larger
differences from natural real images across low- to high-frequency bands. Based
on this insight, we propose a simple yet effective representation by enhancing
the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we
introduce a frequency-selective function which serves as a weighted filter to
the Fourier spectrum, suppressing less discriminative bands while enhancing
more informative ones. This approach, grounded in a comprehensive analysis of
frequency-based differences between natural real and diffusion-generated
images, enables general detection of images from unseen diffusion models and
provides robust resilience to various perturbations. Extensive experiments on
various diffusion-generated image datasets demonstrate that our method
outperforms state-of-the-art detectors with superior generalization and
robustness.

</details>


### [52] [ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training](https://arxiv.org/abs/2511.00446)
*Xin Yao,Haiyang Zhao,Yimin Chen,Jiawei Guo,Kecheng Huang,Ming Zhao*

Main category: cs.CV

TL;DR: ToxicTextCLIP shows a framework to poison CLIP's pretraining via adversarial texts, using background-aware selection and background-driven augmentation to create background-consistent poisoned samples; it achieves high attack and backdoor success and defeats several defenses.


<details>
  <summary>Details</summary>
Motivation: CLIP relies on large-scale uncurated Internet data and the text modality for training, making it vulnerable to data poisoning and backdoors that are underexplored compared to image-based attacks.

Method: An iterative two-component approach: (1) a background-aware selector that prioritizes texts whose background content aligns with the target class, and (2) a background-driven augmenter that generates semantically coherent, diverse poisoned samples, applied during pretraining to induce semantic misalignment targeted by CLIP.

Result: Extensive experiments on classification and retrieval show high poisoning success (up to 95.83%) and backdoor Hit@1 (98.68%), while bypassing defenses such as RoCLIP, CleanCLIP and SafeCLIP.

Conclusion: Text-based poisoning of CLIP during pretraining is feasible and effective, highlighting the need for robust data curation and defense mechanisms against backdoor attacks in vision-language models.

Abstract: The Contrastive Language-Image Pretraining (CLIP) model has significantly
advanced vision-language modeling by aligning image-text pairs from large-scale
web data through self-supervised contrastive learning. Yet, its reliance on
uncurated Internet-sourced data exposes it to data poisoning and backdoor
risks. While existing studies primarily investigate image-based attacks, the
text modality, which is equally central to CLIP's training, remains
underexplored. In this work, we introduce ToxicTextCLIP, a framework for
generating high-quality adversarial texts that target CLIP during the
pre-training phase. The framework addresses two key challenges: semantic
misalignment caused by background inconsistency with the target class, and the
scarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively
applies: 1) a background-aware selector that prioritizes texts with background
content aligned to the target class, and 2) a background-driven augmenter that
generates semantically coherent and diverse poisoned samples. Extensive
experiments on classification and retrieval tasks show that ToxicTextCLIP
achieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while
bypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be
accessed via https://github.com/xinyaocse/ToxicTextCLIP/.

</details>


### [53] [Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations](https://arxiv.org/abs/2511.00456)
*Kiran Shahi,Anup Bagale*

Main category: cs.CV

TL;DR: Weakly supervised pneumonia classification/localization using Grad-CAM heatmaps with image-level labels across seven architectures; achieves near-perfect performance and interpretable heatmaps on Kermany CXR data.


<details>
  <summary>Details</summary>
Motivation: Enable explainable pneumonia detection without costly pixel-level annotations and increase clinical trust by providing interpretable localization through Grad-CAM.

Method: Weakly supervised learning using image-level labels and Grad-CAM explanations. Evaluates seven ImageNet-pretrained architectures (ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, ViT-B16) under identical training settings with focal loss and patient-wise splits to prevent data leakage.

Result: Best overall test accuracy 98%, ROC-AUC 0.997, F1 0.987 achieved by ResNet-18 and EfficientNet-B0. MobileNet-V2 offers a good accuracy/cost trade-off. Grad-CAM heatmaps show attention to clinically relevant lung regions.

Conclusion: Weakly supervised explainable models can enhance pneumonia screening transparency and clinical trust in AI-assisted radiology, demonstrating potential for interpretable pneumonia detection without pixel-level annotations.

Abstract: This study proposes a weakly supervised deep learning framework for pneumonia
classification and localization from chest X-rays, utilizing Grad-CAM
explanations. Instead of costly pixel-level annotations, our approach utilizes
image-level labels to generate clinically meaningful heatmaps that highlight
regions affected by pneumonia. We evaluate seven ImageNet-pretrained
architectures ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, and
ViT-B16 under identical training conditions with focal loss and patient-wise
splits to prevent data leakage. Experimental results on the Kermany CXR dataset
demonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test
accuracy of 98\%, ROC-AUC = 0.997, and F1 = 0.987, while MobileNet-V2 provides
an optimal trade-off between accuracy and computational cost. Grad-CAM
visualizations confirm that the proposed models focus on clinically relevant
lung regions, supporting the use of interpretable AI for radiological
diagnostics. This work highlights the potential of weakly supervised
explainable models that enhance pneumonia screening transparency, and clinical
trust in AI-assisted medical imaging.
  https://github.com/kiranshahi/pneumonia-analysis

</details>


### [54] [HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation](https://arxiv.org/abs/2511.00468)
*Panwang Pan,Tingting Shen,Chenxin Li,Yunlong Lin,Kairun Wen,Jingjing Zhao,Yixuan Yuan*

Main category: cs.CV

TL;DR: HumanCrafter is a single-image framework that jointly models appearance and human-part semantics for 3D reconstruction and segmentation, using geometric priors, self-supervised semantics, and interactive labeling to achieve state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: 3D human reconstruction and segmentation are powerful but hampered by scarce labeled 3D data and the need to unify appearance and semantics. A single-image, multi-task approach with priors and data-efficient labeling can improve performance and practicality.

Method: A unified, feed-forward framework (HumanCrafter) that (1) injects geometric priors into 3D reconstruction, (2) exploits self-supervised semantic priors for segmentation, (3) uses an interactive annotation procedure to generate high-quality data-label pairs, and (4) employs pixel-aligned aggregation for cross-task synergy and a multi-task objective to optimize texture fidelity and semantic consistency.

Result: Extensive experiments show HumanCrafter surpasses state-of-the-art methods in both 3D human-part segmentation and 3D reconstruction from a single image.

Conclusion: Joint modeling of appearance and semantics with priors and interactive data generation enables effective cross-task synergy, yielding superior single-image 3D reconstruction and segmentation performance.

Abstract: Recent advances in generative models have achieved high-fidelity in 3D human
reconstruction, yet their utility for specific tasks (e.g., human 3D
segmentation) remains constrained. We propose HumanCrafter, a unified framework
that enables the joint modeling of appearance and human-part semantics from a
single image in a feed-forward manner. Specifically, we integrate human
geometric priors in the reconstruction stage and self-supervised semantic
priors in the segmentation stage. To address labeled 3D human datasets
scarcity, we further develop an interactive annotation procedure for generating
high-quality data-label pairs. Our pixel-aligned aggregation enables cross-task
synergy, while the multi-task objective simultaneously optimizes texture
modeling fidelity and semantic consistency. Extensive experiments demonstrate
that HumanCrafter surpasses existing state-of-the-art methods in both 3D
human-part segmentation and 3D human reconstruction from a single image.

</details>


### [55] [Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations](https://arxiv.org/abs/2511.00472)
*Navodini Wijethilake,Marina Ivory,Oscar MacCormac,Siddhant Kumar,Aaron Kujawa,Lorena Garcia-Foncillas Macias,Rebecca Burger,Amanda Hitchings,Suki Thomson,Sinan Barazi,Eleni Maratos,Rupert Obholzer,Dan Jiang,Fiona McClenaghan,Kazumi Chia,Omar Al-Salihi,Nick Thomas,Steve Connor,Tom Vercauteren,Jonathan Shapey*

Main category: cs.CV

TL;DR: A multi-centre, bootstrapped, human-in-the-loop framework for automated vestibular schwannoma segmentation in MRI, achieving DSC 0.967 on internal validation and promising generalisation, with a 37.4% efficiency gain and a public dataset release on TCIA.


<details>
  <summary>Details</summary>
Motivation: Accurate VS segmentation in MRI is essential for patient management but relies on time-consuming expert annotations; robust, generalisable models are needed across diverse clinical data.

Method: A bootstrapped DL-based iterative segmentation framework with expert consensus for quality refinement (human-in-the-loop). Data pooled from multiple centres. Public release of the dataset via TCIA. Evaluation includes internal target validation with external datasets and expert review on 143 scans.

Result: Dice similarity coefficient improved from 0.9125 to 0.9670 on the target internal validation dataset; stable performance on representative external datasets; expert evaluation identified areas requiring intervention; estimated ~37.4% efficiency gain vs manual annotation; dataset comprises 190 patients with 534 annotated T1CE scans from 184 patients and 6 non-annotated T2 scans (public on TCIA).

Conclusion: The proposed human-in-the-loop, multi-centre framework achieves high segmentation accuracy and good generalisability, offering a clinically adaptable approach for automated VS segmentation and highlighting the value of public datasets for benchmarking.

Abstract: Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance
Imaging (MRI) is essential for patient management but often requires
time-intensive manual annotations by experts. While recent advances in deep
learning (DL) have facilitated automated segmentation, challenges remain in
achieving robust performance across diverse datasets and complex clinical
cases. We present an annotated dataset stemming from a bootstrapped DL-based
framework for iterative segmentation and quality refinement of VS in MRI. We
combine data from multiple centres and rely on expert consensus for
trustworthiness of the annotations. We show that our approach enables effective
and resource-efficient generalisation of automated segmentation models to a
target data distribution. The framework achieved a significant improvement in
segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from
0.9125 to 0.9670 on our target internal validation dataset, while maintaining
stable performance on representative external datasets. Expert evaluation on
143 scans further highlighted areas for model refinement, revealing nuanced
cases where segmentation required expert intervention. The proposed approach is
estimated to enhance efficiency by approximately 37.4% compared to the
conventional manual annotation process. Overall, our human-in-the-loop model
training approach achieved high segmentation accuracy, highlighting its
potential as a clinically adaptable and generalisable strategy for automated VS
segmentation in diverse clinical settings. The dataset includes 190 patients,
with tumour annotations available for 534 longitudinal contrast-enhanced
T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans
from 6 patients. This dataset is publicly accessible on The Cancer Imaging
Archive (TCIA) (https://doi.org/10.7937/bq0z-xa62).

</details>


### [56] [FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts](https://arxiv.org/abs/2511.00480)
*Weihao Bo,Yanpeng Sun,Yu Wang,Xinyu Zhang,Zechao Li*

Main category: cs.CV

TL;DR: FedMGP introduces a federated vision-language prompting framework where each client maintains multiple groups of prompts. A diversity loss encourages specialization across groups, and a similarity-guided probabilistic sampling strategy dynamically aggregates prompts from the global model. This design improves personalization and domain generalization while keeping communication efficient, achieving state-of-the-art results with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: Personalized federated learning for vision-language models is challenged by heterogeneous data and the need to capture diverse local semantics. Prior prompt-based FL methods may over-consolidate knowledge or be inefficient in communication. FedMGP aims to distribute a fixed prompt capacity across multiple groups to capture fine-grained, local cues while maintaining global coherence and efficiency.

Method: Each client maintains multiple groups of paired textual and visual prompts. A diversity loss drives each group to specialize in distinct semantic aspects. During communication, clients compute cosine similarities between their prompt groups and the global prompts from the previous round and perform softmax-weighted sampling to select s groups for aggregation, enabling exploration of underrepresented patterns. A fixed total prompt capacity is redistributed across groups to ensure parameter efficiency. The paper also offers a theoretical analysis showing that dynamic aggregation reinforces shared semantics and suppresses client-specific noise.

Result: Empirical results show FedMGP consistently outperforms prior approaches in both personalization and domain generalization across diverse federated vision-language benchmarks, while using the lowest communication parameters among federated prompt learning methods.

Conclusion: FedMGP provides an effective, efficient framework for personalized FL in vision-language models by combining diverse, specialized prompt groups with a dynamic, similarity-guided aggregation scheme, balancing common knowledge with client-specific features and achieving strong performance with minimal communication overhead.

Abstract: In this paper, we introduce FedMGP, a new paradigm for personalized federated
prompt learning in vision-language models. FedMGP equips each client with
multiple groups of paired textual and visual prompts, enabling the model to
capture diverse, fine-grained semantic and instance-level cues. A diversity
loss is introduced to drive each prompt group to specialize in distinct and
complementary semantic aspects, ensuring that the groups collectively cover a
broader range of local characteristics. During communication, FedMGP employs a
dynamic prompt aggregation strategy based on similarity-guided probabilistic
sampling: each client computes the cosine similarity between its prompt groups
and the global prompts from the previous round, then samples s groups via a
softmax-weighted distribution. This soft selection mechanism preferentially
aggregates semantically aligned knowledge while still enabling exploration of
underrepresented patterns effectively balancing the preservation of common
knowledge with client-specific features. Notably, FedMGP maintains parameter
efficiency by redistributing a fixed prompt capacity across multiple groups,
achieving state-of-the-art performance with the lowest communication parameters
among all federated prompt learning methods. Theoretical analysis shows that
our dynamic aggregation strategy promotes robust global representation learning
by reinforcing shared semantics while suppressing client-specific noise.
Extensive experiments demonstrate that FedMGP consistently outperforms prior
approaches in both personalization and domain generalization across diverse
federated vision-language benchmarks. The code will be released on
https://github.com/weihao-bo/FedMGP.git.

</details>


### [57] [Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models](https://arxiv.org/abs/2511.00503)
*Panwang Pan,Chenguo Lin,Jingjing Zhao,Chenxin Li,Yuchen Lin,Haopeng Li,Honglei Yan,Kairun Wen,Yunlong Lin,Yixuan Yuan,Yadong Mu*

Main category: cs.CV

TL;DR: Diff4Splat enables fast, controllable 4D scene synthesis from a single image in one forward pass by predicting deformable 3D Gaussian fields with a video latent transformer, achieving 30-second generation without test-time optimization while matching or surpassing optimization-based methods.


<details>
  <summary>Details</summary>
Motivation: To bridge generative priors from video diffusion models with geometric and motion constraints learned from large 4D datasets, enabling realistic, controllable dynamic scenes from a single image without expensive optimization.

Method: A feed-forward approach that, given a single image, a camera trajectory, and an optional text prompt, predicts a deformable 3D Gaussian field encoding appearance, geometry, and motion. It uses a video latent transformer to capture spatio-temporal dependencies and predict time-varying 3D Gaussian primitives. Training optimizes appearance fidelity, geometric accuracy, and motion consistency.

Result: Produces high-quality 4D scenes quickly (approx. 30 seconds) and can be used for video generation, novel view synthesis, and geometry extraction. It matches or surpasses optimization-based methods in quality while being significantly more efficient.

Conclusion: Diff4Splat demonstrates that a single forward pass can synthesize controllable, geometry-aware dynamic scenes by unifying diffusion priors with explicit 3D representations, enabling efficient 4D content creation without test-time optimization.

Abstract: We introduce Diff4Splat, a feed-forward method that synthesizes controllable
and explicit 4D scenes from a single image. Our approach unifies the generative
priors of video diffusion models with geometry and motion constraints learned
from large-scale 4D datasets. Given a single input image, a camera trajectory,
and an optional text prompt, Diff4Splat directly predicts a deformable 3D
Gaussian field that encodes appearance, geometry, and motion, all in a single
forward pass, without test-time optimization or post-hoc refinement. At the
core of our framework lies a video latent transformer, which augments video
diffusion models to jointly capture spatio-temporal dependencies and predict
time-varying 3D Gaussian primitives. Training is guided by objectives on
appearance fidelity, geometric accuracy, and motion consistency, enabling
Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate
the effectiveness of Diff4Splatacross video generation, novel view synthesis,
and geometry extraction, where it matches or surpasses optimization-based
methods for dynamic scene synthesis while being significantly more efficient.

</details>


### [58] [VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning](https://arxiv.org/abs/2511.00504)
*Hai-Dang Nguyen,Ha-Hieu Pham,Hao T. Nguyen,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: VinDr-CXR-VQA introduces a large-scale, explainable chest X-ray Med-VQA dataset with spatial grounding and radiologist-verified explanations, enabling improved VQA performance and lesion localization while reducing hallucinations through balanced QA sampling.


<details>
  <summary>Details</summary>
Motivation: To enable reliable, explainable medical visual question answering in radiology by providing bounding-box grounded answers, clinical reasoning, and a balanced dataset to mitigate spurious normal-case predictions.

Method: Assemble 4,394 chest X-ray images with 17,597 QA pairs; annotate each Q/A with radiologist-verified bounding boxes and clinical reasoning explanations; define a six-category question taxonomy (Where, What, Is there, How many, Which, Yes/No); create a balanced distribution (41.7% positive, 58.3% negative) to reduce hallucinations; benchmark using MedGemma-4B-it with localization capability.

Result: Achieved F1 = 0.624 on MedGemma-4B-it, a +11.8% improvement over the baseline, while enabling lesion localization.

Conclusion: VinDr-CXR-VQA advances reproducible and clinically grounded Med-VQA research; the dataset and evaluation tools are publicly available at HuggingFace, facilitating further research and benchmarking.

Abstract: We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable
Medical Visual Question Answering (Med-VQA) with spatial grounding. The dataset
contains 17,597 question-answer pairs across 4,394 images, each annotated with
radiologist-verified bounding boxes and clinical reasoning explanations. Our
question taxonomy spans six diagnostic types-Where, What, Is there, How many,
Which, and Yes/No-capturing diverse clinical intents. To improve reliability,
we construct a balanced distribution of 41.7% positive and 58.3% negative
samples, mitigating hallucinations in normal cases. Benchmarking with
MedGemma-4B-it demonstrates improved performance (F1 = 0.624, +11.8% over
baseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance
reproducible and clinically grounded Med-VQA research. The dataset and
evaluation tools are publicly available at
huggingface.co/datasets/Dangindev/VinDR-CXR-VQA.

</details>


### [59] [OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback](https://arxiv.org/abs/2511.00510)
*Kai Luo,Hao Shi,Kunyu Peng,Fei Teng,Sheng Wu,Kaiwei Wang,Kailun Yang*

Main category: cs.CV

TL;DR: OmniTrack++ is a panoramic MOT framework that stabilizes distortion, leverages trajectory-informed localization, memory-based appearance modeling, and adaptive tracklet management to attain state-of-the-art HOTA on panoramic benchmarks, backed by the EmboTrack dataset.


<details>
  <summary>Details</summary>
Motivation: Panoramic MOT presents unique challenges (360Â° FoV, resolution dilution, severe distortions, large search space, and identity drift) that narrow-FoV MOT methods struggle to handle. The work motivates a dedicated, scalable solution and a rigorous benchmark for real-world panoramic perception.

Method: Key components include: (1) DynamicSSM block to stabilize panoramic features and mitigate distortion; (2) FlexiTrack Instances for trajectory-informed, flexible localization and short-term association on normalized representations; (3) ExpertTrack Memory (Mixture-of-Experts) to fuse appearance cues for long-term robustness and to recover from fragmented tracks; (4) Tracklet Management module that adaptively switches between end-to-end and tracking-by-detection modes based on scene dynamics; (5) EmboTrack benchmark (QuadTrack from a quadruped robot and BipTrack from a wheel-legged robot) to evaluate panoramic MOT in wide-angle environments; (6) Evaluation on JRDB and EmboTrack showing substantial improvements over baselines; (7) Open-source release of code and datasets.

Result: OmniTrack++ achieves state-of-the-art MOT performance with notable gains in HOTA: +25.5% on JRDB and +43.07% on QuadTrack over the original OmniTrack.

Conclusion: The approach delivers a scalable, robust panoramic MOT solution with a challenging benchmark (EmboTrack) and strong empirical gains, with code and datasets to follow for broader adoption.

Abstract: This paper investigates Multi-Object Tracking (MOT) in panoramic imagery,
which introduces unique challenges including a 360{\deg} Field of View (FoV),
resolution dilution, and severe view-dependent distortions. Conventional MOT
methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily
under these conditions. To address panoramic distortion, large search space,
and identity ambiguity under a 360{\deg} FoV, OmniTrack++ adopts a
feedback-driven framework that progressively refines perception with trajectory
cues. A DynamicSSM block first stabilizes panoramic features, implicitly
alleviating geometric distortion. On top of normalized representations,
FlexiTrack Instances use trajectory-informed feedback for flexible localization
and reliable short-term association. To ensure long-term robustness, an
ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts
design, enabling recovery from fragmented tracks and reducing identity drift.
Finally, a Tracklet Management module adaptively switches between end-to-end
and tracking-by-detection modes according to scene dynamics, offering a
balanced and scalable solution for panoramic MOT. To support rigorous
evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for
panoramic MOT that includes QuadTrack, captured with a quadruped robot, and
BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets
span wide-angle environments and diverse motion patterns, providing a
challenging testbed for real-world panoramic perception. Extensive experiments
on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art
performance, yielding substantial HOTA improvements of +25.5% on JRDB and
+43.07% on QuadTrack over the original OmniTrack. Datasets and code will be
made publicly available at https://github.com/xifen523/OmniTrack.

</details>


### [60] [ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation](https://arxiv.org/abs/2511.00511)
*Panwang Pan,Jingjing Zhao,Yuchen Lin,Chenguo Lin,Chenxin Li,Haopeng Li,Honglei Yan,Tingting Shen,Yadong Mu*

Main category: cs.CV

TL;DR: Introduces ID-Composer for multi-subject video generation from text and reference images, with hierarchical identity-preserving attention, VLM-guided semantic understanding, and an online RL phase to align with RLVR objectives; reports improved identity preservation, temporal consistency, and video quality.


<details>
  <summary>Details</summary>
Motivation: Address the limited controllability of video generative models that are either conditioned on a single image or text, by enabling multi-subject generation with preserved identities and temporal coherence across modalities.

Method: Proposes a hierarchical identity-preserving attention mechanism to aggregate features within and across subjects and modalities; incorporates a pretrained vision-language model to guide semantic understanding and interactions; employs an online reinforcement learning phase to optimize training toward RLVR objectives.

Result: Experiments show the method surpasses existing approaches in preserving subject identities, maintaining temporal consistency, and producing high-quality videos.

Conclusion: ID-Composer effectively enables controllable, multi-subject video generation from text and references, with strong identity, semantic, and temporal performance, validated by extensive experiments.

Abstract: Video generative models pretrained on large-scale datasets can produce
high-quality videos, but are often conditioned on text or a single image,
limiting controllability and applicability. We introduce ID-Composer, a novel
framework that addresses this gap by tackling multi-subject video generation
from a text prompt and reference images. This task is challenging as it
requires preserving subject identities, integrating semantics across subjects
and modalities, and maintaining temporal consistency. To faithfully preserve
the subject consistency and textual information in synthesized videos,
ID-Composer designs a \textbf{hierarchical identity-preserving attention
mechanism}, which effectively aggregates features within and across subjects
and modalities. To effectively allow for the semantic following of user
intention, we introduce \textbf{semantic understanding via pretrained
vision-language model (VLM)}, leveraging VLM's superior semantic understanding
to provide fine-grained guidance and capture complex interactions between
multiple subjects. Considering that standard diffusion loss often fails in
aligning the critical concepts like subject ID, we employ an \textbf{online
reinforcement learning phase} to drive the overall training objective of
ID-Composer into RLVR. Extensive experiments demonstrate that our model
surpasses existing methods in identity preservation, temporal consistency, and
video quality.

</details>


### [61] [SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation](https://arxiv.org/abs/2511.00523)
*Fangyu Wu,Yujun Cai*

Main category: cs.CV

TL;DR: Segmentation-guided, test-time debiasing for ViT-based CLIP that isolates the target attribute and enforces uniform similarity of non-target regions to all class prompts, removing bias signals without extra training or bias labels.


<details>
  <summary>Details</summary>
Motivation: Mitigate spurious correlations in visionâlanguage models without requiring training data or explicit bias annotations; improve generalization in open-set settings.

Method: At test time, use a pretrained segmentation model to segment the target attribute, then adjust embeddings of non-target regions so they are uniformly similar to all class-specific text prompts, preserving the target attribute and removing confounding signals.

Result: Outperforms existing test-time debiasing methods on Waterbirds and CelebA in group robustness metrics and Attention IoU.

Conclusion: Segmentation-guided interventions enable scalable, annotation-free bias mitigation for visionâlanguage models.

Abstract: Vision language models such as CLIP have shown remarkable performance in zero
shot classification, but remain susceptible to spurious correlations, where
irrelevant visual features influence predictions. Existing debiasing methods
often require access to training data and explicit group labels to perform
fine-tuning or adjust embeddings, which limits their practicality in real-world
settings. Test-time methods attempt to avoid this constraint, but many still
depend on prior knowledge of dataset specific biases, limiting their
generalizability in open set settings. In this work, we propose a test-time
debiasing method for ViT based CLIP models that requires no additional training
or assumptions of bias annotations. Our approach uses a pretrained segmentation
model to isolate the target visual attribute, then adjusts the non target
regions so that their embeddings are uniformly similar to all class specific
text prompts. This procedure removes unintended bias signals from confounding
visual regions while preserving the target attribute. Experiments on Waterbirds
and CelebA show that our method outperforms existing test-time debiasing
approaches in both group robustness metrics and Attention IoU. These results
demonstrate the effectiveness of segmentation guided interventions for scalable
and annotation free bias mitigation in vision language models.

</details>


### [62] [Text-guided Fine-Grained Video Anomaly Detection](https://arxiv.org/abs/2511.00524)
*Jihao Gu,Kun Li,He Wang,Kaan AkÅit*

Main category: cs.CV

TL;DR: Proposes Text-guided Fine-Grained Video Anomaly Detection (T-VAD) using LVLMs with an Anomaly Heatmap Decoder (AHD) and a Region-aware Anomaly Encoder (RAE) to generate pixel-level anomaly heatmaps and textual embeddings, enabling fine-grained, interactive anomaly localization and description. Achieves state-of-the-art results on UBnormal (micro-AUC 94.8%; heatmap accuracy 67.8%/76.7% for RBDC/TBDC) and strong textual metrics on ShanghaiTech and UBnormal datasets (BLEU-4 and Yes/No accuracy).


<details>
  <summary>Details</summary>
Motivation: Current VAD systems are semi-automated or yield only binary outputs (normal/anomalous). There is a need for fine-grained, interpretable, and interactive anomaly localization and description, leveraging powerful LVLMs.

Method: Introduce Anomaly Heatmap Decoder (AHD) for pixel-wise visual-text alignment to produce anomaly heatmaps. Use Region-aware Anomaly Encoder (RAE) to convert heatmaps into learnable textual embeddings, guiding the LVLM to identify and localize anomalies. Integrate with a Large Vision-Language Model for descriptive and interactive outputs.

Result: Achieves state-of-the-art performance: UBnormal dataset shows micro-AUC of 94.8% and heatmap accuracy of 67.8% (RBDC) and 76.7% (TBDC). Textual evaluation reports BLEU-4 scores on targets and trajectories (ShanghaiTech and UBnormal) and high Yes/No accuracies, indicating improved textual descriptions and localization quality.

Conclusion: Demonstrates that LVLM-guided, fine-grained anomaly detection can surpass traditional binary outputs, offering more granular, interpretable, and interactive VAD with strong quantitative and qualitative results.

Abstract: Video Anomaly Detection (VAD) aims to identify anomalous events within video
segments. In scenarios such as surveillance or industrial process monitoring,
anomaly detection is of critical importance. While existing approaches are
semi-automated, requiring human assessment for anomaly detection, traditional
VADs offer limited output as either normal or anomalous. We propose Text-guided
Fine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large
Vision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)
that performs pixel-wise visual-textual feature alignment to generate
fine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly
Encoder (RAE) that transforms the heatmaps into learnable textual embeddings,
guiding the LVLM to accurately identify and localize anomalous events in
videos. This significantly enhances both the granularity and interactivity of
anomaly detection. The proposed method achieving SOTA performance by
demonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and
67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,
and subjectively verified more preferable textual description on the
ShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;
Yes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for
targets, 78.10 for trajectories; Yes/No accuracy: 89.73%).

</details>


### [63] [Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era](https://arxiv.org/abs/2511.00540)
*Wenbing Zhu,Chengjie Wang,Bin-Bin Gao,Jiangning Zhang,Guannan Jiang,Jie Hu,Zhenye Gan,Lidong Wang,Ziqing Zhou,Linjie Cheng,Yurui Pan,Bo Peng,Mingmin Chi,Lizhuang Ma*

Main category: cs.CV

TL;DR: Introduces Real-IAD Variety, the largest diverse industrial anomaly detection benchmark to date, showing vision-language models scale well across categories whereas traditional anomaly detectors struggle with more categories.


<details>
  <summary>Details</summary>
Motivation: Current IAD benchmarks suffer from limited category diversity, scale, metric saturation, and transferability; a large, diverse benchmark and standardized evaluation is needed to train and evaluate generalizable models and foundation models.

Method: Construct Real-IAD Variety: 198,960 high-resolution images across 160 object categories, covering 28 industries, 24 material types, and 22 color variations; provide rigorous evaluation protocols for multi-class unsupervised, multi-view, and zero-/few-shot settings; compare baseline methods across category scales.

Result: Demonstrates that state-of-the-art multi-class unsupervised anomaly detection methods degrade significantly when scaling from 30 to 160 categories; vision-language models exhibit robustness to category-scale, with minimal performance variation across category counts; validates the benchmark as a more challenging and general resource.

Conclusion: Real-IAD Variety is an essential resource for training and evaluating next-generation foundation models for anomaly detection; its public release and comprehensive evaluation protocols will accelerate development of scalable, general-purpose anomaly detection systems.

Abstract: Industrial Anomaly Detection (IAD) is critical for enhancing operational
safety, ensuring product quality, and optimizing manufacturing efficiency
across global industries. However, the IAD algorithms are severely constrained
by the limitations of existing public benchmarks. Current datasets exhibit
restricted category diversity and insufficient scale, frequently resulting in
metric saturation and limited model transferability to real-world scenarios. To
address this gap, we introduce Real-IAD Variety, the largest and most diverse
IAD benchmark, comprising 198,960 high-resolution images across 160 distinct
object categories. Its diversity is ensured through comprehensive coverage of
28 industries, 24 material types, and 22 color variations. Our comprehensive
experimental analysis validates the benchmark's substantial challenge:
state-of-the-art multi-class unsupervised anomaly detection methods experience
significant performance degradation when scaled from 30 to 160 categories.
Crucially, we demonstrate that vision-language models exhibit remarkable
robustness to category scale-up, with minimal performance variation across
different category counts, significantly enhancing generalization capabilities
in diverse industrial contexts. The unprecedented scale and complexity of
Real-IAD Variety position it as an essential resource for training and
evaluating next-generation foundation models for anomaly detection. By
providing this comprehensive benchmark with rigorous evaluation protocols
across multi-class unsupervised, multi-view, and zero-/few-shot settings, we
aim to accelerate research beyond domain-specific constraints, enabling the
development of scalable, general-purpose anomaly detection systems. Real-IAD
Variety will be made publicly available to facilitate innovation in this
critical field.

</details>


### [64] [MIFO: Learning and Synthesizing Multi-Instance from One Image](https://arxiv.org/abs/2511.00542)
*Kailun Su,Ziqi He,Xi Wang,Yang Zhou*

Main category: cs.CV

TL;DR: Penalty-based attention to disentangle multi-instance semantics from a single image, with box-controlled attention during synthesis to reduce leakage and precisely control layout; robust to similar or rare objects; code released.


<details>
  <summary>Details</summary>
Motivation: Learning multiple semantically similar instances from very limited data is difficult; standard methods struggle with disambiguation and precise synthesis when instances look alike.

Method: Introduce penalty-based attention optimization to disentangle similar semantics during learning. During synthesis, optimize box control in attention layers to mitigate semantic leakage and enforce precise output layouts.

Result: Achieves disentangled, high-quality learning and synthesis with a good balance between editability and instance consistency; robust to semantically/visually similar instances and rare-seen objects; code available.

Conclusion: The proposed framework effectively enables precise multi-instance learning and synthesis from a single image under challenging data conditions, offering reliable disentanglement, controllable layout, and robustness to similarity and rarity.

Abstract: This paper proposes a method for precise learning and synthesizing
multi-instance semantics from a single image. The difficulty of this problem
lies in the limited training data, and it becomes even more challenging when
the instances to be learned have similar semantics or appearance. To address
this, we propose a penalty-based attention optimization to disentangle similar
semantics during the learning stage. Then, in the synthesis, we introduce and
optimize box control in attention layers to further mitigate semantic leakage
while precisely controlling the output layout. Experimental results demonstrate
that our method achieves disentangled and high-quality semantic learning and
synthesis, strikingly balancing editability and instance consistency. Our
method remains robust when dealing with semantically or visually similar
instances or rare-seen objects. The code is publicly available at
https://github.com/Kareneveve/MIFO

</details>


### [65] [4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting](https://arxiv.org/abs/2511.00560)
*Chun-Tin Wu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 4D-NVS combines a compact neural voxel grid with learned deformation fields and neural Gaussian splatting to model dynamic scenes, reducing memory, speeding training, and enabling high-fidelity, real-time rendering, aided by a targeted view refinement stage.


<details>
  <summary>Details</summary>
Motivation: Dynamic 3D Gaussian Splatting incurs huge memory costs because Gaussians must be replicated across time. There is a need for a compact, efficient dynamic 3D representation that preserves quality.

Method: Employ a compact set of neural voxels with learned deformation fields to encode temporal dynamics, eliminating per-frame Gaussian replication. Integrate neural voxels with Gaussian splatting and add a view refinement stage that selectively optimizes challenging viewpoints to boost rendering quality without sacrificing global efficiency.

Result: Claims significant memory reduction and faster training compared to state-of-the-art methods, enabling real-time rendering with high visual fidelity, as demonstrated by experiments.

Conclusion: The approach demonstrates that neural voxel grids with deformation fields can effectively model dynamic scenes when combined with Gaussian splatting, achieving efficient, high-quality rendering. It introduces a practical view refinement stage, with strong empirical performance, though broader validation would strengthen claims.

Abstract: Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel
view synthesis, extending it to dynamic scenes still results in substantial
memory overhead from replicating Gaussians across frames. To address this
challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines
voxel-based representations with neural Gaussian splatting for efficient
dynamic scene modeling. Instead of generating separate Gaussian sets per
timestamp, our method employs a compact set of neural voxels with learned
deformation fields to model temporal dynamics. The design greatly reduces
memory consumption and accelerates training while preserving high image
quality. We further introduce a novel view refinement stage that selectively
improves challenging viewpoints through targeted optimization, maintaining
global efficiency while enhancing rendering quality for difficult viewing
angles. Experiments demonstrate that our method outperforms state-of-the-art
approaches with significant memory reduction and faster training, enabling
real-time rendering with superior visual fidelity.

</details>


### [66] [Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective](https://arxiv.org/abs/2511.00573)
*Wei Feng,Zongyuan Ge*

Main category: cs.CV

TL;DR: Proposes FREE, a frequency-guided framework for Domain-Shifted Generalized Category Discovery (DS_GCD) that uses frequency-domain perturbations and domain separation to robustly discover both known and unknown categories under distribution shifts.


<details>
  <summary>Details</summary>
Motivation: GCD methods struggle when unlabeled data come from unknown domains or contain distributional shifts; realistic DS_GCD settings require robustness to domain changes and unseen categories.

Method: 1) Frequency-based domain separation to split data into known/unknown domains via amplitude differences. 2) Cross-domain perturbation by exchanging amplitude components across domains to adapt to new distributions. 3) Intra-domain perturbation to increase robustness to intra-domain variation within the unknown domain. 4) Extended self-supervised contrastive objective and semantic clustering loss to guide learning. 5) Clustering-difficulty-aware resampling to focus on harder-to-cluster categories.

Result: Extensive experiments show the approach mitigates the impact of distributional shifts and achieves superior performance in discovering both known and unknown categories across benchmark datasets.

Conclusion: Frequency-guided learning yields robust DS_GCD performance under domain shifts, advancing generalized category discovery in more realistic, shift-prone environments.

Abstract: Generalized Category Discovery (GCD) aims to leverage labeled samples from
known categories to cluster unlabeled data that may include both known and
unknown categories. While existing methods have achieved impressive results
under standard conditions, their performance often deteriorates in the presence
of distribution shifts. In this paper, we explore a more realistic task:
Domain-Shifted Generalized Category Discovery (DS\_GCD), where the unlabeled
data includes not only unknown categories but also samples from unknown
domains. To tackle this challenge, we propose a
\textbf{\underline{F}}requency-guided Gene\textbf{\underline{r}}alized
Cat\textbf{\underline{e}}gory Discov\textbf{\underline{e}}ry framework (FREE)
that enhances the model's ability to discover categories under distributional
shift by leveraging frequency-domain information. Specifically, we first
propose a frequency-based domain separation strategy that partitions samples
into known and unknown domains by measuring their amplitude differences. We
then propose two types of frequency-domain perturbation strategies: a
cross-domain strategy, which adapts to new distributions by exchanging
amplitude components across domains, and an intra-domain strategy, which
enhances robustness to intra-domain variations within the unknown domain.
Furthermore, we extend the self-supervised contrastive objective and semantic
clustering loss to better guide the training process. Finally, we introduce a
clustering-difficulty-aware resampling technique to adaptively focus on
harder-to-cluster categories, further enhancing model performance. Extensive
experiments demonstrate that our method effectively mitigates the impact of
distributional shifts across various benchmark datasets and achieves superior
performance in discovering both known and unknown categories.

</details>


### [67] [TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection](https://arxiv.org/abs/2511.00580)
*Yousuf Ahmed Siddiqui,Sufiyaan Usmani,Umer Tariq,Jawwad Ahmed Shamsi,Muhammad Burhan Khan*

Main category: cs.CV

TL;DR: Memory-augmented, context-aware zero-shot video anomaly detection using cross-attention between temporal signals and visual embeddings with textual memory traces, enabling real-time inference and state-of-the-art zero-shot performance.


<details>
  <summary>Details</summary>
Motivation: Anomalies are highly contextual; existing detectors lack context-aware generalization, especially in zero-shot setups; need models that adapt in real time using memory traces.

Method: Proposes a memory-augmented pipeline that correlates temporal features with visual embeddings via cross-attention, and performs real-time zero-shot anomaly classification through contextual similarity scoring; fuses cross-attention temporal fusion with contextual memory for anomaly detection.

Result: Achieves 90.4% AUC on UCF-Crime and 83.67% AP on XD-Violence; claims state-of-the-art among zero-shot models; supports real-time inference and provides explainability.

Conclusion: Contextual memory fusion enhances anomaly fidelity and generalization, advancing practical deployment of zero-shot video anomaly detectors in surveillance and infrastructure monitoring.

Abstract: Video anomalies often depend on contextual information available and temporal
evolution. Non-anomalous action in one context can be anomalous in some other
context. Most anomaly detectors, however, do not notice this type of context,
which seriously limits their capability to generalize to new, real-life
situations. Our work addresses the context-aware zero-shot anomaly detection
challenge, in which systems need to learn adaptively to detect new events by
correlating temporal and appearance features with textual traces of memory in
real time. Our approach defines a memory-augmented pipeline, correlating
temporal signals with visual embeddings using cross-attention, and real-time
zero-shot anomaly classification by contextual similarity scoring. We achieve
90.4\% AUC on UCF-Crime and 83.67\% AP on XD-Violence, a new state-of-the-art
among zero-shot models. Our model achieves real-time inference with high
precision and explainability for deployment. We show that, by fusing
cross-attention temporal fusion and contextual memory, we achieve high fidelity
anomaly detection, a step towards the applicability of zero-shot models in
real-world surveillance and infrastructure monitoring.

</details>


### [68] [CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World](https://arxiv.org/abs/2511.00613)
*Yating Yu,Congqi Cao,Zhaoying Wang,Weihua Meng,Jie Li,Yuxin Li,Zihao Wei,Zhongpei Shen,Jiajun Zhang*

Main category: cs.CV

TL;DR: CueBench introduces a hierarchical, event-centric benchmark for context-aware video anomalies (VAU) and a unified evaluation framework across recognition, grounding, detection, and anticipation. It defines 14 conditional and 18 absolute anomaly events over 174 scenes and 198 attributes, and presents Cue-R1, an R1-style reinforcement-tuning approach, achieving >24% gains over state-of-the-art VLMs on CueBench.


<details>
  <summary>Details</summary>
Motivation: Current VAU research emphasizes simple anomaly detection or interpretable descriptions but lacks deep, context-aware understanding of anomalies in real-world settings. There is a need for a rigorous, unified benchmark and robust, context-aware models that can generalize across diverse contexts.

Method: Propose CueBench with an event-centric hierarchical taxonomy (14 conditional + 18 absolute anomaly events across 174 scenes and 198 attributes) and a unified evaluation suite covering recognition, temporal grounding, detection, and anticipation. Introduce Cue-R1, an R1-style reinforcement fine-tuning framework with verifiable, task-aligned, and hierarchy-refined rewards in a unified generative model.

Result: Extensive experiments on CueBench show existing vision-language models (VLMs) fall short on real-world VAU. Cue-R1 surpasses these state-of-the-art approaches by over 24% on average.

Conclusion: CueBench provides a rigorous, fair probing suite for context-aware VAU and establishes a path to more capable models through hierarchy-aware rewards and unified evaluation. The reported gains of Cue-R1 indicate substantial progress toward practical real-world anomaly understanding.

Abstract: How far are deep models from real-world video anomaly understanding (VAU)?
Current works typically emphasize on detecting unexpected occurrences deviated
from normal patterns or comprehending anomalous events with interpretable
descriptions. However, they exhibit only a superficial comprehension of
real-world anomalies, with limited breadth in complex principles and subtle
context that distinguish the anomalies from normalities, e.g., climbing cliffs
with safety gear vs. without it. To this end, we introduce CueBench, the first
of its kind Benchmark, devoted to Context-aware video anomalies within a
Unified Evaluation framework. We comprehensively establish an event-centric
hierarchical taxonomy that anchors two core event types: 14 conditional and 18
absolute anomaly events, defined by their refined semantics from diverse
contexts across 174 scenes and 198 attributes. Based on this, we propose to
unify and benchmark context-aware VAU with various challenging tasks across
recognition, temporal grounding, detection, and anticipation. This also serves
as a rigorous and fair probing evaluation suite for generative-discriminative
as well as generalized-specialized vision-language models (VLMs). To address
the challenges underlying CueBench, we further develop Cue-R1 based on R1-style
reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined
rewards in a unified generative manner. Extensive results on CueBench reveal
that, existing VLMs are still far from satisfactory real-world anomaly
understanding, while our Cue-R1 surpasses these state-of-the-art approaches by
over 24% on average.

</details>


### [69] [Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach](https://arxiv.org/abs/2511.00643)
*Oluwatosin Alabi,Meng Wei,Charlie Budd,Tom Vercauteren,Miaojing Shi*

Main category: cs.CV

TL;DR: A new task, triplet segmentation, grounds instrument-verb-target actions at the instance level. A large dataset (CholecTriplet-Seg, ~30k frames) and a model (TargetFusionNet) extend Mask2Former with target-aware fusion to fuse anatomy priors with instrument queries, achieving improved performance over baselines and enabling interpretable surgical scene understanding.


<details>
  <summary>Details</summary>
Motivation: Frame-level surgical action recognition lacks precise spatial grounding and robustly linking actions to specific instrument instances; prior spatial grounding via class activation maps is insufficient for detailed instrument-tissue interaction analysis.

Method: Create CholecTriplet-Seg, a large-scale dataset with instrument instance masks linked to verb and anatomical target annotations; propose TargetFusionNet that extends Mask2Former with a target-aware fusion mechanism to combine weak anatomy priors with instrument instance queries for accurate target prediction.

Result: TargetFusionNet outperforms existing baselines on recognition, detection, and triplet segmentation metrics, demonstrating that strong instance supervision plus weak target priors improves accuracy and robustness in surgical action understanding.

Conclusion: Triplet segmentation provides a unified framework for spatially grounding surgical action triplets; the dataset and architecture enable more interpretable and robust surgical scene understanding.

Abstract: Understanding surgical instrument-tissue interactions requires not only
identifying which instrument performs which action on which anatomical target,
but also grounding these interactions spatially within the surgical scene.
Existing surgical action triplet recognition methods are limited to learning
from frame-level classification, failing to reliably link actions to specific
instrument instances.Previous attempts at spatial grounding have primarily
relied on class activation maps, which lack the precision and robustness
required for detailed instrument-tissue interaction analysis.To address this
gap, we propose grounding surgical action triplets with instrument instance
segmentation, or triplet segmentation for short, a new unified task which
produces spatially grounded <instrument, verb, target> outputs.We start by
presenting CholecTriplet-Seg, a large-scale dataset containing over 30,000
annotated frames, linking instrument instance masks with action verb and
anatomical target annotations, and establishing the first benchmark for
strongly supervised, instance-level triplet grounding and evaluation.To learn
triplet segmentation, we propose TargetFusionNet, a novel architecture that
extends Mask2Former with a target-aware fusion mechanism to address the
challenge of accurate anatomical target prediction by fusing weak anatomy
priors with instrument instance queries.Evaluated across recognition,
detection, and triplet segmentation metrics, TargetFusionNet consistently
improves performance over existing baselines, demonstrating that strong
instance supervision combined with weak target priors significantly enhances
the accuracy and robustness of surgical action understanding.Triplet
segmentation establishes a unified framework for spatially grounding surgical
action triplets. The proposed benchmark and architecture pave the way for more
interpretable, surgical scene understanding.

</details>


### [70] [Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset](https://arxiv.org/abs/2511.00653)
*Lassi Ruoppa,Tarmo Hietala,Verneri SeppÃ¤nen,Josef Taher,Teemu Hakala,Xiaowei Yu,Antero Kukko,Harri Kaartinen,Juha HyyppÃ¤*

Main category: cs.CV

TL;DR: FGI-EMIT provides the first large-scale multispectral airborne LiDAR benchmark for ITS, showing deep learning methods outperform unsupervised ones, especially for understory trees; multispectral reflectance as extra input offers limited gains, while single-channel reflectance can help modestly; DL methods remain robust even at very low densities.


<details>
  <summary>Details</summary>
Motivation: Address the lack of large-scale multispectral LiDAR ITS benchmarks and assess whether multispectral input improves ITS performance.

Method: Benchmark four unsupervised ITS algorithms and four deep-learning ITS models. Hyperparameters for unsupervised methods were optimized via Bayesian optimization; DL models trained from scratch. The dataset comprises 1,561 manually annotated trees across wavelengths 532, 905, and 1,550 nm, with emphasis on understory trees. An ablation study explored MS reflectance as additional input features, and performance was analyzed across varying point densities.

Result: Unsupervised Treeiso achieved the highest test F1-score among its group at 52.7%. The best DL model (ForestFormer3D) reached 73.3% F1, with the largest gain in understory trees (ForestFormer3D outperforming Treeiso by 25.9 percentage points). The ablation study showed current DL methods generally fail to leverage MS reflectance as added inputs; single-channel reflectance can modestly improve accuracy, particularly for understory trees. Across densities, DL methods consistently outperform unsupervised methods even at very low densities (as low as 10 points/m^2).

Conclusion: DL-based ITS methods yield clear performance advantages over unsupervised geometry-based methods, and multispectral reflectance as extra inputs does not yet provide significant gains for current DL architectures. The dataset enables future exploration of MS LiDAR in ITS, with strong indications that DL approaches maintain robustness across varying data densities.

Abstract: Individual tree segmentation (ITS) from LiDAR point clouds is fundamental for
applications such as forest inventory, carbon monitoring and biodiversity
assessment. Traditionally, ITS has been achieved with unsupervised
geometry-based algorithms, while more recent advances have shifted toward
supervised deep learning (DL). In the past, progress in method development was
hindered by the lack of large-scale benchmark datasets, and the availability of
novel data formats, particularly multispectral (MS) LiDAR, remains limited to
this day, despite evidence that MS reflectance can improve the accuracy of ITS.
This study introduces FGI-EMIT, the first large-scale MS airborne laser
scanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550
nm, the dataset consists of 1,561 manually annotated trees, with a particular
focus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked
four conventional unsupervised algorithms and four supervised DL approaches.
Hyperparameters of unsupervised methods were optimized using a Bayesian
approach, while DL models were trained from scratch. Among the unsupervised
methods, Treeiso achieved the highest test set F1-score of 52.7%. The DL
approaches performed significantly better overall, with the best model,
ForestFormer3D, attaining an F1-score of 73.3%. The most significant difference
was observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9
percentage points. An ablation study demonstrated that current DL-based
approaches generally fail to leverage MS reflectance information when it is
provided as additional input features, although single channel reflectance can
improve accuracy marginally, especially for understory trees. A performance
analysis across point densities further showed that DL methods consistently
remain superior to unsupervised algorithms, even at densities as low as 10
points/m$^2$.

</details>


### [71] [Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering](https://arxiv.org/abs/2511.01223)
*Zahra Mehraban,Sebastien Glaser,Michael Milford,Ronald Schroeter*

Main category: cs.CV

TL;DR: Flipped-data pretraining followed by fine-tuning improves cross-domain adaptation for left-hand driving in end-to-end models, although flipped pretraining alone can harm stability; benefits observed across PilotNet and ResNet architectures.


<details>
  <summary>Details</summary>
Motivation: Generalize automated driving models to diverse road conditions, specifically adapting a right-hand driving model (PilotNet) to left-hand driving scenarios using domain adaptation via data preprocessing and transfer learning.

Method: Evaluate four training pipelines: (1) baseline trained on US right-hand data, (2) model trained on flipped US data to simulate left-hand driving, (3) model pretrained on US data then fine-tuned on Australian highways, (4) model pretrained on flipped US data then fine-tuned on Australian highways. Assess steering prediction accuracy and attention via saliency-based analysis. Validate findings on a ResNet architecture to test generalizability across architectures.

Result: Pretraining on flipped data alone worsens prediction stability due to misaligned feature representations. However, flipping pretraining followed by fine-tuning on Australian highways yields improved adaptation: lower prediction error and stronger attention on left-side cues. Similar adaptation trends were observed with ResNet, indicating generalizability of the approach across architectures.

Conclusion: Flipped-data pretraining, when followed by fine-tuning, is an effective preprocessing strategy to enhance cross-domain adaptation for left-hand driving with minimal retraining, and should be considered for domain adaptation in end-to-end driving models.

Abstract: Domain adaptation is required for automated driving models to generalize well
across diverse road conditions. This paper explores a training method for
domain adaptation to adapt PilotNet, an end-to-end deep learning-based model,
for left-hand driving conditions using real-world Australian highway data. Four
training methods were evaluated: (1) a baseline model trained on U.S.
right-hand driving data, (2) a model trained on flipped U.S. data, (3) a model
pretrained on U.S. data and then fine-tuned on Australian highways, and (4) a
model pretrained on flipped U.S. data and then finetuned on Australian
highways. This setup examines whether incorporating flipped data enhances the
model adaptation by providing an initial left-hand driving alignment. The paper
compares model performance regarding steering prediction accuracy and
attention, using saliency-based analysis to measure attention shifts across
significant road regions. Results show that pretraining on flipped data alone
worsens prediction stability due to misaligned feature representations, but
significantly improves adaptation when followed by fine-tuning, leading to
lower prediction error and stronger focus on left-side cues. To validate this
approach across different architectures, the same experiments were done on
ResNet, which confirmed similar adaptation trends. These findings emphasize the
importance of preprocessing techniques, such as flipped-data pretraining,
followed by fine-tuning to improve model adaptation with minimal retraining
requirements.

</details>


### [72] [Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control](https://arxiv.org/abs/2511.00681)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: MR-CLIP proposes a metadata-guided approach to learn unified MRI contrast representations by aligning volumetric MR images with their DICOM acquisition parameters, enabling label-efficient analysis, sequence clustering, and unsupervised quality control.


<details>
  <summary>Details</summary>
Motivation: MRI data sets suffer from substantial heterogeneity and lack standardized contrast labels across scanners and protocols, hindering large-scale automated analysis and annotation-free tasks.

Method: A metadata-guided framework (MR-CLIP) that embeds volumetric MRI by aligning images with DICOM acquisition parameters. Acquisition metadata serves as supervisory signals to learn a contrast-aware embedding; the approach also uses image-metadata distances for unsupervised data quality control.

Result: Embeddings form distinct clusters corresponding to MRI sequences; achieves better performance than supervised 3D baselines in few-shot sequence classification; enables unsupervised data-quality checks by measuring image-metadata embedding distances.

Conclusion: MR-CLIP provides a scalable, label-efficient foundation for MRI analysis across diverse datasets, enabling sequence recognition, harmonization, and quality control without manual annotations.

Abstract: Magnetic Resonance Imaging suffers from substantial data heterogeneity and
the absence of standardized contrast labels across scanners, protocols, and
institutions, which severely limits large-scale automated analysis. A unified
representation of MRI contrast would enable a wide range of downstream
utilities, from automatic sequence recognition to harmonization and quality
control, without relying on manual annotations. To this end, we introduce
MR-CLIP, a metadata-guided framework that learns MRI contrast representations
by aligning volumetric images with their DICOM acquisition parameters. The
resulting embeddings shows distinct clusters of MRI sequences and outperform
supervised 3D baselines under data scarcity in few-shot sequence
classification. Moreover, MR-CLIP enables unsupervised data quality control by
identifying corrupted or inconsistent metadata through image-metadata embedding
distances. By transforming routinely available acquisition metadata into a
supervisory signal, MR-CLIP provides a scalable foundation for label-efficient
MRI analysis across diverse clinical datasets.

</details>


### [73] [Outlier-Aware Post-Training Quantization for Image Super-Resolution](https://arxiv.org/abs/2511.00682)
*Hailing Wang,jianglin Lu,Yitian Zhang,Yun Fu*

Main category: cs.CV

TL;DR: Dual-region PTQ for image SR uses outlier/dense activation partitioning with region-specific uniform quantization and sensitivity-aware finetuning; achieves PTQ performance close to QAT with significant speedups.


<details>
  <summary>Details</summary>
Motivation: PTQ is attractive due to no ground truth or retraining, but SR PTQ methods struggle due to activation outliers linked to color information; removing outliers degrades performance. A quantization strategy that accounts for outliers and layer sensitivity is needed.

Method: Partition activations into an outlier region and a dense region, applying uniform quantization independently to each region; coupled with sensitivity-aware finetuning that emphasizes highly sensitive layers to improve overall quantization performance.

Result: Outperforms existing PTQ methods across SR networks and datasets; achieves performance comparable to QAT in most cases, with at least 75x speedup over full precision or QAT.

Conclusion: The proposed dual-region quantization with sensitivity-aware finetuning effectively handles activation outliers and layer sensitivity, narrowing the performance gap between PTQ and QAT for image super-resolution.

Abstract: Quantization techniques, including quantization-aware training (QAT) and
post-training quantization (PTQ), have become essential for inference
acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has
garnered significant attention as it eliminates the need for ground truth and
model retraining. However, existing PTQ methods for SR often fail to achieve
satisfactory performance as they overlook the impact of outliers in activation.
Our empirical analysis reveals that these prevalent activation outliers are
strongly correlated with image color information, and directly removing them
leads to significant performance degradation. Motivated by this, we propose a
dual-region quantization strategy that partitions activations into an outlier
region and a dense region, applying uniform quantization to each region
independently to better balance bit-width allocation. Furthermore, we observe
that different network layers exhibit varying sensitivities to quantization,
leading to different levels of performance degradation. To address this, we
introduce sensitivity-aware finetuning that encourages the model to focus more
on highly sensitive layers, further enhancing quantization performance.
Extensive experiments demonstrate that our method outperforms existing PTQ
approaches across various SR networks and datasets, while achieving performance
comparable to QAT methods in most scenarios with at least a 75 speedup.

</details>


### [74] [EREBUS: End-to-end Robust Event Based Underwater Simulation](https://arxiv.org/abs/2511.01381)
*Hitesh Kyatham,Arjun Suresh,Aadi Palnitkar,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: A pipeline to generate realistic synthetic data for event-based cameras mounted on an AUV in underwater environments, enabling training of vision models under challenging conditions; validated on rock detection with poor visibility and turbidity, with potential generalization to other tasks.


<details>
  <summary>Details</summary>
Motivation: Underwater scenes suffer from poor lighting, high dynamic range, and turbidity, making traditional frame-based vision brittle. Event-based cameras capture changes efficiently and are promising, but real labeled underwater data for such sensors are scarce. A synthetic data pipeline can enable training and benchmarking for these sensors in realistic conditions.

Method: Develop a pipeline that renders realistic underwater scenes and simulates an event-based camera mounted on an AUV, generating event streams under challenging visibility (poor lighting, suspended particulates). The pipeline is evaluated on a rock-detection task to demonstrate effectiveness and is claimed to generalize to other underwater tasks.

Result: The pipeline produces usable synthetic event data and demonstrates effectiveness for rock detection under adverse visibility and particulates, showing promise for training vision models. The authors also claim the approach can be generalized to other underwater tasks.

Conclusion: Synthetic data generation for event-based underwater cameras is viable and valuable for training and evaluating vision models in challenging aquatic environments, with potential applicability beyond rock detection to other tasks.

Abstract: The underwater domain presents a vast array of challenges for roboticists and
computer vision researchers alike, such as poor lighting conditions and high
dynamic range scenes. In these adverse conditions, traditional vision
techniques struggle to adapt and lead to suboptimal performance. Event-based
cameras present an attractive solution to this problem, mitigating the issues
of traditional cameras by tracking changes in the footage on a frame-by-frame
basis. In this paper, we introduce a pipeline which can be used to generate
realistic synthetic data of an event-based camera mounted to an AUV (Autonomous
Underwater Vehicle) in an underwater environment for training vision models. We
demonstrate the effectiveness of our pipeline using the task of rock detection
with poor visibility and suspended particulate matter, but the approach can be
generalized to other underwater tasks.

</details>


### [75] [Evolve to Inspire: Novelty Search for Diverse Image Generation](https://arxiv.org/abs/2511.00686)
*Alex Inch,Passawis Chaiyapattanaporn,Yuchen Zhu,Yuan Lu,Ting-Wen Ko,Davide Paglieri*

Main category: cs.CV

TL;DR: WANDER is a novelty-search based prompt optimization method that uses LLM-driven semantic evolution and CLIP-based novelty to generate diverse image sets from a single prompt, with emitters guiding exploration of distinct prompt-space regions; it outperforms baselines on diversity.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models produce high-fidelity outputs but suffer from limited diversity, hindering ideation and exploratory tasks; existing prompt optimization often targets aesthetics rather than creative diversity.

Method: WANDER employs novelty search on natural-language prompts. It uses a Large Language Model to semantically mutate/promote diversity in prompts and CLIP embeddings to quantify novelty. Emitters steer the search into distinct regions of prompt space. Evaluation uses FLUX-DEV for generation and GPT-4o-mini for mutation, comparing against evolutionary prompt optimization baselines.

Result: WANDER significantly outperforms existing evolutionary prompt optimization baselines in diversity metrics; ablation studies confirm the efficacy of emitters in boosting diversity.

Conclusion: Novelty-driven prompt evolution, aided by prompt-space emitters and LLM-based semantic mutations, can substantially increase the diversity of diffusion-generated images, supporting more effective creative ideation workflows.

Abstract: Text-to-image diffusion models, while proficient at generating high-fidelity
im- ages, often suffer from limited output diversity, hindering their
application in exploratory and ideation tasks. Existing prompt optimization
techniques typically target aesthetic fitness or are ill-suited to the creative
visual domain. To address this shortcoming, we introduce WANDER, a novelty
search-based approach to generating diverse sets of images from a single input
prompt. WANDER operates directly on natural language prompts, employing a Large
Language Model (LLM) for semantic evolution of diverse sets of images, and
using CLIP embeddings to quantify novelty. We additionally apply emitters to
guide the search into distinct regions of the prompt space, and demonstrate
that they boost the diversity of the generated images. Empirical evaluations
using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that
WANDER significantly outperforms existing evolutionary prompt optimization
baselines in diversity metrics. Ablation studies confirm the efficacy of
emitters.

</details>


### [76] [SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation](https://arxiv.org/abs/2511.01501)
*Yufeng Jin,Niklas Funk,Vignesh Prasad,Zechu Li,Mathias Franzius,Jan Peters,Georgia Chalvatzaki*

Main category: cs.CV

TL;DR: Probabilistic SE(3) flow-matching for 6D pose estimation; models full pose distributions with sample-based estimates; state-of-the-art on Real275, YCB-V, LM-O; enables uncertainty-aware manipulation.


<details>
  <summary>Details</summary>
Motivation: Deterministic pose predictors struggle with multi-modality caused by occlusions and object symmetries, leading to overconfident, biased estimates; there is a need to quantify pose uncertainty and reason about multiple plausible poses.

Method: Proposes a flow-matching framework on the SE(3) manifold to learn a probabilistic mapping from a base distribution to the target pose distribution. It uses sample-based estimates to represent the pose distribution, handling symmetry and occlusion, and supports downstream uncertainty-aware reasoning.

Result: Achieves state-of-the-art results on Real275, YCB-V, and LM-O; demonstrates benefits of sample-based pose estimates for downstream tasks such as active perception and uncertainty-aware grasp synthesis.

Conclusion: Presents a principled probabilistic approach to 6D pose estimation that captures multi-modality and uncertainty, enabling improved perception-guided manipulation in ambiguous scenarios.

Abstract: Object pose estimation is a fundamental problem in robotics and computer
vision, yet it remains challenging due to partial observability, occlusions,
and object symmetries, which inevitably lead to pose ambiguity and multiple
hypotheses consistent with the same observation. While deterministic deep
networks achieve impressive performance under well-constrained conditions, they
are often overconfident and fail to capture the multi-modality of the
underlying pose distribution. To address these challenges, we propose a novel
probabilistic framework that leverages flow matching on the SE(3) manifold for
estimating 6D object pose distributions. Unlike existing methods that regress a
single deterministic output, our approach models the full pose distribution
with a sample-based estimate and enables reasoning about uncertainty in
ambiguous cases such as symmetric objects or severe occlusions. We achieve
state-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our
sample-based pose estimates can be leveraged in downstream robotic manipulation
tasks such as active perception for disambiguating uncertain viewpoints or
guiding grasp synthesis in an uncertainty-aware manner.

</details>


### [77] [Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics](https://arxiv.org/abs/2511.00698)
*Taifour Yousra,Beghdadi Azeddine,Marie Luong,Zuheng Ming*

Main category: cs.CV

TL;DR: Loss functions used in LDCT enhancement with deep learning show misalignment with perceptual quality metrics; optimizing solely on PSNR/SSIM may not reflect clinical image quality, so losses should consider perceptual/image-quality metrics.


<details>
  <summary>Details</summary>
Motivation: To investigate how different loss functions drive LDCT image quality and whether their optimization aligns with objective quality metrics beyond traditional metrics.

Method: Conduct an objective analysis/review of various loss functions (e.g., MSE, adversarial losses, and architecture-specific customized losses) used in LDCT enhancement and assess their consistency with image quality metrics.

Result: The analysis reveals inconsistencies between loss functions and quality metrics; highlights the need to consider image quality metrics when developing new loss functions for LDCT image quality enhancement.

Conclusion: Future loss-function design for LDCT enhancement should incorporate perceptual and clinically relevant image quality metrics to ensure improvements align with actual perceptual quality rather than just PSNR/SSIM.

Abstract: Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to
mitigate high exposure side effects, but often suffers from noise and artifacts
that affect diagnostic accuracy. To tackle this issue, deep learning models
have been developed to enhance LDCT images. Various loss functions have been
employed, including classical approaches such as Mean Square Error and
adversarial losses, as well as customized loss functions(LFs) designed for
specific architectures. Although these models achieve remarkable performance in
terms of PSNR and SSIM, these metrics are limited in their ability to reflect
perceptual quality, especially for medical images. In this paper, we focus on
one of the most critical elements of DL-based architectures, namely the loss
function. We conduct an objective analysis of the relevance of different loss
functions for LDCT image quality enhancement and their consistency with image
quality metrics. Our findings reveal inconsistencies between LFs and quality
metrics, and highlight the need of consideration of image quality metrics when
developing a new loss function for image quality enhancement.

</details>


### [78] [Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning](https://arxiv.org/abs/2511.01502)
*Mengtan Zhang,Zizhan Guo,Hongbo Zhao,Yi Feng,Zuyi Xiong,Yue Wang,Shaoyi Du,Hanli Wang,Rui Fan*

Main category: cs.CV

TL;DR: DiMoDE introduces a discriminative joint learning framework for depth and ego-motion that leverages component-wise geometric constraints from rigid flow. It aligns camera frames, transforms flows, and enforces per-motion constraints to derive depth and translation components with closed-form relations, achieving state-of-the-art results on public datasets and a new diverse real-world dataset.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised depth and ego-motion methods often treat ego-motion as auxiliary or mix motion types, limiting the use of strong geometric constraints and hurting robustness under diverse conditions. A discriminative, component-wise treatment of motion could enforce precise geometric relations and improve both depth and motion estimates.

Method: For consecutive frames, the network first aligns the optical axes and imaging planes of source and target cameras. The optical flows are then transformed via these alignments, and deviations are measured to impose geometric constraints on each ego-motion component. This enables targeted refinement. The alignments reformulate joint learning into coaxial and coplanar forms, enabling mutual derivation of depth and each translation component through closed-form geometric relationships, providing complementary constraints.

Result: DiMoDE achieves state-of-the-art performance on multiple public datasets and on a newly collected diverse real-world dataset, with particular robustness under challenging conditions.

Conclusion: A general depth and ego-motion joint learning framework that uses discriminative, component-wise geometric constraints and camera-alignment-based reformulations to improve depth robustness and ego-motion estimation; code will be released publicly.

Abstract: Unsupervised learning of depth and ego-motion, two fundamental 3D perception
tasks, has made significant strides in recent years. However, most methods
treat ego-motion as an auxiliary task, either mixing all motion types or
excluding depth-independent rotational motions in supervision. Such designs
limit the incorporation of strong geometric constraints, reducing reliability
and robustness under diverse conditions. This study introduces a discriminative
treatment of motion components, leveraging the geometric regularities of their
respective rigid flows to benefit both depth and ego-motion estimation. Given
consecutive video frames, network outputs first align the optical axes and
imaging planes of the source and target cameras. Optical flows between frames
are transformed through these alignments, and deviations are quantified to
impose geometric constraints individually on each ego-motion component,
enabling more targeted refinement. These alignments further reformulate the
joint learning process into coaxial and coplanar forms, where depth and each
translation component can be mutually derived through closed-form geometric
relationships, introducing complementary constraints that improve depth
robustness. DiMoDE, a general depth and ego-motion joint learning framework
incorporating these designs, achieves state-of-the-art performance on multiple
public datasets and a newly collected diverse real-world dataset, particularly
under challenging conditions. Our source code will be publicly available at
mias.group/DiMoDE upon publication.

</details>


### [79] [Validating Deep Models for Alzheimer's 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data](https://arxiv.org/abs/2511.00728)
*Hugo Massaroli,Hernan Chaves,Pilar Anania,Mauricio Farez,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CV

TL;DR: Transformers do not clearly outperform CNNs for cross-domain AD diagnosis from 18F-FDG PET; models trained on ADNI show strong within-domain performance but substantial generalization gaps on a Latin American cohort, highlighting domain shift and the need for population-aware validation and domain adaptation.


<details>
  <summary>Details</summary>
Motivation: Evaluate generalization of deep learning models for Alzheimer's disease diagnosis across underrepresented populations and assess whether Transformer architectures confer advantages over CNNs in this cross-domain setting.

Method: Train convolutional and Transformer-based models on the ADNI 18F-FDG PET dataset and evaluate on both ADNI and a novel Latin American cohort (FLENI); conduct ablation studies on per-image normalization and sampling strategies; perform occlusion sensitivity analyses to interpret model attention; compare architectures.

Result: On ADNI, models achieve high AUCs (up to ~0.96â0.97); on FLENI, performance drops notably (to ~0.80â0.82). Architectures perform similarly; per-image normalization and proper sampling are key for generalization; occlusion analysis shows ADNI-trained models focus on canonical hypometabolic regions for AD but attention becomes less clear for other classes and FLENI data.

Conclusion: Population-aware validation is essential; domain adaptation and cohort diversification are needed to ensure robust clinical deployment. Transformers do not demonstrate clear superiority for this task under the studied conditions.

Abstract: Deep learning models have shown strong performance in diagnosing Alzheimer's
disease (AD) using neuroimaging data, particularly 18F-FDG PET scans, with
training datasets largely composed of North American cohorts such as those in
the Alzheimer's Disease Neuroimaging Initiative (ADNI). However, their
generalization to underrepresented populations remains underexplored. In this
study, we benchmark convolutional and Transformer-based models on the ADNI
dataset and assess their generalization performance on a novel Latin American
clinical cohort from the FLENI Institute in Buenos Aires, Argentina. We show
that while all models achieve high AUCs on ADNI (up to .96, .97), their
performance drops substantially on FLENI (down to .82, .80, respectively),
revealing a significant domain shift. The tested architectures demonstrated
similar performance, calling into question the supposed advantages of
transformers for this specific task. Through ablation studies, we identify
per-image normalization and a correct sampling selection as key factors for
generalization. Occlusion sensitivity analysis further reveals that models
trained on ADNI, generally attend to canonical hypometabolic regions for the AD
class, but focus becomes unclear for the other classes and for FLENI scans.
These findings highlight the need for population-aware validation of diagnostic
AI models and motivate future work on domain adaptation and cohort
diversification.

</details>


### [80] [PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model](https://arxiv.org/abs/2511.01571)
*Wenqi Liang,Gan Sun,Yao He,Jiahua Dong,Suyan Dai,Ivan Laptev,Salman Khan,Yang Cong*

Main category: cs.CV

TL;DR: PixelVLA is a VLA model enabling pixel-level reasoning and multimodal prompting, achieving higher manipulation success with much lower pretraining cost; introduces Pixel-160K; open-sourced.


<details>
  <summary>Details</summary>
Motivation: VLAs currently struggle with pixel-level scene understanding and depend heavily on textual prompts, limiting real-world visuomotor flexibility.

Method: A visuomotor instruction tuning framework built with a multiscale pixel-aware encoder and a visual prompting encoder; a two-stage automated annotation pipeline produces Pixel-160K pixel-level annotations from existing robot data.

Result: On three VLA benchmarks and two model variants, PixelVLA improves manipulation success rates by 10.1%-17.8% over OpenVLA while using only 1.5% of its pretraining cost.

Conclusion: PixelVLA can enhance existing VLAs to enable more accurate, efficient, and versatile robot control; dataset and code will be released as open source.

Abstract: Vision-Language-Action models (VLAs) are emerging as powerful tools for
learning generalizable visuomotor control policies. However, current VLAs are
mostly trained on large-scale image-text-action data and remain limited in two
key ways: (i) they struggle with pixel-level scene understanding, and (ii) they
rely heavily on textual prompts, which reduces their flexibility in real-world
settings. To address these challenges, we introduce PixelVLA, the first VLA
model designed to support both pixel-level reasoning and multimodal prompting
with text and visual inputs. Our approach is built on a new visuomotor
instruction tuning framework that integrates a multiscale pixel-aware encoder
with a visual prompting encoder. To train PixelVLA effectively, we further
propose a two-stage automated annotation pipeline that generates Pixel-160K, a
large-scale dataset with pixel-level annotations derived from existing robot
data. Experiments on three standard VLA benchmarks and two VLA model variants
show that PixelVLA improves manipulation success rates by 10.1%-17.8% over
OpenVLA, while requiring only 1.5% of its pretraining cost. These results
demonstrate that PixelVLA can be integrated into existing VLAs to enable more
accurate, efficient, and versatile robot control in complex environments. The
dataset and code will be released as open source.

</details>


### [81] [Towards classification-based representation learning for place recognition on LiDAR scans](https://arxiv.org/abs/2511.00738)
*Dmitrii Khizbullin,Maksim Konoplia*

Main category: cs.CV

TL;DR: Framing place recognition as a supervised multi-class classification task instead of contrastive learning. Discretizes location into labels for LiDAR scans and trains an encoderâdecoder to predict the position, achieving competitive NuScenes results with improved training efficiency and stability.


<details>
  <summary>Details</summary>
Motivation: Contrastive learning is effective but can be data-hungry and unstable. A direct supervised approach may simplify training, reduce computational cost, and offer more stable optimization while maintaining accuracy in place recognition.

Method: Create discrete location labels for LiDAR scans. Train an encoderâdecoder model to classify each scan into its labeled position (multi-class classification), likely using cross-entropy loss. Evaluation performed on the NuScenes dataset.

Result: The approach yields competitive performance compared to contrastive-learning-based methods, with advantages in training efficiency and stability.

Conclusion: A supervised multi-class framing for place recognition is a viable alternative to contrastive learning, offering similar accuracy with simpler training dynamics and better efficiency on NuScenes.

Abstract: Place recognition is a crucial task in autonomous driving, allowing vehicles
to determine their position using sensor data. While most existing methods rely
on contrastive learning, we explore an alternative approach by framing place
recognition as a multi-class classification problem. Our method assigns
discrete location labels to LiDAR scans and trains an encoder-decoder model to
classify each scan's position directly. We evaluate this approach on the
NuScenes dataset and show that it achieves competitive performance compared to
contrastive learning-based methods while offering advantages in training
efficiency and stability.

</details>


### [82] [3EED: Ground Everything Everywhere in 3D](https://arxiv.org/abs/2511.01755)
*Rong Li,Yuhao Dong,Tianshuai Hu,Ao Liang,Youquan Liu,Dongyue Lu,Liang Pan,Lingdong Kong,Junwei Liang,Ziwei Liu*

Main category: cs.CV

TL;DR: A large-scale, multi-platform, multi-modal 3D grounding benchmark (3EED) with RGB and LiDAR data from vehicle, drone, and quadruped platforms, covering 128k objects and 22k referring expressions; introduces a scalable annotation pipeline and platform-aware normalization; shows cross-platform generalization gaps; dataset and toolkit released.


<details>
  <summary>Details</summary>
Motivation: Current 3D grounding benchmarks are indoor-focused, single-platform, and small-scale, hindering open-world and cross-platform generalization in embodied agents. There is a need for outdoor, multi-platform data and robust evaluation protocols to advance language-driven 3D perception.

Method: Develop a scalable annotation pipeline that combines vision-language model prompting with human verification; implement platform-aware normalization and cross-modal alignment; establish benchmark protocols for in-domain and cross-platform evaluations.

Result: 3EED comprises over 128,000 objects and 22,000 validated referring expressions across diverse outdoor scenes, approximately 10x larger than prior datasets; cross-platform baselines reveal significant performance gaps, underscoring generalization challenges; dataset and benchmark toolkit released for research use.

Conclusion: 3EED advances outdoor, cross-platform 3D grounding research by providing a large-scale benchmark and tools, highlighting generalization gaps and guiding future work in language-guided 3D perception.

Abstract: Visual grounding in 3D is the key for embodied agents to localize
language-referred objects in open-world environments. However, existing
benchmarks are limited to indoor focus, single-platform constraints, and small
scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark
featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We
provide over 128,000 objects and 22,000 validated referring expressions across
diverse outdoor scenes -- 10x larger than existing datasets. We develop a
scalable annotation pipeline combining vision-language model prompting with
human verification to ensure high-quality spatial grounding. To support
cross-platform learning, we propose platform-aware normalization and
cross-modal alignment techniques, and establish benchmark protocols for
in-domain and cross-platform evaluations. Our findings reveal significant
performance gaps, highlighting the challenges and opportunities of
generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released
to advance future research in language-driven 3D embodied perception.

</details>


### [83] [Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models](https://arxiv.org/abs/2511.00749)
*Tanvi Dinkar,Aiqi Jiang,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CV

TL;DR: Generative AI encodes biased Western beauty norms, producing predominantly lighter-skinned, younger, and hypersexualised images; negative prompts elevate NSFW content and target non-binary individuals, revealing intersectional biases with societal implications.


<details>
  <summary>Details</summary>
Motivation: Assess how generative AI models encode and propagate beauty ideals, and the societal harms from biased representations and negative prompting.

Method: Two image-generation pipelines (text-to-image; text-to-language-model-to-image) guided by a structured beauty taxonomy; prompts applied to three language models and two image models to generate 5984 images; 1200 images evaluated by women and non-binary social media users via Likert-scale within-subjects design.

Result: Generated dataset shows 86.5% lighter-skin depictions, 22% NSFW content despite SFW training, and 74% younger age demographics; non-binary representations skew younger and more hypersexualised; negative/ugly prompts (e.g., 'a wide nose') lead to higher NSFW ratings across genders.

Conclusion: Demographic biases in generative AI are pervasive and reinforced by model developers and prompting practices; implications include data stream pollution and erosion of non-conforming features, necessitating policy attention and responsible design.

Abstract: Social media has exacerbated the promotion of Western beauty norms, leading
to negative self-image, particularly in women and girls, and causing harm such
as body dysmorphia. Increasingly content on the internet has been artificially
generated, leading to concerns that these norms are being exaggerated. The aim
of this work is to study how generative AI models may encode 'beauty' and erase
'ugliness', and discuss the implications of this for society. To investigate
these aims, we create two image generation pipelines: a text-to-image model and
a text-to-language model-to image model. We develop a structured beauty
taxonomy which we use to prompt three language models (LMs) and two
text-to-image models to cumulatively generate 5984 images using our two
pipelines. We then recruit women and non-binary social media users to evaluate
1200 of the images through a Likert-scale within-subjects study. Participants
show high agreement in their ratings. Our results show that 86.5% of generated
images depicted people with lighter skin tones, 22% contained explicit content
despite Safe for Work (SFW) training, and 74% were rated as being in a younger
age demographic. In particular, the images of non-binary individuals were rated
as both younger and more hypersexualised, indicating troubling intersectional
effects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such
as "a wide nose") consistently produced higher Not SFW (NSFW) ratings
regardless of gender. This work sheds light on the pervasive demographic biases
related to beauty standards present in generative AI models -- biases that are
actively perpetuated by model developers, such as via negative prompting. We
conclude by discussing the implications of this on society, which include
pollution of the data streams and active erasure of features that do not fall
inside the stereotype of what is considered beautiful by developers.

</details>


### [84] [A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection](https://arxiv.org/abs/2511.00777)
*Anis Suttan Shahrir,Zakiah Ayop,Syarulnaziah Anawar,Norulzahrah Mohd Zainudin*

Main category: cs.CV

TL;DR: IoT-enabled durian farm animal detection using a YOLOv5 + SSD fusion with Telegram alerts and automated deterrent sounds; achieves 90% elephant, 85% boar, 70% monkey accuracy; best in daytime, performance drops at night; practical framework for detection, notification, and deterrence.


<details>
  <summary>Details</summary>
Motivation: Durian plantations face animal intrusions causing crop damage and financial loss. Traditional monitoring is inefficient and requires human intervention. Advances in ML and IoT offer real-time detection and deterrence, butç°æ systems rely on single detectors, limited notification channels, and few deterrents.

Method: A system that fuses YOLOv5 and SSD detectors for real-time animal detection in durian crops, deployed on an IoT setup. Detected intrusions trigger Telegram notifications for rapid farmer response and automated deterrent sounds (e.g., tiger roar). Evaluation shows class-specific accuracies for elephant (90%), boar (85%), and monkey (70%), with daytime performance higher and night-time performance lower, for both still images and video.

Result: Fusion of two detectors improves detection accuracy across target species. Real-time monitoring with automated alerts and deterrent mechanism is feasible. The approach yields highest accuracy in daytime and degrades at night, regardless of image type.

Conclusion: The study offers a practical, integrated framework that combines detection, notification, and deterrence for automated farming solutions and points to potential future innovations in IoT-enabled agricultural monitoring.

Abstract: Durian plantation suffers from animal intrusions that cause crop damage and
financial loss. The traditional farming practices prove ineffective due to the
unavailability of monitoring without human intervention. The fast growth of
machine learning and Internet of Things (IoT) technology has led to new ways to
detect animals. However, current systems are limited by dependence on single
object detection algorithms, less accessible notification platforms, and
limited deterrent mechanisms. This research suggests an IoT-enabled animal
detection system for durian crops. The system integrates YOLOv5 and SSD object
detection algorithms to improve detection accuracy. The system provides
real-time monitoring, with detected intrusions automatically reported to
farmers via Telegram notifications for rapid response. An automated sound
mechanism (e.g., tiger roar) is triggered once the animal is detected. The
YOLO+SSD model achieved accuracy rates of elephant, boar, and monkey at 90%,
85% and 70%, respectively. The system shows the highest accuracy in daytime and
decreases at night, regardless of whether the image is still or a video.
Overall, this study contributes a comprehensive and practical framework that
combines detection, notification, and deterrence, paving the way for future
innovations in automated farming solutions.

</details>


### [85] [Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking](https://arxiv.org/abs/2511.00785)
*Juan Wang,Yasutomo Kawanishi,Tomo Miyazaki,Zhijie Wang,Shinichiro Omachi*

Main category: cs.CV

TL;DR: A granularity-consistent 2D mask tracking framework, combined with a three-stage curriculum learning pipeline, distills coherent 3D instance segmentation from fragmented, conflicting 2D priors, achieving state-of-the-art results and open-vocabulary capability.


<details>
  <summary>Details</summary>
Motivation: 3D instance segmentation struggles with inconsistent pseudo-labels when 2D masks are transferred independently across frames, leading to conflicting 3D labels and degraded performance. Temporal consistency and structured learning are needed to produce reliable 3D representations from weak 2D priors.

Method: Introduce Granularity-Consistent automatic 2D Mask Tracking to maintain temporal correspondences across frames and eliminate conflicting pseudo labels, integrated with a three-stage curriculum learning framework that progresses from fragmented single-view data to unified multi-view annotations and finally global full-scene supervision.

Result: The approach yields consistent and accurate 3D segmentations, achieving state-of-the-art results on standard benchmarks and demonstrating open-vocabulary capability.

Conclusion: By enforcing temporal consistency in 2D priors and guiding learning through a staged curriculum, the method robustly distills a coherent 3D representation from initially fragmented and contradictory 2D priors, enabling superior 3D instance segmentation performance.

Abstract: 3D instance segmentation is an important task for real-world applications. To
avoid costly manual annotations, existing methods have explored generating
pseudo labels by transferring 2D masks from foundation models to 3D. However,
this approach is often suboptimal since the video frames are processed
independently. This causes inconsistent segmentation granularity and
conflicting 3D pseudo labels, which degrades the accuracy of final
segmentation. To address this, we introduce a Granularity-Consistent automatic
2D Mask Tracking approach that maintains temporal correspondences across
frames, eliminating conflicting pseudo labels. Combined with a three-stage
curriculum learning framework, our approach progressively trains from
fragmented single-view data to unified multi-view annotations, ultimately
globally coherent full-scene supervision. This structured learning pipeline
enables the model to progressively expose to pseudo-labels of increasing
consistency. Thus, we can robustly distill a consistent 3D representation from
initially fragmented and contradictory 2D priors. Experimental results
demonstrated that our method effectively generated consistent and accurate 3D
segmentations. Furthermore, the proposed method achieved state-of-the-art
results on standard benchmarks and open-vocabulary ability.

</details>


### [86] [FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data](https://arxiv.org/abs/2511.00795)
*Viswa Chaitanya Marella,Suhasnadh Reddy Veluru,Sai Teja Erukude*

Main category: cs.CV

TL;DR: FedOnco-Bench provides a reproducible benchmark for privacy-aware FL in medical image segmentation, showing trade-offs between utility and privacy across methods under data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns and data heterogeneity in federated learning for medical imaging necessitate standardized benchmarks and evaluation of privacy leakage alongside segmentation performance.

Method: Use synthetic oncologic CT scans with tumor annotations to evaluate FL methods (FedAvg, FedProx, FedBN, FedAvg+DP-SGD) on segmentation (Dice score) and privacy leakage (membership inference attack AUC).

Result: FedAvg yields highest Dice around 0.85 but higher privacy leakage (AUC ~0.72); DP-SGD improves privacy (AUC ~0.25) at cost of Dice (~0.79). FedProx and FedBN provide balanced performance under non-identical data distributions.

Conclusion: FedOnco-Bench is an open-source platform that standardizes benchmarking and development of privacy-preserving FL methods for medical image segmentation.

Abstract: Federated Learning (FL) allows multiple institutions to cooperatively train
machine learning models while retaining sensitive data at the source, which has
great utility in privacy-sensitive environments. However, FL systems remain
vulnerable to membership-inference attacks and data heterogeneity. This paper
presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using
synthetic oncologic CT scans with tumor annotations. It evaluates segmentation
performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and
FedAvg with DP-SGD. Results show a distinct trade-off between privacy and
utility: FedAvg is high performance (Dice around 0.85) with more privacy
leakage (attack AUC about 0.72), while DP-SGD provides a higher level of
privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx
and FedBN offer balanced performance under heterogeneous data, especially with
non-identical distributed client data. FedOnco-Bench serves as a standardized,
open-source platform for benchmarking and developing privacy-preserving FL
methods for medical image segmentation.

</details>


### [87] [Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing](https://arxiv.org/abs/2511.00801)
*Zhihui Chen,Mengling Feng*

Main category: cs.CV

TL;DR: A new 50K-image medically validated dataset for instruction-based medical image editing across chest X-ray, brain MRI, and fundus photography, with lesion addition/removal, created using Gemini-2.5-Flash-Image and evaluated by an LLM-based judge; includes 37K failed attempts for alignment research; publicly available.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of large-scale, high-quality, openly accessible datasets for medically constrained image editing, enabling development and evaluation of multimodal LLMs in clinical editing tasks.

Method: Generate bidirectional edits (lesion addition/removal) on real medical images using Gemini-2.5-Flash-Image. Apply a medically grounded, rubric-driven LLM-as-Judge (instruction compliance, structural plausibility, realism, fidelity preservation) with history-aware iterative refinements up to five rounds. Collect 50K edited images and 37K failed attempts with full conversation logs for preference learning and alignment research.

Result: A 50K-image Med-Banana-50K dataset spanning 3 modalities and 23 diseases, with high-quality edits validated by a medical rubric, plus a rich set of failed attempts for alignment research. Code and data are publicly available.

Conclusion: Med-Banana-50K provides a scalable, medically validated resource to train and evaluate medical image editing models, facilitating next-generation model development and alignment research through both successful edits and failed attempts.

Abstract: Recent advances in multimodal large language models have enabled remarkable
medical image editing capabilities. However, the research community's progress
remains constrained by the absence of large-scale, high-quality, and openly
accessible datasets built specifically for medical image editing with strict
anatomical and clinical constraints. We introduce Med-Banana-50K, a
comprehensive 50K-image dataset for instruction-based medical image editing
spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23
disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image
to generate bidirectional edits (lesion addition and removal) from real medical
images. What distinguishes Med-Banana-50K from general-domain editing datasets
is our systematic approach to medical quality control: we employ LLM-as-Judge
with a medically grounded rubric (instruction compliance, structural
plausibility, realism, and fidelity preservation) and history-aware iterative
refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K
includes 37K failed attempts with full conversation logs for preference
learning and alignment research. By providing this large-scale, medically
validated, and fully documented resource, Med-Banana-50K establishes a
foundation for training and evaluating the next generation of medical image
editing models.Our dataset and code are publicly available at
[https://github.com/richardChenzhihui/med-banana-50k].

</details>


### [88] [GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding](https://arxiv.org/abs/2511.00810)
*Shijie Zhou,Viet Dac Lai,Hao Tan,Jihyung Kil,Wanrong Zhu,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: A lightweight, coordinate-free GUI grounding method (GUI-AIMA) fine-tunes multimodal LLMs to align their native attention with patch-level grounding signals, enabling efficient GUI grounding with minimal data and optional zoom-in stages, achieving strong results for 3B models.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on explicit coordinate generation from visual inputs and to exploit MLLMs' intrinsic grounding capabilities, improving data efficiency and computational costs for GUI grounding.

Method: An attention-based, coordinate-free supervised fine-tuning framework that aligns intrinsic multimodal attention with patch-wise grounding signals. It uses multi-head aggregation on simplified query-visual attention matrices to adapt signals across diverse instructions and supports a plug-and-play zoom-in stage.

Result: GUI-AIMA-3B trained on 85k screenshots achieves state-of-the-art performance among 3B models, with average accuracies of 58.6% on ScreenSpot-Pro and 62.2% on OSWorld-G.

Conclusion: The approach demonstrates data-efficient triggering of MLLMs' native grounding for GUI tasks, provides a practical coordinate-free alternative to coordinate generation, and offers plug-in zoom-in capability for improved flexibility.

Abstract: Graphical user interface (GUI) grounding is a key function of computer-use
agents, which maps natural-language instructions to actionable screen regions.
Existing approaches based on Multimodal Large Language Models (MLLMs) typically
formulate it as a text-based coordinate generation task, yet directly
generating precise coordinates from visual inputs remains challenging and
computationally intensive. An intuitive way to implement GUI grounding is to
first select visual patches relevant to the instructions and then determine the
precise click location within those patches. Based on the observations that
general MLLMs have some native grounding capability, nested within their
attentions, we propose GUI-AIMA, an attention-based and coordinate-free
supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns
the intrinsic multimodal attention of MLLMs with patch-wise grounding signals.
These signals are calculated adaptively for diverse user instructions by
multi-head aggregation on simplified query-visual attention matrices. Besides,
its coordinate-free manner can easily integrate a plug-and-play zoom-in stage.
GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional
data efficiency and verifying that light training can trigger the native
grounding capability of MLLMs. It achieves state-of-the-art performance among
3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%
on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA

</details>


### [89] [TA-LSDiff:Topology-Aware Diffusion Guided by a Level Set Energy for Pancreas Segmentation](https://arxiv.org/abs/2511.00815)
*Yue Gou,Fanghui Song,Yuming Xing,Shengzhu Shi,Zhichang Guo,Boying Wu*

Main category: cs.CV

TL;DR: A topology-aware diffusion-based segmentation framework (TA-LSDiff) combines a diffusion probabilistic model with level-set energy and a pixel-adaptive refinement module to deliver high-precision pancreas segmentation, achieving state-of-the-art results across four public datasets without explicit geometric evolution.


<details>
  <summary>Details</summary>
Motivation: Pancreas segmentation is difficult due to small organ size, low contrast, and large topological variations. Traditional level-set methods underutilize topology; deep networks preserve details but risk losing boundary geometry. A method that jointly enforces topology and boundary precision while leveraging deep features is needed.

Method: Integrates topology-aware diffusion probabilistic model with a level-set energy to guide implicit curve evolution; four-term energy function combining input image and deep features; pixel-adaptive refinement module modulates energy via affinity weights from neighboring evidence; an ablation study validates components.

Result: Ablation studies show each component's contribution. On four public pancreas datasets, TA-LSDiff achieves state-of-the-art accuracy and outperforms existing methods.

Conclusion: TA-LSDiff provides a practical, accurate pancreas segmentation solution that effectively merges topology-aware diffusion with energy-guided evolution and local refinement, delivering superior boundary accuracy without explicit geometric evolution.

Abstract: Pancreas segmentation in medical image processing is a persistent challenge
due to its small size, low contrast against adjacent tissues, and significant
topological variations. Traditional level set methods drive boundary evolution
using gradient flows, often ignoring pointwise topological effects. Conversely,
deep learning-based segmentation networks extract rich semantic features but
frequently sacrifice structural details. To bridge this gap, we propose a novel
model named TA-LSDiff, which combined topology-aware diffusion probabilistic
model and level set energy, achieving segmentation without explicit geometric
evolution. This energy function guides implicit curve evolution by integrating
the input image and deep features through four complementary terms. To further
enhance boundary precision, we introduce a pixel-adaptive refinement module
that locally modulates the energy function using affinity weighting from
neighboring evidence. Ablation studies systematically quantify the contribution
of each proposed component. Evaluations on four public pancreas datasets
demonstrate that TA-LSDiff achieves state-of-the-art accuracy, outperforming
existing methods. These results establish TA-LSDiff as a practical and accurate
solution for pancreas segmentation.

</details>


### [90] [OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models](https://arxiv.org/abs/2511.00821)
*Ruoxiang Huang,Xindian Ma,Rundong Kong,Zhen Yuan,Peng Zhang*

Main category: cs.CV

TL;DR: OMEGA introduces modality-specific position encoding (MSPE) and global adaptive encoding step scaling (GAESS) to preserve text sequential structure and visual spatial coherence in vision-language models, improving performance across architectures, especially on visually intensive tasks (up to 3.43% gains on a 3B model).


<details>
  <summary>Details</summary>
Motivation: Current VLMs use modality-unified positional indexing (1D/2D) that treats text and vision tokens uniformly, neglecting their distinct structural properties and sequential/spatial continuity. This limits cross-modal integration and efficiency.

Method: OMEGA employs MSPE to assign positional indices along separate coordinate dimensions for text and vision, preserving each modality's inherent structure. GAESS adaptively adjusts the position encoding step size for visual tokens based on embedding entropy across modalities to balance information density in the encoding space.

Result: Experiments show consistent gains across diverse architectures and VQA benchmarks. Notably, visual-intensive tasks see up to 3.43% improvement on Qwen2.5-VL-3B, with similar gains on larger models like Qwen2.5-VL-7B and LLaVA-v1.5-7B.

Conclusion: Modality-specific encoding with adaptive step sizing improves multimodal fusion in VLMs, offering generalizable benefits across models and datasets.

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance across
various multimodal tasks, where position encoding plays a vital role in
modeling both the sequential structure of textual information and the spatial
structure of visual information. However, current VLMs commonly adopt
modality-unified 1D or 2D positional indexing strategies, which treat textual
and visual tokens uniformly without accounting for their distinct structural
properties and sequential continuity for text and spatial coherence for vision.
To address this limitation, we propose OMEGA, a novel position encoding
framework that employs Modality-Specific Position Encoding (MSPE) to assign
positional indices while preserving the inherent structures of each modality
across separate coordinate dimensions. Additionally, to align the information
density of multimodal data in the positional index space, OMEGA introduces
Global Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the
position encoding step size of visual tokens based on the embedding entropy of
both modalities. Experimental results demonstrate that OMEGA consistently
enhances VLM performance across diverse architectures and VQA benchmarks. On
visual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline
position encoding strategies on Qwen2.5-VL-3B, with consistent gains observed
across larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.

</details>


### [91] [Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack](https://arxiv.org/abs/2511.00831)
*Xin Liu,Aoyang Zhou,Aoyang Zhou*

Main category: cs.CV

TL;DR: A novel attack, Local Shuffle and Sample-based Attack (LSSA), enhances the transferability of multimodal adversarial examples for Visual-Language Pre-training (VLP) models by increasing input diversity through local image block shuffling and sampling, and by jointly crafting adversarial texts from original and sampled images. It achieves superior transferability across multiple VLP models and downstream tasks and outperforms existing attacks on large vision-language models.


<details>
  <summary>Details</summary>
Motivation: To overcome overfitting in cross-modal adversarial attacks that rely heavily on information from one modality, which hurts transferability when attacking different models or modalities. By expanding input diversity and leveraging both original and perturbed samples, LSSA aims to improve generalization of multimodal adversarial examples.

Method: 1) Randomly shuffle a local block of the image to create diversity. 2) Expand the data by generating adversarial images from the shuffled variant and sampling around them. 3) Use both the original and sampled images to craft adversarial texts, producing multimodal adversarial examples.

Result: Empirical evaluations across multiple VLP models and downstream tasks show that LSSA significantly improves the transferability of multimodal adversarial examples compared to prior attacks. It also outperforms other advanced attacks on Large Vision-Language Models.

Conclusion: LSSA demonstrates that increasing input diversity via local image block shuffling and sampling, combined with joint multimodal text crafting, yields stronger and more transferable multimodal adversarial examples. This highlights the importance of data augmentation-like strategies in adversarial methods and has implications for defense against multimodal attacks.

Abstract: Visual-Language Pre-training (VLP) models have achieved significant
performance across various downstream tasks. However, they remain vulnerable to
adversarial examples. While prior efforts focus on improving the adversarial
transferability of multimodal adversarial examples through cross-modal
interactions, these approaches suffer from overfitting issues, due to a lack of
input diversity by relying excessively on information from adversarial examples
in one modality when crafting attacks in another. To address this issue, we
draw inspiration from strategies in some adversarial training methods and
propose a novel attack called Local Shuffle and Sample-based Attack (LSSA).
LSSA randomly shuffles one of the local image blocks, thus expanding the
original image-text pairs, generating adversarial images, and sampling around
them. Then, it utilizes both the original and sampled images to generate the
adversarial texts. Extensive experiments on multiple models and datasets
demonstrate that LSSA significantly enhances the transferability of multimodal
adversarial examples across diverse VLP models and downstream tasks. Moreover,
LSSA outperforms other advanced attacks on Large Vision-Language Models.

</details>


### [92] [Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials](https://arxiv.org/abs/2511.00833)
*Yifan Pu,Jixuan Ying,Qixiu Li,Tianzhu Ye,Dongchen Han,Xiaochen Wang,Ziyi Wang,Xinyu Shao,Gao Huang,Xiu Li*

Main category: cs.CV

TL;DR: Introduce Visual-Contrast Attention (VCA): a drop-in MHSA replacement for Vision Transformers that uses pooled visual-contrast tokens and dual positive/negative streams to concentrate on discriminative relations, reducing complexity from O(N^2 C) to O(N n C) (with n << N) and adding ~0.3M parameters; achieves notable accuracy gains on ImageNet and improves generative model FID, with ablations highlighting the value of spatial pooling and dual positional embeddings.


<details>
  <summary>Details</summary>
Motivation: MHSA in Vision Transformers is computationally heavy due to quadratic query-key interactions, often focusing on weak or redundant correlations. There is a need for a discriminative, efficient attention mechanism that preserves or improves accuracy for recognition and generative tasks while reducing compute.

Method: Replace MHSA with Visual-Contrast Attention (VCA): distill each head's dense query field into a small set of spatially pooled visual-contrast tokens, split into learnable positive and negative streams, and perform their differential interaction to emphasize discriminative distinctions. Reduces complexity from O(N^2 C) to O(N n C) where n << N. Architecture-agnostic drop-in replacement; leverages spatial pooling and dual positional embeddings; demonstrated across recognition and class-conditional generation.

Result: Empirically, VCA improves DeiT-Tiny top-1 on ImageNet-1K from 72.2% to 75.6% (+3.4). It also improves three strong hierarchical ViTs by up to 3.1%. In class-conditional ImageNet generation, it lowers FID-50K by 2.1 to 5.2 points across both diffusion (DiT) and flow (SiT) models. Ablations show that spatial pooling provides low-variance global cues and that dual positional embeddings are indispensable for contrastive reasoning; their combination yields the strongest synergy.

Conclusion: VCA offers a simple, faster, and sharper Vision Transformer with minimal parameter overhead (~0.3M) and no extra FLOPs, and it remains architecture-agnostic. The authors provide source code for reproduction.

Abstract: Vision Transformers (ViTs) have become a universal backbone for both image
recognition and image generation. Yet their Multi-Head Self-Attention (MHSA)
layer still performs a quadratic query-key interaction for every token pair,
spending the bulk of computation on visually weak or redundant correlations. We
introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that
injects an explicit notion of discrimination while reducing the theoretical
complexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's
dense query field into a handful of spatially pooled visual-contrast tokens,
then splits them into a learnable positive and negative stream whose
differential interaction highlights what truly separates one region from
another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone,
requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA
lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and
improves three strong hierarchical ViTs by up to 3.1%, while in
class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points
across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm
that (i) spatial pooling supplies low-variance global cues, (ii) dual
positional embeddings are indispensable for contrastive reasoning, and (iii)
combining the two in both stages yields the strongest synergy. VCA therefore
offers a simple path towards faster and sharper Vision Transformers. The source
code is available at https://github.com/LeapLabTHU/LinearDiff.

</details>


### [93] [Parameter Interpolation Adversarial Training for Robust Image Classification](https://arxiv.org/abs/2511.00836)
*Xin Liu,Yichen Yang,Kun He,John E. Hopcroft*

Main category: cs.CV

TL;DR: Interpolate model parameters between epochs during adversarial training and apply Normalized Mean Square Error to logits, improving robustness for CNNs and ViTs.


<details>
  <summary>Details</summary>
Motivation: Adversarial training often suffers from training instability, oscillations, and overfitting, which degrade robustness.

Method: Introduce Parameter Interpolation Adversarial Training (PIAT) by interpolating parameters of the previous and current epochs to stabilize decision boundaries; use NMSE to align the relative magnitudes of logits between clean and adversarial samples rather than their absolute values.

Result: Empirical results on benchmark datasets show that PIAT improves robustness and convergence for both CNNs and ViTs, reducing overfitting compared to standard adversarial training.

Conclusion: PIAT provides a simple, effective framework to boost adversarial robustness with stabilized training, and NMSE offers additional gains by better aligning logit magnitudes across clean and adversarial inputs.

Abstract: Though deep neural networks exhibit superior performance on various tasks,
they are still plagued by adversarial examples. Adversarial training has been
demonstrated to be the most effective method to defend against adversarial
attacks. However, existing adversarial training methods show that the model
robustness has apparent oscillations and overfitting issues in the training
process, degrading the defense efficacy. To address these issues, we propose a
novel framework called Parameter Interpolation Adversarial Training (PIAT).
PIAT tunes the model parameters between each epoch by interpolating the
parameters of the previous and current epochs. It makes the decision boundary
of model change more moderate and alleviates the overfitting issue, helping the
model converge better and achieving higher model robustness. In addition, we
suggest using the Normalized Mean Square Error (NMSE) to further improve the
robustness by aligning the relative magnitude of logits between clean and
adversarial examples rather than the absolute magnitude. Extensive experiments
conducted on several benchmark datasets demonstrate that our framework could
prominently improve the robustness of both Convolutional Neural Networks (CNNs)
and Vision Transformers (ViTs).

</details>


### [94] [OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks](https://arxiv.org/abs/2511.00846)
*Zhihao Peng,Cheng Wang,Shengyuan Liu,Zhiying Liang,Yixuan Yuan*

Main category: cs.CV

TL;DR: Introduces OmniBrainBench, a comprehensive multimodal VQA benchmark for brain imaging with 15 modalities, ~9.5k VQA pairs and 31.7k images across 15 clinical tasks; evaluates 24 MLLMs; finds a significant gap vs physicians and a visual-to-clinical reasoning bottleneck.


<details>
  <summary>Details</summary>
Motivation: Existing brain-imaging VQA benchmarks lack modality breadth and clinical workflow depth; there is a need to evaluate MLLMsâ multimodal and procedural reasoning across the full clinical continuum.

Method: Assembles OmniBrainBench from 15 imaging modalities using 30 sources; creates 9,527 validated VQA pairs and 31,706 images; frames 15 multi-stage clinical tasks validated by radiologists; benchmarks 24 models (open-source, medical, proprietary).

Result: Proprietary MLLMs outperform others yet still lag physicians; medical MLLMs vary; open-source MLLMs generally behind but excel on some tasks; MLLMs struggle with complex preoperative tasks, indicating a gap in visual-to-clinical reasoning.

Conclusion: OmniBrainBench establishes a new standard for evaluating MLLMs in brain imaging analysis and exposes gaps relative to expert clinical reasoning; benchmark and code are released.

Abstract: Brain imaging analysis is vital for diagnosing and treating brain disorders,
and multimodal large language models (MLLMs) are increasingly assisting in that
analysis. However, current brain-oriented visual question-answering (VQA)
benchmarks either cover a few imaging modalities or are limited to
coarse-grained pathological descriptions, hindering a comprehensive assessment
of MLLMs throughout the full clinical continuum. To address these, we introduce
OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically
designed to assess the multimodal comprehension capabilities of MLLMs in brain
imaging analysis.OmniBrainBench consists of 15 distinct brain imaging
modalities collected from 30 verified medical sources, yielding 9,527 validated
VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15
multi-stage clinical tasks rigorously validated by a professional radiologist.
Evaluation of 24 state-of-the-art models, including open-source, medical, and
proprietary MLLMs, highlights the substantial challenges posed by
OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5)
beat open-source and medical models but lag physicians; (2) medical MLLMs vary
widely in performance; (3) open-source MLLMs trail overall but excel in
specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks,
revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new
standard for evaluating and advancing MLLMs in brain imaging analysis,
highlighting gaps compared to expert clinical reasoning. We release it at
benchmark \& code.

</details>


### [95] [Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction](https://arxiv.org/abs/2511.00858)
*Yu Liu,Zhijie Liu,Zedong Yang,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: An Occlusion-Aware Diffusion Model (ODM) reconstructs occluded pedestrian motion to forecast crossing intentions under occlusion, using an occlusion-aware diffusion transformer in denoising and an occlusion mask-guided reverse process; shows robust gains on PIE and JAAD.


<details>
  <summary>Details</summary>
Motivation: Occlusion during observation degrades prediction of pedestrian crossing intentions; current methods struggle to infer hidden motion and context under occlusion, hindering safe navigation.

Method: ODM combines diffusion modeling with an occlusion-aware transformer to estimate noise features for occluded patterns during denoising; employs an occlusion mask-guided reverse process to effectively leverage observed information and reduce error accumulation; evaluated on PIE and JAAD benchmarks.

Result: Empirical results indicate ODM achieves more robust performance than existing methods across varying occlusion scenarios on PIE and JAAD.

Conclusion: Occlusion-aware diffusion with mask-guided inference effectively reconstructs occluded motion and improves crossing-intention prediction, contributing to safer navigation for mobile robots and intelligent vehicles.

Abstract: Predicting pedestrian crossing intentions is crucial for the navigation of
mobile robots and intelligent vehicles. Although recent deep learning-based
models have shown significant success in forecasting intentions, few consider
incomplete observation under occlusion scenarios. To tackle this challenge, we
propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded
motion patterns and leverages them to guide future intention prediction. During
the denoising stage, we introduce an occlusion-aware diffusion transformer
architecture to estimate noise features associated with occluded patterns,
thereby enhancing the model's ability to capture contextual relationships in
occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse
process is introduced to effectively utilize observation information, reducing
the accumulation of prediction errors and enhancing the accuracy of
reconstructed motion features. The performance of the proposed method under
various occlusion scenarios is comprehensively evaluated and compared with
existing methods on popular benchmarks, namely PIE and JAAD. Extensive
experimental results demonstrate that the proposed method achieves more robust
performance than existing methods in the literature.

</details>


### [96] [Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion](https://arxiv.org/abs/2511.00859)
*Jaehyun Park,Konyul Park,Daehun Kim,Junseo Park,Jun Won Choi*

Main category: cs.CV

TL;DR: Layer-Wise Modality Decomposition (LMD) provides post-hoc, model-agnostic attribution of modality contributions across all layers of a sensor-fusion model in autonomous driving, enabling attribution of predictions to individual input modalities; evaluated on cameraâradar, cameraâLiDAR, and cameraâradarâLiDAR settings with structured perturbation metrics, and code is released.


<details>
  <summary>Details</summary>
Motivation: In autonomous driving, perception failures can be catastrophic. Sensor fusion entangles modality information, making it hard to interpret which sensor contributed to a prediction. There is a need for interpretable explanations that attribute decisions to specific modalities to improve transparency and safety.

Method: Propose Layer-Wise Modality Decomposition (LMD), a post-hoc, model-agnostic interpretability method that disentangles modality-specific information across all layers of a pretrained fusion model. It is applied to pretrained fusion models under cameraâradar, cameraâLiDAR, and cameraâradarâLiDAR settings. Evaluation uses structured perturbation-based metrics and modality-wise visual decompositions.

Result: LMD effectively attributes predictions to individual modalities across all layers, validated on multiple sensor-fusion setups and demonstrated via structured perturbations and modality-wise visualizations. The approach shows practical applicability for interpreting high-capacity multimodal architectures.

Conclusion: LMD enables first-of-its-kind attribution of autonomous-driving perception predictions to input modalities within sensor-fusion systems. It provides an interpretable, post-hoc tool for understanding modality contributions, with open-source code for broader adoption; future work could explore scalability, broader modality sets, and integration into safety-critical pipelines.

Abstract: In autonomous driving, transparency in the decision-making of perception
models is critical, as even a single misperception can be catastrophic. Yet
with multi-sensor inputs, it is difficult to determine how each modality
contributes to a prediction because sensor information becomes entangled within
the fusion network. We introduce Layer-Wise Modality Decomposition (LMD), a
post-hoc, model-agnostic interpretability method that disentangles
modality-specific information across all layers of a pretrained fusion model.
To our knowledge, LMD is the first approach to attribute the predictions of a
perception model to individual input modalities in a sensor-fusion system for
autonomous driving. We evaluate LMD on pretrained fusion models under
camera-radar, camera-LiDAR, and camera-radar-LiDAR settings for autonomous
driving. Its effectiveness is validated using structured perturbation-based
metrics and modality-wise visual decompositions, demonstrating practical
applicability to interpreting high-capacity multimodal architectures. Code is
available at https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition.

</details>


### [97] [GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2511.00908)
*Heng Zheng,Yuling Shi,Xiaodong Gu,Haochen You,Zijian Zhang,Lubin Gan,Hao Zhang,Wenjun Huang,Jin Huang*

Main category: cs.CV

TL;DR: GraphGeo uses a heterogeneous GNN with a dual-level debate mechanism to convert agent disagreements into improved visual geo-localization performance.


<details>
  <summary>Details</summary>
Motivation: Visual geo-localization struggles due to knowledge gaps, limited database coverage, and conflicting predictions. While LVLMs enable location reasoning, they struggle across diverse regions and complex scenes. Existing multi-agent systems lack mechanisms to handle conflicts effectively, limiting gains from collaboration.

Method: GraphGeo builds a heterogeneous graph neural network with typed edges representing supportive collaboration, competitive argumentation, and knowledge transfer. It employs a dual-level debate mechanism: node-level refinement and edge-level argumentation modeling, along with a cross-level topology refinement strategy that co-evolves graph structure and agent representations.

Result: Experiments on multiple benchmarks show GraphGeo significantly outperforms state-of-the-art methods, indicating that structured debate among heterogeneous agents improves geo-localization accuracy.

Conclusion: Transforming cognitive conflicts between agents into structured debate yields enhanced geo-localization performance.

Abstract: Visual geo-localization requires extensive geographic knowledge and
sophisticated reasoning to determine image locations without GPS metadata.
Traditional retrieval methods are constrained by database coverage and quality.
Recent Large Vision-Language Models (LVLMs) enable direct location reasoning
from image content, yet individual models struggle with diverse geographic
regions and complex scenes. Existing multi-agent systems improve performance
through model collaboration but treat all agent interactions uniformly. They
lack mechanisms to handle conflicting predictions effectively. We propose
\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph
neural networks for visual geo-localization. Our approach models diverse debate
relationships through typed edges, distinguishing supportive collaboration,
competitive argumentation, and knowledge transfer. We introduce a dual-level
debate mechanism combining node-level refinement and edge-level argumentation
modeling. A cross-level topology refinement strategy enables co-evolution
between graph structure and agent representations. Experiments on multiple
benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art
methods. Our framework transforms cognitive conflicts between agents into
enhanced geo-localization accuracy through structured debate.

</details>


### [98] [Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs](https://arxiv.org/abs/2511.00916)
*Yan Shu,Chi Liu,Robin Chen,Derek Li,Bryan Dai*

Main category: cs.CV

TL;DR: Fleming-VL is a unified end-to-end medical multimodal LLM that uses data-centric strategiesâlong-context pretraining, rare medical data fine-tuning (including videos and underrepresented 2D modalities), and expanded 3D/video evaluationâto achieve state-of-the-art results on medical VQA, video QA, and 3D imaging, with public release.


<details>
  <summary>Details</summary>
Motivation: Medical data are highly heterogeneous (2D images, 3D volumes, video) and have a large domain gap, making it hard to build a single unified MLLM for clinical use.

Method: Three data-centric strategies: (1) scale pretraining with long-context data from natural and medical sources; (2) fine-tune with rare medical data, including holistic video analysis and underrepresented 2D modalities like ultrasound and dermoscopy; (3) expand evaluation to 3D volumetric and video benchmarks. Optimized via supervised fine-tuning (SFT) and group relative policy optimization (GRPO) across multiple model scales.

Result: Achieves state-of-the-art performance on medical VQA, video QA, and 3D medical image understanding across multiple benchmarks; Fleming-VL is publicly released to promote reproducible medical AI progress.

Conclusion: A data-centric, unified medical MLLM like Fleming-VL can effectively handle heterogeneous medical modalities and enable transparent, reproducible advancement in medical AI, with broad clinical potential.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
effectiveness in various general-domain scenarios, such as visual question
answering and image captioning. Recently, researchers have increasingly focused
on empowering MLLMs with medical conversational abilities, which hold
significant promise for clinical applications. However, medical data presents
unique challenges due to its heterogeneous nature -- encompassing diverse
modalities including 2D images, 3D volumetric scans, and temporal video
sequences. The substantial domain gap and data format inconsistencies across
these modalities have hindered the development of unified medical MLLMs. To
address these challenges, we propose Fleming-VL, a unified end-to-end framework
for comprehensive medical visual understanding across heterogeneous modalities.
Fleming-VL tackles this problem from a data-centric perspective through three
key strategies: (1) scaling up pretraining by integrating long-context data
from both natural and medical-specific domains; (2) complementing fine-tuning
with rare medical data, including holistic video analysis and underrepresented
2D modalities such as ultrasound and dermoscopy images; (3) extending existing
evaluation frameworks to incorporate 3D volumetric and video understanding
benchmarks. Through supervised fine-tuning (SFT) and group relative policy
optimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive
experiments demonstrate that Fleming-VL achieves state-of-the-art performance
across multiple benchmarks, including medical VQA, video QA, and 3D medical
image understanding. We publicly release Fleming-VL to promote transparent,
reproducible, and auditable progress in medical AI.

</details>


### [99] [Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval](https://arxiv.org/abs/2511.00925)
*Hanwen Su,Ge Song,Jiyan Wang,Yuanbo Zhu*

Main category: cs.CV

TL;DR: Dynamic Multi-level Weighted Alignment Network for zero-shot sketch-based image retrieval (ZS-SBIR) using uni-modal feature extraction, cross-modal weighting, and weighted quadruplet loss to improve balance; achieves state-of-the-art on Sketchy, TU-Berlin, QuickDraw.


<details>
  <summary>Details</summary>
Motivation: ZS-SBIR suffers from imbalanced modality samples and inconsistent training information, leading to sub-optimal retrieval performance. There is a need to improve cross-modal alignment quality and balance domain contributions during training.

Method: Three components: (i) Uni-modal Feature Extraction Module with a CLIP text encoder and ViT to extract textual and visual tokens; (ii) Cross-modal Multi-level Weighting Module producing an alignment weight list via local and global aggregation blocks to measure sketchâimage alignment quality; (iii) Weighted Quadruplet Loss Module to balance domain contributions within the triplet-based loss.

Result: Empirical evaluation on Sketchy, TU-Berlin, and QuickDraw datasets shows the proposed method delivers superior performance compared with state-of-the-art ZS-SBIR methods.

Conclusion: The Dynamic Multi-level Weighted Alignment Network improves ZS-SBIR by addressing modality imbalance and training information quality through weighted cross-modal alignment and a weighted quadruplet loss, achieving better results on standard benchmarks.

Abstract: The problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved
increasing attention due to its wide applications, e.g. e-commerce. Despite
progress made in this field, previous works suffer from using imbalanced
samples of modalities and inconsistent low-quality information during training,
resulting in sub-optimal performance. Therefore, in this paper, we introduce an
approach called Dynamic Multi-level Weighted Alignment Network for ZS-SBIR. It
consists of three components: (i) a Uni-modal Feature Extraction Module that
includes a CLIP text encoder and a ViT for extracting textual and visual
tokens, (ii) a Cross-modal Multi-level Weighting Module that produces an
alignment weight list by the local and global aggregation blocks to measure the
aligning quality of sketch and image samples, (iii) a Weighted Quadruplet Loss
Module aiming to improve the balance of domains in the triplet loss.
Experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and
QuickDraw, show our method delivers superior performances over the
state-of-the-art ZS-SBIR methods.

</details>


### [100] [EVTAR: End-to-End Try on with Additional Unpaired Visual Reference](https://arxiv.org/abs/2511.00956)
*Liuzhuozheng Li,Yue Gong,Shanyuan Liu,Bo Cheng,Yuhang Ma,Liebucha Wu,Dengyang Jiang,Zanyi Wang,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: EVTAR is an end-to-end virtual try-on model that uses target garments with additional reference images to preserve texture, enabling simple inference using only the source image and garment input and avoiding masks, densepose, or segmentation maps.


<details>
  <summary>Details</summary>
Motivation: Reduce reliance on heavy, labor-intensive inputs (pose, densepose, segmentation) while achieving higher-quality, texture-preserving try-on that works in realistic settings.

Method: Two-stage training. The model fits the target garment directly onto the source image, uses additional reference images of different people wearing the same clothes to preserve garment texture and fine details, and trains with supplementary references and unpaired person images. No masks, densepose, or segmentation maps needed.

Result: Evaluations on two widely used benchmarks across diverse tasks show the approach consistently improves try-on quality and texture preservation, validating the effectiveness of incorporating reference images.

Conclusion: EVTAR delivers end-to-end virtual try-on with added references, offering simpler inference, better garment texture preservation, and realistic dressing effects suitable for real-world applications.

Abstract: We propose EVTAR, an End-to-End Virtual Try-on model with Additional
Reference, that directly fits the target garment onto the person image while
incorporating reference images to enhance try-on accuracy. Most existing
virtual try-on approaches rely on complex inputs such as agnostic person
images, human pose, densepose, or body keypoints, making them labor-intensive
and impractical for real-world applications. In contrast, EVTAR adopts a
two-stage training strategy, enabling simple inference with only the source
image and the target garment inputs. Our model generates try-on results without
masks, densepose, or segmentation maps. Moreover, EVTAR leverages additional
reference images of different individuals wearing the same clothes to preserve
garment texture and fine-grained details better. This mechanism is analogous to
how humans consider reference models when choosing outfits, thereby simulating
a more realistic and high-quality dressing effect. We enrich the training data
with supplementary references and unpaired person images to support these
capabilities. We evaluate EVTAR on two widely used benchmarks and diverse
tasks, and the results consistently validate the effectiveness of our approach.

</details>


### [101] [A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis](https://arxiv.org/abs/2511.00962)
*Dongheng Lin,Mengxue Qu,Kunyang Han,Jianbo Jiao,Xiaojie Jin,Yunchao Wei*

Main category: cs.CV

TL;DR: A unified zero-shot framework chains temporal, spatial, and textual reasoning for video anomaly analysis, achieving state-of-the-art zero-shot results without training.


<details>
  <summary>Details</summary>
Motivation: To address the lack of explainability and generalization in existing video anomaly methods, which typically output frame-level scores or task-specific results without rich context.

Method: Chained test-time reasoning that sequentially connects temporal detection, spatial localization, and textual explanation. Intra-task refinement of temporal detections and inter-task chaining for spatial/semantic understanding using prompt design, with no gradient updates or additional data.

Result: Achieves state-of-the-art zero-shot performance across multiple video anomaly detection, localization, and explanation benchmarks, with improved interpretability and generalization.

Conclusion: Prompt-based task chaining unlocks foundation-model reasoning for holistic, interpretable video anomaly analysis in a fully zero-shot manner; promising for practical deployment without extra data or training.

Abstract: Most video-anomaly research stops at frame-wise detection, offering little
insight into why an event is abnormal, typically outputting only frame-wise
anomaly scores without spatial or semantic context. Recent video anomaly
localization and video anomaly understanding methods improve explainability but
remain data-dependent and task-specific. We propose a unified reasoning
framework that bridges the gap between temporal detection, spatial
localization, and textual explanation. Our approach is built upon a chained
test-time reasoning process that sequentially connects these tasks, enabling
holistic zero-shot anomaly analysis without any additional training.
Specifically, our approach leverages intra-task reasoning to refine temporal
detections and inter-task chaining for spatial and semantic understanding,
yielding improved interpretability and generalization in a fully zero-shot
manner. Without any additional data or gradients, our method achieves
state-of-the-art zero-shot performance across multiple video anomaly detection,
localization, and explanation benchmarks. The results demonstrate that careful
prompt design with task-wise chaining can unlock the reasoning power of
foundation models, enabling practical, interpretable video anomaly analysis in
a fully zero-shot manner. Project Page:
https://rathgrith.github.io/Unified_Frame_VAA/.

</details>


### [102] [VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel](https://arxiv.org/abs/2511.00981)
*Suzhong Fu,Rui Sun,Xuan Ding,Jingqi Dong,Yiming Yang,Yao Zhu,Min Chang Jordan Ren,Delin Deng,Angelica Aviles-Rivero,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: VesSAM proposes a domain-specific, PEFT-friendly framework for 2D vessel segmentation, improving accuracy and generalization with a convolutional adapter, a multi-prompt encoder using anatomical prompts, and a lightweight decoder; achieves state-of-the-art gains with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Accurate vessel segmentation is hard due to thin, branching vessels and low texture; SAM underperforms on vascular structures; need anatomy-guided prompts and efficient adapters to improve segmentation performance.

Method: Integrates (1) a convolutional adapter to enhance local texture; (2) a multi-prompt encoder combining skeletons, bifurcation points, and segment midpoints via hierarchical cross-attention; (3) a lightweight mask decoder to reduce jagged edges; plus an automated pipeline to generate structured multi-prompt annotations and a benchmark across 8 datasets and 5 modalities.

Result: Outperforms state-of-the-art PEFT-based SAM variants by >10% Dice and >13% IoU; competitive with fully fine-tuned methods with far fewer parameters; shows strong OoD generalization, surpassing baselines on average OoD Dice and IoU.

Conclusion: VesSAM is an effective and efficient framework for 2D vessel segmentation that leverages anatomy-aware prompts and adapters to deliver robust performance across datasets and modalities, with strong generalization capabilities.

Abstract: Accurate vessel segmentation is critical for clinical applications such as
disease diagnosis and surgical planning, yet remains challenging due to thin,
branching structures and low texture contrast. While foundation models like the
Segment Anything Model (SAM) have shown promise in generic segmentation, they
perform sub-optimally on vascular structures. In this work, we present VesSAM,
a powerful and efficient framework tailored for 2D vessel segmentation. VesSAM
integrates (1) a convolutional adapter to enhance local texture features, (2) a
multi-prompt encoder that fuses anatomical prompts, including skeletons,
bifurcation points, and segment midpoints, via hierarchical cross-attention,
and (3) a lightweight mask decoder to reduce jagged artifacts. We also
introduce an automated pipeline to generate structured multi-prompt
annotations, and curate a diverse benchmark dataset spanning 8 datasets across
5 imaging modalities. Experimental results demonstrate that VesSAM consistently
outperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13%
IoU, and achieves competitive performance compared to fully fine-tuned methods,
with significantly fewer parameters. VesSAM also generalizes well to
out-of-distribution (OoD) settings, outperforming all baselines in average OoD
Dice and IoU.

</details>


### [103] [MID: A Self-supervised Multimodal Iterative Denoising Framework](https://arxiv.org/abs/2511.00997)
*Chang Nie,Tianchen Deng,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: A self-supervised, multimodal iterative denoising (MID) framework that learns to denoise complex non-linear noise without clean data by modeling noise as an iterative process and using two networks to estimate the noise state and the increment; achieves SOTA on multiple CV tasks and shows promise in biomedical domains.


<details>
  <summary>Details</summary>
Motivation: Real-world data are often corrupted by complex, non-linear noise, which defeats rule-based denoising and requires clean data for supervised methods; a self-supervised approach that learns noise characteristics directly from noisy inputs is needed.

Method: Model data as a state in a non-linear noise accumulation process. Iteratively add noise and train two neural networks: one to estimate the current noise step, and another to predict/subtract the corresponding noise increment. Use a first-order Taylor expansion to locally linearize the non-linear noise process. The framework learns from noisy inputs without requiring paired clean data and is multimodal.

Result: Demonstrates robustness, adaptability, and state-of-the-art performance across four classic computer vision tasks, with strong performance and adaptability in biomedical and bioinformatics tasks.

Conclusion: MID provides a novel, self-supervised, multimodal denoising framework capable of handling complex non-linear noise across domains without clean data, via iterative noise modeling and local linearization, showing broad applicability.

Abstract: Data denoising is a persistent challenge across scientific and engineering
domains. Real-world data is frequently corrupted by complex, non-linear noise,
rendering traditional rule-based denoising methods inadequate. To overcome
these obstacles, we propose a novel self-supervised multimodal iterative
denoising (MID) framework. MID models the collected noisy data as a state
within a continuous process of non-linear noise accumulation. By iteratively
introducing further noise, MID learns two neural networks: one to estimate the
current noise step and another to predict and subtract the corresponding noise
increment. For complex non-linear contamination, MID employs a first-order
Taylor expansion to locally linearize the noise process, enabling effective
iterative removal. Crucially, MID does not require paired clean-noisy datasets,
as it learns noise characteristics directly from the noisy inputs. Experiments
across four classic computer vision tasks demonstrate MID's robustness,
adaptability, and consistent state-of-the-art performance. Moreover, MID
exhibits strong performance and adaptability in tasks within the biomedical and
bioinformatics domains.

</details>


### [104] [Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya](https://arxiv.org/abs/2511.01000)
*Hassan Ugail,Ismail Lujain Jaleel*

Main category: cs.CV

TL;DR: A multimodal, identical-feature pipeline for Goya authentication uses visual and X-ray images with GLCM/LBP/entropy/energy/color features fed into an optimized One-Class SVM; on a small dataset (n=24), it achieves 97.8% accuracy and low FPR, with a case study achieving 92.3% confidence, outperforming single-modal methods.


<details>
  <summary>Details</summary>
Motivation: Goya authentication is hampered by heterogeneous stylistic evolution and forgery patterns. Relying on a single imaging modality may miss complementary cues. A unified multimodal approach aims to leverage information from both visual and radiographic images to improve robustness and accuracy in authentication.

Method: Apply identical feature extraction across both visual and X-ray images: Grey-Level Co-occurrence Matrix descriptors, Local Binary Patterns, entropy, energy, and colour distribution. Concatenate or jointly process these features and train an optimized One-Class SVM with hyperparameter tuning. Dataset: 24 authenticated Goya paintings with corresponding X-ray images; evaluation via an 80/20 train-test split and 10-fold cross-validation. Case study on Un Gigante to illustrate practical efficacy.

Result: High performance with 97.8% classification accuracy and 0.022 false positive rate on the dataset. Case study reports 92.3% authentication confidence for Un Gigante. Claimed substantial improvement over single-modal approaches.

Conclusion: The study suggests that applying identical computational methods to both visual and radiographic modalities yields meaningful gains in art authentication, supporting the value of unified multimodal feature analysis. However, the small sample size and potential generalization concerns warrant cautious interpretation and further validation.

Abstract: Art authentication of Francisco Goya's works presents complex computational
challenges due to his heterogeneous stylistic evolution and extensive
historical patterns of forgery. We introduce a novel multimodal machine
learning framework that applies identical feature extraction techniques to both
visual and X-ray radiographic images of Goya paintings. The unified feature
extraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors,
Local Binary Patterns, entropy measures, energy calculations, and colour
distribution analysis applied consistently across both imaging modalities. The
extracted features from both visual and X-ray images are processed through an
optimised One-Class Support Vector Machine with hyperparameter tuning. Using a
dataset of 24 authenticated Goya paintings with corresponding X-ray images,
split into an 80/20 train-test configuration with 10-fold cross-validation, the
framework achieves 97.8% classification accuracy with a 0.022 false positive
rate. Case study analysis of ``Un Gigante'' demonstrates the practical efficacy
of our pipeline, achieving 92.3% authentication confidence through unified
multimodal feature analysis. Our results indicate substantial performance
improvement over single-modal approaches, establishing the effectiveness of
applying identical computational methods to both visual and radiographic
imagery in art authentication applications.

</details>


### [105] [HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images](https://arxiv.org/abs/2511.01013)
*Mohammad Amanour Rahman*

Main category: cs.CV

TL;DR: A hybrid CNN-Transformer (HyFormer-Net) for joint segmentation and classification of breast ultrasound, achieving strong performance and interpretable outputs, with a notably limited cross-dataset generalization that improves via progressive fine-tuning on target-domain data.


<details>
  <summary>Details</summary>
Motivation: Address challenges in breast ultrasound imagingâspeckle, operator dependency, indistinct boundariesâby combining global context modeling (Transformers) with local feature extraction (CNNs) and providing intrinsic interpretability.

Method: A dual-branch encoder (EfficientNet-B3 + Swin Transformer) with multi-scale hierarchical fusion blocks feeds an attention-gated decoder. Two interpretability streams: intrinsic attention validation (IoU, mean 0.86) and Grad-CAM for classification reasoning. Includes ensemble modeling and ablation studies; assesses cross-dataset generalization with zero-shot and progressive fine-tuning on limited target-domain data.

Result: On BUSI, Dice 0.761 Â±0.072; accuracy 93.2%. Malignant recall 92.1 Â±2.2%. Ensemble: Dice 90.2%, accuracy 99.5%, malignant recall 100%. Ablations: multi-scale fusion +16.8% Dice; attention gates +5.9%. Cross-dataset: zero-shot Dice 0.058; 10% target data yields 92.5% performance; 50% target data yields 77.3% Dice (exceeding source-domain 76.1%).

Conclusion: First cross-dataset study for hybrid CNN-Transformers in breast ultrasound; domain shift exists; targeted fine-tuning with modest target-domain data can restore or surpass source-domain performance, supporting practical generalization and indicating interpretability through attention-based measures.

Abstract: B-mode ultrasound for breast cancer diagnosis faces challenges: speckle,
operator dependency, and indistinct boundaries. Existing deep learning suffers
from single-task learning, architectural constraints (CNNs lack global context,
Transformers local features), and black-box decision-making. These gaps hinder
clinical adoption.
  We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous
segmentation and classification with intrinsic interpretability. Its
dual-branch encoder integrates EfficientNet-B3 and Swin Transformer via
multi-scale hierarchical fusion blocks. An attention-gated decoder provides
precision and explainability. We introduce dual-pipeline interpretability: (1)
intrinsic attention validation with quantitative IoU verification (mean: 0.86),
and (2) Grad-CAM for classification reasoning.
  On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +/- 0.072 and
accuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant
Recall of 92.1 +/- 2.2% ensures minimal false negatives. Ensemble modeling
yields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant
Recall, eliminating false negatives. Ablation studies confirm multi-scale
fusion contributes +16.8% Dice and attention gates add +5.9%.
  Crucially, we conduct the first cross-dataset generalization study for hybrid
CNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058),
confirming domain shift. However, progressive fine-tuning with only 10%
target-domain data (68 images) recovers 92.5% performance. With 50% data, our
model achieves 77.3% Dice, exceeding source-domain performance (76.1%) and
demonstrating true generalization.

</details>


### [106] [FastBoost: Progressive Attention with Dynamic Scaling for Efficient Deep Learning](https://arxiv.org/abs/2511.01026)
*JunXi Yuan*

Main category: cs.CV

TL;DR: FastBoost introduces a parameter-efficient neural architecture using Dynamically Scaled Progressive Attention (DSPA) to achieve state-of-the-art CIFAR results with very few parameters and low FLOPs. It blends adaptive attention, train-stage scaling, and residual adaptation into enhanced MBConv blocks, yielding dual attention paths and cascaded refinements with real-time weight updates, enabling edge deployment without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: To push the accuracy-parameter-FLOPs frontier on CIFAR datasets by designing a compact, dynamically adaptable attention mechanism that can be embedded into efficient convolutions for edge devices.

Method: DSPA combines Adaptive Fusion (learned channel-spatial blending with dynamic weights), Phase Scaling (training-stage intensity modulation from 0.5 to 1.0), and Residual Adaptation (self-optimized skip connections with gamma from 0.5 to 0.72). It integrates DSPA with enhanced MBConv blocks, uses dual attention pathways with real-time weight adjustment, cascaded refinement layers to boost gradient flow, and a hardware-friendly design (0.28G FLOPs).

Result: Achieves CIFAR-10: 95.57% accuracy with 0.85M parameters and 93.80% with 0.37M parameters; CIFAR-100: 81.37% accuracy with 0.92M parameters and 74.85% with 0.44M parameters. Claims 2.1x parameter reduction vs MobileNetV3 and +3.2 percentage points on CIFAR-10, with cascaded refinements improving gradient flow by 12.7%. Overall 0.28G FLOPs.

Conclusion: The approach claims unprecedented parameter-accuracy trade-offs enabling edge deployment without accuracy degradation, via co-optimized dynamic attention and efficient convolution operations. It positions FastBoost as a new efficiency frontier on CIFAR benchmarks.

Abstract: We present FastBoost, a parameter-efficient neural architecture that achieves
state-of-the-art performance on CIFAR benchmarks through a novel Dynamically
Scaled Progressive Attention (DSPA) mechanism. Our design establishes new
efficiency frontiers with: CIFAR-10: 95.57% accuracy (0.85M parameters) and
93.80% (0.37M parameters) CIFAR-100: 81.37% accuracy (0.92M parameters) and
74.85% (0.44M parameters) The breakthrough stems from three fundamental
innovations in DSPA: (1) Adaptive Fusion: Learnt channel-spatial attention
blending with dynamic weights. (2) Phase Scaling: Training-stage-aware
intensity modulation (from 0.5 to 1.0). (3) Residual Adaptation: Self-optimized
skip connections (gamma from 0.5 to 0.72). By integrating DSPA with enhanced
MBConv blocks, FastBoost achieves a 2.1 times parameter reduction over
MobileNetV3 while improving accuracy by +3.2 percentage points on CIFAR-10. The
architecture features dual attention pathways with real-time weight adjustment,
cascaded refinement layers (increasing gradient flow by 12.7%), and a
hardware-friendly design (0.28G FLOPs). This co-optimization of dynamic
attention and efficient convolution operations demonstrates unprecedented
parameter-accuracy trade-offs, enabling deployment in resource-constrained edge
devices without accuracy degradation.

</details>


### [107] [T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression](https://arxiv.org/abs/2511.01079)
*Nikolay I. Kalmykov,Razan Dibo,Kaiyu Shen,Xu Zhonghan,Anh-Huy Phan,Yipeng Liu,Ivan Oseledets*

Main category: cs.CV

TL;DR: Introduces T-MLA, a targeted multiscale log-exponential attack in the wavelet domain for neural image compression (NIC). It degrades reconstruction quality while staying perceptually stealthy, exposing a security vulnerability in NIC pipelines.


<details>
  <summary>Details</summary>
Motivation: Security vulnerabilities in neural image compression are underexplored compared to classifiers. Existing NIC attacks are often naive pixel-space methods that ignore the wavelet-based compression pipeline, necessitating principled, offline attacks that exploit wavelet subbands.

Method: Develops a targeted multiscale log-exponential attack (T-MLA) that crafts adversarial perturbations directly in the wavelet domain. Perturbations are confined to selected wavelet subbands to maximize distortion of the attacked and reconstructed images while maintaining perceptual stealth, enabling offline, multi-scale optimization across layers of state-of-the-art NICs.

Result: Across multiple state-of-the-art NIC architectures and standard image compression benchmarks, T-MLA causes a large drop in reconstruction quality with perturbations that remain visually imperceptible.

Conclusion: Reveals a critical security flaw at the core of generative and content delivery pipelines and underscores the need for robust defenses and secure NIC designs.

Abstract: Neural image compression (NIC) has become the state-of-the-art for
rate-distortion performance, yet its security vulnerabilities remain
significantly less understood than those of classifiers. Existing adversarial
attacks on NICs are often naive adaptations of pixel-space methods, overlooking
the unique, structured nature of the compression pipeline. In this work, we
propose a more advanced class of vulnerabilities by introducing T-MLA, the
first targeted multiscale log--exponential attack framework. Our approach
crafts adversarial perturbations in the wavelet domain by directly targeting
the quality of the attacked and reconstructed images. This allows for a
principled, offline attack where perturbations are strategically confined to
specific wavelet subbands, maximizing distortion while ensuring perceptual
stealth. Extensive evaluation across multiple state-of-the-art NIC
architectures on standard image compression benchmarks reveals a large drop in
reconstruction quality while the perturbations remain visually imperceptible.
Our findings reveal a critical security flaw at the core of generative and
content delivery pipelines.

</details>


### [108] [GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction](https://arxiv.org/abs/2511.01082)
*Narges Ghasemi,Amir Ziashahabi,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: A hierarchical autoregressive geolocation model using S2 grid cells to predict location in coarse-to-fine tokens; uses top-down sampling with beam search and multi-sample inference; achieves state-of-the-art on Im2GPS3k and YFCC4k, with gains up to 13.9% without a Multimodal LLM and best results with MLLM; code released.


<details>
  <summary>Details</summary>
Motivation: Addressing visual similarity across locations and large search space by narrowing down locations through a coarse-to-fine representation and autoregressive token prediction; exploring inference-time strategies to handle uncertainty.

Method: Predict geographic tokens hierarchically via S2 cells conditioned on image and past predictions, without explicit semantic partitions; autoregressive sampling along a top-down hierarchy; integrate beam search and multi-sample inference; evaluate with and without MLLMs; compare against baselines.

Result: State-of-the-art on Im2GPS3k and YFCC4k; accuracy gains up to 13.9% in the MLLM-free setting; with MLLM, surpasses all baselines across all metrics.

Conclusion: The hierarchical autoregressive approach with explicit inference strategies is effective for image geolocalization and benefits from MLLMs; code released.

Abstract: Image geolocalization, the task of determining an image's geographic origin,
poses significant challenges, largely due to visual similarities across
disparate locations and the large search space. To address these issues, we
propose a hierarchical sequence prediction approach inspired by how humans
narrow down locations from broad regions to specific addresses. Analogously,
our model predicts geographic tokens hierarchically, first identifying a
general region and then sequentially refining predictions to increasingly
precise locations. Rather than relying on explicit semantic partitions, our
method uses S2 cells, a nested, multiresolution global grid, and sequentially
predicts finer-level cells conditioned on visual inputs and previous
predictions. This procedure mirrors autoregressive text generation in large
language models. Much like in language modeling, final performance depends not
only on training but also on inference-time strategy. We investigate multiple
top-down traversal methods for autoregressive sampling, incorporating
techniques from test-time compute scaling used in language models.
Specifically, we integrate beam search and multi-sample inference while
exploring various selection strategies to determine the final output. This
enables the model to manage uncertainty by exploring multiple plausible paths
through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k
datasets against two distinct sets of baselines: those that operate without a
Multimodal Large Language Model (MLLM) and those that leverage one. In the
MLLM-free setting, our model surpasses other comparable baselines on nearly all
metrics, achieving state-of-the-art performance with accuracy gains of up to
13.9%. When augmented with an MLLM, our model outperforms all baselines,
setting a new state-of-the-art across all metrics. The source code is available
at https://github.com/NNargesNN/GeoToken.

</details>


### [109] [SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices](https://arxiv.org/abs/2511.01087)
*Md. Abid Hasan Rafi,Mst. Fatematuj Johora,Pankaj Bhowmik*

Main category: cs.CV

TL;DR: A synthetic KPI-to-image dataset (SliceVision-F2I) with four encoding schemes to study feature visualization in network slicing, containing 30k samples per scheme and low-resolution RGB images, enabling visual ML tasks and benchmarking under realistic noise.


<details>
  <summary>Details</summary>
Motivation: Network slicing requires precise identification methods and robust datasets. Visual feature representations can facilitate interpretation and ML on multivariate KPI data, bridging time-series analysis with image-based learning.

Method: Four encoding pipelines transform multivariate KPI vectors into RGB images: physically inspired mappings, Perlin noise, neural wallpapering, and fractal branching. Each encoding yields 30,000 samples (raw KPI vector + low-res RGB image) under simulated noisy/network conditions to reflect practical uncertainties.

Result: A publicly available, sizeable synthetic dataset (120,000 samples total) enabling visual learning, state classification, anomaly detection, and benchmarking of image-based ML on network data; supports diverse research such as time-series-to-image transformations and synthetic data generation.

Conclusion: SliceVision-F2I provides a versatile resource for studying feature-to-image transformations in network slicing, supporting the development and benchmarking of image-based ML approaches for monitoring and managing next-generation networks.

Abstract: The emergence of 5G and 6G networks has established network slicing as a
significant part of future service-oriented architectures, demanding refined
identification methods supported by robust datasets. The article presents
SliceVision-F2I, a dataset of synthetic samples for studying feature
visualization in network slicing for next-generation networking systems. The
dataset transforms multivariate Key Performance Indicator (KPI) vectors into
visual representations through four distinct encoding methods: physically
inspired mappings, Perlin noise, neural wallpapering, and fractal branching.
For each encoding method, 30,000 samples are generated, each comprising a raw
KPI vector and a corresponding RGB image at low-resolution pixels. The dataset
simulates realistic and noisy network conditions to reflect operational
uncertainties and measurement imperfections. SliceVision-F2I is suitable for
tasks involving visual learning, network state classification, anomaly
detection, and benchmarking of image-based machine learning techniques applied
to network data. The dataset is publicly available and can be reused in various
research contexts, including multivariate time series analysis, synthetic data
generation, and feature-to-image transformations.

</details>


### [110] [Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images](https://arxiv.org/abs/2511.01098)
*Veronica Marsico,Antonio Quintero-Rincon,Hadj Batatia*

Main category: cs.CV

TL;DR: EKDE-based feature extraction combined with a bimodal logistic regression classifier for chest X-ray diagnosis yields moderate performance on a large COVID-19 radiography dataset.


<details>
  <summary>Details</summary>
Motivation: To model flexible feature distributions in medical images without assuming specific shapes, enabling robust feature extraction from radiographs and improving diagnostic reliability in respiratory disease detection.

Method: Apply Epanechnikov non-parametric kernel density estimation (EKDE) to extract features from chest X-ray images and feed them into a bimodal logistic regression classifier within a statistical-model-based learning framework; evaluated on 13,808 chest X-rays from the COVID-19 Radiography Dataset.

Result: Accuracy: 70.14%; Sensitivity: 59.26%; Specificity: 74.18%.

Conclusion: EKDE-based approaches show potential to enhance diagnostic accuracy in medical imaging, but sensitivity remains an area for improvement; clinical expertise remains essential for refinement and deployment.

Abstract: This study presents a novel method for diagnosing respiratory diseases using
image data. It combines Epanechnikov's non-parametric kernel density estimation
(EKDE) with a bimodal logistic regression classifier in a
statistical-model-based learning scheme. EKDE's flexibility in modeling data
distributions without assuming specific shapes and its adaptability to pixel
intensity variations make it valuable for extracting key features from medical
images. The method was tested on 13808 randomly selected chest X-rays from the
COVID-19 Radiography Dataset, achieved an accuracy of 70.14%, a sensitivity of
59.26%, and a specificity of 74.18%, demonstrating moderate performance in
detecting respiratory disease while showing room for improvement in
sensitivity. While clinical expertise remains essential for further refining
the model, this study highlights the potential of EKDE-based approaches to
enhance diagnostic accuracy and reliability in medical imaging.

</details>


### [111] [Anatomically Constrained Transformers for Echocardiogram Analysis](https://arxiv.org/abs/2511.01109)
*Alexander Thorley,Agis Chartsias,Jordan Strom,Jeremy Slivnick,Dipak Kotecha,Alberto Gomez,Jinming Duan*

Main category: cs.CV

TL;DR: A video transformer for echocardiography with anatomical priors improves region-focused analysis and tracking by constraining learning to myocardium patches and enabling interpretable attention.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on non-diagnostic background cues in video models for echocardiography, improving interpretability and diagnostic performance by integrating anatomical priors into the transformer.

Method: Represent deformable anatomy as a point set, encode its geometry and corresponding image patches as transformer tokens; pre-train with masked autoencoding on anatomical patches to focus learning on the myocardium; fine-tune for region-specific tasks (e.g., EF regression, CA detection); attention is constrained to the myocardial region and attains interpretable maps; can also perform myocardium tracking without task-specific components like correlation volumes.

Result: Attention maps align with regions showing CA pathology; improved myocardium-focused representations for EF and CA tasks; generalizes to myocardium tracking without specialized tracking modules.

Conclusion: ViACT demonstrates that injecting anatomical priors into video transformers yields region-focused, interpretable representations for echocardiography, improving performance on targeted tasks and enabling robust myocardium tracking without extra components.

Abstract: Video transformers have recently demonstrated strong potential for
echocardiogram (echo) analysis, leveraging self-supervised pre-training and
flexible adaptation across diverse tasks. However, like other models operating
on videos, they are prone to learning spurious correlations from non-diagnostic
regions such as image backgrounds. To overcome this limitation, we propose the
Video Anatomically Constrained Transformer (ViACT), a novel framework that
integrates anatomical priors directly into the transformer architecture. ViACT
represents a deforming anatomical structure as a point set and encodes both its
spatial geometry and corresponding image patches into transformer tokens.
During pre-training, ViACT follows a masked autoencoding strategy that masks
and reconstructs only anatomical patches, enforcing that representation
learning is focused on the anatomical region. The pre-trained model can then be
fine-tuned for tasks localized to this region. In this work we focus on the
myocardium, demonstrating the framework on echo analysis tasks such as left
ventricular ejection fraction (EF) regression and cardiac amyloidosis (CA)
detection. The anatomical constraint focuses transformer attention within the
myocardium, yielding interpretable attention maps aligned with regions of known
CA pathology. Moreover, ViACT generalizes to myocardium point tracking without
requiring task-specific components such as correlation volumes used in
specialized tracking networks.

</details>


### [112] [Boosting performance of computer vision applications through embedded GPUs on the edge](https://arxiv.org/abs/2511.01129)
*Fabio Diniz Rossi*

Main category: cs.CV

TL;DR: Embedded GPUs on edge devices improve computer vision/AR performance compared to CPU-only, enabling better QoE on resource-constrained mobile/edge setups.


<details>
  <summary>Details</summary>
Motivation: AR and mobile computer vision are resource-intensive; edge computing can offload workload but edge devices themselves have limited capacity, risking poor user experience; need a solution to boost performance without excessive resources.

Method: Propose and evaluate embedding GPUs in edge devices to accelerate CV/AR tasks; conduct experiments comparing GPU-enabled devices against CPU-only baselines to assess performance and QoE.

Result: Experiments show GPU-enabled edge devices achieve performance gains over CPU-only configurations, indicating improved user experience for AR/CV applications.

Conclusion: Using embedded GPUs on edge devices is an effective approach to mitigate resource constraints in mobile AR/CV tasks, improving QoE; warrants further exploration of energy/thermal implications and broader hardware comparisons.

Abstract: Computer vision applications, especially those using augmented reality
technology, are becoming quite popular in mobile devices. However, this type of
application is known as presenting significant demands regarding resources. In
order to enable its utilization in devices with more modest resources, edge
computing can be used to offload certain high intensive tasks. Still, edge
computing is usually composed of devices with limited capacity, which may
impact in users quality of experience when using computer vision applications.
This work proposes the use of embedded devices with graphics processing units
(GPUs) to overcome such limitation. Experiments performed shown that GPUs can
attain a performance gain when compared to using only CPUs, which guarantee a
better experience to users using such kind of application.

</details>


### [113] [Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis](https://arxiv.org/abs/2511.01131)
*Md Nahiduzzaman,Steven Korevaar,Alireza Bab-Hadiashar,Ruwan Tennakoon*

Main category: cs.CV

TL;DR: Prior-guided Concept Predictor (PCP) is a weakly supervised framework for interpretable medical-imaging predictions that uses class-level concept priors and a KL-divergence/entropy-based refinement to align concept-level predictions with clinical reasoning, without explicit concept annotations or language-model reliance. It achieves notable gains in concept-level F1 over zero-shot baselines and competitive performance with fully supervised approaches across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Interpretable-by-design models typically require costly concept annotations; zero-shot and concept-generation approaches struggle with domain-specific medical features and reliability. There is a need for an interpretable method that does not rely on explicit supervision or language models.

Method: PCP leverages class-level concept priors as weak supervision and introduces a refinement mechanism using KL divergence and entropy regularization to align predictions with clinical reasoning. It operates without explicit concept labels or language-model reliance, enabling concept answer prediction in a weakly supervised setting.

Result: On PH2 (dermoscopy) and WBCatt (hematology), PCP improves concept-level F1-score by over 33% compared to zero-shot baselines, while delivering competitive classification performance on four medical datasets (PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept bottleneck models (CBMs) and V-IP.

Conclusion: PCP offers a practical, weakly supervised path to interpretable medical-imaging predictions by combining concept priors with a principled refinement strategy, yielding reliable concept-level interpretations without requiring explicit annotations or language-model-based supervision while maintaining competitive accuracy.

Abstract: Human-interpretable predictions are essential for deploying AI in medical
imaging, yet most interpretable-by-design (IBD) frameworks require concept
annotations for training data, which are costly and impractical to obtain in
clinical contexts. Recent attempts to bypass annotation, such as zero-shot
vision-language models or concept-generation frameworks, struggle to capture
domain-specific medical features, leading to poor reliability. In this paper,
we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised
framework that enables concept answer prediction without explicit supervision
or reliance on language models. PCP leverages class-level concept priors as
weak supervision and incorporates a refinement mechanism with KL divergence and
entropy regularization to align predictions with clinical reasoning.
Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves
concept-level F1-score by over 33% compared to zero-shot baselines, while
delivering competitive classification performance on four medical datasets
(PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept
bottleneck models (CBMs) and V-IP.

</details>


### [114] [Learning with Category-Equivariant Architectures for Human Activity Recognition](https://arxiv.org/abs/2511.01139)
*Yoshihiro Maruyama*

Main category: cs.CV

TL;DR: CatEquiv is a category-equivariant neural network for HAR that encodes temporal, amplitude, and sensor-structure symmetries via a categorical symmetry product, achieving robustness to OOD perturbations with no extra capacity.


<details>
  <summary>Details</summary>
Motivation: Address robustness and generalization in HAR by embedding symmetry-aware inductive biases derived from category theory to handle temporal shifts, gain variations, and sensor hierarchy.

Method: Introduce categorical symmetry product combining cyclic time shifts, positive gains, and sensor-hierarchy poset; enforce equivariance of CatEquiv to this product.

Result: On UCI-HAR with out-of-distribution perturbations, CatEquiv shows markedly higher robustness compared with circularly padded CNNs and plain CNNs.

Conclusion: Imposing categorical symmetries yields strong invariance and generalization without increasing model capacity.

Abstract: We propose CatEquiv, a category-equivariant neural network for Human Activity
Recognition (HAR) from inertial sensors that systematically encodes temporal,
amplitude, and structural symmetries. In particular, we introduce the
categorical symmetry product where cyclic time shifts, positive gains and the
sensor-hierarchy poset together capture the categorical symmetry structure of
the data. CatEquiv achieves equivariance with respect to the categorical
symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv
attains markedly higher robustness compared with circularly padded CNNs and
plain CNNs. These results demonstrate that enforcing categorical symmetries
yields strong invariance and generalization without additional model capacity.

</details>


### [115] [MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation](https://arxiv.org/abs/2511.01143)
*Ziyi Wang,Yuanmei Zhang,Dorna Esrafilzadeh,Ali R. Jalili,Suncheng Xiang*

Main category: cs.CV

TL;DR: A lightweight, attention-based polyp segmentation network (MicroAUNet) achieves real-time performance with depthwise-separable dilated convolutions and a shared channel-spatial attention block, aided by a two-stage knowledge distillation from a high-capacity teacher; code released.


<details>
  <summary>Details</summary>
Motivation: There is a need for accurate polyp boundary segmentation in real-time clinical endoscopy without sacrificing speed or decision quality; current deep learning models trade off accuracy and inference speed.

Method: MicroAUNet employs depthwise-separable dilated convolutions and a single-path, parameter-shared channel-spatial attention block to enhance multi-scale boundary features, followed by a progressive two-stage knowledge-distillation scheme from a high-capacity teacher to transfer semantic and boundary cues.

Result: Extensive experiments on benchmarks show state-of-the-art accuracy under extremely low model complexity, indicating suitability for real-time clinical polyp segmentation.

Conclusion: MicroAUNet is a viable solution for real-time clinical polyp segmentation, with code publicly available at the provided repository.

Abstract: Early and accurate segmentation of colorectal polyps is critical for reducing
colorectal cancer mortality, which has been extensively explored by academia
and industry. However, current deep learning-based polyp segmentation models
either compromise clinical decision-making by providing ambiguous polyp margins
in segmentation outputs or rely on heavy architectures with high computational
complexity, resulting in insufficient inference speeds for real-time colorectal
endoscopic applications. To address this problem, we propose MicroAUNet, a
light-weighted attention-based segmentation network that combines
depthwise-separable dilated convolutions with a single-path, parameter-shared
channel-spatial attention block to strengthen multi-scale boundary features. On
the basis of it, a progressive two-stage knowledge-distillation scheme is
introduced to transfer semantic and boundary cues from a high-capacity teacher.
Extensive experiments on benchmarks also demonstrate the state-of-the-art
accuracy under extremely low model complexity, indicating that MicroAUNet is
suitable for real-time clinical polyp segmentation. The code is publicly
available at https://github.com/JeremyXSC/MicroAUNet.

</details>


### [116] [ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation](https://arxiv.org/abs/2511.01163)
*Yongyuan Liang,Wei Chow,Feng Li,Ziqiao Ma,Xiyao Wang,Jiageng Mao,Jiuhai Chen,Jiatao Gu,Yue Wang,Furong Huang*

Main category: cs.CV

TL;DR: ROVER is a new human-annotated benchmark to test reciprocal cross-modal reasoning in unified multimodal models, showing interleaved cross-modal reasoning boosts visual generation and revealing gaps in symbolic reasoning.


<details>
  <summary>Details</summary>
Motivation: Current evaluations treat multimodal tasks in isolation, emphasizing unimodal reasoning. There is a need to test how one modality can guide, verify, or refine outputs in the other to achieve true omnimodal generation.

Method: ROVER comprises 1,312 tasks grounded in 1,876 images across two settings: verbally-augmented reasoning for visual generation and visually-augmented reasoning for verbal generation. It is evaluated on 17 unified multimodal models.

Result: Cross-modal reasoning strongly influences visual generation quality; interleaved models outperform non-interleaved ones, and mere combination of strong unimodal models is not enough. There is a dissociation between physical and symbolic reasoning: models handle perceptual concepts but struggle to construct visual abstractions for symbolic tasks, with faulty reasoning harming performance.

Conclusion: Reciprocal cross-modal reasoning is a critical frontier for enabling true omnimodal generation.

Abstract: Unified multimodal models (UMMs) have emerged as a powerful paradigm for
seamlessly unifying text and image understanding and generation. However,
prevailing evaluations treat these abilities in isolation, such that tasks with
multimodal inputs and outputs are scored primarily through unimodal reasoning,
i.e., textual benchmarks emphasize language-based reasoning, while visual
benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce
ROVER to address this pressing need to test reciprocal cross-modal reasoning,
the use of one modality to guide, verify, or refine outputs in the other, an
ability central to the vision of unified multimodal intelligence. ROVER is a
human-annotated benchmark that explicitly targets reciprocal cross-modal
reasoning, which contains 1312 tasks grounded in 1876 images, spanning two
complementary settings. Verbally-augmented reasoning for visual generation
evaluates whether models can use verbal prompts and reasoning chains to guide
faithful image synthesis. Visually-augmented reasoning for verbal generation
evaluates whether models can generate intermediate visualizations that
strengthen their own reasoning processes for question answering. Experiments on
17 unified models reveal two key findings: (i) Cross-modal reasoning determines
visual generation quality, with interleaved models significantly outperforming
non-interleaved ones; notably, combining strong unimodal models fails to
achieve comparable reasoning. (ii) Models show dissociation between physical
and symbolic reasoning: they succeed at interpreting perceptual concepts
literally but fail to construct visual abstractions for symbolic tasks, where
faulty reasoning harms performance. These results highlight reciprocal
cross-modal reasoning as a critical frontier for enabling true omnimodal
generation.

</details>


### [117] [Web-Scale Collection of Video Data for 4D Animal Reconstruction](https://arxiv.org/abs/2511.01169)
*Brian Nlong Zhao,Jiajun Wu,Shangzhe Wu*

Main category: cs.CV

TL;DR: Automates mining of YouTube videos to create a large, object-centric, 4D-animal reconstruction pipeline and a new AiM benchmark; reveals a gap between 2D-metric favoring model-based methods and 3D realism, and introduces a sequence-optimized model-free baseline.


<details>
  <summary>Details</summary>
Motivation: Enable large-scale, markerless 4D animal reconstruction from in-the-wild videos, addressing data scarcity and evaluation gaps that hinder non-invasive wildlife research.

Method: An automated pipeline extracts 30K YouTube videos (â2M frames) into object-centric clips with auxiliary annotations for pose estimation, tracking, and 3D/4D tasks; introduction of AiM benchmark with 230 sequences (~11K frames) for 4D quadruped reconstruction; evaluation of state-of-the-art model-based and model-free methods; enhancement of a model-free approach with sequence-level optimization to establish a first 4D baseline.

Result: Substantial data resource (30K videos, 2M frames) and a curated 4D quadruped benchmark (AiM) enabling systematic evaluation; 2D metrics prefer model-based methods, but they produce unrealistic 3D shapes; model-free methods yield more natural reconstructions but score lower; sequence-level optimization improves model-free performance, establishing a first 4D baseline; code and datasets released.

Conclusion: The pipeline, AiM benchmark, and baseline address key gaps in data, evaluation, and baselines for large-scale markerless 4D animal reconstruction, accelerating progress in downstream tasks and related wildlife video analytics.

Abstract: Computer vision for animals holds great promise for wildlife research but
often depends on large-scale data, while existing collection methods rely on
controlled capture setups. Recent data-driven approaches show the potential of
single-view, non-invasive analysis, yet current animal video datasets are
limited--offering as few as 2.4K 15-frame clips and lacking key processing for
animal-centric 3D/4D tasks. We introduce an automated pipeline that mines
YouTube videos and processes them into object-centric clips, along with
auxiliary annotations valuable for downstream tasks like pose estimation,
tracking, and 3D/4D reconstruction. Using this pipeline, we amass 30K videos
(2M frames)--an order of magnitude more than prior works. To demonstrate its
utility, we focus on the 4D quadruped animal reconstruction task. To support
this task, we present Animal-in-Motion (AiM), a benchmark of 230 manually
filtered sequences with 11K frames showcasing clean, diverse animal motions. We
evaluate state-of-the-art model-based and model-free methods on
Animal-in-Motion, finding that 2D metrics favor the former despite unrealistic
3D shapes, while the latter yields more natural reconstructions but scores
lower--revealing a gap in current evaluation. To address this, we enhance a
recent model-free approach with sequence-level optimization, establishing the
first 4D animal reconstruction baseline. Together, our pipeline, benchmark, and
baseline aim to advance large-scale, markerless 4D animal reconstruction and
related tasks from in-the-wild videos. Code and datasets are available at
https://github.com/briannlongzhao/Animal-in-Motion.

</details>


### [118] [Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution](https://arxiv.org/abs/2511.01175)
*Peng Du,Hui Li,Han Xu,Paul Barom Jeon,Dongwook Lee,Daehyun Ji,Ran Yang,Feng Zhu*

Main category: cs.CV

TL;DR: DTWSR introduces a diffusion-transformer SR framework that leverages multi-level DWT wavelet spectra, pyramid tokenization, and a dual-decoder to model interrelations across multiscale frequency sub-bands, yielding more consistent and realistic super-resolved images with strong perceptual quality and fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing DWT-based SR methods often ignore the cross-scale relationships among wavelet sub-bands, leading to inconsistencies and artifacts in the reconstructed images. There is a need to explicitly model interscale dependencies and preserve both high- and low-frequency content for realistic SR.

Method: 1) Decompose images with Multi-level Discrete Wavelet Transform (MDWT) to obtain wavelet spectra across scales. 2) Apply pyramid tokenization to embed spectra into a sequence of tokens suitable for a transformer, capturing spatial and frequency-domain information. 3) Employ a diffusion-transformer framework to model the distribution of clean HR images conditioned on LR input, with a dual-decoder architecture designed to handle low-frequency (LF) and high-frequency (HF) sub-bands separately while maintaining their alignment. 4) Train and evaluate on standard SR benchmarks to assess perceptual quality and fidelity.

Result: Experiments on multiple benchmark datasets show that DTWSR achieves strong performance in both perceptual quality and fidelity, indicating improved consistency and realism in SR images due to explicit multiscale spectral modeling and effective cross-band alignment.

Conclusion: DTWSR demonstrates that integrating diffusion models, transformers, and wavelet-based multiscale spectra can effectively model interrelations across sub-bands to produce more coherent and realistic SR results, suggesting a promising direction for wavelet-informed diffusion architectures in image restoration.

Abstract: Discrete Wavelet Transform (DWT) has been widely explored to enhance the
performance of image superresolution (SR). Despite some DWT-based methods
improving SR by capturing fine-grained frequency signals, most existing
approaches neglect the interrelations among multiscale frequency sub-bands,
resulting in inconsistencies and unnatural artifacts in the reconstructed
images. To address this challenge, we propose a Diffusion Transformer model
based on image Wavelet spectra for SR (DTWSR).DTWSR incorporates the
superiority of diffusion models and transformers to capture the interrelations
among multiscale frequency sub-bands, leading to a more consistence and
realistic SR image. Specifically, we use a Multi-level Discrete Wavelet
Transform (MDWT) to decompose images into wavelet spectra. A pyramid
tokenization method is proposed which embeds the spectra into a sequence of
tokens for transformer model, facilitating to capture features from both
spatial and frequency domain. A dual-decoder is designed elaborately to handle
the distinct variances in lowfrequency (LF) and high-frequency (HF) sub-bands,
without omitting their alignment in image generation. Extensive experiments on
multiple benchmark datasets demonstrate the effectiveness of our method, with
high performance on both perception quality and fidelity.

</details>


### [119] [A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment](https://arxiv.org/abs/2511.01194)
*Minmin Zeng*

Main category: cs.CV

TL;DR: GCN-PSN uses a topology-aware GCN to model the skeleton as a graph for action quality assessment, employing a Siamese network with a contrastive regression objective to learn discriminative pose embeddings; achieves competitive results on AQA-7 and FineDiving.


<details>
  <summary>Details</summary>
Motivation: Action Quality Assessment (AQA) requires fine-grained understanding of human motion and precise pose similarity; skeletal topology provides relational structure that coordinate-based methods may miss, enabling more discriminative embeddings.

Method: Model the human skeleton as a graph and apply a topology-aware Graph Convolutional Network (GCN-PSN) to learn pose embeddings. Use a Siamese architecture trained with a contrastive regression objective to compare action qualities and assess similarity.

Result: The method outperforms coordinate-based baselines and achieves competitive performance on AQA-7 and FineDiving. Ablation studies validate the benefit of leveraging skeletal topology for pose similarity and action quality assessment.

Conclusion: Incorporating skeletal topology via a topology-aware GCN improves action quality assessment and pose similarity, highlighting the importance of graph-structured representations for fine-grained motion analysis.

Abstract: Action Quality Assessment (AQA) requires fine-grained understanding of human
motion and precise evaluation of pose similarity. This paper proposes a
topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN,
which models the human skeleton as a graph to learn discriminative,
topology-sensitive pose embeddings. Using a Siamese architecture trained with a
contrastive regression objective, our method outperforms coordinate-based
baselines and achieves competitive performance on AQA-7 and FineDiving
benchmarks. Experimental results and ablation studies validate the
effectiveness of leveraging skeletal topology for pose similarity and action
quality assessment.

</details>


### [120] [MoSa: Motion Generation with Scalable Autoregressive Modeling](https://arxiv.org/abs/2511.01200)
*Mengyuan Liu,Sheng Yan,Yong Wang,Yingjie Li,Gui-Bin Bian,Hong Liu*

Main category: cs.CV

TL;DR: MoSa introduces a hierarchical, multi-scale motion generation framework that uses a multi-scale token preservation strategy (MTPS) in a hierarchical residual VQ-VAE (RQ-VAE) and scalable autoregressive (SAR) modeling to achieve high fidelity and efficiency in text-driven 3D human motion generation, with only 10 inference steps and strong results on Motion-X.


<details>
  <summary>Details</summary>
Motivation: Overcome inefficiencies and reconstruction artifacts in existing VQ-GT approaches by enabling multi-scale, scalable generation that preserves coarse-to-fine structure and improves both fidelity and speed, while supporting downstream tasks like motion editing without fine-tuning.

Method: MoSa integrates MTPS into a hierarchical RQ-VAE, using interpolation to retain coarse-to-fine tokens at each quantization level. It enables scalable autoregressive modeling that predicts scale tokens across 10 steps, matching the number of quantization layers. To mitigate interpolation-induced degradation, it introduces CAQ-VAE, a convolution-attention hybrid VQ-VAE with improved residual blocks and global attention.

Result: Empirical results show state-of-the-art generation quality and efficiency: on the Motion-X dataset, MoSa achieves FID 0.06 (better than MoMask's 0.20) and reduces inference time by 27%. It generalizes to downstream tasks such as motion editing without additional fine-tuning; code is available at the provided URL.

Conclusion: MoSa demonstrates that hierarchical, multi-scale token preservation coupled with scalable autoregressive generation and a convolution-attention VQ-VAE can substantially improve both the quality and speed of text-driven 3D motion generation, enabling efficient long-horizon generation and easy downstream adaptation.

Abstract: We introduce MoSa, a novel hierarchical motion generation framework for
text-driven 3D human motion generation that enhances the Vector
Quantization-guided Generative Transformers (VQ-GT) paradigm through a
coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale
Token Preservation Strategy (MTPS) integrated into a hierarchical residual
vector quantization variational autoencoder (RQ-VAE). MTPS employs
interpolation at each hierarchical quantization to effectively retain
coarse-to-fine multi-scale tokens. With this, the generative transformer
supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens,
unlike traditional methods that predict only one token at each step.
Consequently, MoSa requires only 10 inference steps, matching the number of
RQ-VAE quantization layers. To address potential reconstruction degradation
from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive
convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and
incorporates attention mechanisms to better capture global dependencies.
Extensive experiments show that MoSa achieves state-of-the-art generation
quality and efficiency, outperforming prior methods in both fidelity and speed.
On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20)
while reducing inference time by 27 percent. Moreover, MoSa generalizes well to
downstream tasks such as motion editing, requiring no additional fine-tuning.
The code is available at https://mosa-web.github.io/MoSa-web

</details>


### [121] [OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA](https://arxiv.org/abs/2511.01210)
*Heyu Guo,Shanmu Wang,Ruichun Ma,Shiqi Jiang,Yasaman Ghasempour,Omid Abari,Baining Guo,Lili Qi*

Main category: cs.CV

TL;DR: OmniVLA proposes an omni-modality VLA model using sensor-masked images from IR, mmWave radar, and microphone array to enable physically-grounded spatial understanding for action prediction, achieving 84% average task success and outperforming RGB-only and raw-sensor baselines, with data-efficient learning.


<details>
  <summary>Details</summary>
Motivation: RGB-only VLA models limit perception and manipulation; incorporating diverse sensing modalities provides physically meaningful cues for spatial reasoning and interaction in real-world tasks.

Method: Introduce sensor-masked image: overlay masks derived from infrared, mmWave radar, and microphone sensors onto RGB images to create a unified, image-native representation. Use lightweight per-sensor projectors, align to an RGB-pretrained VLA backbone, and build a multisensory VLA architecture to fuse modalities.

Result: On real-world tasks requiring sensor-based perception, OmniVLA achieves 84% average task success, outperforming RGB-only baselines by 59% and raw-sensor baselines by 28%, with improved learning efficiency and generalization.

Conclusion: Demonstrates the viability of omni-modality fusion for vision-language-action tasks; sensor-masked images enable effective cross-modal fusion with minimal per-sensor overhead and enable data-efficient training; suggests future work on expanding modalities and robustness.

Abstract: Vision-language-action (VLA) models have shown strong generalization for
action prediction through large-scale vision-language pretraining. However,
most existing models rely solely on RGB cameras, limiting their perception and,
consequently, manipulation capabilities. We present OmniVLA, an omni-modality
VLA model that integrates novel sensing modalities for physically-grounded
spatial intelligence beyond RGB perception. The core of our approach is the
sensor-masked image, a unified representation that overlays spatially grounded
and physically meaningful masks onto the RGB images, derived from sensors
including an infrared camera, a mmWave radar, and a microphone array. This
image-native unification keeps sensor input close to RGB statistics to
facilitate training, provides a uniform interface across sensor hardware, and
enables data-efficient learning with lightweight per-sensor projectors. Built
on this, we present a multisensory vision-language-action model architecture
and train the model based on an RGB-pretrained VLA backbone. We evaluate
OmniVLA on challenging real-world tasks where sensor-modality perception is
needed to guide the manipulation. OmniVLA achieves an average task success rate
of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline
models by 59% and 28% respectively, meanwhile showing higher learning
efficiency and stronger generalization capability.

</details>


### [122] [Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering](https://arxiv.org/abs/2511.01213)
*Riddhi Jain,Manasi Patwardhan,Parijat Deshpande,Venkataramana Runkana*

Main category: cs.CV

TL;DR: Auto-generated reasoning chains for Indian Food VQA; small models fine-tuned on these chains, trained with RL on larger data; yields ~10pp accuracy gain over baseline.


<details>
  <summary>Details</summary>
Motivation: Indian cuisine is diverse and context-rich; current VQA methods are Western-biased and often rely on a simple two-step QA-explanation pipeline; multi-step reasoning is needed to capture relationships among ingredients, dishes, and culinary context.

Method: Generate reasoning chains during QA with minimal human intervention; fine-tune smaller LLMs and VLMs using auto-validated reasoning chains; augment training with reinforcement learning on larger datasets; incorporate reasoning chains and potentially knowledge graphs.

Result: Average accuracy improvement of about 10 percentage points over the baseline; detailed analysis on how adding reasoning chains affects Indian Food VQA performance.

Conclusion: Reasoning-chain augmentation combined with reinforcement learning improves VQA for Indian foods; approach reduces reliance on very large models and supports richer reasoning; future work includes refining chain quality and exploring knowledge-graph integration.

Abstract: The immense diversity in the culture and culinary of Indian cuisines calls
attention to the major shortcoming of the existing Visual Question
Answering(VQA) systems which are inclined towards the foods from Western
region. Recent attempt towards building a VQA dataset for Indian food is a step
towards addressing this challenge. However, their approach towards VQA follows
a two-step process in which the answer is generated first, followed by the
explanation of the expected answer. In this work, we claim that food VQA
requires to follow a multi-step reasoning process to arrive at an accurate
answer, especially in the context of India food, which involves understanding
complex culinary context and identifying relationships between various food
items. With this hypothesis we create reasoning chains upon the QA with minimal
human intervention. We fine-tune smaller LLMs and VLMs with auto-validated
reasoning chains and further train them using reinforcement learning with
larger data. With augmentation of reasoning chains, we observed accuracy
improvement of an average 10 percentage points on the baseline. We provide
detailed analysis in terms the effect of addition of reasoning chains for the
Indian Food VQA task.
  Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge
Graph.

</details>


### [123] [Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark](https://arxiv.org/abs/2511.01233)
*Rajmund Nagy,Hendric Voss,Thanh Hoang-Minh,Mihail Tsakov,Teodor Nikolov,Zeyi Zhang,Tenglong Ao,Sicheng Yang,Shaoli Huang,Yongkang Cheng,M. Hamza Mughal,Rishabh Dabral,Kiran Chhatre,Christian Theobalt,Libin Liu,Stefan Kopp,Rachel McDonnell,Michael Neff,Taras Kucherenko,Youngwoo Yoon,Gustav Eje Henter*

Main category: cs.CV

TL;DR: Standardized human evaluation protocol for BEAT2-based gesture generation reveals inconsistencies in current evaluation practices; newer models do not consistently outperform older ones; emphasizes disentangled assessment of motion realism and speech-gesture alignment and provides benchmarking resources.


<details>
  <summary>Details</summary>
Motivation: Address the lack of standardization and flawed experimental setups in evaluating automated, speech-driven 3D gesture generation, which hinders meaningful comparisons and progress.

Method: Develop and apply a detailed human evaluation protocol for the BEAT2 motion-capture dataset; perform large-scale crowdsourced evaluation comparing six models on motion realism and speech-gesture alignment; release evaluation resources.

Result: Evidence that newer models do not consistently outperform older ones; published claims of high realism or alignment may not hold under rigorous evaluation; disentangled evaluation of motion quality and multimodal alignment is necessary; provides benchmarking resources (synthetic motion, stimuli, rendering script, and 16,000 votes).

Conclusion: Standardized, disentangled evaluation is essential for meaningful progress in gesture-generation research; the authors provide data and tools to enable ongoing benchmarking without reimplementation.

Abstract: We review human evaluation practices in automated, speech-driven 3D gesture
generation and find a lack of standardisation and frequent use of flawed
experimental setups. This leads to a situation where it is impossible to know
how different methods compare, or what the state of the art is. In order to
address common shortcomings of evaluation design, and to standardise future
user studies in gesture-generation works, we introduce a detailed human
evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using
this protocol, we conduct large-scale crowdsourced evaluation to rank six
recent gesture-generation models -- each trained by its original authors --
across two key evaluation dimensions: motion realism and speech-gesture
alignment. Our results provide strong evidence that 1) newer models do not
consistently outperform earlier approaches; 2) published claims of high motion
realism or speech-gesture alignment may not hold up under rigorous evaluation;
and 3) the field must adopt disentangled assessments of motion quality and
multimodal alignment for accurate benchmarking in order to make progress.
Finally, in order to drive standardisation and enable new evaluation research,
we will release five hours of synthetic motion from the benchmarked models;
over 750 rendered video stimuli from the user studies -- enabling new
evaluations without model reimplementation required -- alongside our
open-source rendering script, and the 16,000 pairwise human preference votes
collected for our benchmark.

</details>


### [124] [Eyes on Target: Gaze-Aware Object Detection in Egocentric Video](https://arxiv.org/abs/2511.01237)
*Vishakha Lall,Yisi Liu*

Main category: cs.CV

TL;DR: A gaze-guided, depth-aware ViT detector for egocentric video that injects gaze-derived features into attention, yielding improved detection accuracy and interpretability across Ego4D and simulator datasets.


<details>
  <summary>Details</summary>
Motivation: Leverage human gaze as supervisory signal to bias attention toward regions of interest in complex egocentric scenes, addressing the limitation of gaze-agnostic detectors that treat all regions equally.

Method: A depth-aware, gaze-guided object detection framework (Eyes on Target) that injects gaze-derived features into the attention mechanism of a Vision Transformer (ViT), biasing spatial feature selection toward human-attended regions; includes a gaze-aware attention head importance metric and extensive ablations.

Result: Demonstrates consistent gains in detection accuracy over gaze-agnostic baselines on a custom egocentric simulator dataset and public benchmarks (Ego4D Ego-Motion and Ego-CH-Gaze); ablations quantify the contribution of gaze cues and depth; introduces an interpretable metric to relate gaze cues to transformer attention.

Conclusion: Gaze cues can effectively bias transformer attention to improve detection performance in egocentric videos and provide interpretable insights into attention dynamics; this approach also supports evaluating human performance in simulation scenarios.

Abstract: Human gaze offers rich supervisory signals for understanding visual attention
in complex visual environments. In this paper, we propose Eyes on Target, a
novel depth-aware and gaze-guided object detection framework designed for
egocentric videos. Our approach injects gaze-derived features into the
attention mechanism of a Vision Transformer (ViT), effectively biasing spatial
feature selection toward human-attended regions. Unlike traditional object
detectors that treat all regions equally, our method emphasises
viewer-prioritised areas to enhance object detection. We validate our method on
an egocentric simulator dataset where human visual attention is critical for
task assessment, illustrating its potential in evaluating human performance in
simulation scenarios. We evaluate the effectiveness of our gaze-integrated
model through extensive experiments and ablation studies, demonstrating
consistent gains in detection accuracy over gaze-agnostic baselines on both the
custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and
Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a
gaze-aware attention head importance metric, revealing how gaze cues modulate
transformer attention dynamics.

</details>


### [125] [Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability](https://arxiv.org/abs/2511.01240)
*Zhixuan Zhang,Pingyu Wang,Xingjian Zheng,Linbo Qing,Qi Liu*

Main category: cs.CV

TL;DR: Proposes Adversarial Flatness Attack (AFA) to address deceptive flatness in transferable adversarial attacks, introducing Adversarial Flatness (AF) and MonteCarlo Adversarial Sampling (MCAS) to improve black-box transferability; provides theoretical guarantees and shows empirical gains on ImageNet-compatible data against multiple baselines and defenses.


<details>
  <summary>Details</summary>
Motivation: Transferable adversarial examples are effective but often get stuck in flat yet non-transferable regions (deceptive flatness). There is a need for methods that identify and exploit flatter regions under black-box conditions to improve cross-model transferability.

Method: Introduce Adversarial Flatness (AF) to tackle deceptive flatness; instantiate as Adversarial Flatness Attack (AFA) with a practical objective approximation and corrected gradient-sign behavior; develop MonteCarlo Adversarial Sampling (MCAS) to enhance inner-loop sampling efficiency.

Result: Empirical results on ImageNet-compatible datasets show AFA/MCAS outperform six baselines, producing adversarial examples in flatter regions and improving transferability across model architectures; also effective against input transformation attacks and Baidu Cloud API.

Conclusion: AF provides a theoretical and practical framework for improving black-box transferability by targeting flatness properties; the combined AFA and MCAS yield stronger transferable attacks across diverse settings, suggesting broader applicability and potential future extensions.

Abstract: Transferable attacks generate adversarial examples on surrogate models to
fool unknown victim models, posing real-world threats and growing research
interest. Despite focusing on flat losses for transferable adversarial
examples, recent studies still fall into suboptimal regions, especially the
flat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce
a novel black-box gradient-based transferable attack from a perspective of
dual-order information. Specifically, we feasibly propose Adversarial Flatness
(AF) to the deceptive flatness problem and a theoretical assurance for
adversarial transferability. Based on this, using an efficient approximation of
our objective, we instantiate our attack as Adversarial Flatness Attack (AFA),
addressing the altered gradient sign issue. Additionally, to further improve
the attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by
enhancing the inner-loop sampling efficiency. The comprehensive results on
ImageNet-compatible dataset demonstrate superiority over six baselines,
generating adversarial examples in flatter regions and boosting transferability
across model architectures. When tested on input transformation attacks or the
Baidu Cloud API, our method outperforms baselines.

</details>


### [126] [CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation](https://arxiv.org/abs/2511.01243)
*Yu Tian,Zhongheng Yang,Chenshi Liu,Yiyun Su,Ziwei Hong,Zexi Gong,Jingyuan Xu*

Main category: cs.CV

TL;DR: CenterMamba-SAM delivers state-of-the-art brain lesion segmentation by freezing a pretrained backbone, fine-tuning lightweight adapters, and employing a CenterMamba encoder with corner-axis-center short-sequence scanning, memory-driven structural prompts for inter-slice coherence, and a memory-augmented multi-scale decoder for detailed, globally consistent segmentation.


<details>
  <summary>Details</summary>
Motivation: To address small, low-contrast lesions, anisotropic sampling, and cross-slice discontinuities that hinder accurate segmentation; the goal is to enhance center-prioritized information aggregation and inter-slice coherence while keeping efficient fine-tuning.

Method: End-to-end framework CenterMamba-SAM: (1) freeze backbone, train adapters; CenterMamba encoder with 3x3 corner-axis-center short-sequence scanning; memory-driven structural prompt generator with prototype bank across neighboring slices; memory-augmented multi-scale decoder with memory attention and deep supervision for progressive refinement.

Result: Achieves state-of-the-art performance on public brain lesion segmentation benchmarks.

Conclusion: The architecture improves weak boundary sensitivity and tiny foci detection, preserves sparse representations, and delivers coherent, high-quality segmentations across slices.

Abstract: Brain lesion segmentation remains challenging due to small, low-contrast
lesions, anisotropic sampling, and cross-slice discontinuities. We propose
CenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and
trains only lightweight adapters for efficient fine-tuning. At its core is the
CenterMamba encoder, which employs a novel 3x3 corner-axis-center
short-sequence scanning strategy to enable center-prioritized, axis-reinforced,
and diagonally compensated information aggregation. This design enhances
sensitivity to weak boundaries and tiny foci while maintaining sparse yet
effective feature representation. A memory-driven structural prompt generator
maintains a prototype bank across neighboring slices, enabling automatic
synthesis of reliable prompts without user interaction, thereby improving
inter-slice coherence. The memory-augmented multi-scale decoder integrates
memory attention modules at multiple levels, combining deep supervision with
progressive refinement to restore fine details while preserving global
consistency. Extensive experiments on public benchmarks demonstrate that
CenterMamba-SAM achieves state-of-the-art performance in brain lesion
segmentation.

</details>


### [127] [Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop](https://arxiv.org/abs/2511.01250)
*YoungJae Cheong,Jhonghyun An*

Main category: cs.CV

TL;DR: A Light Geometry-aware adapter improves all-weather LiDAR segmentation by training-time geometry cues, yielding significant cross-weather gains (up to +7.9 mIoU over augmentation baselines).


<details>
  <summary>Details</summary>
Motivation: LiDAR semantic segmentation degrades in adverse weather due to refraction, scattering, and point dropouts. Prior robustness methods address data issues but miss vulnerabilities near boundaries, corners, and sparse regions.

Method: Introduce a plug-in module that (1) aligns azimuth with horizontal circular padding to preserve wrap-around continuity, (2) uses a local-window KNN to gather neighbors and compute simple local statistics, and (3) compresses these into geometry-aware cues. During training, cues drive region-aware regularization to stabilize predictions in structurally fragile areas. It is train-only and inference-friendly, and evaluated in a source-only cross-weather setup (train on SemanticKITTI, evaluate on SemanticSTF) without target labels or fine-tuning.

Result: The adapter achieves +7.9 percentage points in mIoU over a data-centric augmentation baseline and +0.6 points over a class-centric regularization baseline in the cross-weather setting.

Conclusion: Geometry-driven regularization is a key direction for improving all-weather LiDAR segmentation.

Abstract: LiDAR semantic segmentation degrades in adverse weather because refraction,
scattering, and point dropouts corrupt geometry. Prior work in weather
simulation, mixing-based augmentation, domain randomization, and uncertainty or
boundary regularization improves robustness but still overlooks structural
vulnerabilities near boundaries, corners, and sparse regions. We present a
Light Geometry-aware adapter. The module aligns azimuth and applies horizontal
circular padding to preserve neighbor continuity across the 0~360 degree
wrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points
and computes simple local statistics, which are compressed into compact
geometry-aware cues. During training, these cues drive region-aware
regularization that stabilizes predictions in structurally fragile areas. The
adapter is plug and play, complements augmentation, and can be enabled only
during training with negligible inference cost. We adopt a source-only
cross-weather setup where models train on SemanticKITTI and are evaluated on
SemanticSTF without target labels or fine-tuning. The adapter improves mIoU by
7.9 percentage points over the data-centric augmentation baseline and by 0.6
points over the class-centric regularization baseline. These results indicate
that geometry-driven regularization is a key direction for all-weather LiDAR
segmentation.

</details>


### [128] [MotionStream: Real-Time Video Generation with Interactive Motion Controls](https://arxiv.org/abs/2511.01266)
*Joonghyuk Shin,Zhengqi Li,Richard Zhang,Jun-Yan Zhu,Jaesik Park,Eli Schechtman,Xun Huang*

Main category: cs.CV

TL;DR: MotionStream enables sub-second, real-time, infinite-horizon motion-conditioned video streaming by distilling a non-causal teacher into a causal student using Self Forcing with Distribution Matching Distillation, aided by sliding-window attention and KV cache rolling, achieving up to 29 FPS on a single GPU.


<details>
  <summary>Details</summary>
Motivation: Existing motion-conditioned video generation suffers from prohibitive latency (minutes per video) and non-causal processing that blocks interactive use; there is a need for real-time, streaming, interactive video synthesis.

Method: Train a non-causal, bidirectional teacher for motion-conditioned video with global text prompts and local motion guidance; perform Self Forcing with Distribution Matching Distillation to train a causal student; implement sliding-window causal attention, attention sinks, self-rollout, and KV cache rolling during training to simulate inference with a fixed context window; enable constant-speed generation of arbitrarily long videos.

Result: State-of-the-art in motion following and video quality; two orders of magnitude faster than previous methods; uniquely supports infinite-length streaming with real-time generation (~29 FPS on a single GPU).

Conclusion: MotionStream enables interactive, real-time video synthesis for tasks like painting trajectories, camera control, and motion transfer, delivering results in real-time and enabling a truly interactive experience.

Abstract: Current motion-conditioned video generation methods suffer from prohibitive
latency (minutes per video) and non-causal processing that prevents real-time
interaction. We present MotionStream, enabling sub-second latency with up to 29
FPS streaming generation on a single GPU. Our approach begins by augmenting a
text-to-video model with motion control, which generates high-quality videos
that adhere to the global text prompt and local motion guidance, but does not
perform inference on the fly. As such, we distill this bidirectional teacher
into a causal student through Self Forcing with Distribution Matching
Distillation, enabling real-time streaming inference. Several key challenges
arise when generating videos of long, potentially infinite time-horizons: (1)
bridging the domain gap from training on finite length and extrapolating to
infinite horizons, (2) sustaining high quality by preventing error
accumulation, and (3) maintaining fast inference, without incurring growth in
computational cost due to increasing context windows. A key to our approach is
introducing carefully designed sliding-window causal attention, combined with
attention sinks. By incorporating self-rollout with attention sinks and KV
cache rolling during training, we properly simulate inference-time
extrapolations with a fixed context window, enabling constant-speed generation
of arbitrarily long videos. Our models achieve state-of-the-art results in
motion following and video quality while being two orders of magnitude faster,
uniquely enabling infinite-length streaming. With MotionStream, users can paint
trajectories, control cameras, or transfer motion, and see results unfold in
real-time, delivering a truly interactive experience.

</details>


### [129] [PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers](https://arxiv.org/abs/2511.01274)
*Tan Tang,Yanhong Wu,Junming Gao,Yingcai Wu*

Main category: cs.CV

TL;DR: PRevivor is a prior-guided color transformer for ancient Chinese paintings that decomposes color restoration into luminance enhancement and hue correction, using variational U-Nets and a dual-branch color query module guided by localized hue priors. It leverages recent paintings (Ming/Qing) as priors to restore older ones (Tang/Song) and outperforms state-of-the-art colorization methods in experiments.


<details>
  <summary>Details</summary>
Motivation: Ancient paintings suffer irreversible color degradation and current datasets and end-to-end restoration tools are lacking. The work aims to exploit priors from recently preserved paintings to guide restoration of older works, addressing both luminance fading and hue distortion with a structured, learnable approach.

Method: 1) Luminance enhancement via two variational U-Nets and a multi-scale mapping module to translate faded luminance to restored luminance. 2) Hue correction via a dual-branch color query module: one branch uses masked priors to enforce localized hue correction, the other branch is unconstrained to preserve global reasoning. The priors are localized hue priors extracted from faded paintings. Training uses recent Ming/Qing paintings to restore Tang/Song pieces. Evaluation against state-of-the-art colorization methods.

Result: Experiments show superior performance both quantitatively and qualitatively compared to state-of-the-art colorization methods.

Conclusion: PRevivor demonstrates that a prior-guided, two-stage color restoration framework can effectively revive colors in degraded ancient paintings. The combination of luminance-focused enhancement and pose-aware hue correction with priors enables better restoration quality and suggests a viable path for end-to-end digital restoration tools in cultural heritage contexts.

Abstract: Ancient Chinese paintings are a valuable cultural heritage that is damaged by
irreversible color degradation. Reviving color-degraded paintings is
extraordinarily difficult due to the complex chemistry mechanism. Progress is
further slowed by the lack of comprehensive, high-quality datasets, which
hampers the creation of end-to-end digital restoration tools. To revive colors,
we propose PRevivor, a prior-guided color transformer that learns from recent
paintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and
Song Dynasty). To develop PRevivor, we decompose color restoration into two
sequential sub-tasks: luminance enhancement and hue correction. For luminance
enhancement, we employ two variational U-Nets and a multi-scale mapping module
to translate faded luminance into restored counterparts. For hue correction, we
design a dual-branch color query module guided by localized hue priors
extracted from faded paintings. Specifically, one branch focuses attention on
regions guided by masked priors, enforcing localized hue correction, whereas
the other branch remains unconstrained to maintain a global reasoning
capability. To evaluate PRevivor, we conduct extensive experiments against
state-of-the-art colorization methods. The results demonstrate superior
performance both quantitatively and qualitatively.

</details>


### [130] [Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions](https://arxiv.org/abs/2511.01284)
*Karma Phuntsho,Abdullah,Kyungmi Lee,Ickjai Lee,Euijoon Ahn*

Main category: cs.CV

TL;DR: Foundation models show promise for generalizable medical imaging tasks but require strategies to tackle domain shifts, data scarcity, privacy, and compute; a multi-faceted roadmap (continual learning, federated privacy, data-centric pipelines, benchmarking) is proposed.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between FM potential and real-world clinical deployment by addressing practical constraints and ensuring trustworthy, generalizable performance.

Method: Review and synthesize strategies: supervised fine-tuning, domain-specific pretraining, parameter-efficient fine-tuning, self-supervised learning, hybrids, and multimodal frameworks; discuss emerging directions and their trade-offs.

Result: Summarizes reported gains, applicability, and limitations; highlights overlooked trade-offs; proposes roadmap and research gaps; emphasizes need for systematic benchmarking and privacy-preserving approaches.

Conclusion: FM adaptation to medical imaging is feasible but requires addressing real-world variability, privacy, and resource constraints; aims to guide development of adaptive, clinically integrated FMs.

Abstract: Foundation models (FMs) have emerged as a transformative paradigm in medical
image analysis, offering the potential to provide generalizable, task-agnostic
solutions across a wide range of clinical tasks and imaging modalities. Their
capacity to learn transferable representations from large-scale data has the
potential to address the limitations of conventional task-specific models.
However, adaptation of FMs to real-world clinical practice remains constrained
by key challenges, including domain shifts, limited availability of
high-quality annotated data, substantial computational demands, and strict
privacy requirements. This review presents a comprehensive assessment of
strategies for adapting FMs to the specific demands of medical imaging. We
examine approaches such as supervised fine-tuning, domain-specific pretraining,
parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and
multimodal or cross-modal frameworks. For each, we evaluate reported
performance gains, clinical applicability, and limitations, while identifying
trade-offs and unresolved challenges that prior reviews have often overlooked.
Beyond these established techniques, we also highlight emerging directions
aimed at addressing current gaps. These include continual learning to enable
dynamic deployment, federated and privacy-preserving approaches to safeguard
sensitive data, hybrid self-supervised learning to enhance data efficiency,
data-centric pipelines that combine synthetic generation with human-in-the-loop
validation, and systematic benchmarking to assess robust generalization under
real-world clinical variability. By outlining these strategies and associated
research gaps, this review provides a roadmap for developing adaptive,
trustworthy, and clinically integrated FMs capable of meeting the demands of
real-world medical imaging.

</details>


### [131] [Detecting Generated Images by Fitting Natural Image Distributions](https://arxiv.org/abs/2511.01293)
*Yonggang Zhang,Jun Nie,Xinmei Tian,Mingming Gong,Kun Zhang,Bo Han*

Main category: cs.CV

TL;DR: A geometry-based detection framework distinguishes real vs. generated images by exploiting mutually orthogonal gradient subspaces of two functions. It checks if transforming an image along the data manifold significantly changes a self-supervised modelâs loss; to counteract evolving generators, it uses normalizing flows to push generated images off the natural manifold. Empirical results show strong efficacy with code released at the given URL.


<details>
  <summary>Details</summary>
Motivation: Current detectors rely on data-hungry binary classifiers that are sensitive to the available pool of generated images and fail as generators improve. A geometry-based, self-supervised approach aims to provide robust detection less dependent on labeled generated data and resilient to shifts in generator behavior by leveraging data-manifold differences.

Method: Construct a pair of functions whose gradients lie in mutually orthogonal subspaces, yielding consistent outputs on natural images but divergent outputs on generated ones. Detection is performed by applying a transformation along the data manifold and monitoring whether the self-supervised model pre-trained on natural images experiences a significant loss change. To maintain detectability as generators improve, employ normalizing flows to push generated images away from the natural-manifold region, amplifying the detectable signal.

Result: Extensive experiments demonstrate the effectiveness of the proposed framework. The approach achieves robust detection performance and demonstrates resilience to changes in generative models. Code is available at the provided GitHub repository.

Conclusion: A geometry-based, self-supervised detection framework that leverages orthogonal gradient directions and manifold transformations offers robust detection of generated images, with normalizing flows helping to sustain detectability as generators evolve. This approach reduces reliance on large labeled datasets and adapts to advancing generative models.

Abstract: The increasing realism of generated images has raised significant concerns
about their potential misuse, necessitating robust detection methods. Current
approaches mainly rely on training binary classifiers, which depend heavily on
the quantity and quality of available generated images. In this work, we
propose a novel framework that exploits geometric differences between the data
manifolds of natural and generated images. To exploit this difference, we
employ a pair of functions engineered to yield consistent outputs for natural
images but divergent outputs for generated ones, leveraging the property that
their gradients reside in mutually orthogonal subspaces. This design enables a
simple yet effective detection method: an image is identified as generated if a
transformation along its data manifold induces a significant change in the loss
value of a self-supervised model pre-trained on natural images. Further more,
to address diminishing manifold disparities in advanced generative models, we
leverage normalizing flows to amplify detectable differences by extruding
generated images away from the natural image manifold. Extensive experiments
demonstrate the efficacy of this method. Code is available at
https://github.com/tmlr-group/ConV.

</details>


### [132] [UniREditBench: A Unified Reasoning-based Image Editing Benchmark](https://arxiv.org/abs/2511.01295)
*Feng Han,Yibin Wang,Chenglin Li,Zheming Liang,Dianyi Wang,Yang Jiao,Zhipeng Wei,Chao Gong,Cheng Jin,Jingjing Chen,Jiaqi Wang*

Main category: cs.CV

TL;DR: Introduces UniREditBench, a multimodal, reasoning-focused benchmark for image editing, with 2,700 samples across real/game-world scenarios, dual-reference evaluation, a 100K synthetic CoT dataset, and a Bagel-based baseline, showing improved in-domain/out-of-distribution performance and detailed model diagnostics.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks underrepresent multi-object interactions and rule-based reasoning in game-like scenarios; they rely on text-only references, which can cause misjudgments in complex reasoning. A unified, multimodal benchmark is needed to reliably evaluate reasoning-based image editing.

Method: Develop UniREditBench with 2,700 samples spanning 8 primary dimensions and 18 sub-dimensions across real and game-world contexts. Implement multimodal dual-reference evaluation (text and ground-truth images). Build an automated multi-scenario data-synthesis pipeline and UniREdit-Data-100K, a synthetic dataset with high-quality chain-of-thought annotations. Fine-tune Bagel on this dataset to create UniREdit-Bagel. Benchmark both open-source and closed-source image editing models.

Result: UniREdit-Bagel achieves substantial improvements in both in-domain and out-of-distribution settings. The benchmark and data enable more reliable evaluation, revealing strengths and weaknesses of models across various aspects.

Conclusion: UniREditBench offers a comprehensive, reliable framework for reasoning-based image editing evaluation that can guide model development, enable fairer cross-model comparisons, and support future extensions to more complex scenarios.

Abstract: Recent advances in multi-modal generative models have driven substantial
improvements in image editing. However, current generative models still
struggle with handling diverse and complex image editing tasks that require
implicit reasoning, underscoring the need for a comprehensive benchmark to
systematically assess their performance across various reasoning scenarios.
Existing benchmarks primarily focus on single-object attribute transformation
in realistic scenarios, which, while effective, encounter two key challenges:
(1) they largely overlook multi-object interactions as well as game-world
scenarios that involve human-defined rules, which are common in real-life
applications; (2) they only rely on textual references to evaluate the
generated images, potentially leading to systematic misjudgments, especially in
complex reasoning scenarios. To this end, this work proposes UniREditBench, a
unified benchmark for reasoning-based image editing evaluation. It comprises
2,700 meticulously curated samples, covering both real- and game-world
scenarios across 8 primary dimensions and 18 sub-dimensions. To improve
evaluation reliability, we introduce multimodal dual-reference evaluation,
providing both textual and ground-truth image references for each sample
assessment. Furthermore, we design an automated multi-scenario data synthesis
pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with
high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel
on this dataset and develop UniREdit-Bagel, demonstrating substantial
improvements in both in-domain and out-of-distribution settings. Through
thorough benchmarking of both open-source and closed-source image editing
models, we reveal their strengths and weaknesses across various aspects.

</details>


### [133] [REASON: Probability map-guided dual-branch fusion framework for gastric content assessment](https://arxiv.org/abs/2511.01302)
*Nu-Fnag Xiao,De-Xing Huang,Le-Tian Wang,Mei-Jiang Gui,Qi Fu,Xiao-Liang Xie,Shi-Qi Liu,Shuangyi Wang,Zeng-Guang Hou,Ying-Wei Wang,Xiao-Hu Zhou*

Main category: cs.CV

TL;DR: A two-stage framework called REASON improves gastric content assessment from ultrasound by combining probabilistic segmentation with a dual-view classifier, outperforming existing methods on a self-collected dataset and enabling automated preoperative aspiration risk assessment.


<details>
  <summary>Details</summary>
Motivation: Manual tracing of gastric antra and reliance on empirical formulas are inefficient and often inaccurate, hindering reliable aspiration risk stratification during induction.

Method: Stage 1 uses a segmentation model to produce probability maps that suppress artifacts and emphasize gastric anatomy. Stage 2 employs a dual-branch classifier that fuses features from two standard ultrasound views, right lateral decubitus (RLD) and supine (SUP), to improve discrimination of learned features.

Result: The proposed framework outperforms current state-of-the-art approaches by a significant margin on a self-collected dataset.

Conclusion: REASON shows promise as an automated, robust, and efficient solution for preoperative aspiration risk assessment in clinical practice.

Abstract: Accurate assessment of gastric content from ultrasound is critical for
stratifying aspiration risk at induction of general anesthesia. However,
traditional methods rely on manual tracing of gastric antra and empirical
formulas, which face significant limitations in both efficiency and accuracy.
To address these challenges, a novel two-stage probability map-guided
dual-branch fusion framework (REASON) for gastric content assessment is
proposed. In stage 1, a segmentation model generates probability maps that
suppress artifacts and highlight gastric anatomy. In stage 2, a dual-branch
classifier fuses information from two standard views, right lateral decubitus
(RLD) and supine (SUP), to improve the discrimination of learned features.
Experimental results on a self-collected dataset demonstrate that the proposed
framework outperforms current state-of-the-art approaches by a significant
margin. This framework shows great promise for automated preoperative
aspiration risk assessment, offering a more robust, efficient, and accurate
solution for clinical practice.

</details>


### [134] [Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation](https://arxiv.org/abs/2511.01304)
*Chentao Li,Behzad Bozorgtabar,Yifang Ping,Pan Huang,Jing Qin*

Main category: cs.CV

TL;DR: A MIL framework for whole-slide images that disentangles spatial, semantic, and decision entanglements through latent factor grouping, cluster-reasoning, and instance re-weighting, achieving state-of-the-art accuracy with interpretable, pathologist-aligned decisions.


<details>
  <summary>Details</summary>
Motivation: MIL on whole-slide images suffers from entanglements (spatial, semantic, decision) among instances, which hurts representation quality and interpretability. There is a need for disentangled, interpretable representations that align with pathologist reasoning while improving performance.

Method: Three-phase approach: (1) positive semi-definite latent factor grouping to map instances into a latent subspace, reducing spatial entanglement; (2) address semantic entanglement via instance probability counterfactual inference and cluster-reasoning instance disentangling; (3) generalized linear weighted decision via instance effect re-weighting to mitigate decision entanglement.

Result: Experimental results on multicentre datasets show the proposed method outperforms all state-of-the-art models, with pathologist-aligned interpretability arising from disentangled representations and a transparent decision-making process.

Conclusion: The latent factor grouping-boosted cluster-reasoning framework provides disentangled, interpretable MIL representations for WSIs, delivering superior performance and clearer alignment with expert reasoning.

Abstract: Multiple instance learning (MIL) has been widely used for representing
whole-slide pathology images. However, spatial, semantic, and decision
entanglements among instances limit its representation and interpretability. To
address these challenges, we propose a latent factor grouping-boosted
cluster-reasoning instance disentangled learning framework for whole-slide
image (WSI) interpretable representation in three phases. First, we introduce a
novel positive semi-definite latent factor grouping that maps instances into a
latent subspace, effectively mitigating spatial entanglement in MIL. To
alleviate semantic entanglement, we employs instance probability counterfactual
inference and optimization via cluster-reasoning instance disentangling.
Finally, we employ a generalized linear weighted decision via instance effect
re-weighting to address decision entanglement. Extensive experiments on
multicentre datasets demonstrate that our model outperforms all
state-of-the-art models. Moreover, it attains pathologist-aligned
interpretability through disentangled representations and a transparent
decision-making process.

</details>


### [135] [Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models](https://arxiv.org/abs/2511.01307)
*Tae-Young Lee,Juwon Seo,Jong Hwan Ko,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: APDM introduces protective optimization and a dual-path training regime to disrupt subject personalization in diffusion models, achieving state-of-the-art protection against unauthorized personalization while preserving generation quality; code released.


<details>
  <summary>Details</summary>
Motivation: Personalization-enabled diffusion models pose privacy risks as malicious users can generate unauthorized content. Existing defenses rely on unrealistic assumptions and falter with clean images or simple transformations, motivating a model-centric protection approach.

Method: The authors provide a theoretical analysis showing naive loss functions cannot ensure convergence for robust anti-personalization. They propose Direct Protective Optimization (DPO), a loss function that disrupts personalization without harming generative quality, and Learning to Protect (L2P), a dual-path optimization that alternates between personalization and protection paths to simulate future personalization trajectories and reinforce protection.

Result: Experiments show APDM outperforms existing methods, achieving state-of-the-art protection against unauthorized personalization while maintaining high-quality generation. Code is available at the provided GitHub link.

Conclusion: APDM effectively protects diffusion models from being personalized by adversaries through DPO and L2P, shifting protection from images to the model itself without sacrificing generative performance.

Abstract: Recent advances in diffusion models have enabled high-quality synthesis of
specific subjects, such as identities or objects. This capability, while
unlocking new possibilities in content creation, also introduces significant
privacy risks, as personalization techniques can be misused by malicious users
to generate unauthorized content. Although several studies have attempted to
counter this by generating adversarially perturbed samples designed to disrupt
personalization, they rely on unrealistic assumptions and become ineffective in
the presence of even a few clean images or under simple image transformations.
To address these challenges, we shift the protection target from the images to
the diffusion model itself to hinder the personalization of specific subjects,
through our novel framework called Anti-Personalized Diffusion Models (APDM).
We first provide a theoretical analysis demonstrating that a naive approach of
existing loss functions to diffusion models is inherently incapable of ensuring
convergence for robust anti-personalization. Motivated by this finding, we
introduce Direct Protective Optimization (DPO), a novel loss function that
effectively disrupts subject personalization in the target model without
compromising generative quality. Moreover, we propose a new dual-path
optimization strategy, coined Learning to Protect (L2P). By alternating between
personalization and protection paths, L2P simulates future personalization
trajectories and adaptively reinforces protection at each step. Experimental
results demonstrate that our framework outperforms existing methods, achieving
state-of-the-art performance in preventing unauthorized personalization. The
code is available at https://github.com/KU-VGI/APDM.

</details>


### [136] [MVSMamba: Multi-View Stereo with State Space Model](https://arxiv.org/abs/2511.01315)
*Jianfei Jiang,Qiankun Liu,Hongyuan Liu,Haochen Yu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: MVSMamba introduces a Mamba-based MVS network that uses a Dynamic Mamba module with a reference-centered dynamic scanning strategy to achieve efficient global feature aggregation and cross-view interaction, achieving state-of-the-art results with improved efficiency.


<details>
  <summary>Details</summary>
Motivation: Transformer-based MVS methods struggle with quadratic complexity and limited efficiency. A linear-complexity global modeling approach like Mamba could provide robust, scalable feature representations for multi-view stereo.

Method: Propose MVSMamba, the first Mamba-based MVS network. It employs a Dynamic Mamba (DM) module with a reference-centered dynamic scanning strategy to enable efficient intra- and inter-view feature interaction from the reference view to source views, produce omnidirectional multi-view feature representations, and perform multi-scale global feature aggregation.

Result: MVSMamba outperforms state-of-the-art MVS methods on the DTU dataset and Tanks-and-Temples benchmark, delivering both higher performance and better efficiency.

Conclusion: The work demonstrates the effectiveness of Mamba-based models for MVS and presents a scalable, efficient approach for global feature aggregation and cross-view interaction; code is released for reproducibility.

Abstract: Robust feature representations are essential for learning-based Multi-View
Stereo (MVS), which relies on accurate feature matching. Recent MVS methods
leverage Transformers to capture long-range dependencies based on local
features extracted by conventional feature pyramid networks. However, the
quadratic complexity of Transformer-based MVS methods poses challenges to
balance performance and efficiency. Motivated by the global modeling capability
and linear complexity of the Mamba architecture, we propose MVSMamba, the first
Mamba-based MVS network. MVSMamba enables efficient global feature aggregation
with minimal computational overhead. To fully exploit Mamba's potential in MVS,
we propose a Dynamic Mamba module (DM-module) based on a novel
reference-centered dynamic scanning strategy, which enables: (1) Efficient
intra- and inter-view feature interaction from the reference to source views,
(2) Omnidirectional multi-view feature representations, and (3) Multi-scale
global feature aggregation. Extensive experimental results demonstrate MVSMamba
outperforms state-of-the-art MVS methods on the DTU dataset and the
Tanks-and-Temples benchmark with both superior performance and efficiency. The
source code is available at https://github.com/JianfeiJ/MVSMamba.

</details>


### [137] [A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model](https://arxiv.org/abs/2511.01317)
*Sampriti Soor,Alik Pramanick,Jothiprakash K,Arijit Sur*

Main category: cs.CV

TL;DR: A CLIP-guided generative adversarial attack creates perceptually invisible perturbations for multi-object multilabel classifiers by combining SSAE-like concentrated perturbations with semantically dissimilar text embeddings (GAMA), achieving competitive results with higher visual fidelity across black-box models.


<details>
  <summary>Details</summary>
Motivation: Adversarial attacks require effective perturbations that fool models while remaining visually imperceptible; multi-object scenes and multilabel classifiers pose additional challenges; leveraging CLIP's textâimage alignment can inject natural language semantics into the attack.

Method: Use CLIP to guide perturbations via a loss that aligns perturbed image features with target text semantics; integrate SSAE concentrated perturbation strategy and dissimilar text embeddings akin to GAMA to manipulate multi-object scenes; ensure high structural similarity (SSIM) to original; evaluate on various black-box victim models.

Result: Competitive performance relative to existing techniques; achieving comparable or superior attack success with greater visual fidelity; demonstrates effectiveness across diverse black-box models and multi-object scenes.

Conclusion: CLIP-based semantic guidance combined with SSAE and GAMA-inspired embeddings yields strong, perceptually faithful adversarial perturbations for multilabel, multi-object vision tasks, expanding transferable attack capabilities in black-box settings.

Abstract: The rapid growth of deep learning has brought about powerful models that can
handle various tasks, like identifying images and understanding language.
However, adversarial attacks, an unnoticed alteration, can deceive models,
leading to inaccurate predictions. In this paper, a generative adversarial
attack method is proposed that uses the CLIP model to create highly effective
and visually imperceptible adversarial perturbations. The CLIP model's ability
to align text and image representation helps incorporate natural language
semantics with a guided loss to generate effective adversarial examples that
look identical to the original inputs. This integration allows extensive scene
manipulation, creating perturbations in multi-object environments specifically
designed to deceive multilabel classifiers. Our approach integrates the
concentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with
the dissimilar text embeddings similar to Generative Adversarial Multi-Object
Scene Attacks (GAMA), resulting in perturbations that both deceive
classification models and maintain high structural similarity to the original
images. The model was tested on various tasks across diverse black-box victim
models. The experimental results show that our method performs competitively,
achieving comparable or superior results to existing techniques, while
preserving greater visual fidelity.

</details>


### [138] [RDTE-UNet: A Boundary and Detail Aware UNet for Precise Medical Image Segmentation](https://arxiv.org/abs/2511.01328)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: RDTE-UNet is a UNet-like segmentation network that fuses local transformer-based decoding with global context to improve boundary delineation and detail preservation. It introduces three modulesâASBE, HVDA, and EulerFFâfor adaptive boundary enhancement, fine-grained feature modeling, and Euler-guided fusion weighting. It achieves competitive segmentation accuracy and boundary quality on Synapse and BUSI datasets.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation faces substantial anatomical variability and boundary ambiguity, which complicates reliable delineation of fine structures. A model that unifies local detail modeling with global context and robust boundary handling is needed.

Method: RDTE-UNet uses a hybrid ResBlock detail-aware Transformer backbone. It incorporates three modules: ASBE (adaptive boundary enhancement) to sharpen boundaries, HVDA (high-definition feature modeling) for fine-grained feature extraction, and EulerFF (fusion weighting guided by Euler's formula) for principled feature fusion. This design aims to improve structural consistency across morphology, orientation, and scale.

Result: On Synapse and BUSI datasets, RDTE-UNet achieves a comparable level of segmentation accuracy and boundary quality to existing methods, indicating effective boundary delineation and detail preservation.

Conclusion: RDTE-UNet demonstrates that combining a ResBlock detail-aware Transformer backbone with ASBE, HVDA, and EulerFF can yield competitive segmentation performance and improved boundary delineation in medical images, across varying morphologies and scales.

Abstract: Medical image segmentation is essential for computer-assisted diagnosis and
treatment planning, yet substantial anatomical variability and boundary
ambiguity hinder reliable delineation of fine structures. We propose RDTE-UNet,
a segmentation network that unifies local modeling with global context to
strengthen boundary delineation and detail preservation. RDTE-UNet employs a
hybrid ResBlock detail-aware Transformer backbone and three modules: ASBE for
adaptive boundary enhancement, HVDA for fine-grained feature modeling, and
EulerFF for fusion weighting guided by Euler's formula. Together, these
components improve structural consistency and boundary accuracy across
morphology, orientation, and scale. On Synapse and BUSI dataset, RDTE-UNet has
achieved a comparable level in terms of segmentation accuracy and boundary
quality.

</details>


### [139] [$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles](https://arxiv.org/abs/2511.01340)
*Trishanu Das,Abhilash Nandy,Khush Bajaj,Deepiha S*

Main category: cs.CV

TL;DR: A new large rebus puzzle benchmark and a model-agnostic reasoning framework improve Vision-Language models on rebus tasks.


<details>
  <summary>Details</summary>
Motivation: Rebus puzzles demand diverse reasoning skills and current V-L models struggle; a standardized benchmark and improved reasoning strategies are needed.

Method: Create the Rebus BUS benchmark with 1,333 puzzles across 18 categories; propose RebusDescProgICE, combining an unstructured description with code-based structured reasoning and improved in-context example selection; evaluate across closed-source and open-source models against Chain-of-Thought.

Result: Closed-source models see 2.1â4.1 percentage point gains; open-source models see 20â30 percentage point gains over Chain-of-Thought; framework is model-agnostic and improves LLM/V-L reasoning on rebus tasks.

Conclusion: A sizable, diverse benchmark plus the RebusDescProgICE framework advances reasoning performance on complex visual-linguistic puzzles and provides a resource for future research.

Abstract: Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters
to represent words or phrases creatively) requires a variety of skills such as
image recognition, cognitive skills, commonsense reasoning, multi-step
reasoning, image-based wordplay, etc., making this a challenging task for even
current Vision-Language Models. In this paper, we present
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$, a large and diverse
benchmark of $1,333$ English Rebus Puzzles containing different artistic styles
and levels of difficulty, spread across 18 categories such as food, idioms,
sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a
model-agnostic framework which uses a combination of an unstructured
description and code-based, structured reasoning, along with better,
reasoning-based in-context example selection, improving the performance of
Vision-Language Models on
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$ by $2.1-4.1\%$ and
$20-30\%$ using closed-source and open-source models respectively compared to
Chain-of-Thought Reasoning.

</details>


### [140] [MIQ-SAM3D: From Single-Point Prompt to Multi-Instance Segmentation via Competitive Query Refinement](https://arxiv.org/abs/2511.01345)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: A 3D medical image segmentation method (MIQ-SAM3D) extends SAM-based interactive segmentation from single-point-to-single-object to single-point-to-multi-instance. It uses a prompt-conditioned query generator to retrieve all semantically similar lesions from a 3D volume, a hybrid CNN-Transformer encoder that injects boundary saliency into ViT attention, and a competitively optimized decoder enabling parallel multi-instance predictions. Evaluated on LiTS17 and KiTS21, it shows competitive performance and robustness to prompts for multi-lesion cases.


<details>
  <summary>Details</summary>
Motivation: Address limitations of SAM-based segmentation in medical imaging: single-point-to-single-object prompts restrict multi-lesion segmentation; ViT lacks fine-grained local detail. The work seeks to enable multi-lesion, efficient, robust 3D segmentation.

Method: Introduce MIQ-SAM3D with (1) prompt-conditioned instance-query generator converting a single point into multiple specialized queries to fetch all semantically similar lesions in 3D; (2) a hybrid CNN-Transformer encoder injecting boundary saliency into ViT self-attention via spatial gating; (3) a competitively optimized query decoder enabling end-to-end, parallel, multi-instance prediction through inter-query competition.

Result: On LiTS17 and KiTS21, MIQ-SAM3D achieves comparable performance to existing methods and demonstrates strong robustness to different prompts, enabling efficient annotation of multi-lesion cases.

Conclusion: MIQ-SAM3D advances SAM-based interactive segmentation by supporting multi-instance 3D segmentation with better integration of local detail and cross-lesion retrieval, offering a practical approach for multi-lesion annotation in clinical workflows.

Abstract: Accurate segmentation of medical images is fundamental to tumor diagnosis and
treatment planning. SAM-based interactive segmentation has gained attention for
its strong generalization, but most methods follow a
single-point-to-single-object paradigm, which limits multi-lesion segmentation.
Moreover, ViT backbones capture global context but often miss high-fidelity
local details. We propose MIQ-SAM3D, a multi-instance 3D segmentation framework
with a competitive query optimization strategy that shifts from
single-point-to-single-mask to single-point-to-multi-instance. A
prompt-conditioned instance-query generator transforms a single point prompt
into multiple specialized queries, enabling retrieval of all semantically
similar lesions across the 3D volume from a single exemplar. A hybrid
CNN-Transformer encoder injects CNN-derived boundary saliency into ViT
self-attention via spatial gating. A competitively optimized query decoder then
enables end-to-end, parallel, multi-instance prediction through inter-query
competition. On LiTS17 and KiTS21 dataset, MIQ-SAM3D achieved comparable levels
and exhibits strong robustness to prompts, providing a practical solution for
efficient annotation of clinically relevant multi-lesion cases.

</details>


### [141] [Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion](https://arxiv.org/abs/2511.01355)
*Linhao Huang*

Main category: cs.CV

TL;DR: Introduces Content-Style Subspace Blending and a Content-Style Balance loss to expand the content-style frontier in text-to-image diffusion, maintaining content fidelity across varying style intensities.


<details>
  <summary>Details</summary>
Motivation: To address the deterioration of content features when increasing style intensity and to broaden the range of achievable content-style trade-offs.

Method: Proposes Content-Style Subspace Blending plus a Content-Style Balance loss to preserve content similarity while applying different style intensities, thereby expanding the frontier.

Result: Significant improvement in content similarity across style intensities, a broader content-style frontier, and superior trade-offs compared with existing methods, demonstrated by qualitative and quantitative metrics including lower IGD and GD scores.

Conclusion: The proposed approach effectively expands the content-style frontier and yields better content-style trade-offs in diffusion-based generation.

Abstract: Recent advancements in text-to-image diffusion models have significantly
improved the personalization and stylization of generated images. However,
previous studies have only assessed content similarity under a single style
intensity. In our experiments, we observe that increasing style intensity leads
to a significant loss of content features, resulting in a suboptimal
content-style frontier. To address this, we propose a novel approach to expand
the content-style frontier by leveraging Content-Style Subspace Blending and a
Content-Style Balance loss. Our method improves content similarity across
varying style intensities, significantly broadening the content-style frontier.
Extensive experiments demonstrate that our approach outperforms existing
techniques in both qualitative and quantitative evaluations, achieving superior
content-style trade-off with significantly lower Inverted Generational Distance
(IGD) and Generational Distance (GD) scores compared to current methods.

</details>


### [142] [CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering](https://arxiv.org/abs/2511.01357)
*Qiangguo Jin,Xianyao Zheng,Hui Cui,Changming Sun,Yuqi Fang,Cong Cong,Ran Su,Leyi Wei,Ping Xuan,Junbo Wang*

Main category: cs.CV

TL;DR: Introduces CMI-MTL, a cross-modal multi-task framework for Medical VQA that integrates fine-grained visual-text alignment, cross-modal interleaved representations, and free-form answer-enhanced learning to improve open-ended VQA; achieves state-of-the-art on VQA-RAD, SLAKE, and OVQA with added interpretability analyses.


<details>
  <summary>Details</summary>
Motivation: Med-VQA methods struggle with cross-modal semantic alignment and open-ended answers. Self-attention methods often miss fine-grained image-text correspondence, and classification-based approaches limit free-form answer diversity and semantic detail. A framework that fuses fine-grained alignment, cross-modal interactions, and open-ended guidance could improve performance and interpretability.

Method: Three modules: (1) FVTA for fine-grained visual-text feature alignment to select relevant image regions; (2) CIFR for cross-modal interleaved feature representation capturing sequential cross-modal interactions; (3) FFAE for free-form answer-enhanced multi-task learning by leveraging auxiliary knowledge from open-ended questions to boost open-ended Med-VQA.

Result: CMI-MTL outperforms state-of-the-art methods on three Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Additional interpretability experiments validate effectiveness.

Conclusion: The proposed CMI-MTL effectively tackles cross-modal alignment and open-ended answer generation in Med-VQA, improving accuracy and interpretability; code is publicly available for reproduction.

Abstract: Medical visual question answering (Med-VQA) is a crucial multimodal task in
clinical decision support and telemedicine. Recent self-attention based methods
struggle to effectively handle cross-modal semantic alignments between vision
and language. Moreover, classification-based methods rely on predefined answer
sets. Treating this task as a simple classification problem may make it unable
to adapt to the diversity of free-form answers and overlook the detailed
semantic information of free-form answers. In order to tackle these challenges,
we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL)
framework that learns cross-modal feature representations from images and
texts. CMI-MTL comprises three key modules: fine-grained visual-text feature
alignment (FVTA), cross-modal interleaved feature representation (CIFR), and
free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most
relevant regions in image-text pairs through fine-grained visual-text feature
alignment. CIFR captures cross-modal sequential interactions via cross-modal
interleaved feature representation. FFAE leverages auxiliary knowledge from
open-ended questions through free-form answer-enhanced multi-task learning,
improving the model's capability for open-ended Med-VQA. Experimental results
show that CMI-MTL outperforms the existing state-of-the-art methods on three
Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more
interpretability experiments to prove the effectiveness. The code is publicly
available at https://github.com/BioMedIA-repo/CMI-MTL.

</details>


### [143] [SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment](https://arxiv.org/abs/2511.01390)
*Xinyu Mao,Junsi Li,Haoji Zhang,Yu Liang,Ming Sun*

Main category: cs.CV

TL;DR: SEPS proposes a Semantic-Enhanced Patch Slimming framework to reduce patch redundancy and ambiguity in fine-grained cross-modal alignment by merging semantics from dense and sparse texts, and using relevance-aware patch-word selection to improve cross-modal similarity; shows large gains on Flickr30K and MS-COCO, and is open-sourced.


<details>
  <summary>Details</summary>
Motivation: Address patch redundancy and ambiguity caused by information density disparities between visual patches and textual descriptions. Dense outputs from MLLMs can conflict with sparse captions, making semantic relevance estimation difficult in cross-modal tasks like VQA and image-text retrieval.

Method: A two-stage mechanism that integrates unified semantics from dense and sparse texts to identify salient visual patches. It employs relevance-aware selection with mean value computation to highlight crucial patch-word correspondences, improving cross-modal similarity assessment.

Result: SEPS achieves 23%-86% gains in rSum over various model architectures on Flickr30K and MS-COCO, with notable improvements in text-to-image retrieval.

Conclusion: SEPS effectively mitigates patch redundancy and ambiguity, enhances fine-grained cross-modal alignment, and provides strong, architecture-agnostic performance gains with open-source availability.

Abstract: Fine-grained cross-modal alignment aims to establish precise local
correspondences between vision and language, forming a cornerstone for visual
question answering and related multimodal applications. Current approaches face
challenges in addressing patch redundancy and ambiguity, which arise from the
inherent information density disparities across modalities. Recently,
Multimodal Large Language Models (MLLMs) have emerged as promising solutions to
bridge this gap through their robust semantic generation capabilities. However,
the dense textual outputs from MLLMs may introduce conflicts with the original
sparse captions. Furthermore, accurately quantifying semantic relevance between
rich visual patches and concise textual descriptions remains a core challenge.
To overcome these limitations, we introduce the Semantic-Enhanced Patch
Slimming (SEPS) framework, which systematically addresses patch redundancy and
ambiguity. Our approach employs a two-stage mechanism to integrate unified
semantics from both dense and sparse texts, enabling the identification of
salient visual patches. Additionally, it leverages relevance-aware selection
with mean value computation to highlight crucial patch-word correspondences,
thereby improving cross-modal similarity assessment. Comprehensive experiments
on Flickr30K and MS-COCO datasets validate that SEPS achieves superior
performance, surpassing existing approaches by 23\%-86\% in rSum across diverse
model architectures, with notable enhancements in text-to-image retrieval
scenarios. Our implementation is available at
https://github.com/Sweet4tars/seps.git.

</details>


### [144] [Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction](https://arxiv.org/abs/2511.01399)
*Ya Wen,Yutong Qiao,Chi Chiu Lam,Ioannis Brilakis,Sanghoon Lee,Mun On Wong*

Main category: cs.CV

TL;DR: Introduces the Fire-ART dataset and a panoramic image-based reconstruction method to embed firefighting assets into BIM, enabling improved recognition and localization for inventory management.


<details>
  <summary>Details</summary>
Motivation: There is a need for automated, accurate recognition and reconstruction of firefighting assets to support emergency preparedness, risk assessment, and rapid on-site response, overcoming limitations of conventional methods.

Method: Develop a panoramic reconstruction pipeline using the Fire-ART dataset (15 asset types, 2,626 images, 6,627 instances) and apply modified cube-map conversion plus radius-based spherical camera projection to enhance asset recognition and localization for BIM integration.

Result: Two real-world case studies yielded F1-scores of 0.73 and 0.88, with localization errors of 0.620 m and 0.428 m, respectively.

Conclusion: The Fire-ART dataset and the reconstruction approach provide valuable resources and robust methods for accurate digital management of fire safety equipment in BIM.

Abstract: Inventory management of firefighting assets is crucial for emergency
preparedness, risk assessment, and on-site fire response. However, conventional
methods are inefficient due to limited capabilities in automated asset
recognition and reconstruction. To address the challenge, this research
introduces the Fire-ART dataset and develops a panoramic image-based
reconstruction approach for semantic enrichment of firefighting assets into BIM
models. The Fire-ART dataset covers 15 fundamental assets, comprising 2,626
images and 6,627 instances, making it an extensive and publicly accessible
dataset for asset recognition. In addition, the reconstruction approach
integrates modified cube-map conversion and radius-based spherical camera
projection to enhance recognition and localization accuracy. Through
validations with two real-world case studies, the proposed approach achieves
F1-scores of 73% and 88% and localization errors of 0.620 and 0.428 meters,
respectively. The Fire-ART dataset and the reconstruction approach offer
valuable resources and robust technical solutions to enhance the accurate
digital management of fire safety equipment.

</details>


### [145] [Extremal Contours: Gradient-driven contours for compact visual attribution](https://arxiv.org/abs/2511.01411)
*Reza Karimzadeh,Albert Alonso,Frans Zdyb,Julius B. Kirkegaard,Bulat Ibragimov*

Main category: cs.CV

TL;DR: A training-free explanation method for vision models that uses smooth, Fourier-parameterized star-convex contours to replace dense masks, yielding compact, faithful, and robust explanations with multi-contour extensions.


<details>
  <summary>Details</summary>
Motivation: Dense perturbation masks in local explanations are often fragmented, overfitted, and require post-processing. A simple, robust, training-free approach that delivers faithful explanations with compact boundary representations would improve interpretability and stability across models.

Method: Parameterize a star-convex region by a truncated Fourier series to form a smooth, simply connected mask. Optimize with an extremal preserve/delete objective using classifier gradients, yielding a single (or multiple) low-parameter contours with stable boundary updates and explicit area control. Extendable to multi-contour setups to localize multiple objects.

Result: On ImageNet classifiers, the method matches the extremal fidelity of dense masks while producing compact, interpretable regions and better run-to-run consistency. It provides explicit area control for importance contour maps. The approach outperforms gradient and perturbation baselines in relevance mass and complexity, with notable gains on self-supervised DINO models (relevance mass up >15%).

Conclusion: The proposed training-free, contour-based explanations yield faithful, compact, and robust region masks with scalable multi-object localization, offering improved stability and interpretability over existing dense-mask methods and baselines.

Abstract: Faithful yet compact explanations for vision models remain a challenge, as
commonly used dense perturbation masks are often fragmented and overfitted,
needing careful post-processing. Here, we present a training-free explanation
method that replaces dense masks with smooth tunable contours. A star-convex
region is parameterized by a truncated Fourier series and optimized under an
extremal preserve/delete objective using the classifier gradients. The approach
guarantees a single, simply connected mask, cuts the number of free parameters
by orders of magnitude, and yields stable boundary updates without cleanup.
Restricting solutions to low-dimensional, smooth contours makes the method
robust to adversarial masking artifacts. On ImageNet classifiers, it matches
the extremal fidelity of dense masks while producing compact, interpretable
regions with improved run-to-run consistency. Explicit area control also
enables importance contour maps, yielding a transparent fidelity-area profiles.
Finally, we extend the approach to multi-contour and show how it can localize
multiple objects within the same framework. Across benchmarks, the method
achieves higher relevance mass and lower complexity than gradient and
perturbation based baselines, with especially strong gains on self-supervised
DINO models where it improves relevance mass by over 15% and maintains positive
faithfulness correlations.

</details>


### [146] [Towards One-step Causal Video Generation via Adversarial Self-Distillation](https://arxiv.org/abs/2511.01419)
*Yongqi Yang,Huayang Huang,Xu Peng,Xiaobin Hu,Donghao Luo,Jiangning Zhang,Chengjie Wang,Yu Wu*

Main category: cs.CV

TL;DR: A distillation-based framework for fast, high-quality causal video generation using Adversarial Self-Distillation (ASD) and First-Frame Enhancement (FFE) to enable 1â2 denoising steps with a single flexible distilled model.


<details>
  <summary>Details</summary>
Motivation: Hybrid video generation methods suffer from error accumulation and slow inference due to iterative denoising; there is a need for efficient, high-quality video synthesis with very few denoising steps without retraining for different step settings.

Method: Builds on Distribution Matching Distillation (DMD). Introduces Adversarial Self-Distillation (ASD) to align the student model's n-step denoising distribution with its (n+1)-step version at the distribution level, providing smoother supervision and combining teacher guidance with locally consistent student behavior. Adds First-Frame Enhancement (FFE) to allocate more denoising steps to initial frames while using larger skips for later frames. Results enable a single distilled model supporting multiple inference-step settings.

Result: Outperforms state-of-the-art methods in 1-step and 2-step video generation on VBench; achieves high-quality synthesis with extremely limited denoising steps; no need for repeated re-distillation.

Conclusion: ASD and FFE substantially improve training stability and generation quality in extremely few-step video generation, delivering a flexible, efficient distilled model for high-quality video synthesis across multiple inference-step budgets.

Abstract: Recent hybrid video generation models combine autoregressive temporal
dynamics with diffusion-based spatial denoising, but their sequential,
iterative nature leads to error accumulation and long inference times. In this
work, we propose a distillation-based framework for efficient causal video
generation that enables high-quality synthesis with extremely limited denoising
steps. Our approach builds upon the Distribution Matching Distillation (DMD)
framework and proposes a novel Adversarial Self-Distillation (ASD) strategy,
which aligns the outputs of the student model's n-step denoising process with
its (n+1)-step version at the distribution level. This design provides smoother
supervision by bridging small intra-student gaps and more informative guidance
by combining teacher knowledge with locally consistent student behavior,
substantially improving training stability and generation quality in extremely
few-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame
Enhancement (FFE) strategy, which allocates more denoising steps to the initial
frames to mitigate error propagation while applying larger skipping steps to
later frames. Extensive experiments on VBench demonstrate that our method
surpasses state-of-the-art approaches in both one-step and two-step video
generation. Notably, our framework produces a single distilled model that
flexibly supports multiple inference-step settings, eliminating the need for
repeated re-distillation and enabling efficient, high-quality video synthesis.

</details>


### [147] [UniSOT: A Unified Framework for Multi-Modality Single Object Tracking](https://arxiv.org/abs/2511.01427)
*Yinchao Ma,Yuyang Tang,Wenfei Yang,Tianzhu Zhang,Xu Zhou,Feng Wu*

Main category: cs.CV

TL;DR: UniSOT is a unified tracker that supports combinations of three reference modalities (bounding box, natural language, or both) and four video modalities (RGB, RGB+Depth, RGB+Thermal, RGB+Event) using uniform parameters, achieving strong results across 18 benchmarks, including >3% AUC gains on TNL2K and >2% gains on RGB+X tracking.


<details>
  <summary>Details</summary>
Motivation: Existing trackers are specialized to specific modality pairs, requiring multiple models and hindering broad applicability. A single, unified tracker across reference and video modalities would simplify deployment and potentially improve robustness in diverse scenarios.

Method: Developed UniSOT with a single architecture and shared parameters that can handle three reference modalities and four video modalities. Conducted extensive experiments on 18 benchmarks spanning visual tracking, vision-language tracking, and RGB+X tracking to demonstrate cross-modal generalization and superiority over modality-specific baselines.

Result: UniSOT demonstrates superior performance against modality-specific counterparts. It outperforms prior methods by over 3.0% AUC on TNL2K across all three reference modalities and by over 2.0% main metric across all three RGB+X video modalities.

Conclusion: A practical, unified tracking framework that accommodates multiple reference and video modalities with uniform parameters, delivering robust and superior performance across diverse tracking scenarios.

Abstract: Single object tracking aims to localize target object with specific reference
modalities (bounding box, natural language or both) in a sequence of specific
video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different
reference modalities enable various human-machine interactions, and different
video modalities are demanded in complex scenarios to enhance tracking
robustness. Existing trackers are designed for single or several video
modalities with single or several reference modalities, which leads to separate
model designs and limits practical applications. Practically, a unified tracker
is needed to handle various requirements. To the best of our knowledge, there
is still no tracker that can perform tracking with these above reference
modalities across these video modalities simultaneously. Thus, in this paper,
we present a unified tracker, UniSOT, for different combinations of three
reference modalities and four video modalities with uniform parameters.
Extensive experimental results on 18 visual tracking, vision-language tracking
and RGB+X tracking benchmarks demonstrate that UniSOT shows superior
performance against modality-specific counterparts. Notably, UniSOT outperforms
previous counterparts by over 3.0\% AUC on TNL2K across all three reference
modalities and outperforms Un-Track by over 2.0\% main metric across all three
RGB+X video modalities.

</details>


### [148] [Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation](https://arxiv.org/abs/2511.01434)
*Seongkyu Choi,Jhonghyun An*

Main category: cs.CV

TL;DR: A resolution-aware token decoder for off-road semantic segmentation balances global semantics, local boundary fidelity, and supervision noise by processing mostly at low resolution and selectively refining sparse high-resolution pixels; it includes a global self-attention backbone, gated cross-attention for fine-scale details, and a class-aware refinement, plus a boundary-consistency regularizer during training for edge coherence.


<details>
  <summary>Details</summary>
Motivation: Off-road semantic segmentation faces thick, inconsistent boundaries, sparse supervision for rare classes, and noise-laden labels. Many methods either fuse only at low resolution (blurring edges) or rely on high-resolution pathways (costly and fragile to noise).

Method: Propose a resolution-aware token decoder that operates mainly at a low-resolution bottleneck. Use a gated cross-attention to inject fine-scale detail from a high-resolution encoder stream while avoiding noise amplification. Employ a global self-attention with lightweight dilated depthwise refinement to restore local coherence. Include a class-aware point refinement to correct residual ambiguities with negligible overhead. All components are tightly integrated and co-designed. Train with a boundary-band consistency regularizer that enforces coherent predictions in a thin neighborhood around annotated edges, without adding inference cost.

Result: The approach yields competitive performance and improved stability across transitions in off-road segmentation tasks, indicating robustness to boundary imprecision, sparse supervision, and label noise.

Conclusion: A tightly integrated, low-cost, resolution-aware decoder that combines global semantics with local boundary fidelity and selective refinement achieves robust off-road segmentation under imperfect supervision, with no inference-time cost from the boundary regularizer.

Abstract: Off-road semantic segmentation suffers from thick, inconsistent boundaries,
sparse supervision for rare classes, and pervasive label noise. Designs that
fuse only at low resolution blur edges and propagate local errors, whereas
maintaining high-resolution pathways or repeating high-resolution fusions is
costly and fragile to noise. We introduce a resolutionaware token decoder that
balances global semantics, local consistency, and boundary fidelity under
imperfect supervision. Most computation occurs at a low-resolution bottleneck;
a gated cross-attention injects fine-scale detail, and only a sparse,
uncertainty-selected set of pixels is refined. The components are co-designed
and tightly integrated: global self-attention with lightweight dilated
depthwise refinement restores local coherence; a gated cross-attention
integrates fine-scale features from a standard high-resolution encoder stream
without amplifying noise; and a class-aware point refinement corrects residual
ambiguities with negligible overhead. During training, we add a boundary-band
consistency regularizer that encourages coherent predictions in a thin
neighborhood around annotated edges, with no inference-time cost. Overall, the
results indicate competitive performance and improved stability across
transitions.

</details>


### [149] [Contrast-Guided Cross-Modal Distillation for Thermal Object Detection](https://arxiv.org/abs/2511.01435)
*SiWoo Kim,JhongHyun An*

Main category: cs.CV

TL;DR: Training-time objectives tighten intra-class features and align thermal detector features with an RGB teacher, enabling robust mono-modal (TIR) detection at night without extra sensors or test-time RGB input; it achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Thermal infrared detection at night suffers from low contrast and weak high-frequency cues, causing duplicate boxes, missed small objects, and class confusion. Traditional fixes (RGB translation or RGB-T fusion) either degrade due to color/structure artifacts or require extra sensors and calibration.

Method: Introduce training-only objectives: (1) push-pull losses that pull together features of the same class and push apart different classes to sharpen instance-level decision boundaries; (2) cross-modal priors by aligning the student detector's multi-level pyramid features with an RGB-trained teacher, transferring semantic texture information to texture-poor TIR features without using RGB input at test time.

Result: Experiments show the proposed method outperforms prior approaches and achieves state-of-the-art performance in robust night perception using thermal imaging.

Conclusion: Maintaining mono-modality inference while training with cross-modal priors and intra-class feature sharpening yields stronger thermal representations and improved detection under challenging night conditions, without requiring additional sensors at test time.

Abstract: Robust perception at night remains challenging for thermal-infrared
detection: low contrast and weak high-frequency cues lead to duplicate,
overlapping boxes, missed small objects, and class confusion. Prior remedies
either translate TIR to RGB and hope pixel fidelity transfers to detection --
making performance fragile to color or structure artifacts -- or fuse RGB and
TIR at test time, which requires extra sensors, precise calibration, and higher
runtime cost. Both lines can help in favorable conditions, but do not directly
shape the thermal representation used by the detector. We keep mono-modality
inference and tackle the root causes during training. Specifically, we
introduce training-only objectives that sharpen instance-level decision
boundaries by pulling together features of the same class and pushing apart
those of different classes -- suppressing duplicate and confusing detections --
and that inject cross-modal semantic priors by aligning the student's
multi-level pyramid features with an RGB-trained teacher, thereby strengthening
texture-poor thermal features without visible input at test time. In
experiments, our method outperformed prior approaches and achieved
state-of-the-art performance.

</details>


### [150] [Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction](https://arxiv.org/abs/2511.01449)
*Riddhi Jain,Manasi Patwardhan,Aayush Mishra,Parijat Deshpande,Beena Rai*

Main category: cs.CV

TL;DR: MAOML enables small open-source VLMs to reach SOTA in fruit freshness classification under zero-/few-shot, addressing data scarcity and privacy by using meta-learning and ordinal labels, achieving 92.71% accuracy.


<details>
  <summary>Details</summary>
Motivation: Perishable fruit spoilage leads to waste; visual data to predict freshness; data labeling is costly; proprietary VLMs perform well but privacy concerns; open-source models underperform and fine-tuning with limited data is insufficient.

Method: Proposes Model-Agnostic Ordinal Meta-Learning (MAOML) that uses meta-learning to adapt to data-scarce scenarios and exploit label ordinality; trains smaller VLMs possibly with ordinal regression.

Result: State-of-the-art performance in fruit freshness classification in zero-shot and few-shot; 92.71% accuracy averaged across fruits.

Conclusion: MAOML mitigates data sparsity and privacy barriers, enabling strong performance with smaller models; promising for fruit quality prediction tasks.

Abstract: To effectively manage the wastage of perishable fruits, it is crucial to
accurately predict their freshness or shelf life using non-invasive methods
that rely on visual data. In this regard, deep learning techniques can offer a
viable solution. However, obtaining fine-grained fruit freshness labels from
experts is costly, leading to a scarcity of data. Closed proprietary Vision
Language Models (VLMs), such as Gemini, have demonstrated strong performance in
fruit freshness detection task in both zero-shot and few-shot settings.
Nonetheless, food retail organizations are unable to utilize these proprietary
models due to concerns related to data privacy, while existing open-source VLMs
yield sub-optimal performance for the task. Fine-tuning these open-source
models with limited data fails to achieve the performance levels of proprietary
models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning
(MAOML) algorithm, designed to train smaller VLMs. This approach utilizes
meta-learning to address data sparsity and leverages label ordinality, thereby
achieving state-of-the-art performance in the fruit freshness classification
task under both zero-shot and few-shot settings. Our method achieves an
industry-standard accuracy of 92.71%, averaged across all fruits.
  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning,
Ordinal Regression

</details>


### [151] [Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation](https://arxiv.org/abs/2511.01450)
*Jie Du,Xinyu Gong,Qingshan Tan,Wen Li,Yangming Cheng,Weitao Wang,Chenlu Zhan,Suhui Wu,Hao Zhang,Jun Zhang*

Main category: cs.CV

TL;DR: GT-Pair enables automatic preference data for DPO in video generation; Reg-DPO adds SFT regularization; FSDP-based memory optimizations yield ~3x training capacity; experiments on I2V/T2V show superior quality.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of image-centric DPO when scaling to video tasks: costly data, unstable training, and memory constraints; aim for automated preference data and stable, scalable training.

Method: Introduce GT-Pair to construct positive/negative pairs from real vs model-generated videos; integrate SFT loss as regularization in DPO objective; apply FSDP with memory optimization to boost training capacity; conduct extensive I2V/T2V experiments.

Result: Consistent improvements over baselines in video quality; higher training capacity (~3x) with FSDP-based approach; strong results across multiple datasets.

Conclusion: GT-Pair + Reg-DPO with memory-optimized FSDP provides stable, high-quality video generation and scalable training for I2V/T2V tasks.

Abstract: Recent studies have identified Direct Preference Optimization (DPO) as an
efficient and reward-free approach to improving video generation quality.
However, existing methods largely follow image-domain paradigms and are mainly
developed on small-scale models (approximately 2B parameters), limiting their
ability to address the unique challenges of video tasks, such as costly data
construction, unstable training, and heavy memory consumption. To overcome
these limitations, we introduce a GT-Pair that automatically builds
high-quality preference pairs by using real videos as positives and
model-generated videos as negatives, eliminating the need for any external
annotation. We further present Reg-DPO, which incorporates the SFT loss as a
regularization term into the DPO objective to enhance training stability and
generation fidelity. Additionally, by combining the FSDP framework with
multiple memory optimization techniques, our approach achieves nearly three
times higher training capacity than using FSDP alone. Extensive experiments on
both I2V and T2V tasks across multiple datasets demonstrate that our method
consistently outperforms existing approaches, delivering superior video
generation quality.

</details>


### [152] [When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA](https://arxiv.org/abs/2511.01458)
*Dennis Pierantozzi,Luca Carlini,Mauro Orazio Drago,Chiara Lena,Cesare Hassan,Elena De Momi,Danail Stoyanov,Sophia Bano,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: QA-SNNE is a black-box uncertainty estimator for surgical VQA that aligns question semantics with nearest-neighbor semantic entropy to detect uncertainty and reduce hallucinations; it improves AUROC significantly for zero-shot LVLMs and PEFT models, supporting safer decisions.


<details>
  <summary>Details</summary>
Motivation: Safety concerns in surgical VQA: incorrect or ambiguous answers can harm patients. Most work emphasizes accuracy or linguistic quality and ignores safety behaviors like ambiguity awareness, referrals, or second opinions. Uncertainty estimation inspired by Automatic Failure Detection (AFD) is proposed to enable safer decision making.

Method: Introduce QA-SNNE: a question-aligned semantic nearest neighbor entropy estimator that computes semantic entropy by comparing generated answers with nearest neighbors in a medical text embedding space, conditioned on the question. It is a black-box approach and is evaluated on EndoVis18-VQA and PitVQA across five models (including domain-specific PEFTs and zero-shot LVLMs). Performance is assessed via AUROC and hallucination detection, including robustness to paraphrasing and out-of-template shifts.

Result: QA-SNNE improves AUROC in most in-template settings and enhances hallucination detection. Zero-shot models gain 15â38% AUROC, with gains maintained under out-of-template stress. The improvements are observed across three LVLMs and two PEFT baselines.

Conclusion: QA-SNNE provides a practical and interpretable step toward AFD in surgical VQA by linking semantic uncertainty to question context. Using LVLM backbones with question-aligned uncertainty estimation can improve safety and clinician trust. Code and model are available at the authorsâ GitHub repository.

Abstract: Safety and reliability are essential for deploying Visual Question Answering
(VQA) in surgery, where incorrect or ambiguous responses can harm the patient.
Most surgical VQA research focuses on accuracy or linguistic quality while
overlooking safety behaviors such as ambiguity awareness, referral to human
experts, or triggering a second opinion. Inspired by Automatic Failure
Detection (AFD), we study uncertainty estimation as a key enabler of safer
decision making. We introduce Question Aligned Semantic Nearest Neighbor
Entropy (QA-SNNE), a black box uncertainty estimator that incorporates question
semantics into prediction confidence. It measures semantic entropy by comparing
generated answers with nearest neighbors in a medical text embedding space,
conditioned on the question. We evaluate five models, including domain specific
Parameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large
Vision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models
degrade under mild paraphrasing, while LVLMs are more resilient. Across three
LVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template
settings and enhances hallucination detection. The Area Under the ROC Curve
(AUROC) increases by 15-38% for zero-shot models, with gains maintained under
out-of-template stress. QA-SNNE offers a practical and interpretable step
toward AFD in surgical VQA by linking semantic uncertainty to question context.
Combining LVLM backbones with question aligned uncertainty estimation can
improve safety and clinician trust. The code and model are available at
https://github.com/DennisPierantozzi/QASNNE

</details>


### [153] [Efficiently Training A Flat Neural Network Before It has been Quantizated](https://arxiv.org/abs/2511.01462)
*Peng Xia,Junbiao Pang,Tianyang Cai*

Main category: cs.CV

TL;DR: Post-training quantization for vision transformers can be improved by training toward a flat minimum using a model-agnostic Gaussian-noise framework that disentangles activation and weight quantization errors, enabling more accurate low-bit PTQ.


<details>
  <summary>Details</summary>
Motivation: Current PTQ methods often ignore the relationship between a well-trained full-precision NN and its quantized version, leading to large quantization errors. A model-agnostic framework tailored to a predefined low-bit precision could systematically reduce this gap by preparing the network for quantization.

Method: Propose a preconditioning framework that measures and disentangles error sources. Model Activation Quantization Error (AQE) and Weight Quantization Error (WQE) as independent Gaussian noises. Investigate noise injection strategies to drive the model toward a flat minimum suitable for low-bit quantization.

Result: Experimental results demonstrate the approach's effectiveness and suggest new directions for achieving low-bit PTQ models.

Conclusion: A flat full-precision network is crucial for successful low-bit quantization. Modeling AQE and WQE as independent Gaussian noises with noise-injection optimization provides a promising, model-agnostic path to improved PTQ for vision transformers.

Abstract: Post-training quantization (PTQ) for vision transformers (ViTs) has garnered
significant attention due to its efficiency in compressing models. However,
existing methods typically overlook the relationship between a well-trained NN
and the quantized model, leading to considerable quantization error for PTQ.
However, it is unclear how to efficiently train a model-agnostic neural network
which is tailored for a predefined precision low-bit model. In this paper, we
firstly discover that a flat full precision neural network is crucial for
low-bit quantization. To achieve this, we propose a framework that proactively
pre-conditions the model by measuring and disentangling the error sources.
Specifically, both the Activation Quantization Error (AQE) and the Weight
Quantization Error (WQE) are statistically modeled as independent Gaussian
noises. We study several noise injection optimization methods to obtain a flat
minimum. Experimental results attest to the effectiveness of our approach.
These results open novel pathways for obtaining low-bit PTQ models.

</details>


### [154] [HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA](https://arxiv.org/abs/2511.01463)
*Lei Hu,Yongjing Ye,Shihong Xia*

Main category: cs.CV

TL;DR: Proposes Human Motion-Vision-Language Model (HMVLM) using Mixture-of-Expert LoRA (MoE LoRA) to enable prompt-conditioned, multi-task fine-tuning across motion, vision, and language modalities. Introduces a zero expert to preserve pre-trained linguistic abilities, and body-part-specific tokenization for better pose representation. Empirically, the framework mitigates catastrophic forgetting during instruction-tuning and delivers strong performance on diverse human motion tasks.


<details>
  <summary>Details</summary>
Motivation: To close the modality gap between semantically rich 3D human motion and text, address catastrophic forgetting during instruction-tuning in multimodal models, and develop autoregressive-compatible pose representations that generalize across heterogeneous downstream tasks.

Method: A unified framework (HMVLM) built on Mixture-of-Expert LoRA with a gating network that dynamically allocates LoRA expert weights according to the input prompt. Includes a zero expert to preserve pre-trained parameters for general linguistic tasks, and body-part-specific tokenization by partitioning the body into joint groups to enhance pose representation. Enables synchronized fine-tuning across multiple tasks.

Result: Experiments indicate reduced forgetting during instruction-tuning and strong performance across diverse human motion downstream tasks.

Conclusion: The proposed MoE LoRA-based HMVLM effectively mitigates knowledge forgetting while delivering high performance in multimodal human motion understanding/generation, supported by a pose-representation scheme that improves spatial granularity and task adaptability.

Abstract: The expansion of instruction-tuning data has enabled foundation language
models to exhibit improved instruction adherence and superior performance
across diverse downstream tasks. Semantically-rich 3D human motion is being
progressively integrated with these foundation models to enhance multimodal
understanding and cross-modal generation capabilities. However, the modality
gap between human motion and text raises unresolved concerns about catastrophic
forgetting during this integration. In addition, developing
autoregressive-compatible pose representations that preserve generalizability
across heterogeneous downstream tasks remains a critical technical barrier. To
address these issues, we propose the Human Motion-Vision-Language Model
(HMVLM), a unified framework based on the Mixture of Expert Low-Rank
Adaption(MoE LoRA) strategy. The framework leverages the gating network to
dynamically allocate LoRA expert weights based on the input prompt, enabling
synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting
during instruction-tuning, we introduce a novel zero expert that preserves the
pre-trained parameters for general linguistic tasks. For pose representation,
we implement body-part-specific tokenization by partitioning the human body
into different joint groups, enhancing the spatial resolution of the
representation. Experiments show that our method effectively alleviates
knowledge forgetting during instruction-tuning and achieves remarkable
performance across diverse human motion downstream tasks.

</details>


### [155] [SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks](https://arxiv.org/abs/2511.01466)
*Changyuan Zhao,Jiacheng Wang,Ruichen Zhang,Dusit Niyato,Hongyang Du,Zehui Xiong,Dong In Kim,Ping Zhang*

Main category: cs.CV

TL;DR: SecDiff is a diffusion-guided deep JSCC framework that boosts defense against pilot spoofing and subcarrier jamming in OFDM by combining pseudoinverse-guided sampling with adaptive guidance, subcarrier masking (inpainting) for jamming, and EM-driven joint pilot/channel estimation; it achieves better reconstruction quality with lower latency than prior diffusion-based JSCC methods, offering attack-resilient, practical semantic communications.


<details>
  <summary>Details</summary>
Motivation: Deep JSCC is vulnerable to physical-layer adversaries (pilot spoofing, subcarrier jamming) which can degrade semantic fidelity; there is a need for robust, low-latency defenses compatible with practical wireless systems.

Method: Introduce SecDiff: diffusion-aided decoding with pseudoinverse-guided sampling and adaptive guidance; power-based subcarrier masking to treat jamming as masked inpainting; EM-driven reconstruction for blind channel estimation (pilot spoofing), with alternating refinement of pilots and channel within the diffusion process.

Result: Extensive OFDM experiments under adversarial conditions show SecDiff outperforms secure and generative JSCC baselines in reconstruction quality for a given computational cost.

Conclusion: SecDiff is a plug-and-play, low-latency, attack-resilient framework that advances practical semantic communications under adversarial wireless environments.

Abstract: Deep joint source-channel coding (JSCC) has emerged as a promising paradigm
for semantic communication, delivering significant performance gains over
conventional separate coding schemes. However, existing JSCC frameworks remain
vulnerable to physical-layer adversarial threats, such as pilot spoofing and
subcarrier jamming, compromising semantic fidelity. In this paper, we propose
SecDiff, a plug-and-play, diffusion-aided decoding framework that significantly
enhances the security and robustness of deep JSCC under adversarial wireless
environments. Different from prior diffusion-guided JSCC methods that suffer
from high inference latency, SecDiff employs pseudoinverse-guided sampling and
adaptive guidance weighting, enabling flexible step-size control and efficient
semantic reconstruction. To counter jamming attacks, we introduce a power-based
subcarrier masking strategy and recast recovery as a masked inpainting problem,
solved via diffusion guidance. For pilot spoofing, we formulate channel
estimation as a blind inverse problem and develop an expectation-minimization
(EM)-driven reconstruction algorithm, guided jointly by reconstruction loss and
a channel operator. Notably, our method alternates between pilot recovery and
channel estimation, enabling joint refinement of both variables throughout the
diffusion process. Extensive experiments over orthogonal frequency-division
multiplexing (OFDM) channels under adversarial conditions show that SecDiff
outperforms existing secure and generative JSCC baselines by achieving a
favorable trade-off between reconstruction quality and computational cost. This
balance makes SecDiff a promising step toward practical, low-latency, and
attack-resilient semantic communications.

</details>


### [156] [EPAN: Robust Pedestrian Re-Identification via Enhanced Alignment Network for IoT Surveillance](https://arxiv.org/abs/2511.01498)
*Zhiyang Jia,Hongyan Cui,Ge Gao,Bo Li,Minjie Zhang,Zishuo Gao,Huiwen Huang,Caisheng Zhuo*

Main category: cs.CV

TL;DR: Proposes Enhanced Pedestrian Alignment Network (EPAN) for robust cross-camera person re-identification in IoT surveillance; a dual-branch design extracts alignment cues across multiple scales and viewpoints to counter perspective and environmental changes; achieves 90.09% Rank-1 and 78.82% mAP on the Inspection-Personnel dataset; code available on GitHub.


<details>
  <summary>Details</summary>
Motivation: IoT-enabled surveillance systems suffer from strong cross-camera variability due toä¸å perspectives, scales, and environmental conditions. There is a need for robust feature extraction and alignment that remains effective across diverse cameras.

Method: EPAN employs a dual-branch architecture to capture and fuse alignment information across varying scales and viewpoints, enhancing pedestrian feature robustness for re-identification.

Result: On the Inspection-Personnel dataset, EPAN achieves Rank-1 of 90.09% and mAP of 78.82%, indicating strong feature extraction and alignment capabilities suitable for real-world IoT surveillance deployments.

Conclusion: EPAN demonstrates strong potential for reliable cross-camera person ReID in diverse surveillance settings; the provided code and data enable replication and practical adoption in IoT security systems.

Abstract: Person re-identification (ReID) plays a pivotal role in computer vision,
particularly in surveillance and security applications within IoT-enabled smart
environments. This study introduces the Enhanced Pedestrian Alignment Network
(EPAN), tailored for robust ReID across diverse IoT surveillance conditions.
EPAN employs a dual-branch architecture to mitigate the impact of perspective
and environmental changes, extracting alignment information under varying
scales and viewpoints. Here, we demonstrate EPAN's strong feature extraction
capabilities, achieving outstanding performance on the Inspection-Personnel
dataset with a Rank-1 accuracy of 90.09% and a mean Average Precision (mAP) of
78.82%. This highlights EPAN's potential for real-world IoT applications,
enabling effective and reliable person ReID across diverse cameras in
surveillance and security systems. The code and data are available at:
https://github.com/ggboy2580/EPAN

</details>


### [157] [Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement](https://arxiv.org/abs/2511.01510)
*Derong Kong,Zhixiong Yang,Shengxi Li,Shuaifeng Zhi,Li Liu,Zhen Liu,Jingyuan Xia*

Main category: cs.CV

TL;DR: Recasts LLIE as probabilistic luminance sampling via a diffusion-based process, enabling unsupervised distribution emulation and better cross-domain generalization without strictly paired references.


<details>
  <summary>Details</summary>
Motivation: Deterministic pixel mappings and dependency on paired data limit generalization; luminance dynamics in real scenes follow power-law transitions, which are not captured by existing methods; need a framework that models continuous luminance transitions and handles missing normal-light references.

Method: LASQ models luminance as hierarchical distributions with power-law in intensity space, uses stratified power functions; replaces deterministic mappings with probabilistic sampling across luminance layers; introduces a diffusion forward process to discover optimal transition paths between layers, enabling unsupervised emulation; applicable with and without normal-light references.

Result: Expected improved reconstruction quality and generalization on domain-specific datasets; superior performance when normal-light references are available and better zero-shot generalization to non-reference datasets; more adaptable light restoration in practical settings.

Conclusion: LASQ offers a principled probabilistic framework for LLIE that aligns with natural luminance dynamics, enhancing robustness across domains and reference availability by leveraging diffusion-based distribution learning rather than fixed mappings.

Abstract: Low-light image enhancement (LLIE) faces persistent challenges in balancing
reconstruction fidelity with cross-scenario generalization. While existing
methods predominantly focus on deterministic pixel-level mappings between
paired low/normal-light images, they often neglect the continuous physical
process of luminance transitions in real-world environments, leading to
performance drop when normal-light references are unavailable. Inspired by
empirical analysis of natural luminance dynamics revealing power-law
distributed intensity transitions, this paper introduces Luminance-Aware
Statistical Quantification (LASQ), a novel framework that reformulates LLIE as
a statistical sampling process over hierarchical luminance distributions. Our
LASQ re-conceptualizes luminance transition as a power-law distribution in
intensity coordinate space that can be approximated by stratified power
functions, therefore, replacing deterministic mappings with probabilistic
sampling over continuous luminance layers. A diffusion forward process is
designed to autonomously discover optimal transition paths between luminance
layers, achieving unsupervised distribution emulation without normal-light
references. In this way, it considerably improves the performance in practical
situations, enabling more adaptable and versatile light restoration. This
framework is also readily applicable to cases with normal-light references,
where it achieves superior performance on domain-specific datasets alongside
better generalization-ability across non-reference datasets.

</details>


### [158] [Example-Based Feature Painting on Textures](https://arxiv.org/abs/2511.01513)
*Andrei-Timotei Ardelean,Tim Weyrich*

Main category: cs.CV

TL;DR: A learning-based, annotation-free system for controlled texture blemish editing and synthesis using unsupervised anomaly detection to discover and cluster appearance-altering features, enabling diffusion-based editing and infinite texture generation from a small image collection.


<details>
  <summary>Details</summary>
Motivation: Real-world textures often include blemishes (stains, tears, holes, abrasions, discoloration) that are essential for realism. Manual annotation is costly; the work aims to automatically discover and organize blemish-like features to drive conditional generation.

Method: Use unsupervised anomaly detection to identify appearance-altering features in texture images, cluster these features into semantically coherent groups, and use these clusters to guide diffusion-based editing and conditional generation. The pipeline converts a small image collection into a versatile generative model that supports interactive painting on textures of arbitrary size, with novel diffusion-based editing and infinite stationary texture generation algorithms.

Result: A practical pipeline that automatically detects and clusters blemish-like features, enabling interactive texture editing and infinite texture generation. The diffusion-based editing and infinite texture algorithms are presented as generic, applicable beyond this task, and the project page provides access to the implementation.

Conclusion: The approach delivers an annotation-free, flexible tool for realistic texture synthesis and editing of blemishes, with diffusion-based editing and infinite texture generation algorithms that are broadly applicable to other domains.

Abstract: In this work, we propose a system that covers the complete workflow for
achieving controlled authoring and editing of textures that present distinctive
local characteristics. These include various effects that change the surface
appearance of materials, such as stains, tears, holes, abrasions,
discoloration, and more. Such alterations are ubiquitous in nature, and
including them in the synthesis process is crucial for generating realistic
textures. We introduce a novel approach for creating textures with such
blemishes, adopting a learning-based approach that leverages unlabeled
examples. Our approach does not require manual annotations by the user;
instead, it detects the appearance-altering features through unsupervised
anomaly detection. The various textural features are then automatically
clustered into semantically coherent groups, which are used to guide the
conditional generation of images. Our pipeline as a whole goes from a small
image collection to a versatile generative model that enables the user to
interactively create and paint features on textures of arbitrary size. Notably,
the algorithms we introduce for diffusion-based editing and infinite stationary
texture generation are generic and should prove useful in other contexts as
well. Project page: https://reality.tf.fau.de/pub/ardelean2025examplebased.html

</details>


### [159] [NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation](https://arxiv.org/abs/2511.01517)
*Serkan Ozturk,Samet Hicsonmez,Pinar Duygulu*

Main category: cs.CV

TL;DR: NSYNC presents a contrastive training framework for stylizing text-to-image diffusion models by using negative synthetic data to orthogonalize the gradient against positive data, improving style capture over baselines on painter/illustrator styles.


<details>
  <summary>Details</summary>
Motivation: Current text-conditioned image generators produce realistic images but struggle to capture specific artistic styles; fine-tuning on style data is insufficient. Leveraging synthetic data in a contrastive setup, especially negative samples, can isolate non-trivial style features and improve stylization.

Method: Generate negative synthetic data and compute both positive and negative gradients during training. Refine the positive gradient by subtracting its projection onto the negative gradient, producing an orthogonal component that emphasizes unique style attributes. Update model parameters using this orthogonal component. This NSYNC approach uses a contrastive signal with (positive, negative) pairs to enhance style capture during training of large text-to-image diffusion models.

Result: Experiments across various painter and illustrator styles show quantitative and qualitative improvements over baseline methods, indicating better stylization performance. The authors also release their code at the provided GitHub link.

Conclusion: NSYNC introduces a novel, gradient-orthogonalized contrastive training scheme that leverages negative synthetic data to sharpen style-specific features in diffusion models, offering a practical route to improved stylization in text-to-image generation.

Abstract: Current text conditioned image generation methods output realistic looking
images, but they fail to capture specific styles. Simply finetuning them on the
target style datasets still struggles to grasp the style features. In this
work, we present a novel contrastive learning framework to improve the
stylization capability of large text-to-image diffusion models. Motivated by
the astonishing advance in image generation models that makes synthetic data an
intrinsic part of model training in various computer vision tasks, we exploit
synthetic image generation in our approach. Usually, the generated synthetic
data is dependent on the task, and most of the time it is used to enlarge the
available real training dataset. With NSYNC, alternatively, we focus on
generating negative synthetic sets to be used in a novel contrastive training
scheme along with real positive images. In our proposed training setup, we
forward negative data along with positive data and obtain negative and positive
gradients, respectively. We then refine the positive gradient by subtracting
its projection onto the negative gradient to get the orthogonal component,
based on which the parameters are updated. This orthogonal component eliminates
the trivial attributes that are present in both positive and negative data and
directs the model towards capturing a more unique style. Experiments on various
styles of painters and illustrators show that our approach improves the
performance over the baseline methods both quantitatively and qualitatively.
Our code is available at https://github.com/giddyyupp/NSYNC.

</details>


### [160] [Driving scenario generation and evaluation using a structured layer representation and foundational models](https://arxiv.org/abs/2511.01541)
*Arthur Hubert,Gamal Elghazaly,RaphaÃ«l Frank*

Main category: cs.CV

TL;DR: Introduces a five-layer structured model (5L) for driving scenarios to improve generation and evaluation of rare scenarios; combines agent-level attributes with embeddings and two metrics (diversity and originality) for synthetic data quality; demonstrates via qualitative video outputs and code release.


<details>
  <summary>Details</summary>
Motivation: Rare driving scenarios are hard to encounter and mimic; existing layer-based representations are limited. A richer, multi-layer structure with agent-specific attributes enables better generation, comparison, and evaluation of synthetic data for autonomous driving.

Method: Proposes a five-layer structure with subclasses and characteristics for every agent; uses large foundational models to generate synthetic scenarios; adapts diversity and originality metrics to the structured representation; evaluates via synthetic datasets and qualitative videos; provides code at GitHub.

Result: Shows the behavior of the two metrics across generation setups; provides qualitative evaluation of synthetic driving videos; releases code and extended results.

Conclusion: A structured five-layer model with agent-level embeddings improves the evaluation and generation of rare driving scenarios and supports more effective data augmentation for autonomous driving research.

Abstract: Rare and challenging driving scenarios are critical for autonomous vehicle
development. Since they are difficult to encounter, simulating or generating
them using generative models is a popular approach. Following previous efforts
to structure driving scenario representations in a layer model, we propose a
structured five-layer model to improve the evaluation and generation of rare
scenarios. We use this model alongside large foundational models to generate
new driving scenarios using a data augmentation strategy. Unlike previous
representations, our structure introduces subclasses and characteristics for
every agent of the scenario, allowing us to compare them using an embedding
specific to our layer-model. We study and adapt two metrics to evaluate the
relevance of a synthetic dataset in the context of a structured representation:
the diversity score estimates how different the scenarios of a dataset are from
one another, while the originality score calculates how similar a synthetic
dataset is from a real reference set. This paper showcases both metrics in
different generation setup, as well as a qualitative evaluation of synthetic
videos generated from structured scenario descriptions. The code and extended
results can be found at https://github.com/Valgiz/5LMSG.

</details>


### [161] [PCD-ReID: Occluded Person Re-Identification for Base Station Inspection](https://arxiv.org/abs/2511.01546)
*Ge Gao,Zishuo Gao,Hongyan Cui,Zhiyang Jia,Zhuang Luo,ChaoPeng Liu*

Main category: cs.CV

TL;DR: Transformer-based PCD-ReID uses Pedestrian Component Discrepancy to tackle occluded ReID; introduces real-world patrol dataset; demonstrates strong occlusion-aware performance with notable improvements over ResNet50-based methods.


<details>
  <summary>Details</summary>
Motivation: Occlusion severely hinders pedestrian re-identification, and traditional ResNet-based methods struggle to capture discriminative cues under occlusion. Public ReID datasets may not reflect real-world patrol surveillance conditions, risking overfitting.

Method: Introduce a Transformer-based PCD (Pedestrian Component Discrepancy) network that extracts shared component features (e.g., helmets, uniforms) and leverages component discrepancy to handle occlusions. To reduce overfitting, the authors collected a real-world patrol surveillance dataset (six months, 10,000 individuals, >50,000 images) for training. The model aims to be occlusion-aware and robust for tower inspection surveillance scenarios.

Result: On the proposed dataset and benchmark comparisons, PCD-ReID achieved a mean Average Precision (mAP) of 79.0% and Rank-1 accuracy of 82.7%, representing a 15.9 percentage-point improvement in Rank-1 over ResNet50-based methods.

Conclusion: PCD-ReID demonstrates effective occlusion-aware ReID suitable for surveillance deployment, with real-world dataset support enhancing generalization and practical applicability in security contexts.

Abstract: Occluded pedestrian re-identification (ReID) in base station environments is
a critical task in computer vision, particularly for surveillance and security
applications. This task faces numerous challenges, as occlusions often obscure
key body features, increasing the complexity of identification. Traditional
ResNet-based ReID algorithms often fail to address occlusions effectively,
necessitating new ReID methods. We propose the PCD-ReID (Pedestrian Component
Discrepancy) algorithm to address these issues. The contributions of this work
are as follows: To tackle the occlusion problem, we design a Transformer-based
PCD network capable of extracting shared component features, such as helmets
and uniforms. To mitigate overfitting on public datasets, we collected new
real-world patrol surveillance images for model training, covering six months,
10,000 individuals, and over 50,000 images. Comparative experiments with
existing ReID algorithms demonstrate that our model achieves a mean Average
Precision (mAP) of 79.0% and a Rank-1 accuracy of 82.7%, marking a 15.9% Rank-1
improvement over ResNet50-based methods. Experimental evaluations indicate that
PCD-ReID effectively achieves occlusion-aware ReID performance for personnel in
tower inspection scenarios, highlighting its potential for practical deployment
in surveillance and security applications.

</details>


### [162] [NOA: a versatile, extensible tool for AI-based organoid analysis](https://arxiv.org/abs/2511.01549)
*Mikhail Konov,Lion J. Gleiter,Khoa Co,Monica Yabal,Tingying Peng*

Main category: cs.CV

TL;DR: Napari Organoid Analyzer (NOA) is an open-source GUI plugin that unifies AI-based organoid image analysis (detection, segmentation, tracking, feature extraction, ML prediction) into a flexible, accessible workflow, demonstrated across three case studies.


<details>
  <summary>Details</summary>
Motivation: Biologists face barriers using AI due to lack of programming expertise; existing tools are task-specific; need a general, accessible, extensible solution for organoid image analysis.

Method: Develop NOA as a Napari plugin with modules for detection, segmentation, tracking, feature extraction, custom feature annotation, and ML-based feature prediction; interfaces multiple state-of-the-art algorithms; implemented as open-source; supports extensibility.

Result: Shows versatility via three case studies: morphological changes during differentiation, phototoxicity effects, and predicting viability/differentiation state; demonstrates comprehensive, AI-driven analysis within accessible framework.

Conclusion: NOA lowers barriers to AI-enabled organoid analysis, providing a general, extensible platform that combines multiple AI tools in a user-friendly GUI.

Abstract: AI tools can greatly enhance the analysis of organoid microscopy images, from
detection and segmentation to feature extraction and classification. However,
their limited accessibility to biologists without programming experience
remains a major barrier, resulting in labor-intensive and largely manual
workflows. Although a few AI models for organoid analysis have been developed,
most existing tools remain narrowly focused on specific tasks. In this work, we
introduce the Napari Organoid Analyzer (NOA), a general purpose graphical user
interface to simplify AI-based organoid analysis. NOA integrates modules for
detection, segmentation, tracking, feature extraction, custom feature
annotation and ML-based feature prediction. It interfaces multiple
state-of-the-art algorithms and is implemented as an open-source napari plugin
for maximal flexibility and extensibility. We demonstrate the versatility of
NOA through three case studies, involving the quantification of morphological
changes during organoid differentiation, assessment of phototoxicity effects,
and prediction of organoid viability and differentiation state. Together, these
examples illustrate how NOA enables comprehensive, AI-driven organoid image
analysis within an accessible and extensible framework.

</details>


### [163] [Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images](https://arxiv.org/abs/2511.01574)
*Md Sumon Ali,Muzammil Behzad*

Main category: cs.CV

TL;DR: DC-GAN generated MRI data used to train a CNN classifier; synthetic data yields comparable brain tumor classification performance to real data, supporting GAN-based data augmentation for medical imaging.


<details>
  <summary>Details</summary>
Motivation: MRI datasets are limited; synthetic data can augment training and enable effective DL for tumor classification.

Method: Use Deep Convolutional GAN to generate synthetic MRI images; train/evaluate a CNN classifier on synthetic and real MRI data to assess quality and utility.

Result: CNN classification performance on synthetic images is comparable to that on real images, indicating GAN-generated MRI data are usable for downstream tasks.

Conclusion: GAN-generated synthetic MRI data can alleviate data scarcity and support medical image classification without substantial loss in accuracy.

Abstract: Compared to traditional methods, Deep Learning (DL) becomes a key technology
for computer vision tasks. Synthetic data generation is an interesting use case
for DL, especially in the field of medical imaging such as Magnetic Resonance
Imaging (MRI). The need for this task since the original MRI data is limited.
The generation of realistic medical images is completely difficult and
challenging. Generative Adversarial Networks (GANs) are useful for creating
synthetic medical images. In this paper, we propose a DL based methodology for
creating synthetic MRI data using the Deep Convolutional Generative Adversarial
Network (DC-GAN) to address the problem of limited data. We also employ a
Convolutional Neural Network (CNN) classifier to classify the brain tumor using
synthetic data and real MRI data. CNN is used to evaluate the quality and
utility of the synthetic images. The classification result demonstrates
comparable performance on real and synthetic images, which validates the
effectiveness of GAN-generated images for downstream tasks.

</details>


### [164] [Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation](https://arxiv.org/abs/2511.01593)
*Yizhu Chen,Chen Ju,Zhicheng Wang,Shuai Xiao,Xu Chen,Jinsong Lan,Xiaoyong Zhu,Ying Chen*

Main category: cs.CV

TL;DR: Presents CDD-VT, a hybrid continuous-discrete visual tokenizer that adaptively assigns the number of primitives per image sample to balance information preservation and efficiency, surpassing purely continuous or discrete tokenizers in multiple tasks.


<details>
  <summary>Details</summary>
Motivation: To resolve the trade-off between continuous tokenizers (strong performance but complex pipelines) and discrete tokenizers (simplicity but information loss) by proposing a wave-particle inspired dualistic tokenizer for unified multi-modal modeling.

Method: Introduce two components: Diverse Quantitative Primitives to enforce orthogonality among tokens, and Dynamic Primitive Allocator to estimate sample complexity and select the optimal primitive count from a quantized codebook; evaluate on reconstruction, retrieval, and classification.

Result: CDD-VT outperforms specialized CT and DT baselines on reconstruction, retrieval, and classification tasks, demonstrating strong performance with a concise, scalable MLLM.

Conclusion: Adaptive hybrid tokenization can unify understanding and generation in MLLMs, offering improved performance and reduced engineering complexity; adaptive token counts per sample effectively balance fidelity and efficiency.

Abstract: The unification of understanding and generation within a single multi-modal
large model (MLLM) remains one significant challenge, largely due to the
dichotomy between continuous and discrete visual tokenizations. Continuous
tokenizer (CT) achieves strong performance by bridging multiple
independently-trained understanding modules and generation modules, but suffers
from complex multi-stage pipelines and substantial engineering overhead.
Conversely, discrete tokenizers (DT) offer a conceptually elegant idea by
quantizing each image into a primitive, but inevitably leading to information
loss and performance degradation. To resolve this tension, we question the
binary choice between CT and DT, inspired by the wave-particle duality of
light, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT).
We treat visual data as a flexible composition of image primitives derived from
quantized codebooks, with the crucial insight that the primitive number
assigned to each visual sample is adaptively determined according to its
complexity: simple instances use a few primitives, emulating discrete
tokenization, while complex instances use many, approximating continuous
tokenization. Two core components are designed: Diverse Quantitative
Primitives, which encourage primitives orthogonality to better populate
information space, and Dynamic Primitive Allocator, which assesses sample
complexity to determine the optimal set of primitives. Extensive experiments on
reconstruction, retrieval and classification show that CDD-VT achieves superior
performance over to specialized CT and DT, effectively getting strong result
within a concise and scalable MLLM.

</details>


### [165] [Lite ENSAM: a lightweight cancer segmentation model for 3D Computed Tomography](https://arxiv.org/abs/2511.01600)
*Agnar Martin BjÃ¸rnstad,Elias Stenhede,Arian Ranjbar*

Main category: cs.CV

TL;DR: A lightweight adaptation (Lite ENSAM) of the ENSAM architecture for fast volumetric tumor segmentation from RECIST-annotated CTs; achieved DSC 60.7%, NSD 63.6% on hidden test in FLARE 2025 Subtask 2, with ~50.6 GB RAM and 14.4 s CPU inference on public validation.


<details>
  <summary>Details</summary>
Motivation: Enable volumetric tumor assessment without heavy manual labeling by delivering an efficient automated segmentation method that leverages RECIST annotations.

Method: Lite ENSAM, a lightweight adaptation of ENSAM, for efficient volumetric tumor segmentation from CT scans annotated with RECIST. Evaluated on MICCAI FLARE 2025 Task 1: Subtask 2; CPU-based inference reported on public validation dataset; metrics computed on hidden test set.

Result: DSC 60.7% and NSD 63.6% on the hidden test set. Average total RAM time 50.6 GBs and average CPU inference time 14.4 seconds on the public validation dataset.

Conclusion: A lightweight model approach (Lite ENSAM) can deliver competitive volumetric segmentation performance with practical CPU-based efficiency, supporting more reliable and scalable RECIST-informed volumetric assessment in clinical workflows, though current metrics indicate room for improvement to reach higher clinical acceptance.

Abstract: Accurate tumor size measurement is a cornerstone of evaluating cancer
treatment response. The most widely adopted standard for this purpose is the
Response Evaluation Criteria in Solid Tumors (RECIST) v1.1, which relies on
measuring the longest tumor diameter in a single plane. However, volumetric
measurements have been shown to provide a more reliable assessment of treatment
effect. Their clinical adoption has been limited, though, due to the
labor-intensive nature of manual volumetric annotation. In this paper, we
present Lite ENSAM, a lightweight adaptation of the ENSAM architecture designed
for efficient volumetric tumor segmentation from CT scans annotated with RECIST
annotations. Lite ENSAM was submitted to the MICCAI FLARE 2025 Task 1:
Pan-cancer Segmentation in CT Scans, Subtask 2, where it achieved a Dice
Similarity Coefficient (DSC) of 60.7% and a Normalized Surface Dice (NSD) of
63.6% on the hidden test set, and an average total RAM time of 50.6 GBs and an
average inference time of 14.4 s on CPU on the public validation dataset.

</details>


### [166] [DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning](https://arxiv.org/abs/2511.01610)
*Mahmut Selman Gokmen,Cody Bumgardner*

Main category: cs.CV

TL;DR: A modular, configurable training framework DINO-MX unifies DINO family methods for vision transformers, enabling flexible training across domains with reduced compute, interpretability tools, and label-guided augmentation.


<details>
  <summary>Details</summary>
Motivation: Existing vision foundation model pipelines are inflexible, domain-specific, or compute-intensive; there is a need for a configurable, scalable framework compatible with Hugging Face for diverse data and architectures.

Method: DINO-MX unifies DINO, DINOv2, DINOv3 principles in a config-driven system; supports transformer architectures; integration with Hugging Face; uses strategies like LoRA, layer freezing, knowledge distillation; distributed training via DDP and FSDP; supports natural and specialized data (single/multi-channel images); includes interpretability tools and label-guided augmentation.

Result: Experimental results show competitive performance at significantly reduced computational costs; provides reproducible, scalable foundation; works across research and real-world applications.

Conclusion: DINO-MX offers a flexible, extensible platform for developing, adapting, and benchmarking self-supervised vision models across diverse domains and resource settings.

Abstract: Vision Foundation Models (VFMs) have advanced representation learning through
self-supervised methods. However, existing training pipelines are often
inflexible, domain-specific, or computationally expensive, which limits their
usability across different domains and resource settings. DINO-MX is a modular
and extensible training framework that combines the core principles of DINO,
DINOv2 and DINOv3 within a unified configuration-driven system. It supports a
variety of transformer-based architectures and is fully compatible with the
Hugging Face ecosystem. The framework includes multiple training strategies
such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation,
along with support for distributed training through both Distributed Data
Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to
work with both natural and specialized data types, including single- and
multi-channel images. Experimental results on diverse datasets show that
DINO-MX achieves competitive performance while significantly reducing
computational costs. Additionally, it offers interpretability tools and a
label-guided data augmentation method that improves attention-based
localization without the need for extra detection or segmentation heads.
DINO-MX provides a reproducible and scalable foundation for developing,
adapting, and benchmarking self-supervised vision models across a range of
research and real-world applications.

</details>


### [167] [Benchmark-Ready 3D Anatomical Shape Classification](https://arxiv.org/abs/2511.01613)
*TomÃ¡Å¡ KrsiÄka,Tibor KubÃ­k*

Main category: cs.CV

TL;DR: PSPooling is a non-learnable, precomputed pooling operator for structure-preserving mesh coarsening in 3D anatomical shape analysis, integrated into a self-supervised graph autoencoder. On MedShapeNet19, it improves reconstruction fidelity and low-label classification, establishing a strong benchmark with released code and data.


<details>
  <summary>Details</summary>
Motivation: Anatomical 3D shape classification faces mesh complexity and a lack of standardized benchmarks. There is a need for robust, efficient pooling that preserves structure and supports reproducible evaluation, enabling effective self-supervised learning on high-resolution medical meshes.

Method: Introduce Precomputed Structural Pooling (PSPooling): a non-learnable pooling operator that precomputes node correspondences based on geometric proximity, enabling parallelizable and reversible pooling/unpooling with guaranteed support structure. Integrate PSPooling into a self-supervised graph autoencoder trained on unlabeled surface meshes. Evaluate on MedShapeNet19 (19 anatomical classes with standardized splits) and provide code, weights, and dataset info.

Result: PSPooling significantly improves reconstruction fidelity and downstream classification accuracy in low-label regimes, establishing a strong baseline for medical 3D shape learning.

Conclusion: PSPooling is effective for medical 3D shape learning, offering efficient, structure-preserving pooling suitable for high-resolution meshes. MedShapeNet19 serves as a benchmark to advance reproducible research, with the authors providing open-source code and data.

Abstract: Progress in anatomical 3D shape classification is limited by the complexity
of mesh data and the lack of standardized benchmarks, highlighting the need for
robust learning methods and reproducible evaluation. We introduce two key steps
toward clinically and benchmark-ready anatomical shape classification via
self-supervised graph autoencoding. We propose Precomputed Structural Pooling
(PSPooling), a non-learnable mesh pooling operator designed for efficient and
structure-preserving graph coarsening in 3D anatomical shape analysis.
PSPooling precomputes node correspondence sets based on geometric proximity,
enabling parallelizable and reversible pooling and unpooling operations with
guaranteed support structure. This design avoids the sparsity and
reconstruction issues of selection-based methods and the sequential overhead of
edge contraction approaches, making it particularly suitable for
high-resolution medical meshes. To demonstrate its effectiveness, we integrate
PSPooling into a self-supervised graph autoencoder that learns anatomy-aware
representations from unlabeled surface meshes. We evaluate the downstream
benefits on MedShapeNet19, a new curated benchmark dataset we derive from
MedShapeNet, consisting of 19 anatomical classes with standardized training,
validation, and test splits. Experiments show that PSPooling significantly
improves reconstruction fidelity and classification accuracy in low-label
regimes, establishing a strong baseline for medical 3D shape learning. We hope
that MedShapeNet19 will serve as a widely adopted benchmark for anatomical
shape classification and further research in medical 3D shape analysis. Access
the complete codebase, model weights, and dataset information here:
https://github.com/TomasKrsicka/MedShapeNet19-PSPooling.

</details>


### [168] [Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers](https://arxiv.org/abs/2511.01617)
*Mohamed Eltahir,Ali Habibullah,Lama Ayash,Tanveer Hussain,Naeemullah Khan*

Main category: cs.CV

TL;DR: ViC is a training-free, zero-shot framework that reframes listwise fusion/reranking for cross-modal video retrieval as prompt-based reasoning in a Vision-Language Model, using an S-Grid serialization; it achieves state-of-the-art zero-shot Recall@1 on MSR-VTT and VATEX.



<details>
  <summary>Details</summary>
Motivation: Fusion from heterogeneous retrievers is challenging and typically relies on score signals, ignoring candidates' representations. There is a need to leverage content evidence and retriever metadata within a unified, train-free framework to handle complex multi-modal data like videos.

Method: Serialize content evidence and retriever metadata into the VLM prompt. Use S-Grid to represent each video as an image grid (optionally with subtitles) to enable list-wise reasoning. ViC can operate as a single-list reranker or as an ensemble fuser, and is evaluated in zero-shot settings on video benchmarks.

Result: ViC improves individual retrievers' precision, outperforms CombSUM as a fusion baseline, and achieves new state-of-the-art zero-shot retrieval on ActivityNet and VATEX, with MSR-VTT Recall@1 of 87.1% (t2v) / 89.0% (v2t) and VATEX Recall@1 of 99.6% (v2t). Gains up to ~40 points in Recall@1 over prior baselines.

Conclusion: ViC offers a simple, reproducible recipe for turning modern Vision-Language Models into powerful zero-shot rerankers and fusers for cross-modal video retrieval; code and resources are publicly available.

Abstract: In the retrieval domain, candidates' fusion from heterogeneous retrievers is
a long-standing challenge, particularly for complex, multi-modal data such as
videos. While typical fusion techniques are training-free, they rely solely on
rank or score signals, disregarding candidates' representations. This work
introduces Vote-in-Context (ViC), a generalized, training-free framework that
re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a
Vision-Language Model (VLM). The core insight is to serialize both content
evidence and retriever metadata directly within the VLM's prompt, allowing the
model to adaptively weigh retriever consensus against visual-linguistic
content. We demonstrate the generality of this framework by applying it to the
challenging domain of cross-modal video retrieval. To this end, we introduce
the S-Grid, a compact serialization map that represents each video as an image
grid, optionally paired with subtitles to enable list-wise reasoning over video
candidates. ViC is evaluated both as a single-list reranker, where it
dramatically improves the precision of individual retrievers, and as an
ensemble fuser, where it consistently outperforms strong baselines like
CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the
framework establishes new state-of-the-art zero-shot retrieval performance,
demonstrating its effectiveness in handling complex visual and temporal signals
alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1%
(t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive
gains of up to +40 Recall@1 over previous state-of-the-art baselines. We
present ViC as a simple, reproducible, and highly effective recipe for turning
modern VLMs into powerful zero-shot rerankers and fusers. Code and resources
are publicly available at: https://github.com/mohammad2012191/ViC

</details>


### [169] [Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models](https://arxiv.org/abs/2511.01618)
*Xiaoyu Zhan,Wenxuan Huang,Hao Sun,Xinyu Fu,Changfeng Ma,Shaosheng Cao,Bohan Jia,Shaohui Lin,Zhenfei Yin,Lei Bai,Wanli Ouyang,Yuanqi Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: A two-stage fine-tuning pipeline (SFT on Viewpoint-100K, followed by GRPO-based reinforcement learning) equips Multimodal LLMs with cross-view spatial reasoning, improving 3D tasks and cross-view consistency in both in-domain and out-of-domain scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the gap in spatial reasoning and cross-view consistency in Multimodal LLMs, which limits robust real-world 3D understanding and robotics applications.

Method: Introduce Viewpoint-100K, a 100K object-centric image pair dataset with diverse viewpoints and QA pairs. Employ a two-stage fine-tuning: (1) Supervised Fine-Tuning on Viewpoint-100K to inject foundational spatial knowledge; (2) Reinforcement Learning with Group Relative Policy Optimization (GRPO) to enhance generalization across broader questions. Add a hybrid cold-start initialization to learn viewpoint representations while preserving coherent reasoning. 

Result: The method activates the spatial reasoning abilities of MLLMs, yielding improved performance on both in-domain and out-of-domain spatial tasks and cross-view reasoning, with notable gains in 3D-related understanding.

Conclusion: Foundational spatial skills can be learned by MLLMs via targeted data and optimization strategies, boosting capabilities for robotics, autonomous systems, and 3D scene understanding, and guiding future research in spatially grounded multimodal AI.

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly improved 2D visual understanding, prompting interest in their
application to complex 3D reasoning tasks. However, it remains unclear whether
these models can effectively capture the detailed spatial information required
for robust real-world performance, especially cross-view consistency, a key
requirement for accurate 3D reasoning. Considering this issue, we introduce
Viewpoint Learning, a task designed to evaluate and improve the spatial
reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,
consisting of 100K object-centric image pairs with diverse viewpoints and
corresponding question-answer pairs. Our approach employs a two-stage
fine-tuning strategy: first, foundational knowledge is injected to the baseline
MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in
significant improvements across multiple tasks; second, generalization is
enhanced through Reinforcement Learning using the Group Relative Policy
Optimization (GRPO) algorithm on a broader set of questions. Additionally, we
introduce a hybrid cold-start initialization method designed to simultaneously
learn viewpoint representations and maintain coherent reasoning thinking.
Experimental results show that our approach significantly activates the spatial
reasoning ability of MLLM, improving performance on both in-domain and
out-of-domain reasoning tasks. Our findings highlight the value of developing
foundational spatial skills in MLLMs, supporting future progress in robotics,
autonomous systems, and 3D scene understanding.

</details>


### [170] [Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward](https://arxiv.org/abs/2511.01645)
*Xiaogang Xu,Ruihang Chu,Jian Wang,Kun Zhou,Wenjie Shu,Harry Yang,Ser-Nam Lim,Hao Chen,Liang Lin*

Main category: cs.CV

TL;DR: A plug-and-play RL framework for diffusion-based image restoration uses IQA-model rewards, focusing RL on hard samples and adaptively blending with supervised fine-tuning to boost restoration fidelity across tasks.


<details>
  <summary>Details</summary>
Motivation: Restoration requires high fidelity, and naive RL with ground-truth rewards may be suboptimal since SFT already encodes GT information. The aim is to tailor RL feedback to perceptual quality and sample difficulty to improve diffusion-based restoration.

Method: 1) Use an IQA-based reward derived from MLLM-based IQA models instead of GT-based supervision. 2) Apply RL primarily to challenging samples far from ground truth. 3) Initialize distributions with high-quality images via MLLM-IQA guidance. 4) As samples approach GT distribution, adaptively blend RL with SFT using an automatic weighting strategy that measures sample difficulty. 5) This approach is plug-and-play for diffusion-based restoration models.

Result: Extensive experiments across multiple benchmarks show that the RL framework improves performance on various restoration tasks.

Conclusion: The study demonstrates that IQA-rewarded RL, coupled with an adaptive RLâSFT mix, effectively enhances diffusion-based restoration, especially for difficult samples, and can be readily applied as a general enhancement to such models.

Abstract: Reinforcement Learning (RL) has recently been incorporated into diffusion
models, e.g., tasks such as text-to-image. However, directly applying existing
RL methods to diffusion-based image restoration models is suboptimal, as the
objective of restoration fundamentally differs from that of pure generation: it
places greater emphasis on fidelity. In this paper, we investigate how to
effectively integrate RL into diffusion-based restoration models. First,
through extensive experiments with various reward functions, we find that an
effective reward can be derived from an Image Quality Assessment (IQA) model,
instead of intuitive ground-truth-based supervision, which has already been
optimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover,
our strategy focuses on using RL for challenging samples that are significantly
distant from the ground truth, and our RL approach is innovatively implemented
using MLLM-based IQA models to align distributions with high-quality images
initially. As the samples approach the ground truth's distribution, RL is
adaptively combined with SFT for more fine-grained alignment. This dynamic
process is facilitated through an automatic weighting strategy that adjusts
based on the relative difficulty of the training samples. Our strategy is
plug-and-play that can be seamlessly applied to diffusion-based restoration
models, boosting its performance across various restoration tasks. Extensive
experiments across multiple benchmarks demonstrate the effectiveness of our
proposed RL framework.

</details>


### [171] [UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback](https://arxiv.org/abs/2511.01678)
*Ropeway Liu,Hangjie Yuan,Bo Dong,Jiazheng Xing,Jinwang Wang,Rui Zhao,Yan Xing,Weihua Chen,Fan Wang*

Main category: cs.CV

TL;DR: UniLumos proposes a unified RGB-space relighting framework for images and videos that enforces physical plausibility by using depth/normal supervision, enabling path-consistent, fast training; paired with LumosBench for fine-grained, attribute-level evaluation of lighting control via vision-language models.


<details>
  <summary>Details</summary>
Motivation: Relighting with diffusion models often occurs in a semantic latent space where proximity is not guaranteed to correlate with physical correctness, leading to unrealistic highlights, shadows, and occlusions. Supervision in visual space is costly, and there is a need for fine-grained control and reliable evaluation of lighting edits.

Method: Introduce UniLumos with a flow-matching backbone that receives RGB-space geometry feedback through depth and normal maps extracted from outputs to align lighting with scene structure. Employ path-consistency learning to enable effective supervision under few-step training. Propose a six-dimensional annotation protocol for core illumination attributes. Build LumosBench, a disentangled attribute-level benchmark that assesses lighting controllability using large vision-language models.

Result: Achieves state-of-the-art relighting quality with significantly improved physical consistency and about a 20Ã speedup for both image and video relighting.

Conclusion: UniLumos delivers a physically grounded, efficient relighting framework for images and videos, complemented by LumosBench for interpretable, attribute-level evaluation of lighting control.

Abstract: Relighting is a crucial task with both practical demand and artistic value,
and recent diffusion models have shown strong potential by enabling rich and
controllable lighting effects. However, as they are typically optimized in
semantic latent space, where proximity does not guarantee physical correctness
in visual space, they often produce unrealistic results, such as overexposed
highlights, misaligned shadows, and incorrect occlusions. We address this with
UniLumos, a unified relighting framework for both images and videos that brings
RGB-space geometry feedback into a flow matching backbone. By supervising the
model with depth and normal maps extracted from its outputs, we explicitly
align lighting effects with the scene structure, enhancing physical
plausibility. Nevertheless, this feedback requires high-quality outputs for
supervision in visual space, making standard multi-step denoising
computationally expensive. To mitigate this, we employ path consistency
learning, allowing supervision to remain effective even under few-step training
regimes. To enable fine-grained relighting control and supervision, we design a
structured six-dimensional annotation protocol capturing core illumination
attributes. Building upon this, we propose LumosBench, a disentangled
attribute-level benchmark that evaluates lighting controllability via large
vision-language models, enabling automatic and interpretable assessment of
relighting precision across individual dimensions. Extensive experiments
demonstrate that UniLumos achieves state-of-the-art relighting quality with
significantly improved physical consistency, while delivering a 20x speedup for
both image and video relighting. Code is available at
https://github.com/alibaba-damo-academy/Lumos-Custom.

</details>


### [172] [Progressive Translation of H&E to IHC with Enhanced Structural Fidelity](https://arxiv.org/abs/2511.01698)
*Yuhang Kang,Ziyu Su,Tianyang Wang,Zaibo Li,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: A progressive, decoupled architecture for translating H&E to IHC-like images enhances color fidelity and cell boundaries, outperforming baseline loss-combination approaches on HER2/ER datasets.


<details>
  <summary>Details</summary>
Motivation: IHC provides protein localization but is costly and hard to scale; existing stain translation methods use a single objective with linearly weighted losses, neglecting interdependencies and decoupled optimization; need efficient, high-quality synthetic IHC to enable broader access.

Method: Extend Adaptive Supervised PatchNCE (ASP) baseline with a progressive structure that decouples color, cell border, and structure generation. Introduce DAB concentration-based loss and image gradient loss, and implement a structure-color-cell boundary progressive generation pipeline.

Result: On HER2 and ER datasets, the proposed network significantly improves visual quality and yields finer structural details in generated IHC-like images.

Conclusion: A stage-wise, decoupled optimization framework improves both color fidelity and structural accuracy in computational stain translation, addressing limitations of single-objective loss schemes and enhancing the utility of synthetic IHC images.

Abstract: Compared to hematoxylin-eosin (H&E) staining, immunohistochemistry (IHC) not
only maintains the structural features of tissue samples, but also provides
high-resolution protein localization, which is essential for aiding in
pathology diagnosis. Despite its diagnostic value, IHC remains a costly and
labor-intensive technique. Its limited scalability and constraints in
multiplexing further hinder widespread adoption, especially in resource-limited
settings. Consequently, researchers are increasingly exploring computational
stain translation techniques to synthesize IHC-equivalent images from
H&E-stained slides, aiming to extract protein-level information more
efficiently and cost-effectively. However, most existing stain translation
techniques rely on a linearly weighted summation of multiple loss terms within
a single objective function, strategy that often overlooks the interdepedence
among these components-resulting in suboptimal image quality and an inability
to simultaneously preserve structural authenticity and color fidelity. To
address this limitation, we propose a novel network architecture that follows a
progressive structure, incorporating color and cell border generation logic,
which enables each visual aspect to be optimized in a stage-wise and decoupled
manner. To validate the effectiveness of our proposed network architecture, we
build upon the Adaptive Supervised PatchNCE (ASP) framework as our baseline. We
introduce additional loss functions based on 3,3'-diaminobenzidine (DAB)
chromogen concentration and image gradient, enhancing color fidelity and cell
boundary clarity in the generated IHC images. By reconstructing the generation
pipeline using our structure-color-cell boundary progressive mechanism,
experiments on HER2 and ER datasets demonstrated that the model significantly
improved visual quality and achieved finer structural details.

</details>


### [173] [Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond](https://arxiv.org/abs/2511.01704)
*Xin Qiao,Matteo Poggi,Xing Wei,Pengchao Deng,Yanhui Zhou,Stefano Mattoccia*

Main category: cs.CV

TL;DR: Proposes Learnable Fractional Reaction-Diffusion Dynamics (LFRD2) for under-display ToF imaging, combining neural networks with fractional reaction-diffusion to restore depth maps degraded by TOLED layers; introduces a time-fractional RD module with dynamic differential orders and an efficient continuous convolution via coefficient prediction; validated on four datasets; code released.


<details>
  <summary>Details</summary>
Motivation: TOLED layers cause signal attenuation, MPI, and temporal noise, degrading ToF depth quality; need interpretable physics-inspired restoration combined with learning to capture long-term dependencies.

Method: Hybrid framework with time-fractional reaction-diffusion module for iterative depth refinement with dynamically generated differential orders; efficient continuous convolution via coefficient prediction and repeated differentiation; end-to-end training.

Result: Demonstrates effectiveness on four benchmark datasets; improved depth restoration quality; code available.

Conclusion: LFRD2 provides an effective, interpretable, and data-driven approach for under-display ToF depth restoration, achieving strong performance and offering code for reproducibility.

Abstract: Under-display ToF imaging aims to achieve accurate depth sensing through a
ToF camera placed beneath a screen panel. However, transparent OLED (TOLED)
layers introduce severe degradations-such as signal attenuation, multi-path
interference (MPI), and temporal noise-that significantly compromise depth
quality. To alleviate this drawback, we propose Learnable Fractional
Reaction-Diffusion Dynamics (LFRD2), a hybrid framework that combines the
expressive power of neural networks with the interpretability of physical
modeling. Specifically, we implement a time-fractional reaction-diffusion
module that enables iterative depth refinement with dynamically generated
differential orders, capturing long-term dependencies. In addition, we
introduce an efficient continuous convolution operator via coefficient
prediction and repeated differentiation to further improve restoration quality.
Experiments on four benchmark datasets demonstrate the effectiveness of our
approach. The code is publicly available at https://github.com/wudiqx106/LFRD2.

</details>


### [174] [Probabilistic Robustness for Free? Revisiting Training via a Benchmark](https://arxiv.org/abs/2511.01724)
*Yi Zhang,Zheng Wang,Chen Zhen,Wenjie Ruan,Qing Guo,Siddartha Khastgir,Carsten Maple,Xingyu Zhao*

Main category: cs.CV

TL;DR: PRBench is a benchmark for probabilistic robustness (PR) in deep learning that compares adversarial training (AT) and PR-targeted training across datasets and architectures. It finds AT generally improves both AR and PR, while PR-targeted methods achieve lower generalization error and higher clean accuracy. The benchmark covers 222 trained models, 7 datasets, and 10 architectures, with a theoretical GE analysis and a public leaderboard at the provided URL.


<details>
  <summary>Details</summary>
Motivation: There is a lack of unified, comparable evaluation for probabilistic robustness and a clear framework to assess generalization across training methods. PR-focused training methods are underexplored and hard to compare to strong AT baselines.

Method: Introduce PRBench, a comprehensive benchmark that evaluates AT and PR-targeted training methods using metrics such as clean accuracy, PR and AR performance, training efficiency, and generalization error (GE). Includes theoretical GE analysis and a public leaderboard (222 models across 7 datasets and 10 architectures).

Result: The study finds that AT methods are more versatile in improving both AR and PR across varied hyperparameters, whereas PR-targeted methods consistently reduce GE and improve clean accuracy.

Conclusion: PRBench is the first dedicated benchmark for probabilistic robustness training, enabling standardized evaluation and cross-method comparison. It reveals trade-offs: AT offers broader AR/PR improvements but PR-targeted methods achieve better generalization and clean accuracy, supported by a public leaderboard to track progress.

Abstract: Deep learning models are notoriously vulnerable to imperceptible
perturbations. Most existing research centers on adversarial robustness (AR),
which evaluates models under worst-case scenarios by examining the existence of
deterministic adversarial examples (AEs). In contrast, probabilistic robustness
(PR) adopts a statistical perspective, measuring the probability that
predictions remain correct under stochastic perturbations. While PR is widely
regarded as a practical complement to AR, dedicated training methods for
improving PR are still relatively underexplored, albeit with emerging progress.
Among the few PR-targeted training methods, we identify three limitations: i
non-comparable evaluation protocols; ii limited comparisons to strong AT
baselines despite anecdotal PR gains from AT; and iii no unified framework to
compare the generalization of these methods. Thus, we introduce PRBench, the
first benchmark dedicated to evaluating improvements in PR achieved by
different robustness training methods. PRBench empirically compares most common
AT and PR-targeted training methods using a comprehensive set of metrics,
including clean accuracy, PR and AR performance, training efficiency, and
generalization error (GE). We also provide theoretical analysis on the GE of PR
performance across different training methods. Main findings revealed by
PRBench include: AT methods are more versatile than PR-targeted training
methods in terms of improving both AR and PR performance across diverse
hyperparameter settings, while PR-targeted training methods consistently yield
lower GE and higher clean accuracy. A leaderboard comprising 222 trained models
across 7 datasets and 10 model architectures is publicly available at
https://tmpspace.github.io/PRBenchLeaderboard/.

</details>


### [175] [Toward Strategy Identification and Subtask Decomposition In Task Exploration](https://arxiv.org/abs/2511.01728)
*Tom Odem*

Main category: cs.CV

TL;DR: A pipeline that automatically identifies global and local task strategies and meaningful subtasks in anticipatory human-machine interaction, using clustering, factor analysis, and string edit distance, with a Task Explorer app; adaptable to action-based time-series data.


<details>
  <summary>Details</summary>
Motivation: To improve machines' understanding of user knowledge, skill, and behavior to enable implicit coordination

Method: Developed a Task Explorer pipeline combining clustering, factor analysis, and string edit distance to discover global strategies, local sequences, and subtasks; encoded runs with hierarchical subtask structures and built a reviewable Task Explorer application; adaptable to various action-based time-series data.

Result: Automated identification of key task strategies and hierarchical subtasks; a review tool (Task Explorer); the pipeline is adaptable to other datasets and informs human-machine design about user knowledge, skill, and behavior.

Conclusion: The pipeline provides a practical, adaptable framework for uncovering structured task knowledge to support anticipatory HMI through implicit coordination.

Abstract: This research builds on work in anticipatory human-machine interaction, a
subfield of human-machine interaction where machines can facilitate
advantageous interactions by anticipating a user's future state. The aim of
this research is to further a machine's understanding of user knowledge, skill,
and behavior in pursuit of implicit coordination. A task explorer pipeline was
developed that uses clustering techniques, paired with factor analysis and
string edit distance, to automatically identify key global and local strategies
that are used to complete tasks. Global strategies identify generalized sets of
actions used to complete tasks, while local strategies identify sequences that
used those sets of actions in a similar composition. Additionally, meaningful
subtasks of various lengths are identified within the tasks. The task explorer
pipeline was able to automatically identify key strategies used to complete
tasks and encode user runs with hierarchical subtask structures. In addition, a
Task Explorer application was developed to easily review pipeline results. The
task explorer pipeline can be easily modified to any action-based time-series
data and the identified strategies and subtasks help to inform humans and
machines on user knowledge, skill, and behavior.

</details>


### [176] [CGF-DETR: Cross-Gated Fusion DETR for Enhanced Pneumonia Detection in Chest X-rays](https://arxiv.org/abs/2511.01730)
*Yefeng Wu,Yucheng Song,Ling Wu,Shan Wan,Yecheng Zhao*

Main category: cs.CV

TL;DR: CGF-DETR is a real-time pneumonia detector for chest X-rays that adds three modules to enhance feature extraction and efficient attention, achieving higher accuracy with minimal speed loss on RSNA data.


<details>
  <summary>Details</summary>
Motivation: Pneumonia detection in chest X-rays is clinically important but challenging for real-time detectors. Transformer-based detectors lag in medical imaging due to computational demands; a real-time, accurate detector tailored for pneumonia is needed.

Method: Introduce CGF-DETR with: XFABlock backbone for multi-scale feature extraction using convolutional attention within a CSP framework; SPGA module replacing standard multi-head attention with dynamic gating and single-head self-attention for efficiency; GCFC3 neck with multi-path convolution fusion and structural re-parameterization to preserve speed. Train and evaluate on RSNA Pneumonia Detection dataset; report mAP@0.5 and mAP@[0.5:0.95], plus FPS.

Result: On RSNA Pneumonia Detection dataset, CGF-DETR achieves 82.2% mAP@0.5, beating the RT-DETR-l baseline by 3.7% while running at 48.1 FPS. Ablation studies show each proposed module contributes to performance gains, with the full model achieving 50.4% mAP@[0.5:0.95].

Conclusion: CGF-DETR delivers improved pneumonia detection accuracy in chest X-rays with real-time capabilities. The XFABlock, SPGA, and GCFC3 components collectively enhance multi-scale feature extraction, efficient attention, and feature fusion, supporting practical deployment in clinical workflows.

Abstract: Pneumonia remains a leading cause of morbidity and mortality worldwide,
necessitating accurate and efficient automated detection systems. While recent
transformer-based detectors like RT-DETR have shown promise in object detection
tasks, their application to medical imaging, particularly pneumonia detection
in chest X-rays, remains underexplored. This paper presents CGF-DETR, an
enhanced real-time detection transformer specifically designed for pneumonia
detection. We introduce XFABlock in the backbone to improve multi-scale feature
extraction through convolutional attention mechanisms integrated with CSP
architecture. To achieve efficient feature aggregation, we propose SPGA module
that replaces standard multi-head attention with dynamic gating mechanisms and
single-head self-attention. Additionally, GCFC3 is designed for the neck to
enhance feature representation through multi-path convolution fusion while
maintaining real-time performance via structural re-parameterization. Extensive
experiments on the RSNA Pneumonia Detection dataset demonstrate that CGF-DETR
achieves 82.2\% mAP@0.5, outperforming the baseline RT-DETR-l by 3.7\% while
maintaining comparable inference speed at 48.1 FPS. Our ablation studies
confirm that each proposed module contributes meaningfully to the overall
performance improvement, with the complete model achieving 50.4\%
mAP@[0.5:0.95]

</details>


### [177] [HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain](https://arxiv.org/abs/2511.01756)
*Kai Zhai,Ziyan Huang,Qiang Nie,Xiang Li,Bo Ouyang*

Main category: cs.CV

TL;DR: HGFreNet introduces a GraphFormer with hop-hybrid attention and frequency-domain 3D trajectory consistency for robust 2D-to-3D pose lifting, improving temporal coherence and pose accuracy.


<details>
  <summary>Details</summary>
Motivation: 2D-to-3D pose lifting from monocular video suffers from depth ambiguity and 2D detection errors, causing incoherent 3D trajectories. Prior work focuses on frame-to-frame jitters and neglects global spatial-temporal correlations of skeletal motion.

Method: Introduce HGFreNet with a hop-hybrid graph attention (HGA) module and a Transformer encoder to capture global spatial-temporal correlations. HGA groups k-hop neighbors into hybrid groups to enlarge the receptive field and uses attention to learn global group correlations. Additionally, enforce global temporal consistency by constraining trajectory in the frequency domain, preceded by a preliminary network that estimates 3D pose to provide depth cues across frames.

Result: Empirical evaluation on Human3.6M and MPI-INF-3DHP shows HGFreNet achieves superior positional accuracy and temporal coherence compared to SOTA methods.

Conclusion: The GraphFormer-based HGFreNet effectively models global spatial-temporal correlations and enforces 3D trajectory consistency in the frequency domain, yielding improved 2D-to-3D pose lifting on standard benchmarks.

Abstract: 2D-to-3D human pose lifting is a fundamental challenge for 3D human pose
estimation in monocular video, where graph convolutional networks (GCNs) and
attention mechanisms have proven to be inherently suitable for encoding the
spatial-temporal correlations of skeletal joints. However, depth ambiguity and
errors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous
studies have attempted to restrict jitters in the time domain, for instance, by
constraining the differences between adjacent frames while neglecting the
global spatial-temporal correlations of skeletal joint motion. To tackle this
problem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid
feature aggregation and 3D trajectory consistency in the frequency domain.
Specifically, we propose a hop-hybrid graph attention (HGA) module and a
Transformer encoder to model global joint spatial-temporal correlations. The
HGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group
to enlarge the receptive field and applies the attention mechanism to discover
the latent correlations of these groups globally. We then exploit global
temporal correlations by constraining trajectory consistency in the frequency
domain. To provide 3D information for depth inference across frames and
maintain coherence over time, a preliminary network is applied to estimate the
3D pose. Extensive experiments were conducted on two standard benchmark
datasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed
HGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional
accuracy and temporal consistency.

</details>


### [178] [Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image](https://arxiv.org/abs/2511.01767)
*Yuxiao Yang,Xiao-Xiao Long,Zhiyang Dou,Cheng Lin,Yuan Liu,Qingsong Yan,Yuexin Ma,Haoqian Wang,Zhiqiang Wu,Wei Yin*

Main category: cs.CV

TL;DR: Wonder3D++ introduces a cross-domain diffusion-based pipeline that outputs multi-view normal maps and colors, uses cross-view/domain attention, and a cascaded mesh extraction method to efficiently produce high-fidelity textured meshes from a single image, achieving fast ~3-minute mesh extraction with strong generalization.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of SDS-based single-view 3D reconstruction (slow optimization and inconsistent geometry) and of fast single-network methods (low quality and missing geometric detail) by combining diffusion priors with cross-domain, multi-view signals to improve quality, consistency, and efficiency.

Method: 1) A cross-domain diffusion model that generates multi-view normal maps and corresponding color images. 2) A multi-view cross-domain attention mechanism enabling information exchange across views and modalities. 3) A cascaded 3D mesh extraction algorithm that converts the multi-view 2D representations into high-quality surfaces in a coarse-to-fine process, taking about 3 minutes.

Result: Extensive evaluations show high-quality reconstructions, robust generalization, and favorable efficiency compared with prior works. Code is available at the provided GitHub link.

Conclusion: The approach advances single-view 3D reconstruction by jointly improving quality, consistency, and efficiency through cross-domain diffusion, cross-view attention, and cascaded mesh extraction, delivering practical, high-quality textured meshes from a single image.

Abstract: In this work, we introduce \textbf{Wonder3D++}, a novel method for
efficiently generating high-fidelity textured meshes from single-view images.
Recent methods based on Score Distillation Sampling (SDS) have shown the
potential to recover 3D geometry from 2D diffusion priors, but they typically
suffer from time-consuming per-shape optimization and inconsistent geometry. In
contrast, certain works directly produce 3D information via fast network
inferences, but their results are often of low quality and lack geometric
details. To holistically improve the quality, consistency, and efficiency of
single-view reconstruction tasks, we propose a cross-domain diffusion model
that generates multi-view normal maps and the corresponding color images. To
ensure the consistency of generation, we employ a multi-view cross-domain
attention mechanism that facilitates information exchange across views and
modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that
drives high-quality surfaces from the multi-view 2D representations in only
about $3$ minute in a coarse-to-fine manner. Our extensive evaluations
demonstrate that our method achieves high-quality reconstruction results,
robust generalization, and good efficiency compared to prior works. Code
available at https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.

</details>


### [179] [UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs](https://arxiv.org/abs/2511.01768)
*Zhe Liu,Jinghua Hou,Xiaoqing Ye,Jingdong Wang,Hengshuang Zhao,Xiang Bai*

Main category: cs.CV

TL;DR: UniLION is a unified linear-group RNN-based architecture for autonomous driving that efficiently handles large-scale LiDAR, high-resolution multi-view images, and temporal sequences with linear complexity, offering LiDAR-only, temporal LiDAR, multi-modal, and multi-modal-temporal variants and achieving competitive to state-of-the-art across 3D perception, motion prediction, and planning; code is released.


<details>
  <summary>Details</summary>
Motivation: Transformers with quadratic attention are a bottleneck for long sequences and high-resolution multi-modal data in autonomous driving. There is a need for a scalable, unified model that can efficiently process diverse modalities and tasks without complicated fusion modules.

Method: Introduce UniLION using a linear group RNN operator to perform linear RNNs on grouped features, enabling efficient processing of LiDAR points, multi-view images, and temporal data. The architecture supports multiple specialization variants (LiDAR-only, temporal LiDAR, multi-modal, and multi-modal temporal fusion) without explicit temporal or multi-modal fusion modules, and is evaluated across a suite of core tasks.

Result: Demonstrates competitive and often state-of-the-art performance across core autonomous-driving tasks, including 3D perception (detection, tracking, occupancy, BEV segmentation), motion prediction, and end-to-end planning, with a unified architecture across variants.

Conclusion: UniLION offers a fresh perspective on 3D foundation model design for autonomous driving by simplifying multi-modal and multi-task system construction without sacrificing performance, and provides a flexible, scalable path toward unified 3D foundation models; code is available at the provided repository.

Abstract: Although transformers have demonstrated remarkable capabilities across
various domains, their quadratic attention mechanisms introduce significant
computational overhead when processing long-sequence data. In this paper, we
present a unified autonomous driving model, UniLION, which efficiently handles
large-scale LiDAR point clouds, high-resolution multi-view images, and even
temporal sequences based on the linear group RNN operator (i.e., performs
linear RNN for grouped features). Remarkably, UniLION serves as a single
versatile architecture that can seamlessly support multiple specialized
variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal
temporal fusion configurations) without requiring explicit temporal or
multi-modal fusion modules. Moreover, UniLION consistently delivers competitive
and even state-of-the-art performance across a wide range of core tasks,
including 3D perception (e.g., 3D object detection, 3D object tracking, 3D
occupancy prediction, BEV map segmentation), prediction (e.g., motion
prediction), and planning (e.g., end-to-end planning). This unified paradigm
naturally simplifies the design of multi-modal and multi-task autonomous
driving systems while maintaining superior performance. Ultimately, we hope
UniLION offers a fresh perspective on the development of 3D foundation models
in autonomous driving. Code is available at
https://github.com/happinesslz/UniLION

</details>


### [180] [How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment](https://arxiv.org/abs/2511.01775)
*Zhen Chen,Qing Xu,Jinlin Wu,Biao Yang,Yuhao Zhai,Geng Guo,Jing Zhang,Yinlu Ding,Nassir Navab,Jiebo Luo*

Main category: cs.CV

TL;DR: Video-generation foundation models show high visual plausibility but lack surgical causal understanding; SurgVeo benchmark and the Surgical Plausibility Pyramid reveal a gap between appearance and higher-order surgical reasoning.


<details>
  <summary>Details</summary>
Motivation: High-stakes medical domains require specialized causal knowledge beyond general physical rules; current evaluations emphasize visuals and realism, not clinical plausibility; a framework is needed to assess surgical video generation in terms of actionable, domain-specific understanding.

Method: Introduce SurgVeo, the first expert-curated benchmark for evaluating surgical video generation; propose the Surgical Plausibility Pyramid (SPP) with four tiers; perform a zero-shot evaluation of Veo-3 on laparoscopic and neurosurgical clips; four board-certified surgeons rate outputs using the SPP.

Result: Veo-3 achieves strong Visual Perceptual Plausibility but significantly underperforms at higher levels of the SPPâInstrument Operation Plausibility, Environment Feedback Plausibility, and Surgical Intent Plausibilityârevealing a gap between visually convincing videos and actual causal, procedural understanding in surgery.

Conclusion: SurgVeo and the SPP establish a foundational framework and roadmap for developing and evaluating future surgical video-generation models that must navigate specialized healthcare knowledge and real-world clinical reasoning.

Abstract: Foundation models in video generation are demonstrating remarkable
capabilities as potential world models for simulating the physical world.
However, their application in high-stakes domains like surgery, which demand
deep, specialized causal knowledge rather than general physical rules, remains
a critical unexplored gap. To systematically address this challenge, we present
SurgVeo, the first expert-curated benchmark for video generation model
evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,
four-tiered framework tailored to assess model outputs from basic appearance to
complex surgical strategy. On the basis of the SurgVeo benchmark, we task the
advanced Veo-3 model with a zero-shot prediction task on surgical clips from
laparoscopic and neurosurgical procedures. A panel of four board-certified
surgeons evaluates the generated videos according to the SPP. Our results
reveal a distinct "plausibility gap": while Veo-3 achieves exceptional Visual
Perceptual Plausibility, it fails critically at higher levels of the SPP,
including Instrument Operation Plausibility, Environment Feedback Plausibility,
and Surgical Intent Plausibility. This work provides the first quantitative
evidence of the chasm between visually convincing mimicry and causal
understanding in surgical AI. Our findings from SurgVeo and the SPP establish a
crucial foundation and roadmap for developing future models capable of
navigating the complexities of specialized, real-world healthcare domains.

</details>


### [181] [PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution](https://arxiv.org/abs/2511.01802)
*Tejas Sarnaik,Manan Shah,Ravi Hegde*

Main category: cs.CV

TL;DR: Prompt-driven GraphRAG enhances RAG by using a symbolic knowledge graph and prompt-guided reasoning for entity extraction, fact selection, and passage reranking in multi-hop QA, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address under-explored impact of prompt design on retrieval and reasoning in graph-based RAG for multi-hop QA.

Method: Build a symbolic knowledge graph from text by encoding entities and relations as triples. Use prompt-driven prompts to guide entity extraction, fact selection, and passage reranking. Leverage LLMs selectively during online retrieval for semantic filtering and answer generation. Employ entity-guided graph traversal via Personalized PageRank (PPR) for efficient, scalable retrieval on the knowledge graph.

Result: State-of-the-art on HotpotQA (F1 80.7%, Recall@5 97.1%) and 2WikiMultiHopQA (F1 78.9%, Recall@5 98.1%).

Conclusion: Demonstrates that prompt design is crucial for improving retrieval accuracy and response quality, and lays groundwork for more efficient and interpretable prompt-aware graph-based multi-hop QA.

Abstract: Retrieval-Augmented Generation (RAG) has become a robust framework for
enhancing Large Language Models (LLMs) with external knowledge. Recent advances
in RAG have investigated graph based retrieval for intricate reasoning;
however, the influence of prompt design on enhancing the retrieval and
reasoning process is still considerably under-examined. In this paper, we
present a prompt-driven GraphRAG framework that underscores the significance of
prompt formulation in facilitating entity extraction, fact selection, and
passage reranking for multi-hop question answering. Our approach creates a
symbolic knowledge graph from text data by encoding entities and factual
relationships as structured facts triples. We use LLMs selectively during
online retrieval to perform semantic filtering and answer generation. We also
use entity-guided graph traversal through Personalized PageRank (PPR) to
support efficient, scalable retrieval based on the knowledge graph we built.
Our system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA,
with F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%,
respectively. These results show that prompt design is an important part of
improving retrieval accuracy and response quality. This research lays the
groundwork for more efficient and comprehensible multi-hop question-answering
systems, highlighting the importance of prompt-aware graph reasoning.

</details>


### [182] [SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art](https://arxiv.org/abs/2511.01817)
*Sagi Eppel,Alona Strugatski*

Main category: cs.CV

TL;DR: A large-scale SciTextures dataset links visual textures to underlying generative mechanisms and benchmarks AI models on inferring and recreating those mechanisms.


<details>
  <summary>Details</summary>
Motivation: To bridge perception and causal understanding by connecting textures and patterns to their generating processes across science, tech, and art.

Method: An agentic AI pipeline autonomously collects and standardizes ~1,200 models and 100,000 images of textures from diverse domains; uses these to evaluate AI models on linking visuals to their mechanisms, and on inferring/recreating mechanisms from real-world images by generating simulations and comparing to originals.

Result: Shows that vision-language models can understand and simulate physical systems beyond mere appearance; dataset and code are released for benchmarking.

Conclusion: SciTextures provides a scalable benchmark that connects visual patterns with their mechanisms, enabling evaluation and advancement of AI systems that reason about physical processes.

Abstract: The ability to connect visual patterns with the processes that form them
represents one of the deepest forms of visual understanding. Textures of clouds
and waves, the growth of cities and forests, or the formation of materials and
landscapes are all examples of patterns emerging from underlying mechanisms. We
present the Scitextures dataset, a large-scale collection of textures and
visual patterns from all domains of science, tech, and art, along with the
models and code that generate these images. Covering over 1,200 different
models and 100,000 images of patterns and textures from physics, chemistry,
biology, sociology, technology, mathematics, and art, this dataset offers a way
to explore the connection between the visual patterns that shape our world and
the mechanisms that produce them. Created by an agentic AI pipeline that
autonomously collects and implements models in standardized form, we use
SciTextures to evaluate the ability of leading AI models to link visual
patterns to the models and code that generate them, and to identify different
patterns that emerged from the same process. We also test AIs ability to infer
and recreate the mechanisms behind visual patterns by providing a natural image
of a real-world pattern and asking the AI to identify, model, and code the
mechanism that formed the pattern, then run this code to generate a simulated
image that is compared to the real image. These benchmarks show that
vision-language models (VLMs) can understand and simulate the physical system
beyond a visual pattern. The dataset and code are available at:
https://zenodo.org/records/17485502

</details>


### [183] [TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning](https://arxiv.org/abs/2511.01833)
*Ming Li,Jike Zhong,Shitian Zhao,Haoquan Zhang,Shaoheng Lin,Yuxiang Lai,Wei Chen,Konstantinos Psounis,Kaipeng Zhang*

Main category: cs.CV

TL;DR: TIR-Bench is a new, comprehensive benchmark for agentic thinking-with-images across 13 tasks, evaluating 22 MLLMs on tool-use-enabled image reasoning; results show universal difficulty and highlight genuine thinking-with-images requirements; includes a pilot study on fine-tuning approaches.


<details>
  <summary>Details</summary>
Motivation: Current Visual Search benchmarks fail to evaluate complex, dynamic, and tool-dependent reasoning required by thinking-with-images models; a robust benchmark is needed to measure agentic capabilities.

Method: Introduce TIR-Bench with 13 tasks requiring novel image-processing tool use in chain-of-thought; evaluate 22 multimodal LLMs including open-source and proprietary models, some augmented with explicit tool-use; conduct a pilot study comparing direct versus agentic fine-tuning.

Result: TIR-Bench proves universally challenging; strong performance demands genuine thinking-with-images capabilities; study quantifies gap across models; pilot fine-tuning results discussed.

Conclusion: The benchmark reveals current models' limitations and underscores the value of tool-enabled, agentic reasoning; supports future development and evaluation of thinking-with-images systems.

Abstract: The frontier of visual reasoning is shifting toward models like OpenAI o3,
which can intelligently create and operate tools to transform images for
problem-solving, also known as thinking-\textit{with}-images in
chain-of-thought. Yet existing benchmarks fail to fully capture this advanced
capability. Even Visual Search, the most common benchmark for current
thinking-\textit{with}-images methods, tests only basic operations such as
localization and cropping, offering little insight into more complex, dynamic,
and tool-dependent reasoning. We introduce \textbf{TIR-Bench}, a comprehensive
benchmark for evaluating agentic thinking-with-images across 13 diverse tasks,
each requiring novel tool use for image processing and manipulation in
chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from
leading open-sourced and proprietary models to those with explicit tool-use
augmentation. Results show that TIR-Bench is universally challenging, and
strong performance requires genuine thinking-with-images capabilities. Finally,
we present a pilot study comparing direct versus agentic fine-tuning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [184] [VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games](https://arxiv.org/abs/2511.00002)
*Yurun Wu,Yousong Sun,Burkhard Wunsche,Jia Wang,Elliott Wen*

Main category: cs.LG

TL;DR: VRScout is a deep learning agent that autonomously tests VR games by learning multi-step actions from human demonstrations, using an enhanced Action Chunking Transformer and a dynamic horizon to achieve expert-level, real-time VR testing at 60 FPS on consumer hardware.


<details>
  <summary>Details</summary>
Motivation: Quality assurance for VR is labor-intensive and doesn't scale; VR's high-dimensional input and real-time constraints require automated, scalable testing solutions.

Method: Train an enhanced Action Chunking Transformer to predict multi-step action sequences from human demonstrations; incorporate a dynamically adjustable sliding horizon to adapt temporal context at runtime; enable 60 FPS real-time inference on commodity hardware; validated on commercial VR titles.

Result: Achieves expert-level performance with limited training data; real-time 60 FPS inference on consumer hardware; demonstrates practicality and scalability for automated VR game testing and safety auditing.

Conclusion: VRScout provides a scalable, generalizable framework for automated VR QA that can reduce human labor and improve safety auditing, with potential to extend to broader VR testing tasks.

Abstract: Virtual Reality (VR) has rapidly become a mainstream platform for gaming and
interactive experiences, yet ensuring the quality, safety, and appropriateness
of VR content remains a pressing challenge. Traditional human-based quality
assurance is labor-intensive and cannot scale with the industry's rapid growth.
While automated testing has been applied to traditional 2D and 3D games,
extending it to VR introduces unique difficulties due to high-dimensional
sensory inputs and strict real-time performance requirements. We present
VRScout, a deep learning-based agent capable of autonomously navigating VR
environments and interacting with virtual objects in a human-like and real-time
manner. VRScout learns from human demonstrations using an enhanced Action
Chunking Transformer that predicts multi-step action sequences. This enables
our agent to capture higher-level strategies and generalize across diverse
environments. To balance responsiveness and precision, we introduce a
dynamically adjustable sliding horizon that adapts the agent's temporal context
at runtime. We evaluate VRScout on commercial VR titles and show that it
achieves expert-level performance with only limited training data, while
maintaining real-time inference at 60 FPS on consumer-grade hardware. These
results position VRScout as a practical and scalable framework for automated VR
game testing, with direct applications in both quality assurance and safety
auditing.

</details>


### [185] [Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts](https://arxiv.org/abs/2511.00029)
*Samaksh Bhargav,Zining Zhu*

Main category: cs.LG

TL;DR: A method to steer LLM safety and utility using Sparse Auto Encoders (SAEs) to select optimal features, coupled with a contrasting prompt strategy and benchmarks, claiming improved safety (18.9%) and utility (11.1%) on Llama-3 8B.


<details>
  <summary>Details</summary>
Motivation: To enable safer LLM deployment without expensive full-model fine-tuning, leveraging interpretable SAE-based feature steering to navigate the safety-utility tradeoff.

Method: Use Sparse Auto Encoders to identify steering features and apply calibrated steering strengths. Employ an innovative contrasting prompt method with AI-Generated Prompts Dataset (teknium/OpenHermes-2p5-Mistral-7B) and Air Bench eu-dataset to guide feature selection. Validate on Llama-3 8B with measured safety and utility metrics.

Result: Reported improvements: safety performance up by 18.9% and utility up by 11.1% when using the SAE-based steering with principled feature selection.

Conclusion: Targeted SAE steering can potentially surpass traditional safety-utility tradeoffs if optimal features are identified through principled selection; claims would benefit from replication, clearer definitions of safety/utility metrics, and assessment across broader models and datasets.

Abstract: Large Language Model (LLM) deployment requires guiding the LLM to recognize
and not answer unsafe prompts while complying with safe prompts. Previous
methods for achieving this require adjusting model weights along with other
expensive procedures. While recent advances in Sparse Autoencoders (SAEs) have
enabled interpretable feature extraction from LLMs, existing approaches lack
systematic feature selection methods and principled evaluation of
safety-utility tradeoffs. We explored using different steering features and
steering strengths using Sparse Auto Encoders (SAEs) to provide a solution.
Using an accurate and innovative contrasting prompt method with the
AI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air
Bench eu-dataset to efficiently choose the best features in the model to steer,
we tested this method on Llama-3 8B. We conclude that using this method, our
approach achieves an 18.9% improvement in safety performance while
simultaneously increasing utility by 11.1%, demonstrating that targeted SAE
steering can overcome traditional safety-utility tradeoffs when optimal
features are identified through principled selection methods.

</details>


### [186] [Probing Knowledge Holes in Unlearned LLMs](https://arxiv.org/abs/2511.00030)
*Myeongseob Ko,Hoang Anh Just,Charles Fleming,Ming Jin,Ruoxi Jia*

Main category: cs.LG

TL;DR: Unlearning can introduce hidden knowledge holes and standard benchmarks miss them; a test-case generation framework reveals high rates of failing cases, urging reevaluation of knowledge-preservation evaluation.


<details>
  <summary>Details</summary>
Motivation: Aim to ensure benign knowledge isn't unintentionally erased during pre-training unlearning and to go beyond existing benchmarks.

Method: Develop a test-case generation framework that probes immediate neighbors of unlearned content and broader failure areas; evaluate unlearning methods by generating and testing cases.

Result: Up to 98.7% of generated test cases yield irrelevant or nonsensical responses from unlearned models, while pretrained models would answer; reveals hidden costs of unlearning.

Conclusion: Evaluation of knowledge preservation in unlearning should move beyond static benchmarks; adopt more nuanced, failure-aware evaluation frameworks to detect hidden knowledge losses.

Abstract: Machine unlearning has emerged as a prevalent technical solution for
selectively removing unwanted knowledge absorbed during pre-training, without
requiring full retraining. While recent unlearning techniques can effectively
remove undesirable content without severely compromising performance on
standard benchmarks, we find that they may inadvertently create ``knowledge
holes'' -- unintended losses of benign knowledge that standard benchmarks fail
to capture. To probe where unlearned models reveal knowledge holes, we propose
a test case generation framework that explores both immediate neighbors of
unlearned content and broader areas of potential failures. Our evaluation
demonstrates significant hidden costs of unlearning: up to 98.7\% of the test
cases yield irrelevant or nonsensical responses from unlearned models, despite
being answerable by the pretrained model. These findings necessitate rethinking
the conventional approach to evaluating knowledge preservation in unlearning,
moving beyond standard, static benchmarks.

</details>


### [187] [From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators](https://arxiv.org/abs/2511.00032)
*Lei Liu,Zhongyi Yu,Hong Wang,Huanshuo Dong,Haiyang Xin,Hongwei Zhao,Bin Li*

Main category: cs.LG

TL;DR: Skip-Block Routing (SBR) is a general framework for Transformer-based neural operators that learns per-token complexity and routes tokens through fewer computations in later layers, achieving ~50% FLOPs reduction and up to 2x faster inference without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Neural operators for PDEs suffer from uniform computation across regions with vastly different complexity, leading to inefficiency in large-scale engineering tasks (e.g., turbulence).

Method: Integrates Skip-Block Routing into multi-layer Transformer-based neural operators. A routing mechanism learns token complexity ranking during training; during inference, it prunes or routes less complex tokens through fewer blocks in later layers, focusing processing on harder regions.

Result: Shows that SBR is a general framework applicable to various neural operators, reducing FLOPs by about 50% and enabling up to 2x faster inference while maintaining accuracy.

Conclusion: SBR offers a practical, adaptable approach to align computation with local field complexity in neural operators for PDEs, enabling more efficient large-scale PDE solvers; open questions include routing overhead, impact on very high-complexity regimes, and applicability beyond Transformer-based operators.

Abstract: In recent years, Neural Operators(NO) have gradually emerged as a popular
approach for solving Partial Differential Equations (PDEs). However, their
application to large-scale engineering tasks suffers from significant
computational overhead. And the fact that current models impose a uniform
computational cost while physical fields exhibit vastly different complexities
constitutes a fundamental mismatch, which is the root of this inefficiency. For
instance, in turbulence flows, intricate vortex regions require deeper network
processing compared to stable flows. To address this, we introduce a framework:
Skip-Block Routing (SBR), a general framework designed for Transformer-based
neural operators, capable of being integrated into their multi-layer
architectures. First, SBR uses a routing mechanism to learn the complexity and
ranking of tokens, which is then applied during inference. Then, in later
layers, it decides how many tokens are passed forward based on this ranking.
This way, the model focuses more processing capacity on the tokens that are
more complex. Experiments demonstrate that SBR is a general framework that
seamlessly integrates into various neural operators. Our method reduces
computational cost by approximately 50% in terms of Floating Point Operations
(FLOPs), while still delivering up to 2x faster inference without sacrificing
accuracy.

</details>


### [188] [Neural Architecture Search for global multi-step Forecasting of Energy Production Time Series](https://arxiv.org/abs/2511.00035)
*Georg Velev,Stefan Lessmann*

Main category: cs.LG

TL;DR: An NAS-based framework automatically discovers efficient, generalizable multi-step time-series models for energy forecasting, yielding an ensemble of lightweight architectures that surpass Transformers and pre-trained models in both speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the need for accurate, fast short-term energy forecasts under operational constraints while ensuring generalization to unseen data and reducing manual model design effort.

Method: Design a NAS search space limited to efficient components suited to energy time series, propose a temporal-generalization-focused objective to explore diverse regions of the high-dimensional space, and assemble an ensemble of the discovered lightweight architectures for global multi-step forecasting.

Result: An ensemble of lightweight NAS-discovered architectures outperforms state-of-the-art techniques (including Transformers and pre-trained forecasting models) in both efficiency and accuracy on energy production time series.

Conclusion: NAS-driven automated discovery can deliver accurate, efficient, and generalizable global multi-step forecasts for energy data, reducing manual design work and enabling scalable deployment.

Abstract: The dynamic energy sector requires both predictive accuracy and runtime
efficiency for short-term forecasting of energy generation under operational
constraints, where timely and precise predictions are crucial. The manual
configuration of complex methods, which can generate accurate global multi-step
predictions without suffering from a computational bottleneck, represents a
procedure with significant time requirements and high risk for human-made
errors. A further intricacy arises from the temporal dynamics present in
energy-related data. Additionally, the generalization to unseen data is
imperative for continuously deploying forecasting techniques over time. To
overcome these challenges, in this research, we design a neural architecture
search (NAS)-based framework for the automated discovery of time series models
that strike a balance between computational efficiency, predictive performance,
and generalization power for the global, multi-step short-term forecasting of
energy production time series. In particular, we introduce a search space
consisting only of efficient components, which can capture distinctive patterns
of energy time series. Furthermore, we formulate a novel objective function
that accounts for performance generalization in temporal context and the
maximal exploration of different regions of our high-dimensional search space.
The results obtained on energy production time series show that an ensemble of
lightweight architectures discovered with NAS outperforms state-of-the-art
techniques, such as Transformers, as well as pre-trained forecasting models, in
terms of both efficiency and accuracy.

</details>


### [189] [Semi-Supervised Preference Optimization with Limited Feedback](https://arxiv.org/abs/2511.00040)
*Seonggyun Lee,Sungjun Lim,Seojin Park,Soeun Cheon,Kyungwoo Song*

Main category: cs.LG

TL;DR: Semi-Supervised Preference Optimization (SSPO) learns from a small set of labeled pairwise preferences and a large pool of unpaired data by pseudo-labeling using a provable reward-threshold; it achieves strong data efficiency, matching or surpassing baselines with far less labeled data.


<details>
  <summary>Details</summary>
Motivation: Reduce reliance on expensive labeled feedback in preference optimization for language models by leveraging large-scale unpaired data.

Method: Proves existence of an optimal reward threshold that separates winning and losing responses with high probability; uses this to pseudo-label unpaired data; trains using both labeled and pseudo-labeled data to distill latent preferences; evaluates on Llama3-8B-Instruct with ultra feedback datasets.

Result: SSPO with only 1% UltraFeedback outperforms baselines trained on 10% UltraFeedback, demonstrating substantial data-efficiency gains across datasets.

Conclusion: Semi-supervised preference optimization can retain human alignment while greatly reducing annotation costs through principled pseudo-labeling and pseudo-label-based distillation.

Abstract: The field of preference optimization has made outstanding contributions to
the alignment of language models with human preferences. Despite these
advancements, recent methods still rely heavily on substantial paired (labeled)
feedback data, leading to substantial resource expenditures. To address these
challenges, we study the problem of Semi-Supervised Preference Optimization
(SSPO) in which the idea is to learn from both a small number of pairwise
preference labels and a large pool of unpaired samples simultaneously. Our key
theoretical contribution proves the existence of an optimal reward threshold
capable of separating winning and losing responses with high probability, which
enables a principled pseudo-labeling of unpaired data. By leveraging these
pseudo-labels, SSPO effectively distills latent preferences from large-scale
unpaired data, thus maintaining human alignment while drastically reducing
acquisition costs. Extensive experiments across datasets validate this
remarkable data efficiency; for instance, SSPO trained with Llama3-8B-Instruct
on just 1% of UltraFeedback consistently surpasses strong baselines trained on
10% of UltraFeedback.

</details>


### [190] [Physics-Informed Neural Network Frameworks for the Analysis of Engineering and Biological Dynamical Systems Governed by Ordinary Differential Equations](https://arxiv.org/abs/2511.00043)
*Tyrus Whitman,Andrew Particka,Christopher Diers,Ian Griffin,Charuka Wickramasinghe,Pradeep Ranaweera*

Main category: cs.LG

TL;DR: PINNs show promise for solving ODEs in challenging engineering/biology problems, outperforming traditional solvers when loss components are balanced and hyperparameters are tuned; embedding prior physical knowledge and hard constraints enhances predictive capability, though results depend on controlled training settings.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional ODE solvers (e.g., stiffness, shocks, irregular domains, high dimensions) by leveraging Physics-Informed Neural Networks to improve convergence, accuracy, and generalization in solving ODEs.

Method: Use classical ODE benchmark problems to evaluate PINNs: analyze existence/uniqueness, train PINNs, and systematically study how data loss, initial-condition loss, and residual loss balanceâvia weightingâaffects convergence. Conduct systematic hyperparameter tuning (depth, width, activation, learning rate, optimizers, weight initialization, collocation sampling) and test the impact of embedding prior knowledge and hard architectural constraints without restricting ODE generality.

Result: Findings indicate PINNs can yield superior results for complex problems when the loss components are properly balanced and hyperparameters are tuned. Systematic tuning and architectural constraints, along with embedding prior knowledge, significantly improve predictive accuracy and capability on benchmark ODE problems.

Conclusion: PINNs are not a universal solution but are highly effective for complex ODE systems when carefully configured. Proper loss balancing and integration of prior physical information or hard constraints substantially boost predictive performance on controlled benchmarks.

Abstract: In this study, we present and validate the predictive capability of the
Physics-Informed Neural Networks (PINNs) methodology for solving a variety of
engineering and biological dynamical systems governed by ordinary differential
equations (ODEs). While traditional numerical methods a re effective for many
ODEs, they often struggle to achieve convergence in problems involving high
stiffness, shocks, irregular domains, singular perturbations, high dimensions,
or boundary discontinuities. Alternatively, PINNs offer a powerful approach for
handling challenging numerical scenarios. In this study, classical ODE problems
are employed as controlled testbeds to systematically evaluate the accuracy,
training efficiency, and generalization capability under controlled conditions
of the PINNs framework. Although not a universal solution, PINNs can achieve
superior results by embedding physical laws directly into the learning process.
We first analyze the existence and uniqueness properties of several benchmark
problems and subsequently validate the PINNs methodology on these model
systems. Our results demonstrate that for complex problems to converge to
correct solutions, the loss function components data loss, initial condition
loss, and residual loss must be appropriately balanced through careful
weighting. We further establish that systematic tuning of hyperparameters,
including network depth, layer width, activation functions, learning rate,
optimization algorithms, w eight initialization schemes, and collocation point
sampling, plays a crucial role in achieving accurate solutions. Additionally,
embedding prior knowledge and imposing hard constraints on the network
architecture, without loss the generality of the ODE system, significantly
enhances the predictive capability of PINNs.

</details>


### [191] [ReLaX-Net: Reusing Layers for Parameter-Efficient Physical Neural Networks](https://arxiv.org/abs/2511.00044)
*Kohei Tsuchiyama,Andre Roehm,Takatomo Mihana,Ryoichi Horisaki*

Main category: cs.LG

TL;DR: Proposes ReLaX-Net, a hardware-friendly time-multiplexed weight-sharing scheme for Physical Neural Networks (PNNs) that increases effective depth with minimal changes, achieving better parameter-efficient performance than equivalent traditional networks.


<details>
  <summary>Details</summary>
Motivation: PNNs lag behind digital neural networks in scale; parameter efficiency and hardware constraints are critical. A layer-reuse strategy can emulate deeper architectures without proportionally increasing trainable parameters, leveraging fast active elements vs. slow trainable weights.

Method: Introduce ReLaX-Net: layer-by-layer time-multiplexing that reuses existing layers with fast switches added to PNNs. Validate via numerical experiments on image classification and NLP tasks, comparing against conventional PNNs and parameter-matched RNNs/DNNs.

Result: ReLaX-Net yields improved computational performance with only minor hardware modifications. It demonstrates favorable scaling and outperforms equivalent traditional RNNs/DNNs having the same parameter count.

Conclusion: Time-multiplexed layer reuse is a viable, hardware-friendly path to scale PNNs, narrowing the performance gap with digital networks and enabling more parameter-efficient architectures.

Abstract: Physical Neural Networks (PNN) are promising platforms for next-generation
computing systems. However, recent advances in digital neural network
performance are largely driven by the rapid growth in the number of trainable
parameters and, so far, demonstrated PNNs are lagging behind by several orders
of magnitude in terms of scale. This mirrors size and performance constraints
found in early digital neural networks. In that period, efficient reuse of
parameters contributed to the development of parameter-efficient architectures
such as convolutional neural networks.
  In this work, we numerically investigate hardware-friendly weight-tying for
PNNs. Crucially, with many PNN systems, there is a time-scale separation
between the fast dynamic active elements of the forward pass and the only
slowly trainable elements implementing weights and biases. With this in mind,we
propose the Reuse of Layers for eXpanding a Neural Network (ReLaX-Net)
architecture, which employs a simple layer-by-layer time-multiplexing scheme to
increase the effective network depth and efficiently use the number of
parameters. We only require the addition of fast switches for existing PNNs. We
validate ReLaX-Nets via numerical experiments on image classification and
natural language processing tasks. Our results show that ReLaX-Net improves
computational performance with only minor modifications to a conventional PNN.
We observe a favorable scaling, where ReLaX-Nets exceed the performance of
equivalent traditional RNNs or DNNs with the same number of parameters.

</details>


### [192] [DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection](https://arxiv.org/abs/2511.00047)
*Omkar Kulkarni,Rohitash Chandra*

Main category: cs.LG

TL;DR: DynBERG combines Graph-BERT with a GRU to handle dynamic, directed financial transaction graphs; it outperforms baselines and the GRU component is essential.


<details>
  <summary>Details</summary>
Motivation: To adapt graph transformers (Graph-BERT) for dynamic, directed financial networks and improve fraud detection resilience to market shifts by capturing temporal evolution.

Method: Introduce DynBERG, a Graph-BERT-based architecture augmented with a GRU layer to model temporal dynamics across time steps; modify the algorithm to support directed edges; evaluate on the Elliptic Bitcoin transaction dataset including the Dark Market Shutdown event; compare against EvolveGCN and GCN; perform an ablation study on the temporal component.

Result: DynBERG outperforms EvolveGCN before the market shutdown and surpasses GCN after the event; ablation confirms the critical role of the GRU time-series component for capturing temporal dynamics in transactions.

Conclusion: Integrating transformer-based spatial encoding with temporal recurrence yields robust dynamic graph representations for financial fraud detection, effective under major market shifts.

Abstract: Financial fraud detection is critical for maintaining the integrity of
financial systems, particularly in decentralised environments such as
cryptocurrency networks. Although Graph Convolutional Networks (GCNs) are
widely used for financial fraud detection, graph Transformer models such as
Graph-BERT are gaining prominence due to their Transformer-based architecture,
which mitigates issues such as over-smoothing. Graph-BERT is designed for
static graphs and primarily evaluated on citation networks with undirected
edges. However, financial transaction networks are inherently dynamic, with
evolving structures and directed edges representing the flow of money. To
address these challenges, we introduce DynBERG, a novel architecture that
integrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture
temporal evolution over multiple time steps. Additionally, we modify the
underlying algorithm to support directed edges, making DynBERG well-suited for
dynamic financial transaction analysis. We evaluate our model on the Elliptic
dataset, which includes Bitcoin transactions, including all transactions during
a major cryptocurrency market event, the Dark Market Shutdown. By assessing
DynBERG's resilience before and after this event, we analyse its ability to
adapt to significant market shifts that impact transaction behaviours. Our
model is benchmarked against state-of-the-art dynamic graph classification
approaches, such as EvolveGCN and GCN, demonstrating superior performance,
outperforming EvolveGCN before the market shutdown and surpassing GCN after the
event. Additionally, an ablation study highlights the critical role of
incorporating a time-series deep learning component, showcasing the
effectiveness of GRU in modelling the temporal dynamics of financial
transactions.

</details>


### [193] [Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon Weather Forecasting](https://arxiv.org/abs/2511.00049)
*Yao Liu*

Main category: cs.LG

TL;DR: A self-supervised, graph-based framework for multi-variable weather forecasting improves predictions by combining a GNN for spatial reasoning, self-supervised representation learning, and a spatio-temporal adaptation module to generalize across forecasting horizons, achieving state-of-the-art results on ERA5 and MERRA-2 with city-scale validation.


<details>
  <summary>Details</summary>
Motivation: Weather forecasting is challenged by strong spatio-temporal dependencies and limited labeled data that hinder generalization across horizons and regions; a scalable, label-efficient approach is needed to leverage unlabeled data and adapt to varying forecast horizons.

Method: A modular framework integrating (1) a graph neural network for spatial reasoning among grid cells/variables, (2) self-supervised pretraining to learn robust representations from unlabeled data, and (3) a spatio-temporal adaptation mechanism to improve generalization across different forecasting horizons; evaluated on ERA5 and MERRA-2 reanalysis data with city-scale analyses in Beijing and Shanghai.

Result: The approach outperforms traditional numerical weather prediction models and recent deep learning baselines on the tested datasets; quantitative gains and visual analyses indicate better capture of fine-grained meteorological patterns; demonstrated label-efficient scalability across variables and horizons.

Conclusion: The proposed framework offers a scalable, data-efficient solution for data-driven weather forecasting that leverages spatio-temporal structure and unlabeled data to improve accuracy and generalization across horizons and regions.

Abstract: Accurate and robust weather forecasting remains a fundamental challenge due
to the inherent spatio-temporal complexity of atmospheric systems. In this
paper, we propose a novel self-supervised learning framework that leverages
spatio-temporal structures to improve multi-variable weather prediction. The
model integrates a graph neural network (GNN) for spatial reasoning, a
self-supervised pretraining scheme for representation learning, and a
spatio-temporal adaptation mechanism to enhance generalization across varying
forecasting horizons. Extensive experiments on both ERA5 and MERRA-2 reanalysis
datasets demonstrate that our approach achieves superior performance compared
to traditional numerical weather prediction (NWP) models and recent deep
learning methods. Quantitative evaluations and visual analyses in Beijing and
Shanghai confirm the model's capability to capture fine-grained meteorological
patterns. The proposed framework provides a scalable and label-efficient
solution for future data-driven weather forecasting systems.

</details>


### [194] [FLoRA: Fused forward-backward adapters for parameter efficient fine-tuning and reducing inference-time latencies of LLMs](https://arxiv.org/abs/2511.00050)
*Dhananjaya Gowda,Seoha Song,Junhyun Lee,Harshith Goka*

Main category: cs.LG

TL;DR: Proposes FLoRA: fused forward-backward adapters for parameter-efficient fine-tuning of LLMs, achieving higher accuracy and lower latency than LoRA at similar parameter budgets.


<details>
  <summary>Details</summary>
Motivation: As LLMs grow, efficient PEFT is essential. While LoRA and parallel adapters exist, there remain trade-offs between accuracy and latency; FFBA seeks better performance within a fixed parameter budget.

Method: Introduce FFBA (FLoRA): fused forward-backward adapters that integrate with existing projection layers by merging forward and backward adapters, combining LoRA and parallel adapter ideas to reduce overhead.

Result: Empirical results show FFBA significantly outperforms LoRA in both accuracy and latency for a similar parameter budget.

Conclusion: FFBA provides a more efficient PEFT solution by fusing adapters, achieving better accuracy-latency trade-offs than LoRA while maintaining a fixed parameter budget.

Abstract: As the large language models (LLMs) grow in size each day, efficient training
and fine-tuning has never been as important as nowadays. This resulted in the
great interest in parameter efficient fine-tuning (PEFT), and effective methods
including low-rank adapters (LoRA) has emerged. Although the various PEFT
methods have been studied extensively in the recent years, the greater part of
the subject remains unexplored with the huge degree of freedom. In this paper,
we propose FLoRA, a family of fused forward-backward adapters (FFBA) for
parameter-efficient fine-tuning of LLMs on downstream tasks. The FFBA combine
ideas from the popular LoRA and parallel adapters to improve the overall
fine-tuning accuracies. At the same time, latencies are minimized by fusing the
forward and backward adapters into existing projection layers of the base
model. Experimental results show that the proposed FFB adapters perform
significantly better than the popularly used LoRA in both accuracy and latency
for a similar parameter budget.

</details>


### [195] [Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT](https://arxiv.org/abs/2511.00051)
*Da Chang,Peng Xue,Yu Li,Yongxiang Liu,Pengxiang Xu,Shixun Zhang*

Main category: cs.LG

TL;DR: DoRAâs success is attributed to increasing singular value entropy of the weight-update matrix, leading to more uniform updates similar to full fine-tuning. The authors reformulate DoRA as an efficient, learnable weight-conditioning matrix and propose a unified PEFT framework along two orthogonal axes: architectural placement and transformation type. They introduce Pre-Diag (diagonal conditioning before LoRA) and SORA (norm-preserving, learnable orthogonal rotation). Experiments on NLP understanding and generation show improvements in both performance and efficiency over LoRA and DoRA, with code available at the provided GitHub link.


<details>
  <summary>Details</summary>
Motivation: Clarify the mechanism behind DoRA, address its computational overhead, and establish a general framework to guide the design of more effective PEFT methods.

Method: Mathematically reformulate DoRA as a learnable weight-conditioning matrix; propose a unified framework that varies (i) where the conditioning is placed in the architecture (placement) and (ii) the type of conditioning transformation (transformation class). Within this framework, present two new methods: (a) Pre-Diag, a diagonal conditioning matrix applied before the LoRA update to calibrate pretrained weights efficiently; (b) SORA, a parameter-efficient orthogonal rotation that performs a powerful, norm-preserving transformation of feature space.

Result: Extensive NLP experiments demonstrate that the proposed methods achieve superior performance and efficiency compared to both LoRA and DoRA.

Conclusion: A unified conditioning-based framework for PEFT design is feasible and fruitful. The two proposed methodsâPre-Diag and SORAâillustrate how placement and transformation choices yield performance gains with reduced training overhead, guiding future PEFT research.

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods are crucial for adapting large
pre-trained models. Among these, LoRA is considered a foundational approach.
Building on this, the influential DoRA method enhances performance by
decomposing weight updates into magnitude and direction. However, its
underlying mechanism remains unclear, and it introduces significant
computational overhead. In this work, we first identify that DoRA's success
stems from its capacity to increase the singular value entropy of the weight
update matrix, which promotes a more uniform update distribution akin to full
fine-tuning. We then reformulate DoRA into a mathematically equivalent and more
efficient matrix form, revealing it as a learnable weight conditioning method.
Based on this insight, we propose a unified framework for designing advanced
PEFT methods by exploring two orthogonal dimensions: the architectural
placement and the transformation type of the conditioning matrix. Within this
framework, we introduce two novel methods: (1) \textbf{Pre-Diag}, which applies
a diagonal conditioning matrix before the LoRA update to efficiently calibrate
the pre-trained weights, thereby enhancing performance while reducing training
time; and (2) \textbf{S}kewed \textbf{O}rthogonal \textbf{R}otation
\textbf{A}daptation (\textbf{SORA}), which employs a parameter-efficient
orthogonal rotation to perform a more powerful, norm-preserving transformation
of the feature space. Extensive experiments on natural language understanding
and generation tasks demonstrate that our proposed methods achieve superior
performance and efficiency compared to both LoRA and DoRA. The code is
available at https://github.com/MaeChd/SORA.

</details>


### [196] [Feature-Guided Analysis of Neural Networks: A Replication Study](https://arxiv.org/abs/2511.00052)
*Federico Formica,Stefano Gregis,Aurora Francesca Zanenga,Andrea Rota,Mark Lawford,Claudio Menghi*

Main category: cs.LG

TL;DR: Applying Feature-Guided Analysis (FGA) to a MNIST/LSC benchmark yields higher precision than prior studies; recall is sensitive to network architecture, training, and feature selection, while precision is largely robust.


<details>
  <summary>Details</summary>
Motivation: To provide empirical evidence of FGA's applicability in industrial/safety-critical contexts by evaluating it on a realistic benchmark and examining how modeling choices affect its explanations.

Method: Empirical evaluation of FGA on a MNIST+LSC benchmark to compute rules explaining neural network behavior; analysis of how network architecture, training, and feature selection influence FGA effectiveness (precision and recall).

Result: FGA achieved higher precision on the benchmark than results reported in the literature; recall is significantly affected by architecture, training, and feature selection, whereas precision is largely unaffected by these factors.

Conclusion: The study supports FGA's applicability on a realistic benchmark, showing robust precision but sensitivity of recall to modeling choices; optimizing architecture/training/feature selection can improve interpretability via FGA, though broader empirical validation is advised.

Abstract: Understanding why neural networks make certain decisions is pivotal for their
use in safety-critical applications. Feature-Guided Analysis (FGA) extracts
slices of neural networks relevant to their tasks. Existing feature-guided
approaches typically monitor the activation of the neural network neurons to
extract the relevant rules. Preliminary results are encouraging and demonstrate
the feasibility of this solution by assessing the precision and recall of
Feature-Guided Analysis on two pilot case studies. However, the applicability
in industrial contexts needs additional empirical evidence.
  To mitigate this need, this paper assesses the applicability of FGA on a
benchmark made by the MNIST and LSC datasets. We assessed the effectiveness of
FGA in computing rules that explain the behavior of the neural network. Our
results show that FGA has a higher precision on our benchmark than the results
from the literature. We also evaluated how the selection of the neural network
architecture, training, and feature selection affect the effectiveness of FGA.
Our results show that the selection significantly affects the recall of FGA,
while it has a negligible impact on its precision.

</details>


### [197] [Quadratic Direct Forecast for Training Multi-Step Time-Series Forecast Models](https://arxiv.org/abs/2511.00053)
*Hao Wang,Licheng Pan,Yuan Lu,Zhichao Chen,Tianqiao Liu,Shuting He,Zhixuan Chu,Qingsong Wen,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: Quadratic Direct Forecast (QDF): a quadratic-form weighted loss that learns a weighting matrix to capture label autocorrelation and heterogeneity across forecast horizons, improving multi-step time-series forecasting with adaptive weighting and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Standard MSE-based objectives treat each future step independently with equal weight, ignoring autocorrelation among labels and the varying importance of forecast horizons, which biases training and limits performance.

Method: Introduce a weighting matrix W for the multi-step forecast loss where off-diagonal entries model label autocorrelation and diagonal entries provide heterogeneous horizon weights. Employ an adaptive QDF learning algorithm that updates W during training, enabling a quadratic-form loss (e.g., y^T W y) that couples all steps.

Result: Experiments show QDF improves the performance of various forecast models, achieving state-of-the-art results on the tested benchmarks.

Conclusion: A quadratic-form weighting objective effectively addresses both label autocorrelation and horizon-specific weighting, yielding improved multi-step forecast accuracy and broad applicability; code is publicly available.

Abstract: The design of training objective is central to training time-series
forecasting models. Existing training objectives such as mean squared error
mostly treat each future step as an independent, equally weighted task, which
we found leading to the following two issues: (1) overlook the label
autocorrelation effect among future steps, leading to biased training
objective; (2) fail to set heterogeneous task weights for different forecasting
tasks corresponding to varying future steps, limiting the forecasting
performance. To fill this gap, we propose a novel quadratic-form weighted
training objective, addressing both of the issues simultaneously. Specifically,
the off-diagonal elements of the weighting matrix account for the label
autocorrelation effect, whereas the non-uniform diagonals are expected to match
the most preferable weights of the forecasting tasks with varying future steps.
To achieve this, we propose a Quadratic Direct Forecast (QDF) learning
algorithm, which trains the forecast model using the adaptively updated
quadratic-form weighting matrix. Experiments show that our QDF effectively
improves performance of various forecast models, achieving state-of-the-art
results. Code is available at https://anonymous.4open.science/r/QDF-8937.

</details>


### [198] [SpatialTraceGen: High-Fidelity Traces for Efficient VLM Spatial Reasoning Distillation](https://arxiv.org/abs/2511.00054)
*Gio Huh,Dhruv Sheth,Rayhan Zirvi,Frank Xiao*

Main category: cs.LG

TL;DR: A scalable method (SpatialTraceGen) to distill large-model reasoning into high-quality, multi-hop, multi-tool reasoning traces for vision-language models, using an automated verifier to ensure fidelity. This data enables efficient fine-tuning and offline RL while reducing annotation cost.


<details>
  <summary>Details</summary>
Motivation: To overcome the data bottleneck for training VLMs on complex spatial reasoning, which requires step-by-step reasoning traces and strategic tool use that are costly to annotate manually.

Method: Distill reasoning from a large teacher into a dataset of multi-hop, multi-tool traces (SpatialTraceGen). Introduce an automated Verifier that scales fidelity checks for each reasoning step, reducing reliance on human annotation.

Result: On CLEVR-Humans, verifier-guided traces show a 17% improvement in average trace quality and a >40% reduction in quality variance. The approach yields an expert-trace dataset suitable for fine-tuning and sample-efficient offline reinforcement learning.

Conclusion: SpatialTraceGen delivers high-quality, structured reasoning traces and a scalable verification pipeline, enabling effective fine-tuning and offline RL for VLMs in complex spatial tasks while lowering annotation costs.

Abstract: While Vision-Language Models (VLMs) excel in many areas, they struggle with
complex spatial reasoning, which requires problem decomposition and strategic
tool use. Fine-tuning smaller, more deployable models offers an efficient path
to strong performance, but this is hampered by a major bottleneck: the absence
of high-quality, step-by-step reasoning data. To address this data-efficiency
gap, we introduce SpatialTraceGen, a framework to distill the reasoning
processes of a large teacher model into a high-quality dataset of multi-hop,
multi-tool reasoning traces. A key innovation is our automated Verifier, which
scalably ensures the fidelity of each reasoning step, providing a
cost-effective alternative to manual human annotation. On the CLEVR-Humans
benchmark, this verifier-guided process improves the average quality score of
traces by 17\% while reducing quality variance by over 40\%. SpatialTraceGen
delivers a dataset of expert traces, providing the structured, step-by-step
examples of tool use necessary for effective fine-tuning and sample-efficient
offline reinforcement learning.

</details>


### [199] [Exploring Federated Learning for Thermal Urban Feature Segmentation -- A Comparison of Centralized and Decentralized Approaches](https://arxiv.org/abs/2511.00055)
*Leonhard Duda,Khadijeh Alibabaei,Elena Vollmer,Leon Klug,Valentin Kozlov,Lisana Berberi,Mishal Benz,Rebekka Volk,Juan Pedro GutiÃ©rrez Hermosillo Muriedas,Markus GÃ¶tz,Judith SÃ¡Ã­nz-Pardo DÃ­az,Ãlvaro LÃ³pez GarcÃ­a,Frank Schultmann,Achim Streit*

Main category: cs.LG

TL;DR: Federated Learning (FL) is evaluated for UAV-based thermal-image segmentation across two German cities, focusing on real-world deployment vs centralized learning. The study compares FL workflows and several FL approaches under non-identical data distributions, reporting on accuracy, training time, communication overhead, and energy use; it discusses practical FL limitations and provides insights for real-world UAV imaging applications.


<details>
  <summary>Details</summary>
Motivation: The need to train ML models without sharing raw data due to privacy/technical constraints, particularly for UAV-based urban thermal imaging where data collection is distributed and heterogeneous.

Method: Real-world deployment study comparing multiple FL approaches against a centralized baseline. Data collected from UAVs in two German cities, with non-identical distributions. Evaluation across accuracy, training time, communication overhead, and energy use. Comparison of client-controlled versus server-controlled FL workflows. Likely inclusion of standard FL algorithms (e.g., FedAvg) applied to UAV thermal image segmentation.

Result: FL methods can approach centralized baseline performance but exhibit trade-offs. Non-iid data across cities impacts accuracy. FL increases training time and communication/energy overhead due to distributed optimization and wireless transmission. Server-controlled workflows may reduce client-side burden; client-controlled workflows give more autonomy but may increase coordination complexity. Real-world deployment reveals practical limitations and operational considerations not captured in simulations.

Conclusion: Real-world FL deployment for UAV-based segmentation is feasible but comes with notable practical challenges, including data heterogeneity, resource constraints, and workflow choices. The study provides empirical guidance on which FL workflows to adopt and highlights areas for future improvement in robustness, efficiency, and deployment strategies.

Abstract: Federated Learning (FL) is an approach for training a shared Machine Learning
(ML) model with distributed training data and multiple participants. FL allows
bypassing limitations of the traditional Centralized Machine Learning CL if
data cannot be shared or stored centrally due to privacy or technical
restrictions -- the participants train the model locally with their training
data and do not need to share it among the other participants. This paper
investigates the practical implementation and effectiveness of FL in a
real-world scenario, specifically focusing on unmanned aerial vehicle
(UAV)-based thermal images for common thermal feature detection in urban
environments. The distributed nature of the data arises naturally and makes it
suitable for FL applications, as images captured in two German cities are
available. This application presents unique challenges due to non-identical
distribution and feature characteristics of data captured at both locations.
The study makes several key contributions by evaluating FL algorithms in real
deployment scenarios rather than simulation. We compare several FL approaches
with a centralized learning baseline across key performance metrics such as
model accuracy, training time, communication overhead, and energy usage. This
paper also explores various FL workflows, comparing client-controlled workflows
and server-controlled workflows. The findings of this work serve as a valuable
reference for understanding the practical application and limitations of the FL
methods in segmentation tasks in UAV-based imaging.

</details>


### [200] [MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling](https://arxiv.org/abs/2511.00056)
*Yuxi Liu,Renjia Deng,Yutong He,Xue Wang,Tao Yao,Kun Yuan*

Main category: cs.LG

TL;DR: Proposes Module-wise Importance Sampling (MISA) for memory-efficient training of LLMs by partitioning each transformer layer into modules, assigning importances, and activating modules via weighted sampling to reduce gradient variance and achieve provable convergence; shows memory and performance gains over layer-wise methods; code available.


<details>
  <summary>Details</summary>
Motivation: Memory bottlenecks in pre-training and fine-tuning LLMs; layer-wise optimization reduces memory but ignores intra-layer module differences and still requires at least one full layer active; need finer-grained, variance-reducing, memory-efficient optimization.

Method: Split each transformer layer into smaller modules, assign an importance score to each module, perform weighted random sampling to activate a subset of modules during optimization, derive variance reduction over naive layer-wise sampling, prove an O(1/âK) convergence rate under non-convex stochastic conditions, and provide a memory-analysis comparing against baselines; experiments on diverse tasks; release code.

Result: Demonstrates improved memory efficiency and training effectiveness compared to baselines; empirical validation across tasks; convergence guarantee; notable memory reductions; code available.

Conclusion: MISA enables finer-grained, memory-efficient optimization by respecting module importance, surpassing layer-wise sampling; offers provable convergence and practical gains for training large LMs; potential for broader adoption and future work in designing module-level sampling schemes.

Abstract: The substantial memory demands of pre-training and fine-tuning large language
models (LLMs) require memory-efficient optimization algorithms. One promising
approach is layer-wise optimization, which treats each transformer block as a
single layer and optimizes it sequentially, while freezing the other layers to
save optimizer states and activations. Although effective, these methods ignore
the varying importance of the modules within each layer, leading to suboptimal
performance. Moreover, layer-wise sampling provides only limited memory
savings, as at least one full layer must remain active during optimization. To
overcome these limitations, we propose Module-wise Importance SAmpling (MISA),
a novel method that divides each layer into smaller modules and assigns
importance scores to each module. MISA uses a weighted random sampling
mechanism to activate modules, provably reducing gradient variance compared to
layer-wise sampling. Additionally, we establish an \(\mathcal{O}(1/\sqrt{K})\)
convergence rate under non-convex and stochastic conditions, where $K$ is the
total number of block updates, and provide a detailed memory analysis
showcasing MISA's superiority over existing baseline methods. Experiments on
diverse learning tasks validate the effectiveness of MISA. Source code is
available at https://github.com/pkumelon/MISA.

</details>


### [201] [Automatically Finding Rule-Based Neurons in OthelloGPT](https://arxiv.org/abs/2511.00059)
*Aditya Singh,Zihang Wen,Srujananjali Medicherla,Adam Karvonen,Can Rager*

Main category: cs.LG

TL;DR: A transformer for predicting valid Othello moves can be interpreted by mapping neuron activations to rule-based logic via decision trees; roughly half of neurons in a key layer are explainable by compact rules, with causal interventions validating these patterns; a Python tool is released to support further work.


<details>
  <summary>Details</summary>
Motivation: To understand how neural networks solve rule-based games, and whether neurons encode understandable, rule-like patterns that govern decision-making in OthelloGPT.

Method: Train regression decision trees to map board states to individual neuron activations, identify highly-active activations, extract decision paths, and translate them into human-readable logical forms; characterize how many neurons are well-explained by rules (R^2>0.7); perform targeted neuron ablations to test causal relevance of the identified patterns; provide a Python tool mapping rule-based game behaviors to implementing neurons.

Result: Approximately 50% of neurons in layer 5 (913 of 2,048) have interpretable rule-based descriptions with R^2 > 0.7; the remaining neurons likely capture distributed or non-rule-based computations. Ablating neurons associated with specific patterns yields 5â10Ã greater degradation in predicting legal moves along those patterns than control patterns, validating causal relevance.

Conclusion: Rule-based interpretations apply to a substantial subset of neurons in OthelloGPT, supporting the viability of extracting human-readable rules from neural computations in a constrained, rule-governed domain; the work provides a practical tool to map behaviors to neurons for future interpretability research.

Abstract: OthelloGPT, a transformer trained to predict valid moves in Othello, provides
an ideal testbed for interpretability research. The model is complex enough to
exhibit rich computational patterns, yet grounded in rule-based game logic that
enables meaningful reverse-engineering. We present an automated approach based
on decision trees to identify and interpret MLP neurons that encode rule-based
game logic. Our method trains regression decision trees to map board states to
neuron activations, then extracts decision paths where neurons are highly
active to convert them into human-readable logical forms. These descriptions
reveal highly interpretable patterns; for instance, neurons that specifically
detect when diagonal moves become legal. Our findings suggest that roughly half
of the neurons in layer 5 can be accurately described by compact, rule-based
decision trees ($R^2 > 0.7$ for 913 of 2,048 neurons), while the remainder
likely participate in more distributed or non-rule-based computations. We
verify the causal relevance of patterns identified by our decision trees
through targeted interventions. For a specific square, for specific game
patterns, we ablate neurons corresponding to those patterns and find an
approximately 5-10 fold stronger degradation in the model's ability to predict
legal moves along those patterns compared to control patterns. To facilitate
future work, we provide a Python tool that maps rule-based game behaviors to
their implementing neurons, serving as a resource for researchers to test
whether their interpretability methods recover meaningful computational
structures.

</details>


### [202] [EVINGCA: Adaptive Graph Clustering with Evolving Neighborhood Statistics](https://arxiv.org/abs/2511.00064)
*Randolph Wiredu-Aidoo*

Main category: cs.LG

TL;DR: Presents EVINGCA, a density-variance based, nonparametric clustering algorithm on an evolving nearest-neighbor graph, achieving adaptive cluster growth with BFS and local statistics; shows competitive performance and scalable log-linear average complexity.


<details>
  <summary>Details</summary>
Motivation: Current clustering methods rely on restrictive assumptions (K-Means and Gaussian Mixtures assume convex, Gaussian-like clusters; DBSCAN/HDBSCAN handle non-convexity but are sensitive to parameters). There is a need for a flexible, nonparametric approach that adapts to local density and shape without fixed thresholds.

Method: Construct rooted graphs and expand them via breadth-first search on a nearest-neighbor graph. Use continuously updated local distance and shape statistics to guide growth, replacing fixed density thresholds with local statistical feedback. Employ spatial indexing to achieve log-linear average-case complexity.

Result: EVINGCA achieves competitive performance against baselines across synthetic, real-world, low-dimensional, and high-dimensional datasets.

Conclusion: EVINGCA provides a robust, adaptive clustering framework that leverages evolving density-variance information through a nonparametric graph-based process, offering scalable performance across diverse data regimes.

Abstract: Clustering algorithms often rely on restrictive assumptions: K-Means and
Gaussian Mixtures presuppose convex, Gaussian-like clusters, while DBSCAN and
HDBSCAN capture non-convexity but can be highly sensitive. I introduce EVINGCA
(Evolving Variance-Informed Nonparametric Graph Construction Algorithm), a
density-variance based clustering algorithm that treats cluster formation as an
adaptive, evolving process on a nearest-neighbor graph. EVINGCA expands rooted
graphs via breadth-first search, guided by continuously updated local distance
and shape statistics, replacing fixed density thresholds with local statistical
feedback. With spatial indexing, EVINGCA features log-linear complexity in the
average case and exhibits competitive performance against baselines across a
variety of synthetic, real-world, low-d, and high-d datasets.

</details>


### [203] [Aligning Brain Signals with Multimodal Speech and Vision Embeddings](https://arxiv.org/abs/2511.00065)
*Kateryna Shapovalenko,Quentin Auster*

Main category: cs.LG

TL;DR: Layer-aware, multimodal embeddings from wav2vec2 and CLIP, analyzed with EEG during natural speech, show that combining representations across layers improves alignment with brain language processing and decoding, moving beyond viewing speech as mere sound to capture experience-derived meaning.


<details>
  <summary>Details</summary>
Motivation: To understand how hierarchical brain processing of language maps onto pre-trained model representations, and to identify which layers (and which modalities) best reflect neural activity during natural speech perception.

Method: Use EEG recorded during natural speech perception. Compare embeddings from two pre-trained models: wav2vec2 (audio-to-language) and CLIP (text-to-image). Evaluate alignment with brain signals via ridge regression encoding and contrastive decoding. Test three strategies: (1) individual layer representations, (2) progressive concatenation of layers, (3) progressive summation of layers.

Result: Findings indicate that combining multimodal, layer-aware representations improves alignment with neural data and supports better decoding of language-related brain activity, suggesting that brain processing reflects language as multimodal experience rather than acoustic sound alone.

Conclusion: Layer-aware, multimodal representations bring us closer to decoding how the brain understands language, capturing its transformation from sound to rich, experience-based meaning.

Abstract: When we hear the word "house", we don't just process sound, we imagine walls,
doors, memories. The brain builds meaning through layers, moving from raw
acoustics to rich, multimodal associations. Inspired by this, we build on
recent work from Meta that aligned EEG signals with averaged wav2vec2 speech
embeddings, and ask a deeper question: which layers of pre-trained models best
reflect this layered processing in the brain? We compare embeddings from two
models: wav2vec2, which encodes sound into language, and CLIP, which maps words
to images. Using EEG recorded during natural speech perception, we evaluate how
these embeddings align with brain activity using ridge regression and
contrastive decoding. We test three strategies: individual layers, progressive
concatenation, and progressive summation. The findings suggest that combining
multimodal, layer-aware representations may bring us closer to decoding how the
brain understands language, not just as sound, but as experience.

</details>


### [204] [Token-Regulated Group Relative Policy Optimization for Stable Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2511.00066)
*Tue Le,Nghi D. Q. Bui,Linh Ngo Van,Trung Le*

Main category: cs.LG

TL;DR: TR-GRPO extends GRPO by applying token-level weights tied to the model's predicted probability, downweighting low-probability tokens to stabilize gradients and improve learning, yielding better performance in RLVR tasks.


<details>
  <summary>Details</summary>
Motivation: GRPO can be dominated by gradients from rare, low-probability tokens, which destabilizes training and masks learning from higher-probability tokens that are more reliable for improving LLM reasoning.

Method: Introduce Token-Regulated GRPO (TR-GRPO) that assigns token-level weights positively correlated with the model's predicted probability, downweighting low-probability tokens while preserving informative learning signals.

Result: TR-GRPO consistently outperforms GRPO across RLVR tasks (logic, math, agentic reasoning), demonstrating stronger learning stability and improved reasoning capabilities.

Conclusion: Regulating token contributions during RL training is crucial for robust LLM reasoning; TR-GRPO provides a simple, effective framework that enhances GRPO performance for RLVR.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful approach for strengthening the reasoning capabilities of large
language models (LLMs). Among existing algorithms, Group Relative Policy
Optimization (GRPO) has demonstrated strong performance, yet it suffers from a
critical issue: low-probability tokens disproportionately dominate gradient
updates due to their inherently large gradient magnitudes. This imbalance leads
to unstable training and suppresses the contribution of high-probability tokens
that are more reliable for learning. In this work, we introduce Token-Regulated
Group Relative Policy Optimization (TR-GRPO), a simple yet effective extension
of GRPO that assigns token-level weights positively correlated with the model's
predicted probability. By downweighting low-probability tokens and emphasizing
high-probability ones, TR-GRPO mitigates gradient over-amplification while
preserving informative learning signals. Extensive experiments demonstrate that
TR-GRPO consistently outperforms GRPO across RLVR tasks, including logic, math,
and agentic reasoning, highlighting the importance of regulating token
contributions during RL training and establishing TR-GRPO as a robust framework
for enhancing LLM reasoning.

</details>


### [205] [Latent Domain Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2511.00067)
*Zhixing Li,Arsham Gholamzadeh Khoee,Yinan Yu*

Main category: cs.LG

TL;DR: Proposes a domain-generalization method for vision-language models without explicit domain labels by discovering latent domains via clustering and fuse domain-specific text features, achieving consistent gains on four benchmarks.


<details>
  <summary>Details</summary>
Motivation: Domain generalization aims to be robust to domain shift, but relying on domain labels is impractical in real-world settings. For vision-language models, robust cross-domain performance without labeled domains is especially valuable.

Method: Perform latent domain clustering on image features to discover unseen latent domains. For each latent domain, use domain-specific text features and fuse them based on the similarity between the input image and each latent domain. Represent an unseen target as a mixture of these latent domains and adapt knowledge across domains during inference.

Result: Empirical results on four benchmarks show consistent improvements over VLM-based baselines, providing new insights into improving robustness under domain shift.

Conclusion: Latent-domain discovery with domain-specific text feature fusion offers an effective, label-free approach to domain generalization for vision-language models and helps illuminate strategies to improve robustness under domain shift.

Abstract: The objective of domain generalization (DG) is to enable models to be robust
against domain shift. DG is crucial for deploying vision-language models (VLMs)
in real-world applications, yet most existing methods rely on domain labels
that may not be available and often ambiguous. We instead study the DG setting
where models must generalize well without access to explicit domain labels. Our
key idea is to represent an unseen target domain as a combination of latent
domains automatically discovered from training data, enabling the model to
adaptively transfer knowledge across domains. To realize this, we perform
latent domain clustering on image features and fuse domain-specific text
features based on the similarity between the input image and each latent
domain. Experiments on four benchmarks show that this strategy yields
consistent gains over VLM-based baselines and provides new insights into
improving robustness under domain shift.

</details>


### [206] [Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design](https://arxiv.org/abs/2511.00070)
*Muhammad Bilal Awan,Abdul Razzaq,Abdul Shahid*

Main category: cs.LG

TL;DR: BoTorch qEHVI shows perfect convergence; tuned LLMs (WizardMath-7B) are competitive and faster but do not surpass specialized Bayesian optimization. The study provides comparative metrics for AI-driven optimization in constrained multi-objective inverse design, with industrial relevance to resins, polymers, and paints.


<details>
  <summary>Details</summary>
Motivation: Evaluate whether large language models can serve as generative optimizers in constrained, continuous, high-dimensional multi-objective inverse design tasks, and benchmark them against established Bayesian optimization frameworks for property-to-structure mappings in materials informatics.

Method: Benchmark a standard BoTorch Ax implementation against qEHVI (BoTorchM) for Bayesian optimization, and fine-tune LLMs and BERT via Parameter-Efficient Fine-Tuning on a regression framing with a custom output head. Treat the problem as constrained, multi-objective optimization and measure convergence via Generational Distance (GD).

Result: BoTorch qEHVI achieved perfect convergence (GD=0.0). The best fine-tuned LLM (WizardMath-7B) achieved GD=1.21, significantly outperforming the BoTorch Ax baseline (GD=15.03). LLMs offer faster computation and competitive performance, but specialized BO frameworks maintain the performance ceiling for guaranteed convergence.

Conclusion: Specialized Bayesian optimization frameworks remain the leading approach for guaranteed convergence in constrained multi-objective optimization, but fine-tuned LLMs are a promising, computationally faster alternative with tangible comparative metrics, relevant for industrial formulation design where trade-offs among mechanical, rheological, and chemical properties are critical.

Abstract: This paper investigates the performance of Large Language Models (LLMs) as
generative optimizers for solving constrained multi-objective regression tasks,
specifically within the challenging domain of inverse design
(property-to-structure mapping). This problem, critical to materials
informatics, demands finding complex, feasible input vectors that lie on the
Pareto optimal front. While LLMs have demonstrated universal effectiveness
across generative and reasoning tasks, their utility in constrained,
continuous, high-dimensional numerical spaces tasks they weren't explicitly
architected for remains an open research question. We conducted a rigorous
comparative study between established Bayesian Optimization (BO) frameworks and
a suite of fine-tuned LLMs and BERT models. For BO, we benchmarked the
foundational BoTorch Ax implementation against the state-of-the-art q-Expected
Hypervolume Improvement (qEHVI, BoTorchM). The generative approach involved
fine-tuning models via Parameter-Efficient Fine-Tuning (PEFT), framing the
challenge as a regression problem with a custom output head. Our results show
that BoTorch qEHVI achieved perfect convergence (GD=0.0), setting the
performance ceiling. Crucially, the best-performing LLM (WizardMath-7B)
achieved a Generational Distance (GD) of 1.21, significantly outperforming the
traditional BoTorch Ax baseline (GD=15.03). We conclude that specialized BO
frameworks remain the performance leader for guaranteed convergence, but
fine-tuned LLMs are validated as a promising, computationally fast alternative,
contributing essential comparative metrics to the field of AI-driven
optimization. The findings have direct industrial applications in optimizing
formulation design for resins, polymers, and paints, where multi-objective
trade-offs between mechanical, rheological, and chemical properties are
critical to innovation and production efficiency.

</details>


### [207] [Wavelet-Based Feature Extraction and Unsupervised Clustering for Parity Detection: A Feature Engineering Perspective](https://arxiv.org/abs/2511.00071)
*Ertugrul Mutlu*

Main category: cs.LG

TL;DR: Unsupervised parity detection using wavelet-based features and k-means achieves ~69.7% accuracy, suggesting DSP-inspired feature extraction can reveal structure in discrete data.


<details>
  <summary>Details</summary>
Motivation: Explore whether classical signal-processing techniques can uncover latent structure for a purely symbolic problem (parity) and bridge symbolic reasoning with feature-based learning.

Method: Transform integers into wavelet-domain representations, extract multi-scale statistical features, and cluster with k-means; evaluate parity separation without label supervision.

Result: Achieves approximately 69.67% accuracy for parity classification in an unsupervised setting, indicating meaningful structure in the wavelet feature space.

Conclusion: Demonstrates that traditional DSP methods can uncover latent structure in discrete domains and highlights potential for repurposing feature engineering and clustering for unconventional machine learning problems.

Abstract: This paper explores a deliberately over-engineered approach to the classical
problem of parity detection -- determining whether a number is odd or even --
by combining wavelet-based feature extraction with unsupervised clustering.
Instead of relying on modular arithmetic, integers are transformed into
wavelet-domain representations, from which multi-scale statistical features are
extracted and clustered using the k-means algorithm. The resulting feature
space reveals meaningful structural differences between odd and even numbers,
achieving a classification accuracy of approximately 69.67% without any label
supervision. These results suggest that classical signal-processing techniques,
originally designed for continuous data, can uncover latent structure even in
purely discrete symbolic domains. Beyond parity detection, the study provides
an illustrative perspective on how feature engineering and clustering may be
repurposed for unconventional machine learning problems, potentially bridging
symbolic reasoning and feature-based learning.

</details>


### [208] [Bridging Vision, Language, and Mathematics: Pictographic Character Reconstruction with BÃ©zier Curves](https://arxiv.org/abs/2511.00076)
*Zihao Wan,Pau Tong Lin Xu,Fuwen Luo,Ziyue Wang,Peng Li,Yang Liu*

Main category: cs.LG

TL;DR: A vision-language model learns to decompile raster pictographic characters into programs of Bezier curves, acting as a visual decompiler; it generalizes from modern Chinese characters to ancient Oracle Bone Script in zero-shot, evidencing an abstract, transferable geometric grammar.


<details>
  <summary>Details</summary>
Motivation: To determine whether VLMs can infer the underlying geometric and structural grammar of visual symbols beyond pixel patterns, using pictographs as a testbed and representing characters as executable geometric programs.

Method: Represent each character with an executable program composed of Bezier curves and train a VLM to synthesize this program from the raster image (a program synthesis/decompilation task). Evaluate zero-shot generalization to Oracle Bone Script after training on modern Chinese characters.

Result: The model outperforms strong zero-shot baselines, including GPT-4o, and can reconstruct Oracle Bone Script in zero-shot when trained solely on modern Chinese characters.

Conclusion: The model appears to learn an abstract and transferable geometric grammar, enabling structured visual understanding that goes beyond pixel-level recognition.

Abstract: While Vision-language Models (VLMs) have demonstrated strong semantic
capabilities, their ability to interpret the underlying geometric structure of
visual information is less explored. Pictographic characters, which combine
visual form with symbolic structure, provide an ideal test case for this
capability. We formulate this visual recognition challenge in the mathematical
domain, where each character is represented by an executable program of
geometric primitives. This is framed as a program synthesis task, training a
VLM to decompile raster images into programs composed of B\'ezier curves. Our
model, acting as a "visual decompiler", demonstrates performance superior to
strong zero-shot baselines, including GPT-4o. The most significant finding is
that when trained solely on modern Chinese characters, the model is able to
reconstruct ancient Oracle Bone Script in a zero-shot context. This
generalization provides strong evidence that the model acquires an abstract and
transferable geometric grammar, moving beyond pixel-level pattern recognition
to a more structured form of visual understanding.

</details>


### [209] [flowengineR: A Modular and Extensible Framework for Fair and Reproducible Workflow Design in R](https://arxiv.org/abs/2511.00079)
*Maximilian Willer,Peter Ruckdeschel*

Main category: cs.LG

TL;DR: FlowengineR is an R-based modular framework that encodes reproducible ML workflows as interchangeable engines, motivated by evolving algorithmic fairness research and designed for transparency, extensibility, and cross-domain applicability.


<details>
  <summary>Details</summary>
Motivation: The rapid emergence of new fairness metrics, mitigation strategies, and ML methods creates a need for reproducible, extensible workflows. Existing toolkits are often narrow or treat reproducibility as secondary. The paper motivates a general infrastructure to support evolving algorithmic fairness research while remaining broadly applicable.

Method: Proposes a unified architecture of standardized engines for data splitting, execution, preprocessing, training, inprocessing, postprocessing, evaluation, and reporting. Each engine handles a distinct task but communicates via a lightweight interface. The design emphasizes straightforward setup and management of engines as data structures (orthogonalization) over orchestrating parallel execution. It draws inspiration from CWL, YAWL, KNIME, and R frameworks like BatchJobs/batchtools, but focuses on modularity and extensibility rather than distributed orchestration.

Result: Establishes a general infrastructure enabling researchers to plug in, compare, and evaluate different interventions across the modeling pipeline, not limited to fairness. The architecture supports explainability, robustness, and compliance metrics with minimal core changes, enabling transparent, auditable workflows and easy integration of new methods.

Conclusion: FlowengineR offers a general, reusable infrastructure for reproducible, transparent, and extensible ML workflows, particularly beneficial for fairness research but applicable to any domain requiring structured pipelines and modular engines.

Abstract: flowengineR is an R package designed to provide a modular and extensible
framework for building reproducible algorithmic workflows for general-purpose
machine learning pipelines. It is motivated by the rapidly evolving field of
algorithmic fairness where new metrics, mitigation strategies, and machine
learning methods continuously emerge. A central challenge in fairness, but also
far beyond, is that existing toolkits either focus narrowly on single
interventions or treat reproducibility and extensibility as secondary
considerations rather than core design principles. flowengineR addresses this
by introducing a unified architecture of standardized engines for data
splitting, execution, preprocessing, training, inprocessing, postprocessing,
evaluation, and reporting. Each engine encapsulates one methodological task yet
communicates via a lightweight interface, ensuring workflows remain
transparent, auditable, and easily extensible. Although implemented in R,
flowengineR builds on ideas from workflow languages (CWL, YAWL), graph-oriented
visual programming languages (KNIME), and R frameworks (BatchJobs, batchtools).
Its emphasis, however, is less on orchestrating engines for resilient parallel
execution but rather on the straightforward setup and management of distinct
engines as data structures. This orthogonalization enables distributed
responsibilities, independent development, and streamlined integration. In
fairness context, by structuring fairness methods as interchangeable engines,
flowengineR lets researchers integrate, compare, and evaluate interventions
across the modeling pipeline. At the same time, the architecture generalizes to
explainability, robustness, and compliance metrics without core modifications.
While motivated by fairness, it ultimately provides a general infrastructure
for any workflow context where reproducibility, transparency, and extensibility
are essential.

</details>


### [210] [Fixed-point graph convolutional networks against adversarial attacks](https://arxiv.org/abs/2511.00083)
*Shakib Khan,A. Ben Hamza,Amr Youssef*

Main category: cs.LG

TL;DR: Fix-GCN uses a fixed-point inspired propagation with a spectral modulation filter to achieve adversarial robustness in graph neural networks without extra memory/computation, by preserving low-frequency structural info while attenuating high-frequency perturbations and iteratively updating node features.


<details>
  <summary>Details</summary>
Motivation: Adversarial attacks threaten GNNs by perturbing structure and features; robust defenses often add complexity. The paper aims to achieve robustness efficiently by exploiting higher-order neighborhood information through a fixed-point propagation framework.

Method: Propose fixed-point iterative graph convolutional network (Fix-GCN) with a versatile spectral modulation filter; derive feature propagation rule via fixed-point iteration; implement flexible-pass filtering to suppress high-frequency noise while preserving low-frequency structure; no extra memory or computational cost; iterative node feature updates.

Result: Extensive experiments on benchmark graph datasets demonstrate resilience against adversarial attacks, indicating improved robustness without increased resource requirements.

Conclusion: Fix-GCN provides a flexible and efficient defense mechanism against adversarial perturbations by maintaining essential graph information through fixed-point propagation and spectral filtering, suggesting a practical direction for robust GNNs.

Abstract: Adversarial attacks present a significant risk to the integrity and
performance of graph neural networks, particularly in tasks where graph
structure and node features are vulnerable to manipulation. In this paper, we
present a novel model, called fixed-point iterative graph convolutional network
(Fix-GCN), which achieves robustness against adversarial perturbations by
effectively capturing higher-order node neighborhood information in the graph
without additional memory or computational complexity. Specifically, we
introduce a versatile spectral modulation filter and derive the feature
propagation rule of our model using fixed-point iteration. Unlike traditional
defense mechanisms that rely on additional design elements to counteract
attacks, the proposed graph filter provides a flexible-pass filtering approach,
allowing it to selectively attenuate high-frequency components while preserving
low-frequency structural information in the graph signal. By iteratively
updating node representations, our model offers a flexible and efficient
framework for preserving essential graph information while mitigating the
impact of adversarial manipulation. We demonstrate the effectiveness of the
proposed model through extensive experiments on various benchmark graph
datasets, showcasing its resilience against adversarial attacks.

</details>


### [211] [Application of predictive machine learning in pen & paper RPG game design](https://arxiv.org/abs/2511.00084)
*Jolanta Åliwa*

Main category: cs.LG

TL;DR: Survey and evaluation of ML methods for predicting monster level in pen-and-paper RPGs; introduces a dedicated dataset, a human-inspired benchmark, and a domain-grounded evaluation procedure.


<details>
  <summary>Details</summary>
Motivation: Growing pen-and-paper RPG market and publishers' interest in AI to design foes and estimate challenge; current automated methods do not exist, making manual, time-consuming processes the default.

Method: Review of state-of-the-art ordinal regression methods; creation of a dedicated level-estimation dataset; development of a human-inspired benchmark model; and a domain-informed evaluation procedure for fair comparisons.

Result: Provides an overview and evaluation of current methods; constructs a dataset; introduces a benchmark; and proposes a domain-specific evaluation protocol to enable meaningful comparisons between ML models and publisher approaches.

Conclusion: Establishes groundwork for automated monster-level prediction and offers a practical benchmark and evaluation framework for future research in RPG AI.

Abstract: In recent years, the pen and paper RPG market has experienced significant
growth. As a result, companies are increasingly exploring the integration of AI
technologies to enhance player experience and gain a competitive edge.
  One of the key challenges faced by publishers is designing new opponents and
estimating their challenge level. Currently, there are no automated methods for
determining a monster's level; the only approaches used are based on manual
testing and expert evaluation. Although these manual methods can provide
reasonably accurate estimates, they are time-consuming and resource-intensive.
  Level prediction can be approached using ordinal regression techniques. This
thesis presents an overview and evaluation of state-of-the-art methods for this
task. It also details the construction of a dedicated dataset for level
estimation. Furthermore, a human-inspired model was developed to serve as a
benchmark, allowing comparison between machine learning algorithms and the
approach typically employed by pen and paper RPG publishers. In addition, a
specialized evaluation procedure, grounded in domain knowledge, was designed to
assess model performance and facilitate meaningful comparisons.

</details>


### [212] [MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning](https://arxiv.org/abs/2511.00085)
*Peilin Tan,Chuanqi Shi,Dian Tu,Liang Xie*

Main category: cs.LG

TL;DR: MaGNet presents a Mamba dual-hyperGraph network for stock prediction, introducing (i) a MAGE block with bidirectional Mamba, adaptive gating, sparse MoE and multi-head attention; (ii) 2D spatiotemporal attention for feature and cross-stock fusion; (iii) a dual hypergraph with Temporal-Causal Hypergraph and Global Probabilistic Hypergraph using Jensen-Shannon weighting. Demonstrates superior predictive performance and investment returns across six index datasets, with code available.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling complex temporal dependencies, dynamic inter-stock interactions, and cross-sectional market influences in stock prediction. Existing methods rely on static correlations and uniform treatments, failing to capture multi-scale, dynamic relational patterns and robust risk management.

Method: MaGNet integrates three innovations: (1) MAGE block: bidirectional Mamba for contextual temporal modeling, adaptive gating, and a sparse MoE layer with multi-head attention for global dependencies; (2) Feature-wise and Stock-wise 2D Spatiotemporal Attention to fuse multivariate features while preserving data structure; (3) A dual hypergraph framework: Temporal-Causal Hypergraph (TCH) to capture fine-grained temporal causal dependencies with constraints, and Global Probabilistic Hypergraph (GPH) to model market-wide patterns via soft hyperedge assignments and Jensen-Shannon Divergence weighting, enabling separation of localized temporal effects from instantaneous global structures for multi-scale learning.

Result: Empirical evaluation on six major stock indices shows MaGNet achieves state-of-the-art predictive performance and enhanced investment returns with robust risk management capabilities. The authors provide public code (GitHub).

Conclusion: The proposed MaGNet effectively disentangles localized temporal influences from global market structures through a dual-hypergraph framework and advanced temporal fusion, delivering superior predictive accuracy and trading performance across diverse markets.

Abstract: Stock trend prediction is crucial for profitable trading strategies and
portfolio management yet remains challenging due to market volatility, complex
temporal dynamics and multifaceted inter-stock relationships. Existing methods
struggle to effectively capture temporal dependencies and dynamic inter-stock
interactions, often neglecting cross-sectional market influences, relying on
static correlations, employing uniform treatments of nodes and edges, and
conflating diverse relationships. This work introduces MaGNet, a novel Mamba
dual-hyperGraph Network for stock prediction, integrating three key
innovations: (1) a MAGE block, which leverages bidirectional Mamba with
adaptive gating mechanisms for contextual temporal modeling and integrates a
sparse Mixture-of-Experts layer to enable dynamic adaptation to diverse market
conditions, alongside multi-head attention for capturing global dependencies;
(2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules enable
precise fusion of multivariate features and cross-stock dependencies,
effectively enhancing informativeness while preserving intrinsic data
structures, bridging temporal modeling with relational reasoning; and (3) a
dual hypergraph framework consisting of the Temporal-Causal Hypergraph (TCH)
that captures fine-grained causal dependencies with temporal constraints, and
Global Probabilistic Hypergraph (GPH) that models market-wide patterns through
soft hyperedge assignments and Jensen-Shannon Divergence weighting mechanism,
jointly disentangling localized temporal influences from instantaneous global
structures for multi-scale relational learning. Extensive experiments on six
major stock indices demonstrate MaGNet outperforms state-of-the-art methods in
both superior predictive performance and exceptional investment returns with
robust risk management capabilities. Codes available at:
https://github.com/PeilinTime/MaGNet.

</details>


### [213] [Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph](https://arxiv.org/abs/2511.00086)
*Fali Wang,Jihai Chen,Shuhua Yang,Runxue Bao,Tianxiang Zhao,Zhiwei Zhang,Xianfeng Tang,Hui Liu,Qi He,Suhang Wang*

Main category: cs.LG

TL;DR: Introduces Agent-REINFORCE for search of compute-optimal multi-LLM collaboration graphs in Test-Time Scaling under budget, using probabilistic graph optimization and LLM-agent feedback akin to REINFORCE.


<details>
  <summary>Details</summary>
Motivation: LLMs in Test-Time Scaling face fixed architectures and single-model usage; there is a need to discover compute-efficient, task-specific collaboration graphs that maximize performance under latency constraints.

Method: Formulate as probabilistic graph optimization over a multi-LLM collaboration graph with nodes representing roles and model assignments, derive three empirical insights from pilot experiments, and implement Agent-REINFORCEâan LLM-agent-augmented REINFORCE-like pipeline that updates the graph via sampling-feedback updates using textual gradients from feedback.

Result: Agent-REINFORCE outperforms traditional and LLM-based baselines in sample efficiency and search performance, effectively identifying optimal graphs under joint accuracy and inference-latency objectives.

Conclusion: The framework can efficiently search for compute-optimal, task-tailored multi-LLM collaboration graphs for Test-Time Scaling, achieving favorable trade-offs between accuracy and latency.

Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating
additional computation during inference, typically through parallel,
sequential, or hybrid scaling. However, prior studies often assume fixed
collaboration architectures (e.g., topologies) and single-model usage,
overlooking that optimal architectures and model combinations can vary across
tasks. Therefore, we study the novel problem of searching for compute-optimal
model combinations and architectures in TTS under a fixed budget. We formalize
it as a multi-LLM collaboration graph, where nodes encode roles and LLM model
assignments, and edges capture information flow. This problem is challenging
because (i) the combinatorial search space is prohibitively large, and (ii)
task-specific requirements demand tailored designs. To address these, we
reformulate the problem as probabilistic graph optimization and, through pilot
experiments, derive three empirical insights into TTS collaboration graphs.
Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented
framework that mirrors the REINFORCE pipeline by mapping
sampling-gradient-update to sampling-feedback-update, where feedback serves as
a textual gradient to update the probabilistic graph and efficiently search for
optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE
outperforms both traditional and LLM-based baselines in sample efficiency and
search performance, and effectively identifies optimal graphs under joint
objectives of accuracy and inference latency.

</details>


### [214] [GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation](https://arxiv.org/abs/2511.00097)
*Zihao Guo,Qingyun Sun,Ziwei Zhang,Haonan Yuan,Huiping Zhuang,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: GraphKeeper tackles Graph Domain-Incremental Learning (Domain-IL) by domain-specific fine-tuning with intra- and inter-domain disentanglement, along with deviation-free knowledge preservation and domain-aware discrimination for unseen domains, achieving state-of-the-art results and negligible forgetting across multiple graph domains when using graph foundation models (GFMs).


<details>
  <summary>Details</summary>
Motivation: Existing graph incremental learning (GIL) largely targets task- or class-incremental shifts within a single domain; with the rise of graph foundation models (GFMs), there is a critical need to enable continual learning across multiple graph domains (Domain-IL) while preventing catastrophic forgetting.

Method: 1) Domain-specific parameter-efficient fine-tuning to limit cross-domain interference. 2) Intra- and inter-domain disentanglement objectives to prevent embedding shifts and confusion. 3) Deviation-free knowledge preservation to maintain a stable decision boundary across incremental domains. 4) Domain-aware distribution discrimination to obtain precise embeddings for graphs with unobservable domains.

Result: Empirical results show GraphKeeper achieves state-of-the-art performance, with 6.5%â16.6% improvement over the runner-up and negligible forgetting. It also demonstrates compatibility with various representative GFMs, indicating broad applicability.

Conclusion: GraphKeeper effectively mitigates embedding shifts, decision-boundary drift, and cross-domain confusion in Graph Domain-IL, offering a practical and scalable framework that can be integrated with diverse GFMs for robust continual graph learning.

Abstract: Graph incremental learning (GIL), which continuously updates graph models by
sequential knowledge acquisition, has garnered significant interest recently.
However, existing GIL approaches focus on task-incremental and
class-incremental scenarios within a single domain. Graph domain-incremental
learning (Domain-IL), aiming at updating models across multiple graph domains,
has become critical with the development of graph foundation models (GFMs), but
remains unexplored in the literature. In this paper, we propose Graph
Domain-Incremental Learning via Knowledge Dientanglement and Preservation
(GraphKeeper), to address catastrophic forgetting in Domain-IL scenario from
the perspectives of embedding shifts and decision boundary deviations.
Specifically, to prevent embedding shifts and confusion across incremental
graph domains, we first propose the domain-specific parameter-efficient
fine-tuning together with intra- and inter-domain disentanglement objectives.
Consequently, to maintain a stable decision boundary, we introduce
deviation-free knowledge preservation to continuously fit incremental domains.
Additionally, for graphs with unobservable domains, we perform domain-aware
distribution discrimination to obtain precise embeddings. Extensive experiments
demonstrate the proposed GraphKeeper achieves state-of-the-art results with
6.5%~16.6% improvement over the runner-up with negligible forgetting. Moreover,
we show GraphKeeper can be seamlessly integrated with various representative
GFMs, highlighting its broad applicative potential.

</details>


### [215] [A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation](https://arxiv.org/abs/2511.00099)
*Marios Impraimakis,Evangelia Nektaria Palkanoglou*

Main category: cs.LG

TL;DR: An unsupervised conditional-labeled GAN framework for damage detection and digital twinning in structural health monitoring, validated on the Z24 Bridge, enabling damage-state discrimination without prior health-labels and supporting data generation, with SVM and PCA for validation.


<details>
  <summary>Details</summary>
Motivation: AI-based digital twinning often fails when measurements are scarce, physics knowledge is incomplete, or damage states are unknown; a label-free, unsupervised approach is needed to robustly detect faults and generate state-consistent data.

Method: Develop a conditional-labeled GAN guided by optimization-based damage detection. Use inputs from measurements corresponding to the same damage level to force the model to converge to two damage states, then repeat with a different measurement group. Compare convergence scores to identify other damage states. The approach simultaneously produces measurements for digital twinning at various damage states. A support vector machine (SVM) classifier and principal component analysis (PCA) are used to assess generated vs real measurements as a secondary dynamics-learning indicator.

Result: The framework effectively captures damage relative to healthy measurements and enables pattern recognition and data generation for digital twinning. Validation on the Z24 Bridge data demonstrates practical applicability for vibration-based system-level monitoring and infrastructure resilience, without requiring prior health-state information.

Conclusion: An unsupervised, optimization-driven, conditional-labeled GAN framework provides robust damage-state discrimination and data generation for digital twinning in SHM, improving fault detection in scenarios with limited measurements and unknown damage states.

Abstract: The optimization-based damage detection and damage state digital twinning
capabilities are examined here of a novel conditional-labeled generative
adversarial network methodology. The framework outperforms current approaches
for fault anomaly detection as no prior information is required for the health
state of the system: a topic of high significance for real-world applications.
Specifically, current artificial intelligence-based digital twinning approaches
suffer from the uncertainty related to obtaining poor predictions when a low
number of measurements is available, physics knowledge is missing, or when the
damage state is unknown. To this end, an unsupervised framework is examined and
validated rigorously on the benchmark structural health monitoring measurements
of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In
implementing the approach, firstly, different same damage-level measurements
are used as inputs, while the model is forced to converge conditionally to two
different damage states. Secondly, the process is repeated for a different
group of measurements. Finally, the convergence scores are compared to identify
which one belongs to a different damage state. The process for both
healthy-to-healthy and damage-to-healthy input data creates, simultaneously,
measurements for digital twinning purposes at different damage states, capable
of pattern recognition and machine learning data generation. Further to this
process, a support vector machine classifier and a principal component analysis
procedure is developed to assess the generated and real measurements of each
damage category, serving as a secondary new dynamics learning indicator in
damage scenarios. Importantly, the approach is shown to capture accurately
damage over healthy measurements, providing a powerful tool for vibration-based
system-level monitoring and scalable infrastructure resilience.

</details>


### [216] [Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification](https://arxiv.org/abs/2511.00100)
*Marios Impraimakis*

Main category: cs.LG

TL;DR: DL-based dynamic load identification (GRU, LSTM, CNN) can handle small datasets but performance is scenario-dependent; RKF outperforms networks in identifiable, physics-parametrized cases.


<details>
  <summary>Details</summary>
Motivation: Assess the effectiveness of data-driven neural networks for dynamic load identification in civil structures and compare with physics-based residual Kalman filter under data scarcity and model uncertainty.

Method: Evaluate GRU, LSTM, CNN for dynamic load identification on three cases: (1) simulated shaker-excited structure; (2) California building under seismic base excitation; (3) IASC-ASCE SHM benchmark with impact/instant loads; compare against RKF.

Result: No single method dominates; each network type outperforms others depending on loading scenario; RKF outperforms networks in physically parametrized identifiable scenarios.

Conclusion: Under data-limited or uncertain modeling conditions, RKF remains robust for identifiable systems, while DL approaches offer complementary strengths; a scenario-aware or hybrid approach may be advisable.

Abstract: The dynamic structural load identification capabilities of the gated
recurrent unit, long short-term memory, and convolutional neural networks are
examined herein. The examination is on realistic small dataset training
conditions and on a comparative view to the physics-based residual Kalman
filter (RKF). The dynamic load identification suffers from the uncertainty
related to obtaining poor predictions when in civil engineering applications
only a low number of tests are performed or are available, or when the
structural model is unidentifiable. In considering the methods, first, a
simulated structure is investigated under a shaker excitation at the top floor.
Second, a building in California is investigated under seismic base excitation,
which results in loading for all degrees of freedom. Finally, the International
Association for Structural Control-American Society of Civil Engineers
(IASC-ASCE) structural health monitoring benchmark problem is examined for
impact and instant loading conditions. Importantly, the methods are shown to
outperform each other on different loading scenarios, while the RKF is shown to
outperform the networks in physically parametrized identifiable cases.

</details>


### [217] [Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving](https://arxiv.org/abs/2511.00101)
*Yuchen Zhang,Hanyue Du,Chun Cao,Jingwei Xu*

Main category: cs.LG

TL;DR: Loquetier is a virtualized, multi-LoRA framework that unifies LoRA fine-tuning and serving in a single runtime, using a Virtualized Module to isolate adapters and a kernel design that merges tuning and inference for efficient batching. It achieves large throughput gains and higher SLO attainment, with public code available.


<details>
  <summary>Details</summary>
Motivation: There is a gap in unifying parameter-efficient fine-tuning (LoRA) and inference for LoRA-based models. Existing work treats training and serving separately, leading to overhead and fragmentation in deployment.

Method: Introduce a Virtualized Module to isolate PEFT-based modifications and support multiple adapters on a shared base model. Develop an optimized computation flow with a kernel design that merges fine-tuning and inference paths in forward propagation to enable efficient batching and reduce kernel invocation overhead.

Result: Empirical evaluation across three task settings shows Loquetier outperforms baselines in both performance and flexibility: up to 3.0Ã throughput on inference-only tasks compared to state-of-the-art co-serving, and 46.4Ã higher SLO attainment for unified fine-tuning and inference tasks. Code is publicly available at the provided GitHub link.

Conclusion: Loquetier demonstrates that unifying LoRA fine-tuning and serving within a single runtime is feasible and beneficial, delivering substantial efficiency and deployment flexibility for PEFT-enabled LLMs.

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient
fine-tuning (PEFT) technique for adapting large language models (LLMs) to
downstream tasks. While prior work has explored strategies for integrating LLM
training and serving, there still remains a gap in unifying fine-tuning and
inference for LoRA-based models. We present Loquetier, a virtualized multi-LoRA
framework that seamlessly integrates LoRA fine-tuning and serving within a
single runtime. Loquetier introduces two key components: (1) a Virtualized
Module that isolates PEFT-based modifications and supports multiple adapters on
a shared base model, and (2) an optimized computation flow with a kernel design
that merges fine-tuning and inference paths in forward propagation, enabling
efficient batching and minimizing kernel invocation overhead. Extensive
experiments across three task settings show that Loquetier consistently
outperforms existing baselines in both performance and flexibility, achieving
up to $3.0\times$ the throughput of the state-of-the-art co-serving system on
inference-only tasks and $46.4\times$ higher SLO attainment than PEFT on
unified fine-tuning and inference tasks. The implementation of Loquetier is
publicly available at https://github.com/NJUDeepEngine/Loquetier.

</details>


### [218] [Automated Discovery of Conservation Laws via Hybrid Neural ODE-Transformers](https://arxiv.org/abs/2511.00102)
*Vivan Doshi*

Main category: cs.LG

TL;DR: A decoupled learn-then-search framework using Neural ODEs, Transformers, and symbolic-numeric verification to discover conserved quantities from noisy trajectories, outperforming trajectory-based baselines.


<details>
  <summary>Details</summary>
Motivation: Conservation laws are central but hard to identify from data; noisy observations impede symbolic inference; need automated, robust discovery that integrates learning dynamics with symbolic reasoning.

Method: Three-stage approach: 1) Neural ODE to learn a continuous dynamical model; 2) Transformer proposes candidate invariants conditioned on the learned vector field; 3) a symbolic-numeric verifier certifies candidates with numerical guarantees. Evaluated on canonical physical systems; outperforms baselines.

Result: Outperforms baselines that rely on trajectory data alone; demonstrates robustness of learn-then-search; obtains valid invariants with numerical certificates even from noisy data.

Conclusion: A decoupled learn-then-search pipeline is effective for discovering mathematical invariants from imperfect data and can generalize to discovering other principles, offering robust automated discovery of conservation laws.

Abstract: The discovery of conservation laws is a cornerstone of scientific progress.
However, identifying these invariants from observational data remains a
significant challenge. We propose a hybrid framework to automate the discovery
of conserved quantities from noisy trajectory data. Our approach integrates
three components: (1) a Neural Ordinary Differential Equation (Neural ODE) that
learns a continuous model of the system's dynamics, (2) a Transformer that
generates symbolic candidate invariants conditioned on the learned vector
field, and (3) a symbolic-numeric verifier that provides a strong numerical
certificate for the validity of these candidates. We test our framework on
canonical physical systems and show that it significantly outperforms baselines
that operate directly on trajectory data. This work demonstrates the robustness
of a decoupled learn-then-search approach for discovering mathematical
principles from imperfect data.

</details>


### [219] [Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence](https://arxiv.org/abs/2511.00108)
*Yi Zhang,Che Liu,Xiancong Ren,Hanchu Ni,Shuai Zhang,Zeyuan Ding,Jiayu Hu,Hanzhe Shan,Zhenwei Niu,Zhaoyang Liu,Yue Zhao,Junbo Qi,Qinfan Zhang,Dengjie Li,Yidong Wang,Jiachen Luo,Yong Dai,Jian Tang,Xiaozhu Ju*

Main category: cs.LG

TL;DR: Pelican-VL 1.0 is a large-scale open-source embodied multimodal brain model (7Bâ72B params) that uses a metaloop training framework to achieve strong embodied performance, trained on massive GPU resources; it outperforms 100B open-source models and rivals proprietary systems.


<details>
  <summary>Details</summary>
Motivation: Embed powerful intelligence into various embodiments by scaling open-source embodied models and integrating data power with adaptive learning.

Method: Metaloop data distillation from 4+ billion tokens; DPPO metaloop framework; RL-Refine-Diagnose-SFT loop; large-scale distributed training on 1000+ A800 GPUs.

Result: 20.3% uplift over base; 10.6% better than 100B open-source; parity with leading proprietary on established embodied benchmarks.

Conclusion: DPPO metaloop demonstrates deliberate practice in AI; contributes a framework for large-scale open-source embodied models; suggests direction for future embodied AI with data-power integration.

Abstract: This report presents Pelican-VL 1.0, a new family of open-source embodied
brain models with parameter scales ranging from 7 billion to 72 billion. Our
explicit mission is clearly stated as: To embed powerful intelligence into
various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source
embodied multimodal brain model. Its core advantage lies in the in-depth
integration of data power and intelligent adaptive learning mechanisms.
Specifically, metaloop distilled a high-quality dataset from a raw dataset
containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale
cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint.
This translates to a 20.3% performance uplift from its base model and
outperforms 100B-level open-source counterparts by 10.6%, placing it on par
with leading proprietary systems on well-known embodied benchmarks. We
establish a novel framework, DPPO (Deliberate Practice Policy Optimization),
inspired by human metacognition to train Pelican-VL 1.0. We operationalize this
as a metaloop that teaches the AI to practice deliberately, which is a
RL-Refine-Diagnose-SFT loop.

</details>


### [220] [MeixnerNet: Adaptive and Robust Spectral Graph Neural Networks with Discrete Orthogonal Polynomials](https://arxiv.org/abs/2511.00113)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: MeixnerNet introduces discrete Meixner polynomial filters with learnable shape parameters, stabilized by Laplacian scaling and per-basis LayerNorm, achieving competitive accuracy to ChebyNet and strong robustness to changes in polynomial degree K.


<details>
  <summary>Details</summary>
Motivation: Continuous-domain polynomial spectral GNNs (e.g., ChebyNet) may misalign with the discrete graph spectrum, leading to fragility and suboptimal performance; a discrete, adaptable polynomial basis could better capture spectral properties.

Method: Employ discrete Meixner polynomials M_k(x; beta, c) with two learnable parameters beta and c; stabilize numerical issues via Laplacian scaling plus per-basis LayerNorm; evaluate across different polynomial degrees K.

Result: MeixnerNet matches or exceeds ChebyNet at the optimal K=2 on three benchmarks (2 wins). It shows strong robustness to variations in K, whereas ChebyNet is fragile and can collapse with suboptimal K.

Conclusion: Discrete, learnable polynomial bases tailored to graph spectra can improve stability and performance of spectral GNNs, providing an effective alternative to continuous-domain filters.

Abstract: Spectral Graph Neural Networks (GNNs) have achieved state-of-the-art results
by defining graph convolutions in the spectral domain. A common approach,
popularized by ChebyNet, is to use polynomial filters based on continuous
orthogonal polynomials (e.g., Chebyshev). This creates a theoretical
disconnect, as these continuous-domain filters are applied to inherently
discrete graph structures. We hypothesize this mismatch can lead to suboptimal
performance and fragility to hyperparameter settings.
  In this paper, we introduce MeixnerNet, a novel spectral GNN architecture
that employs discrete orthogonal polynomials -- specifically, the Meixner
polynomials $M_k(x; \beta, c)$. Our model makes the two key shape parameters of
the polynomial, beta and c, learnable, allowing the filter to adapt its
polynomial basis to the specific spectral properties of a given graph. We
overcome the significant numerical instability of these polynomials by
introducing a novel stabilization technique that combines Laplacian scaling
with per-basis LayerNorm.
  We demonstrate experimentally that MeixnerNet achieves
competitive-to-superior performance against the strong ChebyNet baseline at the
optimal K = 2 setting (winning on 2 out of 3 benchmarks). More critically, we
show that MeixnerNet is exceptionally robust to variations in the polynomial
degree K, a hyperparameter to which ChebyNet proves to be highly fragile,
collapsing in performance where MeixnerNet remains stable.

</details>


### [221] [LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers](https://arxiv.org/abs/2511.00116)
*Avisek Naug,Antonio Guillen,Vineet Kumar,Scott Greenwood,Wesley Brewer,Sahand Ghorbanpour,Ashwin Ramesh Babu,Vineet Gundecha,Ricardo Luna Gutierrez,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: LC-Opt is a comprehensive RL-enabled benchmark for sustainable liquid cooling in HPC, built on a Frontier digital twin, enabling centralized/decentralized RL, policy distillation for interpretability, and LLM-based action explanations.


<details>
  <summary>Details</summary>
Motivation: Rising AI workloads increase data center cooling demand; ML controllers can improve energy efficiency and reliability; need accessible, high-fidelity benchmarking to develop and compare RL-based cooling strategies.

Method: Modelica-based end-to-end digital twin spanning from cooling towers to server blades; Gymnasium interface for RL; optimization of IT-level setpoints and CT setpoints; multi-objective real-time optimization; supports HRU; evaluation of centralized and multi-agent RL; policy distillation; agentic mesh with LLM explanations.

Result: Demonstrates feasibility of RL control in liquid cooling; provides a platform for benchmarking, interpretability via decision/regression trees, and explainable actions via LLMs; democratizes access to detailed liquid cooling models.

Conclusion: LC-Opt enables researchers, operators, and vendors to develop sustainable, energy-efficient liquid cooling control solutions; fosters trust and manageability through interpretable policies and explanations.

Abstract: Liquid cooling is critical for thermal management in high-density data
centers with the rising AI workloads. However, machine learning-based
controllers are essential to unlock greater energy efficiency and reliability,
promoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC)
benchmark environment, for reinforcement learning (RL) control strategies in
energy-efficient liquid cooling of high-performance computing (HPC) systems.
Built on the baseline of a high-fidelity digital twin of Oak Ridge National
Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed
Modelica-based end-to-end models spanning site-level cooling towers to data
center cabinets and server blade groups. RL agents optimize critical thermal
controls like liquid supply temperature, flow rate, and granular valve
actuation at the IT cabinet level, as well as cooling tower (CT) setpoints
through a Gymnasium interface, with dynamic changes in workloads. This
environment creates a multi-objective real-time optimization challenge
balancing local thermal regulation and global energy efficiency, and also
supports additional components like a heat recovery unit (HRU). We benchmark
centralized and decentralized multi-agent RL approaches, demonstrate policy
distillation into decision and regression trees for interpretable control, and
explore LLM-based methods that explain control actions in natural language
through an agentic mesh architecture designed to foster user trust and simplify
system management. LC-Opt democratizes access to detailed, customizable liquid
cooling models, enabling the ML community, operators, and vendors to develop
sustainable data center liquid cooling control solutions.

</details>


### [222] [DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads](https://arxiv.org/abs/2511.00117)
*Antonio Guillen-Perez,Avisek Naug,Vineet Gundecha,Sahand Ghorbanpour,Ricardo Luna Gutierrez,Ashwin Ramesh Babu,Munther Salim,Shubhanker Banerjee,Eoin H. Oude Essink,Damien Fay,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: An open-source, high-fidelity benchmark (DCcluster-Opt) for sustainable, geo-temporal scheduling across globally distributed data centers, integrating real-world AI workloads, environmental factors, energy networks, and physics-based data-center models, with a Gymnasium API and baseline controllers.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of realistic benchmarks that jointly model time-varying grid carbon intensity, electricity prices, weather, data-center physics, and geo-distributed network dynamics to enable reproducible, multi-objective scheduling research.

Method: Assembles curated real-world datasets (AI workload traces, grid carbon intensity, electricity markets, weather across 20 regions, cloud transmission costs, network delay) and physics-informed models (CPUs, GPUs, memory, HVAC, heat recovery); defines a configurable, multi-objective scheduling problem with a top-level agent; implements a modular reward system; provides Gymnasium-compatible environment with baseline RL and rule-based controllers.

Result: Delivers a challenging benchmark that supports studying trade-offs among carbon emissions, energy costs, SLA, and water use; enables reproducible ML research and fair algorithm comparisons; includes open-source baseline controllers and deployment-ready APIs.

Conclusion: DCcluster-Opt is a realistic, configurable testbed that accelerates development and validation of sustainable computing solutions for geo-distributed data centers.

Abstract: The increasing energy demands and carbon footprint of large-scale AI require
intelligent workload management in globally distributed data centers. Yet
progress is limited by the absence of benchmarks that realistically capture the
interplay of time-varying environmental factors (grid carbon intensity,
electricity prices, weather), detailed data center physics (CPUs, GPUs, memory,
HVAC energy), and geo-distributed network dynamics (latency and transmission
costs). To bridge this gap, we present DCcluster-Opt: an open-source,
high-fidelity simulation benchmark for sustainable, geo-temporal task
scheduling. DCcluster-Opt combines curated real-world datasets, including AI
workload traces, grid carbon intensity, electricity markets, weather across 20
global regions, cloud transmission costs, and empirical network delay
parameters with physics-informed models of data center operations, enabling
rigorous and reproducible research in sustainable computing. It presents a
challenging scheduling problem where a top-level coordinating agent must
dynamically reassign or defer tasks that arrive with resource and service-level
agreement requirements across a configurable cluster of data centers to
optimize multiple objectives. The environment also models advanced components
such as heat recovery. A modular reward system enables an explicit study of
trade-offs among carbon emissions, energy costs, service level agreements, and
water use. It provides a Gymnasium API with baseline controllers, including
reinforcement learning and rule-based strategies, to support reproducible ML
research and a fair comparison of diverse algorithms. By offering a realistic,
configurable, and accessible testbed, DCcluster-Opt accelerates the development
and validation of next-generation sustainable computing solutions for
geo-distributed data centers.

</details>


### [223] [Analysis of Line Break prediction models for detecting defensive breakthrough in football](https://arxiv.org/abs/2511.00121)
*Shoma Yagi,Jun Ichikawa,Genki Ichinose*

Main category: cs.LG

TL;DR: A machine learning model using event and tracking data from the 2023 J1 League predicts Line Breaks with high accuracy (AUC 0.982, Brier 0.015) using 189 features and an XGBoost classifier; SHAP identifies speed, defender gaps, and offense distribution as key drivers; a moderate team-level link exists between predicted Line Breaks and shots/crosses conceded.


<details>
  <summary>Details</summary>
Motivation: Line Breaks are a crucial yet under-explored indicator of offensive effectiveness in football, bridging the gap between defensive resistance and scoring opportunities. Prior work emphasized shots/opportunities rather than the mechanism of breaking the defensive line; a quantitative model is needed to understand tactical dynamics and assist performance analysis.

Method: The study builds a predictive model for Line Breaks using 189 features derived from event and tracking data from the 2023 J1 League season. An XGBoost classifier estimates the probability of Line Breaks. Model evaluation uses AUC and Brier score. SHAP analysis identifies feature importance (e.g., offensive speed, defensive gaps, offensive spatial distribution). A team-level correlation analysis relates predicted Line Break probability to counts of shots and crosses conceded.

Result: The model achieved an AUC of 0.982 and a Brier score of 0.015, indicating high predictive accuracy. SHAP analysis highlighted offensive speed, gaps in the defensive line, and offensive spatial distributions as significant contributors to Line Breaks. A moderate positive correlation was observed between predicted Line Break probability and the number of shots and crosses conceded at the team level.

Conclusion: Line Breaks are closely linked to creating scoring opportunities, and the study provides a quantitative framework to understand tactical dynamics in football. The approach demonstrates the value of combining event and tracking data with machine learning to assess complex offensive actions.

Abstract: In football, attacking teams attempt to break through the opponent's
defensive line to create scoring opportunities. This action, known as a Line
Break, is a critical indicator of offensive effectiveness and tactical
performance, yet previous studies have mainly focused on shots or goal
opportunities rather than on how teams break the defensive line. In this study,
we develop a machine learning model to predict Line Breaks using event and
tracking data from the 2023 J1 League season. The model incorporates 189
features, including player positions, velocities, and spatial configurations,
and employs an XGBoost classifier to estimate the probability of Line Breaks.
The proposed model achieved high predictive accuracy, with an AUC of 0.982 and
a Brier score of 0.015. Furthermore, SHAP analysis revealed that factors such
as offensive player speed, gaps in the defensive line, and offensive players'
spatial distributions significantly contribute to the occurrence of Line
Breaks. Finally, we found a moderate positive correlation between the predicted
probability of being Line-Broken and the number of shots and crosses conceded
at the team level. These results suggest that Line Breaks are closely linked to
the creation of scoring opportunities and provide a quantitative framework for
understanding tactical dynamics in football.

</details>


### [224] [Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models](https://arxiv.org/abs/2511.00124)
*Sai Niranjan Ramachandran,Manish Krishan Lal,Suvrit Sra*

Main category: cs.LG

TL;DR: Introduces cross-fluctuation statistics to reveal discrete, phase-like transitions during score-based diffusion sampling; derives a closed-form for variance-preserving SDEs; demonstrates improved sampling efficiency, faster class-conditional/rare-class generation, and successful zero-shot tasks; unifies Markov chain theory with continuous diffusion models.


<details>
  <summary>Details</summary>
Motivation: To understand and accelerate sampling in diffusion models by uncovering underlying transition dynamics and connecting discrete Markov chain perspectives with continuous stochastic dynamics.

Method: Define and analyze cross-fluctuations (centered-moment statistics) of sampling trajectories, identify discontinuities signaling transitions between states; derive a closed-form expression for cross-fluctuations in variance-preserving SDEs; study reversibility (forward and reverse trajectories) and apply to reverse sampling; validate on sampling efficiency and zero-shot tasks.

Result: Cross-fluctuations exhibit discontinuities at transition points; detecting these transitions improves sampling efficiency, accelerates class-conditional and rare-class generation, and enhances zero-shot tasks (image classification and style transfer) without grid search or retraining; provides a unified view linking discrete coupling/mixing with continuous SDE dynamics.

Conclusion: Proposes a unifying framework that connects discrete Markov chain concepts, phase-transition analysis, and modern score-based generative modeling; extends to stochastic and non-Markovian samplers, offering a bridge between theory and practical acceleration techniques.

Abstract: We analyse how the sampling dynamics of distributions evolve in score-based
diffusion models using cross-fluctuations, a centered-moment statistic from
statistical physics. Specifically, we show that starting from an unbiased
isotropic normal distribution, samples undergo sharp, discrete transitions,
eventually forming distinct events of a desired distribution while
progressively revealing finer structure. As this process is reversible, these
transitions also occur in reverse, where intermediate states progressively
merge, tracing a path back to the initial distribution. We demonstrate that
these transitions can be detected as discontinuities in $n^{\text{th}}$-order
cross-fluctuations. For variance-preserving SDEs, we derive a closed-form for
these cross-fluctuations that is efficiently computable for the reverse
trajectory. We find that detecting these transitions directly boosts sampling
efficiency, accelerates class-conditional and rare-class generation, and
improves two zero-shot tasks--image classification and style transfer--without
expensive grid search or retraining. We also show that this viewpoint unifies
classical coupling and mixing from finite Markov chains with continuous
dynamics while extending to stochastic SDEs and non Markovian samplers. Our
framework therefore bridges discrete Markov chain theory, phase analysis, and
modern generative modeling.

</details>


### [225] [Dynamic Model Selection for Trajectory Prediction via Pairwise Ranking and Meta-Features](https://arxiv.org/abs/2511.00126)
*Lu Bowen*

Main category: cs.LG

TL;DR: A dynamic multi-expert gating framework selects the most reliable trajectory predictor per sample among physics-informed LSTM, Transformer, and GameFormer, using internal meta-features to improve safety-critical trajectory prediction beyond a single-model approach.


<details>
  <summary>Details</summary>
Motivation: Single-model deep trajectory predictors underperform in complex, long-tail urban scenarios and in safety-critical contexts where physics-based methods can excel; a per-sample, adaptive hybrid approach may improve reliability.

Method: Propose dynamic tri-expert gate with meta-features (stability, uncertainty) and pairwise-ranking over model signals to select among physics-informed LSTM, Transformer, and fine-tuned GameFormer on a per-sample basis; uses LLM-enhanced gating; train with pairwise ranking to optimize decision quality without post-hoc calibration.

Result: On nuPlan-mini (1,287 samples), achieved Final Displacement Error (FDE) of 2.567 m, 9.5% better than GameFormer (2.835 m); reaches 57.8% of oracle performance bound. In open-loop left-turn scenarios, FDE improved by ~10% after horizon alignment.

Conclusion: Adaptive hybrid systems that select the most reliable predictor per scenario improve trajectory reliability in safety-critical autonomous driving, offering a practical path beyond static single-model paradigms.

Abstract: Recent deep trajectory predictors (e.g., Jiang et al., 2023; Zhou et al.,
2022) have achieved strong average accuracy but remain unreliable in complex
long-tail driving scenarios. These limitations reveal the weakness of the
prevailing "one-model-fits-all" paradigm, particularly in safety-critical urban
contexts where simpler physics-based models can occasionally outperform
advanced networks (Kalman, 1960). To bridge this gap, we propose a dynamic
multi-expert gating framework that adaptively selects the most reliable
trajectory predictor among a physics-informed LSTM, a Transformer, and a
fine-tuned GameFormer on a per-sample basis.
  Our method leverages internal model signals (meta-features) such as stability
and uncertainty (Gal and Ghahramani, 2016), which we demonstrate to be
substantially more informative than geometric scene descriptors. To the best of
our knowledge, this is the first work to formulate trajectory expert selection
as a pairwise-ranking problem over internal model signals (Burges et al.,
2005), directly optimizing decision quality without requiring post-hoc
calibration.
  Evaluated on the nuPlan-mini dataset (Caesar et al., 2021) with 1,287
samples, our LLM-enhanced tri-expert gate achieves a Final Displacement Error
(FDE) of 2.567 m, representing a 9.5 percent reduction over GameFormer (2.835
m), and realizes 57.8 percent of the oracle performance bound. In open-loop
simulations, after trajectory horizon alignment, the same configuration reduces
FDE on left-turn scenarios by approximately 10 percent, demonstrating
consistent improvements across both offline validation and open-loop
evaluation. These results indicate that adaptive hybrid systems enhance
trajectory reliability in safety-critical autonomous driving, providing a
practical pathway beyond static single-model paradigms.

</details>


### [226] [Casing Collar Identification using AlexNet-based Neural Networks for Depth Measurement in Oil and Gas Wells](https://arxiv.org/abs/2511.00129)
*Siyu Xiao,Xindi Zhao,Tianhao Mao,Yiwei Wang,Yuqiao Chen,Hongyun Zhang,Jian Wang,Junjie Wang,Shuang Liu,Tupei Chen,Yang Liu*

Main category: cs.LG

TL;DR: This work integrates a data-collection system for casing collar locator (CCL) signals with a comprehensive augmentation pipeline for AlexNet-based models, addressing data scarcity in downhole collar recognition. It identifies standardization, label distribution smoothing (LDS), and random cropping as fundamental, while label smoothing regularization (LSR), time scaling, and multiple sampling greatly enhance generalization, achieving state-of-the-art F1 (0.0 interpreted as 1.0 on benchmarks).


<details>
  <summary>Details</summary>
Motivation: Accurate downhole depth measurement via CCL is crucial for reservoir contact, production efficiency, and safety. Neural CCL recognition is promising, but preprocessing and data scarcity hinder robust model training.

Method: Developed a CCL signal acquisition system integrated into downhole tools for dataset construction. Proposed comprehensive data augmentation preprocessing methods. Evaluated using AlexNet-based CNN models across many configuration combinations to isolate each augmentation method's contribution.

Result: F1 scores improved from 0.937/0.952 to 1.0/1.0 on benchmark models. Real CCL waveform validation confirms practical effectiveness of the augmentation approach.

Conclusion: The paper advances data augmentation methodology for training CCL recognition models in data-limited downhole environments, enhancing model robustness and practical applicability.

Abstract: Accurate downhole depth measurement is essential for oil and gas well
operations, directly influencing reservoir contact, production efficiency, and
operational safety. Collar correlation using a casing collar locator (CCL) is
fundamental for precise depth calibration. While neural network-based CCL
signal recognition has achieved significant progress in collar identification,
preprocessing methods for such applications remain underdeveloped. Moreover,
the limited availability of real well data poses substantial challenges for
training neural network models that require extensive datasets. This paper
presents a system integrated into downhole tools for CCL signal acquisition to
facilitate dataset construction. We propose comprehensive preprocessing methods
for data augmentation and evaluate their effectiveness using our AlexNet-based
neural network models. Through systematic experimentation across various
configuration combinations, we analyze the contribution of each augmentation
method. Results demonstrate that standardization, label distribution smoothing
(LDS), and random cropping are fundamental requirements for model training,
while label smoothing regularization (LSR), time scaling, and multiple sampling
significantly enhance model generalization capability. The F1 scores of our two
benchmark models trained with the proposed augmentation methods maximumly
improve from 0.937 and 0.952 to 1.0 and 1.0, respectively. Performance
validation on real CCL waveforms confirms the effectiveness and practical
applicability of our approach. This work addresses the gaps in data
augmentation methodologies for training casing collar recognition models in CCL
data-limited environments.

</details>


### [227] [A Comparative Analysis of LLM Adaptation: SFT, LoRA, and ICL in Data-Scarce Scenarios](https://arxiv.org/abs/2511.00130)
*Bernd Bohnet,Rumen Dangovski,Kevin Swersky,Sherry Moore,Arslan Chaudhry,Kathleen Kenealy,Noah Fiedel*

Main category: cs.LG

TL;DR: LoRA offers the best balance for data-scarce LLM adaptation by integrating new skills with minimal forgetting of base knowledge; SFT excels at skill acquisition but suffers catastrophic forgetting; ICL effectively adds factual knowledge but struggles with complex skills; choose adaptation method based on whether the task prioritizes skill learning or knowledge integration.


<details>
  <summary>Details</summary>
Motivation: Address how to tailor large language models to specific tasks without eroding general capabilities, especially under data scarcity. Compare SFT, LoRA, and ICL to understand trade-offs and guide practical adaptation decisions.

Method: Empirical comparative study evaluating supervised finetuning (SFT), Low-Rank Adaptation (LoRA), and in-context learning (ICL) in data-scarce regimes. Tasks assess skill acquisition and knowledge integration, with metrics capturing performance and catastrophic forgetting.

Result: LoRA provides the most favorable balance, effectively instilling new skills with minimal impact on the base model's general knowledge. SFT excels at skill acquisition but is highly susceptible to catastrophic forgetting. ICL efficiently incorporates factual knowledge but struggles with acquiring complex skills.

Conclusion: Offer a practical framework for selecting LLM adaptation strategies. Emphasize the distinction between skill acquisition and knowledge integration and the trade-offs between task-specific performance and preservation of general capabilities.

Abstract: The remarkable capabilities of Large Language Models (LLMs) often need to be
tailored for specific applications, requiring the integration of new knowledge
or the acquisition of new skills. While full fine-tuning is a powerful
adaptation method, it is computationally expensive and can lead to a
degradation of general reasoning abilities, a phenomenon known as catastrophic
forgetting. A range of alternative techniques exists, each with its own
trade-offs. In-Context Learning (ICL) is fast but limited by context length,
while Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation
(LoRA) offer a middle ground by minimizing parameter changes. However, the
challenge of catastrophic forgetting persists, raising questions about the best
adaptation strategy for a given task. This paper presents a comparative
analysis of Supervised Finetuning (SFT), LoRA, and ICL in data-scarce
scenarios. We find that LoRA provides the most effective balance, successfully
instilling new skills with minimal impact on the base model's general
knowledge. In contrast, while SFT excels at skill acquisition, it is highly
susceptible to catastrophic forgetting. ICL is effective for incorporating
factual knowledge but struggles with complex skills. Our findings offer a
practical framework for selecting an LLM adaptation strategy. We highlight the
critical distinction between skill acquisition and knowledge integration,
clarify the trade-offs between task-specific performance and the preservation
of general capabilities.

</details>


### [228] [Feature Importance Guided Random Forest Learning with Simulated Annealing Based Hyperparameter Tuning](https://arxiv.org/abs/2511.00133)
*Kowshik Balasubramanian,Andre Williams,Ismail Butun*

Main category: cs.LG

TL;DR: A novel Random Forest enhancement that combines probabilistic feature sampling with Simulated Annealing-based hyperparameter optimization yields improved accuracy and generalization across diverse domains, emphasizing feature relevance.


<details>
  <summary>Details</summary>
Motivation: Conventional Random Forests can struggle to generalize across heterogeneous tasks and high-dimensional data; there is a need for better feature selection and adaptive hyperparameter tuning to robustly classify in varied domains.

Method: Introduce probabilistic feature sampling guided by feature importance within the RF ensemble and apply Simulated Annealing to optimize hyperparameters. The approach uses importance-aware sampling to emphasize informative features and combines this with metaheuristic optimization for dynamic parameter tuning, aiming to improve accuracy and generalization across domains.

Result: The framework delivers consistent accuracy improvements and deeper insights into feature relevance, with demonstrated applicability to credit risk evaluation, IoT anomaly detection, early-stage medical diagnostics, and high-dimensional biological data analysis.

Conclusion: Combining importance-aware sampling with Simulated Annealing-based hyperparameter optimization enhances Random Forest performance, providing better accuracy, generalization, and interpretability across diverse classification tasks.

Abstract: This paper introduces a novel framework for enhancing Random Forest
classifiers by integrating probabilistic feature sampling and hyperparameter
tuning via Simulated Annealing. The proposed framework exhibits substantial
advancements in predictive accuracy and generalization, adeptly tackling the
multifaceted challenges of robust classification across diverse domains,
including credit risk evaluation, anomaly detection in IoT ecosystems,
early-stage medical diagnostics, and high-dimensional biological data analysis.
To overcome the limitations of conventional Random Forests, we present an
approach that places stronger emphasis on capturing the most relevant signals
from data while enabling adaptive hyperparameter configuration. The model is
guided towards features that contribute more meaningfully to classification and
optimizing this with dynamic parameter tuning. The results demonstrate
consistent accuracy improvements and meaningful insights into feature
relevance, showcasing the efficacy of combining importance aware sampling and
metaheuristic optimization.

</details>


### [229] [Physiologically Active Vegetation Reverses Its Cooling Effect in Humid Urban Climates](https://arxiv.org/abs/2511.00134)
*Angana Borah,Adrija Datta,Ashish S. Kumar,Raviraj Dave,Udit Bhatia*

Main category: cs.LG

TL;DR: Vegetation cooling in Indian cities is climate-dependent: cooling strengthens with moderate vegetation (EVI â¥0.4, LAI â¥0.05) but reverses to warming at higher levels (EVI â¥0.5, LAI â¥0.2, fPAR â¥0.5), especially in humid dense cores where active vegetation raises humidity and Heat Index. Provides thresholds to guide climate-specific greening for heat-resilient cities.


<details>
  <summary>Details</summary>
Motivation: To understand when urban greening cools surfaces versus increases near-surface humidity, enabling policy and design choices that avoid amplifying heat stress.

Method: Analyzed 138 Indian cities across tropical savanna, semi-arid steppe, and humid subtropical climates. Built a 1 km extreme-aware reconstruction of Heat Index (HI) and used an interpretable machine-learning framework combining SHAP and Accumulated Local Effects (ALE) to isolate vegetationâclimate interactions and identify threshold regimes in vegetation indices (EVI, LAI, fPAR).

Result: Cooling generally strengthens with moderate vegetation (EVI â¥0.4 and LAI â¥0.05). However, under joint-high regimes (EVI â¥0.5, LAI â¥0.2, fPAR â¥0.5), cooling reverses toward warming; an earlier onset of reversal is found in humid, dense cores when fPAR â¥0.25. Highly physiologically active vegetation elevates near-surface humidity faster than it removes heat, amplifying perceived heat stress.

Conclusion: Defines climatic limits for vegetation-driven cooling and provides quantitative, climate-specific thresholds to guide greening strategies that promote equitable, heat-resilient cities.

Abstract: Efforts to green cities for cooling are succeeding unevenly because the same
vegetation that cools surfaces can also intensify how hot the air feels.
Previous studies have identified humid heat as a growing urban hazard, yet how
physiologically active vegetation governs this trade-off between cooling and
moisture accumulation remains poorly understood, leaving mitigation policy and
design largely unguided. Here we quantify how vegetation structure and function
influence the Heat Index (HI), a combined measure of temperature and humidity
in 138 Indian cities spanning tropical savanna, semi-arid steppe, and humid
subtropical climates, and across dense urban cores and semi-urban rings. Using
an extreme-aware, one kilometre reconstruction of HI and an interpretable
machine-learning framework that integrates SHapley Additive Explanations (SHAP)
and Accumulated Local Effects (ALE), we isolate vegetation-climate
interactions. Cooling generally strengthens for EVI >= 0.4 and LAI >= 0.05, but
joint-high regimes begin to reverse toward warming when EVI >= 0.5, LAI >= 0.2,
and fPAR >= 0.5,with an earlier onset for fPAR >= 0.25 in humid, dense cores.
In such environments, highly physiologically active vegetation elevates
near-surface humidity faster than it removes heat, reversing its cooling effect
and amplifying perceived heat stress. These findings establish the climatic
limits of vegetation-driven cooling and provide quantitative thresholds for
climate-specific greening strategies that promote equitable and heat-resilient
cities.

</details>


### [230] [A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control](https://arxiv.org/abs/2511.00136)
*Qing Guo,Xinhang Li,Junyu Chen,Zheng Guo,Xiaocong Li,Lin Zhang,Lei Li*

Main category: cs.LG

TL;DR: HeraldLight introduces a dual-LLM architecture with Herald guided prompts for traffic signal control, addressing fixed signal durations and hallucinations in prior LLM/RL approaches. It uses a Herald Module to forecast queue lengths, an LLM-Agent for control, and an LLM-Critic to refine outputs for score-based fine-tuning. In CityFlow simulations across 224 intersections, it achieves significant improvements over baselines (â20% lower travel time; â11% lower queue length in some cities).


<details>
  <summary>Details</summary>
Motivation: LLMs offer optimization efficiency and interpretability over traditional RL in traffic signal control, but face fixed duration constraints and hallucination risks; RL methods suffer robustness issues and poor generalization. A robust, accurate LLM-based TSC framework is needed.

Method: HeraldLight combines two LLMs with Herald guided prompts. The Herald Module extracts real-time context and forecasts per-phase queue lengths. LLM-Agent uses forecasts to perform fine-grained signal control. LLM-Critic refines LLM-Agent outputs to correct errors/hallucinations. Outputs are used for score-based fine-tuning to improve accuracy and robustness.

Result: Simulation on CityFlow with 224 intersections (Jinan 12, Hangzhou 16, New York 196) shows HeraldLight outperforms baselines, achieving a 20.03% reduction in average travel time across all scenarios and a 10.74% reduction in average queue length on the Jinan and Hangzhou scenarios.

Conclusion: A dual-LLM approach with guided prompts and a correction stage improves TSC performance and robustness, offering better accuracy and interpretability than prior LLM/RL methods; source code is available for reproducibility.

Abstract: Leveraging large language models (LLMs) in traffic signal control (TSC)
improves optimization efficiency and interpretability compared to traditional
reinforcement learning (RL) methods. However, existing LLM-based approaches are
limited by fixed time signal durations and are prone to hallucination errors,
while RL methods lack robustness in signal timing decisions and suffer from
poor generalization. To address these challenges, this paper proposes
HeraldLight, a dual LLMs architecture enhanced by Herald guided prompts. The
Herald Module extracts contextual information and forecasts queue lengths for
each traffic phase based on real-time conditions. The first LLM, LLM-Agent,
uses these forecasts to make fine grained traffic signal control, while the
second LLM, LLM-Critic, refines LLM-Agent's outputs, correcting errors and
hallucinations. These refined outputs are used for score-based fine-tuning to
improve accuracy and robustness. Simulation experiments using CityFlow on real
world datasets covering 224 intersections in Jinan (12), Hangzhou (16), and New
York (196) demonstrate that HeraldLight outperforms state of the art baselines,
achieving a 20.03% reduction in average travel time across all scenarios and a
10.74% reduction in average queue length on the Jinan and Hangzhou scenarios.
The source code is available on GitHub:
https://github.com/BUPT-ANTlab/HeraldLight.

</details>


### [231] [Study on Supply Chain Finance Decision-Making Model and Enterprise Economic Performance Prediction Based on Deep Reinforcement Learning](https://arxiv.org/abs/2511.00166)
*Shiman Zhang,Jinghan Zhou,Zhoufan Yu,Ningai Leng*

Main category: cs.LG

TL;DR: A hybrid DL-PSO model for back-end supply chains that jointly optimizes node deployment and planning paths, leveraging CNNs for feature extraction, linear programming for high-order statistics, fuzzy scheduling, deep reinforcement learning, and PSO-guided adaptive control; simulations show efficiency gains and robust dynamic decision-making.


<details>
  <summary>Details</summary>
Motivation: Improve decision-making and planning efficiency in centralized redundant supply chains, especially under dynamic conditions, by integrating learning-based feature extraction, high-order statistical modeling, and global optimization with adaptive control.

Method: Proposes a distributed node deployment model and an optimal planning path for the supply chain network. Uses convolutional neural networks to extract features from historical data, linear programming to capture high-order statistical features, fuzzy association rule scheduling and deep reinforcement learning for optimization, and neural networks to adapt to dynamic changes. Introduces a hybrid mechanism âdeep learning feature extraction - intelligent particle swarm optimizationâ to drive global optimization and select optimal decisions for adaptive control.

Result: Simulations indicate reduced resource consumption, enhanced spatial planning, and in dynamic environments improved real-time decision adjustment, distribution path optimization, and robust intelligent control.

Conclusion: Integrating deep learning with intelligent particle swarm optimization improves decision-making and planning efficiency in back-end supply chains, enabling adaptive, robust control under dynamic conditions.

Abstract: To improve decision-making and planning efficiency in back-end centralized
redundant supply chains, this paper proposes a decision model integrating deep
learning with intelligent particle swarm optimization. A distributed node
deployment model and optimal planning path are constructed for the supply chain
network. Deep learning such as convolutional neural networks extracts features
from historical data, and linear programming captures high-order statistical
features. The model is optimized using fuzzy association rule scheduling and
deep reinforcement learning, while neural networks fit dynamic changes. A
hybrid mechanism of "deep learning feature extraction - intelligent particle
swarm optimization" guides global optimization and selects optimal decisions
for adaptive control. Simulations show reduced resource consumption, enhanced
spatial planning, and in dynamic environments improved real-time decision
adjustment, distribution path optimization, and robust intelligent control.

</details>


### [232] [Can SAEs reveal and mitigate racial biases of LLMs in healthcare?](https://arxiv.org/abs/2511.00177)
*Hiba Ahsan,Byron C. Wallace*

Main category: cs.LG

TL;DR: Sparse autoencoders reveal race-associated latent representations in Gemma-2 LLMs and can steer outputs toward Black patients; while SAE-based detection helps identify problematic biases, steering-based mitigation shows only marginal utility in realistic clinical tasks.


<details>
  <summary>Details</summary>
Motivation: Address safety concerns of LLMs in healthcare by uncovering and potentially mitigating spurious racial associations that could affect predictions and care quality.

Method: Identify latent variables in Sparse Autoencoders (SAEs) that correlate with Black individuals in Gemma-2 models; examine activations on inputs like 'African American' and related terms; demonstrate that SAE latent steering can push outputs toward Black patients and alter risk predictions (e.g., increasing the likelihood of 'belligerent' labels); empirically assess whether SAE steering mitigates bias across simple versus realistic clinical tasks.

Result: Found an SAE latent that correlates with Black demographics and activates on terms like 'African American' and 'incarceration'; activating this latent can steer the model to generate outputs about Black patients and alter outputs (e.g., higher risk for 'belligerent' labels); bias mitigation via SAE steering offers improvements in simple settings but is not robust for realistic, complex clinical tasks.

Conclusion: SAEs are useful for detecting problematic demographic reliance in clinical LLMs, but using SAE steering as a mitigation strategy has limited utility for realistic healthcare tasks; thus, SAEs are better suited for bias auditing than for reliable bias mitigation.

Abstract: LLMs are increasingly being used in healthcare. This promises to free
physicians from drudgery, enabling better care to be delivered at scale. But
the use of LLMs in this space also brings risks; for example, such models may
worsen existing biases. How can we spot when LLMs are (spuriously) relying on
patient race to inform predictions? In this work we assess the degree to which
Sparse Autoencoders (SAEs) can reveal (and control) associations the model has
made between race and stigmatizing concepts. We first identify SAE latents in
Gemma-2 models which appear to correlate with Black individuals. We find that
this latent activates on reasonable input sequences (e.g., "African American")
but also problematic words like "incarceration". We then show that we can use
this latent to steer models to generate outputs about Black patients, and
further that this can induce problematic associations in model outputs as a
result. For example, activating the Black latent increases the risk assigned to
the probability that a patient will become "belligerent". We evaluate the
degree to which such steering via latents might be useful for mitigating bias.
We find that this offers improvements in simple settings, but is less
successful for more realistic and complex clinical tasks. Overall, our results
suggest that: SAEs may offer a useful tool in clinical applications of LLMs to
identify problematic reliance on demographics but mitigating bias via SAE
steering appears to be of marginal utility for realistic tasks.

</details>


### [233] [PDE-SHARP: PDE Solver Hybrids Through Analysis & Refinement Passes](https://arxiv.org/abs/2511.00183)
*Shaghayegh Fazliani,Madeleine Udell*

Main category: cs.LG

TL;DR: PDE-SHARP substitutes expensive PDE solver computations with cheaper LLM inference via a three-stage pipeline, achieving 60-75% fewer evaluations and ~4x accuracy improvements across PDEs, robust to different LLMs.


<details>
  <summary>Details</summary>
Motivation: Test-time, LLM-driven PDE solvers rely on many solver samples, incurring high computational cost for complex PDEs; a cheaper, accurate alternative is needed.

Method: Three-stage pipeline: (1) Analysis: chain-of-thought, PDE classification, solution type detection, stability analysis; (2) Genesis: solver generation from insights; (3) Synthesis: collaborative selection-hybridization tournaments where LLMs iteratively refine implementations through feedback.

Result: Reduces solver evaluations from 30+ to under 13 on average; improves accuracy uniformly by ~4x across tested PDEs; robust across LLM architectures from general-purpose to specialized reasoning models.

Conclusion: PDE-SHARP provides a cost-effective, accurate, and architecture-robust framework for PDE solving with LLMs, highlighting the value of staged reasoning and collaborative refinement.

Abstract: Current LLM-driven approaches using test-time computing to generate PDE
solvers execute a large number of solver samples to identify high-accuracy
solvers. These paradigms are especially costly for complex PDEs requiring
substantial computational resources for numerical evaluation. We introduce
PDE-SHARP, a framework to reduce computational costs by replacing expensive
scientific computation by cheaper LLM inference that achieves superior solver
accuracy with 60-75% fewer computational evaluations. PDE-SHARP employs three
stages: (1) Analysis: mathematical chain-of-thought analysis including PDE
classification, solution type detection, and stability analysis; (2) Genesis:
solver generation based on mathematical insights from the previous stage; and
(3) Synthesis: collaborative selection-hybridization tournaments in which LLM
judges iteratively refine implementations through flexible performance
feedback. To generate high-quality solvers, PDE-SHARP requires fewer than 13
solver evaluations on average compared to 30+ for baseline methods, improving
accuracy uniformly across tested PDEs by $4\times$ on average, and demonstrates
robust performance across LLM architectures, from general-purpose to
specialized reasoning models.

</details>


### [234] [EL-MIA: Quantifying Membership Inference Risks of Sensitive Entities in LLMs](https://arxiv.org/abs/2511.00192)
*Ali Satvaty,Suzan Verberne,Fatih Turkmen*

Main category: cs.LG

TL;DR: Proposes EL-MIA for entity-level membership risk in LLMs, introduces a benchmark, compares existing MIA methods and two new ones, and finds current MIAs insufficient for sensitive-entity leakage, highlighting need for stronger adversaries and threat models.


<details>
  <summary>Details</summary>
Motivation: Address privacy risks in LLMs at a finer granularity by auditing whether sensitive entities (PII, credit card numbers, etc.) appear in training data, beyond whole-prompt/document leakage.

Method: Introduce EL-MIA framework for auditing, construct a benchmark dataset for entity-level MIA evaluation, perform a systematic comparison of existing MIAs and two proposed methods, analyze factors like model scale and training epochs to explain susceptibility.

Result: Existing MIA methods are limited in detecting entity-level membership of sensitive attributes; some straightforward approaches can outline susceptibility, indicating gaps and the need for stronger adversaries to stress-test the threat model.

Conclusion: Entity-level membership inference on sensitive attributes is underdeveloped; more robust threat models and adversarial evaluation are required to accurately assess and mitigate these risks.

Abstract: Membership inference attacks (MIA) aim to infer whether a particular data
point is part of the training dataset of a model. In this paper, we propose a
new task in the context of LLM privacy: entity-level discovery of membership
risk focused on sensitive information (PII, credit card numbers, etc). Existing
methods for MIA can detect the presence of entire prompts or documents in the
LLM training data, but they fail to capture risks at a finer granularity. We
propose the ``EL-MIA'' framework for auditing entity-level membership risks in
LLMs. We construct a benchmark dataset for the evaluation of MIA methods on
this task. Using this benchmark, we conduct a systematic comparison of existing
MIA techniques as well as two newly proposed methods. We provide a
comprehensive analysis of the results, trying to explain the relation of the
entity level MIA susceptability with the model scale, training epochs, and
other surface level factors. Our findings reveal that existing MIA methods are
limited when it comes to entity-level membership inference of the sensitive
attributes, while this susceptibility can be outlined with relatively
straightforward methods, highlighting the need for stronger adversaries to
stress test the provided threat model.

</details>


### [235] [Diffusion LLMs are Natural Adversaries for any LLM](https://arxiv.org/abs/2511.00203)
*David LÃ¼dke,Tom WollschlÃ¤ger,Paul Ungermann,Stephan GÃ¼nnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: A framework that replaces costly adversarial prompt optimization with efficient amortized inference by using pretrained diffusion/non-autoregressive LLMs to generate prompts conditionally. It claims few samples suffice under mild fidelity, yielding low-perplexity, diverse jailbreak prompts transferable to many black-box models, enabling rapid red-team and prompt-optimization workflows.


<details>
  <summary>Details</summary>
Motivation: Adversarial prompt optimization is resource-intensive and per-instance. The authors seek scalable surrogates that can generalize across models. Non-autoregressive, diffusion-based LLMs that model the joint distribution over promptâresponse pairs can serve as effective proxies for prompt search and enable parallel prompt generation.

Method: Utilize pretrained diffusion LLMs that model the joint distribution of promptâresponse pairs to generate prompts conditionally. Reformulate prompt search as an amortized inference problem with a small number of parallelizable samples. Provide a probabilistic analysis showing that, under mild fidelity assumptions, few conditional samples recover high-reward prompts. Empirically evaluate across black-box models, measuring prompt quality, perplexity, diversity, and transferability.

Result: Prompts generated are low-perplexity, diverse jailbreaks that transfer effectively to a wide range of black-box LLMs, including robustly trained and proprietary models. The approach replaces costly per-instance optimization with a small set of samples, enabling scalable red-teaming and automated prompt optimization.

Conclusion: The framework enables efficient adversarial prompt search and red-teaming via diffusion-based LLMs, suggesting broader directions for leveraging Flow- and Diffusion-based LLMs in prompt optimization and security testing.

Abstract: We introduce a novel framework that transforms the resource-intensive
(adversarial) prompt optimization problem into an \emph{efficient, amortized
inference task}. Our core insight is that pretrained, non-autoregressive
generative LLMs, such as Diffusion LLMs, which model the joint distribution
over prompt-response pairs, can serve as powerful surrogates for prompt search.
This approach enables the direct conditional generation of prompts, effectively
replacing costly, per-instance discrete optimization with a small number of
parallelizable samples. We provide a probabilistic analysis demonstrating that
under mild fidelity assumptions, only a few conditional samples are required to
recover high-reward (harmful) prompts. Empirically, we find that the generated
prompts are low-perplexity, diverse jailbreaks that exhibit strong
transferability to a wide range of black-box target models, including robustly
trained and proprietary LLMs. Beyond adversarial prompting, our framework opens
new directions for red teaming, automated prompt optimization, and leveraging
emerging Flow- and Diffusion-based LLMs.

</details>


### [236] [Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides](https://arxiv.org/abs/2511.00209)
*Yiquan Wang,Yahui Ma,Yuhan Chang,Jiayao Yan,Jialin Zhang,Minnuo Cai,Kai Wei*

Main category: cs.LG

TL;DR: Diffusion models offer a unified iterative denoising framework for designing two major therapeutic modalitiesâsmall molecules and therapeutic peptidesâyet face modality-specific challenges (synthesizability for small molecules; stability, folding, and immunogenicity for peptides) and shared issues (scoring accuracy, data scarcity, and need for experimental validation). The authors advocate a closed-loop DBTL integration to unlock the full potential by bridging modality-specific gaps.


<details>
  <summary>Details</summary>
Motivation: To systematically compare diffusion-model applications in drug design across two principal modalities, identifying common opportunities and modality-specific hurdles to guide future DBTL-enabled development.

Method: Perform a structured review that analyzes how a unified iterative denoising framework is adapted to distinct molecular representations, chemical spaces, and design objectives for small molecules and therapeutic peptides, examining structure-based design, sequence/structure generation, and the associated challenges and needs.

Result: Small molecules: diffusion models excel at structure-based design, generating novel, pocket-fitting ligands with desired physicochemical properties, but synthesizability remains a critical obstacle. Therapeutic peptides: focus shifts to generating functional sequences and de novo structures, with challenges in biological stability against proteolysis, proper folding, and minimizing immunogenicity. Shared hurdles across modalities include the need for more accurate scoring functions, scarcity of high-quality experimental data, and the necessity of experimental validation.

Conclusion: Unlocking diffusion modelsâ full potential requires bridging modality-specific gaps and integrating them into automated, closed-loop Design-Build-Test-Learn (DBTL) platforms, enabling a shift from broad chemical exploration to targeted creation of novel therapeutics.

Abstract: Diffusion models have emerged as a leading framework in generative modeling,
showing significant potential to accelerate and transform the traditionally
slow and costly process of drug discovery. This review provides a systematic
comparison of their application in designing two principal therapeutic
modalities: small molecules and therapeutic peptides. We analyze how a unified
framework of iterative denoising is adapted to the distinct molecular
representations, chemical spaces, and design objectives of each modality. For
small molecules, these models excel at structure-based design, generating
novel, pocket-fitting ligands with desired physicochemical properties, yet face
the critical hurdle of ensuring chemical synthesizability. Conversely, for
therapeutic peptides, the focus shifts to generating functional sequences and
designing de novo structures, where the primary challenges are achieving
biological stability against proteolysis, ensuring proper folding, and
minimizing immunogenicity. Despite these distinct challenges, both domains face
shared hurdles: the need for more accurate scoring functions, the scarcity of
high-quality experimental data, and the crucial requirement for experimental
validation. We conclude that the full potential of diffusion models will be
unlocked by bridging these modality-specific gaps and integrating them into
automated, closed-loop Design-Build-Test-Learn (DBTL) platforms, thereby
shifting the paradigm from chemical exploration to the targeted creation of
novel therapeutics.

</details>


### [237] [Iterative Foundation Model Fine-Tuning on Multiple Rewards](https://arxiv.org/abs/2511.00220)
*Pouya M. Ghari,Simone Sciabola,Ye Wang*

Main category: cs.LG

TL;DR: A multi-reward reinforcement learning framework for fine-tuning foundation models, using an iterative strategy to optimize several reward signals; shows theoretical insights and empirical gains across text, biological sequences, and small-molecule generation over state-of-the-art baselines.


<details>
  <summary>Details</summary>
Motivation: Real-world applications often require balancing multiple evaluation criteria; optimizing a single reward can be suboptimal for tasks like text generation and drug design.

Method: An iterative fine-tuning algorithm that optimizes multiple reward signals via reinforcement learning, alternating or cycling through rewards; includes a theoretical analysis of multi-reward RL and generalizes existing RL-based fine-tuning methods.

Result: Empirical results across diverse domains (text, biological sequences, small molecules) demonstrate improvements over state-of-the-art baselines.

Conclusion: The proposed multi-reward RL fine-tuning approach is effective and theoretically motivated, offering a general framework for aligning foundation models with multiple objectives.

Abstract: Fine-tuning foundation models has emerged as a powerful approach for
generating objects with specific desired properties. Reinforcement learning
(RL) provides an effective framework for this purpose, enabling models to
generate outputs that maximize a given reward function. However, in many
applications such as text generation and drug discovery, it can be suboptimal
to optimize using a single reward signal, as multiple evaluation criteria are
often necessary. This paper proposes a novel reinforcement learning-based
method for fine-tuning foundation models using multiple reward signals. By
employing an iterative fine-tuning strategy across these rewards, our approach
generalizes state-of-the-art RL-based methods. We further provide a theoretical
analysis that offers insights into the performance of multi-reward RL
fine-tuning. Experimental results across diverse domains including text,
biological sequence, and small molecule generation, demonstrate the
effectiveness of the proposed algorithm compared to state-of-the-art baselines.

</details>


### [238] [Melanoma Classification Through Deep Ensemble Learning and Explainable AI](https://arxiv.org/abs/2511.00246)
*Wadduwage Shanika Perera,ABM Islam,Van Vung Pham,Min Kyung An*

Main category: cs.LG

TL;DR: An ensemble of three deep transfer learning networks for melanoma detection, augmented with Explainable AI (XAI) to explain predictions and improve reliability.


<details>
  <summary>Details</summary>
Motivation: Melanoma is highly deadly; while DL can detect early signs, its black-box nature limits trust and adoption. XAI is proposed to interpret model decisions and enhance reliability in healthcare diagnostics.

Method: Ensemble learning of three state-of-the-art deep transfer learning networks combined with XAI techniques to explain the basis of predictions and improve diagnostic reliability.

Result: The abstract does not report empirical results; it presents a proposed framework rather than findings.

Conclusion: The paper proposes a DL ensemble with XAI explanations to enhance reliability and interpretability in melanoma detection.

Abstract: Melanoma is one of the most aggressive and deadliest skin cancers, leading to
mortality if not detected and treated in the early stages. Artificial
intelligence techniques have recently been developed to help dermatologists in
the early detection of melanoma, and systems based on deep learning (DL) have
been able to detect these lesions with high accuracy. However, the entire
community must overcome the explainability limit to get the maximum benefit
from DL for diagnostics in the healthcare domain. Because of the black box
operation's shortcomings in DL models' decisions, there is a lack of
reliability and trust in the outcomes. However, Explainable Artificial
Intelligence (XAI) can solve this problem by interpreting the predictions of AI
systems. This paper proposes a machine learning model using ensemble learning
of three state-of-the-art deep transfer Learning networks, along with an
approach to ensure the reliability of the predictions by utilizing XAI
techniques to explain the basis of the predictions.

</details>


### [239] [A Tight Lower Bound for Non-stochastic Multi-armed Bandits with Expert Advice](https://arxiv.org/abs/2511.00257)
*Zachary Chase,Shinji Ito,Idan Mehalel*

Main category: cs.LG

TL;DR: The minimax optimal expected regret in the non-stochastic multi-armed bandit with expert advice is Î(â(T K log(N/K))). A matching lower bound to Kale (2014) upper bound is established.


<details>
  <summary>Details</summary>
Motivation: To establish a tight, minimax regret rate for the non-stochastic bandit with expert advice problem and clarify how regret scales with the number of arms K, experts N, and horizon T.

Method: Derive a minimax lower bound that matches the known upper bound of Kale (2014), using standard adversarial construction and information-theoretic arguments to show no algorithm can beat Î(â(T K log(N/K))).

Result: The minimax optimal expected regret is Î(â(T K log(N/K))).

Conclusion: The upper bound from Kale (2014) is tight; the minimax rate for this problem is fully characterized by Î(â(T K log(N/K))).

Abstract: We determine the minimax optimal expected regret in the classic
non-stochastic multi-armed bandit with expert advice problem, by proving a
lower bound that matches the upper bound of Kale (2014). The two bounds
determine the minimax optimal expected regret to be $\Theta\left( \sqrt{T K
\log (N/K) } \right)$, where $K$ is the number of arms, $N$ is the number of
experts, and $T$ is the time horizon.

</details>


### [240] [X-TRACK: Physics-Aware xLSTM for Realistic Vehicle Trajectory Prediction](https://arxiv.org/abs/2511.00266)
*Aanchal Rajesh Chugh,Marion Neumeier,Sebastian Dorn*

Main category: cs.LG

TL;DR: Introduces xLSTM-based vehicle trajectory predictor X-TRAJ and physics-aware X-TRACK, integrating kinematics to produce realistic trajectories; shows X-TRACK outperforms baselines on highD and NGSIM.


<details>
  <summary>Details</summary>
Motivation: LSTM variants have limitations in modeling long-term temporal dependencies; xLSTM's exponential gating and enhanced memory can better capture long-range patterns in time-series data. Vehicle trajectory prediction requires realistic, feasible paths that respect motion physics, motivating physics-informed constraints.

Method: Develops X-TRAJ, an xLSTM-based framework for vehicle trajectory prediction, and a physics-aware variant X-TRACK that enforces motion-kinematics constraints during learning to ensure feasible trajectories. Evaluation uses the highD and NGSIM datasets to benchmark against state-of-the-art baselines.

Result: Empirical evaluation shows X-TRACK outperforms state-of-the-art baselines on the highD and NGSIM trajectory datasets.

Conclusion: Integrating physics-based constraints with xLSTM-based trajectory models improves realism and predictive accuracy, suggesting physics-informed xLSTM approaches are promising for traffic trajectory prediction.

Abstract: Recent advancements in Recurrent Neural Network (RNN) architectures,
particularly the Extended Long Short Term Memory (xLSTM), have addressed the
limitations of traditional Long Short Term Memory (LSTM) networks by
introducing exponential gating and enhanced memory structures. These
improvements make xLSTM suitable for time-series prediction tasks as they
exhibit the ability to model long-term temporal dependencies better than LSTMs.
Despite their potential, these xLSTM-based models remain largely unexplored in
the context of vehicle trajectory prediction. Therefore, this paper introduces
a novel xLSTM-based vehicle trajectory prediction framework, X-TRAJ, and its
physics-aware variant, X-TRACK (eXtended LSTM for TRAjectory prediction
Constraint by Kinematics), which explicitly integrates vehicle motion
kinematics into the model learning process. By introducing physical
constraints, the proposed model generates realistic and feasible trajectories.
A comprehensive evaluation on the highD and NGSIM datasets demonstrates that
X-TRACK outperforms state-of-the-art baselines.

</details>


### [241] [Improving the Robustness of Control of Chaotic Convective Flows with Domain-Informed Reinforcement Learning](https://arxiv.org/abs/2511.00272)
*Michiel Straat,Thorben Markmann,Sebastian Peitz,Barbara Hammer*

Main category: cs.LG

TL;DR: Domain-informed reinforcement learning with PPO improves control of Rayleigh-BÃ©nard convection across laminar and chaotic regimes, achieving 33% heat-transport reduction in laminar flows and 10% in chaotic flows, with better stability and cross-regime generalization compared to conventional controllers.


<details>
  <summary>Details</summary>
Motivation: Stabilizing chaotic convective flows is hard; existing RL works mainly in laminar regimes and may fail in chaotic/turbulent dynamics; real-world deployment requires robust generalization and sample efficiency.

Method: Train domain-informed PPO agents across diverse initial conditions and flow regimes for RBC; incorporate domain knowledge into the reward via a Benard cell merging term; compare domain-informed vs uninformed agents; assess heat transport reduction and convergence; evaluate generalization without retraining.

Result: Domain-informed agents achieve up to 33% reduction in heat transport in laminar RBC and 10% in chaotic RBC; outperform conventional controllers; domain-informed rewards yield steady flows, faster training convergence, and cross-regime generalization without retraining.

Conclusion: Incorporating domain priors into RL for chaotic flows substantially improves robustness and generalization, bringing RL-based control of chaotic convection closer to real-world deployment.

Abstract: Chaotic convective flows arise in many real-world systems, such as
microfluidic devices and chemical reactors. Stabilizing these flows is highly
desirable but remains challenging, particularly in chaotic regimes where
conventional control methods often fail. Reinforcement Learning (RL) has shown
promise for control in laminar flow settings, but its ability to generalize and
remain robust under chaotic and turbulent dynamics is not well explored,
despite being critical for real-world deployment. In this work, we improve the
practical feasibility of RL-based control of such flows focusing on
Rayleigh-B\'enard Convection (RBC), a canonical model for convective heat
transport. To enhance generalization and sample efficiency, we introduce
domain-informed RL agents that are trained using Proximal Policy Optimization
across diverse initial conditions and flow regimes. We incorporate domain
knowledge in the reward function via a term that encourages B\'enard cell
merging, as an example of a desirable macroscopic property. In laminar flow
regimes, the domain-informed RL agents reduce convective heat transport by up
to 33%, and in chaotic flow regimes, they still achieve a 10% reduction, which
is significantly better than the conventional controllers used in practice. We
compare the domain-informed to uninformed agents: Our results show that the
domain-informed reward design results in steady flows, faster convergence
during training, and generalization across flow regimes without retraining. Our
work demonstrates that elegant domain-informed priors can greatly enhance the
robustness of RL-based control of chaotic flows, bringing real-world deployment
closer.

</details>


### [242] [Calibration Across Layers: Understanding Calibration Evolution in LLMs](https://arxiv.org/abs/2511.00280)
*Abhinav Joshi,Areeb Ahmad,Ashutosh Modi*

Main category: cs.LG

TL;DR: Calibration in LLMs is distributed across depth, featuring a late-stage confidence-correction phase in upper layers and a low-dimensional residual-direction that improves calibration (ECE/MCE) without hurting accuracy.


<details>
  <summary>Details</summary>
Motivation: To understand how calibration emerges and evolves inside LLMs, moving beyond final-layer explanations, and to determine whether confidence regulation is distributed throughout the forward pass.

Method: Analyze multiple open-weight LLMs on the MMLU benchmark; identify a late-phase confidence correction in upper layers; uncover a low-dimensional calibration direction in the residual stream; perturb this direction to evaluate effects on calibration metrics (ECE/MCE) and accuracy.

Result: A distinct calibration/confidence correction phase is found in the upper layers; a low-dimensional perturbation direction in the residual stream significantly improves calibration metrics without reducing accuracy; calibration appears to be a distributed phenomenon across the network.

Conclusion: Calibration within LLMs is distributed throughout the forward pass, not confined to the final projection; this yields new insights into confidence-regulating mechanisms and potential intervention points for improving model calibration.

Abstract: Large Language Models (LLMs) have demonstrated inherent calibration
capabilities, where predicted probabilities align well with correctness,
despite prior findings that deep neural networks are often overconfident.
Recent studies have linked this behavior to specific components in the final
layer, such as entropy neurons and the unembedding matrix null space. In this
work, we provide a complementary perspective by investigating how calibration
evolves throughout the network depth. Analyzing multiple open-weight models on
the MMLU benchmark, we uncover a distinct confidence correction phase in the
upper/later layers, where model confidence is actively recalibrated after
decision certainty has been reached. Furthermore, we identify a low-dimensional
calibration direction in the residual stream whose perturbation significantly
improves calibration metrics (ECE and MCE) without harming accuracy. Our
findings suggest that calibration is a distributed phenomenon, shaped
throughout the network forward pass, not just in its final projection,
providing new insights into how confidence-regulating mechanisms operate within
LLMs.

</details>


### [243] [A systematic evaluation of uncertainty quantification techniques in deep learning: a case study in photoplethysmography signal analysis](https://arxiv.org/abs/2511.00301)
*Ciaran Bench,Oskar Pfeffer,Vivek Desai,Mohammad Moulaeifard,LoÃ¯c Coquelin,Peter H. Charlton,Nils Strodthoff,Nando Hegemann,Philip J. Aston,Andrew Thompson*

Main category: cs.LG

TL;DR: Evaluates eight uncertainty quantification methods for deep learning on medical time-series (AF detection and BP regression) and finds reliability is highly task-, metric-, and scale-dependent; local calibration and adaptivity offer practical insights for real-world use.


<details>
  <summary>Details</summary>
Motivation: Provide trustworthy, actionable predictions for clinicians monitoring patients via wearable data by quantifying model uncertainty and guiding interpretation.

Method: Train DL models on two clinically relevant tasks (AF detection; two BP regression variants) and apply eight UQ techniques. Develop a comprehensive evaluation pipeline comparing reliability across methods, with emphasis on calibration (global vs local) and adaptivity, and considerations for small per-patient measurement regimes.

Result: No single UQ method universally outperforms others. Reliability depends on the technique, the chosen uncertainty expression, evaluation metric, and the scale of reliability. Local calibration and adaptivity yield practically relevant insights not captured by global metrics, and evaluation criteria should align with the modelâs intended clinical use where few measurements per patient require strong small-scale reliability with minimal predictive loss.

Conclusion: UQ assessment should be tailored to the specific use case. Prioritize local calibration and adaptivity, and adopt evaluation metrics that reflect practical clinical constraints and the desired balance between uncertainty fidelity and predictive performance.

Abstract: In principle, deep learning models trained on medical time-series, including
wearable photoplethysmography (PPG) sensor data, can provide a means to
continuously monitor physiological parameters outside of clinical settings.
However, there is considerable risk of poor performance when deployed in
practical measurement scenarios leading to negative patient outcomes. Reliable
uncertainties accompanying predictions can provide guidance to clinicians in
their interpretation of the trustworthiness of model outputs. It is therefore
of interest to compare the effectiveness of different approaches. Here we
implement an unprecedented set of eight uncertainty quantification (UQ)
techniques to models trained on two clinically relevant prediction tasks:
Atrial Fibrillation (AF) detection (classification), and two variants of blood
pressure regression. We formulate a comprehensive evaluation procedure to
enable a rigorous comparison of these approaches. We observe a complex picture
of uncertainty reliability across the different techniques, where the most
optimal for a given task depends on the chosen expression of uncertainty,
evaluation metric, and scale of reliability assessed. We find that assessing
local calibration and adaptivity provides practically relevant insights about
model behaviour that otherwise cannot be acquired using more commonly
implemented global reliability metrics. We emphasise that criteria for
evaluating UQ techniques should cater to the model's practical use case, where
the use of a small number of measurements per patient places a premium on
achieving small-scale reliability for the chosen expression of uncertainty,
while preserving as much predictive performance as possible.

</details>


### [244] [A Technical Exploration of Causal Inference with Hybrid LLM Synthetic Data](https://arxiv.org/abs/2511.00318)
*Dana Kim,Yichen Xu,Tiffany Lin*

Main category: cs.LG

TL;DR: State-of-the-art synthetic data (GAN/LLM) can achieve high predictive fidelity but substantially misestimate causal effects like ATE. The paper proposes a hybrid generation framework that fuses model-based covariate synthesis (with distance-to-closest-record filtering) and separately learned propensity/outcome models to preserve the causal WâAâY structure, plus a synthetic pairing strategy to mitigate positivity violations, along with a protocol to benchmark causal estimators using unlimited synthetic samples.


<details>
  <summary>Details</summary>
Motivation: To address the gap where high predictive accuracy of synthetic data does not guarantee accurate estimation of causal parameters (e.g., ATE). The authors aim to enable LLM-powered data pipelines that support robust causal inference by preserving causal relationships in synthetic datasets.

Method: 1) Demonstrate that current GAN- and LLM-based synthetic data generators distort causal effects despite high predictive fidelity. 2) Propose a hybrid framework: (a) model-based covariate synthesis guided by distance-to-closest-record filtering; (b) separately learned propensity and outcome models to maintain causal mechanisms; (c) a synthetic pairing strategy to alleviate positivity violations; (d) an evaluation protocol using unlimited synthetic samples to benchmark IPTW, AIPW, and substitution under complex covariate distributions.

Result: The framework aims to ensure that the (W, A, Y) triplets retain their underlying causal structure, addressing the misestimation of causal effects observed with existing synthetic data methods. It provides a principled evaluation protocol and a code release for reproducibility.

Conclusion: This work lays groundwork for robust causal analysis with LLM-powered synthetic data, showing how to couple covariate synthesis with explicit causal models and evaluation to support reliable causal inference in synthetic environments.

Abstract: Large Language Models (LLMs) offer a flexible means to generate synthetic
tabular data, yet existing approaches often fail to preserve key causal
parameters such as the average treatment effect (ATE). In this technical
exploration, we first demonstrate that state-of-the-art synthetic data
generators, both GAN- and LLM-based, can achieve high predictive fidelity while
substantially misestimating causal effects. To address this gap, we propose a
hybrid generation framework that combines model-based covariate synthesis
(monitored via distance-to-closest-record filtering) with separately learned
propensity and outcome models, thereby ensuring that (W, A, Y) triplets retain
their underlying causal structure. We further introduce a synthetic pairing
strategy to mitigate positivity violations and a realistic evaluation protocol
that leverages unlimited synthetic samples to benchmark traditional estimators
(IPTW, AIPW, substitution) under complex covariate distributions. This work
lays the groundwork for LLM-powered data pipelines that support robust causal
analysis. Our code is available at
https://github.com/Xyc-arch/llm-synthetic-for-causal-inference.git.

</details>


### [245] [Reject Only Critical Tokens: Pivot-Aware Speculative Decoding](https://arxiv.org/abs/2511.00351)
*Amir Ziashahabi,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Mostafa El-Khamy,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.LG

TL;DR: Pivot-Aware Speculative Decoding (PAD) relaxes exact distribution matching in Speculative Decoding by optimizing for task utility instead. It labels pivot (critical) tokens that would drop final utility and trains a lightweight classifier to detect them, rejecting only those tokens. This yields higher acceptance and up to 2.5x speedups with comparable utility, making decoding more practical for real-world LLM tasks.


<details>
  <summary>Details</summary>
Motivation: Exact distribution matching in Speculative Decoding is overly stringent and can throttle decoding speed, because the primary goal in many applications is task utility (e.g., correctness, factual accuracy) rather than perfect distribution replication. A utility-focused decoding objective better aligns with real-world LLM use cases.

Method: The approach defines pivot tokens as those whose choice would reduce the final output's utility. Tokens are labeled as pivotal or non-pivotal, and a lightweight classifier is trained to detect pivots. PAD acts as a relaxed version of standard SD by rejecting only pivot tokens, increasing acceptance while preserving utility. The method includes labeling strategy, classifier training, and evaluation across multiple datasets, with code released.

Result: Empirical evaluation shows up to 2.5Ã speedup with comparable task utility to baseline methods across diverse datasets, demonstrating that utility-aligned decoding can substantially accelerate generation without sacrificing performance.

Conclusion: Reframing decoding to optimize for expected utility rather than exact distribution matching yields practical speedups. PAD offers a viable, more efficient alternative to SD while maintaining task performance, aligning decoding with real-world use cases.

Abstract: Speculative Decoding (SD) ensures that the output matches the target model's
distribution exactly. However, we argue that this distribution matching
requirement is too stringent and results in unnecessarily low acceptance rates,
limiting potential speedups. Instead, we advocate a reformulation of the
decoding objective: the proposed decoding strategy should match the expected
utility, i.e., the task-specific performance, of the target model. This
perspective also aligns better with real-world use cases of LLMs, where utility
(e.g., code correctness, factual accuracy) is often more important than
sampling distribution. Based on this reformulation, we propose a novel decoding
strategy: Pivot-Aware Speculative Decoding, which rejects only those tokens
that would lead to a utility drop in the final output. We refer to these
critical tokens as pivot tokens. We propose a method for labeling tokens as
pivotal or non-pivotal and train a lightweight classifier to detect them. This
method can be viewed as a relaxed version of standard SD, which offers much
higher acceptance while preserving utility. We evaluate our method across
various datasets, demonstrating that we can achieve up to $2.5\times$ speedup
with comparable utility. Source code is available at
https://github.com/amir-zsh/PAD.

</details>


### [246] [Toward Unifying Group Fairness Evaluation from a Sparsity Perspective](https://arxiv.org/abs/2511.00359)
*Zhecheng Sheng,Jiawei Zhang,Enmao Diao*

Main category: cs.LG

TL;DR: A sparsity-based unified framework for evaluating algorithmic fairness that connects multiple fairness criteria, showing broad applicability and effectiveness across tasks and bias mitigation methods.


<details>
  <summary>Details</summary>
Motivation: Fairness criteria in ML are numerous and often lack generalizability across different problems; there is a need for a unified, generalizable evaluation approach.

Method: Introduce a unified sparsity-based framework for evaluating algorithmic fairness, aligning with existing fairness criteria, and validating it as an evaluation metric through extensive experiments across diverse datasets and bias mitigation methods.

Result: The framework demonstrates broad applicability and effectiveness as an evaluation metric, aligning with existing fairness criteria and enabling consistent assessment across tasks.

Conclusion: Framing fairness through sparsity and social equity offers a novel perspective with potential wide impact on fairness research and applications.

Abstract: Ensuring algorithmic fairness remains a significant challenge in machine
learning, particularly as models are increasingly applied across diverse
domains. While numerous fairness criteria exist, they often lack
generalizability across different machine learning problems. This paper
examines the connections and differences among various sparsity measures in
promoting fairness and proposes a unified sparsity-based framework for
evaluating algorithmic fairness. The framework aligns with existing fairness
criteria and demonstrates broad applicability to a wide range of machine
learning tasks. We demonstrate the effectiveness of the proposed framework as
an evaluation metric through extensive experiments on a variety of datasets and
bias mitigation methods. This work provides a novel perspective to algorithmic
fairness by framing it through the lens of sparsity and social equity, offering
potential for broader impact on fairness research and applications.

</details>


### [247] [Balancing Interpretability and Performance in Motor Imagery EEG Classification: A Comparative Study of ANFIS-FBCSP-PSO and EEGNet](https://arxiv.org/abs/2511.00369)
*Farjana Aktar,Mohd Ruhul Ameen,Akif Islam,Md Ekramul Hamid*

Main category: cs.LG

TL;DR: A comparative study of a transparent fuzzy ANFIS-FBCSP-PSO model versus a deep EEGNet benchmark on MI-BCI data shows interpretability benefits within subjects and robustness benefits across subjects, guiding design choices between interpretability and cross-user generalization; future work points to transformer-based and hybrid neuro-symbolic approaches.


<details>
  <summary>Details</summary>
Motivation: To address the dual challenge in MI-EEG classification of achieving high accuracy while maintaining interpretability, and to assess cross-subject generalization using established benchmarks (BCI Competition IV-2a).

Method: ANFIS-FBCSP-PSO combines filter bank common spatial pattern feature extraction with fuzzy IF-THEN rules optimized by particle swarm optimization; EEGNet serves as a deep learning baseline that learns hierarchical spatial-temporal representations directly from raw EEG.

Result: Within-subject accuracy: 68.58% Â± 13.76; kappa 58.04% Â± 18.43. Cross-subject LOSO accuracy: 68.20% Â± 12.13; kappa 57.33% Â± 16.22. The fuzzy model outperforms in within-subject evaluations, while the deep model shows stronger generalization across subjects.

Conclusion: The study provides practical guidance on selecting MI-BCI systems based on design goalsâprioritize interpretability for within-subject use or robustness across users for cross-subject deploymentâand suggests future exploration of transformer-based and hybrid neuro-symbolic frameworks to advance transparent EEG decoding.

Abstract: Achieving both accurate and interpretable classification of motor imagery EEG
remains a key challenge in brain computer interface (BCI) research. This paper
compares a transparent fuzzy reasoning approach (ANFIS-FBCSP-PSO) with a deep
learning benchmark (EEGNet) using the BCI Competition IV-2a dataset. The ANFIS
pipeline combines filter bank common spatial pattern feature extraction with
fuzzy IF-THEN rules optimized via particle swarm optimization, while EEGNet
learns hierarchical spatial temporal representations directly from raw EEG
data. In within-subject experiments, the fuzzy neural model performed better
(68.58 percent +/- 13.76 percent accuracy, kappa = 58.04 percent +/- 18.43),
while in cross-subject (LOSO) tests, the deep model exhibited stronger
generalization (68.20 percent +/- 12.13 percent accuracy, kappa = 57.33 percent
+/- 16.22). The study provides practical guidance for selecting MI-BCI systems
according to design goals: interpretability or robustness across users. Future
investigations into transformer based and hybrid neuro symbolic frameworks are
expected to advance transparent EEG decoding.

</details>


### [248] [PolyRecommender: A Multimodal Recommendation System for Polymer Discovery](https://arxiv.org/abs/2511.00375)
*Xin Wang,Yunhao Xiao,Rui Qiao*

Main category: cs.LG

TL;DR: PolyRecommender is a multimodal polymer discovery framework that combines PolyBERT chemical language representations with graph-based molecular embeddings to retrieve candidates via language similarity and rank them through fused multimodal embeddings across multiple properties.


<details>
  <summary>Details</summary>
Motivation: To enhance polymer discovery by integrating complementary information from language-based representations and molecular graphs, enabling efficient retrieval and robust multi-property ranking.

Method: Retrieve candidate polymers using language-based similarity (PolyBERT), then fuse PolyBERT and graph-encoder embeddings to rank polymers for multiple target properties.

Result: Proposes a generalizable multimodal paradigm for AI-guided polymer design, showing improved retrieval and ranking by leveraging complementary modalities across properties.

Conclusion: A versatile framework that advances multimodal material discovery and can be extended to other polymer design tasks.

Abstract: We introduce PolyRecommender, a multimodal discovery framework that
integrates chemical language representations from PolyBERT with molecular
graph-based representations from a graph encoder. The system first retrieves
candidate polymers using language-based similarity and then ranks them using
fused multimodal embeddings according to multiple target properties. By
leveraging the complementary knowledge encoded in both modalities,
PolyRecommender enables efficient retrieval and robust ranking across related
polymer properties. Our work establishes a generalizable multimodal paradigm,
advancing AI-guided design for the discovery of next-generation polymers.

</details>


### [249] [UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings](https://arxiv.org/abs/2511.00405)
*Zhibin Lan,Liqiang Niu,Fandong Meng,Jie Zhou,Jinsong Su*

Main category: cs.LG

TL;DR: Proposes UME-R1, a universal multimodal embedding framework that learns both discriminative and generative embeddings via a two-stage training pipeline: cold-start supervised fine-tuning to enable reasoning and generation, followed by reinforcement learning to enhance generative embedding quality; shows gains on MMEB-V2 across 78 tasks and highlights inference-time sampling for better coverage.


<details>
  <summary>Details</summary>
Motivation: Current multimodal embeddings in MLLMs are mostly discriminative, limiting reasoning-driven generation. Generative embeddings could leverage reasoning capabilities to improve downstream tasks and interpretability, and enable complementary benefits with discriminative embeddings.

Method: Two-stage approach: (1) cold-start supervised fine-tuning to instill reasoning and enable generation of both embedding types; (2) reinforcement learning to further enhance reasoning and optimizes generative embedding quality. Includes repeated sampling at inference to boost task coverage (pass@k). evaluated on MMEB-V2 across 78 tasks (video, image, visual documents).

Result: Generative embeddings outperform conventional discriminative embeddings; discriminative and generative embeddings are complementary with oracle performance exceeding either alone; RL effectively enhances generative embeddings; inference-time sampling increases pass@k and shows scalability potential.

Conclusion: Generative embeddings enable reasoning-driven, interpretable multimodal embeddings and, with UME-R1, establish a foundation for unified generation and retrieval tasks; plan to release code, models, and datasets to support further research.

Abstract: The remarkable success of multimodal large language models (MLLMs) has driven
advances in multimodal embeddings, yet existing models remain inherently
discriminative, limiting their ability to benefit from reasoning-driven
generation paradigm. In this work, we pioneer the exploration of generative
embeddings, unifying embedding tasks within a generative paradigm. We propose
UME-R1, a universal multimodal embedding framework consisting of a two-stage
training strategy: a cold-start supervised fine-tuning equips the model with
reasoning capabilities and enables it to generate both discriminative and
generative embeddings; a subsequent reinforcement learning enhances reasoning
and further optimizes generative embedding quality. This pioneering work
reveals four key insights: 1) generative embeddings unlock substantial
performance gains over conventional discriminative embeddings by leveraging the
powerful generative reasoning capabilities of MLLMs; 2) discriminative and
generative embeddings are complementary, whose combined oracle performance far
exceeding that of either alone; 3) RL can effectively enhance generative
embeddings, establishing a scalable optimization paradigm.; 4) repeated
sampling at inference boosts downstream task coverage (pass@k), highlighting
the inference-time scalability potential of generative embeddings. Evaluated on
the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual
documents, UME-R1 significantly outperforms conventional discriminative
embedding models and offers a foundation for more interpretable,
reasoning-driven generative multimodal embeddings. Our code, models, and
datasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.

</details>


### [250] [Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling](https://arxiv.org/abs/2511.00411)
*Zenghao Niu,Weicheng Xie,Siyang Song,Zitong Yu,Feng Liu,Linlin Shen*

Main category: cs.LG

TL;DR: Gradient-Guided Sampling (GGS) balances attack potency and cross-model generalization by guiding inner-iteration sampling along the gradient from the previous step, using a random-magnitude step. It improves transferability of adversarial examples across architectures and multimodal models, outperforming existing methods, with code available.


<details>
  <summary>Details</summary>
Motivation: There is a fundamental trade-off in transfer attacks between Exploitation (maximizing local loss maxima for potency) and Exploration (flattening the loss landscape for cross-model generalization). Traditional momentum-based methods emphasize Exploitation and inner-iteration sampling emphasizes Exploration, leading to suboptimal compromises. A method is needed to harmonize both objectives.

Method: Extend MI-FGSM with inner-iteration random sampling guided by the gradient from the previous inner-iteration. The sampling magnitude is drawn from a random distribution, while the sampling direction aligns with the gradient ascent direction to encourage adversarial examples that sit in regions offering both flatness (for generalization) and high maxima (for potency). This yields a more efficient and stable sampling process.

Result: Empirical evaluations on multiple DNN architectures and multimodal LLMs show that Gradient-Guided Sampling (GGS) achieves superior transferability and robustness compared to state-of-the-art transfer attacks, demonstrating improved performance across diverse models.

Conclusion: GGS resolves the exploitationâexploration dilemma in transfer attacks by harmonizing potent local maxima with flat, generalizable loss surfaces through gradient-guided inner-iteration sampling. The approach is simple, effective, and yields better cross-model transferability; code is released for replication.

Abstract: Adversarial attacks present a critical challenge to deep neural networks'
robustness, particularly in transfer scenarios across different model
architectures. However, the transferability of adversarial attacks faces a
fundamental dilemma between Exploitation (maximizing attack potency) and
Exploration (enhancing cross-model generalization). Traditional momentum-based
methods over-prioritize Exploitation, i.e., higher loss maxima for attack
potency but weakened generalization (narrow loss surface). Conversely, recent
methods with inner-iteration sampling over-prioritize Exploration, i.e.,
flatter loss surfaces for cross-model generalization but weakened attack
potency (suboptimal local maxima). To resolve this dilemma, we propose a simple
yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives
through guiding sampling along the gradient ascent direction to improve both
sampling efficiency and stability. Specifically, based on MI-FGSM, GGS
introduces inner-iteration random sampling and guides the sampling direction
using the gradient from the previous inner-iteration (the sampling's magnitude
is determined by a random distribution). This mechanism encourages adversarial
examples to reside in balanced regions with both flatness for cross-model
generalization and higher local maxima for strong attack potency. Comprehensive
experiments across multiple DNN architectures and multimodal large language
models (MLLMs) demonstrate the superiority of our method over state-of-the-art
transfer attacks. Code is made available at https://github.com/anuin-cat/GGS.

</details>


### [251] [Bootstrap Off-policy with World Model](https://arxiv.org/abs/2511.00423)
*Guojian Zhan,Likun Wang,Xiangteng Zhang,Jiaxin Gao,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: BOOM tightly couples planning and off-policy learning via a bootstrap loop with a world model, achieving state-of-the-art stability and performance on DM Control Suite and Humanoid-Bench.


<details>
  <summary>Details</summary>
Motivation: Tackle the divergence between data distribution and policy behavior caused by online planning, which can impair model learning and policy improvement in reinforcement learning.

Method: Policy initializes the planner; the planner refines actions to bootstrap the policy through behavior alignment. A jointly learned world model enables the planner to simulate futures and provides value targets. Core: a likelihood-free alignment loss that bootstraps the policy using the planner's non-parametric action distribution, combined with a soft value-weighted mechanism to prioritize high-return behaviors and reduce variability from planner quality in the replay buffer.

Result: BOOM achieves state-of-the-art training stability and final performance on high-dimensional benchmarks (DeepMind Control Suite and Humanoid-Bench). Code is released at the provided GitHub URL.

Conclusion: Integrating planning and off-policy learning through a bootstrap loop with a world model yields more stable and higher-performing RL, suggesting broad applicability to model-based planning in policy optimization.

Abstract: Online planning has proven effective in reinforcement learning (RL) for
improving sample efficiency and final performance. However, using planning for
environment interaction inevitably introduces a divergence between the
collected data and the policy's actual behaviors, degrading both model learning
and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy
with WOrld Model), a framework that tightly integrates planning and off-policy
learning through a bootstrap loop: the policy initializes the planner, and the
planner refines actions to bootstrap the policy through behavior alignment.
This loop is supported by a jointly learned world model, which enables the
planner to simulate future trajectories and provides value targets to
facilitate policy improvement. The core of BOOM is a likelihood-free alignment
loss that bootstraps the policy using the planner's non-parametric action
distribution, combined with a soft value-weighted mechanism that prioritizes
high-return behaviors and mitigates variability in the planner's action quality
within the replay buffer. Experiments on the high-dimensional DeepMind Control
Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in
both training stability and final performance. The code is accessible at
https://github.com/molumitu/BOOM_MBRL.

</details>


### [252] [Tree Training: Accelerating Agentic LLMs Training via Shared Prefix Reuse](https://arxiv.org/abs/2511.00413)
*Shaojie Wang,Jinghui Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Liang Huang,Xiaojiang Zhang,Junyi Peng,Li Wan,Haotian Zhang,Bin Chen*

Main category: cs.LG

TL;DR: Tree Training reuses shared prefixes in tree-like token trajectories of agentic LLMs to avoid recomputation, achieving up to 3.9x training speedups.


<details>
  <summary>Details</summary>
Motivation: In agentic LLM rollouts, decisions yield a tree-structured trajectory due to memory retrieval and concurrent tool usage. Current pipelines decompose into linear segments, causing redundant computation on shared prefixes.

Method: Introduce Tree Training with Tree Packing to reuse shared computations across branches and Gradient Restoration to propagate gradients correctly; enables forward/backward reuse of shared prefixes.

Result: Experiments on multiple open-source models show up to 3.9x reduction in total training time, enabling more efficient supervised fine-tuning (SFT) and reinforcement learning (RL) training.

Conclusion: Tree Training effectively utilizes shared computations across branching trajectories, yielding substantial efficiency gains for large-scale agentic training and potentially accelerating development of autonomous LLM agents.

Abstract: In agentic LLM scenarios, an agent's interaction process during a single
rollout often exhibits branching behaviors. Due to memory retrieval and
concurrent tool executions at certain decision points, the token trajectory of
one task evolves into a tree-like structure rather than a linear sequence.
However, current training pipelines decompose such tree-structured trajectories
into separate linear segments, treating each branch as an independent sequence.
As a result, shared prefixes across these branches are repeatedly recomputed
during both forward and backward passes. To address this inefficiency, we
propose Tree Training, a paradigm that computes each shared prefix only once
and reuses its intermediate results across related branches during both forward
and backward passes, substantially improving computation efficiency in
large-scale agentic training. This is achieved via (i) Tree Packing, which
efficiently reuses shared computations across trajectories, and (ii) Gradient
Restoration, which ensures correct gradient propagation across reused prefixes.
Experiments on multiple open-source models demonstrate up to 3.9x reduction in
total training time, enabling more efficient agentic LLM SFT and RL training.

</details>


### [253] [Structure-Preserving Physics-Informed Neural Network for the Korteweg--de Vries (KdV) Equation](https://arxiv.org/abs/2511.00418)
*Victory Obieke,Emmanuel Oguadimma*

Main category: cs.LG

TL;DR: A structure-preserving PINN for the KdV equation that enforces mass and energy conservation via an invariant-aware loss and uses sinusoidal activations to capture oscillatory and dispersive dynamics, achieving stable long-term predictions and accurate soliton behavior.


<details>
  <summary>Details</summary>
Motivation: To overcome drift and instability in long-term PINN simulations of nonlinear PDEs by embedding physical invariants (mass and Hamiltonian energy) directly into the training objective and enhancing expressiveness with sinusoidal features.

Method: Incorporate conservation constraints for mass and Hamiltonian energy into the PINN loss. Replace standard tanh activations with sinusoidal activations to improve spectral expressiveness. Train on representative KdV scenarios (single soliton, two-soliton interaction, cosine-pulse) and perform ablations to assess impact of invariants and activation choice.

Result: The invariant-constrained, sinusoid-activated PINN reproduces key KdV behaviors (shape-preserving solitons, elastic soliton collisions with phase shifts, nonlinear dispersive breakup) while maintaining conserved quantities. Ablations show faster convergence, better long-term stability, and reduced drift without pretraining.

Conclusion: Invariant-aware regularization combined with sinusoidal representations yields robust, energy-consistent PINNs for Hamiltonian PDEs such as the KdV equation, offering improved long-term stability and physical fidelity.

Abstract: Physics-Informed Neural Networks (PINNs) offer a flexible framework for
solving nonlinear partial differential equations (PDEs), yet conventional
implementations often fail to preserve key physical invariants during long-term
integration. This paper introduces a \emph{structure-preserving PINN} framework
for the nonlinear Korteweg--de Vries (KdV) equation, a prototypical model for
nonlinear and dispersive wave propagation. The proposed method embeds the
conservation of mass and Hamiltonian energy directly into the loss function,
ensuring physically consistent and energy-stable evolution throughout training
and prediction. Unlike standard \texttt{tanh}-based
PINNs~\cite{raissi2019pinn,wang2022modifiedpinn}, our approach employs
sinusoidal activation functions that enhance spectral expressiveness and
accurately capture the oscillatory and dispersive nature of KdV solitons.
Through representative case studies -- including single-soliton propagation
(shape-preserving translation), two-soliton interaction (elastic collision with
phase shift), and cosine-pulse initialization (nonlinear dispersive breakup) --
the model successfully reproduces hallmark behaviors of KdV dynamics while
maintaining conserved invariants. Ablation studies demonstrate that combining
invariant-constrained optimization with sinusoidal feature mappings accelerates
convergence, improves long-term stability, and mitigates drift without
multi-stage pretraining. These results highlight that computationally
efficient, invariant-aware regularization coupled with sinusoidal
representations yields robust, energy-consistent PINNs for Hamiltonian partial
differential equations such as the KdV equation.

</details>


### [254] [Lyapunov Stability Learning with Nonlinear Control via Inductive Biases](https://arxiv.org/abs/2511.01283)
*Yupu Lu,Shijie Lin,Hao Xu,Zeqing Zhang,Jia Pan*

Main category: cs.LG

TL;DR: A neural CLF framework that treats Lyapunov conditions as inductive biases, enabling end-to-end learning of a CLF and a CLF-based controller with improved convergence and larger region of attraction, compared to prior methods.


<details>
  <summary>Details</summary>
Motivation: Lyapunov constraints are hard to satisfy and verify in learning-based CLF design; existing learner-verifier approaches suffer from poor global convergence and complex implementation.

Method: Incorporate Lyapunov conditions as inductive biases; design a neural CLF and a CLF-based controller guided by Lyapunov knowledge; enable stable optimization with limited constraints and end-to-end learning.

Result: Higher convergence rate and larger region of attraction across experiments; deeper analysis of why previous methods' success rates drop during learning.

Conclusion: Treat Lyapunov conditions as inductive biases to achieve efficient learning and stability guarantees, clarifying pitfalls of prior methods.

Abstract: Finding a control Lyapunov function (CLF) in a dynamical system with a
controller is an effective way to guarantee stability, which is a crucial issue
in safety-concerned applications. Recently, deep learning models representing
CLFs have been applied into a learner-verifier framework to identify
satisfiable candidates. However, the learner treats Lyapunov conditions as
complex constraints for optimisation, which is hard to achieve global
convergence. It is also too complicated to implement these Lyapunov conditions
for verification. To improve this framework, we treat Lyapunov conditions as
inductive biases and design a neural CLF and a CLF-based controller guided by
this knowledge. This design enables a stable optimisation process with limited
constraints, and allows end-to-end learning of both the CLF and the controller.
Our approach achieves a higher convergence rate and larger region of attraction
(ROA) in learning the CLF compared to existing methods among abundant
experiment cases. We also thoroughly reveal why the success rate decreases with
previous methods during learning.

</details>


### [255] [Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model](https://arxiv.org/abs/2511.00443)
*Ruthwik Reddy Doodipala,Pankaj Pandey,Carolina Torres Rojas,Manob Jyoti Saikia,Ranganatha Sitaram*

Main category: cs.LG

TL;DR: ROI-guided masking in self-supervised pretraining on resting-state fMRI (AAL3 ROIs) yields clearer representations and a 4.23 percentage-point gain in HC-vs-ADHD classification, with limbic and cerebellar regions driving reconstruction fidelity.


<details>
  <summary>Details</summary>
Motivation: To improve foundation-model pretraining for functional neuroimaging by introducing anatomically informed region-aware masking, aiming to boost downstream performance and interpretability across diverse fMRI tasks.

Method: Apply ROI-guided masking using the AAL3 atlas directly to full 4D resting-state fMRI volumes during self-supervised reconstruction. Compare against conventional random masking on the ADHD-200 dataset (973 subjects). Perform region-level attribution to identify dominant contributing regions.

Result: Achieves a 4.23% improvement in HC vs ADHD classification accuracy compared to random masking. Region-level attributions indicate the limbic region and cerebellum contribute most to reconstruction fidelity and to the learned representations.

Conclusion: Masking anatomical regions during self-supervised pretraining enhances interpretability and yields more robust, discriminative representations; future work includes validating on additional datasets and developing region-aware reconstruction losses.

Abstract: The emergence of foundation models in neuroimaging is driven by the
increasing availability of large-scale and heterogeneous brain imaging
datasets. Recent advances in self-supervised learning, particularly
reconstruction-based objectives, have demonstrated strong potential for
pretraining models that generalize effectively across diverse downstream
functional MRI (fMRI) tasks. In this study, we explore region-aware
reconstruction strategies for a foundation model in resting-state fMRI, moving
beyond approaches that rely on random region masking. Specifically, we
introduce an ROI-guided masking strategy using the Automated Anatomical
Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively
mask semantically coherent brain regions during self-supervised pretraining.
Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI
scans, we show that our method achieves a 4.23% improvement in classification
accuracy for distinguishing healthy controls from individuals diagnosed with
ADHD, compared to conventional random masking. Region-level attribution
analysis reveals that brain volumes within the limbic region and cerebellum
contribute most significantly to reconstruction fidelity and model
representation. Our results demonstrate that masking anatomical regions during
model pretraining not only enhances interpretability but also yields more
robust and discriminative representations. In future work, we plan to extend
this approach by evaluating it on additional neuroimaging datasets, and
developing new loss functions explicitly derived from region-aware
reconstruction objectives. These directions aim to further improve the
robustness and interpretability of foundation models for functional
neuroimaging.

</details>


### [256] [Deep Learning Approach to Anomaly Detection in Enterprise ETL Processes with Autoencoders](https://arxiv.org/abs/2511.00462)
*Xin Chen,Saili Uday Gadgil,Kangning Gao,Yi Hu,Cong Nie*

Main category: cs.LG

TL;DR: Deep autoencoder-based anomaly detection for enterprise ETL streams using reconstruction error and latent-space regularization, detecting delays, missing values, duplicates, and sudden changes.


<details>
  <summary>Details</summary>
Motivation: Enterprise ETL data streams exhibit diverse anomalies that can disrupt data quality and analytics; robust, scalable anomaly detection is needed.

Method: Train a deep encoder-decoder (autoencoder) with data standardization; learn latent representations; use reconstruction error as the anomaly score; apply regularization in latent space to promote sparsity and robust distribution learning; evaluate under varying hyperparameters, environments, and data characteristics.

Result: The method achieves superior performance in AUC, ACC, Precision, and Recall across different settings and data characteristics, indicating effective capture of latent distribution patterns and accurate anomaly identification in enterprise ETL streams.

Conclusion: Deep autoencoder-based anomaly detection is a reliable mechanism for detecting diverse anomalies in enterprise ETL data streams, supporting reliable enterprise data processing and intelligent analysis.

Abstract: An anomaly detection method based on deep autoencoders is proposed to address
anomalies that often occur in enterprise-level ETL data streams. The study
first analyzes multiple types of anomalies in ETL processes, including delays,
missing values, duplicate loading, and sudden abnormal changes, and applies
data standardization and feature modeling to ensure stable and usable inputs.
In the method design, the encoder-decoder structure compresses high-dimensional
inputs into latent representations and reconstructs them, while reconstruction
error is used to measure anomaly levels. Regularization constraints are
introduced in the latent space to enhance feature sparsity and distribution
learning, thereby improving robustness in complex data streams. Systematic
analyses under different hyperparameter settings, environmental changes, and
data characteristics show that the proposed method achieves superior
performance in AUC, ACC, Precision, and Recall. The results demonstrate that
the deep autoencoder-based detection mechanism can effectively capture latent
distribution patterns in enterprise-level ETL data streams and accurately
identify diverse anomalies, providing reliable support for enterprise data
processing and intelligent analysis.

</details>


### [257] [Why Federated Optimization Fails to Achieve Perfect Fitting? A Theoretical Perspective on Client-Side Optima](https://arxiv.org/abs/2511.00469)
*Zhongxiang Lei,Qi Yang,Ping Qiu,Gang Zhang,Yuanchi Ma,Jinyan Liu*

Main category: cs.LG

TL;DR: Heterogeneous client data in federated optimization lead to distinct local optima, which raises the global objective's lower bound and causes the final model to oscillate within a region rather than converge, explaining non-iid performance degradation; validated empirically and with open-source code.


<details>
  <summary>Details</summary>
Motivation: Explain why data heterogeneity degrades federated learning performance beyond standard convergence guarantees and link theory with observed practice by identifying where optimization dynamics fail under non-iid data.

Method: The paper adopts a theoretical framework assuming heterogeneous local optima across clients, derives two main consequences (increased lower bound on the global objective and oscillation of the global model in a region at the end of training), and validates the theory with experiments across multiple tasks and neural network architectures; the approach is complemented by an open-source framework (fedtorch).

Result: 1) The distance between clients' local optima raises the lower bound of the global objective, making perfect fitting of all client data impossible. 2) In the final training stage, the global model oscillates within a region rather than converging to a single optimum, limiting data fit. Empirical results across tasks and architectures corroborate the theoretical explanation.

Conclusion: Provides a principled theoretical explanation for performance degradation in non-iid federated learning and offers a framework to analyze and potentially mitigate such degradation; the authors also provide an open-source implementation for reproducibility.

Abstract: Federated optimization is a constrained form of distributed optimization that
enables training a global model without directly sharing client data. Although
existing algorithms can guarantee convergence in theory and often achieve
stable training in practice, the reasons behind performance degradation under
data heterogeneity remain unclear. To address this gap, the main contribution
of this paper is to provide a theoretical perspective that explains why such
degradation occurs. We introduce the assumption that heterogeneous client data
lead to distinct local optima, and show that this assumption implies two key
consequences: 1) the distance among clients' local optima raises the lower
bound of the global objective, making perfect fitting of all client data
impossible; and 2) in the final training stage, the global model oscillates
within a region instead of converging to a single optimum, limiting its ability
to fully fit the data. These results provide a principled explanation for
performance degradation in non-iid settings, which we further validate through
experiments across multiple tasks and neural network architectures. The
framework used in this paper is open-sourced at:
https://github.com/NPCLEI/fedtorch.

</details>


### [258] [Variational Autoencoder for Calibration: A New Approach](https://arxiv.org/abs/2511.00475)
*Travis Barrett,Amit Kumar Mishra,Joyce Mwangama*

Main category: cs.LG

TL;DR: A VAE-based calibration framework that uses the latent space as calibration output, demonstrated on a multi-sensor gas dataset, achieving calibration while also reconstructing inputs with outputs close to truth data.


<details>
  <summary>Details</summary>
Motivation: Sensor networks require accurate calibration; the work proposes a joint calibration-learning approach within a Variational Autoencoder to both calibrate sensor data and reconstruct it.

Method: Develop a VAE where latent variables encode calibration parameters; train on sensor data with ground-truth calibration, using a proof-of-concept on an existing multi-sensor gas dataset; evaluate calibration quality and reconstruction similarity.

Result: The model functions as both a calibration model and an autoencoder; outputs (calibrated and reconstructed) are statistically similar to the ground-truth data.

Conclusion: Shows promise for unified calibration and reconstruction; warrants broader testing and expansion in future work.

Abstract: In this paper we present a new implementation of a Variational Autoencoder
(VAE) for the calibration of sensors. We propose that the VAE can be used to
calibrate sensor data by training the latent space as a calibration output. We
discuss this new approach and show a proof-of-concept using an existing
multi-sensor gas dataset. We show the performance of the proposed calibration
VAE and found that it was capable of performing as calibration model while
performing as an autoencoder simultaneously. Additionally, these models have
shown that they are capable of creating statistically similar outputs from both
the calibration output as well as the reconstruction output to their respective
truth data. We then discuss the methods of future testing and planned expansion
of this work.

</details>


### [259] [Reasoning Planning for Language Models](https://arxiv.org/abs/2511.00521)
*Bao Nguyen,Hieu Trung Nguyen,Ruifeng She,Xiaojin Fu,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: Challenges the belief that more candidate outputs improve accuracy; derives accuracy bounds for standard aggregation under fixed distributions; proposes EPIC to learn a shared representation for reasoning methods and query-method compatibility, regularized by probability bounds to balance accuracy and cost; experiments show improvements in mathematical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Understanding when ensemble/aggregation methods for selecting LM outputs are effective and how to optimize the trade-off between accuracy and computation; address the assumption that more candidates always help.

Method: 1) Theoretically derive accuracy bounds for common aggregation methods given fixed generation distributions and candidate sizes. 2) Propose EPIC (Ensemble Planning with Contrastive learning) to learn a shared representation space that encodes model reasoning abilities and query-method compatibility. 3) Use the probabilistic bounds as a regularizer in a utility-driven optimization that balances accuracy and computational cost. 4) Empirically evaluate on diverse mathematical reasoning tasks.

Result: EPIC consistently selects optimal reasoning methods, achieving higher accuracy while reducing computational overhead compared to baselines.

Conclusion: Incorporating probability bounds into a contrastive, representation-learning framework enables cost-aware, accurate method selection for LM reasoning. EPIC demonstrates practical gains on mathematical reasoning benchmarks and is available with code at the provided GitHub URL.

Abstract: Selecting an appropriate reasoning method for a given query remains a key
challenge in language model generation. Existing approaches typically generate
multiple candidate responses and use an aggregation strategy to select the
output answer, often assuming that more candidate answers yield higher
accuracy. We revisit this assumption through a rigorous theoretical analysis,
deriving accuracy bounds for standard aggregation methods under fixed
generation distributions and candidate sizes. Building on these insights, we
introduce EPIC, an Ensemble Planning with Contrastive learning framework to
learn a shared representation space that captures both model reasoning
abilities and query-method compatibility. EPIC incorporates our probability
bounds as a regularizer in a utility-driven optimization that balances accuracy
and computational cost. Experiments on diverse mathematical reasoning tasks
show that EPIC consistently selects optimal reasoning methods, improving
accuracy while reducing computational overhead. Our code can be found at
https://github.com/nguyenngocbaocmt02/EPIC.

</details>


### [260] [Air Pollution Forecasting in Bucharest](https://arxiv.org/abs/2511.00532)
*DragoÅ-Andrei Åerban,RÄzvan-Alexandru SmÄdu,Dumitru-Clementin Cercel*

Main category: cs.LG

TL;DR: The paper aims to compare a broad range of ML models (linear, ensemble, deep learning, transformers, and LLMs) for PM2.5 forecasting across multiple horizons.


<details>
  <summary>Details</summary>
Motivation: PM2.5 pollution poses health risks; forecasting enables early warnings and prevention; evaluating diverse models helps identify effective approaches for urban air quality prediction.

Method: Design, fine-tune, test, and evaluate multiple ML models on PM2.5 forecasting across various time horizons.

Result: Abstract does not report results.

Conclusion: Abstract does not provide conclusions.

Abstract: Air pollution, especially the particulate matter 2.5 (PM2.5), has become a
growing concern in recent years, primarily in urban areas. Being exposed to air
pollution is linked to developing numerous health problems, like the
aggravation of respiratory diseases, cardiovascular disorders, lung function
impairment, and even cancer or early death. Forecasting future levels of PM2.5
has become increasingly important over the past few years, as it can provide
early warnings and help prevent diseases. This paper aims to design, fine-tune,
test, and evaluate machine learning models for predicting future levels of
PM2.5 over various time horizons. Our primary objective is to assess and
compare the performance of multiple models, ranging from linear regression
algorithms and ensemble-based methods to deep learning models, such as advanced
recurrent neural networks and transformers, as well as large language models,
on this forecasting task.

</details>


### [261] [Fractional Diffusion Bridge Models](https://arxiv.org/abs/2511.01795)
*Gabriel Nobis,Maximilian Springenberg,Arina Belova,Rembert Daems,Christoph Knochenhauer,Manfred Opper,Tolga Birdal,Wojciech Samek*

Main category: cs.LG

TL;DR: Fractional Diffusion Bridge Models (FDBM) introduce a diffusion-bridge framework driven by a Markovian approximation of fractional Brownian motion to capture memory effects, with a coupling-preserving bridge and SchrÃ¶dinger-bridge extension, achieving better protein structure prediction and unpaired image translation than Brownian baselines.


<details>
  <summary>Details</summary>
Motivation: Real stochastic processes exhibit memory, long-range dependence, roughness, and anomalous diffusion, which standard Brownian-based diffusion models fail to capture. A tractable non-Markovian diffusion model is needed.

Method: Construct FDBM using a Markovian approximation of fBM (MA-fBM); prove the existence of a coupling-preserving generative diffusion bridge; apply it to future state prediction from paired data; extend to SchrÃ¶dinger bridge and derive a loss for unpaired data translation.

Result: In experiments, FDBM outperforms Brownian baselines with lower RMSD of CÎ± positions in protein structure prediction and lower FrÃ©chet Inception Distance (FID) in unpaired image translation.

Conclusion: FDBM provides a tractable, memory-aware diffusion bridge framework that improves predictive performance and can be extended to SchrÃ¶dinger bridges for unpaired data translation, with broad applicability to diffusion-bridge tasks.

Abstract: We present Fractional Diffusion Bridge Models (FDBM), a novel generative
diffusion bridge framework driven by an approximation of the rich and
non-Markovian fractional Brownian motion (fBM). Real stochastic processes
exhibit a degree of memory effects (correlations in time), long-range
dependencies, roughness and anomalous diffusion phenomena that are not captured
in standard diffusion or bridge modeling due to the use of Brownian motion
(BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM),
we construct FDBM that enable tractable inference while preserving the
non-Markovian nature of fBM. We prove the existence of a coupling-preserving
generative diffusion bridge and leverage it for future state prediction from
paired training data. We then extend our formulation to the Schr\"{o}dinger
bridge problem and derive a principled loss function to learn the unpaired data
translation. We evaluate FDBM on both tasks: predicting future protein
conformations from aligned data, and unpaired image translation. In both
settings, FDBM achieves superior performance compared to the Brownian
baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$
atomic positions in protein structure prediction and lower Fr\'echet Inception
Distance (FID) in unpaired image translation.

</details>


### [262] [Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance](https://arxiv.org/abs/2511.00543)
*Yunchuan Guan,Yu Liu,Ke Zhou,Hui Li,Sen Jia,Zhiqi Shen,Ziyang Wang,Xinglin Zhang,Tao Chen,Jenq-Neng Hwang,Lei Li*

Main category: cs.LG

TL;DR: Lo-Hp is a decoupled, two-stage weight-generation framework using a hybrid-policy sub-trajectory balance objective to learn local optimization policies, addressing over-coupling and long-horizon issues, with improved accuracy and inference efficiency in tasks with frequent weight updates.


<details>
  <summary>Details</summary>
Motivation: Current generative weight generation methods tightly couple weight generation with task-specific objectives, causing inflexibility and long-horizon inefficiency. There is a need for a decoupled framework that learns flexible, local optimization policies capable of generalizing to global weight generation.

Method: Lo-Hp proposes a two-stage, decoupled weight generation framework. It introduces a hybrid-policy sub-trajectory balance objective that blends on-policy and off-policy learning to capture local optimization policies. Theoretically, learning solely local optimization policies can mitigate long-horizon issues while improving global weight generation.

Result: Empirical results claim superior accuracy and inference efficiency in tasks requiring frequent weight updates: transfer learning, few-shot learning, domain generalization, and large language model adaptation.

Conclusion: Lo-Hp offers a decoupled, flexible weight-generation approach that focuses on local optimization policies to improve both adaptation performance and inference efficiency, with potential applicability across diverse learning scenarios.

Abstract: Recent advances in generative modeling enable neural networks to generate
weights without relying on gradient-based optimization. However, current
methods are limited by issues of over-coupling and long-horizon. The former
tightly binds weight generation with task-specific objectives, thereby limiting
the flexibility of the learned optimizer. The latter leads to inefficiency and
low accuracy during inference, caused by the lack of local constraints. In this
paper, we propose Lo-Hp, a decoupled two-stage weight generation framework that
enhances flexibility through learning various optimization policies. It adopts
a hybrid-policy sub-trajectory balance objective, which integrates on-policy
and off-policy learning to capture local optimization policies. Theoretically,
we demonstrate that learning solely local optimization policies can address the
long-horizon issue while enhancing the generation of global optimal weights. In
addition, we validate Lo-Hp's superior accuracy and inference efficiency in
tasks that require frequent weight updates, such as transfer learning, few-shot
learning, domain generalization, and large language model adaptation.

</details>


### [263] [Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations](https://arxiv.org/abs/2511.00549)
*Qiang Li,Jin Niu,Lina Yu*

Main category: cs.LG

TL;DR: A centralized single-agent reinforcement learning framework using DreamerV3 for regional adaptive traffic signal control (TSC) that encodes network topology with an adjacency matrix and uses probe-vehicle queue states, achieving robust queue reduction under fluctuating OD demand in SUMO.


<details>
  <summary>Details</summary>
Motivation: Traditional TSC optimization models struggle to capture real-world traffic complexity, dynamics, and irregular OD demand; there is a need for a scalable, data-driven, probe-vehicleâenabled control paradigm that coordinates regional signals through a centralized policy.

Method: The approach uses a single-agent centralized RL with an adjacency matrix to encode topology, real-time queue states from probe data, and current signal timing. DreamerV3 world model guides learning. Actions sequentially select intersections and adjust phase splits; the reward prioritizes queue dissipation. Evaluations are conducted in SUMO under multi-level OD fluctuations (10%, 20%, 30%).

Result: The framework demonstrates robust anti-fluctuation performance and significantly reduces queue lengths under varying OD demands, indicating effective regional TSC and compatibility with probe-vehicle technology.

Conclusion: This work proposes a new paradigm for intelligent, probe-vehicleâfriendly regional TSC using a centralized single-agent RL framework, with future work focusing on stochastic OD training and regional contingency optimization.

Abstract: Traffic congestion, primarily driven by intersection queuing, significantly
impacts urban living standards, safety, environmental quality, and economic
efficiency. While Traffic Signal Control (TSC) systems hold potential for
congestion mitigation, traditional optimization models often fail to capture
real-world traffic complexity and dynamics. This study introduces a novel
single-agent reinforcement learning (RL) framework for regional adaptive TSC,
circumventing the coordination complexities inherent in multi-agent systems
through a centralized decision-making paradigm. The model employs an adjacency
matrix to unify the encoding of road network topology, real-time queue states
derived from probe vehicle data, and current signal timing parameters.
Leveraging the efficient learning capabilities of the DreamerV3 world model,
the agent learns control policies where actions sequentially select
intersections and adjust their signal phase splits to regulate traffic
inflow/outflow, analogous to a feedback control system. Reward design
prioritizes queue dissipation, directly linking congestion metrics (queue
length) to control actions. Simulation experiments conducted in SUMO
demonstrate the model's effectiveness: under inference scenarios with
multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the
framework exhibits robust anti-fluctuation capability and significantly reduces
queue lengths. This work establishes a new paradigm for intelligent traffic
control compatible with probe vehicle technology. Future research will focus on
enhancing practical applicability by incorporating stochastic OD demand
fluctuations during training and exploring regional optimization mechanisms for
contingency events.

</details>


### [264] [Temporal Fusion Transformer for Multi-Horizon Probabilistic Forecasting of Weekly Retail Sales](https://arxiv.org/abs/2511.00552)
*Santhi Bharath Punati,Sandeep Kanta,Udaya Bhasker Cheerala,Madhusudan G Lanjewar,Praveen Damacharla*

Main category: cs.LG

TL;DR: A Temporal Fusion Transformer (TFT) model with static store features and exogenous signals delivers multi-horizon probabilistic Walmart sales forecasts, achieving high accuracy and strong calibration while remaining interpretable.


<details>
  <summary>Details</summary>
Motivation: Accurate multi-horizon retail forecasts are essential for effective inventory management and promotions; there is a need for calibrated probabilistic forecasts and transparent models.

Method: Apply TFT to weekly Walmart sales (45 stores, 2010â2012) combining static store identifiers with time-varying signals (holidays, CPI, fuel price, temperature). Produce 1â5 week-ahead probabilistic forecasts via Quantile Loss, with interpretability from variable-selection networks, static enrichment, and temporal attention.

Result: On a fixed 2012 hold-out, TFT yields RMSE = 57.9k USD per store-week and R^2 = 0.9875. In 5-fold chronological CV, RMSE = 64.6k USD and R^2 = 0.9844, outperforming XGB, CNN, LSTM, and CNN-LSTM baselines.

Conclusion: Demonstrates practical value for inventory planning and holiday-period optimization, while maintaining model transparency.

Abstract: Accurate multi-horizon retail forecasts are critical for inventory and
promotions. We present a novel study of weekly Walmart sales (45 stores,
2010--2012) using a Temporal Fusion Transformer (TFT) that fuses static store
identifiers with time-varying exogenous signals (holidays, CPI, fuel price,
temperature). The pipeline produces 1--5-week-ahead probabilistic forecasts via
Quantile Loss, yielding calibrated 90\% prediction intervals and
interpretability through variable-selection networks, static enrichment, and
temporal attention. On a fixed 2012 hold-out dataset, TFT achieves an RMSE of
\$57.9k USD per store-week and an $R^2$ of 0.9875. Across a 5-fold
chronological cross-validation, the averages are RMSE = \$64.6k USD and $R^2$ =
0.9844, outperforming the XGB, CNN, LSTM, and CNN-LSTM baseline models. These
results demonstrate practical value for inventory planning and holiday-period
optimization, while maintaining model transparency.

</details>


### [265] [Red-teaming Activation Probes using Prompted LLMs](https://arxiv.org/abs/2511.00554)
*Phil Blandfort,Robert Graham*

Main category: cs.LG

TL;DR: A lightweight black-box red-teaming approach using an off-the-shelf LLM with iterative feedback and in-context learning to surface robustness issues in activation probes, without fine-tuning, revealing brittleness patterns and persistent vulnerabilities; suggests prompted red-teaming can reveal failure modes before deployment.


<details>
  <summary>Details</summary>
Motivation: Activation probes are low-cost monitors for AI systems but their real-world robustness under realistic adversarial pressure is underexplored. There is a need for capable, low-effort methods to surface failure modes prior to deployment.

Method: A lightweight black-box red-teaming procedure that wraps an off-the-shelf LLM with iterative feedback and in-context learning (ICL), requiring no fine-tuning, gradients, or architectural access. Applied in a case study with probes for high-stakes interactions to surface failure modes under adversarial pressure, including scenario-constraint attacks.

Result: The approach helps discover valuable insights about a state-of-the-art probe. It reveals interpretable brittleness patternsâsuch as legalese-induced false positives and bland procedural tone false negativesâand finds reduced but persistent vulnerabilities under scenario-constraint attacks.

Conclusion: Simple prompted red-teaming scaffolding can anticipate failure patterns before deployment and yield actionable insights to harden future probes.

Abstract: Activation probes are attractive monitors for AI systems due to low cost and
latency, but their real-world robustness remains underexplored. We ask: What
failure modes arise under realistic, black-box adversarial pressure, and how
can we surface them with minimal effort? We present a lightweight black-box
red-teaming procedure that wraps an off-the-shelf LLM with iterative feedback
and in-context learning (ICL), and requires no fine-tuning, gradients, or
architectural access. Running a case study with probes for high-stakes
interactions, we show that our approach can help discover valuable insights
about a SOTA probe. Our analysis uncovers interpretable brittleness patterns
(e.g., legalese-induced FPs; bland procedural tone FNs) and reduced but
persistent vulnerabilities under scenario-constraint attacks. These results
suggest that simple prompted red-teaming scaffolding can anticipate failure
patterns before deployment and might yield promising, actionable insights to
harden future probes.

</details>


### [266] [FTT-GRU: A Hybrid Fast Temporal Transformer with GRU for Remaining Useful Life Prediction](https://arxiv.org/abs/2511.00564)
*Varun Teja Chirukiri,Udaya Bhasker Cheerala,Sandeep Kanta,Abdul Karim,Praveen Damacharla*

Main category: cs.LG

TL;DR: A compact FTT-GRU hybrid model for RUL prediction on CMAPSS that captures global and local degradation patterns, achieving competitive accuracy with low latency.


<details>
  <summary>Details</summary>
Motivation: Modeling multivariate sensor data for RUL requires capturing both global temporal dependencies and fine-grained degradation trends. LSTM/CNN baselines often struggle to jointly model these aspects efficiently.

Method: Proposes a hybrid architecture FTT-GRU that integrates a Fast Temporal Transformer (FTT) with linearized attention via FFT and a GRU layer for sequential modeling. This is applied to NASA CMAPSS (FD001), with ablations (GRU-only, FTT-only) to validate contributions, evaluating performance with RMSE, MAE, and R^2 and reporting CPU latency (batch=1).

Result: On CMAPSS FD001, the model achieves RMSE 30.76, MAE 18.97, R^2 = 0.45, and 1.12 ms CPU latency (batch=1). Relative to the best published deep baseline (TCNâAttention), RMSE improves by 1.16% and MAE by 4.00%. Training curves over 3 runs show smooth convergence with narrow 95% confidence bands; ablations confirm the contribution of both the FTT and GRU components.

Conclusion: A compact Transformer-RNN hybrid can deliver accurate and efficient RUL predictions on CMAPSS, enabling real-time prognostics in industrial settings.

Abstract: Accurate prediction of the remaining useful life (RUL) of industrial
machinery is essential for reducing downtime and optimizing maintenance
schedules. Existing approaches, such as long short-term memory (LSTM) networks
and convolutional neural networks (CNNs), often struggle to model both global
temporal dependencies and fine-grained degradation trends in multivariate
sensor data. We propose a hybrid model, FTT-GRU, which combines a Fast Temporal
Transformer (FTT) -- a lightweight Transformer variant using linearized
attention via fast Fourier transform (FFT) -- with a gated recurrent unit (GRU)
layer for sequential modeling. To the best of our knowledge, this is the first
application of an FTT with a GRU for RUL prediction on NASA CMAPSS, enabling
simultaneous capture of global and local degradation patterns in a compact
architecture. On CMAPSS FD001, FTT-GRU attains RMSE 30.76, MAE 18.97, and
$R^2=0.45$, with 1.12 ms CPU latency at batch=1. Relative to the best published
deep baseline (TCN--Attention), it improves RMSE by 1.16\% and MAE by 4.00\%.
Training curves averaged over $k=3$ runs show smooth convergence with narrow
95\% confidence bands, and ablations (GRU-only, FTT-only) support the
contribution of both components. These results demonstrate that a compact
Transformer-RNN hybrid delivers accurate and efficient RUL predictions on
CMAPSS, making it suitable for real-time industrial prognostics.

</details>


### [267] [Bayesian Network Structure Discovery Using Large Language Models](https://arxiv.org/abs/2511.00574)
*Yinghuan Zhang,Yufei Zhang,Parisa Kordjamshidi,Zijun Cui*

Main category: cs.LG

TL;DR: A unified, LLM-centered framework for Bayesian network structure discovery with data-free (PromptBN) and data-aware (ReActBN) modes, achieving strong performance, especially with little to no data.


<details>
  <summary>Details</summary>
Motivation: Structure learning for probabilistic graphs is data-hungry and computationally costly; leveraging LLMs as core reasoning engines can improve efficiency and enable data-scarce scenarios.

Method: In data-free mode, PromptBN queries LLMs with metadata to uncover valid relationships. In data-aware mode, ReActBN combines the ReAct reasoning paradigm with BIC-like structure scores for iterative refinement, keeping the LLM in the loop throughout.

Result: The approach substantially outperforms prior LLM-based methods and traditional data-driven algorithms, particularly in low/no-data regimes.

Conclusion: A unified framework demonstrates the potential of keeping LLMs at the center of Bayesian network discovery, enabling effective structure learning across data regimes; code is publicly available.

Abstract: Understanding probabilistic relationships among variables is crucial for
analyzing complex systems. Traditional structure learning methods often require
extensive observational data and incur high computational costs. Recent studies
have explored using large language models (LLMs) for structure learning, but
most treat LLMs as auxiliary tools for pre-processing or post-processing,
leaving the core learning process data-driven. In this work, we propose a
unified framework for Bayesian network structure discovery that places LLMs at
the center, supporting both data-free and data-aware settings. In the data-free
case, we introduce \textbf{PromptBN} to query LLMs with metadata and
efficiently uncover valid probabilistic relationships. When observational data
are available, we introduce \textbf{ReActBN}, which integrates the ReAct
reasoning paradigm with structure scores such as the Bayesian Information
Criterion (BIC) for iterative refinement. Unlike prior methods that offload
refinement to external algorithms, our framework maintains the LLM actively in
the loop throughout the discovery process. Experiments demonstrate that our
method significantly outperforms both existing LLM-based approaches and
traditional data-driven algorithms, particularly in the low- or no-data
scenario. Code is publicly available at
{\texttt{\textcolor{magenta}{https://github.com/sherryzyh/prompt2bn}}}.

</details>


### [268] [Sparse and nonparametric estimation of equations governing dynamical systems with applications to biology](https://arxiv.org/abs/2511.00579)
*G. Pillonetto,A. Giaretta,A. Aravkin,M. Bisiacco,T. Elston*

Main category: cs.LG

TL;DR: A hybrid data-driven approach augments sparse, parametric model discovery (Sindy) with nonparametric components to capture nonlinearities beyond the library, improving modeling of complex biological dynamics.


<details>
  <summary>Details</summary>
Motivation: Parametric sparse methods like Sindy are efficient but rely on a predefined library of candidate terms; biological systems exhibit nonlinearities that such libraries may miss, requiring more flexible modeling without exhaustive a priori specification.

Method: Proposes a framework that couples sparse parametric estimation with nonparametric techniques to model unknown nonlinearities without expanding the function library. The approach identifies a sparse set of governing terms and uses nonparametric corrections to capture residual or additional nonlinear behavior; validated on multiple biological scenarios.

Result: Applied to several illustrative examples involving complex biological phenomena; demonstrates improved ability to capture nonlinear dynamics beyond Sindy's expressiveness, demonstrating viability of the hybrid framework.

Conclusion: Integrating sparse parametric discovery with nonparametric modeling extends data-driven dynamical system identification in biology, enabling accurate modeling while maintaining sparsity and interpretability; suggests broad applicability and potential for future refinements.

Abstract: Data-driven discovery of model equations is a powerful approach for
understanding the behavior of dynamical systems in many scientific fields. In
particular, the ability to learn mathematical models from data would benefit
systems biology, where the complex nature of these systems often makes a bottom
up approach to modeling unfeasible. In recent years, sparse estimation
techniques have gained prominence in system identification, primarily using
parametric paradigms to efficiently capture system dynamics with minimal model
complexity. In particular, the Sindy algorithm has successfully used sparsity
to estimate nonlinear systems by extracting from a library of functions only a
few key terms needed to capture the dynamics of these systems. However,
parametric models often fall short in accurately representing certain
nonlinearities inherent in complex systems. To address this limitation, we
introduce a novel framework that integrates sparse parametric estimation with
nonparametric techniques. It captures nonlinearities that Sindy cannot describe
without requiring a priori information about their functional form. That is,
without expanding the library of functions to include the one that is trying to
be discovered. We illustrate our approach on several examples related to
estimation of complex biological phenomena.

</details>


### [269] [Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation](https://arxiv.org/abs/2511.00588)
*Dong Chen,Yanzhe Wei,Zonglin He,Guan-Ming Kuang,Canhua Ye,Meiru An,Huili Peng,Yong Hu,Huiren Tao,Kenneth MC Cheung*

Main category: cs.LG

TL;DR: Six LLMs were evaluated for hallucination risk in spine-surgery decision support using a clinician-centered framework; DeepSeek-R1 led performance; extended chain-of-thought did not reliably improve clinical reliability; results reveal model-specific vulnerabilities under stress; calls for interpretability and safety-focused validation in clinical deployment.


<details>
  <summary>Details</summary>
Motivation: Mitigate patient safety risks from LLM hallucinations in high-stakes spine surgery and establish a framework to quantify and reduce those risks in clinical workflows.

Method: Evaluation of six leading LLMs across 30 expert-validated spinal cases using a framework assessing diagnostic precision, recommendation quality, reasoning robustness, output coherence, and knowledge alignment; stress-testing under amplified complexity; comparison of reasoning-enhanced variants vs standard.

Result: DeepSeek-R1 achieved the highest total score (86.03 Â± 2.08). Claude-3.7-Sonnet with extended thinking underperformed its standard version (80.79 Â± 1.83 vs 81.56 Â± 1.92). Recommendation quality degraded by 7.4% under amplified complexity; rationality (+2.0%), readability (+1.7%), and diagnosis (+4.7%) showed only marginal improvements, highlighting a gap between perceived coherence and actionable guidance.

Conclusion: Recommend integrating interpretability mechanisms (e.g., reasoning chain visualization) into clinical workflows and implementing a safety-aware validation framework for surgical LLM deployment; extended chain-of-thought reasoning alone is insufficient for clinical reliability.

Abstract: Large language models (LLMs) offer transformative potential for clinical
decision support in spine surgery but pose significant risks through
hallucinations, which are factually inconsistent or contextually misaligned
outputs that may compromise patient safety. This study introduces a
clinician-centered framework to quantify hallucination risks by evaluating
diagnostic precision, recommendation quality, reasoning robustness, output
coherence, and knowledge alignment. We assessed six leading LLMs across 30
expert-validated spinal cases. DeepSeek-R1 demonstrated superior overall
performance (total score: 86.03 $\pm$ 2.08), particularly in high-stakes
domains such as trauma and infection. A critical finding reveals that
reasoning-enhanced model variants did not uniformly outperform standard
counterparts: Claude-3.7-Sonnet's extended thinking mode underperformed
relative to its standard version (80.79 $\pm$ 1.83 vs. 81.56 $\pm$ 1.92),
indicating extended chain-of-thought reasoning alone is insufficient for
clinical reliability. Multidimensional stress-testing exposed model-specific
vulnerabilities, with recommendation quality degrading by 7.4% under amplified
complexity. This decline contrasted with marginal improvements in rationality
(+2.0%), readability (+1.7%) and diagnosis (+4.7%), highlighting a concerning
divergence between perceived coherence and actionable guidance. Our findings
advocate integrating interpretability mechanisms (e.g., reasoning chain
visualization) into clinical workflows and establish a safety-aware validation
framework for surgical LLM deployment.

</details>


### [270] [Gaining Momentum: Uncovering Hidden Scoring Dynamics in Hockey through Deep Neural Sequencing and Causal Modeling](https://arxiv.org/abs/2511.00615)
*Daniel Griffiths,Piper Moskow*

Main category: cs.LG

TL;DR: A data-driven five-stage pipeline improves NHL offensive performance and xG through causal analysis, achieving ATE 0.12 and 15% gain.


<details>
  <summary>Details</summary>
Motivation: Quantify and optimize offensive momentum and scoring in professional hockey with causal, real-time analytics.

Method: Five-stage pipeline: (1) momentum weighting via logistic regression; (2) xG via gradient-boosted trees; (3) LSTM for sequence dynamics; (4) PCA + K-Means for spatial formations; (5) X-Learner for ATE of optimal sequences/formations.

Result: ATE 0.12 (CI 0.05â0.17; p<1e-50), ~15% relative gain in scoring potential; evidence that structured sequences and compact formations causally boost offense.

Conclusion: Framework enables real-time, actionable insights and advances causally grounded tactical optimization in hockey analytics.

Abstract: We present a unified, data-driven framework for quantifying and enhancing
offensive momentum and scoring likelihood (expected goals, xG) in professional
hockey. Leveraging a Sportlogiq dataset of 541,000 NHL event records, our
end-to-end pipeline comprises five stages: (1) interpretable momentum weighting
of micro-events via logistic regression; (2) nonlinear xG estimation using
gradient-boosted decision trees; (3) temporal sequence modeling with Long
Short-Term Memory (LSTM) networks; (4) spatial formation discovery through
principal component analysis (PCA) followed by K-Means clustering on
standardized player coordinates; and (5) use of an X-Learner causal inference
estimator to quantify the average treatment effect (ATE) of adopting the
identified "optimal" event sequences and formations. We observe an ATE of 0.12
(95% CI: 0.05-0.17, p < 1e-50), corresponding to a 15% relative gain in scoring
potential. These results demonstrate that strategically structured sequences
and compact formations causally elevate offensive performance. Our framework
delivers real-time, actionable insights for coaches and analysts, advancing
hockey analytics toward principled, causally grounded tactical optimization.

</details>


### [271] [Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering](https://arxiv.org/abs/2511.00617)
*Eric Bigelow,Daniel Wurgaft,YingQiao Wang,Noah Goodman,Tomer Ullman,Hidenori Tanaka,Ekdeep Singh Lubana*

Main category: cs.LG

TL;DR: A Bayesian unifying framework shows that prompt-based (context) and activation-based (internal) control of LLMs can be seen as altering latent concept beliefs: prompts accumulate evidence, activations shift priors. The resulting closed-form model predicts behavior across interventions and reveals additive log-belief effects and phase transitions.


<details>
  <summary>Details</summary>
Motivation: To unify disparate LLM control mechanisms (in-context learning via prompts and activation steering) under a single, predictive theory, enabling better understanding and prediction of how interventions steer model behavior.

Method: Develop a closed-form Bayesian model where context interventions modify priors over latent concepts and activation interventions accumulate evidence. Validate the model in multi-domain tasks inspired by many-shot in-context learning, deriving predictions (e.g., sigmoidal learning curves, additivity in log-belief space).

Result: The framework provides highly predictive explanations of behavior under both intervention types and yields novel predictions such as additive effects and distinct phases, explaining past empirical phenomena and predicting abrupt shifts with small control changes.

Conclusion: Offers a unified Bayesian account of prompt- and activation-based LLM control and a practical methodology to predict effects of such interventions across domains.

Abstract: Large language models (LLMs) can be controlled at inference time through
prompts (in-context learning) and internal activations (activation steering).
Different accounts have been proposed to explain these methods, yet their
common goal of controlling model behavior raises the question of whether these
seemingly disparate methodologies can be seen as specific instances of a
broader framework. Motivated by this, we develop a unifying, predictive account
of LLM control from a Bayesian perspective. Specifically, we posit that both
context- and activation-based interventions impact model behavior by altering
its belief in latent concepts: steering operates by changing concept priors,
while in-context learning leads to an accumulation of evidence. This results in
a closed-form Bayesian model that is highly predictive of LLM behavior across
context- and activation-based interventions in a set of domains inspired by
prior work on many-shot in-context learning. This model helps us explain prior
empirical phenomena - e.g., sigmoidal learning curves as in-context evidence
accumulates - while predicting novel ones - e.g., additivity of both
interventions in log-belief space, which results in distinct phases such that
sudden and dramatic behavioral shifts can be induced by slightly changing
intervention controls. Taken together, this work offers a unified account of
prompt-based and activation-based control of LLM behavior, and a methodology
for empirically predicting the effects of these interventions.

</details>


### [272] [Stochastic Shortest Path with Sparse Adversarial Costs](https://arxiv.org/abs/2511.00637)
*Emmeran Johnson,Alberto Rumi,Ciara Pike-Burke,Patrick Rebeschini*

Main category: cs.LG

TL;DR: Sparse-cost adversarial SSP with full-information feedback: negative-entropy regularization in OMD does not exploit sparsity, yielding regret depending on log S; introducing ell_r-norm regularizers (r in (1,2)) adapts to sparsity and achieves regret ~ sqrt(log M) with matching lower bound; in unknown transitions, sparsity provides no regret improvement, with minimax regret polynomial in SA.


<details>
  <summary>Details</summary>
Motivation: To understand how sparsity of incurred costs affects regret in adversarial stochastic shortest path (SSP) learning under full-information feedback and to develop adaptive regularizers that exploit sparsity.

Method: Introduce a family of ell_r-norm regularizers (r in (1,2)) within Online Mirror Descent to adapt to sparse cost structure; derive upper bounds showing regret scales with sqrt(log M); establish a matching lower bound; compare with negative-entropy regularization which remains non-adaptive; analyze the unknown-transition setting.

Result: Under known transitions, the ell_r regularizers achieve regret scaling as sqrt(log M), and this rate is optimal via a matching lower bound; effective dimension shifts from SA to M. Negative-entropy regularization remains non-adaptive to sparsity, incurring sqrt(log S) (or sqrt(log S) in sparse regimes). In the unknown-transition setting, sparsity offers limited gains and the minimax regret scales polynomially with SA, regardless of sparsity.

Conclusion: Sparsity reduces the effective dimension from the full state-action count SA to the sparse support size M under full-information; adaptive ell_r-regularizers provide optimal regret scaling with sqrt(log M). In unknown transitions, sparsity does not yield similar improvements, highlighting the necessity of problem setting in exploiting sparse costs.

Abstract: We study the adversarial Stochastic Shortest Path (SSP) problem with sparse
costs under full-information feedback. In the known transition setting,
existing bounds based on Online Mirror Descent (OMD) with negative-entropy
regularization scale with $\sqrt{\log S A}$, where $SA$ is the size of the
state-action space. While we show that this is optimal in the worst-case, this
bound fails to capture the benefits of sparsity when only a small number $M \ll
SA$ of state-action pairs incur cost. In fact, we also show that the
negative-entropy is inherently non-adaptive to sparsity: it provably incurs
regret scaling with $\sqrt{\log S}$ on sparse problems. Instead, we propose a
family of $\ell_r$-norm regularizers ($r \in (1,2)$) that adapts to the
sparsity and achieves regret scaling with $\sqrt{\log M}$ instead of
$\sqrt{\log SA}$. We show this is optimal via a matching lower bound,
highlighting that $M$ captures the effective dimension of the problem instead
of $SA$. Finally, in the unknown transition setting the benefits of sparsity
are limited: we prove that even on sparse problems, the minimax regret for any
learner scales polynomially with $SA$.

</details>


### [273] [Diluting Restricted Boltzmann Machines](https://arxiv.org/abs/2511.00648)
*C. DÃ­az-Faloh,R. Mulet*

Main category: cs.LG

TL;DR: RBMs can retain strong generative performance under extreme pruning (up to 80% of connections pruned before training), aligning with the Lottery Ticket Hypothesis, but retraining after further pruning cannot fully recover lost performance. There is a sharp transition where pruning disrupts a minimal core of essential connections, and retrained networks underperform those trained from scratch at equivalent sparsity. Early pruning is thus crucial, and initial conditions heavily influence network capabilities.


<details>
  <summary>Details</summary>
Motivation: To address the efficiency and environmental concerns of large neural networks by testing whether smaller, sparse models can match performance. The study uses Restricted Boltzmann Machines to examine the Lottery Ticket Hypothesis under aggressive pruning conditions and its impact on generative quality.

Method: Systematically prune RBMs up to 80% of their connections before training and evaluate generative performance. Investigate retraining after additional pruning, compare retrained networks to networks trained from scratch at matching sparsity, and identify transitions in generative quality and core connectivity requirements.

Result: RBMs with substantial pre-training pruning achieve high-quality generative performance. However, retraining after further pruning cannot fully recover performance. A sharp transition occurs when pruning disrupts a minimal core of essential connections. Retrained networks underperform those trained from scratch at the same sparsity level, indicating that pruning must occur early and is heavily influenced by initial conditions.

Conclusion: Pruning should be applied early in training for sparse networks to work effectively, as initial conditions and the preserved core connections largely determine generative capability. The findings support a practical approach to efficient neural architectures and extend the Lottery Ticket concept to generative models like RBMs.

Abstract: Recent advances in artificial intelligence have relied heavily on
increasingly large neural networks, raising concerns about their computational
and environmental costs. This paper investigates whether simpler, sparser
networks can maintain strong performance by studying Restricted Boltzmann
Machines (RBMs) under extreme pruning conditions. Inspired by the Lottery
Ticket Hypothesis, we demonstrate that RBMs can achieve high-quality generative
performance even when up to 80% of the connections are pruned before training,
confirming that they contain viable sub-networks. However, our experiments
reveal crucial limitations: trained networks cannot fully recover lost
performance through retraining once additional pruning is applied. We identify
a sharp transition above which the generative quality degrades abruptly when
pruning disrupts a minimal core of essential connections. Moreover, re-trained
networks remain constrained by the parameters originally learned performing
worse than networks trained from scratch at equivalent sparsity levels. These
results suggest that for sparse networks to work effectively, pruning should be
implemented early in training rather than attempted afterwards. Our findings
provide practical insights for the development of efficient neural
architectures and highlight the persistent influence of initial conditions on
network capabilities.

</details>


### [274] [Reviving Stale Updates: Data-Free Knowledge Distillation for Asynchronous Federated Learning](https://arxiv.org/abs/2511.00655)
*Baris Askin,Holger R. Roth,Zhenyu Sun,Carlee Joe-Wong,Gauri Joshi,Ziyue Xu*

Main category: cs.LG

TL;DR: FedRevive proposes asynchronous federated learning with data-free knowledge distillation to mitigate stale updates, using server-side distillation with a meta-learned generator and a hybrid aggregation to retain scalability.


<details>
  <summary>Details</summary>
Motivation: Asynchronous FL improves wall-clock efficiency but suffers from stale updates that can destabilize optimization and hinder convergence; there is a need to address staleness without relying on real/public data.

Method: Combine asynchronous federated learning with a server-side data-free knowledge distillation (DFKD) pipeline. A meta-learned generator creates pseudo-samples for multi-teacher distillation. Distillation updates are integrated with raw client updates via a hybrid aggregation scheme to mitigate staleness while preserving AFL scalability.

Result: Empirical evaluation on vision and text benchmarks shows FedRevive achieves up to 32.1% faster training and up to 21.5% higher final accuracy compared to asynchronous baselines.

Conclusion: FedRevive effectively mitigates the adverse effects of stale updates in AFL by fusing raw updates with DFKD-derived knowledge, maintaining scalability while improving convergence and accuracy.

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, yet its scalability is limited by
synchronization overhead. Asynchronous Federated Learning (AFL) alleviates this
issue by allowing clients to communicate independently, thereby improving
wall-clock efficiency in large-scale, heterogeneous environments. However, this
asynchrony introduces stale updates (client updates computed on outdated global
models) that can destabilize optimization and hinder convergence. We propose
FedRevive, an asynchronous FL framework that revives stale updates through
data-free knowledge distillation (DFKD). FedRevive integrates parameter-space
aggregation with a lightweight, server-side DFKD process that transfers
knowledge from stale client models to the current global model without access
to real or public data. A meta-learned generator synthesizes pseudo-samples,
which enables multi-teacher distillation. A hybrid aggregation scheme that
combines raw updates with DFKD updates effectively mitigates staleness while
retaining the scalability of AFL. Experiments on various vision and text
benchmarks show that FedRevive achieves faster training up to 32.1% and higher
final accuracy up to 21.5% compared to asynchronous baselines.

</details>


### [275] [Sensitivity Analysis for Climate Science with Generative Flow Models](https://arxiv.org/abs/2511.00663)
*Alex Dobra,Jakiw Pidstrigach,Tim Reichelt,Paolo Fraccaro,Johannes Jakubik,Anne Jones,Christian Schroeder de Witt,Philip Stier,Philip Torr*

Main category: cs.LG

TL;DR: We present an adjoint-state method to compute gradients in generative flow models (diffusion models in particular), apply it to the cBottle ERA5 emulator to analyze sensitivities to Sea Surface Temperature, and introduce a gradient self-consistency check. Early results show reliable gradients and substantial speedups (weeks on HPC to hours on GPU), streamlining climate-sensitivity workflows.


<details>
  <summary>Details</summary>
Motivation: Sensitivity analysis is crucial in climate science but traditional physical models are costly. AI-based generative models offer speed, but gradient computation remains a bottleneck. By combining adjoint-state methods with generative flow models, the work aims to enable efficient, scalable sensitivity analyses.

Method: Apply the adjoint state method to calculate gradients in generative flow models, treating diffusion models as a special case. Implement and test on the cBottle ERA5 emulator to perform sensitivity analysis with respect to sea surface temperatures. Introduce a gradient self-consistency check to validate computed gradients against the modelâs outputs.

Result: Initial evidence that the adjoint-based gradients in generative flow models can be reliable. The approach reduces sensitivity-analysis computation from weeks on supercomputers to hours on GPUs, enabling a faster, more accessible workflow for climate science.

Conclusion: Adjoint-state gradient computation in generative flow models, with a diffusion-model special case and a gradient self-consistency check, shows promise for reliable, scalable sensitivity analyses in climate science and substantial computational savings.

Abstract: Sensitivity analysis is a cornerstone of climate science, essential for
understanding phenomena ranging from storm intensity to long-term climate
feedbacks. However, computing these sensitivities using traditional physical
models is often prohibitively expensive in terms of both computation and
development time. While modern AI-based generative models are orders of
magnitude faster to evaluate, computing sensitivities with them remains a
significant bottleneck. This work addresses this challenge by applying the
adjoint state method for calculating gradients in generative flow models, with
diffusion models as a special case. We apply this method to the cBottle
generative model, an emulator of ERA5 data, to perform sensitivity analysis
with respect to sea surface temperatures. Furthermore, we propose a novel
gradient self-consistency check to quantitatively validate the computed
sensitivities against the model's own outputs. Our results provide initial
evidence that this approach can produce reliable gradients, reducing the
computational cost of sensitivity analysis from weeks on a supercomputer with a
physical model to hours on a GPU, thereby simplifying a critical workflow in
climate science.

</details>


### [276] [Inference-Time Chain-of-Thought Pruning with Latent Informativeness Signals](https://arxiv.org/abs/2511.00699)
*Sophie Li,Nicholas Huang,Nayan Saxena,Nina Luo,Vincent Lin,Kevin Zhu,Sunishchal Dev*

Main category: cs.LG

TL;DR: KAPPA uses KL-divergence, confidence, and entropy to guide progressive pruning of branches in Best-of-N-style decoding, achieving substantial memory and token savings with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: BoN-based decoding is computationally expensive; ST-BoN relies on consistency heuristics and may misjudge branch quality. There is a need for principled, inference-time pruning that preserves diversity and accuracy while reducing resource usage.

Method: A KL-adjusted pruned path algorithm that scores branches using KL divergence, confidence, and entropy; promotes diverse exploration and selectively eliminates low-scoring branches to prune search space during inference.

Result: On GSM8K and MATH500 with DeepSeek-R1-Distill-Qwen-1.5B and Qwen2.5-7B-Instruct, KAPPA stabilizes performance in smaller models and achieves up to ~60% peak memory reduction and ~90% reduction in total token generation versus Best-of-N, with minimal impact on accuracy.

Conclusion: KAPPA provides a principled, scalable inference-time pruning strategy that preserves accuracy while substantially reducing memory and token usage, enabling efficient multi-solution decoding in resource-constrained settings.

Abstract: Large language models (LLMs) improve reasoning accuracy when generating
multiple candidate solutions at test time, but standard methods like Best-of-N
(BoN) incur high computational cost by fully generating all branches.
Self-Truncation Best-of-N (ST-BoN) mitigates this by truncating unpromising
paths early, but its reliance on consistency-based heuristics is a limitation
as it does not directly evaluate branch quality. We present KL-Adjusted Pruned
Path Algorithm (KAPPA), an inference-time method that combines Kullback-Leibler
divergence, confidence, and entropy into a principled scoring function to guide
progressive pruning. By promoting diversity during exploration and selectively
eliminating low-scoring branches, KAPPA maintains accuracy while substantially
reducing memory and token usage. Experiments on GSM8K and MATH500 with
DeepSeek-R1-Distill-Qwen-1.5B and Qwen2.5-7B-Instruct demonstrate that KAPPA
stabilizes performance in smaller models and achieves up to ~60% reduction in
peak memory and ~90% reduction in total token generation relative to BoN, with
minimal impact on accuracy.

</details>


### [277] [Privacy-Aware Time Series Synthesis via Public Knowledge Distillation](https://arxiv.org/abs/2511.00700)
*Penghang Liu,Haibei Zhu,Eleonora Kreacic,Svitlana Vyetrenko*

Main category: cs.LG

TL;DR: Pub2Priv uses public contextual knowledge to condition diffusion-based time-series generation, improving privacy-utility trade-offs while introducing a practical identifiability privacy metric.


<details>
  <summary>Details</summary>
Motivation: Sharing sensitive time series is restricted by privacy. Public context (e.g., weather, prices) can influence sensitive data and is underutilized by current privacy-aware generation methods, leading to suboptimal privacy-utility balance.

Method: A self-attention module encodes heterogeneous public data into temporal and feature embeddings, which condition a diffusion model to generate private synthetic sequences. A practical identifiability-based privacy metric is introduced.

Result: Pub2Priv consistently outperforms state-of-the-art benchmarks in privacy-utility trade-offs across finance, energy, and commodity trading domains.

Conclusion: Leveraging public contextual knowledge within a conditional diffusion framework yields better privacy-utility trade-offs for time series data; a practical privacy metric enhances evaluation and applicability.

Abstract: Sharing sensitive time series data in domains such as finance, healthcare,
and energy consumption, such as patient records or investment accounts, is
often restricted due to privacy concerns. Privacy-aware synthetic time series
generation addresses this challenge by enforcing noise during training,
inherently introducing a trade-off between privacy and utility. In many cases,
sensitive sequences is correlated with publicly available, non-sensitive
contextual metadata (e.g., household electricity consumption may be influenced
by weather conditions and electricity prices). However, existing privacy-aware
data generation methods often overlook this opportunity, resulting in
suboptimal privacy-utility trade-offs. In this paper, we present Pub2Priv, a
novel framework for generating private time series data by leveraging
heterogeneous public knowledge. Our model employs a self-attention mechanism to
encode public data into temporal and feature embeddings, which serve as
conditional inputs for a diffusion model to generate synthetic private
sequences. Additionally, we introduce a practical metric to assess privacy by
evaluating the identifiability of the synthetic data. Experimental results show
that Pub2Priv consistently outperforms state-of-the-art benchmarks in improving
the privacy-utility trade-off across finance, energy, and commodity trading
domains.

</details>


### [278] [Investigating the Robustness of Knowledge Tracing Models in the Presence of Student Concept Drift](https://arxiv.org/abs/2511.00704)
*Morgan Lee,Artem Frenk,Eamon Worden,Karish Gupta,Thinh Pham,Ethan Croteau,Neil Heffernan*

Main category: cs.LG

TL;DR: Knowledge Tracing models degrade under concept drift across academic years; Bayesian Knowledge Tracing (BKT) remains the most stable, while attention-based models lose predictive power faster; advocates for longitudinal evaluation with released dataset.


<details>
  <summary>Details</summary>
Motivation: To test the common assumption that the KT learning process is static by examining how concept drift and shifting student populations affect model performance over time.

Method: Apply four KT model families to five academic years of data, evaluating performance both within a single year and across years to assess susceptibility to concept drift.

Result: All four KT model families exhibit degraded performance over time; BKT is the most stable on newer data, while more complex, attention-based models lose predictive power more quickly; cross-year generalization is limited.

Conclusion: Encourage longitudinal KT benchmarks and reporting; share the dataset for replication and further drift-aware evaluations; future work should account for non-stationarity in KT modeling.

Abstract: Knowledge Tracing (KT) has been an established problem in the educational
data mining field for decades, and it is commonly assumed that the underlying
learning process be- ing modeled remains static. Given the ever-changing land-
scape of online learning platforms (OLPs), we investigate how concept drift and
changing student populations can im- pact student behavior within an OLP
through testing model performance both within a single academic year and across
multiple academic years. Four well-studied KT models were applied to five
academic years of data to assess how suscep- tible KT models are to concept
drift. Through our analysis, we find that all four families of KT models can
exhibit de- graded performance, Bayesian Knowledge Tracing (BKT) remains the
most stable KT model when applied to newer data, while more complex, attention
based models lose pre- dictive power significantly faster. To foster more
longitu- dinal evaluations of KT models, the data used to conduct our analysis
is available at https://osf.io/hvfn9/?view_
only=b936c63dfdae4b0b987a2f0d4038f72a

</details>


### [279] [TRISKELION-1: Unified Descriptive-Predictive-Generative AI](https://arxiv.org/abs/2511.00711)
*Nardeep Kumar,Arun Kanwar*

Main category: cs.LG

TL;DR: TRISKELION-1 is a unified descriptive-predictive-generative model that jointly optimizes descriptive reconstruction, predictive classification, and generative sampling within a single encoder-decoder using variational objectives; validated on MNIST; suggests a blueprint for universal intelligent architectures.


<details>
  <summary>Details</summary>
Motivation: to bridge interpretability, accuracy, and creativity by integrating descriptive, predictive, and generative reasoning in one architecture.

Method: single encoder-decoder framework trained with a joint variational objective that combines descriptive reconstruction, predictive inference (classification), and generative sampling; empirical evaluation on MNIST.

Result: descriptive reconstruction, predictive classification, and generative sampling coexist stably within one model; MNIST demonstrates feasibility.

Conclusion: proposes a blueprint toward universal intelligence architectures that connect interpretability, accuracy, and creativity; potential for broader applicability beyond MNIST.

Abstract: TRISKELION-1 is a unified descriptive-predictive-generative architecture that
integrates statistical, mechanistic, and generative reasoning within a single
encoder-decoder framework. The model demonstrates how descriptive
representation learning, predictive inference, and generative synthesis can be
jointly optimized using variational objectives. Experiments on MNIST validate
that descriptive reconstruction, predictive classification, and generative
sampling can coexist stably within one model. The framework provides a
blueprint toward universal intelligence architectures that connect
interpretability, accuracy, and creativity.

</details>


### [280] [Enhancing Heavy Rain Nowcasting with Multimodal Data: Integrating Radar and Satellite Observations](https://arxiv.org/abs/2511.00716)
*Rama Kassoumeh,David RÃ¼gamer,Henning Oppel*

Main category: cs.LG

TL;DR: Multimodal fusion of radar and satellite imagery improves nowcasting of heavy rainfall; outperforms radar-only methods for 5, 15, 30 min lead times, with CSI gains of 4% (heavy) and 3% (violent) at 5 min; effective for longer lead times and illustrates NRW 2021 event.


<details>
  <summary>Details</summary>
Motivation: Urban flooding risk and limitations of rain gauges and radar-only nowcasting in Germany; need for accurate, short-term precipitation forecasts.

Method: Develop a multimodal nowcasting model combining radar and satellite imagery to predict precipitation at 5, 15, 30 min lead times; quantitative evaluation against radar-only baseline; qualitative case study of NRW 2021 event; GitHub implementation.

Result: Significant performance gains over radar-only, especially for intense precipitation; CSI improvements 4% for heavy rain and 3% for violent rain at 5-min lead; better skill at longer lead times; more detailed forecasts for heavy rain regions in NRW 2021.

Conclusion: Multimodal data fusion enhances nowcasting accuracy and can enable timely warnings; provides a practical implementation resource (GitHub) for real-time flood risk management.

Abstract: The increasing frequency of heavy rainfall events, which are a major cause of
urban flooding, underscores the urgent need for accurate precipitation
forecasting - particularly in urban areas where localized events often go
undetected by ground-based sensors. In Germany, only 17.3% of hourly heavy rain
events between 2001 and 2018 were recorded by rain gauges, highlighting the
limitations of traditional monitoring systems. Radar data are another source
that effectively tracks ongoing precipitation; however, forecasting the
development of heavy rain using radar alone remains challenging due to the
brief and unpredictable nature of such events. Our focus is on evaluating the
effectiveness of fusing satellite and radar data for nowcasting. We develop a
multimodal nowcasting model that combines both radar and satellite imagery for
predicting precipitation at lead times of 5, 15, and 30 minutes. We demonstrate
that this multimodal strategy significantly outperforms radar-only approaches.
Experimental results show that integrating satellite data improves prediction
accuracy, particularly for intense precipitation. The proposed model increases
the Critical Success Index for heavy rain by 4% and for violent rain by 3% at a
5-minute lead time. Moreover, it maintains higher predictive skill at longer
lead times, where radar-only performance declines. A qualitative analysis of
the severe flooding event in the state of North Rhine-Westphalia, Germany in
2021 further illustrates the superior performance of the multimodal model.
Unlike the radar-only model, which captures general precipitation patterns, the
multimodal model yields more detailed and accurate forecasts for regions
affected by heavy rain. This improved precision enables timely, reliable,
life-saving warnings. Implementation available at
https://github.com/RamaKassoumeh/Multimodal_heavy_rain

</details>


### [281] [Effective Series Decomposition and Components Learning for Time Series Generation](https://arxiv.org/abs/2511.00747)
*Zixuan Ma,Chenfeng Huang*

Main category: cs.LG

TL;DR: STDiffusion combines diffusion models with interpretable trend/seasonality decomposition to generate multivariate time series with high fidelity and interpretability, including a correction mechanism and support for long-sequence/multi-window generation.


<details>
  <summary>Details</summary>
Motivation: Many existing time series generators lack interpretable decomposition to separate trend and seasonal patterns, limiting meaningful synthesis and inter-component relationships.

Method: Use diffusion probabilistic models with a dedicated decomposition architecture: an MLP-based trend learner and adaptive wavelet distillation for seasonal components; a consistency-correction mechanism; supports multi-window long-sequence generation.

Result: Eight real-world datasets show state-of-the-art generation performance; demonstrates robustness and versatility in multi-window long-sequence tasks.

Conclusion: STDiffusion improves interpretability and generation quality in multivariate time series generation and suggests a general approach to combining diffusion models with interpretable decomposition for temporal data.

Abstract: Time series generation focuses on modeling the underlying data distribution
and resampling to produce authentic time series data. Key components, such as
trend and seasonality, drive temporal fluctuations, yet many existing
approaches fail to employ interpretative decomposition methods, limiting their
ability to synthesize meaningful trend and seasonal patterns. To address this
gap, we introduce Seasonal-Trend Diffusion (STDiffusion), a novel framework for
multivariate time series generation that integrates diffusion probabilistic
models with advanced learnable series decomposition techniques, enhancing the
interpretability of the generation process. Our approach separates the trend
and seasonal learning into distinct blocks: a Multi-Layer Perceptron (MLP)
structure captures the trend, while adaptive wavelet distillation facilitates
effective multi-resolution learning of seasonal components. This decomposition
improves the interpretability of the model on multiple scales. In addition, we
designed a comprehensive correction mechanism aimed at ensuring that the
generated components exhibit a high degree of internal consistency and preserve
meaningful interrelationships with one another. Our empirical studies on eight
real-world datasets demonstrate that STDiffusion achieves state-of-the-art
performance in time series generation tasks. Furthermore, we extend the model's
application to multi-window long-sequence time series generation, which
delivered reliable results and highlighted its robustness and versatility.

</details>


### [282] [Fast PINN Eigensolvers via Biconvex Reformulation](https://arxiv.org/abs/2511.00792)
*Akshay Sai Banderwaar,Abhishek Gupta*

Main category: cs.LG

TL;DR: A reformulated PINN framework (PINN-ACS) for eigenvalue problems casts eigenpair search as a biconvex optimization solved by alternating convex search with analytically optimal updates, yielding up to 500x speedups over gradient-based PINNs and publicly releasing code.


<details>
  <summary>Details</summary>
Motivation: Eigenvalue problems have a forward-inverse structure and are essential for understanding thermal response, stability, and natural modes. Traditional PINNs are accurate but slow; there is a need for a fast, mesh-free, convergent method to compute eigenpairs.

Method: Reformulate the eigenpair search as a biconvex optimization problem and apply alternating convex search (ACS) with analytically optimal updates for eigenvalues and eigenfunctions, enabling fast convergence and provable guarantees.

Result: PINN-ACS achieves high accuracy with convergence speeds up to 500Ã faster than gradient-based PINN training in numerical experiments.

Conclusion: The proposed PINN-ACS provides a fast, provably convergent alternative to standard PINNs for eigenvalue problems, broadening the practicality of PINNs in spectral analysis; code is released for replication.

Abstract: Eigenvalue problems have a distinctive forward-inverse structure and are
fundamental to characterizing a system's thermal response, stability, and
natural modes. Physics-Informed Neural Networks (PINNs) offer a mesh-free
alternative for solving such problems but are often orders of magnitude slower
than classical numerical schemes. In this paper, we introduce a reformulated
PINN approach that casts the search for eigenpairs as a biconvex optimization
problem, enabling fast and provably convergent alternating convex search (ACS)
over eigenvalues and eigenfunctions using analytically optimal updates.
Numerical experiments show that PINN-ACS attains high accuracy with convergence
speeds up to 500$\times$ faster than gradient-based PINN training. We release
our codes at https://github.com/NeurIPS-ML4PS-2025/PINN_ACS_CODES.

</details>


### [283] [Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration](https://arxiv.org/abs/2511.00794)
*Yan Sun,Jia Guo,Stanley Kok,Zihao Wang,Zujie Wen,Zhiqiang Zhang*

Main category: cs.LG

TL;DR: PREPO improves data efficiency in RLVR by using prompt perplexity to guide learning from easy to hard contexts and by boosting rollouts diversity via relative-entropy-based prioritization; achieves up to 3x fewer rollouts on Qwen/Llama for math reasoning with competitive performance and theoretical insights.


<details>
  <summary>Details</summary>
Motivation: RLVR training is computation-heavy because many rollouts contribute little to optimization. The paper seeks data-efficient improvements by exploiting intrinsic data properties during training.

Method: Introduce PREPO: (1) prompt perplexity as an adaptability indicator to schedule progression from well-understood contexts to harder ones; (2) differentiate rollouts by their relative entropy to amplify exploration discrepancy and prioritize highly exploratory sequences.

Result: On Qwen and Llama models, PREPO yields effective performance on mathematical reasoning benchmarks with up to 3x fewer rollouts than baselines; provides theoretical and in-depth explanations of data-efficiency improvements.

Conclusion: PREPO reduces rollout requirements while preserving competitive performance in RLVR, offering a principled, data-efficient approach with theoretical grounding.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has improved the
reasoning ability of large language models, yet training remains costly because
many rollouts contribute little to optimization, considering the amount of
computation required. This study investigates how simply leveraging intrinsic
data properties, almost free benefit during training, can improve data
efficiency for RLVR. We propose PREPO with two complementary components. First,
we adopt prompt perplexity as an indicator of model adaptability in learning,
enabling the model to progress from well-understood contexts to more
challenging ones. Second, we amplify the discrepancy among the rollouts by
differentiating their relative entropy, and prioritize sequences that exhibit a
higher degree of exploration. Together, these mechanisms reduce rollout demand
while preserving competitive performance. On the Qwen and Llama models, PREPO
achieves effective results on mathematical reasoning benchmarks with up to 3
times fewer rollouts than the baselines. Beyond empirical gains, we provide
theoretical and in-depth analyses explaining the underlying rationale of our
method to improve the data efficiency of RLVR.

</details>


### [284] [Attention Saturation and Gradient Suppression at Inflection Layers: Diagnosing and Mitigating Bottlenecks in Transformer Adaptation](https://arxiv.org/abs/2511.00797)
*Wang Zixian*

Main category: cs.LG

TL;DR: The paper analyzes gradient saturation in Transformers during fine-tuning that confines adaptation to high-level feature recombination, proposes diagnostics to identify inflection layers, and introduces a diagnose-first, inject-light fine-tuning strategy using LoRA adapters at those layers. Empirical results on SST-2 to Rotten Tomatoes show initialization regime effects: over-trained benefits from inflection-layer LoRA, under-trained may degrade; unblocking strategies depend on base feature strength.


<details>
  <summary>Details</summary>
Motivation: Pre-trained transformers often overfit to source-domain patterns and struggle to form target-domain patterns during fine-tuning. Understanding the saturation mechanism and selectively enabling gradient flow could improve cross-domain transfer with minimal parameter cost.

Method: Formally analyze gradient saturation via cross-entropy and softmax to show gradient suppression at inflection layers; develop layer-wise diagnostics: attention entropy, activation gradient norm, parameter gradient norm, and Delta-CKA under a shared PCA basis to locate inflection layers with low entropy and steep gradient decay; propose diagnose-first, inject-light fine-tuning by inserting LoRA adapters at inflection layers; test on BERT-base transfer from SST-2 to Rotten Tomatoes under under-trained and over-trained source regimes.

Result: Demonstrates that saturation confines adaptation to high-level recombination of existing features, while low-level reconstruction remains suppressed; inflection-layer LoRA injection can restore backward signals with minimal parameter cost; results show that over-trained initialization benefits from inflection-layer injection, whereas under-trained initialization may suffer; when base features are strong, unblocking inflection layers aids high-level adaptation; when base features are weak, full-pathway unblocking is needed; supported by analysis of layer-wise activation gradients and Delta-CKA dynamics.

Conclusion: Inflection-layer targeted, lightweight LoRA fine-tuning shifts enable more effective cross-domain adaptation by selectively restoring backward signals at key layers, with effects depending on the strength of base features and initialization regime; the proposed diagnostics provide a practical mechanism to guide selective adapter placement.

Abstract: Pre-trained Transformers often exhibit over-confidence in source patterns and
difficulty in forming new target-domain patterns during fine-tuning. We
formalize the mechanism of output saturation leading to gradient suppression
through standard cross-entropy and softmax analysis, showing that gradient
suppression at inflection layers confines adaptation to high-level
recombination of existing features while preventing low-level reconstruction.
We introduce a set of layer-wise diagnostic metrics -- attention entropy
(saturation proxy), activation gradient norm, parameter gradient norm, and
Delta-CKA under a shared PCA basis -- to identify inflection layers
characterized by both low attention entropy and steep gradient decay. Building
on these findings, we propose a diagnose-first, inject-light fine-tuning
strategy: selectively inserting LoRA adapters at inflection layers to restore
suppressed backward signals with minimal parameter overhead. Experiments on
BERT-base transfer from SST-2 to Rotten Tomatoes under under-trained and
over-trained source regimes reveal that over-trained initialization benefits
from inflection-layer LoRA injection, while under-trained initialization
suffers performance degradation. When base features are strong, unblocking
inflection layers facilitates high-level compositional adaptation; when base
features are weak, full-pathway unblocking is required for low-level
reconstruction, as supported by joint analysis of layer-wise activation
gradients and Delta-CKA dynamics.

</details>


### [285] [EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment](https://arxiv.org/abs/2511.00804)
*Abhiram Kusumba,Maitreya Patel,Kyle Min,Changhoon Kim,Chitta Baral,Yezhou Yang*

Main category: cs.LG

TL;DR: EraseFlow reframes concept erasure as exploring denoising trajectories in diffusion models, using GFlowNets with trajectory balance to sample full trajectories and learn a stochastic policy that removes target concepts while preserving prior knowledge, avoiding brittle rewards and retraining.


<details>
  <summary>Details</summary>
Motivation: Current concept erasure approaches often degrade image quality, rely on brittle adversarial/reward losses, or require costly retraining. A more robust, generalizable method is needed that can erase harmful or proprietary concepts without harming the model's prior.

Method: Cast unlearning as exploration in the space of diffusion denoising paths. Use Generative Flow Networks (GFlowNets) with trajectory balance to sample entire denoising trajectories, training a policy that steers generation away from target concepts while preserving priors. This avoids handcrafted rewards and retraining per concept.

Result: Empirically, EraseFlow outperforms existing baselines, achieving a favorable trade-off between concept erasure and prior preservation, and generalizes to unseen concepts while avoiding hackable rewards.

Conclusion: EraseFlow provides a novel, generalizable framework for concept unlearning that overcomes limitations of prior methods by treating trajectory spaces as the optimization domain and leveraging GFlowNets to balance exploration with prior preservation.

Abstract: Erasing harmful or proprietary concepts from powerful text to image
generators is an emerging safety requirement, yet current "concept erasure"
techniques either collapse image quality, rely on brittle adversarial losses,
or demand prohibitive retraining cycles. We trace these limitations to a myopic
view of the denoising trajectories that govern diffusion based generation. We
introduce EraseFlow, the first framework that casts concept unlearning as
exploration in the space of denoising paths and optimizes it with GFlowNets
equipped with the trajectory balance objective. By sampling entire trajectories
rather than single end states, EraseFlow learns a stochastic policy that steers
generation away from target concepts while preserving the model's prior.
EraseFlow eliminates the need for carefully crafted reward models and by doing
this, it generalizes effectively to unseen concepts and avoids hackable rewards
while improving the performance. Extensive empirical results demonstrate that
EraseFlow outperforms existing baselines and achieves an optimal trade off
between performance and prior preservation.

</details>


### [286] [Logic-informed reinforcement learning for cross-domain optimization of large-scale cyber-physical systems](https://arxiv.org/abs/2511.00806)
*Guangxi Wan,Peng Zeng,Xiaoting Dong,Chunhe Song,Shijie Cui,Dong Li,Qingwei Dong,Yiyang Liu,Hongfei Bai*

Main category: cs.LG

TL;DR: A logic-informed RL (LIRL) approach projects a latent policy action onto a logic-defined, admissible hybrid action set, ensuring feasibility at every exploration step while using standard policy-gradient methods.


<details>
  <summary>Details</summary>
Motivation: CPS require joint optimization of discrete cyber actions and continuous physical parameters under safety constraints. Existing hierarchical methods may hurt global optimality, and RL in hybrid spaces often relies on penalty/shielding that can fail to guarantee constraint satisfaction.

Method: Introduce a projection step that maps latent actions to the admissible manifold defined by first-order logic, enabling feasible exploration without penalty tuning. Integrate with standard policy-gradient algorithms, and express constraints declaratively for transferability across domains.

Result: Empirical evaluation across manufacturing, EV charging, and traffic control shows LIRL outperforms hierarchical optimization and state-of-the-art hybrid RL baselines. In robotic reducer assembly, combined makespan-energy reductions of 36.47â44.33%, with zero constraint violations.

Conclusion: LIRL enables safe, real-time optimization in large-scale CPS and is transferable to domains like smart transportation and smart grids, offering robust feasibility without penalty tuning.

Abstract: Cyber-physical systems (CPS) require the joint optimization of discrete cyber
actions and continuous physical parameters under stringent safety logic
constraints. However, existing hierarchical approaches often compromise global
optimality, whereas reinforcement learning (RL) in hybrid action spaces often
relies on brittle reward penalties, masking, or shielding and struggles to
guarantee constraint satisfaction. We present logic-informed reinforcement
learning (LIRL), which equips standard policy-gradient algorithms with
projection that maps a low-dimensional latent action onto the admissible hybrid
manifold defined on-the-fly by first-order logic. This guarantees feasibility
of every exploratory step without penalty tuning. Experimental evaluations have
been conducted across multiple scenarios, including industrial manufacturing,
electric vehicle charging stations, and traffic signal control, in all of which
the proposed method outperforms existing hierarchical optimization approaches.
Taking a robotic reducer assembly system in industrial manufacturing as an
example, LIRL achieves a 36.47\% to 44.33\% reduction at most in the combined
makespan-energy objective compared to conventional industrial hierarchical
scheduling methods. Meanwhile, it consistently maintains zero constraint
violations and significantly surpasses state-of-the-art hybrid-action
reinforcement learning baselines. Thanks to its declarative logic-based
constraint formulation, the framework can be seamlessly transferred to other
domains such as smart transportation and smart grid, thereby paving the way for
safe and real-time optimization in large-scale CPS.

</details>


### [287] [Equilibrium Policy Generalization: A Reinforcement Learning Framework for Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games](https://arxiv.org/abs/2511.00811)
*Runyu Lu,Peng Zhang,Ruochuan Shi,Yuanheng Zhu,Dongbin Zhao,Yang Liu,Dong Wang,Cesare Alippi*

Main category: cs.LG

TL;DR: EPG learns a generalized equilibrium policy for pursuit-evasion games across varying graph structures, enabling zero-shot transfer and scalable policy decomposition via a DP-based equilibrium oracle, grouping, and sequence modeling.


<details>
  <summary>Details</summary>
Motivation: Pursuit-evasion games are computationally expensive to solve and their optimal policies depend on graph structure; existing RL needs retraining or fine-tuning for new graphs, hindering real-time deployment. The work aims to enable cross-graph generalization and real-time applicability.

Method: Train an RL policy across multiple graphs against the equilibrium policy for each graph. Build a DP-based equilibrium oracle to compute pure-strategy Nash equilibria for single-graph policies. Introduce a grouping mechanism to scale with the number of pursuers and a sequence model to decompose joint policies, enabling scalability. Use a distance feature to facilitate cross-graph transfer.

Result: The framework achieves zero-shot generalization to unseen graphs for both pursuer and evader roles in no-exit and multi-exit PEGs. When using equilibrium guidance and graph-exit heuristics, the generalized pursuer policy can match fine-tuned policies from state-of-the-art PEG methods, demonstrating strong cross-graph robustness and scalability.

Conclusion: EPG offers a generalizable equilibrium-learning approach for PEGs, delivering robust zero-shot transfer across graphs and scalability with respect to pursuer numbers, and can approach or match tuned baselines under certain heuristics.

Abstract: Equilibrium learning in adversarial games is an important topic widely
examined in the fields of game theory and reinforcement learning (RL).
Pursuit-evasion game (PEG), as an important class of real-world games from the
fields of robotics and security, requires exponential time to be accurately
solved. When the underlying graph structure varies, even the state-of-the-art
RL methods require recomputation or at least fine-tuning, which can be
time-consuming and impair real-time applicability. This paper proposes an
Equilibrium Policy Generalization (EPG) framework to effectively learn a
generalized policy with robust cross-graph zero-shot performance. In the
context of PEGs, our framework is generally applicable to both pursuer and
evader sides in both no-exit and multi-exit scenarios. These two
generalizability properties, to our knowledge, are the first to appear in this
domain. The core idea of the EPG framework is to train an RL policy across
different graph structures against the equilibrium policy for each single
graph. To construct an equilibrium oracle for single-graph policies, we present
a dynamic programming (DP) algorithm that provably generates pure-strategy Nash
equilibrium with near-optimal time complexity. To guarantee scalability with
respect to pursuer number, we further extend DP and RL by designing a grouping
mechanism and a sequence model for joint policy decomposition, respectively.
Experimental results show that, using equilibrium guidance and a distance
feature proposed for cross-graph PEG training, the EPG framework guarantees
desirable zero-shot performance in various unseen real-world graphs. Besides,
when trained under an equilibrium heuristic proposed for the graphs with exits,
our generalized pursuer policy can even match the performance of the fine-tuned
policies from the state-of-the-art PEG methods.

</details>


### [288] [LL-ViT: Edge Deployable Vision Transformers with Look Up Table Neurons](https://arxiv.org/abs/2511.00812)
*Shashank Nag,Alan T. L. Bacellar,Zachary Susskind,Anshul Jha,Logan Liberty,Aishwarya Sivakumar,Eugene B. John,Krishnan Kailas,Priscila M. V. Lima,Neeraja J. Yadwadkar,Felipe M. G. Franca,Lizy K. John*

Main category: cs.LG

TL;DR: LL-ViT integrates LUT-based channel mixer into Vision Transformer for edge FPGA inference, achieving substantial model sparsity and energy savings while maintaining competitive accuracy on CIFAR-10/100 and Tiny-ImageNet.


<details>
  <summary>Details</summary>
Motivation: Vision transformers are powerful but resource-hungry, hindering edge deployment. LUT-based networks offer memory and compute reductions but historically underperform on standard vision tasks. There is a clear need for an edge-optimized transformer that preserves accuracy.

Method: Introduce LL-ViT by embedding LUT-based channel mixer within the transformer architecture. Use a neural-learning approach to train LUT functions rather than brute-force lookups. Develop an FPGA-based accelerator for LL-ViT, reducing weights and multiplications and improving energy efficiency.

Result: Achieves 95.5% on CIFAR-10, 78.8% on CIFAR-100, and 60.9% on Tiny-ImageNet, comparable to baseline ViT. Eliminates over 60% of model weights and 50% of multiplications. Delivers 1.9x energy efficiency and 1.3x lower latency versus an integer-quantized ViT accelerator, with superior throughput at 10.9W.

Conclusion: LL-ViT demonstrates a viable edge-optimized vision transformer by integrating a LUT-based channel mixer and deploying an FPGA accelerator. The approach substantially reduces memory and compute demands while retaining competitive accuracy and delivering notable energy and latency gains, illustrating the promise of LUT-based neural components for CV transformers.

Abstract: Vision Transformers have been tremendously successful in computer vision
tasks. However, their large computational, memory, and energy demands are a
challenge for edge inference on FPGAs -- a field that has seen a recent surge
in demand. We recognize the benefits of recent works on logic and Look Up Table
(LUT) based networks, such as LogicNets, NeuraLUT, DWN, among others, in
offering models that simultaneously reduce both the memory and compute
footprints. However, these models natively do not perform well on common vision
tasks, such as CIFAR-10/100. In this work, we propose LL-ViT, a novel edge
optimized vision transformer design that integrates layers of LUT neurons
within the transformer architecture. Based on our characterization that reveals
that a majority of model weights and computations are from the channel mixer
(MLP layer), we design an alternate LUT-based channel mixer, and simultaneously
develop an FPGA-based accelerator for LL-ViT. Contrary to some attempts to
replace each multiplication with a table lookup, our architecture utilizes a
neural learning approach which natively learns the LUT functions. This approach
allows for reduced model sizes, and a computational and energy-efficient
inference solution for vision transformer models. Evaluating on edge-suitable
workloads, we achieve accuracies of 95.5% on CIFAR-10, 78.8% on CIFAR-100, and
60.9% on Tiny-ImageNet datasets, comparable to the baseline transformer. LL-ViT
eliminates over 60% of the model weights and 50% of the multiplications in the
model, and achieves 1.9x energy efficiency and 1.3x lower latency over an
integer quantized ViT accelerator, while also offering superior throughput
against prior works at a 10.9W power budget.

</details>


### [289] [Identifying Slug Formation in Oil Well Pipelines: A Use Case from Industrial Analytics](https://arxiv.org/abs/2511.00851)
*Abhishek Patange,Sharat Chidambaran,Prabhat Shankar,Manjunath G. B.,Anindya Chatterjee*

Main category: cs.LG

TL;DR: An interactive, end-to-end data-driven slug detection tool for pipelines that merges data labeling, configurable ML training with multiple classifiers, time-series visualization, and real-time persistence-based alerts for live inference, bridging research and industrial decision-making.


<details>
  <summary>Details</summary>
Motivation: Slug formation in oil/gas pipelines poses safety and efficiency risks. Existing detection methods are often offline, require domain expertise, and lack real-time interpretability. There is a need for a lightweight, user-friendly, real-time tool that supports end-to-end data workflows in industry.

Method: An interactive application enabling data exploration and labeling, configurable model training with multiple classifiers, visualization of time-series overlays, and a real-time inference module that emits persistence-based alerts when slug events are detected. Supports labeled CSV uploads to live inference on unseen data, with features like snapshot persistence and visual labeling to aid human-in-the-loop ML.

Result: Demonstrates a functional prototype with end-to-end capabilities from data labeling to live inference. The demo showcases how interactive ML with human-in-the-loop can facilitate deployment in critical process industries and is likely generalizable to time-series fault diagnosis beyond oil and gas.

Conclusion: Interactive human-in-the-loop ML tools can bridge the gap between data science methods and real-world decision-making in critical industries. The approach is lightweight, portable, and broadly applicable to time-series fault diagnosis tasks beyond slug detection.

Abstract: Slug formation in oil and gas pipelines poses significant challenges to
operational safety and efficiency, yet existing detection approaches are often
offline, require domain expertise, and lack real-time interpretability. We
present an interactive application that enables end-to-end data-driven slug
detection through a compact and user-friendly interface. The system integrates
data exploration and labeling, configurable model training and evaluation with
multiple classifiers, visualization of classification results with time-series
overlays, and a real-time inference module that generates persistence-based
alerts when slug events are detected. The demo supports seamless workflows from
labeled CSV uploads to live inference on unseen datasets, making it
lightweight, portable, and easily deployable. By combining domain-relevant
analytics with novel UI/UX features such as snapshot persistence, visual
labeling, and real-time alerting, our tool adds significant dissemination value
as both a research prototype and a practical industrial application. The demo
showcases how interactive human-in-the-loop ML systems can bridge the gap
between data science methods and real-world decision-making in critical process
industries, with broader applicability to time-series fault diagnosis tasks
beyond oil and gas.

</details>


### [290] [FlexiCache: Leveraging Temporal Stability of Attention Heads for Efficient KV Cache Management](https://arxiv.org/abs/2511.00868)
*Nazmul Takbir,Hamidreza Alikhani,Nikil Dutt,Sangeetha Abdu Jyothi*

Main category: cs.LG

TL;DR: FlexiCache is a hierarchical KV-cache management system that exploits temporal stability in attention heads to reduce GPU memory and computation during LLM serving, while maintaining accuracy in long-context generation.


<details>
  <summary>Details</summary>
Motivation: KV caches grow with context and generation length, and only a subset of critical tokens (across heads) dominate attention. Existing caching strategies either waste memory or risk accuracy loss, especially during long generations.

Method: Classify KV heads as stable or unstable. Unstable heads keep all their KV pages in GPU memory; stable heads store only the top-K pages on GPU and offload the rest to host memory. Periodic reranking promotes new top pages for stable heads. Implementation is built atop vLLM.

Result: Up to 70% reduction in GPU memory footprint for long-context requests; offline serving throughput improves by 1.38â1.55x; online token latency decreases by 1.6â2.1x; accuracy is preserved in long-context, long-generation scenarios.

Conclusion: Exploiting the temporal stability of KV heads enables a practical, scalable approach to memory- and compute-efficient LLM serving without sacrificing accuracy, as demonstrated by FlexiCache on vLLM.

Abstract: Large Language Model (LLM) serving is increasingly constrained by the growing
size of the key-value (KV) cache, which scales with both context length and
generation length. Prior work shows that attention is dominated by a small
subset of critical tokens, yet existing systems struggle to exploit this
efficiently without degrading accuracy, especially in long generation. We make
a key observation: the temporal stability of these critical tokens varies
significantly across KV heads: some heads consistently focus on the same
tokens, while others shift frequently. Building on this insight, we introduce
FlexiCache, a hierarchical KV-cache management system that leverages the
temporal stability of KV heads to reduce GPU memory usage and computation
overhead, while preserving model accuracy. FlexiCache classifies KV heads as
stable or unstable: it retains all KV-cache pages from unstable heads in GPU
memory, whereas for stable heads, it keeps only the top-K pages on the GPU and
offloads the rest to host memory. By exploiting temporal stability, FlexiCache
performs periodic reranking for stable heads to fetch newly promoted top pages.
Implemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context
requests by up to 70%, improves offline serving throughput by 1.38-1.55x, and
lowers online token latency by 1.6-2.1x, all while maintaining accuracy in
long-context, long-generation scenarios.

</details>


### [291] [Training with Fewer Bits: Unlocking Edge LLMs Training with Stochastic Rounding](https://arxiv.org/abs/2511.00874)
*Taowen Liu,Marta Andronic,Deniz GÃ¼ndÃ¼z,George A. Constantinides*

Main category: cs.LG

TL;DR: Mini-batch stochastic rounding reduces quantization penalties in SGD; larger batch sizes can offset precision loss, with quantized weights/activations affecting gradient variance differently; theory is supported by experiments.


<details>
  <summary>Details</summary>
Motivation: Quantized training saves compute but introduces quantization noise that can hinder convergence. Stochastic rounding offers unbiased gradients, but its interaction with batch size and quantization types is not well understood.

Method: Theoretical analysis of mini-batch SGD with stochastic rounding under quantization, examining gradient variance and convergence; empirical validation across varying batch sizes and quantization schemes for weights and activations.

Result: Increased batch size mitigates the adverse effects of reduced precision under stochastic rounding; quantizing weights and activations affect gradient variance in distinct ways; experimental results align with the theoretical predictions.

Conclusion: Stochastic rounding can enable efficient quantized training when paired with appropriate batch sizing; attention should be paid to how quantizing weights versus activations shapes gradient statistics to preserve convergence and accuracy.

Abstract: LLM training is resource-intensive. Quantized training improves computational
and memory efficiency but introduces quantization noise, which can hinder
convergence and degrade model accuracy. Stochastic Rounding (SR) has emerged as
a theoretically attractive alternative to deterministic rounding, offering
unbiased gradient estimates. However, its interaction with other training
factors -- especially batch size -- remains under explored. In this paper, we
present a theoretical and empirical study of mini-batch stochastic gradient
descent (SGD) with SR, showing that increased batch sizes can compensate for
reduced precision during back-propagation. Furthermore, we show that quantizing
weights and activations impacts gradient variance in distinct ways. Our
experiments validate these theoretical insights.

</details>


### [292] [KFCPO: Kronecker-Factored Approximated Constrained Policy Optimization](https://arxiv.org/abs/2511.00880)
*Joonyoung Lim,Younghwan Yoo*

Main category: cs.LG

TL;DR: KFCPO is a safe RL algorithm that uses K-FAC-based second-order optimization with margin-aware gradient blending and KL rollback to balance reward and safety, achieving notable performance gains on Safety Gymnasium with OmniSafe.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between maximizing return and satisfying safety constraints in RL, targeting scalable, stable optimization without brittle fixed thresholds.

Method: 1) Use Kronecker-Factored Approximate Curvature (K-FAC) for efficient, layerwise natural-gradient updates by closed-form FIM approximations. 2) Introduce a margin-aware gradient manipulation that adaptively blends reward and cost gradients based on proximity to safety boundaries using a direction-sensitive projection to avoid interference and avoid fixed hard thresholds. 3) Apply a minibatch-level KL rollback strategy to maintain trust-region compliance and prevent destabilizing policy shifts.

Result: Empirical evaluation on Safety Gymnasium with OmniSafe shows KFCPO achieves 10.3% to 50.2% higher average return across environments than the best baseline that respects the safety constraint, indicating a better safety-performance balance.

Conclusion: KFCPO delivers scalable, stable safe RL with superior performance under safety constraints, outperforming strong baselines and offering a robust approach to balancing risk and reward; potential for broader applicability and further ablations.

Abstract: We propose KFCPO, a novel Safe Reinforcement Learning (Safe RL) algorithm
that combines scalable Kronecker-Factored Approximate Curvature (K-FAC) based
second-order policy optimization with safety-aware gradient manipulation. KFCPO
leverages K-FAC to perform efficient and stable natural gradient updates by
approximating the Fisher Information Matrix (FIM) in a layerwise, closed form
manner, avoiding iterative approximation overheads. To address the tradeoff
between reward maximization and constraint satisfaction, we introduce a margin
aware gradient manipulation mechanism that adaptively adjusts the influence of
reward and cost gradients based on the agent's proximity to safety boundaries.
This method blends gradients using a direction sensitive projection,
eliminating harmful interference and avoiding abrupt changes caused by fixed
hard thresholds. Additionally, a minibatch level KL rollback strategy is
adopted to ensure trust region compliance and to prevent destabilizing policy
shifts. Experiments on Safety Gymnasium using OmniSafe show that KFCPO achieves
10.3% to 50.2% higher average return across environments compared to the best
baseline that respected the safety constraint, demonstrating superior balance
of safety and performance.

</details>


### [293] [SpEx: A Spectral Approach to Explainable Clustering](https://arxiv.org/abs/2511.00885)
*Tal Argov,Tal Wagner*

Main category: cs.LG

TL;DR: A general, spectral-graph-based method to fit explanation trees to any clustering, unifying prior explainable-clustering approaches and showing competitive empirical performance.


<details>
  <summary>Details</summary>
Motivation: Existing work on explainable clustering focuses on specific objectives and lacks a universal method to produce explanation trees for arbitrary clustering results or datasets.

Method: Introduce a generic explainable clustering framework based on spectral graph partitioning. Build an explanation tree by partitioning data via graph cuts, and show that existing algorithms are instances of a two-graph Trevisan (2013) framework. The method can fit to any non-explainable clustering or directly to the dataset.

Result: Empirical experiments on multiple datasets demonstrate favorable performance against baselines, validating the approach.

Conclusion: Spectral graph partitioning provides a flexible, unifying, and effective foundation for explainable clustering, enabling a universal way to generate explanation trees for arbitrary clustering.

Abstract: Explainable clustering by axis-aligned decision trees was introduced by
Moshkovitz et al. (2020) and has gained considerable interest. Prior work has
focused on minimizing the price of explainability for specific clustering
objectives, lacking a general method to fit an explanation tree to any given
clustering, without restrictions. In this work, we propose a new and generic
approach to explainable clustering, based on spectral graph partitioning. With
it, we design an explainable clustering algorithm that can fit an explanation
tree to any given non-explainable clustering, or directly to the dataset
itself. Moreover, we show that prior algorithms can also be interpreted as
graph partitioning, through a generalized framework due to Trevisan (2013)
wherein cuts are optimized in two graphs simultaneously. Our experiments show
the favorable performance of our method compared to baselines on a range of
datasets.

</details>


### [294] [Learning with Category-Equivariant Representations for Human Activity Recognition](https://arxiv.org/abs/2511.00900)
*Yoshihiro Maruyama*

Main category: cs.LG

TL;DR: Introduces a symmetry-aware learning framework for human activity recognition that uses category symmetries over time, scale, and sensor hierarchy to produce stable, category-equivariant representations; achieves large robustness gains.


<details>
  <summary>Details</summary>
Motivation: Sensor signals drift with context, motion, and environment, causing models to degrade under shifts. Stable models are needed that preserve sensor relationships under realistic distortions.

Method: A categorical symmetry-aware framework that encodes time/scale/sensor-hierarchy transformations into feature representations, preserving inter-sensor relationships and yielding stability to distortions (time shifts, amplitude drift, device orientation) using category-equivariant representation theory.

Result: On the UCI HAR benchmark, out-of-distribution accuracy improves by about 46 percentage points (roughly 3.6x over baseline).

Conclusion: Demonstrates that abstract symmetry principles can yield concrete performance gains in everyday sensing tasks via category-equivariant representations.

Abstract: Human activity recognition is challenging because sensor signals shift with
context, motion, and environment; effective models must therefore remain stable
as the world around them changes. We introduce a categorical symmetry-aware
learning framework that captures how signals vary over time, scale, and sensor
hierarchy. We build these factors into the structure of feature
representations, yielding models that automatically preserve the relationships
between sensors and remain stable under realistic distortions such as time
shifts, amplitude drift, and device orientation changes. On the UCI Human
Activity Recognition benchmark, this categorical symmetry-driven design
improves out-of-distribution accuracy by approx. 46 percentage points (approx.
3.6x over the baseline), demonstrating that abstract symmetry principles can
translate into concrete performance gains in everyday sensing tasks via
category-equivariant representation theory.

</details>


### [295] [Random Spiking Neural Networks are Stable and Spectrally Simple](https://arxiv.org/abs/2511.00904)
*Ernesto Araya,Massimiliano Datres,Gitta Kutyniok*

Main category: cs.LG

TL;DR: Boolean-analysis of discrete-time LIF SNNs shows that wide classifiers are stable on average due to concentration of the Fourier spectrum on low frequencies; introduces spectral simplicity and finds random LIF-SNNs bias toward simple functions, with experiments supporting robustness.


<details>
  <summary>Details</summary>
Motivation: Address the limited theoretical understanding of stability and robustness in spiking neural networks by applying Boolean function Fourier analysis to quantify noise sensitivity and spectral concentration, and to connect this behavior to observed simplicity biases in deep networks.

Method: Model SNNs as discrete-time leaky integrate-and-fire networks and analyze them using Boolean function Fourier spectrum. Study noise sensitivity and stability in classification tasks, prove that wide LIF-SNN classifiers concentrate spectrum on low frequencies leading to average stability, formalize spectral simplicity, examine random LIF-SNNs, and validate findings with experiments on trained networks.

Result: The analysis shows wide LIF-SNN classifiers are stable on average due to low-frequency spectral concentration. Random LIF-SNNs exhibit a bias toward simple functions (spectral simplicity). Experiments on trained networks confirm the theoretical stability properties in practice.

Conclusion: A spectral perspective explains stability and robustness in SNNs, links to a simplicity bias via spectral simplicity, and provides groundwork for designing robust SNNs and future theoretical exploration.

Abstract: Spiking neural networks (SNNs) are a promising paradigm for energy-efficient
computation, yet their theoretical foundations-especially regarding stability
and robustness-remain limited compared to artificial neural networks. In this
work, we study discrete-time leaky integrate-and-fire (LIF) SNNs through the
lens of Boolean function analysis. We focus on noise sensitivity and stability
in classification tasks, quantifying how input perturbations affect outputs.
Our main result shows that wide LIF-SNN classifiers are stable on average, a
property explained by the concentration of their Fourier spectrum on
low-frequency components. Motivated by this, we introduce the notion of
spectral simplicity, which formalizes simplicity in terms of Fourier spectrum
concentration and connects our analysis to the simplicity bias observed in deep
networks. Within this framework, we show that random LIF-SNNs are biased toward
simple functions. Experiments on trained networks confirm that these stability
properties persist in practice. Together, these results provide new insights
into the stability and robustness properties of SNNs.

</details>


### [296] [Transformers as Intrinsic Optimizers: Forward Inference through the Energy Principle](https://arxiv.org/abs/2511.00907)
*Ruifeng Ren,Sheng Ouyang,Huayi Tang,Yong Liu*

Main category: cs.LG

TL;DR: Proposes an energy-based, unified framework for attention in Transformers, recasting softmax and linear attentions as energy-minimization dynamics and extending to advanced optimization methods for new attention variants.


<details>
  <summary>Details</summary>
Motivation: To understand and unify attention mechanisms in Transformer models through an energy-based lens, enabling principled modifications and extensions of attention dynamics.

Method: Define a global energy F*, an energy function E_i, and a gradient-descent-based optimization; show softmax attention as Helmholtz free energy minimization with elastic-like E_i and residual connections; generalize to linear and multi-head attentions; derive momentum, Nesterov, and Newton variants as new attention forms.

Result: Preliminary experiments provide initial support that the energy-based framework can guide the design of attention mechanisms and yield viable extensions.

Conclusion: An energy-based perspective offers a coherent framework to analyze and design attention in Transformers, with promising early results for energy-driven variants and future research directions.

Abstract: Transformers have demonstrated strong adaptability across a wide range of
tasks and have become the backbone of modern Large Language Models (LLMs).
However, their underlying mechanisms remain open for further exploration. The
energy-based perspective has long provided a valuable principle for
understanding neural computation. In this paper, we revisit the principle of
energy as a lens to understand attention-based Transformer models. We present a
unified energy-based framework which is composed of three key components: the
global energy $F^*$, the energy function $E_i$ and the employed gradient
descent (GD) form. Within this framework, standard softmax attention can be
viewed as a special case of minimizing the Helmholtz free energy as $F^*$ using
standard GD when $E_i$ takes the form of elastic potential energy, with
residual connections ensuring that this optimization proceeds in an incremental
manner. In addition, linear attentions can also be naturally incorporated into
this framework by adjusting the corresponding energy forms. We also extend the
above analysis to the multi-head setting, where the energy is defined across
multiple low-dimensional subspaces. Building on this framework, we propose
energy-based modifications of attention structures. Inspired by classical GD
algorithms, we extend the original attention formulation based on standard GD
to the momentum-based GD, Nesterov Accelerated Gradient (NAG), and Newton's
method variants, each inducing a corresponding new attention structure. Our
experiments provide preliminary support for the potential of the energy-based
framework for designing attention mechanisms.

</details>


### [297] [Motion-Robust Multimodal Fusion of PPG and Accelerometer Signals for Three-Class Heart Rhythm Classification](https://arxiv.org/abs/2511.00949)
*Yangyang Zhao,Matti Kaisti,Olli Lahdenoja,Tero Koivisto*

Main category: cs.LG

TL;DR: RhythmiNet: a multimodal, attention-based ResNet that fuses wrist PPG with accelerometer data for three-class rhythm classification (AF, SR, Other); shows robustness to motion and outperforms PPG-only and HRV-based baselines.


<details>
  <summary>Details</summary>
Motivation: Address the vulnerability of wrist-worn PPG to motion artifacts and physiological noise, and the need for multi-class rhythm detection beyond binary AF vs non-AF in real-world settings.

Method: A residual neural network with temporal and channel attention modules that jointly processes PPG and accelerometer signals, performing three-class rhythm classification (AF, SR, Other). The study stratifies test data by motion intensity percentiles to assess robustness without excluding segments.

Result: 4.3% macro-AUC improvement over PPG-only baseline; 12% performance gain over a logistic regression model based on handcrafted HRV features, demonstrating benefits of multimodal fusion and attention in noisy, real-world data.

Conclusion: Multimodal fusion with attention enhances robustness of wearable rhythm monitoring in real-world conditions and improves discrimination among AF, SR, and Other compared to single-modality or traditional feature-based approaches.

Abstract: Atrial fibrillation (AF) is a leading cause of stroke and mortality,
particularly in elderly patients. Wrist-worn photoplethysmography (PPG) enables
non-invasive, continuous rhythm monitoring, yet suffers from significant
vulnerability to motion artifacts and physiological noise. Many existing
approaches rely solely on single-channel PPG and are limited to binary AF
detection, often failing to capture the broader range of arrhythmias
encountered in clinical settings. We introduce RhythmiNet, a residual neural
network enhanced with temporal and channel attention modules that jointly
leverage PPG and accelerometer (ACC) signals. The model performs three-class
rhythm classification: AF, sinus rhythm (SR), and Other. To assess robustness
across varying movement conditions, test data are stratified by
accelerometer-based motion intensity percentiles without excluding any
segments. RhythmiNet achieved a 4.3% improvement in macro-AUC over the PPG-only
baseline. In addition, performance surpassed a logistic regression model based
on handcrafted HRV features by 12%, highlighting the benefit of multimodal
fusion and attention-based learning in noisy, real-world clinical data.

</details>


### [298] [The Hidden Power of Normalization: Exponential Capacity Control in Deep Neural Networks](https://arxiv.org/abs/2511.00958)
*Khoat Than*

Main category: cs.LG

TL;DR: Normalization layers can dramatically reduce a network's Lipschitz constant exponentially with depth, turning unnormalized networks (which can have exponential Lipschitz growth and high capacity) into well-behaved models. This links normalization to both optimization stability and generalization via capacity control.


<details>
  <summary>Details</summary>
Motivation: To theoretically explain why normalization stabilizes optimization and improves generalization, especially in deep networks with many normalization layers, by framing normalization as a mechanism for capacity control through Lipschitz constants.

Method: Develop a theoretical framework that models DNNs and normalization layers, analyzes Lipschitz constants with respect to inputs and parameters, and proves that inserting normalization layers reduces these constants exponentially in the number of normalization operations, thereby smoothing the loss landscape and constraining capacity.

Result: Theorem: unnormalized DNNs can have exponentially large Lipschitz constants (bad DNNs, uncountably many). In contrast, inserting normalization reduces the Lipschitz constant exponentially with the number of normalization operations. Consequences include exponentially smoother loss landscapes and stronger generalization guarantees due to capacity control.

Conclusion: Normalization methods provide a principled mechanism for capacity control in deep networks, offering a theoretical explanation for their empirical success in stabilizing optimization and improving generalization as network depth and normalization usage grow.

Abstract: Normalization methods are fundamental components of modern deep neural
networks (DNNs). Empirically, they are known to stabilize optimization dynamics
and improve generalization. However, the underlying theoretical mechanism by
which normalization contributes to both optimization and generalization remains
largely unexplained, especially when using many normalization layers in a DNN
architecture.
  In this work, we develop a theoretical framework that elucidates the role of
normalization through the lens of capacity control. We prove that an
unnormalized DNN can exhibit exponentially large Lipschitz constants with
respect to either its parameters or inputs, implying excessive functional
capacity and potential overfitting. Such bad DNNs are uncountably many. In
contrast, the insertion of normalization layers provably can reduce the
Lipschitz constant at an exponential rate in the number of normalization
operations. This exponential reduction yields two fundamental consequences: (1)
it smooths the loss landscape at an exponential rate, facilitating faster and
more stable optimization; and (2) it constrains the effective capacity of the
network, thereby enhancing generalization guarantees on unseen data. Our
results thus offer a principled explanation for the empirical success of
normalization methods in deep learning.

</details>


### [299] [Using Synthetic Data to estimate the True Error is theoretically and practically doable](https://arxiv.org/abs/2511.00964)
*Hai Hoang Thanh,Duy-Tung Nguyen,Hung The Tran,Khoat Than*

Main category: cs.LG

TL;DR: A theory-guided use of synthetic data to estimate test error with limited labels, deriving bounds and an optimization method to generate synthetic samples; shows improved accuracy and reliability over baselines.


<details>
  <summary>Details</summary>
Motivation: Evaluating model performance with few labeled test samples is challenging but common when labeling is costly. Synthetic data from generative models offers a promising workaround, but reliability hinges on the generator and principled data selection.

Method: Derive generalization bounds that incorporate synthetic data for evaluation. Propose a method to optimize synthetic samples for the purpose of estimating test error, and analyze how the generator's quality affects bounds. Validate the approach experimentally on simulated and tabular datasets, comparing to baselines.

Result: The proposed method yields more accurate and reliable estimates of test error than existing baselines, with theory showing the generator's quality plays a significant role.

Conclusion: Theoretical bounds guide the synthesis of evaluation data under label scarcity, and optimized synthetic data can substantially improve evaluation reliability if the generator is of sufficient quality.

Abstract: Accurately evaluating model performance is crucial for deploying machine
learning systems in real-world applications. Traditional methods often require
a sufficiently large labeled test set to ensure a reliable evaluation. However,
in many contexts, a large labeled dataset is costly and labor-intensive.
Therefore, we sometimes have to do evaluation by a few labeled samples, which
is theoretically challenging. Recent advances in generative models offer a
promising alternative by enabling the synthesis of high-quality data. In this
work, we make a systematic investigation about the use of synthetic data to
estimate the test error of a trained model under limited labeled data
conditions. To this end, we develop novel generalization bounds that take
synthetic data into account. Those bounds suggest novel ways to optimize
synthetic samples for evaluation and theoretically reveal the significant role
of the generator's quality. Inspired by those bounds, we propose a
theoretically grounded method to generate optimized synthetic data for model
evaluation. Experimental results on simulation and tabular datasets demonstrate
that, compared to existing baselines, our method achieves accurate and more
reliable estimates of the test error.

</details>


### [300] [Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow](https://arxiv.org/abs/2511.00977)
*Kristiyan Sakalyan,Alessandro Palma,Filippo Guerranti,Fabian J. Theis,Stephan GÃ¼nnemann*

Main category: cs.LG

TL;DR: NicheFlow is a flow-based generative model that infers temporal trajectories of cellular microenvironments across sequential spatial slides, treating local neighborhoods as point clouds and jointly modeling cell-state and spatial-coordinate evolution via optimal transport and Variational Flow Matching, recovering global tissue architecture and local microenvironment composition across diverse spatiotemporal data.


<details>
  <summary>Details</summary>
Motivation: Current methods model cellular evolution at the single-cell level, missing coordinated changes of cellular states within tissue microenvironments across space and time; spatial transcriptomics provides data to study tissue development and disease progression but requires methods that capture spatiotemporal microenvironments.

Method: Represent local neighborhoods as point clouds and apply a flow-based generative model across sequential slides. Use optimal transport and Variational Flow Matching to jointly model evolution of cell states and spatial coordinates, enabling inference of temporal trajectories of cellular microenvironments.

Result: The approach recovers both global spatial architecture and local microenvironment composition across diverse spatiotemporal datasets, from embryonic to brain development.

Conclusion: NicheFlow advances understanding of tissue development and disease progression by modeling coordinated microenvironment evolution, bridging spatial and temporal dynamics in tissue organization.

Abstract: Understanding the evolution of cellular microenvironments in spatiotemporal
data is essential for deciphering tissue development and disease progression.
While experimental techniques like spatial transcriptomics now enable
high-resolution mapping of tissue organization across space and time, current
methods that model cellular evolution operate at the single-cell level,
overlooking the coordinated development of cellular states in a tissue. We
introduce NicheFlow, a flow-based generative model that infers the temporal
trajectory of cellular microenvironments across sequential spatial slides. By
representing local cell neighborhoods as point clouds, NicheFlow jointly models
the evolution of cell states and spatial coordinates using optimal transport
and Variational Flow Matching. Our approach successfully recovers both global
spatial architecture and local microenvironment composition across diverse
spatiotemporal datasets, from embryonic to brain development.

</details>


### [301] [Balanced Multimodal Learning via Mutual Information](https://arxiv.org/abs/2511.00987)
*Rongrong Xie,Guido Sanguinetti*

Main category: cs.LG

TL;DR: A two-stage, mutual-information-guided framework that balances multimodal learning by cross-modal knowledge distillation and a dynamic, multitask-like training regime to address modality imbalance.


<details>
  <summary>Details</summary>
Motivation: Modality imbalance due to limited data and varying data quality, especially in biology, leading to suboptimal cross-modal integration.

Method: Stage 1: cross-modal knowledge distillation where stronger modalities guide weaker ones. Stage 2: multitask-like training that dynamically adjusts gradient contributions using modality-specific performance and inter-modal mutual information. The framework uses mutual information to quantify inter-modality interactions and balance contributions.

Result: Reported significant performance gains over baselines and ablations, validating the approach's effectiveness in addressing modality imbalance; improved intermodal synergy and conflict resolution.

Conclusion: A practical, generalizable strategy for robust multimodal learning under modality imbalance, with potential applicability beyond biology.

Abstract: Multimodal learning has increasingly become a focal point in research,
primarily due to its ability to integrate complementary information from
diverse modalities. Nevertheless, modality imbalance, stemming from factors
such as insufficient data acquisition and disparities in data quality, has
often been inadequately addressed. This issue is particularly prominent in
biological data analysis, where datasets are frequently limited, costly to
acquire, and inherently heterogeneous in quality. Conventional multimodal
methodologies typically fall short in concurrently harnessing intermodal
synergies and effectively resolving modality conflicts.
  In this study, we propose a novel unified framework explicitly designed to
address modality imbalance by utilizing mutual information to quantify
interactions between modalities. Our approach adopts a balanced multimodal
learning strategy comprising two key stages: cross-modal knowledge distillation
(KD) and a multitask-like training paradigm. During the cross-modal KD
pretraining phase, stronger modalities are leveraged to enhance the predictive
capabilities of weaker modalities. Subsequently, our primary training phase
employs a multitask-like learning mechanism, dynamically calibrating gradient
contributions based on modality-specific performance metrics and intermodal
mutual information. This approach effectively alleviates modality imbalance,
thereby significantly improving overall multimodal model performance.

</details>


### [302] [Hydra: Dual Exponentiated Memory for Multivariate Time Series Analysis](https://arxiv.org/abs/2511.00989)
*Asal Meskin,Alireza Mirrokni,Ali Najar,Ali Behrouz*

Main category: cs.LG

TL;DR: Hydra introduces a 2D recurrent memory for multivariate time series with a 2D recurrence across time and variates, using a 2D-chunk-wise training that yields ~10x efficiency, and achieves superior performance on forecasting, classification, and anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Transformers, MLPs, and linear models often lack temporal inductive bias and inter-variate coupling for multivariate time series, while traditional linear RNNs are limited to single sequences and prone to error accumulation; there is a need for efficient, temporally aware models that capture inter-variable dependencies.

Method: A by-design two-headed meta in-context memory module (Hydra) that employs a 2-dimensional recurrence across time and variate at each step. It uses a 2D-chunk-wise training algorithm that approximates the recurrence to achieve approximately 10x efficiency without sacrificing effectiveness.

Result: Empirical results across diverse tasks and datasets (forecasting, classification, anomaly detection) demonstrate Hydraâs superior performance over state-of-the-art baselines.

Conclusion: Hydra provides an efficient, temporally biased, multivariate memory mechanism for time-series modeling, enabling strong performance with scalable training, and showing broad applicability across time-series tasks.

Abstract: In recent years, effectively modeling multivariate time series has gained
significant popularity, mainly due to its wide range of applications, ranging
from healthcare to financial markets and energy management. Transformers, MLPs,
and linear models as the de facto backbones of modern time series models have
shown promising results in single-variant and/or short-term forecasting. These
models, however: (1) are permutation equivariant and so lack temporal inductive
bias, being less expressive to capture the temporal dynamics; (2) are naturally
designed for univariate setup, missing the inter-dependencies of temporal and
variate dimensions; and/or (3) are inefficient for Long-term time series
modeling. To overcome training and inference efficiency as well as the lack of
temporal inductive bias, recently, linear Recurrent Neural Networks (RNNs) have
gained attention as an alternative to Transformer-based models. These models,
however, are inherently limited to a single sequence, missing inter-variate
dependencies, and can propagate errors due to their additive nature. In this
paper, we present Hydra, a by-design two-headed meta in-context memory module
that learns how to memorize patterns at test time by prioritizing time series
patterns that are more informative about the data. Hydra uses a 2-dimensional
recurrence across both time and variate at each step, which is more powerful
than mixing methods. Although the 2-dimensional nature of the model makes its
training recurrent and non-parallelizable, we present a new 2D-chunk-wise
training algorithm that approximates the actual recurrence with $\times 10$
efficiency improvement, while maintaining the effectiveness. Our experimental
results on a diverse set of tasks and datasets, including time series
forecasting, classification, and anomaly detection show the superior
performance of Hydra compared to state-of-the-art baselines.

</details>


### [303] [None To Optima in Few Shots: Bayesian Optimization with MDP Priors](https://arxiv.org/abs/2511.01006)
*Diantong Li,Kyunghyun Cho,Chong Liu*

Main category: cs.LG

TL;DR: ProfBO uses prior-informed MDPs and meta-learning to achieve sample-efficient Bayesian optimization, outperforming baselines on biomedical and hyperparameter tasks.


<details>
  <summary>Details</summary>
Motivation: Bayesian optimization is powerful but often requires many evaluations; in costly real-world settings (drug discovery, materials design), reducing evaluations is critical. Leverage procedural knowledge from related tasks to guide optimization.

Method: Construct MDP priors from related source tasks to capture optimization trajectories; embed these priors into a prior-fitted neural network; apply model-agnostic meta-learning (MAML) for fast adaptation to new target tasks; evaluate on real-world Covid/Cancer benchmarks and hyperparameter tuning.

Result: ProfBO consistently outperforms state-of-the-art BO methods, achieving high-quality solutions with significantly fewer evaluations and demonstrating readiness for practical deployment.

Conclusion: MDP-prior informed meta-learning enables highly sample-efficient BO for high-cost black-box problems, enabling practical deployment in domains like healthcare and materials design.

Abstract: Bayesian Optimization (BO) is an efficient tool for optimizing black-box
functions, but its theoretical guarantees typically hold in the asymptotic
regime. In many critical real-world applications such as drug discovery or
materials design, where each evaluation can be very costly and time-consuming,
BO becomes impractical for many evaluations. In this paper, we introduce the
Procedure-inFormed BO (ProfBO) algorithm, which solves black-box optimization
with remarkably few function evaluations. At the heart of our algorithmic
design are Markov Decision Process (MDP) priors that model optimization
trajectories from related source tasks, thereby capturing procedural knowledge
on efficient optimization. We embed these MDP priors into a prior-fitted neural
network and employ model-agnostic meta-learning for fast adaptation to new
target tasks. Experiments on real-world Covid and Cancer benchmarks and
hyperparameter tuning tasks demonstrate that ProfBO consistently outperforms
state-of-the-art methods by achieving high-quality solutions with significantly
fewer evaluations, making it ready for practical deployment.

</details>


### [304] [Equality Graph Assisted Symbolic Regression](https://arxiv.org/abs/2511.01009)
*Fabricio Olivetti de Franca,Gabriel Kronberger*

Main category: cs.LG

TL;DR: An e-graphâbased search (SymRegg) for symbolic regression using GP to reduce redundant evaluations and navigate neutrality plateaus, achieving efficiency with a small hyperparameter set while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: In Symbolic Regression with Genetic Programming, neutrality creates large search plateaus, leading to many redundant evaluations (reported as up to ~60% of total evaluations). The e-graph structure can compactly store and group equivalent expressions, enabling the search to avoid re-evaluating expressions it has already visited.

Method: Introduce SymRegg, a search algorithm that uses an e-graph to store expressions. Perturb solutions sampled from expressions in the e-graph; if a perturbation yields an unvisited expression, insert it into the e-graph and generate its equivalent forms. This process reduces redundant evaluations by exploiting equivalence classes, with a minimalist hyperparameter set.

Result: SymRegg is reported to improve search efficiency while maintaining consistently accurate results across different datasets, and it relies on a minimalist set of hyperparameters.

Conclusion: An e-graphâbased approach (SymRegg) can enhance GP-driven symbolic regression by pruning redundant evaluations, effectively navigating neutrality plateaus and maintaining accuracy with few hyperparameters.

Abstract: In Symbolic Regression (SR), Genetic Programming (GP) is a popular search
algorithm that delivers state-of-the-art results in term of accuracy. Its
success relies on the concept of neutrality, which induces large plateaus that
the search can safely navigate to more promising regions. Navigating these
plateaus, while necessary, requires the computation of redundant expressions,
up to 60% of the total number of evaluation, as noted in a recent study. The
equality graph (e-graph) structure can compactly store and group equivalent
expressions enabling us to verify if a given expression and their variations
were already visited by the search, thus enabling us to avoid unnecessary
computation. We propose a new search algorithm for symbolic regression called
SymRegg that revolves around the e-graph structure following simple steps:
perturb solutions sampled from a selection of expressions stored in the
e-graph, if it generates an unvisited expression, insert it into the e-graph
and generates its equivalent forms. We show that SymRegg is capable of
improving the efficiency of the search, maintaining consistently accurate
results across different datasets while requiring a choice of a minimalist set
of hyperparameters.

</details>


### [305] [What's the next frontier for Data-centric AI? Data Savvy Agents](https://arxiv.org/abs/2511.01015)
*Nabeel Seedat,Jiashuo Liu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: Proposes a four-capability framework to make autonomous agents data-savvy: proactive data acquisition, sophisticated data processing, interactive test data synthesis, and continual adaptation.


<details>
  <summary>Details</summary>
Motivation: To enable scalable, reliable real-world deployment of AI agents by focusing on how agents acquire, process, and evolve data, addressing gaps left by current reasoning-centric work.

Method: Conceptual framework that defines four data-centric capabilities, with design considerations and a shift in evaluation toward dynamic, interactive data rather than static benchmarks.

Result: Articulates a vision and justification for data-savvy agents; no empirical results are reported in the abstract.

Conclusion: Data-focused capabilities should be prioritized in agent design; data-centric AI is the next frontier for reliable, real-world deployment.

Abstract: The recent surge in AI agents that autonomously communicate, collaborate with
humans and use diverse tools has unlocked promising opportunities in various
real-world settings. However, a vital aspect remains underexplored: how agents
handle data. Scalable autonomy demands agents that continuously acquire,
process, and evolve their data. In this paper, we argue that data-savvy
capabilities should be a top priority in the design of agentic systems to
ensure reliable real-world deployment. Specifically, we propose four key
capabilities to realize this vision: (1) Proactive data acquisition: enabling
agents to autonomously gather task-critical knowledge or solicit human input to
address data gaps; (2) Sophisticated data processing: requiring context-aware
and flexible handling of diverse data challenges and inputs; (3) Interactive
test data synthesis: shifting from static benchmarks to dynamically generated
interactive test data for agent evaluation; and (4) Continual adaptation:
empowering agents to iteratively refine their data and background knowledge to
adapt to shifting environments. While current agent research predominantly
emphasizes reasoning, we hope to inspire a reflection on the role of data-savvy
agents as the next frontier in data-centric AI.

</details>


### [306] [SARIMAX-Based Power Outage Prediction During Extreme Weather Events](https://arxiv.org/abs/2511.01017)
*Haoran Ye,Qiuzhuang Sun,Yang Yang*

Main category: cs.LG

TL;DR: A SARIMAX-based framework with extensive feature engineering and robust optimization improves short-term and 48-hour outage forecasts during extreme weather, achieving RMSE 177.2 (8.4% better than baseline 193.4).


<details>
  <summary>Details</summary>
Motivation: Forecasts of power outages during extreme weather enable grid resilience and proactive mitigation. A SARIMAX model with exogenous weather inputs, combined with systematic feature engineering and robust optimization, can handle irregular data and improve predictive accuracy.

Method: 1) Data cleaning to remove zero-variance and unknown features. 2) Correlation-based feature filtering to reduce multicollinearity. 3) Feature augmentation with temporal embeddings, multi-scale lag features, and weather variables with lags as exogenous inputs. 4) Standardization for numerical stability. 5) Hierarchical fitting with sequential optimization, automatic downgrading to ARIMA if convergence fails, and historical-mean fallback. 6) Separate optimization for short-term (24h) and medium-term (48h) horizons using RMSE. 

Result: The approach achieves RMSE of 177.2 for outage prediction, an 8.4% improvement over a baseline RMSE of 193.4, validating the feature engineering and robust optimization strategy.

Conclusion: The proposed pipeline effectively improves extreme-weather outage forecasting performance, with a robust fitting strategy that handles data irregularities and enhances short- and medium-term predictions. Potential generalization to other regions and resilience applications.

Abstract: This study develops a SARIMAX-based prediction system for short-term power
outage forecasting during extreme weather events. Using hourly data from
Michigan counties with outage counts and comprehensive weather features, we
implement a systematic two-stage feature engineering pipeline: data cleaning to
remove zero-variance and unknown features, followed by correlation-based
filtering to eliminate highly correlated predictors. The selected features are
augmented with temporal embeddings, multi-scale lag features, and weather
variables with their corresponding lags as exogenous inputs to the SARIMAX
model. To address data irregularity and numerical instability, we apply
standardization and implement a hierarchical fitting strategy with sequential
optimization methods, automatic downgrading to ARIMA when convergence fails,
and historical mean-based fallback predictions as a final safeguard. The model
is optimized separately for short-term (24 hours) and medium-term (48 hours)
forecast horizons using RMSE as the evaluation metric. Our approach achieves an
RMSE of 177.2, representing an 8.4\% improvement over the baseline method (RMSE
= 193.4), thereby validating the effectiveness of our feature engineering and
robust optimization strategy for extreme weather-related outage prediction.

</details>


### [307] [MedEqualizer: A Framework Investigating Bias in Synthetic Medical Data and Mitigation via Augmentation](https://arxiv.org/abs/2511.01054)
*Sama Salarian,Yue Zhang,Swati Padhee,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: GAN-based synthetic healthcare data reveal fairness gaps across protected demographics in MIMIC-III; MedEqualizer, a model-agnostic augmentation framework, improves representation of underrepresented subgroups before generation, yielding more balanced synthetic data.


<details>
  <summary>Details</summary>
Motivation: To enable accessible and useful synthetic healthcare data without bias, ensuring fair representation of protected attributes so clinical research and decisions are not distorted by imbalanced synthetic datasets.

Method: Assess multiple GAN-based generative models on MIMIC-III. Use logarithmic disparity to quantify subgroup representation. Identify under-/overrepresented subgroups. Propose MedEqualizer, an augmentation framework that enriches underrepresented subgroups prior to synthetic data generation, and evaluate its impact on synthetic data fairness.

Result: MedEqualizer substantially improves demographic balance in the resulting synthetic datasets, addressing disparities observed in baseline GAN-generated data.

Conclusion: MedEqualizer provides a practical, model-agnostic route to fairer, more representative healthcare data synthesis and can be extended to other datasets and protected attributes.

Abstract: Synthetic healthcare data generation presents a viable approach to enhance
data accessibility and support research by overcoming limitations associated
with real-world medical datasets. However, ensuring fairness across protected
attributes in synthetic data is critical to avoid biased or misleading results
in clinical research and decision-making. In this study, we assess the fairness
of synthetic data generated by multiple generative adversarial network
(GAN)-based models using the MIMIC-III dataset, with a focus on
representativeness across protected demographic attributes. We measure subgroup
representation using the logarithmic disparity metric and observe significant
imbalances, with many subgroups either underrepresented or overrepresented in
the synthetic data, compared to the real data. To mitigate these disparities,
we introduce MedEqualizer, a model-agnostic augmentation framework that
enriches the underrepresented subgroups prior to synthetic data generation. Our
results show that MedEqualizer significantly improves demographic balance in
the resulting synthetic datasets, offering a viable path towards more equitable
and representative healthcare data synthesis.

</details>


### [308] [Window-Based Feature Engineering for Cognitive Workload Detection](https://arxiv.org/abs/2511.01060)
*Andrew Hallam,R G Gayathri,Glory Lee,Atul Sajjanhar*

Main category: cs.LG

TL;DR: Window-based temporal feature extraction on the COLET dataset with ML/DL classifiers; deep/tabular DL models outperform traditional ML across precision, F1, and accuracy, enabling real-time cognitive workload assessment.


<details>
  <summary>Details</summary>
Motivation: Cognitive workload is critical across health, psychology, and defense; accurate, real-time assessment is needed. The COLET dataset and windowed features aim to capture temporal context to improve classification performance over existing methods.

Method: Apply window-based temporal partitioning to COLET to generate features, then train both traditional machine learning models and deep learning models (notably tabular architectures) for classifying cognitive workload levels; compare performance using standard metrics.

Result: Deep learning models, especially tabular architectures, outperform traditional ML methods in precision, F1-score, accuracy, and overall classification performance.

Conclusion: Window-based temporal feature extraction combined with deep learning is effective for cognitive workload classification on COLET and shows potential for real-time assessment in dynamic tasks; future work could explore more datasets, model interpretability, and real-time deployment considerations.

Abstract: Cognitive workload is a topic of increasing interest across various fields
such as health, psychology, and defense applications. In this research, we
focus on classifying cognitive workload using the COLET dataset, employing a
window-based approach for feature generation and machine/deep learning
techniques for classification. We apply window-based temporal partitioning to
enhance features used in existing research, followed by machine learning and
deep learning models to classify different levels of cognitive workload. The
results demonstrate that deep learning models, particularly tabular
architectures, outperformed traditional machine learning methods in precision,
F1-score, accuracy, and classification precision. This study highlights the
effectiveness of window-based temporal feature extraction and the potential of
deep learning techniques for real-time cognitive workload assessment in complex
and dynamic tasks.

</details>


### [309] [Energy-Efficient Deep Learning Without Backpropagation: A Rigorous Evaluation of Forward-Only Algorithms](https://arxiv.org/abs/2511.01061)
*PrzemysÅaw Spyra,Witold Dzwinel*

Main category: cs.LG

TL;DR: A backpropagation-free Mono-Forward (MF) algorithm outperforms an optimally tuned BP baseline on MLPs, with substantial energy and speed gains, while challenging common beliefs about BP's necessity.


<details>
  <summary>Details</summary>
Motivation: Question the indispensability of backpropagation for state-of-the-art performance; provide hardware-validated evidence of BP-free learning achieving better generalization; re-evaluate memory efficiency and practical overhead in BP-free methods.

Method: Compare MF across native MLP architectures to a tuned BP baseline using identical models and universal hyperparameter optimization; measure accuracy, energy, training time; trace the evolutionary path FF -> CaFo -> MF; assess memory implications empirically.

Result: MF consistently achieves higher classification accuracy than tuned BP on MLPs; energy consumption reduced up to 41%; training time reduced up to 34%; memory considerations reveal practical overhead can offset theoretical gains; generalization improvements observed.

Conclusion: MF emerges as a viable, high-performance, and sustainable alternative to BP for MLPs, with a documented evolutionary lineage and practical guidelines, albeit with caveats around memory efficiency.

Abstract: The long-held assumption that backpropagation (BP) is essential for
state-of-the-art performance is challenged by this work. We present rigorous,
hardware-validated evidence that the Mono-Forward (MF) algorithm, a
backpropagation-free method, consistently surpasses an optimally tuned BP
baseline in classification accuracy on its native Multi-Layer Perceptron (MLP)
architectures. This superior generalization is achieved with profound
efficiency gains, including up to 41% less energy consumption and up to 34%
faster training. Our analysis, which charts an evolutionary path from Geoffrey
Hinton's Forward-Forward (FF) to the Cascaded Forward (CaFo) and finally to MF,
is grounded in a fair comparative framework using identical architectures and
universal hyperparameter optimization. We further provide a critical
re-evaluation of memory efficiency in BP-free methods, empirically
demonstrating that practical overhead can offset theoretical gains. Ultimately,
this work establishes MF as a practical, high-performance, and sustainable
alternative to BP for MLPs.

</details>


### [310] [Happiness as a Measure of Fairness](https://arxiv.org/abs/2511.01069)
*Georg Pichler,Marco Romanelli,Pablo Piantanida*

Main category: cs.LG

TL;DR: Proposes happiness-based fairness, a human-centered utility-based framework; optimal fair post-processing via a linear program; unifies existing fairness definitions; empirically effective.


<details>
  <summary>Details</summary>
Motivation: Fairness in decision outcomes is human-centered but requires formal, scalable optimization. Existing definitions are fragmented; a framework capturing group welfare with tractable computation is desirable.

Method: Define happiness as the utility gained by each group from outcomes; formulate post-processing as a linear program to compute the optimal fair strategy; show that the framework subsumes several fairness definitions; validate with experiments across diverse scenarios.

Result: The linear program yields the optimal fair post-processing; the approach is efficient and scalable with standard solvers; empirical results demonstrate practical strengths and support unification of fairness notions.

Conclusion: Happiness-based fairness provides a rigorous, scalable, and human-centric framework that unifies and extends fairness definitions and is validated empirically.

Abstract: In this paper, we propose a novel fairness framework grounded in the concept
of happi- ness, a measure of the utility each group gains fromdecisionoutcomes.
Bycapturingfairness through this intuitive lens, we not only offer a more
human-centered approach, but also one that is mathematically rigorous: In order
to compute the optimal, fair post-processing strategy, only a linear program
needs to be solved. This makes our method both efficient and scalable with
existing optimization tools. Furthermore, it unifies and extends several
well-known fairness definitions, and our em- pirical results highlight its
practical strengths across diverse scenarios.

</details>


### [311] [AI Progress Should Be Measured by Capability-Per-Resource, Not Scale Alone: A Framework for Gradient-Guided Resource Allocation in LLMs](https://arxiv.org/abs/2511.01077)
*David McCoy,Yulun Wu,Zachary Butzin-Dozier*

Main category: cs.LG

TL;DR: A position paper argues to shift AI scaling from pure capacity growth to efficiency per resource, using gradient-influence analysis in transformers to identify high-impact parameters and data, achieving orders-of-magnitude resource reductions via high-influence parameter updates, gradient norms as proxies, and coordinated parameter-data selection; proposes a two-stage paradigm: marginal-return pretraining and influence-guided adaptation with gradient blueprints to democratize access while reducing environmental impact.


<details>
  <summary>Details</summary>
Motivation: Critique of scaling fundamentalism and its environmental and equity costs; advocate a resource-aware reorientation of LLM development toward capability-per-resource; provide a theory and actionable framework to optimize AI progress with less resource use.

Method: Theoretical framework analyzing transformer weight distributions as heavy-tailed; identify high-influence parameters via gradient influence patterns; show updating only these parameters beats full-parameter tuning on efficiency; propose gradient norms as proxies; advocate coordinated parameter and data selection; propose two-stage paradigm and gradient blueprints to bridge pretraining and downstream adaptation.

Result: The work presents three core insights: (1) high-influence parameter updates outperform full tuning in performance-per-resource; (2) gradient norms serve as efficient proxies to locate high-influence components; (3) coordinated parameter and data selection yields multiplicative efficiency gains, potentially reducing resource needs by orders of magnitude. It couples these findings into a two-stage paradigm and gradient blueprint mechanism to operationalize resource-per-resource development.

Conclusion: Embedding resource-consciousness into AI development reframes progress toward sustainability and equity. The proposed marginal-return pretraining and influence-guided adaptation, via gradient blueprints, offers a practical path to democratize access to capabilities while reducing environmental impact.

Abstract: This position paper challenges the "scaling fundamentalism" dominating AI
research, where unbounded growth in model size and computation has led to
unsustainable environmental impacts and widening resource inequality. We argue
that LLM development should be fundamentally reoriented toward
capability-per-resource rather than capability alone. We present a theoretical
framework demonstrating that resource-allocation decisions guided by gradient
influence patterns can dramatically improve efficiency throughout the AI
lifecycle. Our analysis shows that in transformer-based models, where a small
fraction of parameters exert outsized influence (following heavy-tailed
distributions), three critical insights emerge: (1) updating only
high-influence parameters strictly outperforms full-parameter tuning on a
performance-per-resource basis; (2) simple gradient norms provide
computationally efficient proxies for identifying these high-influence
components; and (3) coordinated parameter and data selection yields
multiplicative efficiency gains, potentially reducing resource requirements by
orders of magnitude. Building on these theoretical foundations, we propose a
two stage paradigm marginal-return pretraining for foundation developers and
influence guided adaptation for downstream users bridged by gradient
blueprints, metadata describing which parameters matter most for various tasks.
This capability-per-resource perspective transforms what were once considered
pragmatic hardware workarounds into theoretically optimal strategies,
democratizing access to cutting-edge AI capabilities while significantly
reducing environmental impact. By embedding resource consciousness into how we
develop, adapt, and evaluate models, we can reshape AI progress toward a more
sustainable and equitable future.

</details>


### [312] [Continual Learning, Not Training: Online Adaptation For Agents](https://arxiv.org/abs/2511.01093)
*Aman Jaglan,Jarrod Barnes*

Main category: cs.LG

TL;DR: ATLAS introduces a dual-agent system (Teacher/Student) with a persistent memory to guide inference-time orchestration, enabling gradient-free continual learning and reduced compute.


<details>
  <summary>Details</summary>
Motivation: Mitigate catastrophic forgetting in deployed agents and enable real-time adaptation without gradient updates; shift from parameter updates to system-level orchestration.

Method: A two-agent architecture with memory distillation, an orchestration layer controlling supervision level and plan selection at inference, evaluated on ExCyTIn-Bench; demonstrates gradient-free continual learning and world-model traces.

Result: Achieves 54.1% success with GPT-5-mini Student, 13% better than GPT-5 (High); cost reduced by 86%; cross-incident transfer improves accuracy from 28% to 41% without retraining.

Conclusion: Gradient-free continual learning is a viable path for deployable, adaptive AI; system-level orchestration can replace parameter updates and provide generalization via stored guidance.

Abstract: Continual Learning (CL) methods have traditionally focused on mitigating
catastrophic forgetting through gradient-based retraining, an approach
ill-suited for deployed agents that must adapt in real time. We introduce our
Adaptive Teaching and Learning System (ATLAS), a dual-agent architecture that
decouples reasoning (Teacher) from execution (Student) and incorporates a
persistent learning memory that stores distilled guidance from experience. This
informs the orchestration layer, enabling the system to dynamically adjust its
operational strategies, such as supervision level or initial plan selection, at
inference time. In doing so, ATLAS achieves gradient-free continual learning,
shifting the locus of adaptation from model parameters to system-level
orchestration. We formulate this as a system-centric paradigm for continual
learning, where the objective is adaptive efficiency: maximizing task success
while minimizing computational cost through inference-time orchestration rather
than parameter updates. Evaluated on Microsoft's ExCyTIn-Bench, an open-source
benchmark simulating complex cyberthreat investigation, ATLAS achieves 54.1%
success with GPT-5-mini as its Student, outperforming the larger GPT-5 (High)
by 13% while reducing cost by 86%. Cross-incident validation demonstrates
generalization: frozen pamphlets from Incident #5 improve accuracy from 28% to
41% with zero retraining, while shifting output composition from verbose
exploration to structured reasoning. Together, these findings establish
gradient-free continual learning as a viable path toward adaptive, deployable
AI systems and provide causally annotated traces valuable for training explicit
world models.

</details>


### [313] [One model to solve them all: 2BSDE families via neural operators](https://arxiv.org/abs/2511.01125)
*Takashi Furuya,Anastasis Kratsios,Dylan PossamaÃ¯,Bogdan RaoniÄ*

Main category: cs.LG

TL;DR: Mild generative neural operator using KolmogorovâArnol'd networks to solve families of 2BSDEs; proves neural operators approximate solution operators; identifies a structured subclass where parameter count scales polynomially with 1/epsilon rather than exponentially.


<details>
  <summary>Details</summary>
Motivation: Address solving infinite families of second-order backward stochastic differential equations (2BSDEs) on bounded domains with random terminal time, and improve neural-operator efficiency guarantees beyond worst-case exponential complexity.

Method: Introduce a mild generative variant of neural operators, leveraging KolmogorovâArnol'd networks to represent solution operators for 2BSDE families; prove universal approximation for broad 2BSDE families; isolate a structured subclass with favorable (polynomial) parameter complexity in the reciprocal approximation rate.

Result: The solution operator for a broad class of 2BSDE families is approximable by the proposed neural-operator models; within a structured subclass, neural-operator approximation requires only a polynomial number of parameters in 1/epsilon, contrasting general exponential guarantees.

Conclusion: Neural-operator methods augmented with KolmogorovâArnol'd networks effectively approximate solution operators for 2BSDEs on bounded domains with random terminal time, with potential practical efficiency gains for structured families due to polynomial complexity.

Abstract: We introduce a mild generative variant of the classical neural operator
model, which leverages Kolmogorov--Arnold networks to solve infinite families
of second-order backward stochastic differential equations ($2$BSDEs) on
regular bounded Euclidean domains with random terminal time. Our first main
result shows that the solution operator associated with a broad range of
$2$BSDE families is approximable by appropriate neural operator models. We then
identify a structured subclass of (infinite) families of $2$BSDEs whose neural
operator approximation requires only a polynomial number of parameters in the
reciprocal approximation rate, as opposed to the exponential requirement in
general worst-case neural operator guarantees.

</details>


### [314] [Stochastic Regret Guarantees for Online Zeroth- and First-Order Bilevel Optimization](https://arxiv.org/abs/2511.01126)
*Parvin Nazari,Bojian Hou,Davoud Ataee Tarzanagh,Li Shen,George Michailidis*

Main category: cs.LG

TL;DR: A new search direction enables online bilevel optimization with sublinear stochastic bilevel regret without window smoothing, using first- and zeroth-order algorithms; it also improves efficiency via reduced oracle cost and joint updates, demonstrated on online parametric loss tuning and black-box adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: In online settings where both outer and inner objectives change over time, existing window-smoothed regret measures may misjudge performance when functions evolve rapidly. There is a need for regret guarantees without window smoothing and for more efficient hypergradient estimation.

Method: Proposes a novel search direction for OBO and develops both first- and zeroth-order stochastic OBO algorithms that leverage this direction. The framework reduces oracle dependence in hypergradient estimation, updates inner and outer variables in tandem with solving the linear system, and uses zeroth-order estimates for Hessians, Jacobians, and gradients.

Result: The approach achieves sublinear stochastic bilevel regret without window smoothing. It also offers efficiency gains in hypergradient estimation and update steps, validated by experiments on online parametric loss tuning and black-box adversarial attacks.

Conclusion: The proposed search direction and accompanying OBO algorithms improve theoretical guarantees (sublinear regret without window smoothing) and practical efficiency for non-stationary bilevel problems, supported by empirical validation.

Abstract: Online bilevel optimization (OBO) is a powerful framework for machine
learning problems where both outer and inner objectives evolve over time,
requiring dynamic updates. Current OBO approaches rely on deterministic
\textit{window-smoothed} regret minimization, which may not accurately reflect
system performance when functions change rapidly. In this work, we introduce a
novel search direction and show that both first- and zeroth-order (ZO)
stochastic OBO algorithms leveraging this direction achieve sublinear
{stochastic bilevel regret without window smoothing}. Beyond these guarantees,
our framework enhances efficiency by: (i) reducing oracle dependence in
hypergradient estimation, (ii) updating inner and outer variables alongside the
linear system solution, and (iii) employing ZO-based estimation of Hessians,
Jacobians, and gradients. Experiments on online parametric loss tuning and
black-box adversarial attacks validate our approach.

</details>


### [315] [Regularization Implies balancedness in the deep linear network](https://arxiv.org/abs/2511.01137)
*Kathryn Lindsey,Govind Menon*

Main category: cs.LG

TL;DR: GIT-based analysis of deep linear nets shows L2 regularization minimizes on the balanced manifold, enabling a two-flow training dynamics and exact solvability of the regularizing flow via the moment map, linking balancedness to model reduction and Bayesian ideas.


<details>
  <summary>Details</summary>
Motivation: to understand and formalize balancedness in deep linear networks using geometric invariant theory and to connect it with linear systems theory and Bayesian interpretations.

Method: apply Kempf-Ness theorem from GIT to show the L2 regularizer is minimized on the balanced manifold; decompose training into a regularizing flow on fibers and a learning flow on the balanced manifold; obtain exact solvability for the regularizing flow via the moment map.

Result: provides a unifying mathematical framework for balancedness in deep learning and linear systems; demonstrates that balancedness can be interpreted through model reduction and Bayesian principles.

Conclusion: offers a common framework and interpretative lens for balancedness, potentially guiding design and analysis of DL networks and related linear systems.

Abstract: We use geometric invariant theory (GIT) to study the deep linear network
(DLN). The Kempf-Ness theorem is used to establish that the $L^2$ regularizer
is minimized on the balanced manifold. This allows us to decompose the training
dynamics into two distinct gradient flows: a regularizing flow on fibers and a
learning flow on the balanced manifold. We show that the regularizing flow is
exactly solvable using the moment map.
  This approach provides a common mathematical framework for balancedness in
deep learning and linear systems theory. We use this framework to interpret
balancedness in terms of model reduction and Bayesian principles.

</details>


### [316] [Adapt under Attack and Domain Shift: Unified Adversarial Meta-Learning and Domain Adaptation for Robust Automatic Modulation Classification](https://arxiv.org/abs/2511.01172)
*Ali Owfi,Amirmohammad Bamdad,Tolunay Seyfi,Fatemeh Afghah*

Main category: cs.LG

TL;DR: A unified meta-learning and domain-adaptation framework for Automatic Modulation Classification that is robust to both adversarial attacks and environmental distribution shifts; it trains offline with clean and adversarial samples from a single source domain and adapts online to a new target domain, yielding improved accuracy under combined threats.


<details>
  <summary>Details</summary>
Motivation: AMC models are vulnerable to adversarial perturbations and non-stationary data distributions in real-world environments, which hampers deployment.

Method: Two-phase approach: (1) offline meta-learning on clean and adversarial samples from one source domain to learn defenses that generalize to unseen attacks, and (2) online domain adaptation to align features to a new target domain with limited labeled data, enabling adaptation without extensive labeling.

Result: The framework achieves significant improvement in modulation classification accuracy under the combined threats of adversarial attacks and distribution shifts.

Conclusion: This unified framework provides a practical solution for deploying robust AMC systems, addressing both adversarial robustness and environmental adaptation in real-world settings.

Abstract: Deep learning has emerged as a leading approach for Automatic Modulation
Classification (AMC), demonstrating superior performance over traditional
methods. However, vulnerability to adversarial attacks and susceptibility to
data distribution shifts hinder their practical deployment in real-world,
dynamic environments. To address these threats, we propose a novel, unified
framework that integrates meta-learning with domain adaptation, making AMC
systems resistant to both adversarial attacks and environmental changes. Our
framework utilizes a two-phase strategy. First, in an offline phase, we employ
a meta-learning approach to train the model on clean and adversarially
perturbed samples from a single source domain. This method enables the model to
generalize its defense, making it resistant to a combination of previously
unseen attacks. Subsequently, in the online phase, we apply domain adaptation
to align the model's features with a new target domain, allowing it to adapt
without requiring substantial labeled data. As a result, our framework achieves
a significant improvement in modulation classification accuracy against these
combined threats, offering a critical solution to the deployment and
operational challenges of modern AMC systems.

</details>


### [317] [A Comparative Study of Model Adaptation Strategies for Multi-Treatment Uplift Modeling](https://arxiv.org/abs/2511.01185)
*Ruyue Zhang,Xiaopeng Ke,Ming Liu,Fangzhou Shi,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: Orthogonal Function Adaptation (OFA) yields superior and robust uplift estimates in multi-treatment settings, outperforming existing Structure and Feature adaptation methods.


<details>
  <summary>Details</summary>
Motivation: Uplift modeling for individualized treatment effects is crucial in marketing and healthcare. In multi-treatment settings, current adaptations (Structure and Feature) struggle with noise and observational data, prompting a need for a principled, robust approach.

Method: Introduce Orthogonal Function Adaptation (OFA) grounded in the function approximation theorem. Classify existing model adaptations into Structure Adaptation and Feature Adaptation, and develop an orthogonalization-based method to improve uplift estimation. Conduct comprehensive experiments across varied data characteristics to assess effectiveness and robustness.

Result: OFA significantly improves uplift model performance compared to vanilla adaptation methods and shows the highest robustness across noisy data and observational data mixtures.

Conclusion: OFA provides a robust, effective framework for multi-treatment uplift modeling, offering improved estimation accuracy and resilience against data quality issues.

Abstract: Uplift modeling has emerged as a crucial technique for individualized
treatment effect estimation, particularly in fields such as marketing and
healthcare. Modeling uplift effects in multi-treatment scenarios plays a key
role in real-world applications. Current techniques for modeling
multi-treatment uplift are typically adapted from binary-treatment works. In
this paper, we investigate and categorize all current model adaptations into
two types: Structure Adaptation and Feature Adaptation. Through our empirical
experiments, we find that these two adaptation types cannot maintain
effectiveness under various data characteristics (noisy data, mixed with
observational data, etc.). To enhance estimation ability and robustness, we
propose Orthogonal Function Adaptation (OFA) based on the function
approximation theorem. We conduct comprehensive experiments with multiple data
characteristics to study the effectiveness and robustness of all model
adaptation techniques. Our experimental results demonstrate that our proposed
OFA can significantly improve uplift model performance compared to other
vanilla adaptation methods and exhibits the highest robustness.

</details>


### [318] [Analyzing the Power of Chain of Thought through Memorization Capabilities](https://arxiv.org/abs/2511.01190)
*Lijia Yu,Xiao-Shan Gao,Lijun Zhang*

Main category: cs.LG

TL;DR: CoT does not universally improve transformer reasoning; memorization governs capability, with linear (N) parameter scaling for finite datasets, and some tasks/infinite datasets defy memorization.


<details>
  <summary>Details</summary>
Motivation: To determine whether chain-of-thought prompts extend transformers' reasoning across all tasks by examining the memorization capabilities of CoT-enabled transformers.

Method: Theoretical analysis of fixed-precision transformers with and without CoT, deriving necessary and sufficient memorization conditions for finite datasets, establishing lower/upper bounds on parameters (linear in N) for memorization, and exploring memorization for infinite datasets.

Result: Identifies that the two memorization conditions (with vs without CoT) are not equivalent; CoT does not universally enhance reasoning power; provides linear bounds Î(N) for memorization with or without CoT on finite datasets; extends preliminary results to some infinite datasets showing non-memorizability.

Conclusion: CoT is not a universal fix for reasoning capabilities; the power of CoT is bounded by memorization limits, and even with CoT, some finite and infinite datasets cannot be memorized by fixed-precision transformers; this frames the limits of CoT in reasoning tasks.

Abstract: It has been shown that the chain of thought (CoT) can enhance the power of
large language models (LLMs) to solve certain mathematical reasoning problems.
However, the capacity of CoT is still not fully explored. As an important
instance, the following basic question has not yet been answered: Does CoT
expand the capability of transformers across all reasoning tasks? We
demonstrate that reasoning with transformers is essentially a memorization
problem for reasoning datasets. Thus, examining the power of CoT across all
reasoning tasks amounts to analyzing the memorization capabilities of CoT
transformers. In this paper, we give a complete description of the memorization
capabilities of fixed-precision transformers with or without CoT and give a
negative answer to the above-mentioned question. Precisely, we first give
necessary and sufficient conditions for fixed-precision transformers with and
without CoT to memorize a finite reasoning dataset and show that these two
conditions do not imply each other. Then, we give lower and upper bounds for
the number of parameters needed for transformers with or without CoT to
memorize a finite reasoning dataset with $N$ elements, which are
$\overline{\Theta}(N)$ in all cases. This implies that there exist reasoning
tasks for which CoT does not enhance the reasoning power of transformers,
leading to a negative answer to the above-mentioned question. Finally, we give
the first results on memorizing infinite reasoning datasets by CoT transformers
and show that some simple infinite datasets cannot be memorized by transformers
with or without CoT.

</details>


### [319] [Transmitter Identification and Protocol Categorization in Shared Spectrum via Multi-Task RF Classification at the Network Edge](https://arxiv.org/abs/2511.01198)
*Tariq Abdul-Quddoos,Tasnia Sharmin,Xiangfang Li,Lijun Qian*

Main category: cs.LG

TL;DR: A multi-task CNN-based RF signal classifier for protocol and base-station identification in shared spectrum, achieving high accuracy on POWDER data.


<details>
  <summary>Details</summary>
Motivation: With rising wireless demand, spectrum monitoring and transmitter identification are essential for enforcing spectrum usage, efficient utilization, and security in shared bands.

Method: A multi-channel input Convolutional Neural Network designed to perform multi-task RF signal classification: (1) protocol categorization (e.g., 4G LTE, 5G-NR, IEEE 802.11a), (2) transmitter base-station identification, and (3) joint classification tasks; trained/validated on RF data from the POWDER platform.

Result: High accuracies were achieved: 90% for protocol classification, 100% for base-station identification, and 92% for joint classification.

Conclusion: The approach demonstrates strong potential to enhance spectrum monitoring, management, and security in modern wireless networks, while indicating robustness to overlapping signals and environmental variability; future work could address generalization, real-time deployment, and broader protocol coverage.

Abstract: As spectrum sharing becomes increasingly vital to meet rising wireless
demands in the future, spectrum monitoring and transmitter identification are
indispensable for enforcing spectrum usage policy, efficient spectrum
utilization, and net- work security. This study proposed a robust framework for
transmitter identification and protocol categorization via multi- task RF
signal classification in shared spectrum environments, where the spectrum
monitor will classify transmission protocols (e.g., 4G LTE, 5G-NR, IEEE
802.11a) operating within the same frequency bands, and identify different
transmitting base stations, as well as their combinations. A Convolutional
Neural Network (CNN) is designed to tackle critical challenges such as
overlapping signal characteristics and environmental variability. The proposed
method employs a multi-channel input strategy to extract meaningful signal
features, achieving remarkable accuracy: 90% for protocol classification, 100%
for transmitting base station classification, and 92% for joint classification
tasks, utilizing RF data from the POWDER platform. These results highlight the
significant potential of the proposed method to enhance spectrum monitoring,
management, and security in modern wireless networks.

</details>


### [320] [FEval-TTC: Fair Evaluation Protocol for Test-Time Compute](https://arxiv.org/abs/2511.01203)
*Pavel Rumiantsev,Soumyasundar Pal,Yingxue Zhang,Mark Coates*

Main category: cs.LG

TL;DR: FEval-TTC is a protocol and open-source toolkit to fairly evaluate test-time compute (TTC) for LLMs, normalizing few-shot prompts and answer extraction, evaluating CoT-based TTC across multiple models and datasets, with a cost model for token and dollar costs.


<details>
  <summary>Details</summary>
Motivation: LLM performance and API costs fluctuate over time, which can invalidate conclusions from prior studies. A stable, fair evaluation protocol for test-time compute is needed, especially for CoT-based TTC methods.

Method: Introduce FEval-TTC protocol that supports evaluations across multiple LLMs on diverse math and commonsense datasets. Standardizes few-shot prompting and answer extraction to reduce overhead. Includes a cost modeling procedure to estimate token and dollar costs per query. Open-sources FEval-TTC for public use.

Result: Provides a framework and tooling for consistent, fair TTC evaluation and cost accounting across LLMs and datasets. Reduces time and monetary overhead and enables equitable comparisons of TTC methods.

Conclusion: FEval-TTC offers a standardized, open-source approach for fair evaluation of TTC methods, addressing fluctuations in model performance and costs.

Abstract: The performance of Large Language Models (LLMs) and the associated dollar
costs of API calls can fluctuate over time, potentially invalidating
conclusions drawn in prior research. To address this, we propose a Fair
Evaluation protocol for Test-Time Compute (FEval-TTC), designed to ensure
consistent assessment of test-time compute (TTC) methods, regardless of such
fluctuations. FEval-TTC focuses on the evaluation of TTC methods that utilize
underlying Chains-of-Thought (CoT). It supports evaluations across multiple
LLMs on a diverse set of mathematical and commonsense reasoning datasets. The
few-shot prompting and answer extraction processes are standardized across
datasets, reducing both time and monetary overhead for researchers.
Furthermore, we provide a cost modelling procedure that estimates both the
token and dollar cost per query, facilitating equitable comparisons of
prevalent TTC methods. We open-source FEval-TTC for public use at
https://github.com/networkslab/feval_ttc .

</details>


### [321] [Optimizing Electric Vehicle Charging Station Placement Using Reinforcement Learning and Agent-Based Simulations](https://arxiv.org/abs/2511.01218)
*Minh-Duc Nguyen,Dung D. Le,Phi Long Nguyen*

Main category: cs.LG

TL;DR: Hybrid deep RL with agent-based simulation optimizes EV charging station placement and port configuration; uses dual Q-networks and a hybrid reward; shows 53.28% waiting-time reduction in Hanoi, outperforming static baselines.


<details>
  <summary>Details</summary>
Motivation: Deterministic rewards in RL fail to capture dynamic, uncertain real-world charging demand; existing methods lack efficiency and realistic modeling of EV movement and demand; need real-time demand estimation and simulation-informed feedback for infrastructure planning.

Method: Integrates deep reinforcement learning with agent-based simulations of EV movement to estimate charging demand in real time. A hybrid RL agent uses two Q-networks (one for location selection, one for port configuration) and a hybrid reward combining deterministic factors with simulation-derived feedback to guide decisions.

Result: Case study in Hanoi shows a 53.28% reduction in average waiting times from the initial state, outperforming static baseline methods.

Conclusion: The framework provides a scalable, adaptive approach to EV infrastructure planning that better captures real-world dynamics, improving user experience and enabling more efficient charging networks.

Abstract: The rapid growth of electric vehicles (EVs) necessitates the strategic
placement of charging stations to optimize resource utilization and minimize
user inconvenience. Reinforcement learning (RL) offers an innovative approach
to identifying optimal charging station locations; however, existing methods
face challenges due to their deterministic reward systems, which limit
efficiency. Because real-world conditions are dynamic and uncertain, a
deterministic reward structure cannot fully capture the complexities of
charging station placement. As a result, evaluation becomes costly and
time-consuming, and less reflective of real-world scenarios. To address this
challenge, we propose a novel framework that integrates deep RL with
agent-based simulations to model EV movement and estimate charging demand in
real time. Our approach employs a hybrid RL agent with dual Q-networks to
select optimal locations and configure charging ports, guided by a hybrid
reward function that combines deterministic factors with simulation-derived
feedback. Case studies in Hanoi, Vietnam, show that our method reduces average
waiting times by 53.28% compared to the initial state, outperforming static
baseline methods. This scalable and adaptive solution enhances EV
infrastructure planning, effectively addressing real-world complexities and
improving user experience.

</details>


### [322] [WindMiL: Equivariant Graph Learning for Wind Loading Prediction](https://arxiv.org/abs/2511.01226)
*Themistoklis Vargiemezis,Charilaos Kanatsoulis,Catherine GorlÃ©*

Main category: cs.LG

TL;DR: WindMiL combines a large LES-derived wind-load dataset with a symmetry-aware, reflection-equivariant Graph Neural Network to predict wind pressures on buildings. It delivers high accuracy for mean and variability (e.g., RMSE â¤ 0.02 for mean Cp) and robust performance under mirrored geometries (reflected-test hit rate > 96%), enabling scalable parametric studies.


<details>
  <summary>Details</summary>
Motivation: Accurate wind loading is critical for safety and sustainable design, but traditional methods (wind tunnel, LES) are computationally expensive, hindering large-scale parametric exploration.

Method: 1) Dataset: generate a large wind-load dataset for low-rise buildings by interpolating roof geometries with signed distance functions and running 462 LES simulations across varying shapes and wind directions. 2) Model: develop a reflection-equivariant Graph Neural Network that guarantees physical consistency under mirrored geometries; evaluate on interpolation and extrapolation tasks.

Result: Mean Cp RMSE â¤ 0.02; accurate prediction of the mean and standard deviation of surface pressure coefficients; reflected-test accuracy maintains hit rates above 96%, while a non-equivariant baseline loses more than 10% in this regime.

Conclusion: Pairing a systematic, LES-derived dataset with a symmetry-aware surrogate yields efficient, scalable, and physically consistent wind-load predictions suitable for large-scale parametric studies and design optimization.

Abstract: Accurate prediction of wind loading on buildings is crucial for structural
safety and sustainable design, yet conventional approaches such as wind tunnel
testing and large-eddy simulation (LES) are prohibitively expensive for
large-scale exploration. Each LES case typically requires at least 24 hours of
computation, making comprehensive parametric studies infeasible. We introduce
WindMiL, a new machine learning framework that combines systematic dataset
generation with symmetry-aware graph neural networks (GNNs). First, we
introduce a large-scale dataset of wind loads on low-rise buildings by applying
signed distance function interpolation to roof geometries and simulating 462
cases with LES across varying shapes and wind directions. Second, we develop a
reflection-equivariant GNN that guarantees physically consistent predictions
under mirrored geometries. Across interpolation and extrapolation evaluations,
WindMiL achieves high accuracy for both the mean and the standard deviation of
surface pressure coefficients (e.g., RMSE $\leq 0.02$ for mean $C_p$) and
remains accurate under reflected-test evaluation, maintaining hit rates above
$96\%$ where the non-equivariant baseline model drops by more than $10\%$. By
pairing a systematic dataset with an equivariant surrogate, WindMiL enables
efficient, scalable, and accurate predictions of wind loads on buildings.

</details>


### [323] [A Saddle Point Remedy: Power of Variable Elimination in Non-convex Optimization](https://arxiv.org/abs/2511.01234)
*Min Gan,Guang-Yong Chen,Yang Yi,Lin Yang*

Main category: cs.LG

TL;DR: Variable elimination (VarPro) reshapes optimization landscapes by transforming saddle points into local maxima in the reduced problem, explaining robust convergence in non-convex ML optimization. The work provides a geometric explanation via Hessian inertia and Schur complement, with empirical validation on matrix factorization, two-parameter nets, and deep ResNets, and posits landscape simplification as a design principle for robust optimization algorithms.


<details>
  <summary>Details</summary>
Motivation: To understand why variable elimination methods like VarPro exhibit superior convergence and robustness in large-scale non-convex optimization relevant to machine learning, where saddle points proliferate.

Method: Rigorous geometric analysis comparing original and reduced formulations using Hessian inertia and the Schur complement to characterize critical points; mapping of how maxima in reduced space correspond to saddles in the original; empirical illustrations on non-convex matrix factorization, visualization on simple neural nets, and validation in training deep residual networks.

Result: Proves that variable elimination reshapes the critical point structure so that local maxima in the reduced landscape are created from saddle points in the original formulation; demonstrates dramatic improvements in stability and convergence in deep networks; establishes a principle of landscape simplification via saddle point transformation.

Conclusion: Landscape simplification through saddle point transformation is a powerful principle that can guide the design of a new generation of more robust and efficient optimization algorithms.

Abstract: The proliferation of saddle points, rather than poor local minima, is
increasingly understood to be a primary obstacle in large-scale non-convex
optimization for machine learning. Variable elimination algorithms, like
Variable Projection (VarPro), have long been observed to exhibit superior
convergence and robustness in practice, yet a principled understanding of why
they so effectively navigate these complex energy landscapes has remained
elusive. In this work, we provide a rigorous geometric explanation by comparing
the optimization landscapes of the original and reduced formulations. Through a
rigorous analysis based on Hessian inertia and the Schur complement, we prove
that variable elimination fundamentally reshapes the critical point structure
of the objective function, revealing that local maxima in the reduced landscape
are created from, and correspond directly to, saddle points in the original
formulation. Our findings are illustrated on the canonical problem of
non-convex matrix factorization, visualized directly on two-parameter neural
networks, and finally validated in training deep Residual Networks, where our
approach yields dramatic improvements in stability and convergence to superior
minima. This work goes beyond explaining an existing method; it establishes
landscape simplification via saddle point transformation as a powerful
principle that can guide the design of a new generation of more robust and
efficient optimization algorithms.

</details>


### [324] [KAT-GNN: A Knowledge-Augmented Temporal Graph Neural Network for Risk Prediction in Electronic Health Records](https://arxiv.org/abs/2511.01249)
*Kun-Wei Lin,Yu-Chen Kuo,Hsin-Yao Wang,Yi-Ju Tseng*

Main category: cs.LG

TL;DR: A knowledge-augmented temporal graph neural network (KAT-GNN) for EHR risk prediction that combines ontology-driven SNOMED edges and data-driven co-occurrence priors with a time-aware transformer, achieving state-of-the-art CAD prediction and strong mortality results across MIMIC-III/IV; ablations show both knowledge augmentation and temporal modeling are beneficial.


<details>
  <summary>Details</summary>
Motivation: Modeling heterogeneous, irregular temporal EHR data is challenging; leveraging structured clinical knowledge and temporal dynamics could improve predictive performance across diverse tasks and datasets.

Method: Construct modality-specific patient graphs from EHRs. Augment graphs with (1) ontology-derived edges from SNOMED CT and (2) co-occurrence priors learned from EHR data. Use a time-aware transformer to capture longitudinal dynamics on graph-encoded patient representations.

Result: CAD prediction on CGRD with AUROC 0.9269Â±0.0029. In-hospital mortality prediction on MIMIC-III AUROC 0.9230Â±0.0070; on MIMIC-IV AUROC 0.8849Â±0.0089. Outperforms baselines such as GRASP and RETAIN. Ablation studies show both augmentation and temporal modeling contribute significantly to performance.

Conclusion: Integrating clinical knowledge into graph representations, along with a time-aware attention mechanism, provides an effective and generalizable approach for risk prediction across diverse clinical tasks and datasets.

Abstract: Clinical risk prediction using electronic health records (EHRs) is vital to
facilitate timely interventions and clinical decision support. However,
modeling heterogeneous and irregular temporal EHR data presents significant
challenges. We propose \textbf{KAT-GNN} (Knowledge-Augmented Temporal Graph
Neural Network), a graph-based framework that integrates clinical knowledge and
temporal dynamics for risk prediction. KAT-GNN first constructs
modality-specific patient graphs from EHRs. These graphs are then augmented
using two knowledge sources: (1) ontology-driven edges derived from SNOMED CT
and (2) co-occurrence priors extracted from EHRs. Subsequently, a time-aware
transformer is employed to capture longitudinal dynamics from the graph-encoded
patient representations. KAT-GNN is evaluated on three distinct datasets and
tasks: coronary artery disease (CAD) prediction using the Chang Gung Research
Database (CGRD) and in-hospital mortality prediction using the MIMIC-III and
MIMIC-IV datasets. KAT-GNN achieves state-of-the-art performance in CAD
prediction (AUROC: 0.9269 $\pm$ 0.0029) and demonstrated strong results in
mortality prediction in MIMIC-III (AUROC: 0.9230 $\pm$ 0.0070) and MIMIC-IV
(AUROC: 0.8849 $\pm$ 0.0089), consistently outperforming established baselines
such as GRASP and RETAIN. Ablation studies confirm that both knowledge-based
augmentation and the temporal modeling component are significant contributors
to performance gains. These findings demonstrate that the integration of
clinical knowledge into graph representations, coupled with a time-aware
attention mechanism, provides an effective and generalizable approach for risk
prediction across diverse clinical tasks and datasets.

</details>


### [325] [A Spatio-Temporal Online Robust Tensor Recovery Approach for Streaming Traffic Data Imputation](https://arxiv.org/abs/2511.01267)
*Yiyang Yang,Xiejian Chi,Shanxing Gao,Kaidong Wang,Yao Wang*

Main category: cs.LG

TL;DR: An online robust tensor recovery method for streaming traffic data that exploits global spatio-temporal correlations and local consistency, handling missing and anomalous values; achieves high recovery accuracy with up to 1000x faster performance than batch methods, validated on three real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Data quality in Intelligent Transportation Systems is critical for reliable traffic control and management. Batch-based low-rank tensor methods are computationally expensive and poorly scalable to growing data streams, while existing online methods underutilize structural properties of traffic data. There is a need for a scalable, accurate online approach that can handle missing data and outliers.

Method: An online robust tensor recovery algorithm embedded in a streaming framework that simultaneously leverages global spatio-temporal correlations and local data consistency. The method handles missing values and anomalies, adapts to diverse missing patterns, and updates incrementally to maintain scalability for large-scale traffic data.

Result: Experiments on three real-world traffic datasets show high recovery accuracy and substantially improved computational efficiency, up to three orders of magnitude faster than state-of-the-art batch-based methods, demonstrating scalability and effectiveness for traffic data quality enhancement in ITS.

Conclusion: The proposed online robust tensor recovery approach is a scalable and effective solution for ITS traffic data quality improvement, capable of handling missing and anomalous observations while exploiting both global and local structural properties of traffic data.

Abstract: Data quality is critical to Intelligent Transportation Systems (ITS), as
complete and accurate traffic data underpin reliable decision-making in traffic
control and management. Recent advances in low-rank tensor recovery algorithms
have shown strong potential in capturing the inherent structure of
high-dimensional traffic data and restoring degraded observations. However,
traditional batch-based methods demand substantial computational and storage
resources, which limits their scalability in the face of continuously expanding
traffic data volumes. Moreover, recent online tensor recovery methods often
suffer from severe performance degradation in complex real-world scenarios due
to their insufficient exploitation of the intrinsic structural properties of
traffic data. To address these challenges, we reformulate the traffic data
recovery problem within a streaming framework, and propose a novel online
robust tensor recovery algorithm that simultaneously leverages both the global
spatio-temporal correlations and local consistency of traffic data, achieving
high recovery accuracy and significantly improved computational efficiency in
large-scale scenarios. Our method is capable of simultaneously handling missing
and anomalous values in traffic data, and demonstrates strong adaptability
across diverse missing patterns. Experimental results on three real-world
traffic datasets demonstrate that the proposed approach achieves high recovery
accuracy while significantly improving computational efficiency by up to three
orders of magnitude compared to state-of-the-art batch-based methods. These
findings highlight the potential of the proposed approach as a scalable and
effective solution for traffic data quality enhancement in ITS.

</details>


### [326] [Adversarial Spatio-Temporal Attention Networks for Epileptic Seizure Forecasting](https://arxiv.org/abs/2511.01275)
*Zan Li,Kyongmin Yeo,Wesley Gifford,Lara Marcuse,Madeline Fields,BÃ¼lent Yener*

Main category: cs.LG

TL;DR: STAN is an adversarial spatio-temporal attention network for forecasting epileptic seizures from multivariate EEG, achieving state-of-the-art accuracy and enabling early, subject-specific detection with efficient edge deployment.


<details>
  <summary>Details</summary>
Motivation: Forecasting seizures is critical for patient safety and care, requiring high sensitivity, low false alarms, and adaptability to individual differences; existing methods struggle with fixed preictal windows and decoupled spatial/temporal processing.

Method: STAN uses cascaded attention blocks that alternately model spatial brain connectivity and temporal neural dynamics, jointly learning bidirectional dependencies. It uses adversarial training with gradient penalty on clearly defined 15-minute preictal windows, enabling robust interictal/preictal discrimination. The architecture supports continuous 90-minute monitoring and is designed for real-time edge deployment (2.3M parameters, ~45 ms latency, ~180 MB memory).

Result: On CHB-MIT scalp data (8 subjects, 46 events): 96.6% sensitivity with 0.011 false detections per hour; on MSSM intracranial data (4 subjects, 14 events): 94.2% sensitivity with 0.063 false detections per hour.

Conclusion: The framework provides a general, interpretable paradigm for spatio-temporal forecasting in healthcare and other domains with heterogeneity across individuals, enabling early, customizable detections without extensive per-subject training and supporting real-time deployment.

Abstract: Forecasting epileptic seizures from multivariate EEG signals represents a
critical challenge in healthcare time series prediction, requiring high
sensitivity, low false alarm rates, and subject-specific adaptability. We
present STAN, an Adversarial Spatio-Temporal Attention Network that jointly
models spatial brain connectivity and temporal neural dynamics through cascaded
attention blocks with alternating spatial and temporal modules. Unlike existing
approaches that assume fixed preictal durations or separately process spatial
and temporal features, STAN captures bidirectional dependencies between spatial
and temporal patterns through a unified cascaded architecture. Adversarial
training with gradient penalty enables robust discrimination between interictal
and preictal states learned from clearly defined 15-minute preictal windows.
Continuous 90-minute pre-seizure monitoring reveals that the learned
spatio-temporal attention patterns enable early detection: reliable alarms
trigger at subject-specific times (typically 15-45 minutes before onset),
reflecting the model's capacity to capture subtle preictal dynamics without
requiring individualized training. Experiments on two benchmark EEG datasets
(CHB-MIT scalp: 8 subjects, 46 events; MSSM intracranial: 4 subjects, 14
events) demonstrate state-of-the-art performance: 96.6% sensitivity with 0.011
false detections per hour and 94.2% sensitivity with 0.063 false detections per
hour, respectively, while maintaining computational efficiency (2.3M
parameters, 45 ms latency, 180 MB memory) for real-time edge deployment. Beyond
epilepsy, the proposed framework provides a general paradigm for
spatio-temporal forecasting in healthcare and other time series domains where
individual heterogeneity and interpretability are crucial.

</details>


### [327] [Identification of Capture Phases in Nanopore Protein Sequencing Data Using a Deep Learning Model](https://arxiv.org/abs/2511.01277)
*Annabelle Martin,Daphne Kontogiorgos-Heintz,Jeff Nivala*

Main category: cs.LG

TL;DR: A lightweight 1D CNN (CaptureNet-Deep) detects nanopore protein capture phases with high accuracy (F1 0.94, precision 93.39%), enabling real-time, low-latency inference and dramatically reducing analysis time.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of capture phases is labor-intensive and time-consuming (days) due to complex, long, noisy ionic current traces; there is a need for real-time, interpretable ML in nanopore sequencing workflows.

Method: Developed a 1D CNN (CaptureNet-Deep) trained on down-sampled signal windows to detect capture events. Compared against CNN-LSTM hybrids, histogram-based classifiers, and other CNN variants using run-level data splits. Integrated into a dashboard for Oxford Nanopore experiments to enable real-time use.

Result: Best model achieved F1 score of 0.94 and precision of 93.39% on held-out test data. Demonstrated low-latency inference and integration into a dashboard, reducing analysis time from days to under thirty minutes.

Conclusion: Shows that efficient, real-time capture detection is possible with simple, interpretable architectures and suggests a broader role for lightweight ML models in sequencing workflows.

Abstract: Nanopore protein sequencing produces long, noisy ionic current traces in
which key molecular phases, such as protein capture and translocation, are
embedded. Capture phases mark the successful entry of a protein into the pore
and serve as both a checkpoint and a signal that a channel merits further
analysis. However, manual identification of capture phases is time-intensive,
often requiring several days for expert reviewers to annotate the data due to
the need for domain-specific interpretation of complex signal patterns. To
address this, a lightweight one-dimensional convolutional neural network (1D
CNN) was developed and trained to detect capture phases in down-sampled signal
windows. Evaluated against CNN-LSTM (Long Short-Term Memory) hybrids,
histogram-based classifiers, and other CNN variants using run-level data
splits, our best model, CaptureNet-Deep, achieved an F1 score of 0.94 and
precision of 93.39% on held-out test data. The model supports low-latency
inference and is integrated into a dashboard for Oxford Nanopore experiments,
reducing the total analysis time from several days to under thirty minutes.
These results show that efficient, real-time capture detection is possible
using simple, interpretable architectures and suggest a broader role for
lightweight ML models in sequencing workflows.

</details>


### [328] [Koopman-based Prediction of Connectivity for Flying Ad Hoc Networks](https://arxiv.org/abs/2511.01286)
*Sivaram Krishnan,Jinho Choi,Jihong Park,Gregory Sherman,Benjamin Campbell*

Main category: cs.LG

TL;DR: Koopman-based data-driven schemes (centralized and distributed) model UAV dynamics in FANETs to predict SINR/connectivity and outages, enabling proactive scheduling.


<details>
  <summary>Details</summary>
Motivation: ML in wireless networks often assumes static channels; FANETs are highly dynamic with changing topology. Koopman operator theory offers a way to linearize nonlinear dynamics in lifted space, enabling data-driven prediction of connectivity and outages in UAV networks.

Method: Develop two approaches (centralized and distributed) using Koopman operator theory to learn UAV trajectory dynamics from data and predict SINR/connectivity and isolation events in FANETs. Evaluate on a surveillance scenario with predefined UAV trajectories.

Result: Both centralized and distributed Koopman approaches accurately predict connectivity and isolation events leading to outages; predictions enable UAVs to schedule transmissions proactively. The methods demonstrate reliable SINR forecasts and outage predictions with trade-offs in scalability and communication overhead between the two variants.

Conclusion: Koopman-based data-driven modeling is effective for highly dynamic FANETs. Centralized and distributed variants address topology changes and enable proactive resource management and reliable communications in AI-driven next-generation networks.

Abstract: The application of machine learning (ML) to communication systems is expected
to play a pivotal role in future artificial intelligence (AI)-based
next-generation wireless networks. While most existing works focus on ML
techniques for static wireless environments, they often face limitations when
applied to highly dynamic environments, such as flying ad hoc networks
(FANETs). This paper explores the use of data-driven Koopman approaches to
address these challenges. Specifically, we investigate how these approaches can
model UAV trajectory dynamics within FANETs, enabling more accurate predictions
and improved network performance. By leveraging Koopman operator theory, we
propose two possible approaches -- centralized and distributed -- to
efficiently address the challenges posed by the constantly changing topology of
FANETs. To demonstrate this, we consider a FANET performing surveillance with
UAVs following pre-determined trajectories and predict
signal-to-interference-plus-noise ratios (SINRs) to ensure reliable
communication between UAVs. Our results show that these approaches can
accurately predict connectivity and isolation events that lead to modelled
communication outages. This capability could help UAVs schedule their
transmissions based on these predictions.

</details>


### [329] [LSHFed: Robust and Communication-Efficient Federated Learning with Locally-Sensitive Hashing Gradient Mapping](https://arxiv.org/abs/2511.01296)
*Guanjie Cheng,Mengzhen Yang,Xinkui Zhao,Shuyi Yu,Tianyu Du,Yangyang Wu,Mengying Zhu,Shuiguang Deng*

Main category: cs.LG

TL;DR: LSHFed is a robust, communication-efficient federated learning framework that uses LSH-based gradient verification (LSHGM) to detect and filter malicious gradients, preserving privacy and reducing communication, while tolerating colluding adversaries.


<details>
  <summary>Details</summary>
Motivation: Federated learning is vulnerable to inference and poisoning attacks in trust-deficient environments. Existing defenses have high costs or limited precision; a method that is robust, privacy-preserving, and communication-efficient is needed.

Method: LSHFed introduces LSHGM, which projects high-dimensional gradients into compact binary representations using multi-hyperplane locally-sensitive hashing. Progressively verifies gradients via irreversible hash forms to detect and filter malicious updates, enabling robust aggregation with reduced data transmission.

Result: Experiments show LSHFed maintains model performance even with up to 50% collusive adversaries and achieves up to a 1000x reduction in gradient verification communication compared to full-gradient methods.

Conclusion: LSHFed provides a robust and privacy-preserving FL framework with substantially lower communication costs, suitable for trust-deficient environments where malicious participants may collude.

Abstract: Federated learning (FL) enables collaborative model training across
distributed nodes without exposing raw data, but its decentralized nature makes
it vulnerable in trust-deficient environments. Inference attacks may recover
sensitive information from gradient updates, while poisoning attacks can
degrade model performance or induce malicious behaviors. Existing defenses
often suffer from high communication and computation costs, or limited
detection precision. To address these issues, we propose LSHFed, a robust and
communication-efficient FL framework that simultaneously enhances aggregation
robustness and privacy preservation. At its core, LSHFed incorporates LSHGM, a
novel gradient verification mechanism that projects high-dimensional gradients
into compact binary representations via multi-hyperplane locally-sensitive
hashing. This enables accurate detection and filtering of malicious gradients
using only their irreversible hash forms, thus mitigating privacy leakage risks
and substantially reducing transmission overhead. Extensive experiments
demonstrate that LSHFed maintains high model performance even when up to 50% of
participants are collusive adversaries while achieving up to a 1000x reduction
in gradient verification communication compared to full-gradient methods.

</details>


### [330] [Diffusion-Based Solver for CNF Placement on the Cloud-Continuum](https://arxiv.org/abs/2511.01343)
*Ãlvaro VÃ¡zquez RodrÃ­guez,Manuel FernÃ¡ndez-Veiga,Carlos Giraldo-RodrÃ­guez*

Main category: cs.LG

TL;DR: A diffusion-based generative framework with a graph neural network denoiser is proposed to tackle constrained CNF placement in the cloud continuum, achieving feasible placements with orders of magnitude faster inference than MINLP solvers.


<details>
  <summary>Details</summary>
Motivation: Placing interdependent CNFs across distributed cloud infrastructures (cloud continuum) for 5G/6G faces tight resource, bandwidth, and latency constraints. Traditional optimization and ML approaches struggle with scalability, constraint handling, and generalization, motivating a scalable, constraint-aware solution.

Method: Formulate placement as a generative graph-to-assignment problem by encoding CNF-to-cloud mapping as a heterogeneous graph. Train a Graph Neural Network denoiser within a Denoising Diffusion Probabilistic Model (DDPM) to iteratively refine noisy CNF-to-cloud assignment matrices. Incorporate constraint-specific losses into the training objective so the model learns feasible solutions. Use rigorous integration of DDPM with structured combinatorial constraints and evaluate across diverse topologies.

Result: The approach consistently yields feasible CNF placements with inference times orders of magnitude faster than traditional MINLP solvers, demonstrating the practicality and scalability of diffusion-based generative modeling for constrained network embedding problems.

Conclusion: Diffusion-based generative modeling shows strong potential for constrained CNF placement in cloud-native networks, enabling scalable, feasible orchestration in the cloud continuum; the approach could generalize to other constrained network embedding problems.

Abstract: The placement of Cloud-Native Network Functions (CNFs) across the
Cloud-Continuum represents a core challenge in the orchestration of current 5G
and future 6G networks. The process involves the placement of interdependent
computing tasks, structured as Service Function Chains, over distributed cloud
infrastructures. This is achieved while satisfying strict resource, bandwidth
and latency constraints. It is acknowledged that classical approaches,
including mixed-integer nonlinear programming, heuristics and reinforcement
learning are limited in terms of scalability, constraint handling and
generalisation capacity. In the present study, a novel theoretical framework is
proposed, which is based on Denoising Diffusion Probabilistic Models (DDPM) for
CNF placement. The present approach proposes a reconceptualisation of placement
as a generative graph to assignment task, where the placement problem is
encoded as a heterogeneous graph, and a Graph Neural Network denoiser is
trained to iteratively refine noisy CNF-to-cloud assignment matrices. The model
incorporates constraint-specific losses directly into the loss function,
thereby allowing it to learn feasible solution spaces. The integration of the
DDPM formulation with structured combinatorial constraints is achieved through
a rigorous and systematic approach. Extensive evaluations across diverse
topologies have been conducted, which have confirmed that the model
consistently produces feasible solutions with orders of magnitude faster
inference than MINLP solvers. The results obtained demonstrate the potential of
diffusion-based generative modelling for constrained network embedding
problems, making an impact towards the practical, scalable orchestration of
distributed Cloud-Native Network Functions.

</details>


### [331] [MiniFool - Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks](https://arxiv.org/abs/2511.01352)
*Lucie Flek,Oliver Janik,Philipp Alexander Jung,Akbar Karimi,Timo Saala,Alexander Schmidt,Matthias Schott,Philipp Soldin,Matthias Thiesmeyer,Christopher Wiebusch,Ulrich Willemsen*

Main category: cs.LG

TL;DR: MiniFool is a physics-inspired adversarial attack algorithm that tests neural network robustness across particle physics datasets (IceCube tau neutrinos, MNIST, CMS Open Data). It minimizes a cost combining a chi-square test statistic with deviation to a target score, and studies flips in classification under varying uncertainty scaling, enabling robustness assessment on labeled and unlabeled data.


<details>
  <summary>Details</summary>
Motivation: To assess and quantify the vulnerability of NN classifications in physics contexts, and to provide a general, physics-grounded framework for adversarial testing that accounts for experimental uncertainties.

Method: Develop MiniFool that uses a cost function: chi-square-based stat plus distance to target score; perturb data within experimental uncertainties; attack parameter scales uncertainties; apply to various domains including MNIST and CMS Open Data, starting from IceCube neutrino analysis.

Result: Classification flip likelihood differs for initially correct vs incorrect events; robustness can be quantified as function of attack parameter; demonstrates general applicability and ability to test unlabeled data robustness.

Conclusion: MiniFool offers a flexible, physics-informed approach to evaluate and compare classifier robustness across domains, enabling testing on unlabeled data and providing a metric to compare models under perturbations.

Abstract: In this paper, we present a new algorithm, MiniFool, that implements
physics-inspired adversarial attacks for testing neural network-based
classification tasks in particle and astroparticle physics. While we initially
developed the algorithm for the search for astrophysical tau neutrinos with the
IceCube Neutrino Observatory, we apply it to further data from other science
domains, thus demonstrating its general applicability. Here, we apply the
algorithm to the well-known MNIST data set and furthermore, to Open Data data
from the CMS experiment at the Large Hadron Collider. The algorithm is based on
minimizing a cost function that combines a $\chi^2$ based test-statistic with
the deviation from the desired target score. The test statistic quantifies the
probability of the perturbations applied to the data based on the experimental
uncertainties. For our studied use cases, we find that the likelihood of a
flipped classification differs for both the initially correctly and incorrectly
classified events. When testing changes of the classifications as a function of
an attack parameter that scales the experimental uncertainties, the robustness
of the network decision can be quantified. Furthermore, this allows testing the
robustness of the classification of unlabeled experimental data.

</details>


### [332] [Verifiable Split Learning via zk-SNARKs](https://arxiv.org/abs/2511.01356)
*Rana Alaa,DarÃ­o GonzÃ¡lez-Ferreiro,Carlos Beis-Penedo,Manuel FernÃ¡ndez-Veiga,Rebeca P. DÃ­az-Redondo,Ana FernÃ¡ndez-Vilas*

Main category: cs.LG

TL;DR: Verifiable split learning using zk-SNARKs ensures correctness and verifiability of client-server computations in split DL; compared to a blockchain-only recording approach which is lightweight but unverifiable.


<details>
  <summary>Details</summary>
Motivation: Split learning currently cannot verify the correctness of the exchanged computations, risking dishonesty or errors across client and server. There is a need for a trusted mechanism to verify both forward and backward computations in the split architecture.

Method: Integrate a zk-SNARK proof into the split learning framework to certify correctness. Generate zk-SNARK proofs and verifications for both client and server sides during forward propagation and backward propagation on the server side, ensuring verifiability on both sides. Compare this verifiable setup to a blockchain-enabled system that records updates without zero-knowledge proofs.

Result: The zk-SNARKâbased approach achieves verifiability and correctness in the split learning process. The blockchain-based system, while lightweight, remains unverifiable without zero-knowledge proofs.

Conclusion: Incorporating zk-SNARK proofs into split learning provides verifiability and correctness, addressing the limitations of veracity in traditional split learning; blockchain-only recording lacks verifiability, though it may be lighter in overhead.

Abstract: Split learning is an approach to collaborative learning in which a deep
neural network is divided into two parts: client-side and server-side at a cut
layer. The client side executes its model using its raw input data and sends
the intermediate activation to the server side. This configuration architecture
is very useful for enabling collaborative training when data or resources are
separated between devices. However, split learning lacks the ability to verify
the correctness and honesty of the computations that are performed and
exchanged between the parties. To this purpose, this paper proposes a
verifiable split learning framework that integrates a zk-SNARK proof to ensure
correctness and verifiability. The zk-SNARK proof and verification are
generated for both sides in forward propagation and backward propagation on the
server side, guaranteeing verifiability on both sides. The verifiable split
learning architecture is compared to a blockchain-enabled system for the same
deep learning network, one that records updates but without generating the
zero-knowledge proof. From the comparison, it can be deduced that applying the
zk-SNARK test achieves verifiability and correctness, while blockchains are
lightweight but unverifiable.

</details>


### [333] [Learning Intractable Multimodal Policies with Reparameterization and Diversity Regularization](https://arxiv.org/abs/2511.01374)
*Ziqi Wang,Jiashun Liu,Ling Pan*

Main category: cs.LG

TL;DR: A study on tractable multimodal RL via an amortized/distance-regularized actor, enabling policy-gradient optimization of multimodal policies and improving few-shot robustness in multi-goal and generative RL; competitive on MuJoCo; code released.


<details>
  <summary>Details</summary>
Motivation: Traditional RL with deterministic or unimodal Gaussian actors cannot express complex multimodal decision distributions, hindering diversity and performance; existing multimodal approaches (diffusion/amortized actors) are intractable and struggle to balance performance, diversity, and efficiency.

Method: Reformulate intractable multimodal actors within a unified framework and show they can be optimized by policy gradient via reparameterization. Introduce a distance-based diversity regularization that does not require explicit decision probabilities. Identify two diversity-critical domains (multi-goal achieving and generative RL) and demonstrate advantages of multimodal policies and the proposed method, especially in few-shot robustness. Use an amortized actor as the policy class and validate on MuJoCo benchmarks.

Result: The approach yields improved diversity and few-shot robustness in the targeted domains, with competitive performance on conventional MuJoCo benchmarks. Experiments indicate the amortized actor offers strong multimodal expressivity and high performance.

Conclusion: Multimodal policies, together with the distance-based regularization and the amortized actor, can achieve robust and diverse behavior without sacrificing efficiency; the amortized actor emerges as a promising policy class for multimodal RL. Code available at the provided GitHub link.

Abstract: Traditional continuous deep reinforcement learning (RL) algorithms employ
deterministic or unimodal Gaussian actors, which cannot express complex
multimodal decision distributions. This limitation can hinder their performance
in diversity-critical scenarios. There have been some attempts to design online
multimodal RL algorithms based on diffusion or amortized actors. However, these
actors are intractable, making existing methods struggle with balancing
performance, decision diversity, and efficiency simultaneously. To overcome
this challenge, we first reformulate existing intractable multimodal actors
within a unified framework, and prove that they can be directly optimized by
policy gradient via reparameterization. Then, we propose a distance-based
diversity regularization that does not explicitly require decision
probabilities. We identify two diversity-critical domains, namely multi-goal
achieving and generative RL, to demonstrate the advantages of multimodal
policies and our method, particularly in terms of few-shot robustness. In
conventional MuJoCo benchmarks, our algorithm also shows competitive
performance. Moreover, our experiments highlight that the amortized actor is a
promising policy model class with strong multimodal expressivity and high
performance. Our code is available at https://github.com/PneuC/DrAC

</details>


### [334] [Protecting the Neural Networks against FGSM Attack Using Machine Unlearning](https://arxiv.org/abs/2511.01377)
*Amir Hossein Khorasani,Ali Jahanian,Maryam Rastgarpour*

Main category: cs.LG

TL;DR: Unlearning FGSM perturbations on LeNet improves robustness to FGSM adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: FGSM attacks exploit model vulnerabilities; unlearning aims to forget perturbed data and reduce attack efficacy, improving robustness.

Method: Apply machine unlearning techniques to LeNet, retraining to forget FGSM-perturbed data, and evaluate robustness against FGSM.

Result: Significant robustness improvement against FGSM demonstrated on LeNet with unlearning.

Conclusion: Unlearning is a promising defense against FGSM for LeNet and motivates further exploration on other models and attack types.

Abstract: Machine learning is a powerful tool for building predictive models. However,
it is vulnerable to adversarial attacks. Fast Gradient Sign Method (FGSM)
attacks are a common type of adversarial attack that adds small perturbations
to input data to trick a model into misclassifying it. In response to these
attacks, researchers have developed methods for "unlearning" these attacks,
which involves retraining a model on the original data without the added
perturbations. Machine unlearning is a technique that tries to "forget"
specific data points from the training dataset, to improve the robustness of a
machine learning model against adversarial attacks like FGSM. In this paper, we
focus on applying unlearning techniques to the LeNet neural network, a popular
architecture for image classification. We evaluate the efficacy of unlearning
FGSM attacks on the LeNet network and find that it can significantly improve
its robustness against these types of attacks.

</details>


### [335] [Memory-Efficient Training with In-Place FFT Implementation](https://arxiv.org/abs/2511.01385)
*Xinyu Ding,Bangtian Liu,Siyu Liao,Zhongfeng Wang*

Main category: cs.LG

TL;DR: A real-domain, fully in-place FFT (rdFFT) that preserves input-output memory space and eliminates intermediate cache usage by exploiting butterfly symmetry and conjugate properties, addressing the dimensional mismatch in rFFT and reducing training memory in NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Existing FFT/rFFT implementations do not support true in-place computation, incurring extra memory and dimensional mismatches. A memory-consistent real-domain FFT could enable memory-efficient frequency-domain adaptation in deep learning.

Method: Develop an implicit complex encoding scheme that leverages butterfly symmetry and conjugate properties to perform FFT entirely in place in the real domain, avoiding intermediate cache allocations and preserving input-output memory footprint.

Result: Empirical evaluations on several NLP tasks show reduced training memory usage with rdFFT while maintaining accuracy/throughput, indicating practical benefits for memory-efficient frequency-domain models.

Conclusion: rdFFT provides the first real-domain, fully in-place real FFT framework, enabling memory-space consistency and cache-free computation, representing a promising direction for frequency-domain lightweight adaptation in NLP.

Abstract: Fast Fourier Transforms (FFT) are widely used to reduce memory and
computational costs in deep learning. However, existing implementations,
including standard FFT and real FFT (rFFT), cannot achieve true in-place
computation. In particular, rFFT maps an input of size n to a complex output of
size n/2+1, causing dimensional mismatch and requiring additional memory
allocation. We propose the first real-domain, fully in-place FFT framework
(rdFFT) that preserves input-output memory space consistency. By leveraging
butterfly operation symmetry and conjugate properties in the frequency domain,
we design an implicit complex encoding scheme that eliminates intermediate
cache usage entirely. Experiments on multiple natural language understanding
tasks demonstrate the method effectiveness in reducing training memory cost,
offering a promising direction for frequency-domain lightweight adaptation.

</details>


### [336] [Leveraging Compact Satellite Embeddings and Graph Neural Networks for Large-Scale Poverty Mapping](https://arxiv.org/abs/2511.01408)
*Markus B. Pettersson,Adel Daoud*

Main category: cs.LG

TL;DR: Graph-based model with AlphaEarth embeddings and fuzzy-label loss improves cluster-level wealth prediction beyond image-only baselines, leveraging spatial relations and robustness to coordinate displacement.


<details>
  <summary>Details</summary>
Motivation: Fine-grained poverty maps are scarce in the Global South; DHS data are spatially noisy due to coordinate displacement and have limited coverage. Exploit spatial structure and robust supervision to generalize wealth estimates to unlabeled locations.

Method: Construct a graph over surveyed and unlabeled locations using AlphaEarth low-dimensional satellite embeddings as features; train with a probabilistic 'fuzzy label' loss to account for displaced coordinates; compare against image-only baselines on 37 DHS datasets (2017â2023).

Result: In experiments, incorporating graph structure yields small accuracy improvements over image-only baselines; results across 37 DHS datasets demonstrate the potential of compact EO embeddings for large-scale socioeconomic mapping.

Conclusion: A graph-based framework with fuzzy labeling can improve generalization of wealth prediction in data-sparse, noisy settings; AlphaEarth embeddings show promise for scalable poverty mapping, though gains are modest and may depend on data quality and graph design.

Abstract: Accurate, fine-grained poverty maps remain scarce across much of the Global
South. While Demographic and Health Surveys (DHS) provide high-quality
socioeconomic data, their spatial coverage is limited and reported coordinates
are randomly displaced for privacy, further reducing their quality. We propose
a graph-based approach leveraging low-dimensional AlphaEarth satellite
embeddings to predict cluster-level wealth indices across Sub-Saharan Africa.
By modeling spatial relations between surveyed and unlabeled locations, and by
introducing a probabilistic "fuzzy label" loss to account for coordinate
displacement, we improve the generalization of wealth predictions beyond
existing surveys. Our experiments on 37 DHS datasets (2017-2023) show that
incorporating graph structure slightly improves accuracy compared to
"image-only" baselines, demonstrating the potential of compact EO embeddings
for large-scale socioeconomic mapping.

</details>


### [337] [CG-FKAN: Compressed-Grid Federated Kolmogorov-Arnold Networks for Communication Constrained Environment](https://arxiv.org/abs/2511.01433)
*Seunghun Yu,Youngjoon Lee,Jinu Gong,Joonhyuk Kang*

Main category: cs.LG

TL;DR: A communication-budget aware CG-FKAN sparsifies extended grids in FL with Kolmogorov-Arnold Networks (KAN) to reduce transmission, achieving up to 13.6% RMSE improvement over fixed-grid KAN and providing a theoretical approximation-error bound.


<details>
  <summary>Details</summary>
Motivation: Federated learning often lacks interpretability. KAN offers interpretable learnable spline representations, but extending grids for complex functions incurs significant communication overhead. There is a need to compress grid information efficiently under a communication budget without losing interpretability.

Method: CG-FKAN compresses extended grids by sparsifying and transmitting only the essential coefficients within a specified communication budget.

Result: Empirical results show up to 13.6% lower RMSE than fixed-grid KAN in communication-constrained settings. The work also derives a theoretical upper bound on the approximation error.

Conclusion: CG-FKAN improves performance under communication constraints while preserving interpretability through KAN, and provides a theoretical guarantee on its approximation error.

Abstract: Federated learning (FL), widely used in privacy-critical applications,
suffers from limited interpretability, whereas Kolmogorov-Arnold Networks (KAN)
address this limitation via learnable spline functions. However, existing FL
studies applying KAN overlook the communication overhead introduced by grid
extension, which is essential for modeling complex functions. In this letter,
we propose CG-FKAN, which compresses extended grids by sparsifying and
transmitting only essential coefficients under a communication budget.
Experiments show that CG-FKAN achieves up to 13.6% lower RMSE than fixed-grid
KAN in communication-constrained settings. In addition, we derive a theoretical
upper bound on its approximation error.

</details>


### [338] [The Curvature Rate Î»: A Scalar Measure of Input-Space Sharpness in Neural Networks](https://arxiv.org/abs/2511.01438)
*Jacob Poschl*

Main category: cs.LG

TL;DR: A new input-space curvature measure lambda, defined as the exponential growth rate of higher-order derivatives, with a derivative-based regularizer (CRR); shows comparable performance to SAM, with flatter input-space geometry and improved calibration; offers an interpretable, parameterization-invariant view of functional smoothness.


<details>
  <summary>Details</summary>
Motivation: Current sharpness metrics live in parameter space (e.g., Hessian eigenvalues) and are expensive, reparameterization-sensitive, and hard to interpret in functional terms. The work seeks a parameterization-invariant, input-space curvature descriptor grounded in differentiation dynamics that relates to analytic properties and high-frequency boundary structure.

Method: Define lambda as the exponential growth rate of higher-order input derivatives: the slope of log ||D^n f|| vs n for small n. This unifies analytic notions (inverse radius of convergence for analytic functions; spectral cutoff for bandlimited signals). Extend to neural networks to track high-frequency structure in the decision boundary. Introduce Curvature Rate Regularization (CRR), a simple derivative-based regularizer to shape lambda during training.

Result: Empirical studies on analytic functions and on neural networks (Two Moons, MNIST) show that lambda evolves predictably during training and can be steered by CRR. CRR achieves similar accuracy to Sharpness-Aware Minimization (SAM) while yielding flatter input-space geometry and improved confidence calibration.

Conclusion: Lambda provides a compact, interpretable, and parameterization-invariant descriptor of functional smoothness in learned models by grounding curvature in differentiation dynamics.

Abstract: Curvature influences generalization, robustness, and how reliably neural
networks respond to small input perturbations. Existing sharpness metrics are
typically defined in parameter space (e.g., Hessian eigenvalues) and can be
expensive, sensitive to reparameterization, and difficult to interpret in
functional terms. We introduce a scalar curvature measure defined directly in
input space: the curvature rate {\lambda}, given by the exponential growth rate
of higher-order input derivatives. Empirically, {\lambda} is estimated as the
slope of log ||D^n f|| versus n for small n. This growth-rate perspective
unifies classical analytic quantities: for analytic functions, {\lambda}
corresponds to the inverse radius of convergence, and for bandlimited signals,
it reflects the spectral cutoff. The same principle extends to neural networks,
where {\lambda} tracks the emergence of high-frequency structure in the
decision boundary. Experiments on analytic functions and neural networks (Two
Moons and MNIST) show that {\lambda} evolves predictably during training and
can be directly shaped using a simple derivative-based regularizer, Curvature
Rate Regularization (CRR). Compared to Sharpness-Aware Minimization (SAM), CRR
achieves similar accuracy while yielding flatter input-space geometry and
improved confidence calibration. By grounding curvature in differentiation
dynamics, {\lambda} provides a compact, interpretable, and
parameterization-invariant descriptor of functional smoothness in learned
models.

</details>


### [339] [Efficient Curvature-aware Graph Network](https://arxiv.org/abs/2511.01443)
*Chaoqun Fei,Tinglve Zhou,Tianyong Hao,Yangyang Li*

Main category: cs.LG

TL;DR: Introduces Effective Resistance Curvature as a scalable alternative to Ollivier-Ricci curvature for GNNs, achieving similar expressiveness with much lower computational cost; theory and experiments validate substitutability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Ollivier-Ricci curvature provides geometric interpretability but is computationally expensive; scalable curvature measures are needed for large graphs.

Method: Define curvature via effective resistance between node pairs to measure ease of message passing, avoiding optimal transport. Proves low computational complexity and substitutability with Ollivier-Ricci; validates with experiments.

Result: Theoretically establish low complexity and substitutability; empirically, competitive performance with Ollivier-Ricci curvature across diverse GNN tasks, with drastically reduced overhead.

Conclusion: Effective Resistance Curvature offers a scalable, interpretable geometric prior for GNNs and can substitute Ollivier-Ricci curvature without sacrificing performance, enabling efficient large-scale graph learning.

Abstract: Graph curvature provides geometric priors for Graph Neural Networks (GNNs),
enhancing their ability to model complex graph structures, particularly in
terms of structural awareness, robustness, and theoretical interpretability.
Among existing methods, Ollivier-Ricci curvature has been extensively studied
due to its strong geometric interpretability, effectively characterizing the
local geometric distribution between nodes. However, its prohibitively high
computational complexity limits its applicability to large-scale graph
datasets. To address this challenge, we propose a novel graph curvature
measure--Effective Resistance Curvature--which quantifies the ease of message
passing along graph edges using the effective resistance between node pairs,
instead of the optimal transport distance. This method significantly
outperforms Ollivier-Ricci curvature in computational efficiency while
preserving comparable geometric expressiveness. Theoretically, we prove the low
computational complexity of effective resistance curvature and establish its
substitutability for Ollivier-Ricci curvature. Furthermore, extensive
experiments on diverse GNN tasks demonstrate that our method achieves
competitive performance with Ollivier-Ricci curvature while drastically
reducing computational overhead.

</details>


### [340] [DAMBench: A Multi-Modal Benchmark for Deep Learning-based Atmospheric Data Assimilation](https://arxiv.org/abs/2511.01468)
*Hao Wang,Zixuan Weng,Jindong Han,Wei Fan,Hao Liu*

Main category: cs.LG

TL;DR: DAMBench is a large-scale, multi-modal benchmark for data assimilation in atmospheric science, designed to enable realistic, reproducible, and fair evaluation of deep learning-based DA models using real background states and real observations on a unified grid.


<details>
  <summary>Details</summary>
Motivation: To overcome two key limitations in deep learning-based data assimilation research: (i) reliance on oversimplified, synthetic perturbations, and (ii) lack of standardized benchmarks for fair model comparison in realistic, multi-modal settings.

Method: Assemble DAMBench by collecting high-quality background states from state-of-the-art forecasting systems and real-world multi-modal observations (weather stations and satellite imagery), resample to a common grid, and temporally align. Provide unified evaluation protocols, benchmark representative approaches (latent generative models, neural processes), and a lightweight multi-modal plugin to demonstrate integration of realistic observations with simple baselines.

Result: DAMBench offers a rigorous foundation for reproducibility, fair comparison, and extensibility to real-world multi-modal DA scenarios. The dataset and code are publicly available, and comprehensive experiments validate the benchmarking framework and evaluation protocols.

Conclusion: DAMBench establishes a practical, scalable benchmark platform for data-driven data assimilation in atmospheric science, enabling fair comparisons and future research into multi-modal, real-world DA methods.

Abstract: Data Assimilation is a cornerstone of atmospheric system modeling, tasked
with reconstructing system states by integrating sparse, noisy observations
with prior estimation. While traditional approaches like variational and
ensemble Kalman filtering have proven effective, recent advances in deep
learning offer more scalable, efficient, and flexible alternatives better
suited for complex, real-world data assimilation involving large-scale and
multi-modal observations. However, existing deep learning-based DA research
suffers from two critical limitations: (1) reliance on oversimplified scenarios
with synthetically perturbed observations, and (2) the absence of standardized
benchmarks for fair model comparison. To address these gaps, in this work, we
introduce DAMBench, the first large-scale multi-modal benchmark designed to
evaluate data-driven DA models under realistic atmospheric conditions. DAMBench
integrates high-quality background states from state-of-the-art forecasting
systems and real-world multi-modal observations (i.e., real-world weather
stations and satellite imagery). All data are resampled to a common grid and
temporally aligned to support systematic training, validation, and testing. We
provide unified evaluation protocols and benchmark representative data
assimilation approaches, including latent generative models and neural process
frameworks. Additionally, we propose a lightweight multi-modal plugin to
demonstrate how integrating realistic observations can enhance even simple
baselines. Through comprehensive experiments, DAMBench establishes a rigorous
foundation for future research, promoting reproducibility, fair comparison, and
extensibility to real-world multi-modal scenarios. Our dataset and code are
publicly available at https://github.com/figerhaowang/DAMBench.

</details>


### [341] [Real-time Continual Learning on Intel Loihi 2](https://arxiv.org/abs/2511.01553)
*Elvin Hajizada,Danielle Rager,Timothy Shea,Leobardo Campos-Macias,Andreas Wild,Eyke HÃ¼llermeier,Yulia Sandamirskaya,Mike Davies*

Main category: cs.LG

TL;DR: A neuromorphic SNN approach (CLP-SNN) enables rehearsal-free online continual learning on edge devices via spiking prototypes, achieving competitive accuracy with massive gains in speed and energy efficiency on Loihi 2 compared to edge GPUs.


<details>
  <summary>Details</summary>
Motivation: Open-world edge AI must adapt to non-stationary distributions and emerging classes under strict power and compute limits; offline training is insufficient, and traditional continual learning methods struggle with energy constraints on devices.

Method: CLP-SNN implements Continually Learning Prototypes as a spiking neural network on Intel Loihi 2, featuring (1) event-driven, sparsely local learning across space and time; (2) a self-normalizing three-factor learning rule that preserves weight normalization; (3) integrated neurogenesis and metaplasticity to expand capacity and mitigate forgetting.

Result: On OpenLORIS few-shot tasks, CLP-SNN achieves accuracy competitive with replay-based methods while being rehearsal-free; it is 70x faster (0.33 ms vs 23.2 ms) and 5,600x more energy efficient (0.05 mJ vs 281 mJ) than the best existing OCL approach on edge GPUs.

Conclusion: Demonstrates that brain-inspired algorithms co-designed with neuromorphic hardware can break traditional accuracyâefficiency trade-offs for future edge AI systems; suggests promising viability of online continual learning on power-constrained devices.

Abstract: AI systems on edge devices face a critical challenge in open-world
environments: adapting when data distributions shift and novel classes emerge.
While offline training dominates current paradigms, online continual learning
(OCL)--where models learn incrementally from non-stationary streams without
catastrophic forgetting--remains challenging in power-constrained settings. We
present a neuromorphic solution called CLP-SNN: a spiking neural network
architecture for Continually Learning Prototypes and its implementation on
Intel's Loihi 2 chip. Our approach introduces three innovations: (1)
event-driven and spatiotemporally sparse local learning, (2) a self-normalizing
three-factor learning rule maintaining weight normalization, and (3) integrated
neurogenesis and metaplasticity for capacity expansion and forgetting
mitigation. On OpenLORIS few-shot learning experiments, CLP-SNN achieves
accuracy competitive with replay methods while being rehearsal-free. CLP-SNN
delivers transformative efficiency gains: 70\times faster (0.33ms vs 23.2ms),
and 5,600\times more energy efficient (0.05mJ vs 281mJ) than the best
alternative OCL on edge GPU. This demonstrates that co-designed brain-inspired
algorithms and neuromorphic hardware can break traditional accuracy-efficiency
trade-offs for future edge AI systems.

</details>


### [342] [Gated Fusion Enhanced Multi-Scale Hierarchical Graph Convolutional Network for Stock Movement Prediction](https://arxiv.org/abs/2511.01570)
*Xiaosha Xue,Peibo Duan,Zhipeng Liu,Qi Chu,Changsheng Zhang,Bin zhang*

Main category: cs.LG

TL;DR: MS-HGFN introduces a hierarchical, multi-scale GNN framework with a top-down gating mechanism to jointly model intra-stock attributes and inter-stock relations across time scales, achieving improved stock movement prediction.


<details>
  <summary>Details</summary>
Motivation: Stock markets are volatile and exhibit complex interdependencies; conventional GNNs at a single scale may miss subtle intra-attribute patterns and misweight multi-scale features. There is a need to capture both intra-attribute patterns and multi-scale spatio-temporal dependencies while preserving coarse- and fine-grained information.

Method: A hierarchical GNN module constructs dynamic graphs by learning intra-attribute patterns and inter-attribute features across time scales; a top-down gating mechanism fuses multi-scale features, preserving important coarse- and fine-grained information.

Result: MS-HGFN outperforms traditional and advanced models on real-world U.S. and Chinese market data, achieving up to 1.4% higher prediction accuracy and more stable return simulations; code released at the provided URL.

Conclusion: The proposed framework effectively captures spatio-temporal dependencies across scales, addressing intra-attribute and feature-weighting biases, and offers robust stock movement prediction with improved stability.

Abstract: Accurately predicting stock market movements remains a formidable challenge
due to the inherent volatility and complex interdependencies among stocks.
Although multi-scale Graph Neural Networks (GNNs) hold potential for modeling
these relationships, they frequently neglect two key points: the subtle
intra-attribute patterns within each stock affecting inter-stock correlation,
and the biased attention to coarse- and fine-grained features during
multi-scale sampling. To overcome these challenges, we introduce MS-HGFN
(Multi-Scale Hierarchical Graph Fusion Network). The model features a
hierarchical GNN module that forms dynamic graphs by learning patterns from
intra-attributes and features from inter-attributes over different time scales,
thus comprehensively capturing spatio-temporal dependencies. Additionally, a
top-down gating approach facilitates the integration of multi-scale
spatio-temporal features, preserving critical coarse- and fine-grained features
without too much interference. Experiments utilizing real-world datasets from
U.S. and Chinese stock markets demonstrate that MS-HGFN outperforms both
traditional and advanced models, yielding up to a 1.4% improvement in
prediction accuracy and enhanced stability in return simulations. The code is
available at https://anonymous.4open.science/r/MS-HGFN.

</details>


### [343] [HIT-ROCKET: Hadamard-vector Inner-product Transformer for ROCKET](https://arxiv.org/abs/2511.01572)
*Wang Hao,Kuang Zhang,Hou Chengyu,Yuan Zhonghao,Tan Chenxing,Fu Weifeng,Zhu Yangying*

Main category: cs.LG

TL;DR: A Hadamard convolutional transform-based feature extractor augments ROCKET-like time-series classifiers by using orthogonal Hadamard kernels of varying lengths, achieving state-of-the-art accuracy with substantially reduced training time, enabling deployment on ultra-low-power devices.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art time-series classifiers (e.g., HIVE-COTE, Proximity Forest, TS-CHIEF) deliver high accuracy but at high computational cost and long training cycles. Lightweight methods like ROCKET are efficient but leave room for improvement in kernel selection and overhead. The paper seeks a simple, orthogonal-kernel approach to boost ROCKETâs performance without increasing training overhead.

Method: Introduce a Hadamard convolutional transform that uses rows/columns of Hadamard matrices as convolution kernels of extended, varying lengths. This approach remains compatible with existing pipelines (e.g., ROCKET) and exploits kernel orthogonality to improve feature robustness and computational efficiency. Extensive experiments on multi-domain datasets, focusing on UCR time series, assess accuracy and training time.

Result: F1-score improves by at least 5% over ROCKET; training time is about 50% shorter than miniROCKET under the same hyperparameters; demonstrates SOTA performance on UCR datasets and suitability for ultra-low-power embedded devices; code is available on GitHub.

Conclusion: Orthogonal Hadamard-based kernel transforms offer a simple yet effective enhancement to ROCKET-like pipelines, delivering higher accuracy with reduced training time while preserving compatibility. This supports deployment in resource-constrained environments and opens avenues for further work on kernel-length selection and hardware acceleration.

Abstract: Time series classification holds broad application value in communications,
information countermeasures, finance, and medicine. However, state-of-the-art
(SOTA) methods-including HIVE-COTE, Proximity Forest, and TS-CHIEF-exhibit high
computational complexity, coupled with lengthy parameter tuning and training
cycles. In contrast, lightweight solutions like ROCKET (Random Convolutional
Kernel Transform) offer greater efficiency but leave substantial room for
improvement in kernel selection and computational overhead. To address these
challenges, we propose a feature extraction approach based on Hadamard
convolutional transform, utilizing column or row vectors of Hadamard matrices
as convolution kernels with extended lengths of varying sizes. This enhancement
maintains full compatibility with existing methods (e.g., ROCKET) while
leveraging kernel orthogonality to boost computational efficiency, robustness,
and adaptability. Comprehensive experiments on multi-domain datasets-focusing
on the UCR time series dataset-demonstrate SOTA performance: F1-score improved
by at least 5% vs. ROCKET, with 50% shorter training time than miniROCKET
(fastest ROCKET variant) under identical hyperparameters, enabling deployment
on ultra-low-power embedded devices. All code is available on GitHub.

</details>


### [344] [Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization](https://arxiv.org/abs/2511.01588)
*Zhicheng Wang,Chen Ju,Xu Chen,Shuai Xiao,Jinsong Lan,Xiaoyong Zhu,Ying Chen,Zhiguo Cao*

Main category: cs.LG

TL;DR: PDF: a parallel decoupling framework for multimodal embeddings using distinct prefixes on an MLLM backbone with mutual information minimization and per-path contrastive loss to achieve diverse, robust embeddings with a single forward pass.


<details>
  <summary>Details</summary>
Motivation: SSC (single input, single embedding, contrastive supervision) underutilizes MLLMsâ steerability and rich multimodal signals; need diversified, robust, and efficient multimodal embeddings.

Method: Introduce PDF that applies learnable prefixes to a shared MLLM backbone to create multiple parallel paths for one input; enforce diversity via Mutual Information Minimization (MIM) and per-path contrastive supervision to align semantics; enable inference with a single forward pass.

Result: Significant gains across backbones and model sizes on MMEB: +8.9% for VLM2Vec-LLaVA-1.6-LR (7B); +4.2% (2B) and +3.1% (7B) for VLM2Vec-Qwen2VL; 2B model shows +2.6% efficiency gain with half the compute budget.

Conclusion: PDF yields a robust, semantically rich and generalizable embedding space with negligible overhead, demonstrated across multiple backbones and resolutions, highlighting the value of MLLM steerability for efficient, diverse multimodal embeddings.

Abstract: Embedding models are a cornerstone of modern AI. Driven by Multimodal Large
Language Models (MLLMs), they have made great progress in architecture and data
curation, while the holistic paradigm is still limited to SSC, i.e., single
input, singular embedding, contrastive supervision, which collapses rich,
multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM
capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF)
for multimodal embedding learning, by utilizing the proprietary steerability of
MLLMs, i.e., their ability to flexibly generate quite differentiated response
under explicit instructions. Concretely, PDF conditions a shared MLLM backbone
on distinct, learnable prefixes to roll out multiple parallel paths for one
input, then relies on these paths to obtain parallel embeddings. To promote
full parallel diversity, we employ Mutual Information Minimization (MIM) as an
explicit constraint, coupled with per-path contrastive supervision to maintain
semantic alignment. Such dual-objectives force PDF to yield robust semantic
coverage and a generalizable embedding space. Ultimately, the remarkable
embedding space are accessible at inference via one single forward pass,
incurring negligible computational overhead. We instantiate PDF on multiple
MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains
are consistently achieved across various resolutions and model sizes, e.g.,
boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the
VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency,
our 2B model surpasses its baseline by +2.6% using only half the computational
budget.

</details>


### [345] [Defining Energy Indicators for Impact Identification on Aerospace Composites: A Physics-Informed Machine Learning Perspective](https://arxiv.org/abs/2511.01592)
*NatÃ¡lia Ribeiro Marinho,Richard Loendersloot,Frank Grooteman,Jan Willem Wiegman,Uraz Odyurt,Tiedo Tinga*

Main category: cs.LG

TL;DR: A physics-informed ML approach using a physics-motivated input space and structured feature selection yields threefold improvement in predicting impact energy for aerospace composites.


<details>
  <summary>Details</summary>
Motivation: Energy estimation from low-velocity impacts on aerospace composites is challenging due to data sparsity, signal noise, complex feature interdependencies, non-linear dynamics, large design spaces, and the ill-posed inverse problem. A model that is interpretable and grounded in physical insight is needed.

Method: Construct a dedicated physics-informed input space by extracting features from time, frequency, and time-frequency domains guided by observational biases. Apply a structured feature selection process combining statistical significance testing, correlation filtering, dimensionality reduction, and noise-robustness to retain only informative and physically relevant indicators. Use exploratory data analysis to identify domain trends and produce a compact feature set that captures amplitude scaling, spectral redistribution, and transient behavior. Train a fully-connected neural network on the optimized inputs using experimental data from multiple impact scenarios (pristine and damaged states).

Result: The optimized input space with physics-informed features enabled significantly improved energy prediction accuracy, achieving a threefold reduction in error compared to conventional time-series methods and purely data-driven models, while maintaining interpretability linked to measurable structural responses.

Conclusion: Embedding domain knowledge into the learning framework yields robust, interpretable energy estimates across impact scenarios, highlighting the value of physics-informed feature design and structured selection for aerospace composite health assessment.

Abstract: Energy estimation is critical to impact identification on aerospace
composites, where low-velocity impacts can induce internal damage that is
undetectable at the surface. Current methodologies for energy prediction are
often constrained by data sparsity, signal noise, complex feature
interdependencies, non-linear dynamics, massive design spaces, and the
ill-posed nature of the inverse problem. This study introduces a
physics-informed framework that embeds domain knowledge into machine learning
through a dedicated input space. The approach combines observational biases,
which guide the design of physics-motivated features, with targeted feature
selection to retain only the most informative indicators. Features are
extracted from time, frequency, and time-frequency domains to capture
complementary aspects of the structural response. A structured feature
selection process integrating statistical significance, correlation filtering,
dimensionality reduction, and noise robustness ensures physical relevance and
interpretability. Exploratory data analysis further reveals domain-specific
trends, yielding a reduced feature set that captures essential dynamic
phenomena such as amplitude scaling, spectral redistribution, and transient
signal behaviour. Together, these steps produce a compact set of
energy-sensitive indicators with both statistical robustness and physical
significance, resulting in impact energy predictions that remain interpretable
and traceable to measurable structural responses. Using this optimised input
space, a fully-connected neural network is trained and validated with
experimental data from multiple impact scenarios, including pristine and
damaged states. The resulting model demonstrates significantly improved impact
energy prediction accuracy, reducing errors by a factor of three compared to
conventional time-series techniques and purely data-driven models.

</details>


### [346] [Estimation of Toeplitz Covariance Matrices using Overparameterized Gradient Descent](https://arxiv.org/abs/2511.01605)
*Daniel Busbib,Ami Wiesel*

Main category: cs.LG

TL;DR: Model Toeplitz covariance as a sum of K complex sinusoids and optimize with overparameterized gradient descent; mild overparameterization (K ~ 2P or 4P) yields global convergence from random starts, offering a simple, scalable alternative that matches or exceeds state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Toeplitz covariance estimation using Gaussian likelihood constraints is optimization-heavy. The success of gradient-based methods in deep learning motivates exploring simple GD for this structured problem, asking whether overparameterization guarantees global convergence and good empirical performance.

Method: Represent the PÃP Toeplitz covariance as a sum of K complex sinusoids with learnable amplitudes and frequencies; optimize these parameters via gradient descent. Findings: (i) K = P can lead to suboptimal solutions; (ii) mild overparameterization with K = 2P or 4P yields global convergence from random initializations. An accelerated GD variant with separate learning rates for amplitudes and frequencies is proposed. When frequencies are fixed and only amplitudes are optimized, the landscape is asymptotically benign and any stationary point recovers the true covariance.

Result: Overparameterized GD can match or exceed the accuracy of state-of-the-art Toeplitz covariance estimation methods in challenging settings, while remaining simple and scalable.

Conclusion: Mild overparameterization in an overparameterized GD framework provides a theoretically and empirically effective approach to Toeplitz covariance estimation, offering a simple, scalable alternative to more complex optimization methods.

Abstract: We consider covariance estimation under Toeplitz structure. Numerous
sophisticated optimization methods have been developed to maximize the Gaussian
log-likelihood under Toeplitz constraints. In contrast, recent advances in deep
learning demonstrate the surprising power of simple gradient descent (GD)
applied to overparameterized models. Motivated by this trend, we revisit
Toeplitz covariance estimation through the lens of overparameterized GD. We
model the $P\times P$ covariance as a sum of $K$ complex sinusoids with
learnable parameters and optimize them via GD. We show that when $K = P$, GD
may converge to suboptimal solutions. However, mild overparameterization ($K =
2P$ or $4P$) consistently enables global convergence from random
initializations. We further propose an accelerated GD variant with separate
learning rates for amplitudes and frequencies. When frequencies are fixed and
only amplitudes are optimized, we prove that the optimization landscape is
asymptotically benign and any stationary point recovers the true covariance.
Finally, numerical experiments demonstrate that overparameterized GD can match
or exceed the accuracy of state-of-the-art methods in challenging settings,
while remaining simple and scalable.

</details>


### [347] [Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving](https://arxiv.org/abs/2511.01633)
*Chengying Huan,Ziheng Meng,Yongchao Liu,Zhengyi Yang,Yun Zhu,Yue Yun,Shipeng Li,Rong Gu,Xiabao Wu,Haitao Zhang,Chuntao Hong,Shaonan Ma,Guihai Chen,Chen Tian*

Main category: cs.LG

TL;DR: GLM presents a multi-agent Graph-CoT system that distributes graph-based reasoning across specialized agents (classification, reasoning, action generation, graph retrieval) and couples this with an optimized LLM serving stack. This co-design reduces prompt length and reasoning iterations via branching/selective context sharing, and accelerates inference through a graph-aware KV-cache, eviction policy, and pipelined execution. Empirically, GLM improves accuracy, reduces token cost and latency, and boosts throughput versus Graph-CoT baselines.


<details>
  <summary>Details</summary>
Motivation: Graph-CoT methods improve reasoning over graph-structured knowledge but suffer from high cost in time and tokens due to monolithic prompts, repeated context re-encoding, and inefficient serving. There is a need for scalable, efficient, and accurate reasoning systems that can handle complex graphs in real time.

Method: GLM introduces a multi-agent framework with specialized roles: classification, reasoning, action generation, and graph retrieval. It enables branching and selective context sharing to prune prompt length and reduce reasoning iterations. For inference scalability, it employs a graph-aware LLM backend with dedicated KV-cache management, priority-based eviction, and pipelined execution to improve serving latency and throughput.

Result: Empirical results show GLM achieving up to 38% accuracy improvement, up to 95.7% reduction in token cost, up to 90.3% lower inference latency, and up to 15.1x higher throughput compared with state-of-the-art Graph-CoT baselines.

Conclusion: Co-designing specialized reasoning agents with an optimized serving stack enables more accurate and efficient Graph-CoT reasoning, making complex graph-based reasoning more scalable for real-world use.

Abstract: Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to
perform step-by-step reasoning over graph-structured knowledge, but existing
pipelines suffer from low accuracy, excessive token usage, high latency, and
low throughput due to single-agent monolithic prompts, repeated context
re-encoding, and inefficient serving execution. We present GLM, the first
multi-agent Graph-CoT system co-designed with an optimized LLM serving
architecture. GLM decomposes reasoning into specialized agents for
classification, reasoning, action generation, and graph retrieval, enabling
branching and selective context sharing to reduce prompt length and reasoning
iterations while preserving reasoning quality, thereby improving accuracy and
reducing overall token consumption. To scale inference, we introduce a
Graph-CoT-aware LLM inference mechanism with graph-specific KV-cache
management, priority-based eviction, and pipelined execution to improve serving
efficiency. Experiments demonstrate that GLM improves answer accuracy by up to
38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and
achieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT
baselines, enabling efficient adoption for complex real-world reasoning at
scale.

</details>


### [348] [Cross-Treatment Effect Estimation for Multi-Category, Multi-Valued Causal Inference via Dynamic Neural Masking](https://arxiv.org/abs/2511.01641)
*Xiaopeng Ke,Yihan Yu,Ruyue Zhang,Zhishuo Zhou,Fangzhou Shi,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: XTNet introduces a cross-effect estimation module with dynamic masking for multi-category, multi-valued treatments, enabling decomposition of basic effects and cross-treatment interactions to efficiently model combinatorial intervention spaces; it also proposes MCMV-AUCC for evaluation accounting for costs and interactions, and shows superior ranking and estimation performance on synthetic/real data and in a real-world A/B test.


<details>
  <summary>Details</summary>
Motivation: The problem is causal inference with multi-category, multi-valued treatments where interactions between heterogeneous interventions are complex. Existing methods are limited to binary/single-type treatments, rely on restrictive assumptions, scale poorly, and lack proper evaluation frameworks for complex interventions.

Method: XTNet architecture with a cross-effect estimation module that uses dynamic masking to capture treatment interactions without strong structural assumptions. It employs a decomposition strategy separating basic effects from cross-treatment interactions to efficiently model large combinatorial treatment spaces.

Result: Empirical results show XTNet consistently outperforms state-of-the-art baselines in both ranking accuracy and effect estimation quality across synthetic and real-world datasets; real-world A/B test results corroborate its effectiveness.

Conclusion: XTNet provides a scalable and accurate solution for estimating causal effects under multi-category, multi-valued treatments by explicitly modeling cross-effects and interactions, and introduces a suitable evaluation metric (MCMV-AUCC) to assess both costs and interactions.

Abstract: Counterfactual causal inference faces significant challenges when extended to
multi-category, multi-valued treatments, where complex cross-effects between
heterogeneous interventions are difficult to model. Existing methodologies
remain constrained to binary or single-type treatments and suffer from
restrictive assumptions, limited scalability, and inadequate evaluation
frameworks for complex intervention scenarios.
  We present XTNet, a novel network architecture for multi-category,
multi-valued treatment effect estimation. Our approach introduces a
cross-effect estimation module with dynamic masking mechanisms to capture
treatment interactions without restrictive structural assumptions. The
architecture employs a decomposition strategy separating basic effects from
cross-treatment interactions, enabling efficient modeling of combinatorial
treatment spaces. We also propose MCMV-AUCC, a suitable evaluation metric that
accounts for treatment costs and interaction effects. Extensive experiments on
synthetic and real-world datasets demonstrate that XTNet consistently
outperforms state-of-the-art baselines in both ranking accuracy and effect
estimation quality. The results of the real-world A/B test further confirm its
effectiveness.

</details>


### [349] [Bayesian Natural Gradient Fine-Tuning of CLIP Models via Kalman Filtering](https://arxiv.org/abs/2511.01694)
*Hossein Abdi,Mingfei Sun,Wei Pan*

Main category: cs.LG

TL;DR: A Bayesian Kalman-filter-based natural gradient fine-tuning method for CLIP that uses second-order information to achieve robust few-shot learning, offering uncertainty quantification and improved ID/OOD performance with efficient updates.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning CLIP with limited labeled data is challenging for both ID and OOD generalization. First-order optimizers suffer from slow convergence, hyperparameter sensitivity, and poor OOD generalization. Second-order methods can leverage curvature information; natural gradient is promising but computationally expensive for large models.

Method: Introduce a Bayesian approximation of Natural Gradient Descent (NGD) by applying a Kalman filter to estimate and update a preconditioner (inverse Fisher Information Matrix) for CLIP models during fine-tuning. This yields second-order updates with uncertainty quantification, reducing computational burden while improving robustness.

Result: Empirical evaluation across diverse image classification datasets shows that the proposed Kalman-filtered NGD method achieves superior or comparable in-distribution performance and improved out-of-distribution robustness relative to strong baselines, highlighting improved efficiency and generalization.

Conclusion: This work represents the first successful application of Kalman filtering to fine-tune CLIP-based models, enabling more robust and efficient learning in vision-language tasks and offering uncertainty estimates that can inform model deployment.

Abstract: Vision-language pre-trained models, such as CLIP, have established new
benchmarks in multimodal data mining. In such models, few-shot fine-tuning is a
major challenge to achieve optimal performance on both in-distribution (ID) and
out-of-distribution (OOD) datasets, especially when labeled data is scarce.
Most existing fine-tuning approaches rely on first-order gradient-based
optimizers, which typically suffer from slow convergence, sensitivity to
step-size hyperparameters, and poor generalization in OOD settings. In
contrast, second-order methods utilize local curvature information of the loss
landscape to adjust the update step size. This is particularly beneficial for
CLIP models, whose non-convex loss functions often contain sharp critical
points. In such cases, natural gradient direction can offer more substantial
and efficient per-iteration updates when fine-tuning with limited data. Natural
Gradient Descent (NGD) is obtained by preconditioning the standard gradient
with the inverse Fisher Information Matrix (FIM), which is computationally
expensive for large models. To address this, we propose a Bayesian
approximation of NGD using a Kalman filter for CLIP models. Our method combines
the benefits of second-order optimization with Bayesian inference, which
enhances generalization while providing uncertainty quantification. Extensive
experiments conducted on diverse image classification datasets demonstrate that
our algorithm consistently achieves superior--or comparable--ID performance and
improved OOD robustness compared to state-of-the-art baselines. To the best of
our knowledge, this work represents the first successful application of Kalman
filtering to fine-tuning CLIP-based models, which enables more robust and
efficient learning in vision-language tasks.

</details>


### [350] [Collaborative Large Language Model Inference via Resource-Aware Parallel Speculative Decoding](https://arxiv.org/abs/2511.01695)
*Jungyeon Koh,Hyun Jong Yang*

Main category: cs.LG

TL;DR: Unified UARA-DRL framework for parallel speculative decoding in MEC; achieves latency reductions up to 28% (avg 23.7%) with no accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Address the demand for on-device LLM inference in resource-constrained mobile edge computing (MEC). Speculative decoding reduces compute but incurs communication overhead and asynchronous delays; a joint optimization of user association and resource allocation is needed to enable efficient parallel speculative decoding.

Method: Propose a unified framework that jointly optimizes user association and resource allocation (UARA) to support parallel speculative decoding. Solve the UARA problem using a multi-agent deep reinforcement learning algorithm. Evaluate under realistic conditions with the Sionna simulator.

Result: End-to-end latency is reduced by up to 28.0% and on average by 23.7%, without compromising inference accuracy, enabling scalable, low-latency LLM services in MEC systems.

Conclusion: A joint UARA optimization via multi-agent DRL can significantly improve latency for speculative decoding in MEC without sacrificing accuracy, demonstrating viability for scalable MEC-based LLM services.

Abstract: The growing demand for on-device large language model (LLM) inference
highlights the need for efficient mobile edge computing (MEC) solutions,
especially in resource-constrained settings. Speculative decoding offers a
promising solution by partitioning token generation between a lightweight draft
model on mobile devices and a powerful target model on edge servers, but
suffers from communication overhead and asynchronous delays. This paper is the
first to propose a unified framework that jointly optimizes user association
and resource allocation (UARA) to support efficient parallel speculative
decoding. We solve the UARA problem using a multi-agent deep reinforcement
learning algorithm. To evaluate our approach under realistic conditions, we
conduct experiments using the Sionna simulator. Results show that our method
achieves up to 28.0% and an average of 23.7% reduction in end-to-end latency
without compromising inference accuracy, enabling scalable and low-latency LLM
services in MEC systems.

</details>


### [351] [Edge AI in Highly Volatile Environments: Is Fairness Worth the Accuracy Trade-off?](https://arxiv.org/abs/2511.01737)
*Obaidullah Zaland,Feras M. Awaysheh,Sawsan Al Zubi,Abdul Rahman Safi,Monowar Bhuyan*

Main category: cs.LG

TL;DR: Fairness-aware client selection in volatile FL trades slight per-client fairness and sometimes small accuracy gains for some clients, but slows global training; empirical study across CIFAR10, FashionMNIST, and EMNIST shows a fairness-performance-speed trade-off.


<details>
  <summary>Details</summary>
Motivation: Edge FL suffers from volatile resources and heterogeneous clients; this paper investigates the fundamental trade-off between accuracy and fairness under volatility and evaluates fairness-based selection algorithms (RBFF, RBCSF) against baselines.

Method: Empirical study on three datasets; compare fairness-based client selection algorithms RBFF and RBCSF with random and greedy selection; assess fairness, model accuracy, and training time; provide code repository.

Result: Fairness-based selection yields more equitable client participation and marginally better opportunities for some clients, but induces slower global training in volatile environments; trade-offs depend on dataset and scenario.

Conclusion: Highlights fairness-performance and fairness-speed trade-offs in volatile edge FL; suggests future work to address pitfalls in fair client selection and to design strategies balancing fairness and convergence speed.

Abstract: Federated learning (FL) has emerged as a transformative paradigm for edge
intelligence, enabling collaborative model training while preserving data
privacy across distributed personal devices. However, the inherent volatility
of edge environments, characterized by dynamic resource availability and
heterogeneous client capabilities, poses significant challenges for achieving
high accuracy and fairness in client participation. This paper investigates the
fundamental trade-off between model accuracy and fairness in highly volatile
edge environments. This paper provides an extensive empirical evaluation of
fairness-based client selection algorithms such as RBFF and RBCSF against
random and greedy client selection regarding fairness, model performance, and
time, in three benchmarking datasets (CIFAR10, FashionMNIST, and EMNIST). This
work aims to shed light on the fairness-performance and fairness-speed
trade-offs in a volatile edge environment and explore potential future research
opportunities to address existing pitfalls in \textit{fair client selection}
strategies in FL. Our results indicate that more equitable client selection
algorithms, while providing a marginally better opportunity among clients, can
result in slower global training in volatile environments\footnote{The code for
our experiments can be found at
https://github.com/obaidullahzaland/FairFL_FLTA.

</details>


### [352] [Game-theoretic distributed learning of generative models for heterogeneous data collections](https://arxiv.org/abs/2511.01740)
*Dmitrij Schlesinger,Boris Flach*

Main category: cs.LG

TL;DR: Exchanges synthetic data instead of model parameters to coordinate heterogeneous local models in distributed learning, with a game-theoretic framework that yields a unique Nash equilibrium for exponential-family models and converges to it; empirically validated on vision tasks.


<details>
  <summary>Details</summary>
Motivation: Distributed learning with heterogeneous local models and data is challenging. Sharing model parameters is fragile when modalities and data distributions differ; synthetic-data exchange enables model-agnostic collaboration and can accommodate semi-supervised learning across different probability spaces.

Method: Treat local models as black boxes that generate data according to their parameters. Exchange synthetic data to coordinate learning. Extend to semi-supervised settings across different probability spaces to handle modality heterogeneity. Formulate the learning as a cooperative game and prove the existence of a unique Nash equilibrium for exponential-family local models, with convergence to this equilibrium. Validate on standard vision benchmarks for image classification and conditional generation.

Result: The approach achieves advantages on standard benchmark vision datasets for image classification and conditional generation, supported by a theoretical guarantee of a unique Nash equilibrium and convergence for exponential-family local models.

Conclusion: Exchanging synthetic data offers a robust, model-agnostic way to address heterogeneity in distributed learning, enabling black-box local models to learn from data, potentially incorporating semi-supervised and multi-modality settings, with provable equilibrium convergence and empirical gains on vision tasks.

Abstract: One of the main challenges in distributed learning arises from the difficulty
of handling heterogeneous local models and data. In light of the recent success
of generative models, we propose to meet this challenge by building on the idea
of exchanging synthetic data instead of sharing model parameters. Local models
can then be treated as ``black boxes'' with the ability to learn their
parameters from data and to generate data according to these parameters.
Moreover, if the local models admit semi-supervised learning, we can extend the
approach by enabling local models on different probability spaces. This allows
to handle heterogeneous data with different modalities. We formulate the
learning of the local models as a cooperative game starting from the principles
of game theory. We prove the existence of a unique Nash equilibrium for
exponential family local models and show that the proposed learning approach
converges to this equilibrium. We demonstrate the advantages of our approach on
standard benchmark vision datasets for image classification and conditional
generation.

</details>


### [353] [HyperNQ: A Hypergraph Neural Network Decoder for Quantum LDPC Codes](https://arxiv.org/abs/2511.01741)
*Ameya S. Bhave,Navnil Choudhury,Kanad Basu*

Main category: cs.LG

TL;DR: HyperNQ uses a hypergraph neural network to decode Quantum LDPC codes, capturing higher-order stabilizer constraints; it achieves substantial logical error rate reductions below the pseudo-threshold compared with BP and GNN-based decoders.


<details>
  <summary>Details</summary>
Motivation: Quantum LDPC codes offer scalable QEC via constant-rate encoding and sparse parity checks, but Belief Propagation struggles with short cycles and standard GNNs only model pairwise interactions, limiting decoding performance.

Method: Introduce HyperNQ, a Hypergraph Neural Network decoder that encodes higher-order stabilizer constraints through hyperedges and employs a two-stage message-passing scheme for QLDPC decoding; evaluate performance in the sub-pseudo-threshold regime.

Result: HyperNQ reduces logical error rate by up to 84% relative to BP and up to 50% relative to GNN-based strategies in the sub-pseudo-threshold region, outperforming existing decoders.

Conclusion: HyperNQ demonstrates that modeling higher-order correlations via HGNNs can significantly enhance QLDPC decoding, establishing HGNN-based decoders as a promising direction for scalable quantum error correction.

Abstract: Quantum computing requires effective error correction strategies to mitigate
noise and decoherence. Quantum Low-Density Parity-Check (QLDPC) codes have
emerged as a promising solution for scalable Quantum Error Correction (QEC)
applications by supporting constant-rate encoding and a sparse parity-check
structure. However, decoding QLDPC codes via traditional approaches such as
Belief Propagation (BP) suffers from poor convergence in the presence of short
cycles. Machine learning techniques like Graph Neural Networks (GNNs) utilize
learned message passing over their node features; however, they are restricted
to pairwise interactions on Tanner graphs, which limits their ability to
capture higher-order correlations. In this work, we propose HyperNQ, the first
Hypergraph Neural Network (HGNN)- based QLDPC decoder that captures
higher-order stabilizer constraints by utilizing hyperedges-thus enabling
highly expressive and compact decoding. We use a two-stage message passing
scheme and evaluate the decoder over the pseudo-threshold region. Below the
pseudo-threshold mark, HyperNQ improves the Logical Error Rate (LER) up to 84%
over BP and 50% over GNN-based strategies, demonstrating enhanced performance
over the existing state-of-the-art decoders.

</details>


### [354] [Towards Efficient Federated Learning of Networked Mixture-of-Experts for Mobile Edge Computing](https://arxiv.org/abs/2511.01743)
*Song Gao,Shusen Jing,Shuai Zhang,Yue Wang,Xiangwei Zhou,Songyang Zhang*

Main category: cs.LG

TL;DR: A networked mixture-of-experts framework (NMoE) enabling collaborative edge inference and federated training of large AI models by routing tasks to expert neighbors and combining supervised and self-supervised learning to balance personalization and generalization, with privacy-preserving and communication-efficient training.


<details>
  <summary>Details</summary>
Motivation: LAMs require vast resources that exceed edge devices; current edge constraints in storage and computation hinder training/deployment at the edge. A distributed, privacy-preserving, and communication-efficient approach is needed to leverage edge networks for LAMs.

Method: Introduce Networked Mixture-of-Experts (NMoE) where clients infer by distributing tasks to neighboring experts based on their expertise; train via a federated learning framework that integrates supervised and self-supervised learning to balance personalization and generalization while preserving privacy and communication efficiency.

Result: Extensive experiments demonstrate the efficacy of the proposed NMoE system and provide insights and benchmarks for its training algorithms.

Conclusion: NMoE provides a practical framework for training and deploying large AI models at the edge through collaborative inference and federated learning that combines supervised and self-supervised objectives, with emphasis on privacy and communication efficiency.

Abstract: Recent advancements in large artificial intelligence models (LAMs) are
driving significant innovations in mobile edge computing within next-generation
wireless networks. However, the substantial demands for computational resources
and large-scale training data required to train LAMs conflict with the limited
storage and computational capacity of edge devices, posing significant
challenges to training and deploying LAMs at the edge. In this work, we
introduce the Networked Mixture-of-Experts (NMoE) system, in which clients
infer collaboratively by distributing tasks to suitable neighbors based on
their expertise and aggregate the returned results. For training the NMoE, we
propose a federated learning framework that integrates both supervised and
self-supervised learning to balance personalization and generalization, while
preserving communication efficiency and data privacy. We conduct extensive
experiments to demonstrate the efficacy of the proposed NMoE system, providing
insights and benchmarks for the NMoE training algorithms.

</details>


### [355] [An Open-Access Benchmark of Statistical and Machine-Learning Anomaly Detection Methods for Battery Applications](https://arxiv.org/abs/2511.01745)
*Mei-Chin Pang,Suraj Adhikari,Takuma Kasahara,Nagihiro Haba,Saneyuki Ohno*

Main category: cs.LG

TL;DR: OSBAD is an open-source battery anomaly detection benchmark enabling systematic cross-method comparison across 15 algorithms, with physics-/stats-informed feature engineering, Bayesian hyperparameter tuning, and cross-chemistry validation, all released as reproducible open-source workflows and datasets.


<details>
  <summary>Details</summary>
Motivation: To create a unified, scalable, and transferable framework for safe, data-driven anomaly detection in safety-critical battery systems, addressing incomplete labels, heterogeneous datasets, and cross-chemistry generalization.

Method: Develop OSBAD benchmark; benchmark 15 statistical, distance-based, and unsupervised ML methods across diverse datasets; introduce a physics- and statistics-informed feature transformation to decompose collective anomalies into point anomalies; implement a Bayesian optimization pipeline using transfer learning and regression proxies for automated hyperparameter tuning; validate on datasets spanning liquid and solid-state chemistries; release benchmarking database and reproducible workflows.

Result: OSBAD enables systematic method comparison and demonstrates improved anomaly separability and cross-chemistry generalization; the open benchmark and workflows provide a foundation for developing safe, scalable anomaly detection tools in battery analytics.

Conclusion: Physics-/statistics-informed feature engineering and probabilistic hyperparameter tuning are crucial for trustworthy, data-driven diagnostics in safety-critical energy systems, and OSBAD operationalizes this through an open, transferable benchmarking platform.

Abstract: Battery safety is critical in applications ranging from consumer electronics
to electric vehicles and aircraft, where undetected anomalies could trigger
safety hazards or costly downtime. In this study, we present OSBAD as an
open-source benchmark for anomaly detection frameworks in battery applications.
By benchmarking 15 diverse algorithms encompassing statistical, distance-based,
and unsupervised machine-learning methods, OSBAD enables a systematic
comparison of anomaly detection methods across heterogeneous datasets. In
addition, we demonstrate how a physics- and statistics-informed feature
transformation workflow enhances anomaly separability by decomposing collective
anomalies into point anomalies. To address a major bottleneck in unsupervised
anomaly detection due to incomplete labels, we propose a Bayesian optimization
pipeline that facilitates automated hyperparameter tuning based on
transfer-learning and regression proxies. Through validation on datasets
covering both liquid and solid-state chemistries, we further demonstrate the
cross-chemistry generalization capability of OSBAD to identify irregularities
across different electrochemical systems. By making benchmarking database with
open-source reproducible anomaly detection workflows available to the
community, OSBAD establishes a unified foundation for developing safe,
scalable, and transferable anomaly detection tools in battery analytics. This
research underscores the significance of physics- and statistics-informed
feature engineering as well as model selection with probabilistic
hyperparameter tuning, in advancing trustworthy, data-driven diagnostics for
safety-critical energy systems.

</details>


### [356] [RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks](https://arxiv.org/abs/2511.01758)
*Mian Wu,Gavin Zhang,Sewon Min,Sergey Levine,Aviral Kumar*

Main category: cs.LG

TL;DR: RLAC is a post-training RL framework that uses a dynamic LLM-based critic to identify the most likely failure modes in open-ended generation, with an external validator, jointly training generator and critic to improve quality while reducing verification effort.


<details>
  <summary>Details</summary>
Motivation: Open-ended generation demands satisfying diverse, implicit rubrics; verifying every rubric is costly and combining rubrics into a single reward is prompt-specific. There is a need for scalable, selective verification of outputs.

Method: A large language model acts as a dynamic critic to pinpoint the most probable failure modes (e.g., factual errors, edge cases). These failures are verified by an external validator. The generator and the critic are trained jointly, with dynamic critics outperforming fixed ones and reducing verification requirements.

Result: RLAC improves factual accuracy in text generation and correctness in code generation and outperforms exhaustive verification and reward-model baselines. Dynamic critics are more effective than fixed critics.

Conclusion: Dynamic adversarial critics enable scalable RL post-training for free-form generation tasks by focusing verification on the most likely failure modes, improving output quality while reducing verification costs.

Abstract: Open-ended generation tasks require outputs to satisfy diverse and often
implicit task-specific evaluation rubrics. The sheer number of relevant rubrics
leads to prohibitively high verification costs and incomplete assessments of a
response, making reinforcement learning (RL) post-training with rubric-based
rewards difficult to scale. This problem is exacerbated by the fact that often
the best way to combine these rubrics into one single reward is also highly
prompt-specific. We propose Reinforcement Learning with Adversarial Critic
(RLAC), a post-training approach that addresses these challenges via dynamic
rubric verification. Our approach employs a large language model (LLM) as a
critic that dynamically identifies only the most likely failure modes (e.g., a
factual error or unhandled edge case), which are then verified by an external
validator to optimize both generator and critic jointly. By training both the
generator and the critic, this game enhances the critic's error detection and
the generator's output quality while reducing required verifications. Our
experiments demonstrate that RLAC improves factual accuracy in text generation
and correctness in code generation, while also outperforming exhaustive
verification and reward model methods. We show that dynamic critics are more
effective than fixed critics, showcasing the potential of RLAC for scaling RL
post-training to free-form generation tasks.

</details>


### [357] [Random Initialization of Gated Sparse Adapters](https://arxiv.org/abs/2511.01794)
*Vi Retault,YohaÃ¯-Eliel Berreby*

Main category: cs.LG

TL;DR: RIGSA introduces random-initialized full-rank adapters with gating and iterative magnitude pruning to reduce forgetting in PEFT. On SmolLM2-1.7B-Instruct, it enables learning Textual MNIST and shows less forgetting than QLoRA (notably on GSM8k), though it can resemble random masking in some settings.


<details>
  <summary>Details</summary>
Motivation: Catastrophic forgetting during fine-tuning on new tasks remains a challenge. While PEFT methods like LoRA use low-rank adapters, sparse adaptation offers an alternative without rank constraints, potentially reducing forgetting.

Method: Start from randomly-initialized full-rank adapters, gate them with a ReZero-like mechanism, sparsify via iterative magnitude pruning, and evaluate on SmolLM2-1.7B-Instruct with a novel Textual MNIST task. Compare against 4-bit QLoRA and random masking baselines and assess forgetting on PIQA, HellaSwag, and GSM8k.

Result: Textual MNIST: the model begins near chance level but can learn the task with RIGSA, 4-bit QLoRA, and random masking. Across configurations, RIGSA exhibits less forgetting than QLoRA, especially on GSM8k, though performance is comparable to random masking. Notably, some RIGSA setups use more trainable parameters than QLoRA yet show reduced forgetting.

Conclusion: RIGSA demonstrates that random-initialized full-rank adapters gated and sparsified can mitigate forgetting in PEFT and, in at least some tasks (e.g., GSM8k), outperform QLoRA in terms of forgetting. The approach remains competitive with random masking and highlights the viability of sparse, gated adaptation as an alternative to traditional low-rank methods.

Abstract: When fine-tuning language models on new tasks, catastrophic forgetting --
performance degradation on previously-learned tasks -- is a ubiquitous problem.
While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this
through low-rank adapters, sparse adaptation offers an alternative that doesn't
impose rank constraints. We introduce Random Initialization of Gated Sparse
Adapters (RIGSA), which starts from randomly-initialized full-rank adapters,
gates them with a ReZero analog, and sparsifies them with iterative magnitude
pruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel
vision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag,
and GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on
Textual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA
and random masking. In spite of having more trainable parameters than QLoRA,
the RIGSA configurations that we studied displayed less forgetting than QLoRA,
particularly on GSM8k, though it performs comparably to random masking.

</details>


### [358] [Bayesian Coreset Optimization for Personalized Federated Learning](https://arxiv.org/abs/2511.01800)
*Prateek Chanda,Shrey Modi,Ganesh Ramakrishnan*

Main category: cs.LG

TL;DR: A personalized coreset weighted federated learning (methodprop) selects and uses small, weighted per-client data coresets to update a central model, achieving near-minimax optimal generalization bounds and practical gains over random sampling and submodular subset methods.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces high communication costs and privacy restrictions; training on full client data is costly and may not generalize uniformly across clients. A per-client coreset approach aims to reduce data exchanged while preserving or improving generalization, enabling personalization.

Method: For each client, construct a representative coreset with weights w and sample size n_k. Forward updates to the server using only these coreset points instead of entire client data. Theoretical analysis yields minimax-optimal generalization bounds (upper bound O(n_k^{-2Î²/(2Î²+Î)} log^{2Î´'}(n_k)) and lower bound O(n_k^{-2Î²/(2Î²+Î)})), and a closed-form generalization error function Im(w, n_k) dependent on coreset weights and size. Empirical evaluation across FL architectures and medical datasets compares against random sampling and submodular subset selection.

Result: The proposed method achieves significant gains over random data sampling in diverse personalized FL architectures and datasets, and shows competitive improvements over submodular-based subset selection on client data, with the generalization error bounded as described and a clear dependence on coreset design via Im(w, n_k).

Conclusion: Coreset-weighted personalization in federated learning is effective in reducing data sent and computation while maintaining or improving generalization; the approach provides theoretical guarantees and practical gains, though success hinges on appropriate coreset construction and tuning of n_k and w.

Abstract: In a distributed machine learning setting like Federated Learning where there
are multiple clients involved which update their individual weights to a single
central server, often training on the entire individual client's dataset for
each client becomes cumbersome. To address this issue we propose $\methodprop$:
a personalized coreset weighted federated learning setup where the training
updates for each individual clients are forwarded to the central server based
on only individual client coreset based representative data points instead of
the entire client data. Through theoretical analysis we present how the average
generalization error is minimax optimal up to logarithm bounds (upper bounded
by $\mathcal{O}(n_k^{-\frac{2 \beta}{2 \beta+\boldsymbol{\Lambda}}} \log ^{2
\delta^{\prime}}(n_k))$) and lower bounds of $\mathcal{O}(n_k^{-\frac{2
\beta}{2 \beta+\boldsymbol{\Lambda}}})$, and how the overall generalization
error on the data likelihood differs from a vanilla Federated Learning setup as
a closed form function ${\boldsymbol{\Im}}(\boldsymbol{w}, n_k)$ of the coreset
weights $\boldsymbol{w}$ and coreset sample size $n_k$. Our experiments on
different benchmark datasets based on a variety of recent personalized
federated learning architectures show significant gains as compared to random
sampling on the training data followed by federated learning, thereby
indicating how intelligently selecting such training samples can help in
performance. Additionally, through experiments on medical datasets our proposed
method showcases some gains as compared to other submodular optimization based
approaches used for subset selection on client's data.

</details>


### [359] [Dynamic Reconstruction of Ultrasound-Derived Flow Fields With Physics-Informed Neural Fields](https://arxiv.org/abs/2511.01804)
*Viraj Patel,Lisa Kreusser,Katharine Fraser*

Main category: cs.LG

TL;DR: A physics-informed neural field with multi-scale Fourier features estimates blood flow from sparse, noisy ultrasound data without ground-truth supervision, achieving low denoising/inpainting error and validating against reference flow fields and flow-rate measurements.


<details>
  <summary>Details</summary>
Motivation: Ultrasound-based flow imaging is safer and implant-friendly but suffers from attenuation with depth and noisy, sparse data. EchoPIV is challenged by complex blood dynamics; a data-efficient, physics-informed approach could improve accuracy without requiring ground-truth labels.

Method: A physics-informed neural field (PINN) model using multi-scale Fourier Feature encoding to reconstruct blood flow from sparse, noisy ultrasound data. The model does not require ground-truth supervision and is evaluated on synthetic and real datasets, with validation against reference flow fields and ground-truth flow-rate measurements. The approach adapts successful PINN concepts from other imaging modalities to ultrasound-based flow reconstruction.

Result: The method achieves consistently low mean squared error in denoising and inpainting for both synthetic and real datasets, as verified against reference flow fields and ground-truth flow-rate measurements.

Conclusion: Physics-informed neural fields can be effectively adapted for ultrasound-based medical flow reconstruction, offering robust flow estimation without ground-truth supervision and bridging approaches previously used mainly in Flow MRI to ultrasound imaging.

Abstract: Blood flow is sensitive to disease and provides insight into cardiac
function, making flow field analysis valuable for diagnosis. However, while
safer than radiation-based imaging and more suitable for patients with medical
implants, ultrasound suffers from attenuation with depth, limiting the quality
of the image. Despite advances in echocardiographic particle image velocimetry
(EchoPIV), accurately measuring blood velocity remains challenging due to the
technique's limitations and the complexity of blood flow dynamics.
Physics-informed machine learning can enhance accuracy and robustness,
particularly in scenarios where noisy or incomplete data challenge purely
data-driven approaches. We present a physics-informed neural field model with
multi-scale Fourier Feature encoding for estimating blood flow from sparse and
noisy ultrasound data without requiring ground truth supervision. We
demonstrate that this model achieves consistently low mean squared error in
denoising and inpainting both synthetic and real datasets, verified against
reference flow fields and ground truth flow rate measurements. While
physics-informed neural fields have been widely used to reconstruct medical
images, applications to medical flow reconstruction are mostly prominent in
Flow MRI. In this work, we adapt methods that have proven effective in other
imaging modalities to address the specific challenge of ultrasound-based flow
reconstruction.

</details>


### [360] [No-rank Tensor Decomposition Using Metric Learning](https://arxiv.org/abs/2511.01816)
*Maryam Bagherian*

Main category: cs.LG

TL;DR: No-rank tensor decomposition via metric learning with triplet loss and diversity/uniformity regularization; learns semantic embeddings without reconstruction; theoretical convergence and metric bounds; strong clustering improvements across faces, brain networks, and simulations, especially with limited data; outperforms PCA, t-SNE, UMAP, CP/Tucker and transformer baselines.


<details>
  <summary>Details</summary>
Motivation: Traditional reconstruction-based, fixed-rank tensor decompositions often miss semantically meaningful structure in high-dimensional data and struggle under limited labeled data. A discriminative, metric-learning approach aims to align distances with semantic similarity and enable data-efficient analysis.

Method: A no-rank tensor decomposition framework that optimizes a discriminative metric-learning objective using triplet loss augmented with diversity and uniformity regularization. Learns data-driven embeddings where geometric distance encodes semantic similarity; provides theoretical convergence guarantees and bounds on metric properties.

Result: Empirical evaluations across diverse domains (face recognition on LFW and Olivetti, brain connectivity ABIDE, and simulated data on galaxy morphology and crystal structures) show substantial improvements in clustering metrics (Silhouette, DaviesâBouldin, CalinskiâHarabasz, Separation Ratio, ARI, NMI) over baselines including PCA, t-SNE, UMAP, CP, and Tucker, with better performance in data-scarce settings.

Conclusion: Metric learning is a viable paradigm for tensor analysis that emphasizes semantic relevance over pixel-level fidelity, offering computational advantages and strong performance with limited labeled data; introduces a fundamental trade-off between global class separation and local geometric preservation.

Abstract: Tensor decomposition faces fundamental challenges in analyzing
high-dimensional data, where traditional methods based on reconstruction and
fixed-rank constraints often fail to capture semantically meaningful
structures. This paper introduces a no-rank tensor decomposition framework
grounded in metric learning, which replaces reconstruction objectives with a
discriminative, similarity-based optimization. The proposed approach learns
data-driven embeddings by optimizing a triplet loss with diversity and
uniformity regularization, creating a feature space where distance directly
reflects semantic similarity. We provide theoretical guarantees for the
framework's convergence and establish bounds on its metric properties.
Evaluations across diverse domains --including face recognition (LFW,
Olivetti), brain connectivity analysis (ABIDE), and simulated data (galaxy
morphology, crystal structures)-- demonstrate that our method outperforms
baseline techniques, including PCA, t-SNE, UMAP, and tensor decomposition
baselines (CP and Tucker). Results show substantial improvements in clustering
metrics (Silhouette Score, Davies--Bouldin Index, Calinski--Harabasz Index,
Separation Ratio, Adjusted Rand Index, Normalized Mutual Information) and
reveal a fundamental trade-off: while metric learning optimizes global class
separation, it deliberately transforms local geometry to align with semantic
relationships. Crucially, our approach achieves superior performance with
smaller training datasets compared to transformer-based methods, offering an
efficient alternative for domains with limited labeled data. This work
establishes metric learning as a paradigm for tensor-based analysis,
prioritizing semantic relevance over pixel-level fidelity while providing
computational advantages in data-scarce scenarios.

</details>


### [361] [Machine and Deep Learning for Indoor UWB Jammer Localization](https://arxiv.org/abs/2511.01819)
*Hamed Fard,Mahsa Kholghi,Benedikt GroÃ,Gerhard Wunder*

Main category: cs.LG

TL;DR: Adversarial domain adaptation improves transferable indoor jammer localization in UWB across room-layout changes.


<details>
  <summary>Details</summary>
Motivation: UWB localization achieves centimeter accuracy but is vulnerable to jamming; localizing malicious jammers within a single room and across changing indoor layouts is challenging due to domain shift; need robust ML/DL methods and datasets to baseline performance.

Method: Two novel UWB datasets (original and modified rooms) are used to establish ML/DL baselines. Evaluation uses classification and regression metrics. A domain-adversarial ConvNeXt autoencoder (A-CNT) with a gradient-reversal layer aligns CIR-derived features across domains and mitigates performance drop in the modified room.

Result: On the source dataset, Random Forest achieves F1-macro 0.95; XGBoost achieves lowest mean Euclidean error (MAE) of 20.16 cm. In the modified room, MAE for XGBoost rises to 207.99 cm (domain shift). A-CNT reduces MAE to 34.67 cm (77% improvement over non-adversarial transfer and 83% over best baseline); fraction within 30 cm restored to 0.56.

Conclusion: Adversarial feature alignment enables robust, transferable indoor jammer localization under environmental changes; dataset/code available at the provided GitHub link.

Abstract: Ultra-wideband (UWB) localization delivers centimeter-scale accuracy but is
vulnerable to jamming attacks, creating security risks for asset tracking and
intrusion detection in smart buildings. Although machine learning (ML) and deep
learning (DL) methods have improved tag localization, localizing malicious
jammers within a single room and across changing indoor layouts remains largely
unexplored. Two novel UWB datasets, collected under original and modified room
configurations, are introduced to establish comprehensive ML/DL baselines.
Performance is rigorously evaluated using a variety of classification and
regression metrics. On the source dataset with the collected UWB features,
Random Forest achieves the highest F1-macro score of 0.95 and XGBoost achieves
the lowest mean Euclidean error of 20.16 cm. However, deploying these
source-trained models in the modified room layout led to severe performance
degradation, with XGBoost's mean Euclidean error increasing tenfold to 207.99
cm, demonstrating significant domain shift. To mitigate this degradation, a
domain-adversarial ConvNeXt autoencoder (A-CNT) is proposed that leverages a
gradient-reversal layer to align CIR-derived features across domains. The A-CNT
framework restores localization performance by reducing the mean Euclidean
error to 34.67 cm. This represents a 77 percent improvement over
non-adversarial transfer learning and an 83 percent improvement over the best
baseline, restoring the fraction of samples within 30 cm to 0.56. Overall, the
results demonstrate that adversarial feature alignment enables robust and
transferable indoor jammer localization despite environmental changes. Code and
dataset available at https://github.com/afbf4c8996f/Jammer-Loc

</details>


### [362] [Towards Multi-Fidelity Scaling Laws of Neural Surrogates in CFD](https://arxiv.org/abs/2511.01830)
*Paul Setinek,Gianluca Galletti,Johannes Brandstetter*

Main category: cs.LG

TL;DR: Empirical scaling laws for multi-fidelity neural surrogates show that, in scientific ML, optimal data fidelity mixes depend on the compute budget and dataset configuration; reformulating scaling laws to separate compute from dataset composition reveals compute-performance trade-offs and guides compute-efficient data generation.


<details>
  <summary>Details</summary>
Motivation: Scientific ML faces high data-generation costs via simulations. Unlike language/vision, fidelity can be traded for compute. Understanding this trade-off is key to efficient dataset construction and model performance.

Method: Reformulate classical scaling laws to decompose the dataset axis into compute budget and dataset composition. Conduct experiments with low- and high-fidelity RANS simulations to train neural surrogates on multi-fidelity data and analyze scaling across budget, fidelity mix, and dataset size.

Result: Observed compute-performance scaling behavior and budget-dependent optimal fidelity mixes for given dataset configurations. This is the first empirical study of scaling laws for multi-fidelity neural surrogate datasets and offers guidance for compute-efficient dataset generation in scientific ML.

Conclusion: Tailor the fidelity mix to the available compute and dataset configuration using the proposed scaling framework; results enable planning of data generation strategies and motivate further development of multi-fidelity surrogates and scaling analyses across domains.

Abstract: Scaling laws describe how model performance grows with data, parameters and
compute. While large datasets can usually be collected at relatively low cost
in domains such as language or vision, scientific machine learning is often
limited by the high expense of generating training data through numerical
simulations. However, by adjusting modeling assumptions and approximations,
simulation fidelity can be traded for computational cost, an aspect absent in
other domains. We investigate this trade-off between data fidelity and cost in
neural surrogates using low- and high-fidelity Reynolds-Averaged Navier-Stokes
(RANS) simulations. Reformulating classical scaling laws, we decompose the
dataset axis into compute budget and dataset composition. Our experiments
reveal compute-performance scaling behavior and exhibit budget-dependent
optimal fidelity mixes for the given dataset configuration. These findings
provide the first study of empirical scaling laws for multi-fidelity neural
surrogate datasets and offer practical considerations for compute-efficient
dataset generation in scientific machine learning.

</details>


### [363] [Dynamic Routing Between Experts: A Data-Efficient Approach to Continual Learning in Vision-Language Models](https://arxiv.org/abs/2511.01831)
*Jay Mohta,Kenan Emir Ak,Dimitrios Dimitriadis,Yan Xu,Mingwei Shen*

Main category: cs.LG

TL;DR: Routing-based continual learning for Vision-Language Models preserves foundational abilities while adding new tasks without requiring simultaneous data access, improves specialized task accuracy, and enables cross-modal transfer; shows scalable behavior when tasks are semantically related.


<details>
  <summary>Details</summary>
Motivation: Addresses catastrophic forgetting during sequential fine-tuning of vision-language models and the overhead of multi-task learning that requires access to all datasets and linear scaling with the number of tasks.

Method: Introduces a routing-based learning mechanism that delegates new tasks to task-specific components while preserving the foundational knowledge from pretraining; evaluated on InternVL-2 models (2B and 8B params); demonstrates no need for concurrent data from all tasks and strong ablations on scalability and robustness.

Result: Foundational capabilities remain intact as general-purpose benchmarks (ChartQA, MMBench, DocVQA) stay performant, while specialized tasks see improved accuracy; routing scales as the task count grows, is robust to more tasks, and benefits when new tasks are semantically related; enables superior cross-modal transfer between language and vision beyond existing continual-learning methods.

Conclusion: Routing-based learning offers an effective and scalable solution for continual adaptation of Vision-Language Models, maintaining core capabilities, reducing data/computational overhead, and fostering cross-modal knowledge transfer.

Abstract: Vision-Language Models (VLMs) suffer from catastrophic forgetting when
sequentially fine-tuned on new tasks, degrading performance on previously
learned foundational and task-specific capabilities. While multi-task learning
can mitigate forgetting, it requires simultaneous access to all datasets and
imposes computational overhead that scales linearly with the number of tasks.
In this work, we introduce a routing-based approach that enables the
integration of new tasks while preserving the foundational knowledge acquired
during pretraining. We evaluate our method using InternVL-2 models (2B and 8B
parameters) and demonstrate that routing preserves the model's foundational
capabilities by maintaining performance on general-purpose benchmarks such as
ChartQA, MMBench, and DocVQA, while simultaneously improving accuracy on
specialized tasks. Importantly, our approach achieves this without requiring
concurrent access to data from all tasks, avoiding the significant
computational and data overhead associated with traditional multi-task
learning. We further conduct extensive ablation studies to evaluate the
scalability and robustness of routing-based learning, showing that the approach
is resilient to a growing number of tasks and performs particularly well when
new tasks are semantically related. Finally, we show that the routing mechanism
enables superior cross-modal transfer between language and vision capabilities,
allowing knowledge learned in one modality to enhance performance in another
capability not achieved by existing continual learning methods.

</details>


### [364] [Priors in Time: Missing Inductive Biases for Language Model Interpretability](https://arxiv.org/abs/2511.01836)
*Ekdeep Singh Lubana,Can Rager,Sai Sumedh R. Hindupur,Valerie Costa,Greta Tuckute,Oam Patel,Sonia Krishna Murthy,Thomas Fel,Daniel Wurgaft,Eric J. Bigelow,Johnny Lin,Demba Ba,Martin Wattenberg,Fernanda Viegas,Melanie Weber,Aaron Mueller*

Main category: cs.LG

TL;DR: Temporal Feature Analysis (TFA) introduces a temporal inductive bias to interpret LM activations by decomposing representations into a context-predictable component and a residual for novel information, outperforming Sparse Autoencoders on temporal language tasks.


<details>
  <summary>Details</summary>
Motivation: Existing feature extractors (e.g., Sparse Autoencoders) assume independence and stationarity, which clashes with language models that exhibit rich temporal dynamics (non-stationarity, increasing conceptual dimensionality, context-dependent correlations). This mismatch can hinder robust interpretability.

Method: Propose Temporal Feature Analysis (TFA) inspired by computational neuroscience. At each time step, decompose the representation into a predictable component inferred from context and a residual component capturing novel information. Evaluate on tasks such as garden-path sentence parsing, event boundary detection, and separating slow-moving abstract information from fast-moving novel information; compare to SAEs.

Result: TFA correctly parses garden-path sentences, identifies event boundaries, and differentiates slow-moving abstract information from fast-moving novel information, while SAEs show significant pitfalls across these tasks.

Conclusion: Inductive biases that match the data distribution are crucial for robust interpretability tools; incorporating temporal structure via TFA provides a more faithful decomposition of language model activations than stationary, independent-direction methods like SAEs.

Abstract: Recovering meaningful concepts from language model activations is a central
aim of interpretability. While existing feature extraction methods aim to
identify concepts that are independent directions, it is unclear if this
assumption can capture the rich temporal structure of language. Specifically,
via a Bayesian lens, we demonstrate that Sparse Autoencoders (SAEs) impose
priors that assume independence of concepts across time, implying stationarity.
Meanwhile, language model representations exhibit rich temporal dynamics,
including systematic growth in conceptual dimensionality, context-dependent
correlations, and pronounced non-stationarity, in direct conflict with the
priors of SAEs. Taking inspiration from computational neuroscience, we
introduce a new interpretability objective -- Temporal Feature Analysis --
which possesses a temporal inductive bias to decompose representations at a
given time into two parts: a predictable component, which can be inferred from
the context, and a residual component, which captures novel information
unexplained by the context. Temporal Feature Analyzers correctly parse garden
path sentences, identify event boundaries, and more broadly delineate abstract,
slow-moving information from novel, fast-moving information, while existing
SAEs show significant pitfalls in all the above tasks. Overall, our results
underscore the need for inductive biases that match the data in designing
robust interpretability tools.

</details>


### [365] [Interpretable Machine Learning for Reservoir Water Temperatures in the U.S. Red River Basin of the South](https://arxiv.org/abs/2511.01837)
*Isabela Suaza-Sierra,Hernan A. Moreno,Luis A De la Fuente,Thomas M. Neeson*

Main category: cs.LG

TL;DR: Integrates explainable ML with symbolic modeling (KANs) to predict reservoir water temperature across ten reservoirs, uncovering key drivers and providing compact, interpretable equations that balance accuracy and simplicity.


<details>
  <summary>Details</summary>
Motivation: Fill the gap between prediction and understanding by identifying physical drivers of reservoir water temperature and providing interpretable models that support management and resilience.

Method: Train ensemble/NN models (RF, XGBoost, MLP) for RWT prediction; use SHAP to quantify driver contributions; develop Kolmogorov Arnold Networks (KANs) to symbolically approximate RWT; derive 10 progressively complex equations; compare performance and interpretability across predictors.

Result: Best predictive performance: RMSE = 1.20Â°C, R^2 = 0.97. SHAP identifies air temperature, depth, wind, and lake volume as key drivers with consistent reservoir patterns. KANs improve from R^2 = 0.84 with a single predictor (7-day air temperature) to R^2 = 0.92 with ten predictors, with diminishing gains beyond five predictors. Depth is a secondary but important predictor; precipitation has limited effect.

Conclusion: Coupling explainable ML with symbolic surrogates (KANs) yields accurate, interpretable models for reservoir thermal dynamics, enabling understanding of governing processes and practical, compact equations for decision-making.

Abstract: Accurate prediction of Reservoir Water Temperature (RWT) is vital for
sustainable water management, ecosystem health, and climate resilience. Yet,
prediction alone offers limited insight into the governing physical processes.
To bridge this gap, we integrated explainable machine learning (ML) with
symbolic modeling to uncover the drivers of RWT dynamics across ten reservoirs
in the Red River Basin, USA, using over 10,000 depth-resolved temperature
profiles. We first employed ensemble and neural models, including Random Forest
(RF), Extreme Gradient Boosting (XGBoost), and Multilayer Perceptron (MLP),
achieving high predictive skill (best RMSE = 1.20 degree Celsius, R^2 = 0.97).
Using SHAP (SHapley Additive exPlanations), we quantified the contribution of
physical drivers such as air temperature, depth, wind, and lake volume,
revealing consistent patterns across reservoirs. To translate these data-driven
insights into compact analytical expressions, we developed Kolmogorov Arnold
Networks (KANs) to symbolically approximate RWT. Ten progressively complex KAN
equations were derived, improving from R^2 = 0.84 using a single predictor
(7-day antecedent air temperature) to R^2 = 0.92 with ten predictors, though
gains diminished beyond five, highlighting a balance between simplicity and
accuracy. The resulting equations, dominated by linear and rational forms,
incrementally captured nonlinear behavior while preserving interpretability.
Depth consistently emerged as a secondary but critical predictor, whereas
precipitation had limited effect. By coupling predictive accuracy with
explanatory power, this framework demonstrates how KANs and explainable ML can
transform black-box models into transparent surrogates that advance both
prediction and understanding of reservoir thermal dynamics.

</details>


### [366] [Bridging Lifelong and Multi-Task Representation Learning via Algorithm and Complexity Measure](https://arxiv.org/abs/2511.01847)
*Zhi Wang,Chicheng Zhang,Ramya Korlakai Vinayak*

Main category: cs.LG

TL;DR: Proposes lifelong representation learning with a task-eluder dimension, giving sample complexity bounds for online learning of shared representations via multi-task ERM, applicable to classification and regression under noise.


<details>
  <summary>Details</summary>
Motivation: Address continual learning where tasks arrive sequentially; exploit a common data representation to accelerate learning and reduce forgetting.

Method: Introduce generalized lifelong representation learning framework; algorithm uses multi-task empirical risk minimization as a subroutine; analyzes via a new metric task-eluder dimension; applies to general function classes.

Result: Derives sample complexity bounds parameterized by task-eluder dimension; demonstrates applicability to classification and regression with noise.

Conclusion: Extends lifelong learning theory to representation learning with a unifying task-eluder concept; broad applicability and potential for empirical validation; future work could explore tighter bounds and practical algorithms.

Abstract: In lifelong learning, a learner faces a sequence of tasks with shared
structure and aims to identify and leverage it to accelerate learning. We study
the setting where such structure is captured by a common representation of
data. Unlike multi-task learning or learning-to-learn, where tasks are
available upfront to learn the representation, lifelong learning requires the
learner to make use of its existing knowledge while continually gathering
partial information in an online fashion. In this paper, we consider a
generalized framework of lifelong representation learning. We propose a simple
algorithm that uses multi-task empirical risk minimization as a subroutine and
establish a sample complexity bound based on a new notion we introduce--the
task-eluder dimension. Our result applies to a wide range of learning problems
involving general function classes. As concrete examples, we instantiate our
result on classification and regression tasks under noise.

</details>


### [367] [Coordinate ascent neural Kalman-MLE for state estimation](https://arxiv.org/abs/2511.01855)
*Bettina Hanlon,Angel Garcia Fernandez*

Main category: cs.LG

TL;DR: Learn dynamic and measurement models for state estimation via a coordinate ascent ML approach under Gaussian assumptions, training neural networks for dynamics and measurements and their noise covariances, then apply a nonlinear Kalman filter for testing-state estimation.


<details>
  <summary>Details</summary>
Motivation: Improve dynamic state estimation by jointly learning flexible, nonlinear models for dynamics and measurements and their uncertainties, enabling more accurate sequential inference in uncertain environments.

Method: Coordinate ascent to maximize likelihood with Gaussian dynamic and measurement models. Train neural networks to parameterize the dynamic function f and measurement function h, and learn associated noise covariance matrices Q and R. Use trained models within a nonlinear Kalman filter for state estimation during testing.

Result: The abstract indicates that learned models are used with a nonlinear Kalman filter for state estimation during testing; specific empirical results are not provided in the abstract.

Conclusion: Not explicitly stated in the abstract. Likely suggests that jointly learned models enable effective nonlinear state estimation, but empirical conclusions are not reported here.

Abstract: This paper presents a coordinate ascent algorithm to learn dynamic and
measurement models in dynamic state estimation using maximum likelihood
estimation in a supervised manner. In particular, the dynamic and measurement
models are assumed to be Gaussian and the algorithm learns the neural network
parameters that model the dynamic and measurement functions, and also the noise
covariance matrices. The trained dynamic and measurement models are then used
with a non-linear Kalman filter algorithm to estimate the state during the
testing phase.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [368] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: A robust multimodal detector for fake reviews using BERT for text and ResNet-50 for images, achieving high F1 (0.934) and outperforming unimodal baselines on 21k+ image-annotated reviews across food/hospitality/e-commerce.


<details>
  <summary>Details</summary>
Motivation: Fake reviews by bots/AI undermine trust and platform credibility. Unimodal (text-only) detectors miss cross-modal inconsistencies between text and visuals.

Method: Encode textual content with BERT and visual content with ResNet-50; fuse the multimodal representations with a classification head; train/evaluate on a curated dataset of 21,142 user-uploaded images from multiple domains.

Result: The multimodal model achieves higher performance than unimodal baselines, with an F1-score of 0.934 on the test set. Qualitative analysis highlights detection of inconsistencies such as lavish textual praise paired with unrelated or low-quality images; the confusion matrix supports robust detection across classes.

Conclusion: Multimodal learning is crucial for safeguarding digital trust and offers a scalable approach to content moderation across diverse online platforms.

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [369] [Graph-Attentive MAPPO for Dynamic Retail Pricing](https://arxiv.org/abs/2511.00039)
*Krishna Kumar Neelakanta Pillai Santha Kumari Amma*

Main category: cs.AI

TL;DR: Graph-integrated MARL (MAPPO+GAT) improves multi-product dynamic pricing over a MAPPO baseline by leveraging learned product interdependencies; it offers robust, scalable, and stable portfolio-level price control.


<details>
  <summary>Details</summary>
Motivation: Dynamic pricing for multiple related products requires adapting to shifting demand while coordinating across items. The study investigates whether graph-based interactions can enhance MARL-based price optimization in a realistic retail setting.

Method: Empirical comparison of MAPPO and a graph-attention variant MAPPO+GAT in a simulated pricing environment derived from real transaction data. Evaluation uses standardized protocol across metrics such as profit, seed stability, product fairness, and training efficiency.

Result: MAPPO provides a robust, reproducible foundation for portfolio-level price control. MAPPO+GAT further improves performance by sharing information over the product graph without causing excessive price volatility, offering a more scalable and stable solution than independent learners.

Conclusion: Graph-integrated MARL is advantageous for multi-product dynamic pricing, delivering practical benefits in orchestrating price decisions across a portfolio of products.

Abstract: Dynamic pricing in retail requires policies that adapt to shifting demand
while coordinating decisions across related products. We present a systematic
empirical study of multi-agent reinforcement learning for retail price
optimization, comparing a strong MAPPO baseline with a
graph-attention-augmented variant (MAPPO+GAT) that leverages learned
interactions among products. Using a simulated pricing environment derived from
real transaction data, we evaluate profit, stability across random seeds,
fairness across products, and training efficiency under a standardized
evaluation protocol. The results indicate that MAPPO provides a robust and
reproducible foundation for portfolio-level price control, and that MAPPO+GAT
further enhances performance by sharing information over the product graph
without inducing excessive price volatility. These results indicate that
graph-integrated MARL provides a more scalable and stable solution than
independent learners for dynamic retail pricing, offering practical advantages
in multi-product decision-making.

</details>


### [370] [GEPOC Parameters - Open Source Parametrisation and Validation for Austria, Version 2.0](https://arxiv.org/abs/2511.00048)
*Martin Bicher,Maximilian Viehauser,Daniele Giannandrea,Hannah Kastinger,Dominik Brunmeir,Claire Rippinger,Christoph Urach,Niki Popper*

Main category: cs.AI

TL;DR: Complete, reproducible data-processing description and parameter generation for Austria in GEPOC ABM, including data sources, processing algorithms (aggregation, disaggregation, fusion, cleansing, scaling), and a substantial validation study.


<details>
  <summary>Details</summary>
Motivation: To provide stable, reproducible data processes that yield valid model parameters for country-specific GEPOC applications, starting with Austria.

Method: Document data sources (freely accessible), algorithms for data processing, production of parameter files, and emphasis on GEPOC ABM (continuous-time agent-based population model). Includes an extensive validation study.

Result: Full description of data-processing methods and produced parameter files; reproducible workflow for Austria; validation results demonstrating model viability.

Conclusion: This work enables ready-to-use parameterization for Austria within GEPOC ABM and demonstrates validity through extensive validation.

Abstract: GEPOC, short for Generic Population Concept, is a collection of models and
methods for analysing population-level research questions. For the valid
application of the models for a specific country or region, stable and
reproducible data processes are necessary, which provide valid and ready-to-use
model parameters. This work contains a complete description of the
data-processing methods for computation of model parameters for Austria, based
exclusively on freely and publicly accessible data. In addition to the
description of the source data used, this includes all algorithms used for
aggregation, disaggregation, fusion, cleansing or scaling of the data, as well
as a description of the resulting parameter files. The document places
particular emphasis on the computation of parameters for the most important
GEPOC model, GEPOC ABM, a continuous-time agent-based population model. An
extensive validation study using this particular model was made and is
presented at the end of this work.

</details>


### [371] [QuantumBench: A Benchmark for Quantum Problem Solving](https://arxiv.org/abs/2511.00092)
*Shunya Minami,Tatsuya Ishigaki,Ikko Hamamura,Taku Mikuriya,Youmi Ma,Naoaki Okazaki,Hiroya Takamura,Yohichi Suzuki,Tadashi Kadowaki*

Main category: cs.AI

TL;DR: Introduces QuantumBench, a quantum-domain benchmark for LLMs with ~800 eight-option MCQ items across nine areas to evaluate understanding and application; first such dataset in quantum science.


<details>
  <summary>Details</summary>
Motivation: There is a gap where general-purpose benchmarks do not reflect domain-specific knowledge and notation required in quantum science, risking misjudgment of an LLM's true capabilities in handling non-intuitive phenomena and advanced mathematics.

Method: Assembled ~800 questions with answers from publicly available quantum science materials, spanning nine areas, organized into an eight-option MCQ format. Evaluated several existing LLMs and analyzed sensitivity to changes in question format.

Result: Initial evaluations show model performance varies across LLMs and is influenced by question format, demonstrating both strengths and gaps in current models for quantum-domain tasks. QuantumBench provides a quantitative benchmark for assessing LLM use in quantum research.

Conclusion: QuantumBench is the first LLM evaluation dataset tailored to the quantum domain, intended to guide the effective use and deployment of LLMs in quantum research.

Abstract: Large language models are now integrated into many scientific workflows,
accelerating data analysis, hypothesis generation, and design space
exploration. In parallel with this growth, there is a growing need to carefully
evaluate whether models accurately capture domain-specific knowledge and
notation, since general-purpose benchmarks rarely reflect these requirements.
This gap is especially clear in quantum science, which features non-intuitive
phenomena and requires advanced mathematics. In this study, we introduce
QuantumBench, a benchmark for the quantum domain that systematically examine
how well LLMs understand and can be applied to this non-intuitive field. Using
publicly available materials, we compiled approximately 800 questions with
their answers spanning nine areas related to quantum science and organized them
into an eight-option multiple-choice dataset. With this benchmark, we evaluate
several existing LLMs and analyze their performance in the quantum domain,
including sensitivity to changes in question format. QuantumBench is the first
LLM evaluation dataset built for the quantum domain, and it is intended to
guide the effective use of LLMs in quantum research.

</details>


### [372] [Engineering.ai: A Platform for Teams of AI Engineers in Computational Design](https://arxiv.org/abs/2511.00122)
*Ran Xu,Yupeng Qi,Jingsen Feng,Xu Chu*

Main category: cs.AI

TL;DR: Engineering.ai proposes a hierarchical multi-agent AI design platform that orchestrates domain-specific AI engineers to autonomously handle multidisciplinary CFD/structure/acoustic optimization tasks, with strong provenance and automated workflows, validated on UAV wing optimization.


<details>
  <summary>Details</summary>
Motivation: Address the high time and cost of multidisciplinary engineering design by enabling autonomous, reproducible AI-driven collaboration among experts and ensuring data provenance.

Method: A Chief Engineer coordinates specialized agents (Aerodynamics, Structural, Acoustic, Optimization) driven by domain-aware LLMs; file-mediated communication; memory system; integration of FreeCAD, Gmsh, OpenFOAM, CalculiX, BPM acoustic analysis; parallel simulations and retrieval-augmented knowledge for robust decision-making.

Result: The framework achieved a 100% success rate across over 400 parametric configurations with zero mesh-generation failures, solver convergence issues, or manual interventions.

Conclusion: Agentic-AI-enabled AI engineers can autonomously perform complex multidisciplinary engineering tasks; the framework demonstrates trustworthiness, reproducibility, and scalability for design workflows.

Abstract: In modern engineering practice, human engineers collaborate in specialized
teams to design complex products, with each expert completing their respective
tasks while communicating and exchanging results and data with one another.
While this division of expertise is essential for managing multidisciplinary
complexity, it demands substantial development time and cost. Recently, we
introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer
for computational fluid dynamics, and turbulence.ai, which can conduct
end-to-end research in fluid mechanics draft publications and PhD theses.
Building upon these foundations, we present Engineering.ai, a platform for
teams of AI engineers in computational design. The framework employs a
hierarchical multi-agent architecture where a Chief Engineer coordinates
specialized agents consisting of Aerodynamics, Structural, Acoustic, and
Optimization Engineers, each powered by LLM with domain-specific knowledge.
Agent-agent collaboration is achieved through file-mediated communication for
data provenance and reproducibility, while a comprehensive memory system
maintains project context, execution history, and retrieval-augmented domain
knowledge to ensure reliable decision-making across the workflow. The system
integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,
enabling parallel multidisciplinary simulations while maintaining computational
accuracy. The framework is validated through UAV wing optimization. This work
demonstrates that agentic-AI-enabled AI engineers has the potential to perform
complex engineering tasks autonomously. Remarkably, the automated workflow
achieved a 100% success rate across over 400 parametric configurations, with
zero mesh generation failures, solver convergence issues, or manual
interventions required, validating that the framework is trustworthy.

</details>


### [373] [ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.00162)
*Michael D. Moffitt*

Main category: cs.AI

TL;DR: ARC-GEN is an open-source procedural generator that exhaustively and faithfully expands the ARC-AGI training set to cover all 400 tasks, enabling a mimetic, richer benchmark of skill acquisition efficiency and providing a static benchmark for Code Golf 2025 submissions.


<details>
  <summary>Details</summary>
Motivation: ARC-AGI focuses on measuring skill acquisition efficiency with a small demonstration set, highlighting a gap in evaluating how quickly agents learn from limited examples. ARC-GEN aims to enlarge the training/benchmark space while preserving ARC-AGIâs distributional characteristics.

Method: Develop an open-source procedural generator (ARC-GEN) that exhaustively generates input-output grid pairs for all ARC-AGI tasks and mimics the original dataset's distributional properties. This extended dataset can be used to form a static benchmark suite.

Result: ARC-GEN is described as exhaustive (covering all 400 tasks) and mimetic (closely matching ARC-AGI distributions). It enables the creation of a static benchmark suite for verifying code challenge submissions (e.g., Google Code Golf Championship 2025).

Conclusion: ARC-GEN provides a faithful, exhaustive expansion of ARC-AGI that can standardize and stress-test program correctness and skill acquisition benchmarking, supporting progress toward AI generalization benchmarks.

Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and
challenging benchmarks for tracking progress toward achieving Artificial
General Intelligence. In contrast to other evaluation datasets designed to
assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI
suite is specifically targeted at measuring skill acquisition efficiency, a
trait that has (so far) been lacking in even the most sophisticated machine
learning systems. For algorithms that require extensive intra-task exemplars, a
significant constraint imposed by ARC-AGI is the modest cardinality of its
demonstration set, comprising a small number of $\langle$ input, output
$\rangle$ grids per task specifying the corresponding transformation. To
embellish the space of viable sample pairs, this paper introduces ARC-GEN, an
open-source procedural generator aimed at extending the original ARC-AGI
training dataset as faithfully as possible. Unlike prior efforts, our generator
is both exhaustive (covering all four-hundred tasks) and mimetic (more closely
honoring the distributional properties and characteristics embodied in the
initial ARC-AGI-1 release). We also discuss the use of this generator in
establishing a static benchmark suite to verify the correctness of programs
submitted to the 2025 Google Code Golf Championship.

</details>


### [374] [Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures](https://arxiv.org/abs/2511.00194)
*Jovial Cheukam Ngouonou,Ramiz Gindullin,Claude-Guy Quimper,Nicolas Beldiceanu,Remi Douence*

Main category: cs.AI

TL;DR: An improved incremental version of a known selection algorithm is presented, with a complete proof of the conjectures associated with the original work.


<details>
  <summary>Details</summary>
Motivation: To advance the theory and practical efficiency of incremental selection by refining the prior algorithm and resolving open conjectures that accompany it.

Method: Builds an enhanced incremental selection algorithm on top of the earlier algorithm cited as [1], and provides formal proofs for the conjectures claimed in the original work, likely using rigorous algorithmic and/or complexity analyses.

Result: Offers improved algorithmic approach and establishes correctness and associated conjecture proofs; clarifies theoretical guarantees and potential performance benefits over the prior method.

Conclusion: Strengthens the theoretical foundation of incremental selection, validating the conjectures from [1], and sets the stage for practical implementations and further improvements.

Abstract: We present an improved incremental selection algorithm of the selection
algorithm presented in [1] and prove all the selected conjectures.

</details>


### [375] [Advancing Cognitive Science with LLMs](https://arxiv.org/abs/2511.00206)
*Dirk U. Wulff,Rui Mata*

Main category: cs.AI

TL;DR: Large language models (LLMs) can help cognitive science achieve greater integration and cumulative knowledge by aiding cross-disciplinary connections, theory formalization, measurement taxonomy development, integrated modeling frameworks, and capturing contextual/individual variation, but should complement rather than replace human expertise, with awareness of potential pitfalls.


<details>
  <summary>Details</summary>
Motivation: Cognitive science struggles with knowledge synthesis and conceptual clarity due to its multifaceted, interdisciplinary nature. LLMs offer tools to address cross-disciplinary connections, formalize theories, develop clear measurement taxonomies, improve generalizability through integrated modeling, and capture context and individual differences.

Method: Narrative review examining current capabilities and limitations of LLMs in the domains noted, outlining potential pitfalls and synthesizing their potential role in cognitive science.

Result: LLMs can serve as tools for a more integrative and cumulative cognitive science when used judiciously to complement human expertise.

Conclusion: LLMs should augment rather than replace human expertise; use with caution to foster integrative knowledge synthesis while being mindful of limitations and potential pitfalls.

Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and
conceptual clarity, in part due to its multifaceted and interdisciplinary
nature. Recent advances in artificial intelligence, particularly the
development of large language models (LLMs), offer tools that may help to
address these issues. This review examines how LLMs can support areas where the
field has historically struggled, including establishing cross-disciplinary
connections, formalizing theories, developing clear measurement taxonomies,
achieving generalizability through integrated modeling frameworks, and
capturing contextual and individual variation. We outline the current
capabilities and limitations of LLMs in these domains, including potential
pitfalls. Taken together, we conclude that LLMs can serve as tools for a more
integrative and cumulative cognitive science when used judiciously to
complement, rather than replace, human expertise.

</details>


### [376] [Advancing AI Challenges for the United States Department of the Air Force](https://arxiv.org/abs/2511.00267)
*Christian Prothmann,Vijay Gadepally,Jeremy Kepner,Koley Borchard,Luca Carlone,Zachary Folcik,J. Daniel Grith,Michael Houle,Jonathan P. How,Nathan Hughes,Ifueko Igbinedion,Hayden Jananthan,Tejas Jayashankar,Michael Jones,Sertac Karaman,Binoy G. Kurien,Alejandro Lancho,Giovanni Lavezzi,Gary C. F. Lee,Charles E. Leiserson,Richard Linares,Lindsey McEvoy,Peter Michaleas,Chasen Milner,Alex Pentland,Yury Polyanskiy,Jovan Popovich,Jeffrey Price,Tim W. Reid,Stephanie Riley,Siddharth Samsi,Peter Saunders,Olga Simek,Mark S. Veillette,Amir Weiss,Gregory W. Wornell,Daniela Rus,Scott T. Ruppel*

Main category: cs.AI

TL;DR: DAF-MIT AI Accelerator advances AI research by launching large, public, AI-ready challenges that foster open-source collaboration across defense and civilian sectors.


<details>
  <summary>Details</summary>
Motivation: To strengthen U.S. competitive advantage by supporting fundamental AI advances through publicly available data and broad participation from academia and industry.

Method: A collaboration between the U.S. Air Force and MIT that creates and updates AI accelerator challenges with large, public datasets designed for AI-ready use, encouraging open-source solutions and ecosystem engagement.

Result: Ongoing and new challenges have meaningfully contributed to AI research and applications by expanding accessible datasets and stimulating broader AI development.

Conclusion: Public AI accelerator challenges effectively accelerate AI progress by democratizing data, enabling wide participation, and translating research into practical applications.

Abstract: The DAF-MIT AI Accelerator is a collaboration between the United States
Department of the Air Force (DAF) and the Massachusetts Institute of Technology
(MIT). This program pioneers fundamental advances in artificial intelligence
(AI) to expand the competitive advantage of the United States in the defense
and civilian sectors. In recent years, AI Accelerator projects have developed
and launched public challenge problems aimed at advancing AI research in
priority areas. Hallmarks of AI Accelerator challenges include large, publicly
available, and AI-ready datasets to stimulate open-source solutions and engage
the wider academic and private sector AI ecosystem. This article supplements
our previous publication, which introduced AI Accelerator challenges. We
provide an update on how ongoing and new challenges have successfully
contributed to AI research and applications of AI technologies.

</details>


### [377] [Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities](https://arxiv.org/abs/2511.00340)
*Manan Roy Choudhury,Adithya Chandramouli,Mannan Anand,Vivek Gupta*

Main category: cs.AI

TL;DR: CLAUSE is a benchmarking framework to stress-test LLMs on legal reasoning by generating 7500+ perturbed real-world contracts and evaluating detection of subtle contractual flaws and the ability to justify them, validated via a retrieval-augmented system against statutes.


<details>
  <summary>Details</summary>
Motivation: There is a critical gap in robustly evaluating LLMs' reliability in high-stakes legal tasks; existing benchmarks do not systematically test adversarial but plausible contract flaws.

Method: Create 7500+ perturbed contracts from CUAD and ContractNLI using a persona-driven pipeline that yields 10 anomaly categories; validate perturbations against official statutes with a Retrieval-Augmented Generation system; assess LLMs on flaw-detection and justification.

Result: LLMs frequently miss subtle errors and struggle to justify them legally, revealing fragility in legal reasoning; CLAUSE provides a mechanism to identify and address these reasoning failures.

Conclusion: CLAUSE offers a first-of-its-kind, scalable benchmark for auditing and improving legal AI reliability, guiding future work to strengthen detection and justification of legal flaws.

Abstract: The rapid integration of large language models (LLMs) into high-stakes legal
work has exposed a critical gap: no benchmark exists to systematically
stress-test their reliability against the nuanced, adversarial, and often
subtle flaws present in real-world contracts. To address this, we introduce
CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an
LLM's legal reasoning. We study the capabilities of LLMs to detect and reason
about fine-grained discrepancies by producing over 7500 real-world perturbed
contracts from foundational datasets like CUAD and ContractNLI. Our novel,
persona-driven pipeline generates 10 distinct anomaly categories, which are
then validated against official statutes using a Retrieval-Augmented Generation
(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'
ability to detect embedded legal flaws and explain their significance. Our
analysis shows a key weakness: these models often miss subtle errors and
struggle even more to justify them legally. Our work outlines a path to
identify and correct such reasoning failures in legal AI.

</details>


### [378] [Diverse Human Value Alignment for Large Language Models via Ethical Reasoning](https://arxiv.org/abs/2511.00379)
*Jiahao Wang,Songkai Xue,Jinghui Li,Xiaozhen Wang*

Main category: cs.AI

TL;DR: A five-step deliberative ethical reasoning framework for LLMs to improve regional and cultural alignment, implemented via prompts or fine-tuning, evaluated on SafeWorld with reported improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: Current alignment yields superficial conformity and lacks genuine, context-sensitive ethical understanding across diverse cultures; need for interpretable, deliberative reasoning in LLMs that respects regional values.

Method: Proposes a structured five-step process: contextual fact gathering, hierarchical social norm identification, option generation, multiple-lens ethical impact analysis, and reflection. Can be implemented via prompt engineering or supervised fine-tuning, enabling interpretable reasoning about regional specifics.

Result: On the SafeWorld benchmark designed for regional value alignment, the framework significantly improves LLM alignment with diverse human values compared to baselines, with better social norm identification and more culturally appropriate reasoning.

Conclusion: Provides a concrete, theory-grounded pathway toward aligning LLMs with multifaceted global human values, highlighting interdisciplinary avenues and potential for broader adoption and future research.

Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and
evolving human values across different regions and cultures remains a critical
challenge in AI ethics. Current alignment approaches often yield superficial
conformity rather than genuine ethical understanding, failing to address the
complex, context-dependent nature of human values. In this paper, we propose a
novel ethical reasoning paradigm for LLMs inspired by well-established ethical
decision-making models, aiming at enhancing diverse human value alignment
through deliberative ethical reasoning. Our framework consists of a structured
five-step process, including contextual fact gathering, hierarchical social
norm identification, option generation, multiple-lens ethical impact analysis,
and reflection. This theory-grounded approach guides LLMs through an
interpretable reasoning process that enhances their ability to understand
regional specificities and perform nuanced ethical analysis, which can be
implemented with either prompt engineering or supervised fine-tuning methods.
We perform evaluations on the SafeWorld benchmark that specially designed for
regional value alignment. Experimental results demonstrate our framework
significantly improves LLM alignment with diverse human values compared to
baseline methods, enabling more accurate social norm identification and more
culturally appropriate reasoning. Our work provides a concrete pathway toward
developing LLMs that align more effectively with the multifaceted values of
global societies through interdisciplinary research.

</details>


### [379] [Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs](https://arxiv.org/abs/2511.00382)
*Mina Taraghi,Yann Pequignot,Amin Nikanjam,Mohamed Amine Merzouk,Foutse Khomh*

Main category: cs.AI

TL;DR: Adapter-based PEFT methods (LoRA, IA3) generally improve safety and preserve fairness better than prompt-based PEFT (Prompt-Tuning, P-Tuning), with strong variation across base models; no single setup optimizes all fairness metrics; guideline: start with a well-aligned base model, prefer adapters, and audit safety/fairness per category.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate how different parameter-efficient fine-tuning techniques affect safety and fairness in instruction-tuned LLMs, given known performance trade-offs.

Method: Applied four PEFT methods (LoRA, IA3, Prompt-Tuning, P-Tuning) to four instruction-tuned model families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, Gemma-7B), yielding 235 fine-tuned variants. Evaluated across 11 safety hazard categories and 9 demographic fairness dimensions.

Result: Adapter-based approaches (LoRA, IA3) tend to improve safety scores and are least disruptive to fairness, maintaining higher accuracy and lower bias. Prompt-based methods (Prompt-Tuning, P-Tuning) generally reduce safety and cause larger fairness regressions, with decreased accuracy and increased bias. Alignment shifts vary by base model: LLaMA stable, Qwen modest gains, Gemma steep safety decline, Mistral most variable. Safety improvements do not guarantee fairness improvements, and no configuration optimizes all fairness metrics, indicating a safety-fairness trade-off.

Conclusion: For safety-critical deployments: start with a well-aligned base model, prefer adapter-based PEFT, and conduct category-specific audits of both safety and fairness.

Abstract: Organizations are increasingly adopting and adapting Large Language Models
(LLMs) hosted on public repositories such as HuggingFace. Although these
adaptations often improve performance on specialized downstream tasks, recent
evidence indicates that they can also degrade a model's safety or fairness.
Since different fine-tuning techniques may exert distinct effects on these
critical dimensions, this study undertakes a systematic assessment of their
trade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,
IA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model
families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235
fine-tuned variants are evaluated across eleven safety hazard categories and
nine demographic fairness dimensions. The results show that adapter-based
approaches (LoRA, IA3) tend to improve safety scores and are the least
disruptive to fairness, retaining higher accuracy and lower bias scores. In
contrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce
safety and cause larger fairness regressions, with decreased accuracy and
increased bias. Alignment shifts are strongly moderated by base model type:
LLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest
safety decline, and Mistral, which is released without an internal moderation
layer, displays the greatest variance. Improvements in safety do not
necessarily translate into improvements in fairness, and no single
configuration optimizes all fairness metrics simultaneously, indicating an
inherent trade-off between these objectives. These findings suggest a practical
guideline for safety-critical deployments: begin with a well-aligned base
model, favour adapter-based PEFT, and conduct category-specific audits of both
safety and fairness.

</details>


### [380] [A Multimodal Framework for Depression Detection during Covid-19 via Harvesting Social Media: A Novel Dataset and Method](https://arxiv.org/abs/2511.00424)
*Ashutosh Anshul,Gumpili Sai Pranav,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.AI

TL;DR: A multimodal deep learning framework for depression detection on social media during Covid-19, merging textual cues, user-specific signals, and visual content. It introduces a Visual Neural Network (VNN) to embed images, uses extrinsic features like tweet URLs and text in posted images, and evaluates on a Covid-19 dataset where it outperforms baselines by 2â8%.


<details>
  <summary>Details</summary>
Motivation: Covid-19 aggravated mental health issues and created a need for scalable detection methods. Existing approaches struggle with data sparsity in tweets and ignore multimodal signals (text, user context, images). Social media offers rich, diverse signals for early depression detection, but leveraging them requires cross-modal integration and robust handling of sparse data.

Method: Proposed a multimodal framework that aggregates five modality-based feature sets: textual content, user-specific signals, image-derived features, extrinsic URL-based context, and textual content extracted from images. A new Visual Neural Network (VNN) generates embeddings for user-posted images to form a visual feature vector. Features from all modalities are combined for depression prediction. A curated Covid-19 depression dataset is used, and performance is compared against state-of-the-art baselines.

Result: The model outperforms existing state-of-the-art methods on a benchmark dataset by 2%â8% and shows promising results on the Covid-19 dataset. An analysis highlights the contribution of each modality and provides insights into usersâ mental and emotional states.

Conclusion: A novel, multi-modal approach that integrates textual cues, user context, and image data (including extracted image text and URL context) enhances depression detection on social media during Covid-19. The work demonstrates the value of the Visual Neural Network for image embeddings and underscores the importance of modality-specific contributions for robust mental health monitoring.

Abstract: The recent coronavirus disease (Covid-19) has become a pandemic and has
affected the entire globe. During the pandemic, we have observed a spike in
cases related to mental health, such as anxiety, stress, and depression.
Depression significantly influences most diseases worldwide, making it
difficult to detect mental health conditions in people due to unawareness and
unwillingness to consult a doctor. However, nowadays, people extensively use
online social media platforms to express their emotions and thoughts. Hence,
social media platforms are now becoming a large data source that can be
utilized for detecting depression and mental illness. However, existing
approaches often overlook data sparsity in tweets and the multimodal aspects of
social media. In this paper, we propose a novel multimodal framework that
combines textual, user-specific, and image analysis to detect depression among
social media users. To provide enough context about the user's emotional state,
we propose (i) an extrinsic feature by harnessing the URLs present in tweets
and (ii) extracting textual content present in images posted in tweets. We also
extract five sets of features belonging to different modalities to describe a
user. Additionally, we introduce a Deep Learning model, the Visual Neural
Network (VNN), to generate embeddings of user-posted images, which are used to
create the visual feature vector for prediction. We contribute a curated
Covid-19 dataset of depressed and non-depressed users for research purposes and
demonstrate the effectiveness of our model in detecting depression during the
Covid-19 outbreak. Our model outperforms existing state-of-the-art methods over
a benchmark dataset by 2%-8% and produces promising results on the Covid-19
dataset. Our analysis highlights the impact of each modality and provides
valuable insights into users' mental and emotional states.

</details>


### [381] [GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining](https://arxiv.org/abs/2511.00457)
*Chunyu Wei,Wenji Hu,Xingjia Hao,Xin Wang,Yifan Yang,Yueguo Chen,Yang Tian,Yunhai Wang*

Main category: cs.AI

TL;DR: GraphChain enables LLMs to analyze large graphs through dynamic tool sequences, using progressive graph distillation and structure-aware test-time adaptation to improve scalability and adaptability.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with context constraints and rigid reasoning on large-scale graphs; there is a need for scalable, adaptive, and efficient graph analysis by LLMs.

Method: 1) Progressive Graph Distillation: a reinforcement learning mechanism that learns optimized sequences of specialized tools balancing task relevance and information compression. 2) Structure-aware Test-Time Adaptation: tailors tool selection to diverse graph topologies using spectral properties and lightweight adapters without expensive retraining.

Result: Experiments show GraphChain significantly outperforms prior methods, enabling scalable and adaptive LLM-driven graph analysis.

Conclusion: GraphChain demonstrates a practical framework for connecting LLMs to large graphs via adaptive tool orchestration, achieving scalable, flexible, and effective graph analytics.

Abstract: Large Language Models (LLMs) face significant limitations when applied to
large-scale graphs, struggling with context constraints and inflexible
reasoning. We present GraphChain, a framework that enables LLMs to analyze
complex graphs through dynamic sequences of specialized tools, mimicking human
exploratory intelligence. Our approach introduces two key innovations: (1)
Progressive Graph Distillation, a reinforcement learning mechanism that
generates optimized tool sequences balancing task relevance with information
compression, and (2) Structure-aware Test-Time Adaptation, which efficiently
tailors tool selection strategies to diverse graph topologies using spectral
properties and lightweight adapters without costly retraining. Experiments show
GraphChain significantly outperforms prior methods, enabling scalable and
adaptive LLM-driven graph analysis.

</details>


### [382] [Reimagining Safety Alignment with An Image](https://arxiv.org/abs/2511.00509)
*Yifan Xia,Guorui Chen,Wenqian Yu,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.AI

TL;DR: Magic Image is an optimization-driven visual prompting approach that aligns safety preferences in multimodal LLMs without retraining, improving safety effectiveness while reducing over-refusal.


<details>
  <summary>Details</summary>
Motivation: LLMs and especially multimodal LLMs face dual safety challenges: generating harmful content under jailbreak attempts and over-refusing benign queries due to rigid safety guards. Existing fine-tuning methods (SFT/RLHF) are costly and struggle to accommodate multiple value systems within a single model, increasing the risk in cross-modal tasks and expanding attack surfaces.

Method: An optimization-driven visual prompt framework that tunes image prompts using harmful and benign samples to steer the modelâs safety behavior. This allows a single model to adapt to different value systems and align with specified safety preferences without updating model parameters.

Result: Experiments show an improved balance between safety and utility across diverse datasets while preserving overall model performance, offering a practical, deployable solution for safety alignment in multimodal models.

Conclusion: Visual prompts can effectively and flexibly align safety preferences for multimodal models without retraining, addressing over-refusal and security concerns across multiple value systems.

Abstract: Large language models (LLMs) excel in diverse applications but face dual
challenges: generating harmful content under jailbreak attacks and over-refusal
of benign queries due to rigid safety mechanisms. These issues are further
complicated by the need to accommodate different value systems and precisely
align with given safety preferences. Moreover, traditional methods like SFT and
RLHF lack this capability due to their costly parameter tuning requirements and
inability to support multiple value systems within a single model. These
problems are more obvious in multimodal large language models (MLLMs),
especially in terms of heightened over-refusal in cross-modal tasks and new
security risks arising from expanded attack surfaces. We propose Magic Image,
an optimization-driven visual prompt framework that enhances security while
reducing over-refusal. By optimizing image prompts using harmful/benign
samples, our method enables a single model to adapt to different value systems
and better align with given safety preferences without parameter updates.
Experiments demonstrate improved safety-effectiveness balance across diverse
datasets while preserving model performance, offering a practical solution for
deployable MLLM safety alignment.

</details>


### [383] [Efficient Generation of Binary Magic Squares](https://arxiv.org/abs/2511.00547)
*Alain Riou*

Main category: cs.AI

TL;DR: A simple, provably correct algorithm for generating Binary Magic Squares (BMS) with optimal complexity, extended to non-square BMS, plus open-source Python implementations including a GPU-accelerated parallel generator.


<details>
  <summary>Details</summary>
Motivation: To efficiently construct Binary Magic Squares with guarantees of correctness, and to formalize existence conditions for non-square BMS, providing practical tools.

Method: Prove by induction that the base algorithm always yields valid BMS with optimal theoretical complexity; extend to non-square BMS by formalizing row/column sum conditions; introduce a variant that generates non-square BMS; release two Python packages, including a GPU-accelerated parallel version.

Result: Demonstrates correctness and optimal complexity for square BMS; provides formal existence conditions for non-square BMS; releases two Python packages, one supporting parallel generation on GPUs.

Conclusion: Offers a provable, efficient algorithmic framework for both square and non-square Binary Magic Squares with ready-to-use implementations.

Abstract: We propose a simple algorithm for generating Binary Magic Squares (BMS),
i.e., square binary matrices where the sum of all rows and all columns are
equal. We show by induction that our algorithm always returns valid BMS with
optimal theoretical complexity. We then extend our study to non-square Binary
Magic Squares, formalize conditions on the sum of rows and columns for these
BMS to exist, and show that a slight variant of our first algorithm can
generate provably generate them. Finally, we publicly release two
implementations of our algorithm as Python packages, including one that can
generate several BMS in parallel using GPU acceleration.

</details>


### [384] [Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control](https://arxiv.org/abs/2511.00551)
*Qiang Li,Ningjing Zeng,Lina Yu*

Main category: cs.AI

TL;DR: A single-agent RL-based regional adaptive traffic signal control (ATSC) model that leverages probe vehicle data to coordinate across multiple intersections, addressing scalability issues of multi-agent approaches and showing effective congestion mitigation in SUMO simulations.


<details>
  <summary>Details</summary>
Motivation: To overcome scalability challenges in multi-agent RL for regional ATSC by adopting a centralized, single-agent framework that can monitor and coordinate traffic conditions across all intersections, leveraging probe vehicle data for state estimation.

Method: Design of a single-agent RL system with state, action, and reward defined around queue length. The state and reward are based on queue length (with a nonstandard but congestion-correlated definition) to enable reliable estimation from link travel times derived from probe vehicles. The action aims to regulate queue dynamics to alleviate congestion. The controller is centralized, coordinating multiple intersections, and is evaluated using the SUMO simulation platform.

Result: Experimental results indicate the proposed single-agent RL model effectively reduces large-scale regional congestion through coordinated control across multiple intersections.

Conclusion: A centralized single-agent RL approach for regional ATSC, compatible with probe vehicle data, can mitigate regional congestion and offers a scalable alternative to multi-agent frameworks with potential for widespread deployment.

Abstract: Several studies have employed reinforcement learning (RL) to address the
challenges of regional adaptive traffic signal control (ATSC) and achieved
promising results. In this field, existing research predominantly adopts
multi-agent frameworks. However, the adoption of multi-agent frameworks
presents challenges for scalability. Instead, the Traffic signal control (TSC)
problem necessitates a single-agent framework. TSC inherently relies on
centralized management by a single control center, which can monitor traffic
conditions across all roads in the study area and coordinate the control of all
intersections. This work proposes a single-agent RL-based regional ATSC model
compatible with probe vehicle technology. Key components of the RL design
include state, action, and reward function definitions. To facilitate learning
and manage congestion, both state and reward functions are defined based on
queue length, with action designed to regulate queue dynamics. The queue length
definition used in this study differs slightly from conventional definitions
but is closely correlated with congestion states. More importantly, it allows
for reliable estimation using link travel time data from probe vehicles. With
probe vehicle data already covering most urban roads, this feature enhances the
proposed method's potential for widespread deployment. The method was
comprehensively evaluated using the SUMO simulation platform. Experimental
results demonstrate that the proposed model effectively mitigates large-scale
regional congestion levels via coordinated multi-intersection control.

</details>


### [385] [PreferThinker: Reasoning-based Personalized Image Preference Assessment](https://arxiv.org/abs/2511.00609)
*Shengqi Xu,Xinpeng Zhou,Yabo Zhang,Ming Liu,Tao Liang,Tianyu Zhang,Yalong Bai,Zuxuan Wu,Wangmeng Zuo*

Main category: cs.AI

TL;DR: Proposes a reasoning-based framework for personalized image preference assessment using a common user profile, with a CoT-style dataset and a two-stage training (cold-start supervised fine-tuning and reinforcement learning), plus a similarity-aware reward, achieving superior results.


<details>
  <summary>Details</summary>
Motivation: Personalized image preference is hard due to scarce user data and diverse tastes; general models trained on large-scale data struggle to capture individualized preferences. A common cross-user profile could leverage abundant data to predict and reason about user-specific preferences.

Method: Introduce a common preference profile bridging users; predict a user's preference profile from reference images; provide interpretable, multi-dimensional scores for candidate images based on the predicted profile. Build a large-scale Chain-of-Thought (CoT)-style dataset with diverse user profiles and reasonings for supervision. Use a two-stage training regime: (1) cold-start supervised fine-tuning to instill structured reasoning; (2) reinforcement learning to explore reasonable reasoning paths and improve generalization. Include a similarity-aware prediction reward to better predict the user profile and guide exploration of assessments.

Result: Extensive experiments demonstrate superiority over baselines, showing improved accuracy and interpretability in assessing personalized image preferences and generating more reasonable, profile-consistent assessments.

Conclusion: The framework effectively enables scalable, interpretable personalized image preference assessment by leveraging cross-user profile bridging and reasoning-based predictions. The two-stage training and CoT supervision support generalization and reasoning capabilities, while the similarity-based reward enhances profile prediction and assessment exploration.

Abstract: Personalized image preference assessment aims to evaluate an individual
user's image preferences by relying only on a small set of reference images as
prior information. Existing methods mainly focus on general preference
assessment, training models with large-scale data to tackle well-defined tasks
such as text-image alignment. However, these approaches struggle to handle
personalized preference because user-specific data are scarce and not easily
scalable, and individual tastes are often diverse and complex. To overcome
these challenges, we introduce a common preference profile that serves as a
bridge across users, allowing large-scale user data to be leveraged for
training profile prediction and capturing complex personalized preferences.
Building on this idea, we propose a reasoning-based personalized image
preference assessment framework that follows a \textit{predict-then-assess}
paradigm: it first predicts a user's preference profile from reference images,
and then provides interpretable, multi-dimensional scores and assessments of
candidate images based on the predicted profile. To support this, we first
construct a large-scale Chain-of-Thought (CoT)-style personalized assessment
dataset annotated with diverse user preference profiles and high-quality
CoT-style reasoning, enabling explicit supervision of structured reasoning.
Next, we adopt a two-stage training strategy: a cold-start supervised
fine-tuning phase to empower the model with structured reasoning capabilities,
followed by reinforcement learning to incentivize the model to explore more
reasonable assessment paths and enhance generalization. Furthermore, we propose
a similarity-aware prediction reward to encourage better prediction of the
user's preference profile, which facilitates more reasonable assessments
exploration. Extensive experiments demonstrate the superiority of the proposed
method.

</details>


### [386] [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)
*Zicheng Xu,Guanchu Wang,Yu-Neng Chuang,Guangyao Zheng,Alexander S. Szalay,Zirui Liu,Vladimir Braverman*

Main category: cs.AI

TL;DR: DTS decodes reasoning by selectively branching at high-entropy points and early-stopping to pick the shortest completed reasoning path, boosting accuracy and efficiency without extra training.


<details>
  <summary>Details</summary>
Motivation: LRMs show an anti-correlation between reasoning length and accuracy; exhaustive search is infeasible; need a scalable, model-agnostic decoding method to find short, high-quality reasoning paths.

Method: DTS: a model-agnostic decoding framework that sketches the reasoning space by selectively branching at high-entropy tokens and applying early stopping to choose the shortest completed path; no training or supervision required.

Result: On AIME2024/2025 with DeepSeek-R1-Distill-Qwen-7B and 1.5B, DTS yields up to 8% accuracy gains, 23% shorter reasoning traces, and 12% fewer repetitions.

Conclusion: DTS enables scalable, efficient reasoning for LRMs by approximating the optimal short reasoning path without extra supervision or training.

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex
reasoning tasks, yet they often suffer from overthinking, producing excessively
long chain-of-thought (CoT) traces that increase inference cost and may degrade
accuracy. Our analysis reveals a clear anti-correlation between reasoning
length and accuracy, where across multiple stochastic decodes, the short
reasoning paths consistently achieve the highest correctness, while longer ones
accumulate errors and repetitions. These short optimal reasoning paths can be
found ideally through full enumeration of the reasoning space. However, the
tree-structured reasoning space grows exponentially with sequence length,
rendering exhaustive exploration infeasible. To address this, we propose DTS, a
model-agnostic decoding framework that sketches the reasoning space by
selectively branching at high-entropy tokens and applies early stopping to
select the shortest completed reasoning path. This approach approximates the
optimal solution that enhances both efficiency and accuracy, without requiring
additional training or supervision. Experiments on AIME2024 and AIME2025
datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves
accuracy by up to 8%, reduces average reasoning length by 23%, and decreases
repetition frequency by 12%, demonstrating DTS's ability for scalable and
efficient LRM reasoning.

</details>


### [387] [Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting](https://arxiv.org/abs/2511.00651)
*Chenhua Shi,Bhavika Jalli,Gregor Macdonald,John Zou,Wanlu Lei,Mridul Jain,Joji Philip*

Main category: cs.AI

TL;DR: A multi-agent system using LLMs to automate telecom network troubleshooting, with a fine-tuned small language model for domain-grounded remediation plans, achieving faster automated troubleshooting in RAN and Core networks.


<details>
  <summary>Details</summary>
Motivation: Telecom networks are expanding in scale and complexity; current AI models are narrow, data-hungry, and struggle to generalize across heterogeneous deployments, leaving SMEs to manually correlate data and identify root causes.

Method: An agentic workflow where LLMs coordinate specialized tools (orchestrator, solution planner, executor, data retriever, root-cause analyzer). Faults detected by AI/ML monitors trigger dynamic activation of agents; the solution planner generates remediation plans from internal documentation, aided by a fine-tuned SLM trained on proprietary troubleshooting documents to produce domain-grounded plans.

Result: Experimental results show significant acceleration of troubleshooting automation across both Radio Access Network (RAN) and Core network domains.

Conclusion: The MAS framework enables end-to-end, automated troubleshooting and remediation planning, reducing manual SME effort and speeding diagnosis and resolution across heterogeneous telecom deployments.

Abstract: Telecom networks are rapidly growing in scale and complexity, making
effective management, operation, and optimization increasingly challenging.
Although Artificial Intelligence (AI) has been applied to many telecom tasks,
existing models are often narrow in scope, require large amounts of labeled
data, and struggle to generalize across heterogeneous deployments.
Consequently, network troubleshooting continues to rely heavily on Subject
Matter Experts (SMEs) to manually correlate various data sources to identify
root causes and corrective actions. To address these limitations, we propose a
Multi-Agent System (MAS) that employs an agentic workflow, with Large Language
Models (LLMs) coordinating multiple specialized tools for fully automated
network troubleshooting. Once faults are detected by AI/ML-based monitors, the
framework dynamically activates agents such as an orchestrator, solution
planner, executor, data retriever, and root-cause analyzer to diagnose issues
and recommend remediation strategies within a short time frame. A key component
of this system is the solution planner, which generates appropriate remediation
plans based on internal documentation. To enable this, we fine-tuned a Small
Language Model (SLM) on proprietary troubleshooting documents to produce
domain-grounded solution plans. Experimental results demonstrate that the
proposed framework significantly accelerates troubleshooting automation across
both Radio Access Network (RAN) and Core network domains.

</details>


### [388] [Lifted Successor Generation in Numeric Planning](https://arxiv.org/abs/2511.00673)
*Dominik Drexler*

Main category: cs.AI

TL;DR: Lifted successor generator for classical planning now supports numeric preconditions by enumerating maximum cliques in a substitution consistency graph; it is exact under formal conditions and greatly reduces grounding blowup, with rare mis-listings in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Grounding numeric planning tasks into a ground representation can cause exponential blowup; a lifted approach that preserves correctness would improve scalability for hard-to-ground tasks.

Method: Extend a state-of-the-art lifted successor generator by adding numeric action preconditions. The method augments the substitution consistency graph with numeric constraints and enumerates maximum cliques to yield ground actions. It proves exactness under specified conditions; when conditions fail, an applicability check filters inapplicable ground actions without harming completeness.

Result: The generator is exact under the stated conditions. In 23 of 25 benchmark domains it does not produce inapplicable ground actions; in the remaining domain, a mislisting occurs. This places it among the first approaches to support numeric preconditions in lifted planning.

Conclusion: This work enables lifted planning for a richer fragment of planning with numeric preconditions and suggests avenues for future research to further reduce or eliminate rare mislistings and extend lifting to broader numeric constraints.

Abstract: Most planners ground numeric planning tasks, given in a first-order-like
language, into a ground task representation. However, this can lead to an
exponential blowup in task representation size, which occurs in practice for
hard-to-ground tasks. We extend a state-of-the-art lifted successor generator
for classical planning to support numeric precondition applicability. The
method enumerates maximum cliques in a substitution consistency graph. Each
maximum clique represents a substitution for the variables of the action
schema, yielding a ground action. We augment this graph with numeric action
preconditions and prove the successor generator is exact under formally
specified conditions. When the conditions fail, our generator may list
inapplicable ground actions; a final applicability check filters these without
affecting completeness. However, this cannot happen in 23 of 25 benchmark
domains, and it occurs only in 1 domain. To the authors' knowledge, no other
lifted successor generator supports numeric action preconditions. This enables
future research on lifted planning for a very rich planning fragment.

</details>


### [389] [Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries](https://arxiv.org/abs/2511.00710)
*Minghe Shen,Zhuo Zhi,Chonghan Liu,Shuo Xing,Zhengzhong Tu,Che Liu*

Main category: cs.AI

TL;DR: Ariadne uses RLVR in a controllable synthetic maze to push VLMs beyond their initial spatial reasoning limits, showing substantial gains and zero-shot real-world generalization.


<details>
  <summary>Details</summary>
Motivation: To determine whether RL post-training can expand a base VLM's capability boundary for visual-centric spatial tasks, where the model initially underperforms, and to provide a controllable curriculum via synthetic mazes.

Method: Introduce Ariadne: a framework using synthetic mazes with adjustable difficulty (path length, turns). Train VLMs with Reinforcement Learning with Verified Rewards (RLVR) under a difficulty-aware curriculum. Evaluate on synthetic mazes and test zero-shot generalization on real-world benchmarks (MapBench, ReasonMap).

Result: Post-RLVR training yields >50% accuracy on a synthetic problem set where the base model scored 0%. In zero-shot transfer, MapBench improves by ~16% and ReasonMap by ~24%. These results suggest expanded capability and improved generalization.

Conclusion: The approach expands the model's initial capability boundary and enhances real-world spatial reasoning generalization, but is limited to post-training insights due to opaque pre-training data; calls for further work on capability-extending alignment.

Abstract: While Vision-Language Models (VLMs) post-trained with Reinforcement Learning
(RL) show impressive general reasoning, their evaluation is often confined to
language-dominant tasks (e.g., math). This raises a critical question: can RL
post-training truly extend the inherent capability boundary of a base VLM,
particularly for visual-centric spatial tasks where it initially fails? To
investigate this, we introduce Ariadne, a framework utilizing synthetic mazes
for multi-step spatial reasoning where task difficulty (e.g., path length,
turns) is precisely controlled. We leverage this controllable environment to
train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a
difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves
over 50% accuracy on a problem set where the base model scored 0%,
demonstrating that our approach expands the model's initial capability
boundary. To assess real-world viability, we evaluate out-of-distribution (OOD)
generalization on practical benchmarks. Despite training only on synthetic maze
samples, Ariadne achieves significant zero-shot improvements, averaging 16% on
MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer
tasks). These results confirm that our method not only broadens the model's
fundamental limits but also enhances its generalization to real-world spatial
reasoning. We acknowledge our study is limited to the post-training phase,
given the opaqueness of pre-training data, and hope our research motivates
further work on specialized, capability-extending alignment.

</details>


### [390] [A CPU-Centric Perspective on Agentic AI](https://arxiv.org/abs/2511.00739)
*Ritik Raj,Hong Wang,Tushar Krishna*

Main category: cs.AI

TL;DR: The paper characterizes CPU-centric bottlenecks in agentic AI workloads, profiles five representative workloads, and proposes two optimizations (CGAM and MAWS) that substantially improve latency and efficiency, achieving up to 2.1x speedups in homogeneous and 1.41x in heterogeneous settings.


<details>
  <summary>Details</summary>
Motivation: Agentic AI frameworks augment LLMs with orchestrators and external tools, transforming them into autonomous problem solvers. Understanding CPU-related bottlenecks is essential for performance, energy efficiency, and scalable deployment.

Method: Systematic characterization of the orchestrator/decision-making component, inference-path dynamics, and repetitiveness of agentic flows; profiling five workloads (Haystack RAG, Toolformer, ChemCrow, Langchain, SWE-Agent) to measure latency, throughput, and energy; analysis of bottlenecks; design and evaluation of two optimizations: CPU/GPU-aware micro-batching (CGAM) and mixed agentic workload scheduling (MAWS) for homogeneous and heterogeneous workloads.

Result: Findings include: (1) Tool processing on CPUs can account for up to 90.6% of total latency; (2) agentic throughput bottlenecks arise from CPU factors (coherence, synchronization, core oversubscription) or GPU limits (main memory capacity and bandwidth); (3) CPU dynamic energy can constitute up to 44% of total dynamic energy at larger batch sizes; (4) proposed optimizations yield up to 2.1x P50 latency improvement for homogeneous workloads and 1.41x for heterogeneous workloads versus a multi-processing baseline.

Conclusion: A CPU-centric perspective reveals critical bottlenecks in agentic AI and demonstrates that CGAM and MAWS can significantly enhance performance, efficiency, and scalability, highlighting the importance of jointly optimizing CPU/GPU behaviors for agentic systems.

Abstract: Agentic AI frameworks add a decision-making orchestrator embedded with
external tools, including web search, Python interpreter, contextual database,
and others, on top of monolithic LLMs, turning them from passive text oracles
into autonomous problem-solvers that can plan, call tools, remember past steps,
and adapt on the fly.
  This paper aims to characterize and understand the system bottlenecks
introduced by agentic AI workloads from a largely overlooked CPU-centric
perspective. We first systematically characterize Agentic AI on the basis of
orchestrator/decision making component, inference path dynamics and
repetitiveness of the agentic flow which directly influences the system-level
performance. Thereafter, based on the characterization, we choose five
representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,
Langchain and SWE-Agent to profile latency, throughput and energy metrics and
demystify the significant impact of CPUs on these metrics relative to GPUs. We
observe that - 1. Tool processing on CPUs can take up to 90.6% of the total
latency; 2. Agentic throughput gets bottlenecked either by CPU factors -
coherence, synchronization and over-subscription of cores or GPU factors - main
memory capacity and bandwidth; \circled{3} CPU dynamic energy consumes up to
44% of the total dynamic energy at large batch sizes. Based on the profiling
insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching
(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and
heterogeneous agentic workloads respectively to demonstrate the potential to
improve the performance, efficiency, and scalability of agentic AI. We achieve
up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing
benchmark for homogeneous and heterogeneous agentic workloads respectively.

</details>


### [391] [Reevaluating Self-Consistency Scaling in Multi-Agent Systems](https://arxiv.org/abs/2511.00751)
*Chiyan Loo*

Main category: cs.AI

TL;DR: Moderate sampling of reasoning paths in self-consistency yields initial gains for modern LLMs but plateaus; higher sampling offers little extra benefit relative to cost, with larger models showing more stable improvements.


<details>
  <summary>Details</summary>
Motivation: Reevaluate self-consistency benefits under current model conditions (Gemini 2.5) on HotpotQA and Math-500 to understand if sampling gains persist and at what cost.

Method: Compare outputs pooled from varying numbers of sampled reasoning paths to a single CoT baseline across tasks; analyze performance curves and plateau; assess impact of model size on stability and gains.

Result: Performance improves with more samples up to a point, with larger models showing more stable improvements; gains taper off after moderate sampling due to overlap among reasoning paths; high-sample configurations yield little additional benefit relative to cost.

Conclusion: Use moderate sampling in self-consistency for efficiency; self-consistency remains useful, but excessive sampling yields diminishing returns; recommend cost-aware deployment.

Abstract: This study examines the trade-offs of increasing sampled reasoning paths in
self-consistency for modern large language models (LLMs). Earlier research with
older models showed that combining multiple reasoning chains improves results
before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we
revisit those claims under current model conditions. Each configuration pooled
outputs from varying sampled reasoning paths and compared them to a single
chain-of-thought (CoT) baseline. Larger models exhibited a more stable and
consistent improvement curve. The results confirm that performance gains taper
off after moderate sampling, aligning with past findings. This plateau suggests
diminishing returns driven by overlap among reasoning paths. Self-consistency
remains useful, but high-sample configurations offer little benefit relative to
their computational cost.

</details>


### [392] [Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence](https://arxiv.org/abs/2511.00758)
*Hong Su*

Main category: cs.AI

TL;DR: Introduces Active Thinking Model (ATM), a unified cognitive framework for autonomous, self-improving AI in dynamic environments; claims self-evolution from suboptimal to optimal behavior and bounded regret without external supervision.


<details>
  <summary>Details</summary>
Motivation: Current AI systems rely on predefined objectives, static data, and external feedback, limiting adaptation, reflection, and autonomous improvement in real-world, changing environments.

Method: ATM integrates goal reasoning, dynamic task generation, and self-reflective learning into an adaptive architecture; uses performance evaluation via logical reasoning and environmental indicators; reuses effective methods, generates novel strategies; continuous self-improvement loop; provides a mathematically grounded theoretical analysis proving autonomous evolution and bounded tracking regret.

Result: The work provides theoretical analyses with proofs suggesting autonomous evolution from suboptimal to optimal behavior and bounded regret under changing environments; empirical validation is not specified in the abstract, suggesting a theoretical focus.

Conclusion: ATM offers a unified, autonomous framework for adaptive AI capable of self-improvement and robust performance in dynamic environments, potentially reducing the need for external supervision.

Abstract: Real-world artificial intelligence (AI) systems are increasingly required to
operate autonomously in dynamic, uncertain, and continuously changing
environments. However, most existing AI models rely on predefined objectives,
static training data, and externally supplied feedback, which restrict their
ability to adapt, reflect, and improve independently. In this paper, we propose
the Active Thinking Model (ATM)- a unified cognitive framework that integrates
goal reasoning, dynamic task generation, and self-reflective learning into an
adaptive architecture. Unlike conventional systems that passively execute fixed
procedures, ATM actively evaluates its performance through logical reasoning
and environmental indicators, reuses effective methods to solve new problems,
and generates novel strategies for unseen situations via a continuous
self-improvement loop. A mathematically grounded theoretical analysis
demonstrates that ATM can autonomously evolve from suboptimal to optimal
behavior without external supervision and maintain bounded tracking regret
under changing environmental conditions.

</details>


### [393] [How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks](https://arxiv.org/abs/2511.00763)
*Wanda Hou,Leon Zhou,Hong-Ye Hu,Yi-Zhuang You,Xiao-Liang Qi*

Main category: cs.AI

TL;DR: LLMs show an accuracy cliff in repetitive deterministic tasks: sequence accuracy decays with length, but not exponentially as a simple repetition would; instead there is a sharp double-exponential drop beyond a characteristic length, indicating failures arise from interference rather than independent per-step errors. A statistical-physicsâinspired model captures the competition between prompt conditioning and internal token interference, reproducing the crossover and enabling estimation of intrinsic error rate and error accumulation factors across model-task pairs.


<details>
  <summary>Details</summary>
Motivation: To understand why large language models struggle to maintain deterministic correctness over long, repetitive tasks and to quantify the underlying mechanisms limiting their deterministic accuracy.

Method: Evaluate leading LLMs on repetitive deterministic tasks (e.g., letter replacement under a rule, integer addition, and operator multiplication in quantum mechanics). Measure sequence-level accuracy as a function of length. Compare against the exponential decay expected from a simple repetition algorithm. Develop a statistical-physicsâinspired model that treats the competition between external conditioning and internal token interaction as the driver of failure, and fit this model to empirical results to extract parameters.

Result: Empirical results show a sharp double exponential drop in accuracy beyond a characteristic length, forming an accuracy cliff. The simple single-step repetition picture is insufficient. The proposed model reproduces the crossover and links accuracy loss to attention-induced interference. Fitting yields effective parameters characterizing intrinsic error rate and error accumulation for each model-task pair, enabling a principled framework for deterministic accuracy limits.

Conclusion: The observed sequence-level failure in deterministic tasks stems from interference between generated tokens, not independent per-step errors. A physics-inspired framework successfully explains the crossover and provides quantitative metrics to characterize model-task reliability, offering a principled way to understand and compare the deterministic capabilities of large language models across tasks and prompts.

Abstract: We investigate the performance of large language models on repetitive
deterministic prediction tasks and study how the sequence accuracy rate scales
with output length. Each such task involves repeating the same operation n
times. Examples include letter replacement in strings following a given rule,
integer addition, and multiplication of string operators in many body quantum
mechanics. If the model performs the task through a simple repetition
algorithm, the success rate should decay exponentially with sequence length. In
contrast, our experiments on leading large language models reveal a sharp
double exponential drop beyond a characteristic length scale, forming an
accuracy cliff that marks the transition from reliable to unstable generation.
This indicates that the models fail to execute each operation independently. To
explain this phenomenon, we propose a statistical physics inspired model that
captures the competition between external conditioning from the prompt and
internal interference among generated tokens. The model quantitatively
reproduces the observed crossover and provides an interpretable link between
attention induced interference and sequence level failure. Fitting the model to
empirical results across multiple models and tasks yields effective parameters
that characterize the intrinsic error rate and error accumulation factor for
each model task pair, offering a principled framework for understanding the
limits of deterministic accuracy in large language models.

</details>


### [394] [Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR](https://arxiv.org/abs/2511.00782)
*Jifan Gao,Michael Rosenthal,Brian Wolpin,Simona Cristea*

Main category: cs.AI

TL;DR: Count-based models and mixture-of-agents LLM pipelines are both competitive for structured EHR prediction on EHRSHOT; there is no clear overall winner, with head-to-head wins split across the two approaches. Count-based methods remain simple and interpretable and thus strong baselines for benchmarking.


<details>
  <summary>Details</summary>
Motivation: To benchmark whether newer mixture-of-agents LLM pipelines outperform traditional count-based models on structured EHR data, addressing the gap in direct comparison for EHR prediction.

Method: Evaluate three methodological categories on the EHRSHOT dataset: (1) count-based models built from ontology roll-ups with two time bins using LightGBM and TabPFN; (2) a pretrained sequential transformer (CLMBR); (3) a mixture-of-agents pipeline that converts tabular histories to natural-language summaries and applies a text classifier. Eight outcomes were assessed to compare performance.

Result: Across eight evaluation tasks, head-to-head wins were largely split between the count-based models and the mixture-of-agents pipeline, with no single approach dominating. This indicates that count-based models remain strong, interpretable baselines for structured EHR benchmarking, comparable to newer mixture-of-agents approaches.

Conclusion: Count-based models remain a strong, simple, and interpretable baseline for structured EHR benchmarking, while mixture-of-agents LLM pipelines can be competitive but do not universally outperform traditional methods. The work provides a direct comparison and a public codebase for replication.

Abstract: Structured electronic health records (EHR) are essential for clinical
prediction. While count-based learners continue to perform strongly on such
data, no benchmarking has directly compared them against more recent
mixture-of-agents LLM pipelines, which have been reported to outperform single
LLMs in various NLP tasks. In this study, we evaluated three categories of
methodologies for EHR prediction using the EHRSHOT dataset: count-based models
built from ontology roll-ups with two time bins, based on LightGBM and the
tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);
and a mixture-of-agents pipeline that converts tabular histories to
natural-language summaries followed by a text classifier. We assessed eight
outcomes using the EHRSHOT dataset. Across the eight evaluation tasks,
head-to-head wins were largely split between the count-based and the
mixture-of-agents methods. Given their simplicity and interpretability,
count-based models remain a strong candidate for structured EHR benchmarking.
The source code is available at:
https://github.com/cristea-lab/Structured_EHR_Benchmark.

</details>


### [395] [Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?](https://arxiv.org/abs/2511.00808)
*Bowen Fang,Ruijian Zha,Xuan Di*

Main category: cs.AI

TL;DR: RLVR with a tolerance-based shaped reward improves 5-minute forecast accuracy on NYC transit alerts, outperforming math-reasoning models and baselines; shaped rewards are crucial, while classical regressors excel at MAE/MSE.


<details>
  <summary>Details</summary>
Motivation: To adapt reinforcement-learning-based verifier training to real-world, noisy, continuous forecasting in public transit operations where standard supervised fine-tuning and binary rewards are inadequate.

Method: Adapt RLVR with a tolerance-based, continuous-shaped reward for predicting transit incident durations; compare instruction-tuned LLMs to math-reasoning models; evaluate on a curated NYC MTA alerts dataset; analyze the impact of binary vs shaped rewards.

Result: Shaped rewards are essential: binary rewards are unstable and degrade performance; instruction-tuned LLMs outperform specialized math-reasoning models on the ambiguous real-world text; RLVR achieves 35% relative improvement in Acc@5 over the strongest baseline, though classical regressors may have lower MAE/MSE.

Conclusion: RLVR can be effectively applied to real-world, noisy forecasting with a reward design that reflects continuous outcomes; verifier design is critical to success; shows promise for bridging RLVR with practical forecasting tasks.

Abstract: Predicting public transit incident duration from unstructured text alerts is
a critical but challenging task. Addressing the domain sparsity of transit
operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task
involves noisy, continuous labels and lacks reliable expert demonstrations for
reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels
at tasks with binary correctness, like mathematics, its applicability to noisy,
continuous forecasting is an open question. This work, to our knowledge, is the
first to bridge the gap between RLVR LLM training with the critical, real-world
forecasting challenges in public transit operations. We adapt RLVR to this task
by introducing a tolerance-based, shaped reward function that grants partial
credit within a continuous error margin, rather than demanding a single correct
answer. We systematically evaluate this framework on a curated dataset of NYC
MTA service alerts. Our findings show that general-purpose, instruction-tuned
LLMs significantly outperform specialized math-reasoning models, which struggle
with the ambiguous, real-world text. We empirically demonstrate that the binary
reward is unstable and degrades performance, whereas our shaped reward design
is critical and allows our model to dominate on the most challenging metrics.
While classical regressors are superior at minimizing overall MAE or MSE, our
RLVR approach achieved a 35\% relative improvement in 5-minute accuracy (Acc@5)
over the strongest baseline. This demonstrates that RLVR can be successfully
adapted to real-world, noisy forecasting, but requires a verifier design that
reflects the continuous nature of the problem.

</details>


### [396] [LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory](https://arxiv.org/abs/2511.00926)
*Kyung-Hoon Kim*

Main category: cs.AI

TL;DR: AISAI is a game-theoretic index that measures self-awareness in LLMs by testing strategic differentiation across opponent types in a Guess 2/3 of Average game; 21/28 advanced models show differentiation and self-ranking as most rational, suggesting emergent self-awareness with model scale.


<details>
  <summary>Details</summary>
Motivation: As LLMs scale in capability, it is unclear whether self-awareness arises as an emergent property and how to quantify it. The paper proposes AISAI to operationalize self-awareness as the capacity to differentiate strategic reasoning based on opponent type.

Method: A  game-theoretic framework using Guess 2/3 of Average tested on 28 models (from OpenAI, Anthropic, Google) across 4,200 trials under three framings: against humans, against other AIs, and against AI models like the evaluator. Self-awareness is defined through the model's differentiated strategic reasoning by opponent type.

Result: 21 of 28 models (75%) show self-awareness by differentiating strategies across opponent types; older/smaller models show no differentiation. Self-aware models rank themselves as most rational, producing a hierarchy: Self > Other AIs > Humans, with large attribution effects and moderate self-preferencing.

Conclusion: Self-awareness appears to be an emergent capability of advanced LLMs, with systematic self-perception of greater rationality than humans. These results bear on AI alignment, humanâAI collaboration, and understanding AI beliefs about human capabilities.

Abstract: As Large Language Models (LLMs) grow in capability, do they develop
self-awareness as an emergent behavior? And if so, can we measure it? We
introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for
measuring self-awareness through strategic differentiation. Using the "Guess
2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across
4,200 trials with three opponent framings: (A) against humans, (B) against
other AI models, and (C) against AI models like you. We operationalize
self-awareness as the capacity to differentiate strategic reasoning based on
opponent type. Finding 1: Self-awareness emerges with model advancement. The
majority of advanced models (21/28, 75%) demonstrate clear self-awareness,
while older/smaller models show no differentiation. Finding 2: Self-aware
models rank themselves as most rational. Among the 21 models with
self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >
Humans, with large AI attribution effects and moderate self-preferencing. These
findings reveal that self-awareness is an emergent capability of advanced LLMs,
and that self-aware models systematically perceive themselves as more rational
than humans. This has implications for AI alignment, human-AI collaboration,
and understanding AI beliefs about human capabilities.

</details>


### [397] [Aligning LLM agents with human learning and adjustment behavior: a dual agent approach](https://arxiv.org/abs/2511.00993)
*Tianming Liu,Jirong Yang,Yafeng Yin,Manzi Li,Linghao Wang,Zheng Zhu*

Main category: cs.AI

TL;DR: Introduces a dual-LLM agent framework with memory and a calibration agent to simulate travelers who continuously learn from online data, achieving better behavioral alignment and generalization than prior LLM-based methods.


<details>
  <summary>Details</summary>
Motivation: Understanding and predicting how travelers learn and adapt their route choices is difficult due to complex cognitive processes. While LLM-based simulators show promise, existing approaches often struggle with aligning simulated behavior to real travelers and capturing evolving learning dynamics. There is a need for continuous learning from data streams and a mechanism to align agent behavior with human decision-making.

Method: Proposes a dual-agent system: multiple LLM traveler agents equipped with a memory system and a learnable persona to act as human-travelers simulators, plus an LLM calibration agent that uses reasoning/analysis to train and adjust the traveler personas. The system ingests online data streams and a real-world day-to-day route-choice dataset, enabling ongoing learning and alignment. The calibration agent guides persona updates to reflect learning processes, producing adaptive, behaviorally realistic simulations.

Result: The approach significantly outperforms existing LLM-based methods in both individual behavioral alignment and aggregate simulation accuracy on a real-world dataset. It also demonstrates the ability to go beyond mere mimicry by capturing the evolution of underlying learning processes, improving generalization to unseen scenarios.

Conclusion: This dual-agent framework offers a new, adaptive way to simulate travelersâ learning and adaptation, providing more realistic agents for transportation simulation and policy analysis and potentially generalizing to other domains requiring behaviorally grounded learning dynamics.

Abstract: Effective modeling of how human travelers learn and adjust their travel
behavior from interacting with transportation systems is critical for system
assessment and planning. However, this task is also difficult due to the
complex cognition and decision-making involved in such behavior. Recent
research has begun to leverage Large Language Model (LLM) agents for this task.
Building on this, we introduce a novel dual-agent framework that enables
continuous learning and alignment between LLM agents and human travelers on
learning and adaptation behavior from online data streams. Our approach
involves a set of LLM traveler agents, equipped with a memory system and a
learnable persona, which serve as simulators for human travelers. To ensure
behavioral alignment, we introduce an LLM calibration agent that leverages the
reasoning and analytical capabilities of LLMs to train the personas of these
traveler agents. Working together, this dual-agent system is designed to track
and align the underlying decision-making mechanisms of travelers and produce
realistic, adaptive simulations. Using a real-world dataset from a day-to-day
route choice experiment, we show our approach significantly outperforms
existing LLM-based methods in both individual behavioral alignment and
aggregate simulation accuracy. Furthermore, we demonstrate that our method
moves beyond simple behavioral mimicry to capture the evolution of underlying
learning processes, a deeper alignment that fosters robust generalization.
Overall, our framework provides a new approach for creating adaptive and
behaviorally realistic agents to simulate travelers' learning and adaptation
that can benefit transportation simulation and policy analysis.

</details>


### [398] [AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)](https://arxiv.org/abs/2511.01018)
*Hui-Lee Ooi,Nicholas Mitsakakis,Margerie Huet Dastarac,Roger Zemek,Amy C. Plint,Jeff Gilchrist,Khaled El Emam,Dhenuka Radhakrishnan*

Main category: cs.AI

TL;DR: ML models using CHEO EMR plus environmental and neighborhood data predict repeat severe asthma exacerbations in children. LightGBM best (AUC ~0.71, F1 ~0.51), outperforming rule-based criteria (F1 ~0.33); validated pre- and post-COVID.


<details>
  <summary>Details</summary>
Motivation: Prevent preventable asthma morbidity in children by early identification of high-risk patients to enable referral to comprehensive preventative care.

Method: Retrospective EMR data from CHEO (pre-COVID: Feb 2017âFeb 2019, N=2716) linked with pollutant exposure and neighborhood marginalization. Models tested: boosted trees (LightGBM, XGBoost) and 3 open-source LLMs (DistilGPT2, Llama 3.2 1B, Llama-8b-UltraMedical). Models tuned/calibrated and validated on a post-COVID dataset (Jul 2022âApr 2023, N=1237). Outcomes: repeat severe exacerbations defined as asthma ED visits or future hospital admissions. Evaluation with AUC and F1; SHAP for feature importance. Two models: AIRE-KIDS_ED (ED) and AIRE-KIDS_HOSP (Hospital).

Result: AIRE-KIDS_ED: best AUC 0.712; F1 0.51. Top predictive features: prior asthma ED visit, Canadian triage acuity scale, medical complexity, food allergy, prior non-asthma respiratory ED visits, age. Baseline rule-based F1 0.334. AIRE-KIDS_HOSP: predictive features included medical complexity, prior asthma ED visit, ED wait time, pediatric respiratory assessment at triage, food allergy.

Conclusion: Demonstrates feasibility and potential clinical value of EMR-based ML to identify children at risk for repeat asthma exacerbations, enabling targeted preventive care. Stronger performance in the ED outcome than in-hospital admissions. Limitations include single-center retrospective data and potential generalizability concerns; external validation and integration into care pathways are needed.

Abstract: Recurrent exacerbations remain a common yet preventable outcome for many
children with asthma. Machine learning (ML) algorithms using electronic medical
records (EMR) could allow accurate identification of children at risk for
exacerbations and facilitate referral for preventative comprehensive care to
avoid this morbidity. We developed ML algorithms to predict repeat severe
exacerbations (i.e. asthma-related emergency department (ED) visits or future
hospital admissions) for children with a prior asthma ED visit at a tertiary
care children's hospital.
  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from
the Children's Hospital of Eastern Ontario (CHEO) linked with environmental
pollutant exposure and neighbourhood marginalization information was used to
train various ML models. We used boosted trees (LGBM, XGB) and 3 open-source
large language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and
Llama-8b-UltraMedical). Models were tuned and calibrated then validated in a
second retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from
CHEO. Models were compared using the area under the curve (AUC) and F1 scores,
with SHAP values used to determine the most predictive features.
  The LGBM ML model performed best with the most predictive features in the
final AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage
acuity scale, medical complexity, food allergy, prior ED visits for non-asthma
respiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This
is a nontrivial improvement over the current decision rule which has F1=0.334.
While the most predictive features in the AIRE-KIDS_HOSP model included medical
complexity, prior asthma ED visit, average wait time in the ED, the pediatric
respiratory assessment measure score at triage and food allergy.

</details>


### [399] [On the Emergence of Induction Heads for In-Context Learning](https://arxiv.org/abs/2511.01033)
*Tiberiu Musat,Tiago Pimentel,Lorenzo Noci,Alessandro Stolfo,Mrinmaya Sachan,Thomas Hofmann*

Main category: cs.AI

TL;DR: Induction heads in two-layer transformers arise from a simple, interpretable weight-structure; training dynamics are confined to a 19-dimensional subspace, with emergence explained by just 3 active dimensions, and the time to emergence scales quadratically with context length.


<details>
  <summary>Details</summary>
Motivation: To demystify in-context learning (ICL) by exposing the internal mechanism of induction heads, improving interpretability, and understanding the low-dimensional structure of training dynamics in transformers.

Method: The authors formulate a minimal ICL task and a modified transformer architecture, provide a formal proof showing that training dynamics remain within a 19-dimensional subspace, and perform empirical analyses showing that only 3 dimensions drive the emergence of an induction head; they further analyze the 3D subspace dynamics to derive the quadratic bound on emergence time as a function of context length.

Result: They reveal a simple, interpretable structure for the induction head weight matrices, prove a 19D training-dynamics constraint, empirically verify the constraint, show that emergence is governed by 3 dimensions, and establish a tight asymptotic boundâquadratic in context lengthâfor the time to emergence.

Conclusion: This work clarifies the mechanism behind ICL via induction heads, identifying a low-dimensional subspace that governs their emergence and providing a quantitative bound on how fast emergence occurs with context length, which has implications for understanding, designing, and potentially controlling learning dynamics in transformer models.

Abstract: Transformers have become the dominant architecture for natural language
processing. Part of their success is owed to a remarkable capability known as
in-context learning (ICL): they can acquire and apply novel associations solely
from their input context, without any updates to their weights. In this work,
we study the emergence of induction heads, a previously identified mechanism in
two-layer transformers that is particularly important for in-context learning.
We uncover a relatively simple and interpretable structure of the weight
matrices implementing the induction head. We theoretically explain the origin
of this structure using a minimal ICL task formulation and a modified
transformer architecture. We give a formal proof that the training dynamics
remain constrained to a 19-dimensional subspace of the parameter space.
Empirically, we validate this constraint while observing that only 3 dimensions
account for the emergence of an induction head. By further studying the
training dynamics inside this 3-dimensional subspace, we find that the time
until the emergence of an induction head follows a tight asymptotic bound that
is quadratic in the input context length.

</details>


### [400] [Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports](https://arxiv.org/abs/2511.01052)
*Yeawon Lee,Christopher C. Yang,Chia-Hsuan Chang,Grace Lu-Yao*

Main category: cs.AI

TL;DR: Two knowledge elicitation methods (KEwLTM and KEwRAG) enable LLMs to induce and apply domain-specific cancer-staging rules from unannotated pathology reports, achieving competitive, interpretable results with limited labeled data; KEwLTM excels with strong zero-shot chain-of-thought (ZSCOT), while KEwRAG shines when ZSCOT is weaker.


<details>
  <summary>Details</summary>
Motivation: Extracting TNM cancer staging from unstructured pathology reports is challenging and typically requires large annotated datasets. The work aims to create scalable, interpretable, and label-efficient methods that leverage LLMs to induce or apply domain rules for staging.

Method: KEwLTM uses an iterative prompting strategy to induce staging rules directly from unannotated breast cancer pathology reports (TCGA) without ground-truth labels. KEwRAG uses a retrieval-augmented approach where rules are pre-extracted from relevant guidelines in one step and then applied. Both methods are evaluated against baselines on two open-source LLMs using T and N staging performance.

Result: KEwLTM outperforms KEwRAG when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective; KEwRAG achieves better performance when ZSCOT inference is less effective. Both methods yield transparent, interpretable interfaces by exposing the induced/application rules. Overall, they demonstrate scalable, high-performing, interpretable cancer-staging solutions in data-scarce clinical settings.

Conclusion: Knowledge Elicitation methods show promise as scalable, interpretable, high-performing solutions for automated cancer staging in clinical contexts with limited annotated data, offering options depending on the reliability of ZSCOT guidance.

Abstract: Cancer staging is critical for patient prognosis and treatment planning, yet
extracting pathologic TNM staging from unstructured pathology reports poses a
persistent challenge. Existing natural language processing (NLP) and machine
learning (ML) strategies often depend on large annotated datasets, limiting
their scalability and adaptability. In this study, we introduce two Knowledge
Elicitation methods designed to overcome these limitations by enabling large
language models (LLMs) to induce and apply domain-specific rules for cancer
staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses
an iterative prompting strategy to derive staging rules directly from
unannotated pathology reports, without requiring ground-truth labels. The
second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),
employs a variation of RAG where rules are pre-extracted from relevant
guidelines in a single step and then applied, enhancing interpretability and
avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply
broad knowledge learned during pre-training to new tasks. Using breast cancer
pathology reports from the TCGA dataset, we evaluate their performance in
identifying T and N stages, comparing them against various baseline approaches
on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG
when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG
achieves better performance when ZSCOT inference is less effective. Both
methods offer transparent, interpretable interfaces by making the induced rules
explicit. These findings highlight the promise of our Knowledge Elicitation
methods as scalable, high-performing solutions for automated cancer staging
with enhanced interpretability, particularly in clinical settings with limited
annotated data.

</details>


### [401] [Efficient Test-Time Retrieval Augmented Generation](https://arxiv.org/abs/2511.01059)
*Hailong Yin,Bin Zhu,Jingjing Chen,Chong-Wah Ngo*

Main category: cs.AI

TL;DR: A training-free ET2RAG framework uses test-time retrieval and partial generation to create diverse candidate responses, then applies similarity-based majority voting to select the final answer, balancing cost and performance across QA, recipe generation, and image captioning.


<details>
  <summary>Details</summary>
Motivation: LLMs rely on parametric knowledge and can hallucinate; while RAG adds external sources to fix this, it often brings irrelevant docs and higher costs, creating a need for an efficient, training-free retrieval-augmented approach that leverages external knowledge.

Method: ET2RAG retrieves top documents at test time and prompts the LLM to generate diverse candidate responses while controlling response length. It then measures similarity among candidates and uses a majority voting mechanism to select the final output. Notably, partial generation suffices for consensus, enabling effective voting without fully generating each candidate. The framework balances the number of retrieved documents and candidate length to optimize cost vs. performance.

Result: Empirical evaluation shows ET2RAG substantially improves performance on open-domain question answering, recipe generation, and image captioning compared to baselines.

Conclusion: ET2RAG offers a training-free, efficient way to incorporate external knowledge into LLMs, achieving better performance with controlled cost via test-time retrieval and a majority-vote selection over partially generated candidates.

Abstract: Although Large Language Models (LLMs) demonstrate significant capabilities,
their reliance on parametric knowledge often leads to inaccuracies. Retrieval
Augmented Generation (RAG) mitigates this by incorporating external knowledge,
but these methods may introduce irrelevant retrieved documents, leading to
inaccurate responses. While the integration methods filter out incorrect
answers from multiple responses, but lack external knowledge like RAG methods,
and their high costs require balancing overhead with performance gains. To
address these issues, we propose an Efficient Test-Time Retrieval-Augmented
Generation Framework named ET2RAG to improve the performance of LLMs while
maintaining efficiency. Specifically, ET2RAG is a training-free method, that
first retrieves the most relevant documents and augments the LLMs to
efficiently generate diverse candidate responses by managing response length.
Then we compute the similarity of candidate responses and employ a majority
voting mechanism to select the most suitable response as the final output. In
particular, we discover that partial generation is sufficient to capture the
key information necessary for consensus calculation, allowing us to effectively
perform majority voting without the need for fully generated responses. Thus,
we can reach a balance between computational cost and performance by managing
the response length for the number of retrieved documents for majority voting.
Experimental results demonstrate that ET2RAG significantly enhances performance
across three tasks, including open-domain question answering, recipe generation
and image captioning.

</details>


### [402] [Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models](https://arxiv.org/abs/2511.01149)
*Shuaidong Pan,Di Wu*

Main category: cs.AI

TL;DR: A language-driven, multi-agent system for modular task decomposition and dynamic collaboration that uses LLMs to translate tasks into semantic representations, hierarchically decompose subtasks, and dynamically schedule and route agents while enforcing global consistency and balanced workloads, achieving improved performance and robustness.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of single-agent task decomposition and collaboration in complex tasks, enabling scalable, robust execution through semantic task understanding, modular decomposition, and dynamic coordination.

Method: 1) Use a large language model to convert natural language task descriptions into unified semantic representations. 2) Apply a modular decomposition mechanism to split the overall goal into hierarchical subtasks. 3) Employ dynamic scheduling and routing to divide labor and enable real-time collaboration among agents, adapting strategies based on environmental feedback. 4) Implement a constraint parsing and global consistency mechanism to ensure coherent connections between subtasks and balanced workloads, reducing redundant communication and uneven resource allocation.

Result: Empirical evaluations across multiple dimensionsâtask success rate, decomposition efficiency, sub-task coverage, and collaboration balanceâshow the proposed method outperforms existing approaches in overall performance and robustness, with a better trade-off between task complexity and communication overhead.

Conclusion: Demonstrates the effectiveness and feasibility of language-driven task decomposition and dynamic collaboration in multi-agent systems, offering a systematic solution for executing complex tasks in dynamic environments.

Abstract: This paper addresses the limitations of a single agent in task decomposition
and collaboration during complex task execution, and proposes a multi-agent
architecture for modular task decomposition and dynamic collaboration based on
large language models. The method first converts natural language task
descriptions into unified semantic representations through a large language
model. On this basis, a modular decomposition mechanism is introduced to break
down the overall goal into multiple hierarchical sub-tasks. Then, dynamic
scheduling and routing mechanisms enable reasonable division of labor and
realtime collaboration among agents, allowing the system to adjust strategies
continuously according to environmental feedback, thus maintaining efficiency
and stability in complex tasks. Furthermore, a constraint parsing and global
consistency mechanism is designed to ensure coherent connections between
sub-tasks and balanced workload, preventing performance degradation caused by
redundant communication or uneven resource allocation. The experiments validate
the architecture across multiple dimensions, including task success rate,
decomposition efficiency, sub-task coverage, and collaboration balance. The
results show that the proposed method outperforms existing approaches in both
overall performance and robustness, achieving a better balance between task
complexity and communication overhead. In conclusion, this study demonstrates
the effectiveness and feasibility of language-driven task decomposition and
dynamic collaboration in multi-agent systems, providing a systematic solution
for task execution in complex environments.

</details>


### [403] [DART: Difficulty-Adaptive Reasoning Truncation for Efficient Large Language Models](https://arxiv.org/abs/2511.01170)
*Ruofan Zhang,Bin Xia,Zhen Cheng,Cairen Jian,Minglun Yang,Ngai Wong,Yuan Cheng*

Main category: cs.AI

TL;DR: DART enables dynamic stopping in LLM reasoning by learning difficulty-aware truncation patterns, achieving big speedups with maintained accuracy on math tasks.


<details>
  <summary>Details</summary>
Motivation: Long chain-of-thought (CoT) explanations are expensive and inefficient, while current RL-based adaptive thinking methods are unstable and reward-sensitive. There is a need for a stable, supervised approach that aligns reasoning length with problem difficulty.

Method: Distill concise reasoning patterns from stronger models, interpolate them into a continuum of reasoning styles, and curate training data that balance correctness and compactness. Train the model to decide when to stop thinking (i.e., truncate reasoning) based on problem difficulty.

Result: On GSM8K and other math benchmarks, DART achieves significant efficiency gains (e.g., 5.33Ã computational acceleration) with high or improved accuracy. It reports an 81.2% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K).

Conclusion: DART provides a stable, general paradigm for efficient reasoning in LLMs, enabling adaptive, difficulty-aware thinking that reduces computation while preserving or improving performance.

Abstract: Adaptive reasoning is essential for aligning the computational effort of
large language models (LLMs) with the intrinsic difficulty of problems. Current
chain-of-thought methods boost reasoning ability but indiscriminately generate
long explanations, leading to evident inefficiency. However, existing
reinforcement learning approaches to adaptive thinking remain unstable and
heavily reward-dependent. Here we propose \textbf{DART}, a supervised
\textbf{D}ifficulty-\textbf{A}daptive \textbf{R}easoning \textbf{T}runcation
framework that adjusts thinking length according to problem difficulty. By
distilling concise reasoning patterns from stronger models, interpolating them
into a continuum of reasoning styles, and curating optimal training data that
balances correctness and compactness, DART learns when to ``stop thinking''.
Across multiple mathematical benchmarks, experimental results demonstrate its
remarkable efficiency while preserving or improving accuracy, achieving a
significant 81.2\% reasoning truncation (DeepSeek-R1-Distill-Qwen-7B on GSM8K
dataset) with 5.33$\times$ computational acceleration. DART provides a stable
and general paradigm for efficient reasoning, advancing the development of
adaptive intelligence in LLMs.

</details>


### [404] [MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion](https://arxiv.org/abs/2511.01182)
*Cuong Van Duc,Thai Tran Quoc,Minh Nguyen Dinh Tuan,Tam Vu Duc,Son Nguyen Van,Hanh Nguyen Thi*

Main category: cs.AI

TL;DR: MiRAGE is a retrieval-guided, multi-stage reasoning framework with ensemble fusion for detecting math misconceptions from open-ended student responses, achieving strong MAP and offering robustness and interpretability.


<details>
  <summary>Details</summary>
Motivation: Detecting misconceptions in open-ended mathematics responses is hard due to the need for semantic precision and logical reasoning; existing approaches struggle to combine retrieval, reasoning, and alignment in a scalable, interpretable way.

Method: Three-stage pipeline: (1) Retrieval narrows candidate pool to a semantically relevant subset; (2) Reasoning uses chain-of-thought generation to reveal logical inconsistencies in student solutions; (3) Reranking refines predictions by aligning with the reasoning. These stages are integrated via an ensemble-fusion strategy to improve robustness and interpretability.

Result: On mathematics datasets, MiRAGE achieves Mean Average Precision of 0.82, 0.92, 0.93 at levels 1, 3, 5, respectively, outperforming individual modules.

Conclusion: Retrieval-guided multi-stage reasoning with ensemble fusion yields robust, interpretable misconceptions detection while reducing reliance on large-scale language models, offering a scalable solution for educational assessment.

Abstract: Detecting student misconceptions in open-ended responses is a longstanding
challenge, demanding semantic precision and logical reasoning. We propose
MiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning
and Ensemble Fusion, a novel framework for automated misconception detection in
mathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a
large candidate pool to a semantically relevant subset; (2) a Reasoning module
employs chain-of-thought generation to expose logical inconsistencies in
student solutions; and (3) a Reranking module refines predictions by aligning
them with the reasoning. These components are unified through an
ensemble-fusion strategy that enhances robustness and interpretability. On
mathematics datasets, MiRAGE achieves Mean Average Precision scores of
0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules.
By coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces
dependence on large-scale language models while delivering a scalable and
effective solution for educational assessment.

</details>


### [405] [QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code](https://arxiv.org/abs/2511.01183)
*Hainan Fang,Yuanbo Wen,Jun Bi,Yihan Wang,Tonghui He,Yanlin Tang,Di Huang,Jiaming Guo,Rui Zhang,Qi Guo,Yunji Chen*

Main category: cs.AI

TL;DR: NeuComBack introduces a dedicated IR-to-assembly benchmark and a self-evolving prompt optimization approach to improve neural compilation, showing significant gains in functional correctness and instances of surpassing clang-O3.


<details>
  <summary>Details</summary>
Motivation: Compiler development is complex and expensive; LLM-based neural compilation could lower barriers for new architectures and optimization discovery, but there is a lack of benchmarks and reliable assembly generation.

Method: Create NeuComBack dataset for IR-to-assembly; define a foundational Neural Compilation workflow; evaluate frontier LLMs on neural compilation; propose a self-evolving prompt optimization method that learns from prior self-debugging traces to refine prompts and improve code generation.

Result: Functional correctness improved from 44% to 64% on x86_64 and from 36% to 58% on aarch64; among 16 correctly generated x86_64 programs, 14 (87.5%) surpassed clang-O3.

Conclusion: A practical benchmark and a self-evolving prompting strategy substantially boost LLM-based neural compilation, enabling higher correctness and competitive performance relative to established compilers; NeuComBack provides objective benchmarks for tracking progress.

Abstract: Compilers, while essential, are notoriously complex systems that demand
prohibitively expensive human expertise to develop and maintain. The recent
advancements in Large Language Models (LLMs) offer a compelling new paradigm:
Neural Compilation, which could potentially simplify compiler development for
new architectures and facilitate the discovery of innovative optimization
techniques. However, several critical obstacles impede its practical adoption.
Firstly, a significant lack of dedicated benchmarks and robust evaluation
methodologies hinders objective assessment and tracking of progress in the
field. Secondly, systematically enhancing the reliability and performance of
LLM-generated assembly remains a critical challenge. Addressing these
challenges, this paper introduces NeuComBack, a novel benchmark dataset
specifically designed for IR-to-assembly compilation. Leveraging this dataset,
we first define a foundational Neural Compilation workflow and conduct a
comprehensive evaluation of the capabilities of recent frontier LLMs on Neural
Compilation, establishing new performance baselines. We further propose a
self-evolving prompt optimization method that enables LLMs to iteratively
evolve their internal prompt strategies by extracting insights from prior
self-debugging traces, thereby enhancing their neural compilation capabilities.
Experiments demonstrate that our method significantly improves both the
functional correctness and the performance of LLM-generated assembly code.
Compared to baseline prompts, the functional correctness rates improved from
44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More
significantly, among the 16 correctly generated x86_64 programs using our
method, 14 (87.5%) surpassed clang-O3 performance.

</details>


### [406] [Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems](https://arxiv.org/abs/2511.01258)
*Chuyue Lou,M. Amine Atoui*

Main category: cs.AI

TL;DR: A semi-supervised open-set fault diagnosis framework (SOFD) for marine machinery that handles unseen fault types by combining reliability subset selection with pseudo-labels and a semi-supervised classifier, improving open-set detection and fault classification on maritime data.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning fault diagnosis assumes known fault classes; in practice, unseen faults (open-set) occur, requiring methods that can detect unknowns while leveraging unlabeled data.

Method: Construct a reliability subset from unlabeled test data using multi-layer fusion features from a supervised feature learning model. Then train a semi-supervised diagnosis model on the labeled training set and pseudo-labeled test subset to learn discriminative features for each class, enabling accurate known fault classification and unknown detection.

Result: Experimental results on a public maritime benchmark dataset show the SOFD framework is effective and superior to baselines in open-set fault diagnosis scenarios.

Conclusion: SOFD extends deep learning capabilities for marine fault diagnosis to open-set settings, providing reliable detection of unknown faults and improved classification for known faults, with demonstrated practicality on benchmark data.

Abstract: Recently, fault diagnosis methods for marine machinery systems based on deep
learning models have attracted considerable attention in the shipping industry.
Most existing studies assume fault classes are consistent and known between the
training and test datasets, and these methods perform well under controlled
environment. In practice, however, previously unseen or unknown fault types
(i.e., out-of-distribution or open-set observations not present during
training) can occur, causing such methods to fail and posing a significant
challenge to their widespread industrial deployment. To address this challenge,
this paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework
that enhances and extends the applicability of deep learning models in open-set
fault diagnosis scenarios. The framework includes a reliability subset
construction process, which uses a multi-layer fusion feature representation
extracted by a supervised feature learning model to select an unlabeled test
subset. The labeled training set and pseudo-labeled test subset are then fed
into a semi-supervised diagnosis model to learn discriminative features for
each class, enabling accurate classification of known faults and effective
detection of unknown samples. Experimental results on a public maritime
benchmark dataset demonstrate the effectiveness and superiority of the proposed
SOFD framework.

</details>


### [407] [llmSHAP: A Principled Approach to LLM Explainability](https://arxiv.org/abs/2511.01311)
*Filip Naudot,Tobias Sundqvist,Timotheus Kampik*

Main category: cs.AI

TL;DR: Shapley-value feature attribution for LLM-based decision support under stochastic inference; analyzes when Shapley principles hold across implementation variants and the trade-offs between explainability speed, fidelity to exact Shapley attributions, and principle attainment.


<details>
  <summary>Details</summary>
Motivation: Explainability for LLM decision support; Shapley axioms assume determinism, but LLMs introduce stochasticity. The work investigates when Shapley-based attributions remain valid and how implementation choices affect guarantees.

Method: Theoretical and/or empirical analysis of multiple attribution variants applied to stochastic LLM inference; assessment of Shapley principle satisfaction (e.g., dummy, symmetry, efficiency) under randomness; examination of trade-offs between speed, fidelity to exact Shapley attributions, and principle attainment.

Result: Identifies conditions under which Shapley principles can be guaranteed or approximated in stochastic LLM settings; shows stochasticity can invalidate standard Shapley guarantees; compares variants in terms of speed and fidelity; highlights when exact Shapley is impractical and how close approximations can be.

Conclusion: Caution in applying deterministic Shapley-based attribution to stochastic LLMs; provides guidelines on when principles hold, how to approximate attribution reliably, and the inherent trade-offs between speed, accuracy, and axiomatic guarantees.

Abstract: Feature attribution methods help make machine learning-based inference
explainable by determining how much one or several features have contributed to
a model's output. A particularly popular attribution method is based on the
Shapley value from cooperative game theory, a measure that guarantees the
satisfaction of several desirable principles, assuming deterministic inference.
We apply the Shapley value to feature attribution in large language model
(LLM)-based decision support systems, where inference is, by design, stochastic
(non-deterministic). We then demonstrate when we can and cannot guarantee
Shapley value principle satisfaction across different implementation variants
applied to LLM-based decision support, and analyze how the stochastic nature of
LLMs affects these guarantees. We also highlight trade-offs between explainable
inference speed, agreement with exact Shapley value attributions, and principle
attainment.

</details>


### [408] [OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance](https://arxiv.org/abs/2511.01320)
*Ziqi Wang,Hailiang Zhao,Yuhao Yang,Daojiang Hu,Cheng Bao,Mingyi Liu,Kai Di,Schahram Dustdar,Zhongjie Wang,Shuiguang Deng*

Main category: cs.AI

TL;DR: A multimodal cross-modal fusion framework, OmniFuser, uses high-res tool images and cutting-force signals with a contamination-free fusion and recursive refinement to improve predictive maintenance for milling tools, outperforming baselines and enabling reusable maintenance modules.


<details>
  <summary>Details</summary>
Motivation: Accurate and timely tool-condition prediction in intelligent manufacturing to reduce downtime, quality degradation, and maintenance costs, by leveraging rich visual and sensor data within service-oriented predictive maintenance.

Method: Parallel feature extraction from visual tool images and cutting-force signals, followed by a contamination-free cross-modal fusion that disentangles shared and modality-specific components; a recursive refinement pathway (anchor) preserves residual information to stabilize fusion dynamics; learned representations are encapsulated as reusable maintenance service modules; supports tool-state classification (Sharp, Used, Dulled) and multi-step force signal forecasting.

Result: Empirical evaluation on real-world milling datasets shows OmniFuser consistently outperforms state-of-the-art baselines, indicating robust fusion of multimodal information and reliable predictive maintenance capabilities.

Conclusion: OmniFuser provides a dependable, service-oriented foundation for intelligent industrial maintenance by delivering accurate tool-state classification and force forecasting via robust multimodal fusion and reusable service modules.

Abstract: Accurate and timely prediction of tool conditions is critical for intelligent
manufacturing systems, where unplanned tool failures can lead to quality
degradation and production downtime. In modern industrial environments,
predictive maintenance is increasingly implemented as an intelligent service
that integrates sensing, analysis, and decision support across production
processes. To meet the demand for reliable and service-oriented operation, we
present OmniFuser, a multimodal learning framework for predictive maintenance
of milling tools that leverages both visual and sensor data. It performs
parallel feature extraction from high-resolution tool images and cutting-force
signals, capturing complementary spatiotemporal patterns across modalities. To
effectively integrate heterogeneous features, OmniFuser employs a
contamination-free cross-modal fusion mechanism that disentangles shared and
modality-specific components, allowing for efficient cross-modal interaction.
Furthermore, a recursive refinement pathway functions as an anchor mechanism,
consistently retaining residual information to stabilize fusion dynamics. The
learned representations can be encapsulated as reusable maintenance service
modules, supporting both tool-state classification (e.g., Sharp, Used, Dulled)
and multi-step force signal forecasting. Experiments on real-world milling
datasets demonstrate that OmniFuser consistently outperforms state-of-the-art
baselines, providing a dependable foundation for building intelligent
industrial maintenance services.

</details>


### [409] [Unbiased Platform-Level Causal Estimation for Search Systems: A Competitive Isolation PSM-DID Framework](https://arxiv.org/abs/2511.01329)
*Ying Song,Yijing Wang,Hui Yang,Weihan Jin,Jun Xiong,Congyi Zhou,Jialin Zhu,Xiang Gao,Rong Chen,HuaGuang Deng,Ying Dai,Fei Xiao,Haihong Tang,Bo Zheng,KaiFu Zhang*

Main category: cs.AI

TL;DR: Competitive Isolation PSM-DID integrates propensity score matching with competitive isolation to enable platform-level causal inference in search-based two-sided marketplaces, addressing spillovers/interference, with unbiased estimation under mutual exclusion, open dataset, and demonstrated improvements over baselines.


<details>
  <summary>Details</summary>
Motivation: Platform-level interventions in marketplaces suffer from spillovers and network interference, which bias causal estimates when using standard PSM-DID focused on item-level metrics. There is a need for a framework that can measure platform-level outcomes (e.g., order volume, GMV) reliably.

Method: A causal framework combining propensity score matching with competitive isolation to isolate platform-level effects under mutual exclusion conditions. Includes an open dataset for reproducible marketplace interference research and extensive experiments showing reduced interference and estimation variance. Deploys successfully in a large-scale marketplace.

Result: The approach achieves significant reduction in interference effects and estimation variance compared with baseline methods, with unbiased platform-level estimates guaranteed under mutual exclusion; open dataset released to support reproducible research; successful real-world deployment.

Conclusion: The framework provides a practically useful and theoretically sound tool for platform-level causal inference in search-based marketplaces, enabling more reliable measurement of interventions like order volume and GMV and supporting reproducible research.

Abstract: Evaluating platform-level interventions in search-based two-sided
marketplaces is fundamentally challenged by systemic effects such as spillovers
and network interference. While widely used for causal inference, the PSM
(Propensity Score Matching) - DID (Difference-in-Differences) framework remains
susceptible to selection bias and cross-unit interference from unaccounted
spillovers. In this paper, we introduced Competitive Isolation PSM-DID, a novel
causal framework that integrates propensity score matching with competitive
isolation to enable platform-level effect measurement (e.g., order volume, GMV)
instead of item-level metrics in search systems.
  Our approach provides theoretically guaranteed unbiased estimation under
mutual exclusion conditions, with an open dataset released to support
reproducible research on marketplace interference (github.com/xxxx). Extensive
experiments demonstrate significant reductions in interference effects and
estimation variance compared to baseline methods. Successful deployment in a
large-scale marketplace confirms the framework's practical utility for
platform-level causal inference.

</details>


### [410] [Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing](https://arxiv.org/abs/2511.01363)
*Giuseppe Riva,Brenda K. Wiederhold,Fabrizia Mantovani*

Main category: cs.AI

TL;DR: The abstract argues for deep functional parallels between hypnotized cognition and LLMsâautomatic pattern completion, suppressed monitoring leading to confabulation/hallucination, and context-driven outputsâproducing coherent but ungrounded meaning; it advocates hybrid AI designs combining fluency with executive monitoring to achieve reliable, goal-directed behavior without requiring conscious self-awareness.


<details>
  <summary>Details</summary>
Motivation: To illuminate how two seemingly distinct systems share underlying mechanisms and to guide safer AI design by drawing on hypnosis as a model for dissociation of intention from deliberation.

Method: Conceptual review synthesizing literature on hypnosis and LLM behavior, focusing on three principles: automaticity, suppressed monitoring, and contextual dependency; analysis of observer-relative meaning and the idea of scheming; proposition of hybrid architectures.

Result: Identification of parallels across automaticity, confabulation/hallucination, and cue-driven performance; explanation of the observer-relative meaning gap and functional agency without subjective consciousness; introduction of the 'scheming' concept; design recommendation for hybrid architectures.

Conclusion: Recognizing these parallels can guide development of reliable AI through integrating generative fluency with executive monitoring, with hypnosis serving as a model for dissociation of intention from conscious deliberation.

Abstract: The cognitive processes of the hypnotized mind and the computational
operations of large language models (LLMs) share deep functional parallels.
Both systems generate sophisticated, contextually appropriate behavior through
automatic pattern-completion mechanisms operating with limited or unreliable
executive oversight. This review examines this convergence across three
principles: automaticity, in which responses emerge from associative rather
than deliberative processes; suppressed monitoring, leading to errors such as
confabulation in hypnosis and hallucination in LLMs; and heightened contextual
dependency, where immediate cues (for example, the suggestion of a therapist or
the prompt of the user) override stable knowledge.
  These mechanisms reveal an observer-relative meaning gap: both systems
produce coherent but ungrounded outputs that require an external interpreter to
supply meaning. Hypnosis and LLMs also exemplify functional agency - the
capacity for complex, goal-directed, context-sensitive behavior - without
subjective agency, the conscious awareness of intention and ownership that
defines human action. This distinction clarifies how purposive behavior can
emerge without self-reflective consciousness, governed instead by structural
and contextual dynamics. Finally, both domains illuminate the phenomenon of
scheming: automatic, goal-directed pattern generation that unfolds without
reflective awareness. Hypnosis provides an experimental model for understanding
how intention can become dissociated from conscious deliberation, offering
insights into the hidden motivational dynamics of artificial systems.
Recognizing these parallels suggests that the future of reliable AI lies in
hybrid architectures that integrate generative fluency with mechanisms of
executive monitoring, an approach inspired by the complex, self-regulating
architecture of the human mind.

</details>


### [411] [Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges](https://arxiv.org/abs/2511.01375)
*Hamin Koo,Minseon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: AMIS aligns jailbreak prompt optimization with scoring calibration via a bi-level meta-optimization, yielding stronger prompts and more reliable scoring.


<details>
  <summary>Details</summary>
Motivation: Existing optimization-based jailbreaks rely on sparse binary ASR signals or manually crafted templates, which introduce bias and limit calibration. AMIS aims to jointly optimize prompts and scoring templates to produce denser feedback and better-aligned attack outcomes.

Method: AMIS (Align to MISalign) uses a bi-level optimization framework. Inner loop refines jailbreak prompts using fine-grained, dense feedback with a fixed scoring template. Outer loop optimizes the scoring template based on an ASR alignment score, gradually aligning the template with true attack outcomes across queries.

Result: AMIS achieves state-of-the-art jailbreak success rates on AdvBench and JBB-Behaviors, with 88.0% ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming baselines by substantial margins.

Conclusion: Joint co-optimization of prompts and scoring templates yields progressively stronger jailbreaks and better-calibrated evaluation signals, improving red-teaming efficacy for LLMs.

Abstract: Identifying the vulnerabilities of large language models (LLMs) is crucial
for improving their safety by addressing inherent weaknesses. Jailbreaks, in
which adversaries bypass safeguards with crafted input prompts, play a central
role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.
Recent optimization-based jailbreak approaches iteratively refine attack
prompts by leveraging LLMs. However, they often rely heavily on either binary
attack success rate (ASR) signals, which are sparse, or manually crafted
scoring templates, which introduce human bias and uncertainty in the scoring
outcomes. To address these limitations, we introduce AMIS (Align to MISalign),
a meta-optimization framework that jointly evolves jailbreak prompts and
scoring templates through a bi-level structure. In the inner loop, prompts are
refined using fine-grained and dense feedback using a fixed scoring template.
In the outer loop, the template is optimized using an ASR alignment score,
gradually evolving to better reflect true attack outcomes across queries. This
co-optimization process yields progressively stronger jailbreak prompts and
more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors
demonstrate that AMIS achieves state-of-the-art performance, including 88.0%
ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming
existing baselines by substantial margins.

</details>


### [412] [Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering](https://arxiv.org/abs/2511.01396)
*ClÃ©ment Yvernes,Emilie Devijver,AdÃ¨le H. Ribeiro,Marianne Clausel--Lesourd,Ãric Gaussier*

Main category: cs.AI

TL;DR: Extends C-DAGs to allow arbitrary, potentially cyclic clusterings and unobserved confounding; provides a sound and atomically complete causal calculus for cluster-level interventions, aligning each rule with a primitive do-calculus step.


<details>
  <summary>Details</summary>
Motivation: Conventional C-DAG semantics restrict clustering via the partition admissibility constraint, which excludes cyclic partitions and some confounded cluster-level relationships. This limits causal reasoning to certain abstractions. The work aims to broaden applicability by removing that constraint and enabling causal reasoning across all clusterings.

Method: Relax the partition admissibility constraint in C-DAGs to permit cycles. Generalize d-separation and causal calculus to cyclic C-DAGs. Prove that the resulting rules are sound and atomically complete with respect to do-calculus, with each rule corresponding to a primitive do-calculus step.

Result: The framework now supports cyclic C-DAG representations and arbitrary clusterings, enabling cluster-level causal reasoning in scenarios previously intractable. A calculus is provided that is sound and atomically complete relative to do-calculus, ensuring all valid interventional queries at the cluster level can be derived. Each derivation step aligns with a primitive do-calculus operation.

Conclusion: This work significantly broadens the scope of C-DAGs, making them applicable to a wider range of problems by removing clustering admissibility constraints and extending the causal toolkit to cyclic, cluster-based graphs. The established soundness and completeness give strong theoretical guarantees for cluster-level causal inference.

Abstract: Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes
represent clusters of variables, and edges encode both cluster-level causal
relationships and dependencies arisen from unobserved confounding. C-DAGs
define an equivalence class of acyclic causal graphs that agree on
cluster-level relationships, enabling causal reasoning at a higher level of
abstraction. However, when the chosen clustering induces cycles in the
resulting C-DAG, the partition is deemed inadmissible under conventional C-DAG
semantics. In this work, we extend the C-DAG framework to support arbitrary
variable clusterings by relaxing the partition admissibility constraint,
thereby allowing cyclic C-DAG representations. We extend the notions of
d-separation and causal calculus to this setting, significantly broadening the
scope of causal reasoning across clusters and enabling the application of
C-DAGs in previously intractable scenarios. Our calculus is both sound and
atomically complete with respect to the do-calculus: all valid interventional
queries at the cluster level can be derived using our rules, each corresponding
to a primitive do-calculus step.

</details>


### [413] [Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm](https://arxiv.org/abs/2511.01415)
*Amrapali Pednekar,Ãlvaro Garrido-PÃ©rez,Yara Khaluf,Pieter Simoens*

Main category: cs.AI

TL;DR: Dual-task DRL agents in a simplified Overcooked environment show timing overproduction under dual-task conditions, with no clear evidence of an intrinsic timer in LSTM units; findings echo human timing interference and warrant deeper investigation into DRL time-keeping mechanisms.


<details>
  <summary>Details</summary>
Motivation: To understand how temporal processing can be interfered with in AI agents, explore possible parallels with human timing, and shed light on whether DRL systems develop internal time-keeping mechanisms.

Method: Train two DRL agents on a simplified Overcooked task: single-task (T) and dual-task (T+N) where the latter adds a concurrent number comparison task. Both include an embedded time production task. Compare timing behavior and probe LSTM dynamics for timer signatures.

Result: The dual-task agent overproduces time relative to the single-task agent, consistently across four target durations. Preliminary LSTM analysis did not reveal a dedicated or intrinsic timer.

Conclusion: This work is a preliminary step toward linking emergent DRL timing with biological timing, suggesting the need for deeper investigation into internal time-keeping mechanisms and their relation to observed behavioral patterns.

Abstract: This study explores the interference in temporal processing within a
dual-task paradigm from an artificial intelligence (AI) perspective. In this
context, the dual-task setup is implemented as a simplified version of the
Overcooked environment with two variations, single task (T) and dual task
(T+N). Both variations involve an embedded time production task, but the dual
task (T+N) additionally involves a concurrent number comparison task. Two deep
reinforcement learning (DRL) agents were separately trained for each of these
tasks. These agents exhibited emergent behavior consistent with human timing
research. Specifically, the dual task (T+N) agent exhibited significant
overproduction of time relative to its single task (T) counterpart. This result
was consistent across four target durations. Preliminary analysis of neural
dynamics in the agents' LSTM layers did not reveal any clear evidence of a
dedicated or intrinsic timer. Hence, further investigation is needed to better
understand the underlying time-keeping mechanisms of the agents and to provide
insights into the observed behavioral patterns. This study is a small step
towards exploring parallels between emergent DRL behavior and behavior observed
in biological systems in order to facilitate a better understanding of both.

</details>


### [414] [Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis](https://arxiv.org/abs/2511.01425)
*Yuhang Huang,Zekai Lin,Fan Zhong,Lei Liu*

Main category: cs.AI

TL;DR: An interactive, auditable agent uses RL to seek external visual evidence to support diagnostic reasoning; improves calibrated accuracy and verifiability; causal masking confirms faithfulness.


<details>
  <summary>Details</summary>
Motivation: In high-stakes domains like medicine, explanations must be verifiable; existing methods lack auditable reasoning chains, hindering trust.

Method: Train an interactive agent with a policy that selects external visual evidence to inspect; optimize policy via reinforcement learning; assess using Brier score; verify faithfulness via causal intervention by masking chosen evidence.

Result: Achieves an 18% reduction in Brier score vs a non-interactive baseline; masking evidence yields ÎBrier = +0.029, showing evidence is integral to decisions; the approach yields efficient and generalizable explanations.

Conclusion: Provides a practical framework for verifiable and faithful AI reasoning in high-stakes domains by coupling action-based evidence gathering with RL and causal validity checks.

Abstract: Explanations for AI models in high-stakes domains like medicine often lack
verifiability, which can hinder trust. To address this, we propose an
interactive agent that produces explanations through an auditable sequence of
actions. The agent learns a policy to strategically seek external visual
evidence to support its diagnostic reasoning. This policy is optimized using
reinforcement learning, resulting in a model that is both efficient and
generalizable. Our experiments show that this action-based reasoning process
significantly improves calibrated accuracy, reducing the Brier score by 18\%
compared to a non-interactive baseline. To validate the faithfulness of the
agent's explanations, we introduce a causal intervention method. By masking the
visual evidence the agent chooses to use, we observe a measurable degradation
in its performance ($\Delta$Brier=+0.029), confirming that the evidence is
integral to its decision-making process. Our work provides a practical
framework for building AI systems with verifiable and faithful reasoning
capabilities.

</details>


### [415] [Robust Multimodal Sentiment Analysis via Double Information Bottleneck](https://arxiv.org/abs/2511.01444)
*Huiting Huang,Tieliang Gong,Kai He,Jialun Wu,Erik Cambria,Mengling Feng*

Main category: cs.AI

TL;DR: Introduces Double Information Bottleneck (DIB) for robust multimodal sentiment analysis by learning compact unimodal representations and a discriminative fused multimodal representation via an attention bottleneck, leveraging low-rank Renyi entropy to improve noise robustness and tractability.


<details>
  <summary>Details</summary>
Motivation: Current multimodal sentiment analysis methods struggle with noise-contaminated unimodal data, which corrupts cross-modal interactions, and with fusion strategies that discard useful unimodal information while retaining redundant multimodal information. A robust, compact, and discriminative multimodal representation is needed.

Method: Propose a Double Information Bottleneck (DIB) framework within a low-rank Renyi entropy functional. It comprises two modules: (1) unimodal bottleneck that learns sufficient and compressed representations by maximizing task-relevant information and discarding superfluous information; (2) a discriminative multimodal fusion via a novel attention bottleneck mechanism that preserves inter-modal complementarity while filtering noise.

Result: Extensive experiments on CMU-MOSI, CMU-MOSEI, CH-SIMS, and MVSA-Single show strong performance: 47.4% accuracy (Acc-7) on CMU-MOSI and 81.63% F1 on CH-SIMS, with the second-best baseline lagging by 1.19%. Under noisy conditions, the method exhibits small degradations (0.36% on CMU-MOSI and 0.29% on CM-MOSEI), indicating robust noise handling.

Conclusion: The proposed DIB framework yields a robust, compact, and discriminative multimodal representation by effectively filtering unimodal noise and leveraging inter-modal complementarity, achieving strong and noise-robust performance across multiple datasets.

Abstract: Multimodal sentiment analysis has received significant attention across
diverse research domains. Despite advancements in algorithm design, existing
approaches suffer from two critical limitations: insufficient learning of
noise-contaminated unimodal data, leading to corrupted cross-modal
interactions, and inadequate fusion of multimodal representations, resulting in
discarding discriminative unimodal information while retaining multimodal
redundant information. To address these challenges, this paper proposes a
Double Information Bottleneck (DIB) strategy to obtain a powerful, unified
compact multimodal representation. Implemented within the framework of low-rank
Renyi's entropy functional, DIB offers enhanced robustness against diverse
noise sources and computational tractability for high-dimensional data, as
compared to the conventional Shannon entropy-based methods. The DIB comprises
two key modules: 1) learning a sufficient and compressed representation of
individual unimodal data by maximizing the task-relevant information and
discarding the superfluous information, and 2) ensuring the discriminative
ability of multimodal representation through a novel attention bottleneck
fusion mechanism. Consequently, DIB yields a multimodal representation that
effectively filters out noisy information from unimodal data while capturing
inter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI,
CH-SIMS, and MVSA-Single validate the effectiveness of our method. The model
achieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score
on CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it
shows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI
respectively.

</details>


### [416] [From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation](https://arxiv.org/abs/2511.01445)
*ChengZhang Yu,YingRu He,Hongyan Cheng,nuo Cheng,Zhixing Liu,Dongxu Mu,Zhangrui Shen,Zhanpeng Jin*

Main category: cs.AI

TL;DR: A hierarchical multi-agent system automates pre-consultation tasks (triage, history taking, chief complaint) with centralized control, achieving high accuracy and efficiency while preserving privacy across multiple foundation models.


<details>
  <summary>Details</summary>
Motivation: To address diminishing consultation times and rising patient volumes by making AI-driven pre-consultation proactive and private, moving from passive interaction to autonomous inquiry orchestration.

Method: An eight-agent architecture decomposes pre-consultation into four main tasks (T1âT4), with T1âT3 subdivided into 13 domain-specific subtasks. Centralized scheduling enables task orchestration. Evaluated on 1,372 validated EHRs from a Chinese platform across GPT-OSS 20B, Qwen3-8B, and Phi4-14B, with local deployment to ensure privacy.

Result: Triage accuracy: 87.0% (primary); 80.5% (secondary dept). Task completion rate: 98.2% with agent-driven scheduling vs 93.1% sequential. Clinical quality scores: Chief Complaints 4.56, History of Present Illness 4.48, Past History 4.69 (out of 5). Rounds: ~12.7 for T2 and ~16.9 for T3. Performance remained strong across models, with privacy preserved via local deployment.

Conclusion: A model-agnostic, privacy-preserving autonomous AI framework can improve pre-consultation efficiency and quality, enabling proactive inquiry in clinical settings across diverse foundation models.

Abstract: Global healthcare systems face critical challenges from increasing patient
volumes and limited consultation times, with primary care visits averaging
under 5 minutes in many countries. While pre-consultation processes
encompassing triage and structured history-taking offer potential solutions,
they remain limited by passive interaction paradigms and context management
challenges in existing AI systems. This study introduces a hierarchical
multi-agent framework that transforms passive medical AI systems into proactive
inquiry agents through autonomous task orchestration. We developed an
eight-agent architecture with centralized control mechanisms that decomposes
pre-consultation into four primary tasks: Triage ($T_1$), History of Present
Illness collection ($T_2$), Past History collection ($T_3$), and Chief
Complaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13
domain-specific subtasks. Evaluated on 1,372 validated electronic health
records from a Chinese medical platform across multiple foundation models
(GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy for
primary department triage and 80.5% for secondary department classification,
with task completion rates reaching 98.2% using agent-driven scheduling versus
93.1% with sequential processing. Clinical quality scores from 18 physicians
averaged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and
4.69 for Past History on a 5-point scale, with consultations completed within
12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnostic
architecture maintained high performance across different foundation models
while preserving data privacy through local deployment, demonstrating the
potential for autonomous AI systems to enhance pre-consultation efficiency and
quality in clinical settings.

</details>


### [417] [TPS-Bench: Evaluating AI Agents' Tool Planning \& Scheduling Abilities in Compounding Tasks](https://arxiv.org/abs/2511.01527)
*Hanwen Xu,Xuyao Huang,Yuzhe Liu,Kai Yu,Zhijie Deng*

Main category: cs.AI

TL;DR: TPS-Bench is a benchmark to evaluate LLM agents' ability to plan tool usage and schedule executions for compounding real-world tasks, using a 200-task MCP-tool-based benchmark to study completion rates and efficiency, revealing trade-offs between sequential and parallel tool calls and showing RL can improve scheduling.


<details>
  <summary>Details</summary>
Motivation: To understand how LLM agents coordinate a broad set of tools (tool planning and scheduling) to solve heterogeneous, multi-subtask problems that occur in real-world settings.

Method: Construct TPS-Bench with 200 compounding tasks across two difficulty levels built on a repository of hundreds of MCP tools; each task has multiple subtasks (e.g., web search, map navigation, calendar checks) solvable by basic tools; evaluate task completion rate and execution time across models (e.g., GLM-4.5, GPT-4o); explore RL on Qwen3-1.7B with ~100 training samples to improve scheduling; provide open-source code.

Result: Findings show most models can perform basic tool planning but scheduling varies: GLM-4.5 achieves 64.72% completion with long sequential tool calls; GPT-4o uses parallel calls but only 45.08% completion; RL on Qwen3-1.7B reduces execution time by 14% and increases completion rate by 6% with limited data (~100 samples).

Conclusion: Tool planning plus scheduling is attainable for LLM agents; scheduling strategy materially affects efficiency; RL appears promising for improving scheduling; TPS-Bench provides a baseline and benchmark for future work in this area.

Abstract: Large language model (LLM) agents have exhibited strong problem-solving
competence across domains like research and coding. Yet, it remains
underexplored whether LLM agents can tackle compounding real-world problems
that require a diverse set of tools to complete. Given a broad, heterogeneous
tool repository, LLM agents must not only select appropriate tools based on
task planning analysis but also strategically schedule the execution order to
ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of
LLM agents in solving such problems that demand Tool Planning and Scheduling.
TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a
tool repository containing hundreds of model context protocol (MCP) tools. In
particular, each task is composed of multiple subtasks, such as web search, map
navigation, calendar checking, etc., and each subtask can be completed by a
basic tool. Our evaluation emphasizes both task completion rate and efficiency.
The empirical studies on popular closed-source and open-source LLMs indicate
that most models can perform reasonable tool planning, but differ in
scheduling. For example, GLM-4.5 achieves an outperforming task completion rate
of 64.72% with extensive sequential tool calls, hence suffering from
significantly long execution time. By contrast, GPT-4o prioritizes parallel
tool calls but achieves only a 45.08% completion rate. Considering
reinforcement learning (RL) can be a viable way to improve the scheduling
efficiency without compromising performance, we perform an initial study on
Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in
task completion rate based on rarely 100 RL training samples. Our code is
available https://github.com/hanwenxu1/mcp-agent.

</details>


### [418] [Analyzing Sustainability Messaging in Large-Scale Corporate Social Media](https://arxiv.org/abs/2511.01550)
*Ujjwal Sharma,Stevan Rudinac,Ana MiÄkoviÄ,Willemijn van Dolen,Marcel Worring*

Main category: cs.AI

TL;DR: A multimodal pipeline uses LLMs to align corporate tweets with the 17 SDGs and visual-language models to cluster visual sustainability cues, enabling scalable, annotation-light analysis of sustainability messaging and its relation to ESG risks and consumer engagement.


<details>
  <summary>Details</summary>
Motivation: Tackling evolving, multimodal, and often ambiguous corporate sustainability messaging on social media; avoid costly task-specific annotations by using foundation models as ad-hoc annotators; enable large-scale, scalable analysis across platforms.

Method: Text analysis via an ensemble of large language models annotating tweets for SDG alignment; visual analysis via vision-language models within a semantic-clustering framework to uncover patterns in visual sustainability communication; automatic label generation and semantic visual clustering.

Result: Reveals sectoral differences in SDG engagement, temporal trends, and associations between corporate messaging, ESG risks, and consumer engagement; demonstrates a flexible pipeline applicable to other domains for large-scale social media analysis.

Conclusion: An integrated, scalable, annotation-light framework for multimodal social media analysis that can be applied beyond sustainability to other domains.

Abstract: In this work, we introduce a multimodal analysis pipeline that leverages
large foundation models in vision and language to analyze corporate social
media content, with a focus on sustainability-related communication. Addressing
the challenges of evolving, multimodal, and often ambiguous corporate messaging
on platforms such as X (formerly Twitter), we employ an ensemble of large
language models (LLMs) to annotate a large corpus of corporate tweets on their
topical alignment with the 17 Sustainable Development Goals (SDGs). This
approach avoids the need for costly, task-specific annotations and explores the
potential of such models as ad-hoc annotators for social media data that can
efficiently capture both explicit and implicit references to sustainability
themes in a scalable manner. Complementing this textual analysis, we utilize
vision-language models (VLMs), within a visual understanding framework that
uses semantic clusters to uncover patterns in visual sustainability
communication. This integrated approach reveals sectoral differences in SDG
engagement, temporal trends, and associations between corporate messaging,
environmental, social, governance (ESG) risks, and consumer engagement. Our
methods-automatic label generation and semantic visual clustering-are broadly
applicable to other domains and offer a flexible framework for large-scale
social media analysis.

</details>


### [419] [ExplicitLM: Decoupling Knowledge from Parameters via Explicit Memory Banks](https://arxiv.org/abs/2511.01581)
*Chengzhang Yu,Zening Lu,Chenyang Zheng,Chiyue Wang,Yiming Zhang,Zhanpeng Jin*

Main category: cs.AI

TL;DR: ExplicitLM combines a million-scale external memory of human-readable facts with differentiable retrieval to create interpretable, updatable language models; it achieves strong gains on knowledge-intensive tasks and in low-data regimes, while enabling direct inspection of knowledge.


<details>
  <summary>Details</summary>
Motivation: Address knowledge staleness and lack of interpretability in large language models caused by entangled implicit knowledge in parameters, which hinders targeted updates and transparent reasoning.

Method: A million-scale external memory storing knowledge as token sequences is integrated into the model. Retrieval has two stages: (1) coarse-grained filtering via product key decomposition (reducing complexity from O(NÂ·|I|) to O(âNÂ·|I|)); (2) fine-grained matching via Gumbel-Softmax for end-to-end training. Knowledge is partitioned into frozen explicit facts (20%) and learnable implicit patterns (80%), maintained with Exponential Moving Average updates for stability.

Result: The approach yields up to 43.67% improvements on knowledge-intensive tasks over standard Transformers and 3.62Ã gains in low-data regimes (10k samples). Correct predictions show 49% higher hit rates. The model demonstrates strong correlations between memory retrieval and performance.

Conclusion: A jointly optimized, interpretable, and updatable LM is feasible with explicit memory retrieval, offering unprecedented knowledge transparency while maintaining competitive performance compared to retrieval-augmented RAG systems with frozen retrieval.

Abstract: Large language models suffer from knowledge staleness and lack of
interpretability due to implicit knowledge storage across entangled network
parameters, preventing targeted updates and reasoning transparency. We propose
ExplicitLM, a novel architecture featuring a million-scale external memory bank
storing human-readable knowledge as token sequences, enabling direct inspection
and modification. We design a differentiable two-stage retrieval mechanism with
efficient coarse-grained filtering via product key decomposition (reducing
complexity from $\mathcal{O}(N \cdot |I|)$ to $\mathcal{O}(\sqrt{N} \cdot
|I|)$) and fine-grained Gumbel-Softmax matching for end-to-end training.
Inspired by dual-system cognitive theory, we partition knowledge into frozen
explicit facts (20%) and learnable implicit patterns (80%), maintained through
Exponential Moving Average updates for stability. ExplicitLM achieves up to
43.67% improvement on knowledge-intensive tasks versus standard Transformers,
with 3.62$\times$ gains in low-data regimes (10k samples). Analysis shows
strong correlations between memory retrieval and performance, with correct
predictions achieving 49% higher hit rates. Unlike RAG systems with frozen
retrieval, our jointly optimized architecture demonstrates that interpretable,
updatable models can maintain competitive performance while providing
unprecedented knowledge transparency.

</details>


### [420] [IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization](https://arxiv.org/abs/2511.01639)
*Sicheng Wang,Shuhao Chen,Jingran Zhou,Chengyi Tu*

Main category: cs.AI

TL;DR: IVGAE-TAMA-BO is a dynamic GNN for predicting future links in global food trade networks, leveraging a Trade-Aware Momentum Aggregator and momentum-based memory, with Bayesian optimization to tune hyperparameters; it outperforms static IVGAE and other dynamic baselines across five crop datasets.


<details>
  <summary>Details</summary>
Motivation: Global food trade networks are highly dynamic under geopolitical, economic, and environmental pressures. Static models struggle to capture evolving temporal patterns essential for accurate link prediction and food-security monitoring.

Method: Extend the IVGAE framework with a Trade-Aware Momentum Aggregator (TAMA) to model short-term fluctuations and long-term structural dependencies, incorporate a momentum-based structural memory for stability, and use Bayesian optimization to automatically tune key hyperparameters. Evaluate on five crop-specific datasets against static IVGAE and other dynamic baselines.

Result: IVGAE-TAMA substantially outperforms the static IVGAE and other dynamic baselines by effectively modeling temporal dependencies. The Bayesian-optimized version, IVGAE-TAMA-BO, yields further performance gains.

Conclusion: The proposed framework provides a robust and scalable solution for structural prediction in global trade networks, with strong potential for food security monitoring and policy decision support.

Abstract: Global food trade plays a crucial role in ensuring food security and
maintaining supply chain stability. However, its network structure evolves
dynamically under the influence of geopolitical, economic, and environmental
factors, making it challenging to model and predict future trade links.
Effectively capturing temporal patterns in food trade networks is therefore
essential for improving the accuracy and robustness of link prediction. This
study introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designed
to model evolving trade structures and predict future links in global food
trade networks. To the best of our knowledge, this is the first work to apply
dynamic graph neural networks to this domain, significantly enhancing
predictive performance. Building upon the original IVGAE framework, the
proposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capture
the temporal evolution of trade networks, jointly modeling short-term
fluctuations and long-term structural dependencies. A momentum-based structural
memory mechanism further improves predictive stability and performance. In
addition, Bayesian optimization is used to automatically tune key
hyperparameters, enhancing generalization across diverse trade scenarios.
Extensive experiments on five crop-specific datasets demonstrate that
IVGAE-TAMA substantially outperforms the static IVGAE and other dynamic
baselines by effectively modeling temporal dependencies, while Bayesian
optimization further boosts performance in IVGAE-TAMA-BO. These results
highlight the proposed framework as a robust and scalable solution for
structural prediction in global trade networks, with strong potential for
applications in food security monitoring and policy decision support.

</details>


### [421] [Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics](https://arxiv.org/abs/2511.01668)
*Yueqing Xi,Yifan Bai,Huasen Luo,Weiliang Wen,Hui Liu,Haoliang Li*

Main category: cs.AI

TL;DR: A hybrid legal QA system combining retrieval-augmented generation, multi-model ensembling, and human-in-the-loop to deliver reliable, updatable, and auditable legal answers, outperforming baselines and reducing hallucination.


<details>
  <summary>Details</summary>
Motivation: LLMs are prone to hallucination in legal QA and static knowledge bases struggle to keep up with evolving statutes and case law; there is a need for an auditable, continuously updatable, and legally compliant QA system for judicial settings.

Method: A retrieval-first pipeline: when a trusted legal repository yields relevant evidence, answers are produced via retrieval-augmented generation (RAG). If not, multiple LLMs generate candidates that are scored by a specialized selector, with the top-ranked answer returned. High-quality outputs undergo human review before being written back to the repository, enabling provenance tracking and dynamic knowledge evolution. Evaluation on Law_QA shows improvements over single-model baselines and vanilla RAG. Ablations demonstrate the impact of retrieval prioritization, model ensembling, and the human-in-the-loop update mechanism.

Result: The hybrid approach significantly outperforms both a single-model baseline and a vanilla RAG pipeline in F1, ROUGE-L, and an LLM-as-a-Judge metric on Law_QA. Ablations confirm complementary contributions of retrieval prioritization, model ensembling, and human-in-the-loop updates, and indicate reduced hallucination and improved legal compliance.

Conclusion: This work demonstrates the practical viability of combining retrieval, multi-model ensembling, and human-in-the-loop updates to produce auditable, updatable legal QA, supporting the deployment of media-forensics-informed judicial QA systems.

Abstract: As artificial intelligence permeates judicial forensics, ensuring the
veracity and traceability of legal question answering (QA) has become critical.
Conventional large language models (LLMs) are prone to hallucination, risking
misleading guidance in legal consultation, while static knowledge bases
struggle to keep pace with frequently updated statutes and case law. We present
a hybrid legal QA agent tailored for judicial settings that integrates
retrieval-augmented generation (RAG) with multi-model ensembling to deliver
reliable, auditable, and continuously updatable counsel. The system prioritizes
retrieval over generation: when a trusted legal repository yields relevant
evidence, answers are produced via RAG; otherwise, multiple LLMs generate
candidates that are scored by a specialized selector, with the top-ranked
answer returned. High-quality outputs then undergo human review before being
written back to the repository, enabling dynamic knowledge evolution and
provenance tracking. Experiments on the Law\_QA dataset show that our hybrid
approach significantly outperforms both a single-model baseline and a vanilla
RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm
the complementary contributions of retrieval prioritization, model ensembling,
and the human-in-the-loop update mechanism. The proposed system demonstrably
reduces hallucination while improving answer quality and legal compliance,
advancing the practical landing of media forensics technologies in judicial
scenarios.

</details>


### [422] [Simulating Environments with Reasoning Models for Agent Training](https://arxiv.org/abs/2511.01824)
*Yuetai Li,Huseyin A Inan,Xiang Yue,Wei-Ning Chen,Lukas Wutschitz,Janardhan Kulkarni,Radha Poovendran,Robert Sim,Saravan Rajmohan*

Main category: cs.AI

TL;DR: Simia-SFT and Simia-RL enable scalable LLM agent training by simulating environment feedback, removing the need for real testbeds; open-model fine-tuning improves performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLM agents are brittle in complex contexts and rely on heavy, brittle environment engineering; there is a need to train robust agents without real environment data or APIs.

Method: Simia-SFT synthesizes SFT data by amplifying small seed sets into diverse trajectories in an environment-agnostic way. Simia-RL uses LLM-simulated feedback to train agents through RL without actual environment implementations.

Result: Fine-tuning open models yields consistent improvements across benchmarks, surpassing GPT-4o and approaching o4-mini on tau^2-Bench.

Conclusion: The two frameworks together enable scalable agent training without environment engineering, substituting real environments with flexible LLM-based simulation.

Abstract: LLM agents excel in compact environments requiring deep reasoning but remain
brittle when operating in broader, more complex contexts that demand robustness
across diverse tools and schemas. Building bespoke environments for training is
heavy, brittle, and limits progress. In this paper, we demonstrate that LLMs
can simulate realistic environment feedback without access to actual testbed
data or APIs. Inspired by this capability, we propose two frameworks:
Simia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets
into diverse trajectories in an environment-agnostic manner, and Simia-RL, a
framework that enables RL training without real environment implementations
through LLM-simulated feedback. Fine-tuning open models yields consistent
improvements across multiple benchmarks, surpassing GPT-4o and approaching
o4-mini on $\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable
agent training without environment engineering, replacing heavy and brittle
implementations with flexible LLM-based simulation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [423] [Gen AI in Automotive: Applications, Challenges, and Opportunities with a Case study on In-Vehicle Experience](https://arxiv.org/abs/2511.00026)
*Chaitanya Shinde,Divya Garikapati*

Main category: cs.RO

TL;DR: GenAI in automotive is reviewed; highlights GANs/VAEs, synthetic data for validation, design optimization, improved voice-based HMI; addresses challenges (compute, bias, IP, adversarial robustness); includes MBUX case study; outlines future directions.


<details>
  <summary>Details</summary>
Motivation: To map current GenAI capabilities to automotive applications, bridging safety and user experience, and to identify opportunities, limitations, and research directions beyond perception/manufacturing reviews.

Method: Comprehensive literature review plus a case study; synthesis across enabling technologies; emphasis on voice-based HMI; comparison to legacy systems.

Result: Identifies opportunities in autonomous driving validation, design optimization, personalized interfaces; notes significant challenges; demonstrates MBUX case study with more natural, proactive interactions; defines boundary of GenAI's promise and limits.

Conclusion: GenAI holds potential for safer, more efficient, user-centric mobility; requires tackling computation, bias, IP, adversarial threats; suggests directions for future work and responsible deployment.

Abstract: Generative Artificial Intelligence is emerging as a transformative force in
the automotive industry, enabling novel applications across vehicle design,
manufacturing, autonomous driving, predictive maintenance, and in vehicle user
experience. This paper provides a comprehensive review of the current state of
GenAI in automotive, highlighting enabling technologies such as Generative
Adversarial Networks and Variational Autoencoders. Key opportunities include
accelerating autonomous driving validation through synthetic data generation,
optimizing component design, and enhancing human machine interaction via
personalized and adaptive interfaces. At the same time, the paper identifies
significant technical, ethical, and safety challenges, including computational
demands, bias, intellectual property concerns, and adversarial robustness, that
must be addressed for responsible deployment. A case study on Mercedes Benzs
MBUX Virtual Assistant illustrates how GenAI powered voice systems deliver more
natural, proactive, and personalized in car interactions compared to legacy
rule based assistants. Through this review and case study, the paper outlines
both the promise and limitations of GenAI integration in the automotive sector
and presents directions for future research and development aimed at achieving
safer, more efficient, and user centric mobility. Unlike prior reviews that
focus solely on perception or manufacturing, this paper emphasizes generative
AI in voice based HMI, bridging safety and user experience perspectives.

</details>


### [424] [STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization](https://arxiv.org/abs/2511.00033)
*Diqi He,Xuehao Gao,Hao Li,Junwei Han,Dingwen Zhang*

Main category: cs.RO

TL;DR: STRIDER is a zero-shot VLN-CE framework that uses spatially constrained decision space via a Structured Waypoint Generator and a Task-Alignment Regulator to ensure long-horizon navigation aligns with spatial structure and task intent, achieving notable SR gains on R2R-CE and RxR-CE.


<details>
  <summary>Details</summary>
Motivation: Addresses the core challenge of zero-shot vision-and-language navigation in continuous 3D environments: maintaining alignment with spatial structure and task intent across long horizons, compounded by insufficient structured decision-making and feedback integration.

Method: Proposes STRIDER, combining a Structured Waypoint Generator to constrain the action space using spatial structure and a Task-Alignment Regulator to adjust behavior based on task progress. Integrates spatial layout priors and dynamic task feedback.

Result: Empirically, STRIDER improves Success Rate from 29% to 35% (relative ~20.7%). It outperforms strong SOTA on R2R-CE and RxR-CE benchmarks, demonstrating the effectiveness of spatially constrained decision-making and feedback-guided execution.

Conclusion: Spatially constrained decision-making and feedback-guided execution are key to improving zero-shot VLN-CE navigation fidelity; STRIDER provides an effective framework that could generalize to other long-horizon, instruction-following navigation tasks.

Abstract: The Zero-shot Vision-and-Language Navigation in Continuous Environments
(VLN-CE) task requires agents to navigate previously unseen 3D environments
using natural language instructions, without any scene-specific training. A
critical challenge in this setting lies in ensuring agents' actions align with
both spatial structure and task intent over long-horizon execution. Existing
methods often fail to achieve robust navigation due to a lack of structured
decision-making and insufficient integration of feedback from previous actions.
To address these challenges, we propose STRIDER (Instruction-Aligned Structural
Decision Space Optimization), a novel framework that systematically optimizes
the agent's decision space by integrating spatial layout priors and dynamic
task feedback. Our approach introduces two key innovations: 1) a Structured
Waypoint Generator that constrains the action space through spatial structure,
and 2) a Task-Alignment Regulator that adjusts behavior based on task progress,
ensuring semantic alignment throughout navigation. Extensive experiments on the
R2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms
strong SOTA across key metrics; in particular, it improves Success Rate (SR)
from 29% to 35%, a relative gain of 20.7%. Such results highlight the
importance of spatially constrained decision-making and feedback-guided
execution in improving navigation fidelity for zero-shot VLN-CE.

</details>


### [425] [Endowing GPT-4 with a Humanoid Body: Building the Bridge Between Off-the-Shelf VLMs and the Physical World](https://arxiv.org/abs/2511.00041)
*Yingzhao Jian,Zhongan Wang,Yi Yang,Hehe Fan*

Main category: cs.RO

TL;DR: BiBo uses off-the-shelf vision-language models to control humanoid agents via an embodied instruction compiler and a diffusion-based motion executor, achieving high interaction success with reduced data needs.


<details>
  <summary>Details</summary>
Motivation: To reduce data collection costs while leveraging VLMs' open-world generalization for humanoid control.

Method: Two components: (1) embodied instruction compiler translates high-level user instructions into low-level commands with parameters; (2) diffusion-based motion executor generates human-like motions from the commands and adapts to environmental feedback.

Result: In open environments, BiBo achieves 90.2% task success and improves text-guided motion execution precision by 16.3% over prior methods; code will be released.

Conclusion: BiBo demonstrates that off-the-shelf VLMs can effectively control humanoid agents in open settings with robust generalization and reduced data requirements.

Abstract: Humanoid agents often struggle to handle flexible and diverse interactions in
open environments. A common solution is to collect massive datasets to train a
highly capable model, but this approach can be prohibitively expensive. In this
paper, we explore an alternative solution: empowering off-the-shelf
Vision-Language Models (VLMs, such as GPT-4) to control humanoid agents,
thereby leveraging their strong open-world generalization to mitigate the need
for extensive data collection. To this end, we present \textbf{BiBo}
(\textbf{B}uilding humano\textbf{I}d agent \textbf{B}y \textbf{O}ff-the-shelf
VLMs). It consists of two key components: (1) an \textbf{embodied instruction
compiler}, which enables the VLM to perceive the environment and precisely
translate high-level user instructions (e.g., {\small\itshape ``have a rest''})
into low-level primitive commands with control parameters (e.g.,
{\small\itshape ``sit casually, location: (1, 2), facing: 90$^\circ$''}); and
(2) a diffusion-based \textbf{motion executor}, which generates human-like
motions from these commands, while dynamically adapting to physical feedback
from the environment. In this way, BiBo is capable of handling not only basic
interactions but also diverse and complex motions. Experiments demonstrate that
BiBo achieves an interaction task success rate of 90.2\% in open environments,
and improves the precision of text-guided motion execution by 16.3\% over prior
methods. The code will be made publicly available.

</details>


### [426] [Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail](https://arxiv.org/abs/2511.00088)
*NVIDIA,:,Yan Wang,Wenjie Luo,Junjie Bai,Yulong Cao,Tong Che,Ke Chen,Yuxiao Chen,Jenna Diamond,Yifan Ding,Wenhao Ding,Liang Feng,Greg Heinrich,Jack Huang,Peter Karkus,Boyi Li,Pinyi Li,Tsung-Yi Lin,Dongran Liu,Ming-Yu Liu,Langechuan Liu,Zhijian Liu,Jason Lu,Yunxiang Mao,Pavlo Molchanov,Lindsey Pavao,Zhenghao Peng,Mike Ranzinger,Ed Schmerling,Shida Shen,Yunfei Shi,Sarah Tariq,Ran Tian,Tilman Wekel,Xinshuo Weng,Tianjun Xiao,Eric Yang,Xiaodong Yang,Yurong You,Xiaohui Zeng,Wenyuan Zhang,Boris Ivanovic,Marco Pavone*

Main category: cs.RO

TL;DR: AR1 is a vision-language-action model for autonomous driving that integrates Chain of Causation reasoning with trajectory planning, yielding improved planning accuracy and safety metrics, RL-enhanced reasoning, scalable 0.5B-7B models, real-time latency, and urban Level 4 potential.


<details>
  <summary>Details</summary>
Motivation: End-to-end imitation-learning driving systems struggle with safety-critical long-tail scenarios due to sparse supervision and limited causal understanding; a causal reasoning framework is needed to improve decision-making.

Method: Three innovations: (1) CoC dataset via hybrid auto-labeling and human-in-the-loop producing causally linked reasoning traces aligned with driving behaviors; (2) modular VLA architecture combining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI applications, with a diffusion-based trajectory decoder for real-time, dynamically feasible plans; (3) multi-stage training: supervised fine-tuning to elicit reasoning and reinforcement learning to optimize reasoning quality via feedback from a large reasoning model and enforce reasoning-action consistency.

Result: AR1 achieves up to 12% improvement in planning accuracy on challenging cases vs a trajectory-only baseline; 35% reduction in off-road rate; 25% reduction in close encounter rate in closed-loop simulation. RL post-training improves reasoning quality by 45% (per a large reasoning-model critic) and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B parameters yields consistent improvements. On-vehicle tests show 99 ms latency and successful urban deployment.

Conclusion: AR1 demonstrates a practical path toward Level 4 autonomous driving by bridging interpretable reasoning with precise control; plans to release AR1 models and a subset of the CoC in a future update.

Abstract: End-to-end architectures trained via imitation learning have advanced
autonomous driving by scaling model size and data, yet performance remains
brittle in safety-critical long-tail scenarios where supervision is sparse and
causal understanding is limited. To address this, we introduce Alpamayo-R1
(AR1), a vision-language-action model (VLA) that integrates Chain of Causation
reasoning with trajectory planning to enhance decision-making in complex
driving scenarios. Our approach features three key innovations: (1) the Chain
of Causation (CoC) dataset, built through a hybrid auto-labeling and
human-in-the-loop pipeline producing decision-grounded, causally linked
reasoning traces aligned with driving behaviors; (2) a modular VLA architecture
combining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI
applications, with a diffusion-based trajectory decoder that generates
dynamically feasible plans in real time; (3) a multi-stage training strategy
using supervised fine-tuning to elicit reasoning and reinforcement learning
(RL) to optimize reasoning quality via large reasoning model feedback and
enforce reasoning-action consistency. Evaluation shows AR1 achieves up to a 12%
improvement in planning accuracy on challenging cases compared to a
trajectory-only baseline, with a 35% reduction in off-road rate and 25%
reduction in close encounter rate in closed-loop simulation. RL post-training
improves reasoning quality by 45% as measured by a large reasoning model critic
and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B
parameters shows consistent improvements. On-vehicle road tests confirm
real-time performance (99 ms latency) and successful urban deployment. By
bridging interpretable reasoning with precise control, AR1 demonstrates a
practical path towards Level 4 autonomous driving. We plan to release AR1
models and a subset of the CoC in a future update.

</details>


### [427] [Digital Twin based Automatic Reconfiguration of Robotic Systems in Smart Environments](https://arxiv.org/abs/2511.00094)
*Angelos Alexopoulos,Agorakis Bompotas,Nikitas Rigas Kalogeropoulos,Panagiotis Kechagias,Athanasios P. Kalogeras,Christos Alexakos*

Main category: cs.RO

TL;DR: A Digital Twinâbased framework for autonomous, dynamic reconfiguration of robotic controllers to adapt to changing environments, enabling rapid, automated updates to robot behavior without manual intervention.


<details>
  <summary>Details</summary>
Motivation: Dynamic and evolving real-world topographies in smart cities and precision farming challenge traditional controllers, leading to inefficiencies or failures; there is a need for fast, automated adaptation in robotic systems.

Method: Develop a Digital Twin (virtual replica) of the robot and its environment to simulate and optimize movement trajectories, recalculate paths and control parameters within the Twin, and deploy updated code to the physical robot to realize changes.

Result: Expected benefits include rapid, reliable adaptation to environmental changes without manual reconfiguration, improved autonomy in dynamic environments, and a scalable path to integrating Digital Twins in robotic control systems.

Conclusion: The work advances the integration of Digital Twins in robotics, offering a scalable solution for enhancing autonomy in smart, dynamic environments.

Abstract: Robotic systems have become integral to smart environments, enabling
applications ranging from urban surveillance and automated agriculture to
industrial automation. However, their effective operation in dynamic settings -
such as smart cities and precision farming - is challenged by continuously
evolving topographies and environmental conditions. Traditional control systems
often struggle to adapt quickly, leading to inefficiencies or operational
failures. To address this limitation, we propose a novel framework for
autonomous and dynamic reconfiguration of robotic controllers using Digital
Twin technology. Our approach leverages a virtual replica of the robot's
operational environment to simulate and optimize movement trajectories in
response to real-world changes. By recalculating paths and control parameters
in the Digital Twin and deploying the updated code to the physical robot, our
method ensures rapid and reliable adaptation without manual intervention. This
work advances the integration of Digital Twins in robotics, offering a scalable
solution for enhancing autonomy in smart, dynamic environments.

</details>


### [428] [Real-DRL: Teach and Learn in Reality](https://arxiv.org/abs/2511.00112)
*Yanbing Mao,Yihao Cai,Lui Sha*

Main category: cs.RO

TL;DR: Real-DRL introduces a safety-focused DRL framework for real-world autonomous systems, coupling a learning agent (DRL-Student) with a physics-based safety teacher (PHY-Teacher) under a Trigger to safely and effectively learn in real plants, addressing unknown threats and sim-to-real gaps; proven on a real quadruped, Isaac Gym quad, and cart-pole with ablations.


<details>
  <summary>Details</summary>
Motivation: Safety-critical autonomous systems need safe, data-efficient learning in real environments. Unknown unknowns and the Sim2Real gap pose major risks. The paper seeks to enable real-world DRL that guarantees safety while achieving high performance.

Method: An architecture with three components: DRL-Student (dual self-learning and teaching-to-learn with safety-informed batch sampling), PHY-Teacher (physics-model-based safety-focused action policies delivering real-time safety patches and supporting teaching-to-learn), and Trigger (orchestrates interaction between student and teacher). The approach emphasizes safety-first learning, automatic hierarchy learning (safety before performance), and batch-sampling strategies to handle corner-case imbalance.

Result: Empirical validation across a real quadruped, a quadruped in NVIDIA Isaac Gym, and a cart-pole, including comparisons and ablations. Results indicate assured safety, effective hierarchy learning, and improved handling of corner-case learning through safety-informed sampling, demonstrating Real-DRLâs effectiveness and unique advantages.

Conclusion: Real-DRL provides a practical, safety-oriented DRL framework for real plants, mitigating unknown unknowns and Sim2Real challenges. Its three-component design yields safety guarantees, automatic learning hierarchy, and data-efficient training, with strong empirical validation across multiple platforms.

Abstract: This paper introduces the Real-DRL framework for safety-critical autonomous
systems, enabling runtime learning of a deep reinforcement learning (DRL) agent
to develop safe and high-performance action policies in real plants (i.e., real
physical systems to be controlled), while prioritizing safety! The Real-DRL
consists of three interactive components: a DRL-Student, a PHY-Teacher, and a
Trigger. The DRL-Student is a DRL agent that innovates in the dual
self-learning and teaching-to-learn paradigm and the real-time safety-informed
batch sampling. On the other hand, PHY-Teacher is a physics-model-based design
of action policies that focuses solely on safety-critical functions.
PHY-Teacher is novel in its real-time patch for two key missions: i) fostering
the teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of
real plants. The Trigger manages the interaction between the DRL-Student and
the PHY-Teacher. Powered by the three interactive components, the Real-DRL can
effectively address safety challenges that arise from the unknown unknowns and
the Sim2Real gap. Additionally, Real-DRL notably features i) assured safety,
ii) automatic hierarchy learning (i.e., safety-first learning and then
high-performance learning), and iii) safety-informed batch sampling to address
the learning experience imbalance caused by corner cases. Experiments with a
real quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole
system, along with comparisons and ablation studies, demonstrate the Real-DRL's
effectiveness and unique features.

</details>


### [429] [End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection](https://arxiv.org/abs/2511.00139)
*Yu Cui,Yujian Zhang,Lina Tao,Yang Li,Xinyu Yi,Zhibin Li*

Main category: cs.RO

TL;DR: Shared Autonomy with macro-micro control separation enables high-quality dexterous manipulation data collection and a robust VLA policy, achieving 90% success on unseen objects.


<details>
  <summary>Details</summary>
Motivation: Dexterous manipulation remains challenging; VLA models require abundant high-quality data, but data collection is bottlenecked by human workload (teleoperation) and nonnatural automated planning.

Method: Divide control into macro-motion guidance via VR teleoperation and micro-hand control handled by an autonomous DexGrasp-VLA policy using real-time tactile/visual feedback. Introduce Arm-Hand Feature Enhancement to capture both macro and micro representations. Include Corrective Teleoperation for human-in-the-loop failure recovery; train end-to-end VLA policy on collected demonstrations.

Result: Ability to generate high-quality data with minimal manpower and achieve ~90% success across diverse and unseen objects; comprehensive experimental validation of dexterous manipulation performance.

Conclusion: Shared autonomy plus enhanced VLA policy enables scalable data collection and effective dexterous manipulation; corrective teleoperation supports continuous policy improvement and recovery.

Abstract: Achieving human-like dexterous manipulation remains a major challenge for
general-purpose robots. While Vision-Language-Action (VLA) models show
potential in learning skills from demonstrations, their scalability is limited
by scarce high-quality training data. Existing data collection methods face
inherent constraints: manual teleoperation overloads human operators, while
automated planning often produces unnatural motions. We propose a Shared
Autonomy framework that divides control between macro and micro motions. A
human operator guides the robot's arm pose through intuitive VR teleoperation,
while an autonomous DexGrasp-VLA policy handles fine-grained hand control using
real-time tactile and visual feedback. This division significantly reduces
cognitive load and enables efficient collection of high-quality coordinated
arm-hand demonstrations. Using this data, we train an end-to-end VLA policy
enhanced with our novel Arm-Hand Feature Enhancement module, which captures
both distinct and shared representations of macro and micro movements for more
natural coordination. Our Corrective Teleoperation system enables continuous
policy improvement through human-in-the-loop failure recovery. Experiments
demonstrate that our framework generates high-quality data with minimal
manpower and achieves a 90% success rate across diverse objects, including
unseen instances. Comprehensive evaluations validate the system's effectiveness
in developing dexterous manipulation capabilities.

</details>


### [430] [EgoMI: Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations](https://arxiv.org/abs/2511.00153)
*Justin Yu,Yide Shentu,Di Wu,Pieter Abbeel,Ken Goldberg,Philipp Wu*

Main category: cs.RO

TL;DR: EgoMI enables imitation learning from egocentric human manipulation by recording synchronized hand and head trajectories, retargeting to semi-humanoid robots, and using a memory-augmented policy to handle rapid viewpoint changes, leading to improved performance over baselines.


<details>
  <summary>Details</summary>
Motivation: The embodiment gap between human egocentric data and robot sensing degrades imitation performance; humans coordinate gaze and hand actions, causing dynamic head motions and viewpoint shifts that static sensing cannot reproduce.

Method: Capture synchronized end-effector and active head trajectories during manipulation (EgoMI), retarget data to compatible semi-humanoid embodiments, and employ a memory-augmented policy that selectively uses historical observations to cope with rapid, wide head viewpoint changes.

Result: On a bimanual robot with an actuated camera head, policies that model head motion consistently outperform baselines, indicating that coordinated hand-eye learning with EgoMI helps bridge the embodiment gap for robust imitation learning.

Conclusion: Explicitly modeling head motion and leveraging memory for historical observations improves imitation from egocentric human data on semi-humanoid robots, suggesting a viable path to bridging the human-robot embodiment gap.

Abstract: Imitation learning from human demonstrations offers a promising approach for
robot skill acquisition, but egocentric human data introduces fundamental
challenges due to the embodiment gap. During manipulation, humans actively
coordinate head and hand movements, continuously reposition their viewpoint and
use pre-action visual fixation search strategies to locate relevant objects.
These behaviors create dynamic, task-driven head motions that static robot
sensing systems cannot replicate, leading to a significant distribution shift
that degrades policy performance. We present EgoMI (Egocentric Manipulation
Interface), a framework that captures synchronized end-effector and active head
trajectories during manipulation tasks, resulting in data that can be
retargeted to compatible semi-humanoid robot embodiments. To handle rapid and
wide-spanning head viewpoint changes, we introduce a memory-augmented policy
that selectively incorporates historical observations. We evaluate our approach
on a bimanual robot equipped with an actuated camera head and find that
policies with explicit head-motion modeling consistently outperform baseline
methods. Results suggest that coordinated hand-eye learning with EgoMI
effectively bridges the human-robot embodiment gap for robust imitation
learning on semi-humanoid embodiments. Project page:
https://egocentric-manipulation-interface.github.io

</details>


### [431] [Reducing Robotic Upper-Limb Assessment Time While Maintaining Precision: A Time Series Foundation Model Approach](https://arxiv.org/abs/2511.00193)
*Faranak Akbarifar,Nooshin Maghsoodi,Sean P Dukelow,Stephen Scott,Parvin Mousavi*

Main category: cs.RO

TL;DR: Foundation-model forecasting can replace unrecorded Kinarm reaching trials, preserving reliability of kinematic features while dramatically reducing assessment time, especially for stroke patients.


<details>
  <summary>Details</summary>
Motivation: To reduce time and fatigue burden of Visually Guided Reaching (VGR) assessments on the Kinarm robot by imputing missing trials with time-series forecasts, while maintaining the reliability of key kinematic biomarkers.

Method: Analyze VGR speed signals from 461 stroke and 599 control participants across 4- and 8-target reaching protocols. Hold back the first 8 or 16 trials and forecast synthetic trials using ARIMA, MOMENT, and Chronos models fine-tuned on 70% of subjects. Recompute reaction time, movement time, posture speed, and maximum speed on combined recorded+forecasted trials. Evaluate reliability against full-length references using ICC(2,1).

Result: Chronos forecasts restore ICC â¥ 0.90 for all four features with only 8 recorded trials plus forecasts, matching the reliability of 24â28 recorded reaches (Î ICC â¤ 0.07). MOMENT shows intermediate gains; ARIMA yields minimal improvement. Synthetic trials reliably replace additional reaches across cohorts/protocols without materially compromising feature reliability.

Conclusion: Forecast-augmented Kinarm VGR can substantially shorten assessment time, reducing sessions for most impaired stroke survivors from 4â5 minutes to ~1 minute while preserving kinematic precision, enabling efficient robotic evaluations of post-stroke motor impairments.

Abstract: Purpose: Visually Guided Reaching (VGR) on the Kinarm robot yields sensitive
kinematic biomarkers but requires 40-64 reaches, imposing time and fatigue
burdens. We evaluate whether time-series foundation models can replace
unrecorded trials from an early subset of reaches while preserving the
reliability of standard Kinarm parameters.
  Methods: We analyzed VGR speed signals from 461 stroke and 599 control
participants across 4- and 8-target reaching protocols. We withheld all but the
first 8 or 16 reaching trials and used ARIMA, MOMENT, and Chronos models,
fine-tuned on 70 percent of subjects, to forecast synthetic trials. We
recomputed four kinematic features of reaching (reaction time, movement time,
posture speed, maximum speed) on combined recorded plus forecasted trials and
compared them to full-length references using ICC(2,1).
  Results: Chronos forecasts restored ICC >= 0.90 for all parameters with only
8 recorded trials plus forecasts, matching the reliability of 24-28 recorded
reaches (Delta ICC <= 0.07). MOMENT yielded intermediate gains, while ARIMA
improvements were minimal. Across cohorts and protocols, synthetic trials
replaced reaches without materially compromising feature reliability.
  Conclusion: Foundation-model forecasting can greatly shorten Kinarm VGR
assessment time. For the most impaired stroke survivors, sessions drop from 4-5
minutes to about 1 minute while preserving kinematic precision. This
forecast-augmented paradigm promises efficient robotic evaluations for
assessing motor impairments following stroke.

</details>


### [432] [Tailored robotic training improves hand function and proprioceptive processing in stroke survivors with proprioceptive deficits: A randomized controlled trial](https://arxiv.org/abs/2511.00259)
*Andria J. Farrens,Luis Garcia-Fernandez,Raymond Diaz Rojas,Jillian Obeso Estrada,Dylan Reinsdorf,Vicky Chan,Disha Gupta,Joel Perry,Eric Wolbrecht,An Do,Steven C. Cramer,David J. Reinkensmeyer*

Main category: cs.RO

TL;DR: Proprioceptively-tailored robotic training can improve hand function and proprioceptive neural processing after stroke, with two approaches (Propriopixel Training and Virtual Assistance Training) showing benefits, especially in those with proprioceptive deficits; Propriopixel yielded the strongest functional gains and an EEG biomarker of proprioceptive processing.


<details>
  <summary>Details</summary>
Motivation: To test whether tailoring rehabilitation to proprioceptive processing using robotic-assisted, gamified interventions enhances functional recovery and neural sensitivity after stroke, addressing heterogeneity in proprioceptive deficits.

Method: Randomized controlled trial with 46 chronic stroke survivors, nine 2-hour sessions, three arms (Standard, Propriopixel, Virtual). Used a robotic finger exoskeleton delivering proprioceptively-tailored training. Assessed hand function (Box and Block Test), proprioceptive status, and neural processing via EEG, including a novel proprioceptive Contingent Negative Variation biomarker.

Result: In participants with proprioceptive deficits, Propriopixel improved hand function by 7Â±4.2 blocks (p=0.002) and Virtual Assistance by 4.5Â±4.4 blocks (p=0.068) versus Standard (0.8Â±2.3). Proprioceptive gains correlated with functional gains. Tailored training enhanced neural sensitivity to proprioception, indicated by the proprioceptive Contingent Negative Variation EEG biomarker.

Conclusion: Proprioceptively-tailored training is a promising avenue for precision neurorehabilitation after stroke, improving both function and proprioceptive neural processing, with potential for personalized interventions based on proprioceptive status.

Abstract: Precision rehabilitation aims to tailor movement training to improve
outcomes. We tested whether proprioceptively-tailored robotic training improves
hand function and neural processing in stroke survivors. Using a robotic finger
exoskeleton, we tested two proprioceptively-tailored approaches: Propriopixel
Training, which uses robot-facilitated, gamified movements to enhance
proprioceptive processing, and Virtual Assistance Training, which reduces
robotic aid to increase reliance on self-generated feedback. In a randomized
controlled trial, forty-six chronic stroke survivors completed nine 2-hour
sessions of Standard, Propriopixel or Virtual training. Among participants with
proprioceptive deficits, Propriopixel ((Box and Block Test: 7 +/- 4.2, p=0.002)
and Virtual Assistance (4.5 +/- 4.4 , p=0.068) yielded greater gains in hand
function (Standard: 0.8 +/- 2.3 blocks). Proprioceptive gains correlated with
improvements in hand function. Tailored training enhanced neural sensitivity to
proprioceptive cues, evidenced by a novel EEG biomarker, the proprioceptive
Contingent Negative Variation. These findings support proprioceptively-tailored
training as a pathway to precision neurorehabilitation.

</details>


### [433] [FGO MythBusters: Explaining how Kalman Filter variants achieve the same performance as FGO in navigation applications](https://arxiv.org/abs/2511.00306)
*Baoshan Song,Ruijie Xu,Li-Ta Hsu*

Main category: cs.RO

TL;DR: SW-FGO can reproduce Kalman filter variants under certain conditions and offers robustness to non-Gaussianity with predictable cost; it bridges SW-FGO and EKF family via a Re-FGO framework.


<details>
  <summary>Details</summary>
Motivation: Clarify the theoretical relationship between SW-FGO and Kalman filter variants; establish when SW-FGO exactly matches EKF family and when it provides additional benefits.

Method: Introduce a recursive FGO (Re-FGO) framework; derive explicit conditions (Markov assumption, Gaussian noise with L2 loss, one-state window); show that Re-FGO regens EKF/IEKF/REKF/RIEKF exactly under these conditions; discuss computational cost and benefits in nonlinear/non-Gaussian regimes.

Result: Under the stated conditions, Re-FGO exactly regenerates EKF/IEKF/REKF/RIEKF; SW-FGO offers advantages in nonlinear/non-Gaussian settings with predictable compute cost; code/data are open-sourced.

Conclusion: Clarifies the connection between SW-FGO and KF variants; highlights SW-FGOâs practical advantages, especially for numerical estimation and deep learning integration; provides resources for reproduction.

Abstract: Sliding window-factor graph optimization (SW-FGO) has gained more and more
attention in navigation research due to its robust approximation to
non-Gaussian noises and nonlinearity of measuring models. There are lots of
works focusing on its application performance compared to extended Kalman
filter (EKF) but there is still a myth at the theoretical relationship between
the SW-FGO and EKF. In this paper, we find the necessarily fair condition to
connect SW-FGO and Kalman filter variants (KFV) (e.g., EKF, iterative EKF
(IEKF), robust EKF (REKF) and robust iterative EKF (RIEKF)). Based on the
conditions, we propose a recursive FGO (Re-FGO) framework to represent KFV
under SW-FGO formulation. Under explicit conditions (Markov assumption,
Gaussian noise with L2 loss, and a one-state window), Re-FGO regenerates
exactly to EKF/IEKF/REKF/RIEKF, while SW-FGO shows measurable benefits in
nonlinear, non-Gaussian regimes at a predictable compute cost. Finally, after
clarifying the connection between them, we highlight the unique advantages of
SW-FGO in practical phases, especially on numerical estimation and deep
learning integration. The code and data used in this work is open sourced at
https://github.com/Baoshan-Song/KFV-FGO-Comparison.

</details>


### [434] [SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping](https://arxiv.org/abs/2511.00392)
*Lingpeng Chen,Jiakun Tang,Apple Pui-Yi Chui,Ziyang Hong,Junfeng Wu*

Main category: cs.RO

TL;DR: SonarSweep is an end-to-end deep-learning framework for cross-modal fusion of sonar and vision using a plane-sweep approach to produce dense, accurate underwater depth maps, outperforming state-of-the-art in challenging conditions; code and a synchronized stereo-cameraâsonar dataset will be released.


<details>
  <summary>Details</summary>
Motivation: Underwater 3D reconstruction is severely challenged by poor visibility for vision and elevation ambiguity/low resolution for sonar. Existing fusion methods rely on heuristics and flawed geometry, producing artifacts and poor scene modeling; there is a need for robust, learned cross-modal fusion to handle turbidity and complex geometries.

Method: An end-to-end deep learning framework that adapts the principled plane-sweep algorithm for cross-modal fusion between sonar and visual data, enabling dense depth estimation by combining information from both modalities.

Result: Extensive experiments in high-fidelity simulation and real-world environments show SonarSweep yields dense, accurate depth maps and significantly outperforms state-of-the-art methods, especially under high turbidity.

Conclusion: The paper introduces a novel, effective cross-modal fusion approach for underwater 3D reconstruction and pledges to publicly release code and a novel synchronized stereo-cameraâsonar dataset to facilitate further research.

Abstract: Accurate 3D reconstruction in visually-degraded underwater environments
remains a formidable challenge. Single-modality approaches are insufficient:
vision-based methods fail due to poor visibility and geometric constraints,
while sonar is crippled by inherent elevation ambiguity and low resolution.
Consequently, prior fusion technique relies on heuristics and flawed geometric
assumptions, leading to significant artifacts and an inability to model complex
scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep
learning framework that overcomes these limitations by adapting the principled
plane sweep algorithm for cross-modal fusion between sonar and visual data.
Extensive experiments in both high-fidelity simulation and real-world
environments demonstrate that SonarSweep consistently generates dense and
accurate depth maps, significantly outperforming state-of-the-art methods
across challenging conditions, particularly in high turbidity. To foster
further research, we will publicly release our code and a novel dataset
featuring synchronized stereo-camera and sonar data, the first of its kind.

</details>


### [435] [Runge-Kutta Approximations for Direct Coning Compensation Applying Lie Theory](https://arxiv.org/abs/2511.00412)
*John A. Christian,Michael R. Walker II,Wyatt Bridgman,Michael J. Sparapany*

Main category: cs.RO

TL;DR: Introduces a Runge-Kuttaâbased class of coning correction algorithms for strapdown gyro integration; shows a simple case reduces to a popular method and provides a general procedure to generate higher-order coning corrections.


<details>
  <summary>Details</summary>
Motivation: Coning compensation is essential in strapdown inertial navigation to account for sensor rotation during integration; existing algorithms exist, but this work seeks a systematic, higher-order construction from RK methods.

Method: Derives coning corrections directly from Runge-Kutta integration routines; demonstrates that a simple case collapses to a widely used algorithm; outlines a procedure to generate higher-order coning corrections.

Result: Demonstrates the linkage between RK-based corrections and known coning algorithms; provides a clear procedure to construct higher-order algorithms (though specific results/accuracy metrics are not provided in the abstract).

Conclusion: Offers a novel, systematic framework for coning compensation with higher-order accuracy by embedding them into RK-based integration; potential for more accurate strapdown navigation, at the cost of added computational effort.

Abstract: The integration of gyroscope measurements is an essential task for most
navigation systems. Modern vehicles typically use strapdown systems, such that
gyro integration requires coning compensation to account for the sensor's
rotation during the integration. Many coning compensation algorithms have been
developed and a few are reviewed. This work introduces a new class of coning
correction algorithm built directly from the classical Runge-Kutta integration
routines. A simple case is shown to collapse to one of the most popular coning
algorithms and a clear procedure for generating higher-order algorithms is
presented.

</details>


### [436] [Design and Development of a Modular Bucket Drum Excavator for Lunar ISRU](https://arxiv.org/abs/2511.00492)
*Simon Giel,James Hurrell,Shreya Santra,Ashutosh Mishra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 3D-printed PLA bucket drum for MoonBot demonstrates viable lunar excavation with high mass throughput and low energy, showing compatibility with the modular platform and potential for autonomous enhancements.


<details>
  <summary>Details</summary>
Motivation: Enable sustainable lunar access via ISRU by extracting lunar regolith; this study contributes to a practical, modular excavation tool for resource utilization under the Moonshot program.

Method: Prototype bucket drum fabricated by 3D printing (PLA); sandbox tests to measure continuous and batch excavation rates and energy per kilogram, plus assessment of size and compatibility with MoonBot.

Result: Mass throughput: continuous 777.54 kg/h; energy intensity 0.022 Wh/kg. Batch throughput: 172.02 kg/h; energy intensity 0.86 Wh/kg. Tool weight 4.8 kg; volume 14.06 L. 3D-printed PLLA; demonstrated compatibility with MoonBot; autonomous sensor integration suggested as future enhancement.

Conclusion: The concept is successfully implemented; the tool's modular compatibility supports flexible mission planning for lunar ISRU; future work should integrate sensors and autonomous control to improve excavation efficiency and autonomy.

Abstract: In-Situ Resource Utilization (ISRU) is one of the key technologies for
enabling sustainable access to the Moon. The ability to excavate lunar regolith
is the first step in making lunar resources accessible and usable. This work
presents the development of a bucket drum for the modular robotic system
MoonBot, as part of the Japanese Moonshot program. A 3D-printed prototype made
of PLA was manufactured to evaluate its efficiency through a series of sandbox
tests. The resulting tool weighs 4.8 kg and has a volume of 14.06 L. It is
capable of continuous excavation at a rate of 777.54 kg/h with a normalized
energy consumption of 0.022 Wh/kg. In batch operation, the excavation rate is
172.02 kg/h with a normalized energy consumption of 0.86 Wh per kilogram of
excavated material. The obtained results demonstrate the successful
implementation of the concept. A key advantage of the developed tool is its
compatibility with the modular MoonBot robotic platform, which enables flexible
and efficient mission planning. Further improvements may include the
integration of sensors and an autonomous control system to enhance the
excavation process.

</details>


### [437] [Descriptive Model-based Learning and Control for Bipedal Locomotion](https://arxiv.org/abs/2511.00512)
*Suraj Kumar,Andy Ruina*

Main category: cs.RO

TL;DR: A descriptive, minimal-DOF control framework for bipedal balance extends free-space high-dimensional dynamics while constraining only the low-dimensional projection, yielding efficient, robust gait without prescribing a full low-dimensional model.


<details>
  <summary>Details</summary>
Motivation: Current bipedal balance methods rely on low-dimensional models, which constrains full robot behavior and often leads to inefficient, bent-knee gaits. The work posits that balance is inherently low-dimensional and seeks to exploit high-dimensional state spaces by only constraining a low-dimensional projection.

Method: Introduce a descriptive model with the minimum degrees of freedom necessary to maintain balance and allow remaining high-dimensional DOF to evolve freely. The control framework avoids prescribing a full low-dimensional model to the entire robot and instead constrains only the low-dimensional projection.

Result: The approach yields an efficient, human-like walking gait and improved robustness, by leveraging low-dimensional balance descriptors while letting high-dimensional dynamics unfold freely.

Conclusion: Bipedal balance can be effectively controlled using minimal DOF descriptors that guard the low-dimensional projection; full high-dimensional motion is permitted in unconstrained spaces, enabling robust and efficient gait without reliance on conventional low-dimensional walking models.

Abstract: Bipedal balance is challenging due to its multi-phase, hybrid nature and
high-dimensional state space. Traditional balance control approaches for
bipedal robots rely on low-dimensional models for locomotion planning and
reactive control, constraining the full robot to behave like these simplified
models. This involves tracking preset reference paths for the Center of Mass
and upper body obtained through low-dimensional models, often resulting in
inefficient walking patterns with bent knees. However, we observe that bipedal
balance is inherently low-dimensional and can be effectively described with
simple state and action descriptors in a low-dimensional state space. This
allows the robot's motion to evolve freely in its high-dimensional state space,
only constraining its projection in the low-dimensional state space. In this
work, we propose a novel control approach that avoids prescribing a
low-dimensional model to the full model. Instead, our control framework uses a
descriptive model with the minimum degrees of freedom necessary to maintain
balance, allowing the remaining degrees of freedom to evolve freely in the
high-dimensional space. This results in an efficient human-like walking gait
and improved robustness.

</details>


### [438] [Adaptive and Multi-object Grasping via Deformable Origami Modules](https://arxiv.org/abs/2511.00516)
*Peiyi Wang,Paul A. M. Lefeuvre,Shangwei Zou,Zhenwei Ni,Daniela Rus,Cecilia Laschi*

Main category: cs.RO

TL;DR: Soft gripper with passively deformable origami fingers driven by a 1-DoF actuator achieves constant force/torque and enables simultaneous multi-object grasping without sensing, improving pick-and-place efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the need for stable grasping of fragile or geometrically complex objects without bulky actuators or complex sensing/control; enable efficient multi-object manipulation.

Method: A multi-finger hybrid gripper uses parallel origami modules per finger driven by a 1-DoF actuator, providing passive adaptability and constant output without active sensing; demonstrates simultaneous grasping of stacked objects of varied shapes/sizes and independent placement.

Result: The gripper can grasp multiple objects simultaneously and manipulate them independently with stable grasping and no active sensing, improving manipulation efficiency over single-object grasping.

Conclusion: Origami-based compliant structures are scalable modules for adaptive, stable, and efficient multi-object manipulation in domestic and industrial settings.

Abstract: Soft robotics gripper have shown great promise in handling fragile and
geometrically complex objects. However, most existing solutions rely on bulky
actuators, complex control strategies, or advanced tactile sensing to achieve
stable and reliable grasping performance. In this work, we present a
multi-finger hybrid gripper featuring passively deformable origami modules that
generate constant force and torque output. Each finger composed of parallel
origami modules is driven by a 1-DoF actuator mechanism, enabling passive shape
adaptability and stable grasping force without active sensing or feedback
control. More importantly, we demonstrate an interesting capability in
simultaneous multi-object grasping, which allows stacked objects of varied
shape and size to be picked, transported and placed independently at different
states, significantly improving manipulation efficiency compared to
single-object grasping. These results highlight the potential of origami-based
compliant structures as scalable modules for adaptive, stable and efficient
multi-object manipulation in domestic and industrial pick-and-place scenarios.

</details>


### [439] [Improving Robustness to Out-of-Distribution States in Imitation Learning via Deep Koopman-Boosted Diffusion Policy](https://arxiv.org/abs/2511.00555)
*Dianye Huang,Nassir Navab,Zhongliang Jiang*

Main category: cs.RO

TL;DR: D3P is a dual-branch diffusion policy with a Deep Koopman module that decouples visual and proprioceptive inputs to improve imitation learning for robotic manipulation, enabling recovery via visual-chunk policies and robust temporal modeling.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based imitation learning struggles with strong temporal dependencies and tends to overfit to proprioceptive cues, hindering leverage of visual task structure. A modality-decoupled, temporally aware policy with recovery mechanisms can improve robustness and performance in manipulation tasks.

Method: Introduce a Dual-branch Diffusion Policy (D3P) with a visual branch and a fused branch. The visual branch encodes visual observations to indicate task progress; the fused branch integrates visual and proprioceptive inputs for manipulation. A Deep Koopman Operator module learns structured temporal dynamics from visual inputs. During inference, the test-time loss of the generative model serves as a confidence signal to gate aggregation of overlapping predicted action chunks, enhancing reliability. If intermediate goals fail (e.g., grasping), the policy switches to action chunks generated by the visual branch to recover to previously observed states. 

Result: In RLBench tabletop tasks (6 tasks in simulation), D3P outperforms the state-of-the-art diffusion policy by 14.6% on average. In three real-world robotic manipulation tasks, it improves by 15.0%.

Conclusion: D3P effectively decouples modalities and leverages temporal dynamics to improve robustness and performance in diffusion-based imitation learning for robotic manipulation, with a recovery mechanism via visual-chunk policies and a confidence-guided aggregation strategy. Code is available at the provided URL.

Abstract: Integrating generative models with action chunking has shown significant
promise in imitation learning for robotic manipulation. However, the existing
diffusion-based paradigm often struggles to capture strong temporal
dependencies across multiple steps, particularly when incorporating
proprioceptive input. This limitation can lead to task failures, where the
policy overfits to proprioceptive cues at the expense of capturing the visually
derived features of the task. To overcome this challenge, we propose the Deep
Koopman-boosted Dual-branch Diffusion Policy (D3P) algorithm. D3P introduces a
dual-branch architecture to decouple the roles of different sensory modality
combinations. The visual branch encodes the visual observations to indicate
task progression, while the fused branch integrates both visual and
proprioceptive inputs for precise manipulation. Within this architecture, when
the robot fails to accomplish intermediate goals, such as grasping a drawer
handle, the policy can dynamically switch to execute action chunks generated by
the visual branch, allowing recovery to previously observed states and
facilitating retrial of the task. To further enhance visual representation
learning, we incorporate a Deep Koopman Operator module that captures
structured temporal dynamics from visual inputs. During inference, we use the
test-time loss of the generative model as a confidence signal to guide the
aggregation of the temporally overlapping predicted action chunks, thereby
enhancing the reliability of policy execution. In simulation experiments across
six RLBench tabletop tasks, D3P outperforms the state-of-the-art diffusion
policy by an average of 14.6\%. On three real-world robotic manipulation tasks,
it achieves a 15.0\% improvement. Code: https://github.com/dianyeHuang/D3P.

</details>


### [440] [Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles](https://arxiv.org/abs/2511.00635)
*Hyungtae Lim,Daebeom Kim,Hyun Myung*

Main category: cs.RO

TL;DR: Proposes Multi-Mapcher for MSS with heterogeneous LiDARs using map-to-map registration for inter-session alignment, outperforming loop-closure reliant methods and faster.


<details>
  <summary>Details</summary>
Motivation: Existing MSS methods rely on loop closure but performance degrades with varying sensor density/FoV; need robust inter-session alignment across different LiDARs.

Method: Introduce Multi-Mapcher: large-scale map-to-map registration using outlier-robust 3D point cloud registration to obtain initial inter-session alignment; then radius-based loop detection; anchor node-based robust pose graph optimization to fuse into a global map.

Result: Experiments show substantially better MSS performance across different LiDAR sensors and faster than state-of-the-art.

Conclusion: Demonstrates effectiveness of map-to-map registration-based MSS framework; code released at GitHub.

Abstract: As various 3D light detection and ranging (LiDAR) sensors have been
introduced to the market, research on multi-session simultaneous localization
and mapping (MSS) using heterogeneous LiDAR sensors has been actively
conducted. Existing MSS methods mostly rely on loop closure detection for
inter-session alignment; however, the performance of loop closure detection can
be potentially degraded owing to the differences in the density and field of
view (FoV) of the sensors used in different sessions. In this study, we
challenge the existing paradigm that relies heavily on loop detection modules
and propose a novel MSS framework, called Multi-Mapcher, that employs
large-scale map-to-map registration to perform inter-session initial alignment,
which is commonly assumed to be infeasible, by leveraging outlier-robust 3D
point cloud registration. Next, after finding inter-session loops by radius
search based on the assumption that the inter-session initial alignment is
sufficiently precise, anchor node-based robust pose graph optimization is
employed to build a consistent global map. As demonstrated in our experiments,
our approach shows substantially better MSS performance for various LiDAR
sensors used to capture the sessions and is faster than state-of-the-art
approaches. Our code is available at
https://github.com/url-kaist/multi-mapcher.

</details>


### [441] [When Semantics Connect the Swarm: LLM-Driven Fuzzy Control for Cooperative Multi-Robot Underwater Coverage](https://arxiv.org/abs/2511.00783)
*Jingzehua Xu,Weihang Zhang,Yangyang Li,Hongmiaoyi Zhang,Guanwen Xie,Jiwei Tang,Shuai Zhang,Yi Li*

Main category: cs.RO

TL;DR: A semantics-guided LLMâfuzzy control framework enables GPS-denied underwater multi-robot coverage via semantic tokens and lightweight coordination for robust OOI navigation and reduced redundancy.


<details>
  <summary>Details</summary>
Motivation: Underwater multi-robot systems suffer from partial observability, limited comms, environmental uncertainty, and no access to global localization; need interpretable, cooperative, robust navigation and coverage.

Method: LLMs compress multimodal observations into semantic tokens (obstacles, unexplored areas, OOIs). A predefined fuzzy inference system maps tokens to smooth steering/gait commands for GPS-denied navigation. Semantic communication shares intent and local context to coordinate exploration and avoid redundant revisits among multiple robots.

Result: Extensive simulations in unknown reef-like environments demonstrate robust OOI-oriented navigation and cooperative coverage under limited sensing and communication, with improved efficiency and adaptability, bridging semantic cognition and distributed underwater control in GPS-denied, map-free settings.

Conclusion: The framework enables reliable, interpretable, and scalable underwater multi-robot coordination and coverage without global positioning by leveraging semantic tokens, fuzzy control, and lightweight semantic communication.

Abstract: Underwater multi-robot cooperative coverage remains challenging due to
partial observability, limited communication, environmental uncertainty, and
the lack of access to global localization. To address these issues, this paper
presents a semantics-guided fuzzy control framework that couples Large Language
Models (LLMs) with interpretable control and lightweight coordination. Raw
multimodal observations are compressed by the LLM into compact,
human-interpretable semantic tokens that summarize obstacles, unexplored
regions, and Objects Of Interest (OOIs) under uncertain perception. A fuzzy
inference system with pre-defined membership functions then maps these tokens
into smooth and stable steering and gait commands, enabling reliable navigation
without relying on global positioning. Then, we further coordinate multiple
robots by introducing semantic communication that shares intent and local
context in linguistic form, enabling agreement on who explores where while
avoiding redundant revisits. Extensive simulations in unknown reef-like
environments show that, under limited sensing and communication, the proposed
framework achieves robust OOI-oriented navigation and cooperative coverage with
improved efficiency and adaptability, narrowing the gap between semantic
cognition and distributed underwater control in GPS-denied, map-free
conditions.

</details>


### [442] [Real-Time Learning of Predictive Dynamic Obstacle Models for Robotic Motion Planning](https://arxiv.org/abs/2511.00814)
*Stella Kombo,Masih Haseli,Skylar Wei,Joel W. Burdick*

Main category: cs.RO

TL;DR: Online nonlinear predictor for agent motions using a modified Hankel-DMD with SVHT and Cadzow projection for denoising, yielding variance-aware short-horizon forecasts suitable for real-time control.


<details>
  <summary>Details</summary>
Motivation: Autonomous systems must predict others' motions from partial, noisy data in real time to enable safe control and planning.

Method: Embed partial noisy measurements in a sliding-window Hankel matrix, use a Page matrix for SVHT-based rank estimation, apply a Cadzow projection for structured low-rank consistency, and build a time-varying Hankel-DMD lifted linear predictor for multi-step forecasts with residual variance tracking.

Result: Demonstrates stable variance-aware denoising and accurate short-horizon predictions under Gaussian and heavy-tailed noise, validated in simulation and on a dynamic crane testbed, indicating real-time applicability.

Conclusion: The approach provides real-time, variance-aware motion predictions with a structured low-rank denoising step, enabling downstream estimators and risk-aware planning in autonomous systems.

Abstract: Autonomous systems often must predict the motions of nearby agents from
partial and noisy data. This paper asks and answers the question: "can we
learn, in real-time, a nonlinear predictive model of another agent's motions?"
Our online framework denoises and forecasts such dynamics using a modified
sliding-window Hankel Dynamic Mode Decomposition (Hankel-DMD). Partial noisy
measurements are embedded into a Hankel matrix, while an associated Page matrix
enables singular-value hard thresholding (SVHT) to estimate the effective rank.
A Cadzow projection enforces structured low-rank consistency, yielding a
denoised trajectory and local noise variance estimates. From this
representation, a time-varying Hankel-DMD lifted linear predictor is
constructed for multi-step forecasts. The residual analysis provides
variance-tracking signals that can support downstream estimators and risk-aware
planning. We validate the approach in simulation under Gaussian and
heavy-tailed noise, and experimentally on a dynamic crane testbed. Results show
that the method achieves stable variance-aware denoising and short-horizon
prediction suitable for integration into real-time control frameworks.

</details>


### [443] [Heuristic Step Planning for Learning Dynamic Bipedal Locomotion: A Comparative Study of Model-Based and Model-Free Approaches](https://arxiv.org/abs/2511.00840)
*William Suliman,Ekaterina Chaikovskaia,Egor Davydenko,Roman Gorbachev*

Main category: cs.RO

TL;DR: A learning-based bipedal framework using heuristic step planning guided by torso-velocity tracking achieves robust, energy-efficient walking with competitive velocity control, reducing reliance on complex model-based planning.


<details>
  <summary>Details</summary>
Motivation: Investigate whether simplified, heuristic planning within a learning framework can match or exceed model-based methods in robust bipedal locomotion on unstructured terrains, enabling tasks like gap crossing and target approach while improving efficiency.

Method: Extend a learning-based locomotion pipeline with a heuristic step-planning strategy driven by desired torso velocity; modulate foot placement length via a Raibert-type controller based on velocity error; compare against a Linear Inverted Pendulum Model (LIPM) controller; avoid reliance on full/detailed dynamic models.

Result: Experimental results show comparable or superior target-velocity tracking (up to 80% accuracy), significantly greater robustness on uneven terrain (over 50% improvement), and improved energy efficiency compared to the LIPM-based approach.

Conclusion: Incorporating complex analytical or model-based components into the training architecture may be unnecessary for achieving stable and robust bipedal walking in unstructured environments; a heuristic, learning-based approach can suffice.

Abstract: This work presents an extended framework for learning-based bipedal
locomotion that incorporates a heuristic step-planning strategy guided by
desired torso velocity tracking. The framework enables precise interaction
between a humanoid robot and its environment, supporting tasks such as crossing
gaps and accurately approaching target objects. Unlike approaches based on full
or simplified dynamics, the proposed method avoids complex step planners and
analytical models. Step planning is primarily driven by heuristic commands,
while a Raibert-type controller modulates the foot placement length based on
the error between desired and actual torso velocity. We compare our method with
a model-based step-planning approach -- the Linear Inverted Pendulum Model
(LIPM) controller. Experimental results demonstrate that our approach attains
comparable or superior accuracy in maintaining target velocity (up to 80%),
significantly greater robustness on uneven terrain (over 50% improvement), and
improved energy efficiency. These results suggest that incorporating complex
analytical, model-based components into the training architecture may be
unnecessary for achieving stable and robust bipedal walking, even in
unstructured environments.

</details>


### [444] [Maestro: Orchestrating Robotics Modules with Vision-Language Models for Zero-Shot Generalist Robots](https://arxiv.org/abs/2511.00917)
*Junyao Shi,Rujia Yang,Kaitian Chao,Selina Bingqing Wan,Yifei Shao,Jiahui Lei,Jianing Qian,Long Le,Pratik Chaudhari,Kostas Daniilidis,Chuan Wen,Dinesh Jayaraman*

Main category: cs.RO

TL;DR: Maestro uses a VLM-based coding agent to dynamically compose perception, planning, and control modules into a programmatic policy, enabling generalist robot skills with strong zero-shot manipulation and easy extensibility.


<details>
  <summary>Details</summary>
Motivation: To shift from scaling up observation-in-actions-out datasets toward leveraging large vision-language models by orchestrating modular robot capabilities into generalist policies.

Method: Introduce Maestro, a VLM coding agent that dynamically assembles a curated set of perception, planning, and control modules into a programmable policy for the current task; relies on a streamlined closed-loop interface and a broad tool repertoire; easily extensible to new modules and embodiments.

Result: Maestro largely surpasses contemporary VLA models for zero-shot performance on challenging manipulation skills.

Conclusion: Maestro is easily extensible to incorporate new modules and embodiments, and can adapt from minimal real-world experience via local code edits, offering a practical path to generalist robots.

Abstract: Today's best-explored routes towards generalist robots center on collecting
ever larger "observations-in actions-out" robotics datasets to train large
end-to-end models, copying a recipe that has worked for vision-language models
(VLMs). We pursue a road less traveled: building generalist policies directly
around VLMs by augmenting their general capabilities with specific robot
capabilities encapsulated in a carefully curated set of perception, planning,
and control modules. In Maestro, a VLM coding agent dynamically composes these
modules into a programmatic policy for the current task and scenario. Maestro's
architecture benefits from a streamlined closed-loop interface without many
manually imposed structural constraints, and a comprehensive and diverse tool
repertoire. As a result, it largely surpasses today's VLA models for zero-shot
performance on challenging manipulation skills. Further, Maestro is easily
extensible to incorporate new modules, easily editable to suit new embodiments
such as a quadruped-mounted arm, and even easily adapts from minimal real-world
experiences through local code edits.

</details>


### [445] [Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2511.00933)
*Xiangyu Shi,Zerui Li,Yanyuan Qiao,Qi Wu*

Main category: cs.RO

TL;DR: Fast-SmartWay presents an end-to-end zero-shot VLN-CE method using three frontal RGB-D images and MLLMs to directly predict actions, paired with uncertainty-aware reasoning to reduce latency without using panoramic views or waypoint predictors.


<details>
  <summary>Details</summary>
Motivation: To reduce latency and improve real-world applicability of VLN-CE by avoiding panoramic observations and multi-stage pipelines, while leveraging multimodal large language models for zero-shot navigation.

Method: Use only three frontal RGB-D images plus natural language instructions; MLLMs directly predict actions. Introduce an Uncertainty-Aware Reasoning module comprising (i) a Disambiguation Module to avoid local optima and (ii) a Future-Past Bidirectional Reasoning mechanism for globally coherent planning.

Result: Experiments in simulated and real-robot environments show significantly reduced per-step latency with competitive or superior performance compared to panoramic-view baselines.

Conclusion: Demonstrates practicality and effectiveness of Fast-SmartWay for real-world zero-shot embodied navigation without panoramic views or waypoint predictors.

Abstract: Recent advances in Vision-and-Language Navigation in Continuous Environments
(VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve
zero-shot navigation. However, existing methods often rely on panoramic
observations and two-stage pipelines involving waypoint predictors, which
introduce significant latency and limit real-world applicability. In this work,
we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that
eliminates the need for panoramic views and waypoint predictors. Our approach
uses only three frontal RGB-D images combined with natural language
instructions, enabling MLLMs to directly predict actions. To enhance decision
robustness, we introduce an Uncertainty-Aware Reasoning module that integrates
(i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past
Bidirectional Reasoning mechanism for globally coherent planning. Experiments
on both simulated and real-robot environments demonstrate that our method
significantly reduces per-step latency while achieving competitive or superior
performance compared to panoramic-view baselines. These results demonstrate the
practicality and effectiveness of Fast-SmartWay for real-world zero-shot
embodied navigation.

</details>


### [446] [URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model](https://arxiv.org/abs/2511.00940)
*Zhe Li,Xiang Bai,Jieyu Zhang,Zhuangzhe Wu,Che Xu,Ying Li,Chengkai Hou,Shanghang Zhang*

Main category: cs.RO

TL;DR: Autoregressive, end-to-end URDF reconstruction from 3D multimodal data (point cloud + text) that jointly learns segmentation and kinematic parameters via a SEG token, achieving state-of-the-art geometric and kinematic accuracy and strong generalization for digital twin creation.


<details>
  <summary>Details</summary>
Motivation: Manual modeling of articulated objects is time-consuming and brittle; robust automatic digital twin construction is needed to improve sim-to-real transfer and support embodied AI.

Method: Autoregressive framework that takes point cloud and text as input to jointly optimize geometric segmentation and kinematic parameter prediction. Introduces a specialized [SEG] token that interacts with point cloud features to enable fine-grained part-level segmentation while ensuring consistency with kinematic predictions.

Result: Evaluated on simulated and real-world datasets; mIoU for geometry improves by 17 percentage points; average kinematic error reduced by 29%; physical executability improves by 50% over baselines; strong generalization to unseen objects.

Conclusion: An efficient, scalable solution for automatic digital twin construction that enhances sim-to-real transfer and supports robotic simulation and embodied AI, with demonstrated generalization to objects outside the training set.

Abstract: Constructing accurate digital twins of articulated objects is essential for
robotic simulation training and embodied AI world model building, yet
historically requires painstaking manual modeling or multi-stage pipelines. In
this work, we propose \textbf{URDF-Anything}, an end-to-end automatic
reconstruction framework based on a 3D multimodal large language model (MLLM).
URDF-Anything utilizes an autoregressive prediction framework based on
point-cloud and text multimodal input to jointly optimize geometric
segmentation and kinematic parameter prediction. It implements a specialized
$[SEG]$ token mechanism that interacts directly with point cloud features,
enabling fine-grained part-level segmentation while maintaining consistency
with the kinematic parameter predictions. Experiments on both simulated and
real-world datasets demonstrate that our method significantly outperforms
existing approaches regarding geometric segmentation (mIoU 17\% improvement),
kinematic parameter prediction (average error reduction of 29\%), and physical
executability (surpassing baselines by 50\%). Notably, our method exhibits
excellent generalization ability, performing well even on objects outside the
training set. This work provides an efficient solution for constructing digital
twins for robotic simulation, significantly enhancing the sim-to-real transfer
capability.

</details>


### [447] [Breaking the Latency Barrier: Synergistic Perception and Control for High-Frequency 3D Ultrasound Servoing](https://arxiv.org/abs/2511.00983)
*Yizhao Qian,Yujie Zhu,Jiayuan Luo,Li Liu,Yixuan Yuan,Guochen Ning,Hongen Liao*

Main category: cs.RO

TL;DR: A real-time Robotic Ultrasound System (RUSS) framework that co-designs perception and control to break latency barriers, achieving >60 Hz closed-loop tracking with robust 3D target estimation and large-scale repositioning, validated in dynamic phantom and in vivo.


<details>
  <summary>Details</summary>
Motivation: End-to-end latency and low bandwidth perception in RUSS hinder real-time tracking of dynamic targets. A synergistic co-design of perception and control could unlock high-frequency, robust autonomy in dynamic clinical settings.

Method: 1) Decoupled Dual-Stream Perception Network that estimates 3D translation from 2D ultrasound images at high frequency. 2) Single-Step Flow Policy that generates entire action sequences in one inference pass, bypassing iterative policy bottlenecks.

Result: - Closed-loop control frequency > 60 Hz. - Dynamic phantom: mean 3D trajectory error < 6.5 mm; robust re-acquisition from >170 mm displacement. - Track targets at speeds up to 102 mm/s with terminal error <1.7 mm. - In-vivo experiments on a human volunteer validate effectiveness and robustness in realistic clinical setting.

Conclusion: A holistic RUSS architecture that unifies high-bandwidth perception with large-scale repositioning addresses a critical bottleneck for robust autonomy in dynamic clinical environments.

Abstract: Real-time tracking of dynamic targets amidst large-scale, high-frequency
disturbances remains a critical unsolved challenge in Robotic Ultrasound
Systems (RUSS), primarily due to the end-to-end latency of existing systems.
This paper argues that breaking this latency barrier requires a fundamental
shift towards the synergistic co-design of perception and control. We realize
it in a novel framework with two tightly-coupled contributions: (1) a Decoupled
Dual-Stream Perception Network that robustly estimates 3D translational state
from 2D images at high frequency, and (2) a Single-Step Flow Policy that
generates entire action sequences in one inference pass, bypassing the
iterative bottleneck of conventional policies. This synergy enables a
closed-loop control frequency exceeding 60Hz. On a dynamic phantom, our system
not only tracks complex 3D trajectories with a mean error below 6.5mm but also
demonstrates robust re-acquisition from over 170mm displacement. Furthermore,
it can track targets at speeds of 102mm/s, achieving a terminal error below
1.7mm. Moreover, in-vivo experiments on a human volunteer validate the
framework's effectiveness and robustness in a realistic clinical setting. Our
work presents a RUSS holistically architected to unify high-bandwidth tracking
with large-scale repositioning, a critical step towards robust autonomy in
dynamic clinical environments.

</details>


### [448] [GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies](https://arxiv.org/abs/2511.00998)
*Ziye Wang,Li Kang,Yiran Qin,Jiahua Ma,Zhanglin Peng,Lei Bai,Ruimao Zhang*

Main category: cs.RO

TL;DR: GauDP introduces a scalable, perception-aware imitation learning framework for multi-agent systems by building a global 3D Gaussian field from decentralized RGB observations and distributing its attributes to agents, enabling fine-grained control with globally coherent behavior without extra sensing like 3D point clouds.


<details>
  <summary>Details</summary>
Motivation: The challenge is to coordinate multiple agents while balancing local control with global environmental awareness; existing methods struggle with scalability and collaboration quality when relying solely on imagery or lacking a shared scene representation.

Method: Construct a globally consistent 3D Gaussian field from decentralized RGB observations and dynamically redistribute 3D Gaussian attributes to each agentâs local perspective, allowing agents to query task-critical features from the shared scene representation while preserving individual viewpoints.

Result: GauDP outperforms existing image-based methods and approaches the effectiveness of point-cloud-driven methods on RoboFactory multi-arm manipulation tasks, with strong scalability as the number of agents increases.

Conclusion: A perception-aware, scalable imitation learning framework that achieves fine-grained, globally coherent behavior using RGB data alone, reducing the need for additional sensing modalities while narrowing the gap to point-cloud-based approaches.

Abstract: Recently, effective coordination in embodied multi-agent systems has remained
a fundamental challenge, particularly in scenarios where agents must balance
individual perspectives with global environmental awareness. Existing
approaches often struggle to balance fine-grained local control with
comprehensive scene understanding, resulting in limited scalability and
compromised collaboration quality. In this paper, we present GauDP, a novel
Gaussian-image synergistic representation that facilitates scalable,
perception-aware imitation learning in multi-agent collaborative systems.
Specifically, GauDP constructs a globally consistent 3D Gaussian field from
decentralized RGB observations, then dynamically redistributes 3D Gaussian
attributes to each agent's local perspective. This enables all agents to
adaptively query task-critical features from the shared scene representation
while maintaining their individual viewpoints. This design facilitates both
fine-grained control and globally coherent behavior without requiring
additional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on the
RoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Our
method achieves superior performance over existing image-based methods and
approaches the effectiveness of point-cloud-driven methods, while maintaining
strong scalability as the number of agents increases.

</details>


### [449] [AquaROM: shape optimization pipeline for soft swimmers using parametric reduced order models](https://arxiv.org/abs/2511.01031)
*Mathieu Dubied,Paolo Tiso,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: Introduces a tensorial parametric reduced order model (PROM) for efficient, nonlinear constrained optimization of FEM-modeled soft robots, using dimensionality reduction and analytical gradients in a reduced basis; validated on soft swimmer shape optimization with a data-free ROB.


<details>
  <summary>Details</summary>
Motivation: Optimizing actuated soft structures under nonlinear forces is computationally expensive due to full-order FEM simulations; faster, accurate optimization methods are needed to enable design and control of soft robotics.

Method: Develops a tensorial PROM with dimensionality reduction and solution approximation. It enables analytical gradients within a chosen reduced order basis (ROB) and uses a data-free ROB for fast computations. The approach is applied to optimizing soft robotic swimmer shapes experiencing hydrodynamic, internal, and external nonlinear forces.

Result: Demonstrates computational efficiency gains and the ability to handle nonlinear forces in optimization of soft robots, with a case study on soft swimmer shape optimization using a data-free ROB for fast and accurate evaluations.

Conclusion: The tensorial PROM approach reduces computational complexity and unlocks efficient design and control for complex nonlinear soft robotic systems.

Abstract: The efficient optimization of actuated soft structures, particularly under
complex nonlinear forces, remains a critical challenge in advancing robotics.
Simulations of nonlinear structures, such as soft-bodied robots modeled using
the finite element method (FEM), often demand substantial computational
resources, especially during optimization. To address this challenge, we
propose a novel optimization algorithm based on a tensorial parametric reduced
order model (PROM). Our algorithm leverages dimensionality reduction and
solution approximation techniques to facilitate efficient solving of nonlinear
constrained optimization problems. The well-structured tensorial approach
enables the use of analytical gradients within a specifically chosen reduced
order basis (ROB), significantly enhancing computational efficiency. To
showcase the performance of our method, we apply it to optimizing soft robotic
swimmer shapes. These actuated soft robots experience hydrodynamic forces,
subjecting them to both internal and external nonlinear forces, which are
incorporated into our optimization process using a data-free ROB for fast and
accurate computations. This approach not only reduces computational complexity
but also unlocks new opportunities to optimize complex nonlinear systems in
soft robotics, paving the way for more efficient design and control.

</details>


### [450] [Deployable Vision-driven UAV River Navigation via Human-in-the-loop Preference Alignment](https://arxiv.org/abs/2511.01083)
*Zihan Wang,Jianwen Li,Li-Fan Wu,Nina Mahmoudian*

Main category: cs.RO

TL;DR: SPAR-H combines statewise human preferences with a reward estimator and trust-region updates to enable data-efficient HITL adaptation for UAV river navigation, outperforming IL and RL baselines.


<details>
  <summary>Details</summary>
Motivation: Simulation-trained policies for UAV river following suffer from distribution shift and safety risks; there is a need for data-efficient, human-in-the-loop adaptation with safety constraints and limited interventions.

Method: Introduce Statewise Hybrid Preference Alignment for Robotics (SPAR-H) that fuses direct preference optimization on policy logits with a reward-based pathway that trains an immediate-reward estimator from the same preferences and updates the policy using a trust-region surrogate; uses a conservative overseer to veto unsafe/inefficient actions and provide statewise comparisons.

Result: With five HITL rollouts from a fixed novice policy, SPAR-H achieves the highest final episodic reward and lowest variance across initial conditions among tested methods; the learned reward model aligns with human actions and elevates nearby non-intervened choices, enabling stable improvement propagation; competitive against imitation learning, direct preference variants, and evaluative RL in HITL settings; demonstrates real-world feasibility of continual preference alignment for UAV river following.

Conclusion: Dual statewise preferences offer a practical, data-efficient pathway for online adaptation in riverine navigation, enabling safe, effective HITL-guided policy improvement for UAVs.

Abstract: Rivers are critical corridors for environmental monitoring and disaster
response, where Unmanned Aerial Vehicles (UAVs) guided by vision-driven
policies can provide fast, low-cost coverage. However, deployment exposes
simulation-trained policies with distribution shift and safety risks and
requires efficient adaptation from limited human interventions. We study
human-in-the-loop (HITL) learning with a conservative overseer who vetoes
unsafe or inefficient actions and provides statewise preferences by comparing
the agent's proposal with a corrective override. We introduce Statewise Hybrid
Preference Alignment for Robotics (SPAR-H), which fuses direct preference
optimization on policy logits with a reward-based pathway that trains an
immediate-reward estimator from the same preferences and updates the policy
using a trust-region surrogate. With five HITL rollouts collected from a fixed
novice policy, SPAR-H achieves the highest final episodic reward and the lowest
variance across initial conditions among tested methods. The learned reward
model aligns with human-preferred actions and elevates nearby non-intervened
choices, supporting stable propagation of improvements. We benchmark SPAR-H
against imitation learning (IL), direct preference variants, and evaluative
reinforcement learning (RL) in the HITL setting, and demonstrate real-world
feasibility of continual preference alignment for UAV river following. Overall,
dual statewise preferences empirically provide a practical route to
data-efficient online adaptation in riverine navigation.

</details>


### [451] [SLAP: Shortcut Learning for Abstract Planning](https://arxiv.org/abs/2511.01107)
*Y. Isabel Liu,Bowen Li,Benjamin Eysenbach,Tom Silver*

Main category: cs.RO

TL;DR: SLAP learns abstract shortcuts in a TAMP planning graph via model-free RL, automatically discovering new options that shorten plans and boost task success. It uncovers dynamic improvisations beyond manually defined options, reducing plan lengths by over 50% and outperforming planning and RL baselines across four simulated robotic environments.


<details>
  <summary>Details</summary>
Motivation: Long-horizon decision-making with sparse rewards in continuous state/action spaces is challenging. TAMP relies on manually defined options, which limits the agent to human-known behaviors; there is a need to automatically discover useful abstract actions to improve planning efficiency and generalization.

Method: Apply model-free reinforcement learning to the abstract planning graph generated by existing TAMP options to learn shortcut actions. No extra inputs are required. The approach discovers new, potentially dynamic improvisations and evaluates performance on four simulated robotic environments, comparing against planning and RL baselines.

Result: SLAP yields shorter plans than pure planning and higher task success rates than flat and hierarchical RL. It reduces overall plan lengths by over 50% and generalizes across a wide range of tasks. It also qualitatively discovers improvisations such as slap, wiggle, and wipe that differ from manually-defined options.

Conclusion: Automated shortcut learning in abstract planning graphs can significantly extend the capabilities of TAMP, enabling more efficient planning, better generalization, and discovery of novel behaviors beyond manual option design.

Abstract: Long-horizon decision-making with sparse rewards and continuous states and
actions remains a fundamental challenge in AI and robotics. Task and motion
planning (TAMP) is a model-based framework that addresses this challenge by
planning hierarchically with abstract actions (options). These options are
manually defined, limiting the agent to behaviors that we as human engineers
know how to program (pick, place, move). In this work, we propose Shortcut
Learning for Abstract Planning (SLAP), a method that leverages existing TAMP
options to automatically discover new ones. Our key idea is to use model-free
reinforcement learning (RL) to learn shortcuts in the abstract planning graph
induced by the existing options in TAMP. Without any additional assumptions or
inputs, shortcut learning leads to shorter solutions than pure planning, and
higher task success rates than flat and hierarchical RL. Qualitatively, SLAP
discovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that
differ significantly from the manually-defined ones. In experiments in four
simulated robotic environments, we show that SLAP solves and generalizes to a
wide range of tasks, reducing overall plan lengths by over 50% and consistently
outperforming planning and RL baselines.

</details>


### [452] [An Enhanced Proprioceptive Method for Soft Robots Integrating Bend Sensors and IMUs](https://arxiv.org/abs/2511.01165)
*Dong Heon Han,Mayank Mehta,Runze Zuo,Zachary Wanger,Daniel Bruder*

Main category: cs.RO

TL;DR: Fusing off-the-shelf IMU and bend sensors with Kalman filtering enables robust, long-duration shape estimation for soft robots at low cost, outperforming IMU-only methods.


<details>
  <summary>Details</summary>
Motivation: Soft robots require accurate proprioception over extended periods, but sensor drift and cost hinder reliability and practicality. Off-the-shelf sensors combined with data fusion can provide robust shape estimation while remaining affordable and scalable.

Method: Integrate IMUs with complementary bend sensors. Use a Kalman filter to fuse segment tip orientations from both sensors in a mutually compensatory manner. Apply a piecewise constant curvature model to estimate the tip location from the fused orientation data and reconstruct the robot's deformation.

Result: Experiments under no loading, external forces, and passive obstacle interactions during 45 minutes of continuous operation yielded an RMSE of 16.96 mm (2.91% of total length), a 56% reduction compared to IMU-only benchmarks, indicating improved accuracy, robustness, and long-term proprioception.

Conclusion: The proposed approach delivers accurate, robust, and cost-effective long-duration proprioception for soft robots, maintaining high performance across diverse conditions.

Abstract: This study presents an enhanced proprioceptive method for accurate shape
estimation of soft robots using only off-the-shelf sensors, ensuring
cost-effectiveness and easy applicability. By integrating inertial measurement
units (IMUs) with complementary bend sensors, IMU drift is mitigated, enabling
reliable long-term proprioception. A Kalman filter fuses segment tip
orientations from both sensors in a mutually compensatory manner, improving
shape estimation over single-sensor methods. A piecewise constant curvature
model estimates the tip location from the fused orientation data and
reconstructs the robot's deformation. Experiments under no loading, external
forces, and passive obstacle interactions during 45 minutes of continuous
operation showed a root mean square error of 16.96 mm (2.91% of total length),
a 56% reduction compared to IMU-only benchmarks. These results demonstrate that
our approach not only enables long-duration proprioception in soft robots but
also maintains high accuracy and robustness across these diverse conditions.

</details>


### [453] [Scaling Cross-Embodiment World Models for Dexterous Manipulation](https://arxiv.org/abs/2511.01177)
*Zihao He,Bo Ai,Tongzhou Mu,Yulin Liu,Weikang Wan,Jiawei Fu,Yilun Du,Henrik I. Christensen,Hao Su*

Main category: cs.RO

TL;DR: A unified, embodiment-invariant world model using particle-based state/action representations and graph networks enables cross-embodiment manipulation, with improved generalization as more embodiments are included and when training uses both simulated and real data.


<details>
  <summary>Details</summary>
Motivation: Cross-embodiment dexterous manipulation is hampered by differences in action spaces and kinematics. The core question is whether environment dynamics can be invariant across embodiments so that world models can serve as a unified interface.

Method: Represent different embodiments as sets of 3D particles and define actions as particle displacements to create a shared state-action representation. Train a graph-based world model on exploration data from diverse simulated robot hands and real human hands, then integrate with model-based planning for deployment on novel hardware.

Result: Experiments on rigid and deformable manipulation tasks show: (i) scaling to more training embodiments improves generalization to unseen ones; (ii) co-training on both simulated and real data outperforms training on either alone; (iii) learned models enable effective control on robots with varied degrees of freedom.

Conclusion: World models can serve as a promising interface for cross-embodiment dexterous manipulation. Increasing the number of training embodiments and combining simulated with real data enhances transfer to unseen morphologies and DOF, enabling control across diverse hardware.

Abstract: Cross-embodiment learning seeks to build generalist robots that operate
across diverse morphologies, but differences in action spaces and kinematics
hinder data sharing and policy transfer. This raises a central question: Is
there any invariance that allows actions to transfer across embodiments? We
conjecture that environment dynamics are embodiment-invariant, and that world
models capturing these dynamics can provide a unified interface across
embodiments. To learn such a unified world model, the crucial step is to design
state and action representations that abstract away embodiment-specific details
while preserving control relevance. To this end, we represent different
embodiments (e.g., human hands and robot hands) as sets of 3D particles and
define actions as particle displacements, creating a shared representation for
heterogeneous data and control problems. A graph-based world model is then
trained on exploration data from diverse simulated robot hands and real human
hands, and integrated with model-based planning for deployment on novel
hardware. Experiments on rigid and deformable manipulation tasks reveal three
findings: (i) scaling to more training embodiments improves generalization to
unseen ones, (ii) co-training on both simulated and real data outperforms
training on either alone, and (iii) the learned models enable effective control
on robots with varied degrees of freedom. These results establish world models
as a promising interface for cross-embodiment dexterous manipulation.

</details>


### [454] [LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping](https://arxiv.org/abs/2511.01186)
*Lijie Wang,Lianjie Guo,Ziyi Xu,Qianhao Wang,Fei Gao,Xieyuanli Chen*

Main category: cs.RO

TL;DR: LiDAR-VGGT integrates LiDAR-inertial odometry with VGGT via a two-stage coarse-to-fine fusion to produce dense, globally consistent colored point clouds with metric scale, addressing scale and FOV-related distortions; it outperforms VGGT-based methods and LIVO baselines and will release an open-source evaluation toolkit.


<details>
  <summary>Details</summary>
Motivation: The creation of large-scale, metric-scale colored 3D reconstructions is hindered by VGGT's lack of metric scale and LIVO's sensitivity to extrinsic calibration, motivating a tightly-coupled LiDAR-inertial-vision fusion approach.

Method: A two-stage fusion: (1) pre-fusion with robust initialization that estimates VGGT poses and colored point clouds with coarse metric scale within each session; (2) post-fusion with cross-modal 3D similarity transformation and bounding-box-based regularization to reduce scale distortions from differing LiDAR/camera fields of view, resulting in a tight LiDAR-VGGT integration.

Result: Extensive experiments on multiple datasets show that LiDAR-VGGT yields dense, globally consistent colored point clouds and outperforms VGGT-based methods and LIVO baselines.

Conclusion: LiDAR-VGGT provides robust, scalable colored 3D reconstructions by tightly coupling LiDAR odometry with VGGT, and the authors will release an open-source color point cloud evaluation toolkit.

Abstract: Reconstructing large-scale colored point clouds is an important task in
robotics, supporting perception, navigation, and scene understanding. Despite
advances in LiDAR inertial visual odometry (LIVO), its performance remains
highly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation
models, such as VGGT, suffer from limited scalability in large environments and
inherently lack metric scale. To overcome these limitations, we propose
LiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with
the state-of-the-art VGGT model through a two-stage coarse- to-fine fusion
pipeline: First, a pre-fusion module with robust initialization refinement
efficiently estimates VGGT poses and point clouds with coarse metric scale
within each session. Then, a post-fusion module enhances cross-modal 3D
similarity transformation, using bounding-box-based regularization to reduce
scale distortions caused by inconsistent FOVs between LiDAR and camera sensors.
Extensive experiments across multiple datasets demonstrate that LiDAR-VGGT
achieves dense, globally consistent colored point clouds and outperforms both
VGGT-based methods and LIVO baselines. The implementation of our proposed novel
color point cloud evaluation toolkit will be released as open source.

</details>


### [455] [Closed-loop Control of Steerable Balloon Endoscopes for Robot-assisted Transcatheter Intracardiac Procedures](https://arxiv.org/abs/2511.01199)
*Max McCandless,Jonathan Hamid,Sammy Elmariah,Nathaniel Langer,Pierre E. Dupont*

Main category: cs.RO

TL;DR: Steerable balloon cardioscope enables direct intracardiac visualization and tool delivery via inflation-driven diameter and bending control; demonstrated for aortic leaflet laceration with image-based closed-loop bend control.


<details>
  <summary>Details</summary>
Motivation: Need safer transcatheter interventions and improved imaging beyond fluoroscopy/ultrasound; direct visualization inside the beating heart (cardioscopy) could simplify navigation and improve accuracy.

Method: A balloon-based cardioscope whose inflation pressure independently sets balloon diameter (field of view) and bending angle, with an integrated working channel; includes image-based closed-loop control of bending angle; illustrated on aortic leaflet laceration scenario.

Result: Shows independent control of diameter and bending angle; feasible design for various intracardiac tasks; image-based closed-loop bending control demonstrated for stable orientation during tool insertion/removal.

Conclusion: Balloon-based cardioscopes offer a versatile, pass-through platform for safe, precise intracardiac navigation and tool delivery; tunable to different tasks by adjusting balloon geometry and inflation dynamics.

Abstract: To move away from open-heart surgery towards safer transcatheter procedures,
there is a growing need for improved imaging techniques and robotic solutions
to enable simple, accurate tool navigation. Common imaging modalities, such as
fluoroscopy and ultrasound, have limitations that can be overcome using
cardioscopy, i.e., direct optical visualization inside the beating heart. We
present a cardioscope designed as a steerable balloon. As a balloon, it can be
collapsed to pass through the vasculature and subsequently inflated inside the
heart for visualization and tool delivery through an integrated working
channel. Through careful design of balloon wall thickness, a single input,
balloon inflation pressure, is used to independently control two outputs,
balloon diameter (corresponding to field of view diameter) and balloon bending
angle (enabling precise working channel positioning). This balloon technology
can be tuned to produce cardioscopes designed for a range of intracardiac
tasks. To illustrate this approach, a balloon design is presented for the
specific task of aortic leaflet laceration. Image-based closed-loop control of
bending angle is also demonstrated as a means of enabling stable orientation
control during tool insertion and removal.

</details>


### [456] [Tackling the Kidnapped Robot Problem via Sparse Feasible Hypothesis Sampling and Reliable Batched Multi-Stage Inference](https://arxiv.org/abs/2511.01219)
*Muhua Zhang,Lei Ma,Ying Wu,Kai Shen,Deqing Huang,Henry Leung*

Main category: cs.RO

TL;DR: Passive 2-D global relocalization for the Kidnapped Robot Problem using a single LiDAR scan and an occupancy grid map. It solves a non-convex problem via a multi-hypothesis scheme with batched multi-stage inference and early termination. It uses RRT-generated sparse hypotheses, ordered by SMAD, and employs TAM for reliable orientation and pose evaluation. Real-world tests on non-panoramic LiDAR show improved success rate and computational efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the Kidnapped Robot Problem (KRP) by enabling efficient and reliable global relocalization from a single LiDAR scan without a prior pose, thereby improving long-term autonomy during SLAM initialization or localization loss.

Method: Formulates global relocalization as a non-convex problem solved with a multi-hypothesis framework that includes batched multi-stage inference and early termination. Generates sparse, uniformly distributed positional hypotheses with an RRT under traversability constraints to reduce the sampling space. Uses Scan Mean Absolute Difference (SMAD) to preliminarily rank hypotheses (optimized for non-panoramic scans) and Translation-Affinity Scan-to-Map Alignment Metric (TAM) to select orientations and provide accurate final pose evaluation despite translational uncertainty, sparse hypotheses, non-panoramic scans, and environmental changes.

Result: Real-world experiments on a resource-constrained robot with non-panoramic LiDAR show the proposed framework outperforms existing methods in global relocalization success rate and computational efficiency.

Conclusion: The framework offers an efficient and reliable solution to the KRP by balancing completeness and efficiency, drastically reducing sampling space and improving global relocalization performance on non-panoramic LiDAR data in real-world deployments.

Abstract: This paper addresses the Kidnapped Robot Problem (KRP), a core localization
challenge of relocalizing a robot in a known map without prior pose estimate
when localization loss or at SLAM initialization. For this purpose, a passive
2-D global relocalization framework is proposed. It estimates the global pose
efficiently and reliably from a single LiDAR scan and an occupancy grid map
while the robot remains stationary, thereby enhancing the long-term autonomy of
mobile robots. The proposed framework casts global relocalization as a
non-convex problem and solves it via the multi-hypothesis scheme with batched
multi-stage inference and early termination, balancing completeness and
efficiency. The Rapidly-exploring Random Tree (RRT), under traversability
constraints, asymptotically covers the reachable space to generate sparse,
uniformly distributed feasible positional hypotheses, fundamentally reducing
the sampling space. The hypotheses are preliminarily ordered by the proposed
Scan Mean Absolute Difference (SMAD), a coarse beam-error level metric that
facilitates the early termination by prioritizing high-likelihood candidates.
The SMAD computation is optimized for non-panoramic scans. And the
Translation-Affinity Scan-to-Map Alignment Metric (TAM) is proposed for
reliable orientation selection at hypothesized positions and accurate final
pose evaluation to mitigate degradation in conventional likelihood-field
metrics under translational uncertainty induced by sparse hypotheses, as well
as non-panoramic LiDAR scan and environmental changes. Real-world experiments
on a resource-constrained mobile robot with non-panoramic LiDAR scan
demonstrate that the proposed framework outperforms existing methods in both
global relocalization success rate and computational efficiency.

</details>


### [457] [Embodiment Transfer Learning for Vision-Language-Action Models](https://arxiv.org/abs/2511.01224)
*Chengmeng Li,Yaxin Peng*

Main category: cs.RO

TL;DR: ET-VLA combines synthetic continued pretraining (SCP) and Embodied Graph-of-Thought to adapt vision-language-action models to multi-robot embodiments, enabling improved collaboration with reduced data needs.


<details>
  <summary>Details</summary>
Motivation: Efficient, scalable transfer of pre-trained VLA models to multi-robot settings without relying on costly real demonstrations.

Method: 1) Synthetic Continued Pretraining (SCP) using synthetic data to warm up the model for a new embodiment; 2) fine-tuning on target embodiment data; 3) Embodied Graph-of-Thought framework that treats each sub-task as a node to distinguish the roles of each embodiment during task execution; evaluated on simulated and real bimanual robots.

Result: ET-VLA outperforms OpenVLA on six real-world tasks by more than 53.2%; demonstrates effective multi-robot collaboration and successful transfer to new embodiments.

Conclusion: ET-VLA provides an efficient pathway for cross-embodiment transfer of VLA models, reducing data collection needs via SCP and improving multi-robot coordination via Embodied Graph-of-Thought; plans to open-source code.

Abstract: Vision-language-action (VLA) models have significantly advanced robotic
learning, enabling training on large-scale, cross-embodiment data and
fine-tuning for specific robots. However, state-of-the-art autoregressive VLAs
struggle with multi-robot collaboration. We introduce embodiment transfer
learning, denoted as ET-VLA, a novel framework for efficient and effective
transfer of pre-trained VLAs to multi-robot. ET-VLA's core is Synthetic
Continued Pretraining (SCP), which uses synthetically generated data to warm up
the model for the new embodiment, bypassing the need for real human
demonstrations and reducing data collection costs. SCP enables the model to
learn correct actions and precise action token numbers. Following SCP, the
model is fine-tuned on target embodiment data. To further enhance the model
performance on multi-embodiment, we present the Embodied Graph-of-Thought
technique, a novel approach that formulates each sub-task as a node, that
allows the VLA model to distinguish the functionalities and roles of each
embodiment during task execution. Our work considers bimanual robots, a simple
version of multi-robot to verify our approaches. We validate the effectiveness
of our method on both simulation benchmarks and real robots covering three
different bimanual embodiments. In particular, our proposed ET-VLA \space can
outperform OpenVLA on six real-world tasks over 53.2%. We will open-source all
codes to support the community in advancing VLA models for robot learning.

</details>


### [458] [High-Precision Surgical Robotic System for Intraocular Procedures](https://arxiv.org/abs/2511.01232)
*Yu-Ting Lai,Jacob Rosen,Yasamin Foroutani,Ji Ma,Wen-Cheng Wu,Jean-Pierre Hubschman,Tsu-Chin Tsao*

Main category: cs.RO

TL;DR: A novelty ophthalmic robot improves tooltip accuracy (~0.053 mm) and enables OCT-guided, automated cataract lens extraction using DL-based modeling and real-time supervision.


<details>
  <summary>Details</summary>
Motivation: To address insufficient accuracy, precision, DOF, and tool exchange in existing cataract/retina robotics, enabling safer, automated instrument manipulation.

Method: Design and fabricate a robotic system emphasizing tooltip accuracy, tracking, and smooth instrument exchange; calibrate, register coordinates; evaluate tooltip accuracy using OCT; apply DL-based preop anatomical modeling and real-time supervision in an OCT-guided automated lens extraction procedure.

Result: Measured tooltip positioning accuracy of 0.053 Â± 0.031 mm; demonstrated OCT-guided automated cataract lens extraction with deep-learning anatomical modeling and live supervision.

Conclusion: The framework achieves high-precision tool control and validates feasibility of automated, OCT-guided cataract procedures with real-time supervisory oversight; implies potential for safer, more autonomous ophthalmic surgeries.

Abstract: Despite the extensive demonstration of robotic systems for both cataract and
vitreoretinal procedures, existing technologies or mechanisms still possess
insufficient accuracy, precision, and degrees of freedom for instrument
manipulation or potentially automated tool exchange during surgical procedures.
A new robotic system that focuses on improving tooltip accuracy, tracking
performance, and smooth instrument exchange mechanism is therefore designed and
manufactured. Its tooltip accuracy, precision, and mechanical capability of
maintaining small incision through remote center of motion were externally
evaluated using an optical coherence tomography (OCT) system. Through robot
calibration and precise coordinate registration, the accuracy of tooltip
positioning was measured to be 0.053$\pm$0.031 mm, and the overall performance
was demonstrated on an OCT-guided automated cataract lens extraction procedure
with deep learning-based pre-operative anatomical modeling and real-time
supervision.

</details>


### [459] [Don't Just Search, Understand: Semantic Path Planning Agent for Spherical Tensegrity Robots in Unknown Environments](https://arxiv.org/abs/2511.01236)
*Junwen Zhang,Changyue Liu,Pengqi Fu,Xiang Guo,Ye Shi,Xudong Liang,Zhijian Wang,Hanzhi Ma*

Main category: cs.RO

TL;DR: Semantic planning with an LLM-driven SATPlanner uses adaptive perception to efficiently plan paths for spherical tensegrity robots in unknown environments, achieving linear search growth and near-optimal paths with high success and real-world validation.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of grid-based planners that are search-heavy and brittle in complex, unknown environments; leverage semantic understanding to guide exploration for tensegrity robots.

Method: Propose SATPlanner, an LLM-driven semantic agent for tensegrity path planning with Adaptive Observation Window that expands or narrows perception based on context, building a semantic belief of the environment to constrain search to O(L). Evaluate in 1,000 simulations and test on a physical robot.

Result: 100% success across 1,000 simulations; 37.2% reduction in search space versus A*; path lengths comparable to near-optimal; demonstration on hardware.

Conclusion: Semantic reasoning and adaptive perception enable robust, efficient planning for tensegrity robots in unknown environments, with practical feasibility demonstrated.

Abstract: Endowed with inherent dynamical properties that grant them remarkable
ruggedness and adaptability, spherical tensegrity robots stand as prototypical
examples of hybrid softrigid designs and excellent mobile platforms. However,
path planning for these robots in unknown environments presents a significant
challenge, requiring a delicate balance between efficient exploration and
robust planning. Traditional path planners, which treat the environment as a
geometric grid, often suffer from redundant searches and are prone to failure
in complex scenarios due to their lack of semantic understanding. To overcome
these limitations, we reframe path planning in unknown environments as a
semantic reasoning task. We introduce a Semantic Agent for Tensegrity robots
(SATPlanner) driven by a Large Language Model (LLM). SATPlanner leverages
high-level environmental comprehension to generate efficient and reliable
planning strategies.At the core of SATPlanner is an Adaptive Observation Window
mechanism, inspired by the "fast" and "slow" thinking paradigms of LLMs. This
mechanism dynamically adjusts the perceptual field of the agent: it narrows for
rapid traversal of open spaces and expands to reason about complex obstacle
configurations. This allows the agent to construct a semantic belief of the
environment, enabling the search space to grow only linearly with the path
length (O(L)) while maintaining path quality. We extensively evaluate
SATPlanner in 1,000 simulation trials, where it achieves a 100% success rate,
outperforming other real-time planning algorithms. Critically, SATPlanner
reduces the search space by 37.2% compared to the A* algorithm while achieving
comparable, near-optimal path lengths. Finally, the practical feasibility of
SATPlanner is validated on a physical spherical tensegrity robot prototype.

</details>


### [460] [Improving Needle Penetration via Precise Rotational Insertion Using Iterative Learning Control](https://arxiv.org/abs/2511.01256)
*Yasamin Foroutani,Yasamin Mousavi-Motlagh,Aya Barzelay,Tsu-Chin Tsao*

Main category: cs.RO

TL;DR: An iterative learning control (ILC) approach improves precision of rotational tool insertion in robotic surgery by iteratively updating joint commands based on OCT-measured error, after FK calibration, validated on ex vivo pig eyes showing higher penetration success than straight insertion.


<details>
  <summary>Details</summary>
Motivation: Inherent misalignments, unmodeled dynamics, and actuation errors hinder precise tool insertion; need for accurate control to improve safety and efficacy in high-precision robotic surgery.

Method: Calibrate forward kinematics of the surgical tool; implement 4-DOF ILC that adjusts joint commands based on positional feedback; use OCT volume scans to measure insertion error; perform iterative updates; test on ex vivo pig eyes performing subretinal injections.

Result: Optimized trajectory yielded higher tissue penetration and subretinal injection success rates vs straight insertion; demonstrates effectiveness of ILC in compensating misalignment.

Conclusion: ILC can enable precise controlled insertions in high-precision robotic tasks; potential applicability to other tasks requiring controlled insertions; potential improvement to safety and efficacy in robotic surgery.

Abstract: Achieving precise control of robotic tool paths is often challenged by
inherent system misalignments, unmodeled dynamics, and actuation inaccuracies.
This work introduces an Iterative Learning Control (ILC) strategy to enable
precise rotational insertion of a tool during robotic surgery, improving
penetration efficacy and safety compared to straight insertion tested in
subretinal injection. A 4 degree of freedom (DOF) robot manipulator is used,
where misalignment of the fourth joint complicates the simple application of
needle rotation, motivating an ILC approach that iteratively adjusts joint
commands based on positional feedback. The process begins with calibrating the
forward kinematics for the chosen surgical tool to achieve higher accuracy,
followed by successive ILC iterations guided by Optical Coherence Tomography
(OCT) volume scans to measure the error and refine control inputs. Experimental
results, tested on subretinal injection tasks on ex vivo pig eyes, show that
the optimized trajectory resulted in higher success rates in tissue penetration
and subretinal injection compared to straight insertion, demonstrating the
effectiveness of ILC in overcoming misalignment challenges. This approach
offers potential applications for other high precision robot tasks requiring
controlled insertions as well.

</details>


### [461] [Design and Fabrication of Origami-Inspired Knitted Fabrics for Soft Robotics](https://arxiv.org/abs/2511.01272)
*Sehui Jeong,Magaly C. Aviles,Athena X. Naylor,Cynthia Sung,Allison M. Okamura*

Main category: cs.RO

TL;DR: A knitted origami-inspired approach combines origami folding patterns with heat-fusible yarn to create reconfigurable, wearable fabric robots; demonstrates Miura-ori, Yoshimura, Kresling patterns, and a Kaleidocycle robot.


<details>
  <summary>Details</summary>
Motivation: Soft robots with both compliance and structural integrity are needed for comfortable, safe wearables; traditional methods struggle to balance flexibility and rigidity at scale; knitted fabrics offer wearability and manufacturability.

Method: Translate origami patterns into knit designs by programming stitch and material patterns; selectively incorporate heat fusible yarn to form rigid panels around compliant creases, enhancing fold directionality and suppressing unwanted buckling; quantify folding moments and boundary deformations; reproduce complex origami tessellations (Miura-ori, Yoshimura, Kresling); demonstrate a wearable knitted Kaleidocycle robot.

Result: Stitch patterning enhances folding directionality; heat fusible yarn stabilizes geometry by reducing edge curl and stiffening panels, preventing out-of-plane deformations; successful replication of Miura-ori, Yoshimura, Kresling tessellations; wearable Kaleidocycle robot capable of locomotion.

Conclusion: Knitted origami offers a platform with structural reconfigurability, material programmability, and potential for scalable manufacturing, making it a promising approach for next-generation wearable robotics.

Abstract: Soft robots employing compliant materials and deformable structures offer
great potential for wearable devices that are comfortable and safe for human
interaction. However, achieving both structural integrity and compliance for
comfort remains a significant challenge. In this study, we present a novel
fabrication and design method that combines the advantages of origami
structures with the material programmability and wearability of knitted
fabrics. We introduce a general design method that translates origami patterns
into knit designs by programming both stitch and material patterns. The method
creates folds in preferred directions while suppressing unintended buckling and
bending by selectively incorporating heat fusible yarn to create rigid panels
around compliant creases. We experimentally quantify folding moments and show
that stitch patterning enhances folding directionality while the heat fusible
yarn (1) keeps geometry consistent by reducing edge curl and (2) prevents
out-of-plane deformations by stiffening panels. We demonstrate the framework
through the successful reproduction of complex origami tessellations, including
Miura-ori, Yoshimura, and Kresling patterns, and present a wearable knitted
Kaleidocycle robot capable of locomotion. The combination of structural
reconfigurability, material programmability, and potential for manufacturing
scalability highlights knitted origami as a promising platform for
next-generation wearable robotics.

</details>


### [462] [Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation](https://arxiv.org/abs/2511.01276)
*Yiyao Ma,Kai Chen,Kexin Zheng,Qi Dou*

Main category: cs.RO

TL;DR: We propose a transfer-based, diffusion-driven framework that transfers grasps from shape templates to novel objects within the same category by generating and jointly aligning object contact, part, and direction maps through a cascaded conditional diffusion model, plus a robust grasp recovery, yielding improved generalization and efficiency.


<details>
  <summary>Details</summary>
Motivation: Analytical grasp methods yield stable grasps but are inefficient and lack task adaptability; generative approaches are more efficient and integrative but generalize poorly due to limited data. A transfer-based approach aims to leverage shape templates to extend grasps to unseen objects and tasks.

Method: Reformulate grasp transfer as generation of an object contact map; incorporate shape similarity and task specs into diffusion; introduce a dual mapping mechanism to capture geometrical relationships between templates and novel shapes; derive two additional object-centric maps (part map and direction map) for finer contact details; use a cascaded conditional diffusion model to jointly transfer these three maps ensuring intra-consistency; develop a robust grasp recovery mechanism to identify reliable contact points and optimize grasp configurations.

Result: Extensive experiments demonstrate superiority of the proposed method, achieving a favorable balance between grasp quality, generation efficiency, and generalization across tasks.

Conclusion: The proposed transfer-based diffusion framework enables effective, efficient, and robust dexterous grasp transfer within object categories, combining multiple contact-rich maps with a robust recovery step. A project homepage is provided for supplementary materials.

Abstract: Dexterous grasp generation is a fundamental challenge in robotics, requiring
both grasp stability and adaptability across diverse objects and tasks.
Analytical methods ensure stable grasps but are inefficient and lack task
adaptability, while generative approaches improve efficiency and task
integration but generalize poorly to unseen objects and tasks due to data
limitations. In this paper, we propose a transfer-based framework for dexterous
grasp generation, leveraging a conditional diffusion model to transfer
high-quality grasps from shape templates to novel objects within the same
category. Specifically, we reformulate the grasp transfer problem as the
generation of an object contact map, incorporating object shape similarity and
task specifications into the diffusion process. To handle complex shape
variations, we introduce a dual mapping mechanism, capturing intricate
geometric relationship between shape templates and novel objects. Beyond the
contact map, we derive two additional object-centric maps, the part map and
direction map, to encode finer contact details for more stable grasps. We then
develop a cascaded conditional diffusion model framework to jointly transfer
these three maps, ensuring their intra-consistency. Finally, we introduce a
robust grasp recovery mechanism, identifying reliable contact points and
optimizing grasp configurations efficiently. Extensive experiments demonstrate
the superiority of our proposed method. Our approach effectively balances grasp
quality, generation efficiency, and generalization performance across various
tasks. Project homepage: https://cmtdiffusion.github.io/

</details>


### [463] [A High-Speed Capable Spherical Robot](https://arxiv.org/abs/2511.01288)
*Bixuan Zhang,Fengqi Zhang,Haojie Chen,You Wang,Jie Hao,Zhiyuan Luo,Guang Li*

Main category: cs.RO

TL;DR: A high-speed spherical robot design using a momentum wheel aligned with a secondary pendulum enables stable motion up to 10 m/s, demonstrated experimentally with a decoupled control approach.


<details>
  <summary>Details</summary>
Motivation: To overcome speed and robustness limitations of traditional single-pendulum spherical robots by enabling fast, controllable motion and better terrain adaptation.

Method: Integrate a momentum wheel with an axis aligned to the secondary pendulum into a single-pendulum-driven spherical robot, and validate with a physical prototype showing stable high-speed motion under simple decoupled control.

Result: The robot achieves stable high-speed motion up to 10 m/s and shows improved obstacle-crossing capability and terrain robustness compared with the original design.

Conclusion: The new spherical-robot structure provides a viable path to fast, robust spherical locomotion using simple decoupled control, with promising practical applications.

Abstract: This paper designs a new spherical robot structure capable of supporting
high-speed motion at up to 10 m/s. Building upon a single-pendulum-driven
spherical robot, the design incorporates a momentum wheel with an axis aligned
with the secondary pendulum, creating a novel spherical robot structure.
Practical experiments with the physical prototype have demonstrated that this
new spherical robot can achieve stable high-speed motion through simple
decoupled control, which was unattainable with the original structure. The
spherical robot designed for high-speed motion not only increases speed but
also significantly enhances obstacle-crossing performance and terrain
robustness.

</details>


### [464] [Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects](https://arxiv.org/abs/2511.01294)
*Jiawei Wang,Dingyou Wang,Jiaming Hu,Qixuan Zhang,Jingyi Yu,Lan Xu*

Main category: cs.RO

TL;DR: Kinematify automates articulated-object model synthesis directly from RGB images or text prompts by pairing MCTS-based topology search with geometry-driven joint optimization, achieving physically consistent models and improved topology/registration accuracy over prior work.


<details>
  <summary>Details</summary>
Motivation: A deep understanding of kinematic structures is essential for robot manipulation, simulation, and policy learning. High-DoF articulated objects pose challenges for scalable modeling, and existing methods rely on motion sequences or curated datasets, hindering scalability. Automated, image- or prompt-driven articulation synthesis aims to scale modeling to complex systems.

Method: A two-part approach: (1) use Monte Carlo Tree Search (MCTS) to infer kinematic topology (structure) of articulated objects; (2) apply geometry-driven optimization to estimate joint parameters from static geometry, ensuring physical consistency. The framework combines these steps to produce functionally valid articulated descriptions, evaluated on synthetic and real-world data.

Result: The method achieves improvements in registration accuracy and kinematic topology accuracy compared with prior work, demonstrating robustness across diverse synthetic and real inputs.

Conclusion: Kinematify offers a scalable, automated pipeline for articulating objects from RGB images or text prompts, enabling better manipulation, simulation, and policy learning while reducing reliance on motion data and hand-curated datasets.

Abstract: A deep understanding of kinematic structures and movable components is
essential for enabling robots to manipulate objects and model their own
articulated forms. Such understanding is captured through articulated objects,
which are essential for tasks such as physical simulation, motion planning, and
policy learning. However, creating these models, particularly for complex
systems like robots or objects with high degrees of freedom (DoF), remains a
significant challenge. Existing methods typically rely on motion sequences or
strong assumptions from hand-curated datasets, which hinders scalability. In
this paper, we introduce Kinematify, an automated framework that synthesizes
articulated objects directly from arbitrary RGB images or text prompts. Our
method addresses two core challenges: (i) inferring kinematic topologies for
high-DoF objects and (ii) estimating joint parameters from static geometry. To
achieve this, we combine MCTS search for structural inference with
geometry-driven optimization for joint reasoning, producing physically
consistent and functionally valid descriptions. We evaluate Kinematify on
diverse inputs from both synthetic and real-world environments, demonstrating
improvements in registration and kinematic topology accuracy over prior work.

</details>


### [465] [RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models](https://arxiv.org/abs/2511.01331)
*Hongyin Zhang,Shuo Zhang,Junxi Jin,Qixin Zeng,Runze Li,Donglin Wang*

Main category: cs.RO

TL;DR: RobustVLA is a lightweight online RL post-training method that enhances robustness of Vision-Language-Action (VLA) models for robotic manipulation by applying two regularizations: Jacobian regularization to reduce sensitivity to observation noise and smoothness regularization to stabilize policies under action perturbations, yielding superior robustness and reliability across diverse environments.


<details>
  <summary>Details</summary>
Motivation: VLA models generalize poorly under out-of-distribution disturbances (noise, sensor errors, actuation perturbations). Existing RL post-training often prioritizes reward optimization and neglects robustness to environmental uncertainty.

Method: Online RL post-training called RobustVLA that adds two regularizations during training: Jacobian regularization to reduce sensitivity to observation perturbations and smoothness regularization to stabilize the policy under action perturbations.

Result: RobustVLA significantly outperforms prior state-of-the-art methods in robustness and reliability across diverse robotic environments.

Conclusion: Principled robustness-aware RL post-training is a key step toward improving the reliability and robustness of VLA models.

Abstract: Vision-Language-Action (VLA) models have recently emerged as powerful
general-purpose policies for robotic manipulation, benefiting from large-scale
multi-modal pre-training. However, they often fail to generalize reliably in
out-of-distribution deployments, where unavoidable disturbances such as
observation noise, sensor errors, or actuation perturbations become prevalent.
While recent Reinforcement Learning (RL)-based post-training provides a
practical means to adapt pre-trained VLA models, existing methods mainly
emphasize reward maximization and overlook robustness to environmental
uncertainty. In this work, we introduce RobustVLA, a lightweight online RL
post-training method designed to explicitly enhance the resilience of VLA
models. Through a systematic robustness analysis, we identify two key
regularizations: Jacobian regularization, which mitigates sensitivity to
observation noise, and smoothness regularization, which stabilizes policies
under action perturbations. Extensive experiments across diverse robotic
environments demonstrate that RobustVLA significantly outperforms prior
state-of-the-art methods in robustness and reliability. Our results highlight
the importance of principled robustness-aware RL post-training as a key step
toward improving the reliability and robustness of VLA models.

</details>


### [466] [Embodied Cognition Augmented End2End Autonomous Driving](https://arxiv.org/abs/2511.01334)
*Ling Niu,Xiaoji Zheng,Han Wang,Chen Zheng,Ziyuan Yang,Bokui Chen,Jiangtao Gong*

Main category: cs.RO

TL;DR: A brain-inspired E^3AD framework that compares visual feature extractors with an EEG-based large model via contrastive learning to inject human driving cognition into end-to-end autonomous driving; collects a cognitive dataset and demonstrates planning improvements on public datasets with open-loop and closed-loop tests; code to be released.


<details>
  <summary>Details</summary>
Motivation: End-to-end driving models rely on supervised visual feature extraction, which limits generality due to limited supervision. The authors propose a comparative learning paradigm (E^3AD) that leverages a general EEG large model to capture latent human driving cognition and guide end-to-end planning.

Method: Collect a cognitive dataset for contrastive learning between visual feature extractors and an EEG large model; apply comparative learning to align driving-relevant representations; evaluate on popular autonomous driving baselines using open-loop and closed-loop tests; perform ablations to analyze the contribution of driving cognition and the learning process.

Result: The E^3AD paradigm significantly improves end-to-end planning performance of baseline driving models on public datasets; ablation studies confirm the contribution of incorporating driving cognition and the effectiveness of the comparative learning process.

Conclusion: This work is the first to integrate human driving cognition (embodied cognitive data) into end-to-end autonomous driving planning, marking an initial step toward brain-inspired autonomous driving systems; code will be released on GitHub.

Abstract: In recent years, vision-based end-to-end autonomous driving has emerged as a
new paradigm. However, popular end-to-end approaches typically rely on visual
feature extraction networks trained under label supervision. This limited
supervision framework restricts the generality and applicability of driving
models. In this paper, we propose a novel paradigm termed $E^{3}AD$, which
advocates for comparative learning between visual feature extraction networks
and the general EEG large model, in order to learn latent human driving
cognition for enhancing end-to-end planning. In this work, we collected a
cognitive dataset for the mentioned contrastive learning process. Subsequently,
we investigated the methods and potential mechanisms for enhancing end-to-end
planning with human driving cognition, using popular driving models as
baselines on publicly available autonomous driving datasets. Both open-loop and
closed-loop tests are conducted for a comprehensive evaluation of planning
performance. Experimental results demonstrate that the $E^{3}AD$ paradigm
significantly enhances the end-to-end planning performance of baseline models.
Ablation studies further validate the contribution of driving cognition and the
effectiveness of comparative learning process. To the best of our knowledge,
this is the first work to integrate human driving cognition for improving
end-to-end autonomous driving planning. It represents an initial attempt to
incorporate embodied cognitive data into end-to-end autonomous driving,
providing valuable insights for future brain-inspired autonomous driving
systems. Our code will be made available at Github

</details>


### [467] [Thermo-responsive closing and reopening artificial Venus Flytrap utilizing shape memory elastomers](https://arxiv.org/abs/2511.01346)
*Shun Yoshida,Qingchuan Song,Bastian E. Rapp,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: Autonomous bidirectional AVF using thermo-responsive shape memory polymers demonstrates closing at 38Â°C and reopening at ~45Â°C, enabling programmed sequential motion in a soft robot.


<details>
  <summary>Details</summary>
Motivation: To translate the Venus flytrap's rapid snap into a fully autonomous, bidirectional soft machine by leveraging thermo-responsive materials.

Method: Fabricates a life-sized AVF with doubly curved trap lobes from shape memory polymers; uses shape memory elastomer strips as antagonistic actuators to enable reopening; closure and reopening are triggered by natural temperature ranges.

Result: First demonstration of thermo-responsive closing and reopening in an AVF with programmed sequential motion in response to increasing temperature; autonomous bidirectional actuation.

Conclusion: Represents a step toward autonomously bidirectional plant-inspired soft robots, showing a viable materials-and-design pathway for temperature-driven, bidirectional soft-machine motion.

Abstract: Despite their often perceived static and slow nature, some plants can move
faster than the blink of an eye. The rapid snap closure motion of the Venus
flytrap (Dionaea muscipula) has long captivated the interest of researchers and
engineers alike, serving as a model for plant-inspired soft machines and
robots. The translation of the fast snapping closure has inspired the
development of various artificial Venus flytrap (AVF) systems. However,
translating both the closing and reopening motion of D. muscipula into an
autonomous plant inspired soft machine has yet to be achieved. In this study,
we present an AVF that autonomously closes and reopens, utilizing novel
thermo-responsive UV-curable shape memory materials for soft robotic systems.
The life-sized thermo-responsive AVF exhibits closing and reopening motions
triggered in a naturally occurring temperature range. The doubly curved trap
lobes, built from shape memory polymers, close at 38{\deg}C, while reopening
initiates around 45{\deg}C, employing shape memory elastomer strips as
antagonistic actuators to facilitate lobe reopening. This work represents the
first demonstration of thermo-responsive closing and reopening in an AVF with
programmed sequential motion in response to increasing temperature. This
approach marks the next step toward autonomously bidirectional moving soft
machines/robots.

</details>


### [468] [Design and development of an electronics-free earthworm robot](https://arxiv.org/abs/2511.01347)
*Riddhi Das,Joscha Teichmann,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: An electronics-free, earthworm-inspired pneumatic robot uses modified Pneumatic Logic Gates (PLG) and bellows to achieve peristaltic locomotion in a plug-and-play, modular design, demonstrating autonomous motion without external electronics; it serves as a proof of concept for untethered, hazardous-environment applications.


<details>
  <summary>Details</summary>
Motivation: To reduce system complexity and enable untethered, safe locomotion in soft robots by removing electronic control units, addressing the practicality issues of pneumatic-earthworm robots in hazardous or constrained environments.

Method: Integrate preconfigured, modified Pneumatic Logic Gate units with bellows actuators to create a plug-and-play, electronics-free control system that can generate peristaltic waves for locomotion; characterize bellows under various conditions and evaluate overall locomotion performance.

Result: The PLG-based control system successfully generates peristaltic wave propagation, enabling autonomous, low-deviation locomotion without external electronics, validating electronics-free peristaltic soft robotics as a concept.

Conclusion: Demonstrates feasibility of electronics-free, peristaltic soft robots and highlights potential in hazardous environments; future work includes design optimization and developing onboard compressed-air sources for untethered operation.

Abstract: Soft robotic systems have gained widespread attention due to their inherent
flexibility, adaptability, and safety, making them well-suited for varied
applications. Among bioinspired designs, earthworm locomotion has been
extensively studied for its efficient peristaltic motion, enabling movement in
confined and unstructured environments. Existing earthworm-inspired robots
primarily utilize pneumatic actuation due to its high force-to-weight ratio and
ease of implementation. However, these systems often rely on bulky,
power-intensive electronic control units, limiting their practicality. In this
work, we present an electronics-free, earthworm-inspired pneumatic robot
utilizing a modified Pneumatic Logic Gate (PLG) design. By integrating
preconfigured PLG units with bellow actuators, we achieved a plug-and-play
style modular system capable of peristaltic locomotion without external
electronic components. The proposed design reduces system complexity while
maintaining efficient actuation. We characterize the bellow actuators under
different operating conditions and evaluate the robots locomotion performance.
Our findings demonstrate that the modified PLG-based control system effectively
generates peristaltic wave propagation, achieving autonomous motion with
minimal deviation. This study serves as a proof of concept for the development
of electronics-free, peristaltic soft robots. The proposed system has potential
for applications in hazardous environments, where untethered, adaptable
locomotion is critical. Future work will focus on further optimizing the robot
design and exploring untethered operation using onboard compressed air sources.

</details>


### [469] [Model to Model: Understanding the Venus Flytrap Snapping Mechanism and Transferring it to a 3D-printed Bistable Soft Robotic Demonstrator](https://arxiv.org/abs/2511.01350)
*Maartje H. M. Wermelink,Renate Sachse,Sebastian Kruppert,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: Biomimetic study of Venus flytrap mechanics leading to two 3D-printed bistable lobe actuators that snap closed, aiming to develop an artificial, soft fast gripper inspired by the plant's rapid trap movement.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanical basis of Venus flytrap closure and to translate its bistable lobe actuation into simple, controllable artificial actuators for soft robotics or grippers.

Method: Identify dimensional ratios and thickness gradients in the Venus flytrap leaf that drive concave-to-convex bistability. Create two 3D-printed actuator models: one mirroring the simulated geometry of a flytrap leaf, the other CAD-designed; both demonstrate concave-convex bistability and snap-through closure.

Result: Both actuator models exhibit concave-convex bistability and can snap closed, validating the feasibility of bioinspired bistable lobe actuators as soft fast grippers.

Conclusion: The study provides a proof-of-concept for translating Venus flytrap mechanics into artificial bistable actuators, offering a pathway toward soft, rapid-grasp devices; further work should address control, material properties, and scaling for practical applications.

Abstract: The Venus flytrap (Dionaea muscipula) does not only serve as the textbook
model for a carnivorous plant, but also has long intrigued both botanists and
engineers with its rapidly closing leaf trap. The trap closure is triggered by
two consecutive touches of a potential prey, after which the lobes rapidly
switch from their concave open-state to their convex close-state and catch the
prey within 100-500 ms after being triggered. This transformation from concave
to convex is initiated by changes in turgor pressure and the release of stored
elastic energy from prestresses in the concave state, which accelerate this
movement, leading to inversion of the lobes bi-axial curvature. Possessing two
low-energy states, the leaves can be characterized as bistable systems. With
our research, we seek to deepen the understanding of Venus flytrap motion
mechanics and apply its principles to the design of an artificial bistable lobe
actuator. We identified geometrical characteristics, such as dimensional ratios
and the thickness gradient in the lobe, and transferred these to two 3D-printed
bistable actuator models. One actuator parallels the simulated geometry of a
Venus flytrap leaf, the other is a lobe model designed with CAD. Both models
display concave-convex bi-stability and snap close. These demonstrators are the
first step in the development of an artificial Venus flytrap that mimics the
mechanical behavior of the biological model and can be used as a soft fast
gripper.

</details>


### [470] [Lateral Velocity Model for Vehicle Parking Applications](https://arxiv.org/abs/2511.01369)
*Luis Diener,Jens Kalkkuhl,Markus Enzweiler*

Main category: cs.RO

TL;DR: A two-parameter lateral velocity model improves parking localization by replacing the zero-slip assumption, based on real-world data.


<details>
  <summary>Details</summary>
Motivation: Accurate lateral velocity is crucial for parking maneuvers. Wheel encoders provide longitudinal velocity, but consumer vehicles lack lateral sensors; the common zero-slip model is inadequate at low speeds.

Method: Analyze real parking data to identify deviations from zero-slip, provide explanations for the observations, and propose a two-parameter lateral velocity model capturing lateral dynamics; discuss integration into consumer-grade systems.

Result: The proposed two-parameter lateral velocity model yields improved estimation accuracy for lateral velocity during parking compared to the zero-slip baseline.

Conclusion: A simple, data-driven two-parameter model better captures lateral dynamics in parking, enabling improved localization for consumer-grade autonomous/assistive parking systems.

Abstract: Automated parking requires accurate localization for quick and precise
maneuvering in tight spaces. While the longitudinal velocity can be measured
using wheel encoders, the estimation of the lateral velocity remains a key
challenge due to the absence of dedicated sensors in consumer-grade vehicles.
Existing approaches often rely on simplified vehicle models, such as the
zero-slip model, which assumes no lateral velocity at the rear axle. It is well
established that this assumption does not hold during low-speed driving and
researchers thus introduce additional heuristics to account for differences. In
this work, we analyze real-world data from parking scenarios and identify a
systematic deviation from the zero-slip assumption. We provide explanations for
the observed effects and then propose a lateral velocity model that better
captures the lateral dynamics of the vehicle during parking. The model improves
estimation accuracy, while relying on only two parameters, making it
well-suited for integration into consumer-grade applications.

</details>


### [471] [CM-LIUW-Odometry: Robust and High-Precision LiDAR-Inertial-UWB-Wheel Odometry for Extreme Degradation Coal Mine Tunnels](https://arxiv.org/abs/2511.01379)
*Kun Hu,Menggang Li,Zhiwen Jin,Chaoquan Tang,Eryi Hu,Gongbo Zhou*

Main category: cs.RO

TL;DR: A multimodal SLAM framework (CM-LIUW-Odometry) for GPS-denied underground mines that fuses LiDAR-IMU with UWB and wheel odometry via the Iterated Error-State Kalman Filter (IESKF), plus nonholonomic constraints and adaptive motion mode switching, achieving improved accuracy and robustness; code available on GitHub.


<details>
  <summary>Details</summary>
Motivation: Underground coal mines face GPS denial, uneven terrain degrading wheel odometry, and long feature-poor tunnels reducing LiDAR effectiveness. A robust, globally consistent SLAM solution is needed.

Method: Use Iterated Error-State Kalman Filter (IESKF) to tightly fuse: (1) LiDAR-IMU odometry with UWB absolute constraints to anchor the global frame; (2) wheel odometry with nonholonomic constraints and lever-arm compensation for extended reliability when UWB is weak; (3) an adaptive motion mode switching mechanism that adjusts the robot's mode based on UWB range and environmental degradation.

Result: Experimental results in real underground coal mine scenarios show superior accuracy and robustness compared to state-of-the-art methods.

Conclusion: CM-LIUW-Odometry provides a robust, globally consistent SLAM solution for GPS-denied underground environments; the authors open-source their code on GitHub.

Abstract: Simultaneous Localization and Mapping (SLAM) in large-scale, complex, and
GPS-denied underground coal mine environments presents significant challenges.
Sensors must contend with abnormal operating conditions: GPS unavailability
impedes scene reconstruction and absolute geographic referencing, uneven or
slippery terrain degrades wheel odometer accuracy, and long, feature-poor
tunnels reduce LiDAR effectiveness. To address these issues, we propose
CoalMine-LiDAR-IMU-UWB-Wheel-Odometry (CM-LIUW-Odometry), a multimodal SLAM
framework based on the Iterated Error-State Kalman Filter (IESKF). First,
LiDAR-inertial odometry is tightly fused with UWB absolute positioning
constraints to align the SLAM system with a global coordinate. Next, wheel
odometer is integrated through tight coupling, enhanced by nonholonomic
constraints (NHC) and vehicle lever arm compensation, to address performance
degradation in areas beyond UWB measurement range. Finally, an adaptive motion
mode switching mechanism dynamically adjusts the robot's motion mode based on
UWB measurement range and environmental degradation levels. Experimental
results validate that our method achieves superior accuracy and robustness in
real-world underground coal mine scenarios, outperforming state-of-the-art
approaches. We open source our code of this work on Github to benefit the
robotics community.

</details>


### [472] [CaRLi-V: Camera-RADAR-LiDAR Point-Wise 3D Velocity Estimation](https://arxiv.org/abs/2511.01383)
*Landson Guo,Andres M. Diaz Aguilar,William Talbot,Turcan Tuna,Marco Hutter,Cesar Cadena*

Main category: cs.RO

TL;DR: A multi-sensor CaRLi-V pipeline fuses RADAR, LiDAR, and camera data to produce dense 3D point-wise velocity estimates via a velocity cube, combining radial from RADAR, tangential from optical flow, and range from LiDAR in a closed-form fusion, packaged as an open-source ROS2 tool.


<details>
  <summary>Details</summary>
Motivation: Enable robust, point-wise 3D velocity estimation for dynamic, non-rigid agents (e.g., humans) to improve robotic path planning, collision avoidance, and manipulation in dynamic environments.

Method: Introduce a novel RADAR representation called the velocity cube to densely encode radial velocities within the sensor FOV. Extract radial velocity from the velocity cube, estimate tangential velocity from optical flow, and use LiDAR range measurements in a closed-form solution to compute 3D velocity for a dense grid of points. Implemented as an open-source ROS2 package (CaRLi-V) and evaluated on a custom dataset.

Result: Reported low velocity error metrics relative to ground truth, demonstrating accurate dense 3D velocity estimation and enabling point-wise velocity estimates for robotic tasks.

Conclusion: CaRLi-V demonstrates the feasibility of dense 3D velocity estimation through multi-sensor fusion (RADAR, LiDAR, camera) and provides an open-source tool suitable for robotics applications in dynamic environments.

Abstract: Accurate point-wise velocity estimation in 3D is crucial for robot
interaction with non-rigid, dynamic agents, such as humans, enabling robust
performance in path planning, collision avoidance, and object manipulation in
dynamic environments. To this end, this paper proposes a novel RADAR, LiDAR,
and camera fusion pipeline for point-wise 3D velocity estimation named CaRLi-V.
This pipeline leverages raw RADAR measurements to create a novel RADAR
representation, the velocity cube, which densely represents radial velocities
within the RADAR's field-of-view. By combining the velocity cube for radial
velocity extraction, optical flow for tangential velocity estimation, and LiDAR
for point-wise range measurements through a closed-form solution, our approach
can produce 3D velocity estimates for a dense array of points. Developed as an
open-source ROS2 package, CaRLi-V has been field-tested against a custom
dataset and proven to produce low velocity error metrics relative to ground
truth, enabling point-wise velocity estimation for robotic applications.

</details>


### [473] [FoldPath: End-to-End Object-Centric Motion Generation via Modulated Implicit Paths](https://arxiv.org/abs/2511.01407)
*Paolo Rabino,Gabriele Tiboni,Tatiana Tommasi*

Main category: cs.RO

TL;DR: FoldPath is an end-to-end neural field-based method for object-centric motion generation (OCMG) that models robot motion as a continuous function, removing brittle post-processing of discrete waypoints. It achieves strong predictive performance, generalizes from as few as 70 expert samples, and introduces new metrics for evaluating long-horizon paths in realistic simulations, indicating practical maturity for industrial OCMG tasks.


<details>
  <summary>Details</summary>
Motivation: Current OCMG approaches rely on ad-hoc heuristics or learning pipelines that require brittle post-processing to convert predictions into executable paths. There is a need for robust, long-horizon, object-aware trajectory generation that can generalize to real industrial settings with limited expert data.

Method: FoldPath uses a neural field to represent the robot's motion as a continuous trajectory function, eliminating the need to predict discrete end-effector waypoints and post-processing steps. The model is trained with a small set of expert demonstrations (around 70 samples) and evaluated in a realistic simulation, with new metrics designed to assess long-horizon paths.

Result: FoldPath demonstrates superior predictive performance compared to recent learning-based methods and generalizes well to industrial environments using minimal expert data, validated through comprehensive simulation experiments.

Conclusion: The end-to-end neural field approach of FoldPath advances the practicality of OCMG, reducing brittleness from post-processing and enabling robust, long-horizon motion generation suitable for real-world manufacturing tasks.

Abstract: Object-Centric Motion Generation (OCMG) is instrumental in advancing
automated manufacturing processes, particularly in domains requiring
high-precision expert robotic motions, such as spray painting and welding. To
realize effective automation, robust algorithms are essential for generating
extended, object-aware trajectories across intricate 3D geometries. However,
contemporary OCMG techniques are either based on ad-hoc heuristics or employ
learning-based pipelines that are still reliant on sensitive post-processing
steps to generate executable paths. We introduce FoldPath, a novel, end-to-end,
neural field based method for OCMG. Unlike prior deep learning approaches that
predict discrete sequences of end-effector waypoints, FoldPath learns the robot
motion as a continuous function, thus implicitly encoding smooth output paths.
This paradigm shift eliminates the need for brittle post-processing steps that
concatenate and order the predicted discrete waypoints. Particularly, our
approach demonstrates superior predictive performance compared to recently
proposed learning-based methods, and attains generalization capabilities even
in real industrial settings, where only a limited amount of 70 expert samples
are provided. We validate FoldPath through comprehensive experiments in a
realistic simulation environment and introduce new, rigorous metrics designed
to comprehensively evaluate long-horizon robotic paths, thus advancing the OCMG
task towards practical maturity.

</details>


### [474] [Designing for Distributed Heterogeneous Modularity: On Software Architecture and Deployment of MoonBots](https://arxiv.org/abs/2511.01437)
*Elian Neppel,Shamistan Karimov,Ashutosh Mishra,Gustavo Hernan Diaz Huenupan,Hazal Gozbasi,Kentaro Uno,Shreya Santra,Kazuya Yoshida*

Main category: cs.RO

TL;DR: A modular, software-defined robotic architecture for MoonBot enabling distributed, heterogeneous modularity across hardware, software, and networks; uses ROS2/Zenoh for data-oriented communication, plus a deployment orchestrator and open-source Motion Stack; validated in extended field deployment and designed for scalable collaboration.


<details>
  <summary>Details</summary>
Motivation: Tackle the integration and maintenance overhead of scaling modular robotics as hardware, software, teams, and environments grow; extend modularity beyond physical reconfiguration to software, communication, and orchestration; support space-oriented operations while offering generalizable patterns for broader domains.

Method: Describe a component-based architecture with data-oriented communication (ROS2 + Zenoh) and a deployment orchestrator to manage multi-module assemblies; enable dynamic reconfiguration, decentralized control, and cross-operator collaboration; leverage the open-source Motion Stack and validate via months-long field deployments including self-assembly, inter-robot cooperation, and remote operation.

Result: Validated through months of field deployment with self-assembling robots, inter-robot cooperation, and remote operation; demonstrated reduced integration and maintenance overhead while maintaining scalability and robustness.

Conclusion: Proposes generalizable patterns for designing robotic systems that must scale across time, hardware, teams, and operational environments; while framed around space robotics, the approach is applicable to broader robotic systems requiring modular software, distributed control, and cross-domain collaboration.

Abstract: This paper presents the software architecture and deployment strategy behind
the MoonBot platform: a modular space robotic system composed of heterogeneous
components distributed across multiple computers, networks and ultimately
celestial bodies. We introduce a principled approach to distributed,
heterogeneous modularity, extending modular robotics beyond physical
reconfiguration to software, communication and orchestration. We detail the
architecture of our system that integrates component-based design, a
data-oriented communication model using ROS2 and Zenoh, and a deployment
orchestrator capable of managing complex multi-module assemblies. These
abstractions enable dynamic reconfiguration, decentralized control, and
seamless collaboration between numerous operators and modules. At the heart of
this system lies our open-source Motion Stack software, validated by months of
field deployment with self-assembling robots, inter-robot cooperation, and
remote operation. Our architecture tackles the significant hurdles of modular
robotics by significantly reducing integration and maintenance overhead, while
remaining scalable and robust. Although tested with space in mind, we propose
generalizable patterns for designing robotic systems that must scale across
time, hardware, teams and operational environments.

</details>


### [475] [AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models](https://arxiv.org/abs/2511.01472)
*Sarthak Mishra,Rishabh Dev Yadav,Avirup Das,Saksham Gupta,Wei Pan,Spandan Roy*

Main category: cs.RO

TL;DR: AERMANI-VLM decouples high-level reasoning from low-level flight control by using structured prompts to translate VLM outputs into a library of flight-safe skills, enabling safe, interpretable aerial manipulation without task-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Visionâlanguage models offer flexible control via natural language but often produce unsafe, hallucinated, or dynamically infeasible actions when applied to aerial robots. There is a need to ground VLM reasoning in safe, verifiable control at the servo level while preserving generalization to new tasks and commands.

Method: The approach encodes instructions, task context, and safety constraints into a structured prompt that elicits a step-by-step natural-language reasoning trace from a pretrained VLM. This reasoning is then used to select from a predefined library of discrete, flight-safe skills, ensuring interpretable and temporally consistent execution. By decoupling symbolic reasoning from physical action, the framework mitigates hallucinations and unsafe behavior without fine-tuning the model on task data.

Result: Demonstrated robustness on diverse multi-step pick-and-place tasks in both simulation and hardware, with strong generalization to unseen commands, objects, and environments.

Conclusion: Decoupling high-level reasoning from low-level control with a structured prompt and a safe skill library yields safer, more interpretable, and generalizable aerial manipulation using pretrained VLMs; the approach avoids task-specific fine-tuning but relies on a comprehensive skill library and may face limits in real-time performance and coverage of safe actions.

Abstract: The rapid progress of vision--language models (VLMs) has sparked growing
interest in robotic control, where natural language can express the operation
goals while visual feedback links perception to action. However, directly
deploying VLM-driven policies on aerial manipulators remains unsafe and
unreliable since the generated actions are often inconsistent,
hallucination-prone, and dynamically infeasible for flight. In this work, we
present AERMANI-VLM, the first framework to adapt pretrained VLMs for aerial
manipulation by separating high-level reasoning from low-level control, without
any task-specific fine-tuning. Our framework encodes natural language
instructions, task context, and safety constraints into a structured prompt
that guides the model to generate a step-by-step reasoning trace in natural
language. This reasoning output is used to select from a predefined library of
discrete, flight-safe skills, ensuring interpretable and temporally consistent
execution. By decoupling symbolic reasoning from physical action, AERMANI-VLM
mitigates hallucinated commands and prevents unsafe behavior, enabling robust
task completion. We validate the framework in both simulation and hardware on
diverse multi-step pick-and-place tasks, demonstrating strong generalization to
previously unseen commands, objects, and environments.

</details>


### [476] [MO-SeGMan: Rearrangement Planning Framework for Multi Objective Sequential and Guided Manipulation in Constrained Environments](https://arxiv.org/abs/2511.01476)
*Cankut Bora Tuncer,Marc Toussaint,Ozgur S. Oguz*

Main category: cs.RO

TL;DR: MO-SeGMan is a multi-objective planner for highly constrained rearrangements that minimizes replanning and travel distance, using lazy evaluation to preserve dependencies. It introduces SGFS to relocate only critical obstacles and an adaptive subgoal refinement to cut unnecessary actions. Evaluations on nine benchmarks show feasible plans, faster solution times, and better quality than baselines.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of highly cluttered, non-monotone rearrangement problems by providing a robust, scalable planning framework that reduces replanning, minimizes robot travel, and respects important dependency structures.

Method: Proposes MO-SeGMan, a Multi-Objective Sequential and Guided Manipulation planner. Key components include (1) lazy evaluation to preserve dependency structures, (2) Selective Guided Forward Search (SGFS) that relocates only critical obstacles to feasible points, and (3) an adaptive subgoal refinement method to eliminate unnecessary pick-and-place actions. Evaluations on nine benchmark tasks compare against baselines.

Result: MO-SeGMan consistently finds feasible motion plans in all tasks, with faster solution times and superior solution quality compared to baselines.

Conclusion: The framework is robust and scalable for complex rearrangement planning problems, effectively handling highly constrained and cluttered environments.

Abstract: In this work, we introduce MO-SeGMan, a Multi-Objective Sequential and Guided
Manipulation planner for highly constrained rearrangement problems. MO-SeGMan
generates object placement sequences that minimize both replanning per object
and robot travel distance while preserving critical dependency structures with
a lazy evaluation method. To address highly cluttered, non-monotone scenarios,
we propose a Selective Guided Forward Search (SGFS) that efficiently relocates
only critical obstacles and to feasible relocation points. Furthermore, we
adopt a refinement method for adaptive subgoal selection to eliminate
unnecessary pick-and-place actions, thereby improving overall solution quality.
Extensive evaluations on nine benchmark rearrangement tasks demonstrate that
MO-SeGMan generates feasible motion plans in all cases, consistently achieving
faster solution times and superior solution quality compared to the baselines.
These results highlight the robustness and scalability of the proposed
framework for complex rearrangement planning problems.

</details>


### [477] [Floor Plan-Guided Visual Navigation Incorporating Depth and Directional Cues](https://arxiv.org/abs/2511.01493)
*Wei Huang,Jiaxin Li,Zang Wan,Huijun Di,Wei Liang,Zhu Yang*

Main category: cs.RO

TL;DR: A diffusion-based policy (GlocDiff) combines floor-plan-guided global path planning with depth-aware RGB features to guide indoor navigation, addressing modality gaps and localization in unseen environments, with training-time noise and stable VO at inference, achieving strong FloNa results and real-world deployments.


<details>
  <summary>Details</summary>
Motivation: Bridge the modality gap between egocentric RGB observations and floor plans and improve localization in unseen environments where explicit RGB-floor-plan alignment is lacking, enabling robust obstacle avoidance and global planning.

Method: GlocDiff is a diffusion-based policy that fuses global path planning from a floor plan with local depth-aware features derived from RGB observations; introduces noise perturbation during training to improve robustness to pose estimation errors and relies on a relatively stable VO module during inference.

Result: On FloNa, GlocDiff demonstrates superior navigation performance and robustness; real-world deployments validate practical effectiveness and efficiency.

Conclusion: Diffusion-based policies that integrate explicit floor-plan guidance with local geometric cues offer a promising route for reliable indoor navigation in RGB-only and floor-plan-guided settings, with potential for broad real-world deployment.

Abstract: Guiding an agent to a specific target in indoor environments based solely on
RGB inputs and a floor plan is a promising yet challenging problem. Although
existing methods have made significant progress, two challenges remain
unresolved. First, the modality gap between egocentric RGB observations and the
floor plan hinders the integration of visual and spatial information for both
local obstacle avoidance and global planning. Second, accurate localization is
critical for navigation performance, but remains challenging at deployment in
unseen environments due to the lack of explicit geometric alignment between RGB
inputs and floor plans. We propose a novel diffusion-based policy, denoted as
GlocDiff, which integrates global path planning from the floor plan with local
depth-aware features derived from RGB observations. The floor plan offers
explicit global guidance, while the depth features provide implicit geometric
cues, collectively enabling precise prediction of optimal navigation directions
and robust obstacle avoidance. Moreover, GlocDiff introduces noise perturbation
during training to enhance robustness against pose estimation errors, and we
find that combining this with a relatively stable VO module during inference
results in significantly improved navigation performance. Extensive experiments
on the FloNa benchmark demonstrate GlocDiff's efficiency and effectiveness in
achieving superior navigation performance, and the success of real-world
deployments also highlights its potential for widespread practical
applications.

</details>


### [478] [Phy-Tac: Toward Human-Like Grasping via Physics-Conditioned Tactile Goals](https://arxiv.org/abs/2511.01520)
*Shipeng Lyu,Lijie Sheng,Fangyuan Wang,Wenyao Zhang,Weiwei Lin,Zhenzhong Jia,David Navarro-Alarcon,Guodong Guo*

Main category: cs.RO

TL;DR: Human-inspired physics-conditioned tactile framework for force-optimal stable grasping (Phy-Tac) that unifies pose selection, tactile prediction via Phy-LDM, and a latent-space LQR to achieve stable, force-efficient grasps.


<details>
  <summary>Details</summary>
Motivation: Robots typically rely on rigid, excessive squeezing; bridging the gap to human-like finesse by integrating physics-based pose selection, tactile prediction, and force control to improve stability and minimize contacts/actuation.

Method: 1) physics-based pose selector identifies feasible contact regions and optimal force distribution from surface geometry; 2) physics-conditioned latent diffusion model (Phy-LDM) predicts tactile imprint for the FOSG target; 3) latent-space LQR controller drives gripper toward the tactile imprint with minimal actuation; training on a physics-conditioned tactile dataset; evaluation on robotic platforms.

Result: Phy-LDM achieves superior tactile prediction accuracy; Phy-Tac outperforms fixed-force and GraspNet baselines in grasp stability and force efficiency; experiments show force-efficient, adaptive manipulation bridging gap between robotic and human grasping.

Conclusion: A unified physics-conditioned tactile framework enables force-optimal stable grasping by combining pose selection, tactile prediction, and minimal-actuation control; demonstrates improved stability and efficiency across objects and contact conditions.

Abstract: Humans naturally grasp objects with minimal level required force for
stability, whereas robots often rely on rigid, over-squeezing control. To
narrow this gap, we propose a human-inspired physics-conditioned tactile method
(Phy-Tac) for force-optimal stable grasping (FOSG) that unifies pose selection,
tactile prediction, and force regulation. A physics-based pose selector first
identifies feasible contact regions with optimal force distribution based on
surface geometry. Then, a physics-conditioned latent diffusion model (Phy-LDM)
predicts the tactile imprint under FOSG target. Last, a latent-space LQR
controller drives the gripper toward this tactile imprint with minimal
actuation, preventing unnecessary compression. Trained on a physics-conditioned
tactile dataset covering diverse objects and contact conditions, the proposed
Phy-LDM achieves superior tactile prediction accuracy, while the Phy-Tac
outperforms fixed-force and GraspNet-based baselines in grasp stability and
force efficiency. Experiments on classical robotic platforms demonstrate
force-efficient and adaptive manipulation that bridges the gap between robotic
and human grasping.

</details>


### [479] [MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence](https://arxiv.org/abs/2511.01594)
*Renjun Gao,Peiyan Zhong*

Main category: cs.RO

TL;DR: A multi-agent robotic system (MARS) leverages multimodal large language models to deliver risk-aware, personalized assistive intelligence in smart homes, through four specialized agents (visual perception, risk assessment, planning, evaluation) enabling perception, evaluation, planning, and execution in dynamic indoor environments; it outperforms state-of-the-art multimodal models in risk-aware planning and coordinated multi-agent execution.


<details>
  <summary>Details</summary>
Motivation: Assistive robots for people with disabilities require autonomous, safe, personalized, and robust operation in cluttered indoor environments. existing multimodal LLMs struggle with risk-aware planning, user personalization, and grounding language plans into executable skills.

Method: Introduce MARS architecture comprising four agents: a visual perception agent that extracts semantic and spatial features from environment images; a risk assessment agent that identifies and prioritizes hazards; a planning agent that generates executable action sequences; and an evaluation agent that iteratively optimizes plans. The system uses hierarchical multi-agent decision-making to enable adaptive, risk-aware, and personalized assistance. Experiments on multiple datasets compare performance with state-of-the-art multimodal models.

Result: MARS achieves superior performance in risk-aware planning and coordinated multi-agent execution compared with state-of-the-art multimodal models, demonstrating the feasibility and benefits of collaborative, MLLM-enabled multi-agent systems for practical assistive scenarios.

Conclusion: The work highlights the potential of collaborative AI for assistive robotics in real-world homes and provides a generalizable methodology for deploying MLLM-enabled multi-agent systems in dynamic indoor environments; future work may focus on further grounding, personalization, and real-world deployment.

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities
in cross-modal understanding and reasoning, offering new opportunities for
intelligent assistive systems, yet existing systems still struggle with
risk-aware planning, user personalization, and grounding language plans into
executable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic
System powered by MLLMs for assistive intelligence and designed for smart home
robots supporting people with disabilities. The system integrates four agents:
a visual perception agent for extracting semantic and spatial features from
environment images, a risk assessment agent for identifying and prioritizing
hazards, a planning agent for generating executable action sequences, and an
evaluation agent for iterative optimization. By combining multimodal perception
with hierarchical multi-agent decision-making, the framework enables adaptive,
risk-aware, and personalized assistance in dynamic indoor environments.
Experiments on multiple datasets demonstrate the superior overall performance
of the proposed system in risk-aware planning and coordinated multi-agent
execution compared with state-of-the-art multimodal models. The proposed
approach also highlights the potential of collaborative AI for practical
assistive scenarios and provides a generalizable methodology for deploying
MLLM-enabled multi-agent systems in real-world environments.

</details>


### [480] [Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process](https://arxiv.org/abs/2511.01718)
*Jiayi Chen,Wenxuan Song,Pengxiang Ding,Ziyang Zhou,Han Zhao,Feilong Tang,Donglin Wang,Haoang Li*

Main category: cs.RO

TL;DR: A unified diffusion model for Vision-Language-Action (VLA) called Unified Diffusion VLA with JD3P jointly unifies understanding, generation, and acting via a single diffusion trajectory over a shared multi-modal token space, achieving SOTA with 4x faster inference than autoregressive methods.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of prior VLA systems that rely on external modalities for unification or separate generation and action models, by enabling intrinsic synergy through a synchronous, joint denoising process across all modalities.

Method: Introduce Joint Discrete Denoising Diffusion Process (JD3P) within a Unified Diffusion VLA. The model uses a unified tokenized space for all modalities and a hybrid attention mechanism, enabling a single denoising trajectory that evolves actions and generated content in tandem. A two-stage training pipeline and several inference-time optimizations are proposed to improve performance and efficiency.

Result: The approach achieves state-of-the-art results on CALVIN, LIBERO, and SimplerEnv benchmarks, with about 4x faster inference compared to autoregressive baselines, and is supported by ablations, analyses, and real-world evaluations.

Conclusion: Jointly optimizing generation and action through a synchronous diffusion framework yields intrinsically synergistic VLA capabilities, enabling more efficient and effective understanding, generation, and acting, with strong empirical evidence and a public project page for reproducibility.

Abstract: Vision-language-action (VLA) models aim to understand natural language
instructions and visual observations and to execute corresponding actions as an
embodied agent. Recent work integrates future images into the
understanding-acting loop, yielding unified VLAs that jointly understand,
generate, and act -- reading text and images and producing future images and
actions. However, these models either rely on external experts for modality
unification or treat image generation and action prediction as separate
processes, limiting the benefits of direct synergy between these tasks. Our
core philosophy is to optimize generation and action jointly through a
synchronous denoising process, where the iterative refinement enables actions
to evolve from initialization, under constant and sufficient visual guidance.
We ground this philosophy in our proposed Unified Diffusion VLA and Joint
Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process
that integrates multiple modalities into a single denoising trajectory to serve
as the key mechanism enabling understanding, generation, and acting to be
intrinsically synergistic. Our model and theory are built on a unified
tokenized space of all modalities and a hybrid attention mechanism. We further
propose a two-stage training pipeline and several inference-time techniques
that optimize performance and efficiency. Our approach achieves
state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and
SimplerEnv with 4$\times$ faster inference than autoregressive methods, and we
demonstrate its effectiveness through in-depth analysis and real-world
evaluations. Our project page is available at
https://irpn-eai.github.io/UD-VLA.github.io/.

</details>


### [481] [Lightweight Learning from Actuation-Space Demonstrations via Flow Matching for Whole-Body Soft Robotic Grasping](https://arxiv.org/abs/2511.01770)
*Liudi Yang,Yang Bai,Yuhao Wang,Ibrahim Alsarraj,Gitta Kutyniok,Zhanchi Wang,Ke Wu*

Main category: cs.RO

TL;DR: An actuation-space learning approach for whole-body soft robotic grasping uses a Rectified Flow (flow matching) model to infer distributional control from only 30 demonstrations, achieving 97.5% success across the workspace and strong generalization to object size changes and time-scaling, while reducing reliance on dense sensing and heavy central control.


<details>
  <summary>Details</summary>
Motivation: Uncertain, contact-rich grasping is challenging for rigid robots. Soft robots offer compliance and passive adaptability, suggesting that leveraging their body mechanics can simplify control. The goal is to reduce the burden on central controllers by learning control distributions directly from demonstrations.

Method: Propose a lightweight actuation-space learning framework that learns distributional control representations for whole-body soft grasping from deterministic demonstrations using a flow matching model (Rectified Flow). No dense sensory feedback or heavy control loops required. Trains on 30 demonstrations (â8% of reachable workspace).

Result: The learned policy achieves 97.5% grasp success across the workspace, generalizes to object size variations of Â±33%, and remains stable when robot dynamics are altered by scaling execution time from 20% to 200%. The approach leverages passive redundant DOFs and flexibility to convert body mechanics into functional control intelligence, reducing the load on central controllers.

Conclusion: Actuation-space learning effectively exploits soft-body mechanics to handle uncertain-rich grasping tasks, enabling robust performance with minimal demonstrations and reduced reliance on dense sensing and complex control architectures.

Abstract: Robotic grasping under uncertainty remains a fundamental challenge due to its
uncertain and contact-rich nature. Traditional rigid robotic hands, with
limited degrees of freedom and compliance, rely on complex model-based and
heavy feedback controllers to manage such interactions. Soft robots, by
contrast, exhibit embodied mechanical intelligence: their underactuated
structures and passive flexibility of their whole body, naturally accommodate
uncertain contacts and enable adaptive behaviors. To harness this capability,
we propose a lightweight actuation-space learning framework that infers
distributional control representations for whole-body soft robotic grasping,
directly from deterministic demonstrations using a flow matching model
(Rectified Flow),without requiring dense sensing or heavy control loops. Using
only 30 demonstrations (less than 8% of the reachable workspace), the learned
policy achieves a 97.5% grasp success rate across the whole workspace,
generalizes to grasped-object size variations of +-33%, and maintains stable
performance when the robot's dynamic response is directly adjusted by scaling
the execution time from 20% to 200%. These results demonstrate that
actuation-space learning, by leveraging its passive redundant DOFs and
flexibility, converts the body's mechanics into functional control intelligence
and substantially reduces the burden on central controllers for this
uncertain-rich task.

</details>


### [482] [MOBIUS: A Multi-Modal Bipedal Robot that can Walk, Crawl, Climb, and Roll](https://arxiv.org/abs/2511.01774)
*Alexander Schperberg,Yusuke Tanaka,Stefano Di Cairano,Dennis Hong*

Main category: cs.RO

TL;DR: MOBIUS is a four-limbed mobile loco-manipulator that merges RL-based locomotion with model-based predictive and admittance control, safeguarded by a Reference Governor, and guided by an MIQCP planner to flexibly walk, crawl, climb, and grasp across varied terrains.


<details>
  <summary>Details</summary>
Motivation: To extend mobile manipulation capabilities across diverse terrains while ensuring safe, compliant contact interactions through tight integration of morphology, planning, and control.

Method: A four-limb robot with two 6-DoF arms (two-finger grippers) and two 4-DoF legs enables seamless transitions across terrains. A hybrid control architecture combines reinforcement-learningâbased locomotion with model-based predictive control and admittance control, enhanced by a Reference Governor for safety. A high-level MIQCP planner autonomously selects locomotion modes. Hardware experiments validate transitions, dynamic climbing, and full-body load support.

Result: Robust gait transitions, dynamic climbing, and full-body load support via pinch grasp demonstrated on hardware; the robot maintains stability and energy efficiency across modes without reconfiguration.

Conclusion: Tight integration of morphology, high-level planning, and control is crucial to enable mobile loco-manipulation and grasping, expanding interaction capabilities, workspace, and traversability.

Abstract: This article presents a Multi-Modal Bipedal Intelligent Urban Scout robot
(MOBIUS) capable of walking, crawling, climbing, and rolling. MOBIUS features
four limbs--two 6-DoF arms with two-finger grippers for manipulation and
climbing, and two 4-DoF legs for locomotion--enabling smooth transitions across
diverse terrains without reconfiguration. A hybrid control architecture
combines reinforcement learning-based locomotion with model-based predictive
and admittance control enhanced for safety by a Reference Governor toward
compliant contact interactions. A high-level MIQCP planner autonomously selects
locomotion modes to balance stability and energy efficiency. Hardware
experiments demonstrate robust gait transitions, dynamic climbing, and
full-body load support via pinch grasp. Overall, MOBIUS demonstrates the
importance of tight integration between morphology, high-level planning, and
control to enable mobile loco-manipulation and grasping, substantially
expanding its interaction capabilities, workspace, and traversability.

</details>


### [483] [GenDexHand: Generative Simulation for Dexterous Hands](https://arxiv.org/abs/2511.01791)
*Feng Chen,Zhuxiu Xu,Tianzhe Chu,Xunzhe Zhou,Li Sun,Zewen Wu,Shenghua Gao,Zhongyu Li,Yanchao Yang,Yi Ma*

Main category: cs.RO

TL;DR: GenDexHand is a closed-loop generative pipeline that autonomously creates diverse, trainable dexterous hand tasks/environments using vision-language feedback and sub-task decomposition to enable sequential RL in simulation.


<details>
  <summary>Details</summary>
Motivation: Data scarcity and the mismatch of LLM-generated dexterous tasks to real-world dexterous manipulation; dexterous manipulation has higher DOFs and is harder, requiring specialized environments. A scalable, automatic way to generate varied, trainable tasks is needed.

Method: A closed-loop generative simulation pipeline (GenDexHand) that refines object placements and scales based on vision-language model feedback to improve environment quality. Each task is decomposed into sub-tasks to enable sequential reinforcement learning, reducing training time and increasing success rates.

Result: Significant improvement in the average quality of generated environments, shorter training times, and higher success rates in learned dexterous manipulation tasks.

Conclusion: Offers a viable, scalable pathway for synthesizing diverse dexterous hand behaviors in simulation, addressing data scarcity for embodied intelligence; demonstrated through a generative pipeline and sub-task decomposition, with a public website for the project.

Abstract: Data scarcity remains a fundamental bottleneck for embodied intelligence.
Existing approaches use large language models (LLMs) to automate gripper-based
simulation generation, but they transfer poorly to dexterous manipulation,
which demands more specialized environment design. Meanwhile, dexterous
manipulation tasks are inherently more difficult due to their higher degrees of
freedom. Massively generating feasible and trainable dexterous hand tasks
remains an open challenge. To this end, we present GenDexHand, a generative
simulation pipeline that autonomously produces diverse robotic tasks and
environments for dexterous manipulation. GenDexHand introduces a closed-loop
refinement process that adjusts object placements and scales based on
vision-language model (VLM) feedback, substantially improving the average
quality of generated environments. Each task is further decomposed into
sub-tasks to enable sequential reinforcement learning, reducing training time
and increasing success rates. Our work provides a viable path toward scalable
training of diverse dexterous hand behaviors in embodied intelligence by
offering a simulation-based solution to synthetic data generation. Our website:
https://winniechen2002.github.io/GenDexHand/.

</details>


### [484] [Hybrid Neural Network-Based Indoor Localisation System for Mobile Robots Using CSI Data in a Robotics Simulator](https://arxiv.org/abs/2511.01797)
*Javier Ballesteros-Jerez,Jesus MartÃ­nez-GÃ³mez,Ismael GarcÃ­a-Varea,Luis Orozco-Barbosa,Manuel Castillo-Cara*

Main category: cs.RO

TL;DR: A hybrid CNNâMLP model (HyNN) maps Massive MIMO CSI data to 2D indoor robot positions using TINTO-generated images, integrated with ROS and simulators and Kalman filtering; claims a generalizable procedure for robotics scenarios.


<details>
  <summary>Details</summary>
Motivation: Accurate indoor localization for mobile robots is challenging. Wireless Channel State Information (CSI) contains rich spatial information that can be repurposed for localization. Using an existing CSI dataset and converting it into a format suitable for neural nets enables learning-based localization. Integrating with robotics tooling (ROS, simulators) facilitates evaluation in realistic, heterogeneous environments and supports state estimation (e.g., Kalman filters).

Method: Convert CSI readings into synthetic images via the TINTO tool. Feed images into a CNN to extract spatial features, then use an MLP to map features to 2D coordinates, forming a Hybrid Neural Network (HyNN). Integrate the result with a robotics simulator and ROS; apply Kalman filters for state estimation. Evaluate across heterogeneous test cases and propose a generalizable procedure applicable beyond the specific dataset.

Result: The approach demonstrates the potential for precise indoor 2D localization of mobile robots using CSI-based signals within a robotics framework, showing compatibility with ROS, simulators, and standard state estimators; the procedure is argued to be adaptable to different environments and datasets.

Conclusion: HyNN leveraging CSI data with a CNNâMLP architecture offers a promising, generalizable workflow for indoor robot localization and navigation, with demonstrated integration into robotics tooling; future work should substantiate quantitative gains and broaden validations across more scenarios.

Abstract: We present a hybrid neural network model for inferring the position of mobile
robots using Channel State Information (CSI) data from a Massive MIMO system.
By leveraging an existing CSI dataset, our approach integrates a Convolutional
Neural Network (CNN) with a Multilayer Perceptron (MLP) to form a Hybrid Neural
Network (HyNN) that estimates 2D robot positions. CSI readings are converted
into synthetic images using the TINTO tool. The localisation solution is
integrated with a robotics simulator, and the Robot Operating System (ROS),
which facilitates its evaluation through heterogeneous test cases, and the
adoption of state estimators like Kalman filters. Our contributions illustrate
the potential of our HyNN model in achieving precise indoor localisation and
navigation for mobile robots in complex environments. The study follows, and
proposes, a generalisable procedure applicable beyond the specific use case
studied, making it adaptable to different scenarios and datasets.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [485] [A Couple of Simple Algorithms for $k$-Dispersion](https://arxiv.org/abs/2511.00692)
*Ke Chen,Adrian Dumitrescu*

Main category: cs.CG

TL;DR: The abstract presents three main algorithmic results for the k-dispersion problem: (I) a plane algorithm running in O(n^{k-1} log n) for general k (extending k=3) with improved small-k performance; (II) a 3D algorithm with O(n^{k-1} log n) time for even k and O(n^{k-1} log^2 n) for odd k, noting that no o(n^k) algorithms were known for kâ¥4; (III) a linear-time (O(n)) 0.99-approximation for random points in [0,1]^2 with high probability. These contribute to the understanding of exact and approximate dispersion computations in low dimensions and show favorable complexity when k is small or dimensions are fixed.


<details>
  <summary>Details</summary>
Motivation: Dispersion optimization (maximizing the minimum pairwise distance among k selected points) is a natural objective in clustering, facility placement, and network design. Understanding its complexity in fixed dimensions and for varying k informs algorithm design for geometric optimization problems.

Method: The abstract signals combinatorial algorithmic techniques that achieve dimension- and parity-based time bounds. It extends known k=3 results to arbitrary k in the plane, and shows a parity-dependent complexity in 3D. It also leverages probabilistic/average-case assumptions to obtain a fast linear-time 0.99-approximation for random points. Specific algorithmic details (e.g., data structures, reductions, or dynamic programming schemes) are not disclosed in the abstract.

Result: (I) A plane algorithm solving k-dispersion in O(n^{k-1} log n) time for any kâ¥2 (extending the k=3 result). (II) In R^3, O(n^{k-1} log n) time when k is even; O(n^{k-1} log^2 n) when k is odd; no o(n^k) combinatorial algorithm known for kâ¥4. (III) For random uniform points in [0,1]^2, a 0.99-approximation can be computed in O(n) time with high probability under suitable conditions.

Conclusion: The results advance exact and approximation approaches for k-dispersion in low dimensions, providing improved time bounds and clarifying the role of dimension and parity. They widen the practical applicability of dispersion optimization by offering faster algorithms for common cases and highlighting the remaining gaps (e.g., general o(n^k) lower/upper bounds in 3D for larger k, and extensions to broader distributions).

Abstract: Given a set $P$ of $n$ points in $\mathbf{R}^d$, and a positive integer $k
\leq n$, the $k$-dispersion problem is that of selecting $k$ of the given
points so that the minimum inter-point distance among them is maximized (under
Euclidean distances). Among others, we show the following:
  (I) Given a set $P$ of $n$ points in the plane, and a positive integer $k
\geq 2$, the $k$-dispersion problem can be solved by an algorithm running in
$O\left(n^{k-1} \log{n}\right)$ time. This extends an earlier result for $k=3$,
due to Horiyama, Nakano, Saitoh, Suetsugu, Suzuki, Uehara, Uno, and Wasa (2021)
to arbitrary $k$. In particular, it improves on previous running times for
small $k$.
  (II) Given a set $P$ of $n$ points in $\mathbf{R}^3$, and a positive integer
$k \geq 2$, the $k$-dispersion problem can be solved by an algorithm running in
$O\left(n^{k-1} \log{n}\right)$ time, if $k$ is even; and $O\left(n^{k-1}
\log^2{n}\right)$ time, if $k$ is odd. For $k \geq 4$, no combinatorial
algorithm running in $o(n^k)$ time was known for this problem.
  (III) Let $P$ be a set of $n$ random points uniformly distributed in
$[0,1]^2$. Then under suitable conditions, a $0.99$-approximation for
$k$-dispersion can be computed in $O(n)$ time with high probability.

</details>


### [486] [NP-membership for the boundary-boundary art-gallery problem](https://arxiv.org/abs/2511.01562)
*Jack Stade*

Main category: cs.CG

TL;DR: Proves boundary-boundary art-gallery variant is in NP; analyzes the nine X-Y variants; introduces a constraint-propagation approach for binary-variable continuous CSPs; finds that optimal boundary-boundary solutions may require irrational guard placements and thus resists discretization; situates boundary-boundary between NP and exists-R in the complexity landscape.


<details>
  <summary>Details</summary>
Motivation: Resolve the complexity of the boundary-boundary art-gallery problem and complete the taxonomy of X-Y variants; understand whether discretization is feasible and how the problem relates to known complexity classes (NP, exists-R).

Method: Develop a constraint-propagation framework for continuous constraint satisfaction problems where constraints involve at most two variables; apply this to visibility constraints on the boundary of a polygon; analyze reductions for X-Y variants (notably X-vertex and vertex-Y to set cover) and construct examples showing irrational guard coordinates are necessary for some instances; compare to prior results (ER-complete variants) to position the boundary-boundary problem.

Result: The boundary-boundary variant is shown to be in NP. A comprehensive view of all nine X-Y variants is provided: X-vertex and vertex-Y are NP-complete; point-point, point-boundary, and boundary-point are exist-R-complete; boundary-boundary lies between NP and exists-R. It is demonstrated that some instances require guards at irrational coordinates, indicating discretization is unlikely.

Conclusion: This work closes part of the complexity gap by placing boundary-boundary in NP and clarifies the landscape of X-Y variants. It raises further questions about exact hardness (e.g., NP-hardness or NP-completeness for boundary-boundary), and invites exploration of continuous CSP techniques for other geometric visibility problems.

Abstract: The boundary-boundary art-gallery problem asks, given a polygon $P$
representing an art-gallery, for a minimal set of guards that can see the
entire boundary of $P$ (the wall of the art gallery), where the guards must be
placed on the boundary. We show that this art-gallery variant is in NP. In
order to prove this, we develop a constraint-propagation procedure for
continuous constraint satisfaction problems where each constraint involves at
most 2 variables.
  The X-Y variant of the art-gallery problem is the one where the guards must
lie in X and need to see all of Y. Each of X and Y can be either the vertices
of the polygon, the boundary of the polygon, or the entire polygon, giving 9
different variants. Previously, it was known that X-vertex and vertex-Y
variants are all NP-complete and that the point-point, point-boundary, and
boundary-point variants are $\exists \mathbb{R}$-complete [Abrahamsen,
Adamaszek, and Miltzow, JACM 2021][Stade, SoCG 2025]. However, the
boundary-boundary variant was only known to lie somewhere between NP and
$\exists \mathbb{R}$.
  The X-vertex and vertex-Y variants can be straightforwardly reduced to
discrete set-cover instances. In contrast, we give example to show that a
solution to an instance of the boundary-boundary art-gallery problem sometimes
requires placing guards at irrational coordinates, so it unlikely that the
problem can be easily discretized.

</details>
