<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 123]
- [cs.RO](#cs.RO) [Total: 55]
- [cs.AI](#cs.AI) [Total: 48]
- [cs.LG](#cs.LG) [Total: 125]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset](https://arxiv.org/abs/2509.18159)
*Akwasi Asare,Ulas Bagci*

Main category: cs.CV

TL;DR: PolypSeg-GradCAM is an explainable U-Net-based polyp segmentation model using Grad-CAM for transparency, achieving high IoU and Dice on Kvasir-SEG, promoting trustworthy AI-assisted colonoscopy.


<details>
  <summary>Details</summary>
Motivation: CRC prevention; need accurate polyp segmentation; limited interpretability hinders clinical adoption.

Method: U-Net architecture integrated with Grad-CAM to provide localization explanations; trained/evaluated on Kvasir-SEG (1000 images).

Result: Mean IoU 0.9257 on test set; Dice/F-score > 0.96 on training/validation; Grad-CAM visualizations show predictions guided by clinically relevant regions.

Conclusion: Combines accuracy with interpretability to advance trustworthy AI-assisted colonoscopy and early CRC prevention.

Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related
morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as
critical precursors according to the World Health Organization (WHO). Early and
accurate segmentation of polyps during colonoscopy is essential for reducing
CRC progression, yet manual delineation is labor-intensive and prone to
observer variability. Deep learning methods have demonstrated strong potential
for automated polyp analysis, but their limited interpretability remains a
barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an
explainable deep learning framework that integrates the U-Net architecture with
Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp
segmentation. The model was trained and evaluated on the Kvasir-SEG dataset of
1000 annotated endoscopic images. Experimental results demonstrate robust
segmentation performance, achieving a mean Intersection over Union (IoU) of
0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)
on training and validation sets. Grad-CAM visualizations further confirmed that
predictions were guided by clinically relevant regions, enhancing transparency
and trust in the model's decisions. By coupling high segmentation accuracy with
interpretability, PolypSeg-GradCAM represents a step toward reliable,
trustworthy AI-assisted colonoscopy and improved early colorectal cancer
prevention.

</details>


### [2] [PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2509.18160)
*Akwasi Asare,Isaac Baffour Senkyire,Emmanuel Freeman,Simon Hilary Ayinedenaba Aluze-Ele,Kelvin Kwao*

Main category: cs.CV

TL;DR: A deep learning teleophthalmology system (PerceptronCARE) uses CNNs (ResNet-18, EfficientNet-B0, SqueezeNet) to detect diabetic retinopathy with 85.4% accuracy, enabling real-time screening in cloud-based, multi-user telemedicine settings.


<details>
  <summary>Details</summary>
Motivation: Address the global burden of diabetic retinopathy and the need for accessible, scalable screening in underserved regions through AI-powered telemedicine.

Method: Comparative evaluation of multiple CNN architectures (ResNet-18, EfficientNet-B0, SqueezeNet) for automated DR detection from retinal images; selection of a model balancing accuracy and efficiency; deployment in a cloud-based, multi-user teleophthalmology platform.

Result: Final model achieves 85.4% accuracy in disease severity classification and supports real-time screening; demonstrates potential for reducing costs and improving access in clinical and telemedicine contexts.

Conclusion: AI-driven telemedicine like PerceptronCARE can expand DR screening access, particularly in remote or resource-constrained settings, while integrating secure data management and scalable infrastructure.

Abstract: Diabetic retinopathy is a leading cause of vision loss among adults and a
major global health challenge, particularly in underserved regions. This study
presents PerceptronCARE, a deep learning-based teleophthalmology application
designed for automated diabetic retinopathy detection using retinal images. The
system was developed and evaluated using multiple convolutional neural
networks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine
the optimal balance between accuracy and computational efficiency. The final
model classifies disease severity with an accuracy of 85.4%, enabling real-time
screening in clinical and telemedicine settings. PerceptronCARE integrates
cloud-based scalability, secure patient data management, and a multi-user
framework, facilitating early diagnosis, improving doctor-patient interactions,
and reducing healthcare costs. This study highlights the potential of AI-driven
telemedicine solutions in expanding access to diabetic retinopathy screening,
particularly in remote and resource-constrained environments.

</details>


### [3] [Self Identity Mapping](https://arxiv.org/abs/2509.18165)
*Xiuding Cai,Yaoyao Zhu,Linjie Fu,Dong Miao,Yu Yao*

Main category: cs.CV

TL;DR: Self Identity Mapping (SIM) is a simple, data-intrinsic regularizer that reconstructs the input from a transformed output to improve representation learning. The ρSIM instantiation reduces computation via patch-level sampling and projection-based latent-feature reconstruction. It is model- and task-agnostic and plug-and-play, with empirical gains across classification, few-shot prompting, and domain generalization, and potential benefits for dense-to-dense tasks and non-visual domains. Code is released.


<details>
  <summary>Details</summary>
Motivation: Regularization in deep learning often relies on heuristic tricks that may not generalize well across tasks and domains. There is a need for a simple, data-intrinsic, model- and task-agnostic regularizer that preserves semantic information and improves gradient flow.

Method: Introduce Self Identity Mapping (SIM) that reconstructs input from its transformed output, thereby reducing information loss and smoothing gradient flow. To reduce cost, instantiate SIM as ρSIM by incorporating patch-level feature sampling and a projection-based method to reconstruct latent features, making it efficient. The approach is designed as a plug-and-play module that works across network architectures and tasks.

Result: ρSIM yields consistent improvements over baselines across image classification, few-shot prompt learning, and domain generalization. It is orthogonal to existing regularization methods and enhances their effectiveness. It preserves semantic information and improves performance in dense-to-dense tasks (e.g., semantic segmentation, image translation) and in non-visual domains (audio classification, time-series anomaly detection). Code is publicly available.

Conclusion: SIM offers a simple, effective, data-intrinsic regularization framework that enhances representation learning and generalization while being model- and task-agnostic and easily integrable as a plug-and-play module.它

Abstract: Regularization is essential in deep learning to enhance generalization and
mitigate overfitting. However, conventional techniques often rely on
heuristics, making them less reliable or effective across diverse settings. We
propose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic
regularization framework that leverages an inverse mapping mechanism to enhance
representation learning. By reconstructing the input from its transformed
output, SIM reduces information loss during forward propagation and facilitates
smoother gradient flow. To address computational inefficiencies, We instantiate
SIM as $ \rho\text{SIM} $ by incorporating patch-level feature sampling and
projection-based method to reconstruct latent features, effectively lowering
complexity. As a model-agnostic, task-agnostic regularizer, SIM can be
seamlessly integrated as a plug-and-play module, making it applicable to
different network architectures and tasks.
  We extensively evaluate $\rho\text{SIM}$ across three tasks: image
classification, few-shot prompt learning, and domain generalization.
Experimental results show consistent improvements over baseline methods,
highlighting $\rho\text{SIM}$'s ability to enhance representation learning
across various tasks. We also demonstrate that $\rho\text{SIM}$ is orthogonal
to existing regularization methods, boosting their effectiveness. Moreover, our
results confirm that $\rho\text{SIM}$ effectively preserves semantic
information and enhances performance in dense-to-dense tasks, such as semantic
segmentation and image translation, as well as in non-visual domains including
audio classification and time series anomaly detection. The code is publicly
available at https://github.com/XiudingCai/SIM-pytorch.

</details>


### [4] [MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion](https://arxiv.org/abs/2509.18170)
*Zhanting Zhou,Jinbo Wang,Zeqin Wu,Fengli Zhang*

Main category: cs.CV

TL;DR: MAGIA is a momentum-based adaptive gradient-inversion attack for single-round averaged gradient (SAG) that reconstructs multiple images from large batches without labels, using a closed-form rescaling bound and batch-subset loss mixing; it achieves state-of-the-art fidelity with a compute footprint similar to standard solvers.


<details>
  <summary>Details</summary>
Motivation: In SAG, per-sample cues are entangled in a batch mean, making inversion hard and privacy risks poorly understood. There is a need for strong attacks that do not rely on label information and scale to large batches.

Method: Two innovations: (1) closed-form combinatorial rescaling to tighten the optimization bound; (2) momentum-based mixing of whole-batch and subset losses, plus probing random data subsets to sense latent per-image signals; optimization occurs without requiring labels.

Result: Experiments show MAGIA significantly outperforms existing methods, enabling high-fidelity multi-image reconstruction in large batch SAG where prior works fail; computational cost is comparable to standard solvers; no auxiliary information required.

Conclusion: MAGIA reveals a strong privacy risk in SAG and demonstrates robustness to batch size, motivating future defenses and more secure gradient-sharing protocols.

Abstract: We study gradient inversion in the challenging single round averaged gradient
SAG regime where per sample cues are entangled within a single batch mean
gradient. We introduce MAGIA a momentum based adaptive correction on gradient
inversion attack a novel label inference free framework that senses latent per
image signals by probing random data subsets. MAGIA objective integrates two
core innovations 1 a closed form combinatorial rescaling that creates a
provably tighter optimization bound and 2 a momentum based mixing of whole
batch and subset losses to ensure reconstruction robustness. Extensive
experiments demonstrate that MAGIA significantly outperforms advanced methods
achieving high fidelity multi image reconstruction in large batch scenarios
where prior works fail. This is all accomplished with a computational footprint
comparable to standard solvers and without requiring any auxiliary information.

</details>


### [5] [Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR](https://arxiv.org/abs/2509.18174)
*Khalil Hennara,Muhammad Hreden,Mohamed Motasim Hamed,Ahmad Bastati,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CV

TL;DR: Baseer is a vision-language model fine-tuned for Arabic document OCR using decoder-only fine-tuning on a mixed synthetic-real dataset, introducing Misraj-DocOCR benchmark; achieves state-of-the-art performance with WER 0.25.


<details>
  <summary>Details</summary>
Motivation: Arabic OCR is challenging due to cursive script, font diversity, diacritics, and RTL orientation, and existing MLLMs underperform on Arabic; a domain-specific, robust model is needed.

Method: Fine-tune a pre-trained multimodal LLM with a decoder-only strategy on a large-scale dataset combining synthetic and real Arabic documents; also develop Misraj-DocOCR as a high-quality benchmark for rigorous evaluation.

Result: Baseer significantly outperforms existing open-source and commercial solutions, achieving a WER of 0.25 and setting a new state-of-the-art for Arabic document OCR.

Conclusion: Domain-specific adaptation of general-purpose MLLMs yields substantial gains for morphologically rich languages like Arabic and establishes a strong baseline for high-accuracy OCR.

Abstract: Arabic document OCR remains a challenging task due to the language's cursive
script, diverse fonts, diacritics, and right-to-left orientation. While modern
Multimodal Large Language Models (MLLMs) have advanced document understanding
for high-resource languages, their performance on Arabic remains limited. In
this work, we introduce Baseer, a vision-language model fine- tuned
specifically for Arabic document OCR. Leveraging a large-scale dataset
combining synthetic and real-world documents, Baseer is trained using a
decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving
general visual features. We also present Misraj-DocOCR, a high-quality,
expert-verified benchmark designed for rigorous evaluation of Arabic OCR
systems. Our experiments show that Baseer significantly outperforms existing
open-source and commercial solutions, achieving a WER of 0.25 and establishing
a new state-of-the-art in the domain of Arabic document OCR. Our results
highlight the benefits of domain-specific adaptation of general-purpose MLLMs
and establish a strong baseline for high-accuracy OCR on morphologically rich
languages like Arabic.

</details>


### [6] [A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland](https://arxiv.org/abs/2509.18176)
*Wendong Yao,Saeed Azadnejad,Binhua Huang,Shane Donohue,Soumyabrata Dev*

Main category: cs.CV

TL;DR: A spatio-temporal CNN-LSTM framework forecasts ground deformation from sparse InSAR data by converting to a dense tensor and achieving state-of-the-art performance against LGBM and LASSO, with improved spatial coherence and interpretability.


<details>
  <summary>Details</summary>
Motivation: Forecasting future deformation is crucial for urban infrastructure stability and hazard mitigation, but sparse InSAR time-series data pose a significant challenge for accurate forecasting.

Method: Convert sparse InSAR measurements into a dense spatio-temporal tensor; develop a hybrid CNN-LSTM model to learn spatial and temporal dependencies; benchmark against LightGBM and LASSO using Sentinel-1 data from eastern Ireland.

Result: The proposed architecture yields significantly more accurate and spatially coherent forecasts, setting a new benchmark; interpretability analysis shows baselines rely on persistence, highlighting the value of the spatio-temporal approach.

Conclusion: Spatio-temporal deep learning is effective for high-resolution deformation forecasting and has strong potential for real-world hazard monitoring.

Abstract: Monitoring ground displacement is crucial for urban infrastructure stability
and mitigating geological hazards. However, forecasting future deformation from
sparse Interferometric Synthetic Aperture Radar (InSAR) time-series data
remains a significant challenge. This paper introduces a novel deep learning
framework that transforms these sparse point measurements into a dense
spatio-temporal tensor. This methodological shift allows, for the first time,
the direct application of advanced computer vision architectures to this
forecasting problem. We design and implement a hybrid Convolutional Neural
Network and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to
simultaneously learn spatial patterns and temporal dependencies from the
generated data tensor. The model's performance is benchmarked against powerful
machine learning baselines, Light Gradient Boosting Machine and LASSO
regression, using Sentinel-1 data from eastern Ireland. Results demonstrate
that the proposed architecture provides significantly more accurate and
spatially coherent forecasts, establishing a new performance benchmark for this
task. Furthermore, an interpretability analysis reveals that baseline models
often default to simplistic persistence patterns, highlighting the necessity of
our integrated spatio-temporal approach to capture the complex dynamics of
ground deformation. Our findings confirm the efficacy and potential of
spatio-temporal deep learning for high-resolution deformation forecasting.

</details>


### [7] [A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts](https://arxiv.org/abs/2509.18177)
*George Corrêa de Araújo,Helena de Almeida Maia,Helio Pedrini*

Main category: cs.CV

TL;DR: Scrapbook is a framework that generates large, diverse datasets to probe AI models’ understanding of basic concepts (object recognition, positions, attributes) and to evaluate reasoning under varied linguistic forms.


<details>
  <summary>Details</summary>
Motivation: To systematically assess foundational concept comprehension in AI models before tackling complex tasks and to identify gaps in understanding of objects, positions, and attributes.

Method: Generate extensive question datasets about individual concepts with wide linguistic variation; test across models to evaluate comprehension of objects, absolute/relative positions, and attributes; analyze answer correctness and consistency.

Result: Models perform well on object recognition/enumeration but struggle with positional reasoning and constrained questions; MobileVLM-V2 shows significant answer disagreements and plausible wrong answers; many models biased toward affirmative responses and struggle with geometry-related questions.

Conclusion: The Scrapbook framework is a valuable tool for generating diverse datasets to systematically assess and improve AI models’ basic-concept understanding, highlighting specific areas (positional reasoning, constraints) to target for improvement.

Abstract: In this paper, we present the Scrapbook framework, a novel methodology
designed to generate extensive datasets for probing the learned concepts of
artificial intelligence (AI) models. The framework focuses on fundamental
concepts such as object recognition, absolute and relative positions, and
attribute identification. By generating datasets with a large number of
questions about individual concepts and a wide linguistic variation, the
Scrapbook framework aims to validate the model's understanding of these basic
elements before tackling more complex tasks. Our experimental findings reveal
that, while contemporary models demonstrate proficiency in recognizing and
enumerating objects, they encounter challenges in comprehending positional
information and addressing inquiries with additional constraints. Specifically,
the MobileVLM-V2 model showed significant answer disagreements and plausible
wrong answers, while other models exhibited a bias toward affirmative answers
and struggled with questions involving geometric shapes and positional
information, indicating areas for improvement in understanding and consistency.
The proposed framework offers a valuable instrument for generating diverse and
comprehensive datasets, which can be utilized to systematically assess and
enhance the performance of AI models.

</details>


### [8] [The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes](https://arxiv.org/abs/2509.18179)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: Quantifies the describe-then-generate bottleneck in vision–language–vision pipelines using 150 image-pairs and standard image metrics, finding pervasive perceptual and structural information loss when visual content is mediated by text.


<details>
  <summary>Details</summary>
Motivation: As multimodal AI becomes integrated into creative workflows, there is a need to quantify how much information is lost when visual content is translated into natural language and then reconstructed, i.e., the describe-then-generate bottleneck.

Method: Construct a describe-then-generate pipeline to produce 150 image pairs, evaluate preservation of visual information using perceptual (LPIPS), structural (SSIM), and chromatic (color distance) metrics across perceptual, structural, and color dimensions, and report degradation rates.

Result: Consistent degradation observed: 99.3% of samples show substantial perceptual degradation; 91.5% show significant structural information loss, indicating a measurable bottleneck in current multimodal systems.

Conclusion: The describe-then-generate bottleneck is a real and measurable limitation. These results highlight the need for better intermediate representations or end-to-end multimodal approaches that preserve richer visual information than a textual intermediary allows.

Abstract: With the increasing integration of multimodal AI systems in creative
workflows, understanding information loss in vision-language-vision pipelines
has become important for evaluating system limitations. However, the
degradation that occurs when visual content passes through textual
intermediation remains poorly quantified. In this work, we provide empirical
analysis of the describe-then-generate bottleneck, where natural language
serves as an intermediate representation for visual information. We generated
150 image pairs through the describe-then-generate pipeline and applied
existing metrics (LPIPS, SSIM, and color distance) to measure information
preservation across perceptual, structural, and chromatic dimensions. Our
evaluation reveals that 99.3% of samples exhibit substantial perceptual
degradation and 91.5% demonstrate significant structural information loss,
providing empirical evidence that the describe-then-generate bottleneck
represents a measurable and consistent limitation in contemporary multimodal
systems.

</details>


### [9] [AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines](https://arxiv.org/abs/2509.18182)
*Isabelle Tingzon,Yoji Toriumi,Caroline Gevaert*

Main category: cs.CV

TL;DR: AI-driven workflow to infer rooftop attributes from high-res imagery in SIDS; compares foundation models with shallow classifiers; adds neighboring SIDS data; best F1 0.88 (roof pitch) and 0.83 (roof material); aims to support urban governance.


<details>
  <summary>Details</summary>
Motivation: Address lack of detailed structural data in climate-vulnerable Small Island Developing States (SIDS) to improve hazard damage estimation and urban resilience; leverage AI and Earth Observation data to fill data gaps and enable evidence-based planning.

Method: Evaluate geospatial foundation models combined with shallow classifiers vs fine-tuned deep learning models for rooftop classification; apply to Saint Vincent and the Grenadines; assess impact of incorporating training data from neighboring SIDS.

Result: Best models achieved F1 scores of 0.88 for roof pitch and 0.83 for roof material; demonstrates viability of the AI/EO workflow; potential performance gains when using training data from neighboring SIDS.

Conclusion: AI-enabled rooftop attribute inference can empower SIDS with data-driven urban governance; emphasizes need for local capacity building to deploy these tools in practice.

Abstract: Detailed structural building information is used to estimate potential damage
from hazard events like cyclones, floods, and landslides, making them critical
for urban resilience planning and disaster risk reduction. However, such
information is often unavailable in many small island developing states (SIDS)
in climate-vulnerable regions like the Caribbean. To address this data gap, we
present an AI-driven workflow to automatically infer rooftop attributes from
high-resolution satellite imagery, with Saint Vincent and the Grenadines as our
case study. Here, we compare the utility of geospatial foundation models
combined with shallow classifiers against fine-tuned deep learning models for
rooftop classification. Furthermore, we assess the impact of incorporating
additional training data from neighboring SIDS to improve model performance.
Our best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof
material classification, respectively. Combined with local capacity building,
our work aims to provide SIDS with novel capabilities to harness AI and Earth
Observation (EO) data to enable more efficient, evidence-based urban
governance.

</details>


### [10] [VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation](https://arxiv.org/abs/2509.18183)
*Jinyue Bian,Zhaoxing Zhang,Zhengyu Liang,Shiwei Zheng,Shengtao Zhang,Rong Shen,Chen Yang,Anzhou Hou*

Main category: cs.CV

TL;DR: A lightweight 2D-data–driven module (VLA-LPAF) enhances perspective adaptivity of Visual-Language-Action models, fused in latent space, improving task success across multiple benchmarks when added to RoboFlamingo.


<details>
  <summary>Details</summary>
Motivation: Perspective heterogeneity across cameras and viewpoints reduces generality of VLA models; need an efficient method that uses only 2D data to adapt to multiple views.

Method: Propose VLA-LPAF, a lightweight finetuned module using a single-view 2D image; fuses other multiview observations in latent space; instantiated as RoboFlamingo-LPAF.

Result: Avg improvements of ~8% on CALVIN, ~15% on LIBERO, ~30% on a customized simulation benchmark; real-world demonstrations show view-adaptive behavior.

Conclusion: VLA-LPAF effectively bridges perspective gaps, improving generalization of VLA models with minimal computational burden; validated on RoboFlamingo and real-world tasks.

Abstract: The Visual-Language-Action (VLA) models can follow text instructions
according to visual observations of the surrounding environment. This ability
to map multimodal inputs to actions is derived from the training of the VLA
model on extensive standard demonstrations. These visual observations captured
by third-personal global and in-wrist local cameras are inevitably varied in
number and perspective across different environments, resulting in significant
differences in the visual features. This perspective heterogeneity constrains
the generality of VLA models. In light of this, we first propose the
lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models
using only 2D data. VLA-LPAF is finetuned using images from a single view and
fuses other multiview observations in the latent space, which effectively and
efficiently bridge the gap caused by perspective inconsistency. We instantiate
our VLA-LPAF framework with the VLA model RoboFlamingo to construct
RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves
around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a
customized simulation benchmark. We also demonstrate the developed viewadaptive
characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.

</details>


### [11] [URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation](https://arxiv.org/abs/2509.18184)
*Yifeng Cheng,Alois Knoll,Hu Cao*

Main category: cs.CV

TL;DR: Proposes URNet, an uncertainty-aware refinement network for event-based stereo depth estimation that uses a local-global refinement module and KL-divergence-based uncertainty modeling to achieve state-of-the-art results on DSEC.


<details>
  <summary>Details</summary>
Motivation: Event cameras offer high temporal resolution and dynamic range but robust, reliable depth estimation remains challenging; there's a need to integrate fine-grained local details with long-range global context and quantify prediction uncertainty to improve reliability.

Method: Develop URNet with a local-global refinement module to capture local details and global context; incorporate KL-divergence-based uncertainty modeling; train and evaluate on the DSEC dataset; comparative experiments against SOTA.

Result: URNet consistently outperforms state-of-the-art methods on DSEC in both qualitative and quantitative evaluations.

Conclusion: Uncertainty-aware refinement enhances reliability and accuracy for event-based stereo depth estimation; integrating local-global refinement with KL-based uncertainty modeling is a promising direction for robust event-based perception.

Abstract: Event cameras provide high temporal resolution, high dynamic range, and low
latency, offering significant advantages over conventional frame-based cameras.
In this work, we introduce an uncertainty-aware refinement network called URNet
for event-based stereo depth estimation. Our approach features a local-global
refinement module that effectively captures fine-grained local details and
long-range global context. Additionally, we introduce a Kullback-Leibler (KL)
divergence-based uncertainty modeling method to enhance prediction reliability.
Extensive experiments on the DSEC dataset demonstrate that URNet consistently
outperforms state-of-the-art (SOTA) methods in both qualitative and
quantitative evaluations.

</details>


### [12] [Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases](https://arxiv.org/abs/2509.18185)
*Giammarco La Barbera,Enzo Bonnot,Thomas Isla,Juan Pablo de la Plata,Joy-Rose Dunoyer de Segonzac,Jennifer Attali,Cécile Lozach,Alexandre Bellucci,Louis Marcellin,Laure Fournier,Sabine Sarnacki,Pietro Gori,Isabelle Bloch*

Main category: cs.CV

TL;DR: Hybrid AI Visionerves framework integrates deep-learning segmentation with symbolic spatial reasoning for peripheral nerve tractography, achieving improved accuracy in endometriosis-related nerve imaging without manual ROIs.


<details>
  <summary>Details</summary>
Motivation: Imaging peripheral nerves in endometriosis is challenging; conventional tractography relies on manual ROIs and often lacks anatomical encoding; need automated, reproducible, non-invasive neuropathy diagnosis.

Method: Two-phase pipeline: (A) automatic segmentation of anatomical structures via deep learning; (B) tractography and nerve recognition using symbolic spatial reasoning with fuzzy spatial relationships, leveraging multi-gradient DWI and MRI morphology; removes manual ROI selection.

Result: Applied to lumbosacral plexus in 10 women; improves Dice score by up to 25% vs standard tractography; spatial errors reduced to <5 mm;

Conclusion: The approach enables automatic, detailed nerve analysis and could enable non-invasive diagnosis of endometriosis-related neuropathy and other nerve-involved conditions.

Abstract: Endometriosis often leads to chronic pelvic pain and possible nerve
involvement, yet imaging the peripheral nerves remains a challenge. We
introduce Visionerves, a novel hybrid AI framework for peripheral nervous
system recognition from multi-gradient DWI and morphological MRI data. Unlike
conventional tractography, Visionerves encodes anatomical knowledge through
fuzzy spatial relationships, removing the need for selection of manual ROIs.
The pipeline comprises two phases: (A) automatic segmentation of anatomical
structures using a deep learning model, and (B) tractography and nerve
recognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in
10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated
substantial improvements over standard tractography, with Dice score
improvements of up to 25% and spatial errors reduced to less than 5 mm. This
automatic and reproducible approach enables detailed nerve analysis and paves
the way for non-invasive diagnosis of endometriosis-related neuropathy, as well
as other conditions with nerve involvement.

</details>


### [13] [V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling](https://arxiv.org/abs/2509.18187)
*Muhammad Naveed,Nazia Perwaiz,Sidra Sultana,Mohaira Ahmad,Muhammad Moazam Fraz*

Main category: cs.CV

TL;DR: Privacy-preserving, multimodal driver-behavior dataset V-SenseDrive collected in Pakistan using smartphone sensors and road-facing video, addressing data privacy and representation gaps for ADAS and safety research.


<details>
  <summary>Details</summary>
Motivation: Need diverse, privacy-compliant driver-behavior data in emerging economies to improve road safety, calibrate ADAS, and enable data-driven decisions; existing datasets are biased toward developed countries and raise privacy concerns.

Method: Android app collects high-frequency inertial (accelerometer, gyroscope) and GPS data, synchronized with continuous road-facing video; precise time alignment; three behaviours (normal, aggressive, risky); datasets on urban, secondary, motorways; data structured into raw/processed/semantic layers; privacy-preserving collection.

Result: First privacy-preserving multimodal driver-behavior dataset in Pakistan; V-SenseDrive; data acquisition framework, synchronization, and layered dataset structure enabling multimodal analysis for driver behavior classification, safety research, and ADAS development.

Conclusion: Addresses representation gaps and privacy concerns, enabling context-aware ITS research in Pakistan and contributing to global driver behavior datasets; supports more accurate, locally relevant safety tools and policies.

Abstract: Road traffic accidents remain a major public health challenge, particularly
in countries with heterogeneous road conditions, mixed traffic flow, and
variable driving discipline, such as Pakistan. Reliable detection of unsafe
driving behaviours is a prerequisite for improving road safety, enabling
advanced driver assistance systems (ADAS), and supporting data driven decisions
in insurance and fleet management. Most of existing datasets originate from the
developed countries with limited representation of the behavioural diversity
observed in emerging economies and the driver's face recording voilates the
privacy preservation. We present V-SenseDrive, the first privacy-preserving
multimodal driver behaviour dataset collected entirely within the Pakistani
driving environment. V-SenseDrive combines smartphone based inertial and GPS
sensor data with synchronized road facing video to record three target driving
behaviours (normal, aggressive, and risky) on multiple types of roads,
including urban arterials, secondary roads, and motorways. Data was gathered
using a custom Android application designed to capture high frequency
accelerometer, gyroscope, and GPS streams alongside continuous video, with all
sources precisely time aligned to enable multimodal analysis. The focus of this
work is on the data acquisition process, covering participant selection,
driving scenarios, environmental considerations, and sensor video
synchronization techniques. The dataset is structured into raw, processed, and
semantic layers, ensuring adaptability for future research in driver behaviour
classification, traffic safety analysis, and ADAS development. By representing
real world driving in Pakistan, V-SenseDrive fills a critical gap in the global
landscape of driver behaviour datasets and lays the groundwork for context
aware intelligent transportation solutions.

</details>


### [14] [Qianfan-VL: Domain-Enhanced Universal Vision-Language Models](https://arxiv.org/abs/2509.18189)
*Daxiang Dong,Mingming Zheng,Dong Xu,Bairong Zhuang,Wenyu Zhang,Chunhua Luo,Haoran Wang,Zijian Zhao,Jie Li,Yuxuan Li,Hanjun Zhong,Mengyue Liu,Jieting Chen,Shupeng Li,Lun Tian,Yaping Feng,Xin Li,Donggang Jiang,Yong Chen,Yehua Xu,Duohao Qin,Chen Feng,Dan Wang,Henghua Zhang,Jingjing Ha,Jinhui He,Yanfeng Zhai,Chengxin Zheng,Jiayi Mao,Jiacheng Chen,Ruchang Yao,Ziye Yuan,Jianmin Wu,Guangjun Xie,Dou Shen*

Main category: cs.CV

TL;DR: Qianfan-VL is a family of 3B–70B multimodal LLMs with domain-enhanced training, achieving SOTA on domain benchmarks and competitive general performance, especially strong in OCR, document understanding, and reasoning, trained on Baidu's Kunlun hardware.


<details>
  <summary>Details</summary>
Motivation: To enable domain-specific multimodal capabilities without sacrificing general performance, enabling enterprise deployment with scalable training on massive hardware.

Method: Multi-stage progressive training, high-precision data synthesis pipelines, domain-enhancement strategies, and long-chain-of-thought capabilities in larger variants, trained on Kunlun P800 chips with high scaling efficiency across thousands of chips.

Result: Comparable to top open-source models on general benchmarks; state-of-the-art on CCBench, SEEDBench IMG, ScienceQA, MMStar; strong OCR/DocVQA results (OCRBench 873, DocVQA 94.75%); MathVista 78.6% with long chain-of-thought; 90% scaling efficiency on 5000 chips.

Conclusion: Demonstrates a practical methodology for domain-enhanced multimodal models suitable for diverse enterprise deployments, validating scalable, domain-focused AI infrastructure for SOTA performance.

Abstract: We present Qianfan-VL, a series of multimodal large language models ranging
from 3B to 70B parameters, achieving state-of-the-art performance through
innovative domain enhancement techniques. Our approach employs multi-stage
progressive training and high-precision data synthesis pipelines, which prove
to be critical technologies for enhancing domain-specific capabilities while
maintaining strong general performance. Qianfan-VL achieves comparable results
to leading open-source models on general benchmarks, with state-of-the-art
performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and
MMStar. The domain enhancement strategy delivers significant advantages in OCR
and document understanding, validated on both public benchmarks (OCRBench 873,
DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B
variants incorporate long chain-of-thought capabilities, demonstrating superior
performance on mathematical reasoning (MathVista 78.6%) and logical inference
tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating
the capability of large-scale AI infrastructure to train SOTA-level multimodal
models with over 90% scaling efficiency on 5000 chips for a single task. This
work establishes an effective methodology for developing domain-enhanced
multimodal models suitable for diverse enterprise deployment scenarios.

</details>


### [15] [HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing](https://arxiv.org/abs/2509.18190)
*Junseong Shin,Seungwoo Chung,Yunjeong Yang,Tae Hyun Kim*

Main category: cs.CV

TL;DR: HazeFlow reframes the atmospheric scattering model as an ODE to map hazy images to clean ones via an optimal trajectory, inspired by Rectified Flow, and uses Markov Chain Brownian Motion to synthesize realistic non-homogeneous haze for data augmentation, achieving state-of-the-art results on real-world dehazing benchmarks with a single inference step.


<details>
  <summary>Details</summary>
Motivation: Dehazing suffers from a lack of paired real-world training data and domain gaps; ASM-based methods often fail to capture real-world haze complexity. A physics-grounded approach with robust data generation is needed to improve real-world generalization.

Method: Reformulate the Atmospheric Scattering Model as an ordinary differential equation and learn an optimal trajectory from hazy to clean images, enabling one-inference-step dehazing. Introduce a non-homogeneous haze generation method using Markov Chain Brownian Motion (MCBM) to synthesize diverse, realistic haze patterns for training.

Result: Extensive experiments show that HazeFlow achieves state-of-the-art performance across various real-world dehazing benchmark datasets.

Conclusion: HazeFlow demonstrates that combining a physics-grounded ODE formulation with realistic haze data generation improves generalization to real-world scenarios, delivering efficient, high-quality dehazing in a single inference step.

Abstract: Dehazing involves removing haze or fog from images to restore clarity and
improve visibility by estimating atmospheric scattering effects. While deep
learning methods show promise, the lack of paired real-world training data and
the resulting domain gap hinder generalization to real-world scenarios. In this
context, physics-grounded learning becomes crucial; however, traditional
methods based on the Atmospheric Scattering Model (ASM) often fall short in
handling real-world complexities and diverse haze patterns. To solve this
problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM
as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),
HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,
enhancing real-world dehazing performance with only a single inference step.
Additionally, we introduce a non-homogeneous haze generation method using
Markov Chain Brownian Motion (MCBM) to address the scarcity of paired
real-world data. By simulating realistic haze patterns through MCBM, we enhance
the adaptability of HazeFlow to diverse real-world scenarios. Through extensive
experiments, we demonstrate that HazeFlow achieves state-of-the-art performance
across various real-world dehazing benchmark datasets.

</details>


### [16] [TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection](https://arxiv.org/abs/2509.18193)
*Omar H. Khater,Abdul Jabbar Siddiqui,Aiman El-Maleh,M. Shamim Hossain*

Main category: cs.CV

TL;DR: A compressed EcoWeedNet with structured pruning, QAT, and TensorRT on Jetson Orin Nano achieves substantial size and compute reductions while preserving weed detection accuracy on CottonWeedDet12, outperforming lightweight YOLO baselines.


<details>
  <summary>Details</summary>
Motivation: Edge deployments of deep learning for agriculture face resource limits; need efficient, fast, accurate weed detection on low-power devices.

Method: Combine structured channel pruning, quantization-aware training (QAT), and NVIDIA TensorRT acceleration on Jetson Orin Nano. Address pruning of residual shortcuts, attention modules, concatenations, and CSP blocks; report 68.5% model size reduction, 3.2 GFLOPs reduction, 184 FPS FP16 (28.7% faster than baseline); 39.5% pruning ratio on CottonWeedDet12; compare with YOLO11n/YOLO12n.

Result: On CottonWeedDet12, pruned EcoWeedNet achieves 83.7% precision, 77.5% recall, 85.9% mAP50, surpassing YOLO11n and YOLO12n with only 20% pruning; demonstrates both efficiency and effectiveness for precision agriculture.

Conclusion: Structured pruning plus QAT and TensorRT can yield large reductions in size and compute with minimal accuracy loss on edge devices for weed detection; promising for deploying DL models in resource-constrained ag settings, though generalization to other architectures/datasets and the complexity of pruning advanced modules may pose challenges.

Abstract: Deploying deep learning models in agriculture is difficult because edge
devices have limited resources, but this work presents a compressed version of
EcoWeedNet using structured channel pruning, quantization-aware training (QAT),
and acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the
challenges of pruning complex architectures with residual shortcuts, attention
mechanisms, concatenations, and CSP blocks, the model size was reduced by up to
68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at
FP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the
pruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n
(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%
mAP50, proving it to be both efficient and effective for precision agriculture.

</details>


### [17] [Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction](https://arxiv.org/abs/2509.18284)
*Yi Gu,Kuniaki Saito,Jiaxin Ma*

Main category: cs.CV

TL;DR: A multimodal learning framework with enhanced modalities dropout and contrastive learning handles missing modalities and modality imbalance, using learnable modality tokens for robust fusion; achieves state-of-the-art results on clinical visual/tabular data, especially when only a single modality is available, and integrates with a CT foundation model. Code is released.


<details>
  <summary>Details</summary>
Motivation: In medical settings, data from multiple modalities are common but often incomplete or imbalanced. Robust fusion that can leverage available modalities while gracefully handling missing ones is crucial for reliable disease detection and prediction. Existing methods struggle under missingness and modality heterogeneity, limiting real-world applicability.

Method: Introduce learnable modality tokens and enhanced modalities dropout to enable missingness-aware fusion of heterogeneous modalities. Augment unimodal contrastive objectives with fused multimodal representations to align and discriminate across modalities. Validate on large-scale clinical datasets spanning visual and tabular data, and demonstrate compatibility with a recent CT foundation model.

Result: Achieves state-of-the-art performance, particularly in challenging scenarios where only a single modality is available. Demonstrates strong robustness to missing modalities and modality imbalance. Shows adaptability by integrating with a CT foundation model. Code released for reproducibility.

Conclusion: The proposed framework offers a scalable, efficient, and generalizable approach for multimodal learning in clinical applications, effectively leveraging available modalities while gracefully handling missingness and heterogeneity.

Abstract: As medical diagnoses increasingly leverage multimodal data, machine learning
models are expected to effectively fuse heterogeneous information while
remaining robust to missing modalities. In this work, we propose a novel
multimodal learning framework that integrates enhanced modalities dropout and
contrastive learning to address real-world limitations such as modality
imbalance and missingness. Our approach introduces learnable modality tokens
for improving missingness-aware fusion of modalities and augments conventional
unimodal contrastive objectives with fused multimodal representations. We
validate our framework on large-scale clinical datasets for disease detection
and prediction tasks, encompassing both visual and tabular modalities.
Experimental results demonstrate that our method achieves state-of-the-art
performance, particularly in challenging and practical scenarios where only a
single modality is available. Furthermore, we show its adaptability through
successful integration with a recent CT foundation model. Our findings
highlight the effectiveness, efficiency, and generalizability of our approach
for multimodal learning, offering a scalable, low-cost solution with
significant potential for real-world clinical applications. The code is
available at https://github.com/omron-sinicx/medical-modality-dropout.

</details>


### [18] [Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model](https://arxiv.org/abs/2509.18308)
*Yixin Zhang,Ryan Chamberlain,Lawrance Ngo,Kevin Kramer,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: Benchmark of 9 segmentation architectures (CNN and ViT) on a densely annotated 490-scan CTPA dataset for pulmonary embolism, finding 3D CNNs (notably 3D U-Net with a ResNet encoder) outperform ViTs, pretraining on classification can hurt segmentation, and distal emboli remain challenging; best Dice ~0.713 with 181 emboli detected (49 FP, 28 FN) from 60 scans, with results generalizing to public data.


<details>
  <summary>Details</summary>
Motivation: Pulmonary embolism segmentation in CTPA scans is clinically valuable but challenging due to emboli morphology and data scarcity. The study systematically compares mainstream segmentation architectures (CNN vs ViT) under a unified framework, investigates the impact of pretraining, and assesses generalization to public datasets to identify robust approaches and guide future model development.

Method: Create an in-house, densely annotated dataset of 490 CTPA scans. Evaluate nine segmentation architectures spanning CNN and Vision Transformer families, initialized with either pretrained or random weights, under a unified testing framework. Compare 3D vs 2D models, analyze impact of classification-based pretraining vs training from scratch, and assess segmentation performance, particularly on central/large vs distal emboli. Validate findings on public datasets.

Result: Key findings: (1) 3D U-Net with a ResNet encoder is highly effective for PE segmentation. (2) 3D models suit embolus morphology well. (3) CNN-based models generally outperform ViTs in PE segmentation. (4) Classification-based pretraining can harm segmentation performance, suggesting different discriminative features for classification vs segmentation. (5) Model architecture performance patterns are consistent when trained on the same data. (6) Distal emboli are challenging due to task complexity and data scarcity. Best model achieves mean Dice of 0.7131; detects 181 emboli with 49 false positives and 28 false negatives on 60 in-house testing scans; generalizes to public datasets.

Conclusion: For PE segmentation in CTPA, 3D CNNs, particularly 3D U-Net with a ResNet encoder, are preferred. Training from scratch may outperform pretraining on classification. There remains a data gap for distal emboli; expanding high-quality annotations and diverse datasets is crucial for improving generalization and clinical utility.

Abstract: In this study, we curated a densely annotated in-house dataset comprising 490
CTPA scans. Using this dataset, we systematically evaluated nine widely used
segmentation architectures from both the CNN and Vision Transformer (ViT)
families, initialized with either pretrained or random weights, under a unified
testing framework as a performance audit. Our study leads to several important
observations: (1) 3D U-Net with a ResNet encoder remains a highly effective
architecture for PE segmentation; (2) 3D models are particularly well-suited to
this task given the morphological characteristics of emboli; (3) CNN-based
models generally yield superior performance compared to their ViT-based
counterparts in PE segmentation; (4) classification-based pretraining, even on
large PE datasets, can adversely impact segmentation performance compared to
training from scratch, suggesting that PE classification and segmentation may
rely on different sets of discriminative features; (5) different model
architectures show a highly consistent pattern of segmentation performance when
trained on the same data; and (6) while central and large emboli can be
segmented with satisfactory accuracy, distal emboli remain challenging due to
both task complexity and the scarcity of high-quality datasets. Besides these
findings, our best-performing model achieves a mean Dice score of 0.7131 for
segmentation. It detects 181 emboli with 49 false positives and 28 false
negatives from 60 in-house testing scans. Its generalizability is further
validated on public datasets.

</details>


### [19] [Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach](https://arxiv.org/abs/2509.18309)
*Alessa Carbo,Eric Nalisnick*

Main category: cs.CV

TL;DR: A novel graph neural network separates temporal dynamics from static handshape configurations to improve sign language handshape recognition, achieving 46% accuracy across 37 classes versus ~25% for baselines.


<details>
  <summary>Details</summary>
Motivation: Handshapes are central to sign language phonology, but computational models seldom model them explicitly, limiting recognition accuracy and linguistic analysis.

Method: An anatomically-informed graph structure captures static handshape configurations while separate temporal dynamics are modeled; contrastive learning is used to handle subtle interclass differences and temporal variation; this creates a new benchmark for structured handshape recognition in signing sequences.

Result: First benchmark for structured handshape recognition in signing sequences; 46% accuracy across 37 handshape classes, outperforming baselines at ~25%.

Conclusion: Demonstrates the value of explicit handshape modeling using graph-based representations and contrastive learning; provides a baseline benchmark and a scalable framework for improved sign language understanding.

Abstract: Handshapes serve a fundamental phonological role in signed languages, with
American Sign Language employing approximately 50 distinct shapes.
However,computational approaches rarely model handshapes explicitly, limiting
both recognition accuracy and linguistic analysis.We introduce a novel graph
neural network that separates temporal dynamics from static handshape
configurations. Our approach combines anatomically-informed graph structures
with contrastive learning to address key challenges in handshape recognition,
including subtle interclass distinctions and temporal variations. We establish
the first benchmark for structured handshape recognition in signing sequences,
achieving 46% accuracy across 37 handshape classes (with baseline methods
achieving 25%).

</details>


### [20] [Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound](https://arxiv.org/abs/2509.18326)
*Chun Kit Wong,Anders N. Christensen,Cosmin I. Bercea,Julia A. Schnabel,Martin G. Tolsgaard,Aasa Feragen*

Main category: cs.CV

TL;DR: OOD detection in fetal ultrasound depends on the chosen classification task; uncertainty quantification methods vary in effectiveness across tasks, and the best task depends on whether OOD stems from image characteristic shifts or anatomical feature shifts. Strong OOD detection does not guarantee effective abstention, so task and uncertainty strategy must be aligned with the downstream medical application.


<details>
  <summary>Details</summary>
Motivation: To ensure safe deployment of deep learning in fetal ultrasound, where image characteristics and clinical settings vary, by investigating how the choice of classification task affects OOD detection performance beyond standard uncertainty estimation.

Method: Empirical evaluation using eight uncertainty quantification (UQ) methods across four classification tasks, analyzing OOD detection performance under two ID-OOD criteria: shifts in image characteristics and shifts in anatomical features.

Result: OOD detection performance varies significantly with the chosen task, and the optimal task depends on the specific ID-OOD criterion. Moreover, superior OOD detection does not necessarily translate into better abstention (refraining from making a prediction), highlighting the need to align task selection and uncertainty strategies with the downstream medical application.

Conclusion: Researchers should carefully select the classification task in tandem with the uncertainty estimation strategy based on the downstream use case in medical image analysis, particularly when distinguishing between image-characteristic shifts and anatomical-feature shifts in OOD settings.

Abstract: Reliable out-of-distribution (OOD) detection is important for safe deployment
of deep learning models in fetal ultrasound amidst heterogeneous image
characteristics and clinical settings. OOD detection relies on estimating a
classification model's uncertainty, which should increase for OOD samples.
While existing research has largely focused on uncertainty quantification
methods, this work investigates the impact of the classification task itself.
Through experiments with eight uncertainty quantification methods across four
classification tasks, we demonstrate that OOD detection performance
significantly varies with the task, and that the best task depends on the
defined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an
image characteristic shift or ii) an anatomical feature shift. Furthermore, we
reveal that superior OOD detection does not guarantee optimal abstained
prediction, underscoring the necessity to align task selection and uncertainty
strategies with the specific downstream application in medical image analysis.

</details>


### [21] [OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata](https://arxiv.org/abs/2509.18350)
*Oussema Dhaouadi,Riccardo Marin,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: OrthoLoC provides a large-scale orthophoto-based localization dataset and AdHoP refinement, enabling offline, resource-efficient localization with domain-shift analysis and substantial gains in matching accuracy and pose error.


<details>
  <summary>Details</summary>
Motivation: The problem is accurate, high-precision visual localization under resource constraints (no internet or GNSS/GPS) with limited storage; existing methods rely on heavy 3D maps or image retrieval pipelines without exploiting lightweight orthophotos, limiting offline applicability and robustness to domain shifts.

Method: Introduce OrthoLoC, a dataset with 16,425 UAV images from Germany and the United States across multiple modalities, designed to bridge UAV imagery and geospatial orthophotos. The dataset allows decoupled benchmarking by separating image retrieval from feature matching and evaluating localization/calibration under domain shifts, resolutions, and covisibility. Propose AdHoP, a refinement technique that can be integrated with any feature matcher to improve correspondences and pose estimates.

Result: OrthoLoC enables fair benchmarking of localization pipelines using orthophotos, demonstrates the impact of domain shift, resolution, and covisibility on accuracy, and shows that AdHoP can boost matching performance by up to 95% and reduce translation error by up to 63%. The dataset and code are publicly released for the community.

Conclusion: The work fills a gap in offline, lightweight localization by leveraging orthophotographs as a practical data source, and provides a general refinement approach (AdHoP) that can be paired with existing matchers to yield substantial performance gains, supporting robust large-scale localization in mapping, inspection, and search-and-rescue tasks.

Abstract: Accurate visual localization from aerial views is a fundamental problem with
applications in mapping, large-area inspection, and search-and-rescue
operations. In many scenarios, these systems require high-precision
localization while operating with limited resources (e.g., no internet
connection or GNSS/GPS support), making large image databases or heavy 3D
models impractical. Surprisingly, little attention has been given to leveraging
orthographic geodata as an alternative paradigm, which is lightweight and
increasingly available through free releases by governmental authorities (e.g.,
the European Union). To fill this gap, we propose OrthoLoC, the first
large-scale dataset comprising 16,425 UAV images from Germany and the United
States with multiple modalities. The dataset addresses domain shifts between
UAV imagery and geospatial data. Its paired structure enables fair benchmarking
of existing solutions by decoupling image retrieval from feature matching,
allowing isolated evaluation of localization and calibration performance.
Through comprehensive evaluation, we examine the impact of domain shifts, data
resolutions, and covisibility on localization accuracy. Finally, we introduce a
refinement technique called AdHoP, which can be integrated with any feature
matcher, improving matching by up to 95% and reducing translation error by up
to 63%. The dataset and code are available at:
https://deepscenario.github.io/OrthoLoC.

</details>


### [22] [A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data](https://arxiv.org/abs/2509.18354)
*Mehrdad Moradi,Shengzhe Chen,Hao Yan,Kamran Paynabar*

Main category: cs.CV

TL;DR: A zero-shot single-image anomaly localization method (SSDnet) that leverages a CNN prior inspired by Deep Image Prior to reconstruct a test image with patch-based training, masking, shuffling, and noise to localize anomalies without external data, achieving strong AUROC/AUPRC on standard datasets.


<details>
  <summary>Details</summary>
Motivation: In many real-world scenarios, training data or references are unavailable. The authors exploit the inductive biases of CNNs and the Deep Image Prior to enable anomaly localization from a single image by assuming natural images have unified textures and that anomalies are localized deviations.

Method: Patch-based self-reconstruction: feed the input image into a network to learn a prior for reconstruction rather than mapping noise to an image. To avoid trivial identity mapping, apply masking, patch shuffling, and small Gaussian noise. Use a perceptual loss based on inner-product similarity to capture structure beyond pixel fidelity. No external training data or labels are used. Robust to noise and missing pixels.

Result: SSDnet reports 0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the fabric dataset, outperforming state-of-the-art methods.

Conclusion: Demonstrates effective zero-shot anomaly localization using a DIP-inspired prior without training data, robust to common imperfections, and with competitive or superior performance on standard benchmarks.

Abstract: Anomaly detection in images is typically addressed by learning from
collections of training data or relying on reference samples. In many
real-world scenarios, however, such training data may be unavailable, and only
the test image itself is provided. We address this zero-shot setting by
proposing a single-image anomaly localization method that leverages the
inductive bias of convolutional neural networks, inspired by Deep Image Prior
(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key
assumption is that natural images often exhibit unified textures and patterns,
and that anomalies manifest as localized deviations from these repetitive or
stochastic patterns. To learn the deep image prior, we design a patch-based
training framework where the input image is fed directly into the network for
self-reconstruction, rather than mapping random noise to the image as done in
DIP. To avoid the model simply learning an identity mapping, we apply masking,
patch shuffling, and small Gaussian noise. In addition, we use a perceptual
loss based on inner-product similarity to capture structure beyond pixel
fidelity. Our approach needs no external training data, labels, or references,
and remains robust in the presence of noise or missing pixels. SSDnet achieves
0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the
fabric dataset, outperforming state-of-the-art methods. The implementation code
will be released at https://github.com/mehrdadmoradi124/SSDnet

</details>


### [23] [Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning](https://arxiv.org/abs/2509.18369)
*Riad Ahmed Anonto,Sardar Md. Saffat Zabin,M. Saifur Rahman*

Main category: cs.CV

TL;DR: A compute-aware Bengali captioning pipeline using PAL, InfoNCE, and Sinkhorn OT to align ground truth and synthetic patches, trained on LaBSE-verified EN-BN data and 110k bilingual-prompted images, yielding notable gains in grounding for Bengali captioning on Flickr30k-1k and MSCOCO-1k.


<details>
  <summary>Details</summary>
Motivation: Grounding vision–language models in low-resource languages like Bengali is hard due to scarce aligned data, translation pivots that disrupt alignment, and English-centric pretraining that misses target-language semantics. This work aims to build a compute-aware, cross-lingual grounding solution that directly aligns real and synthetic patches while leveraging bilingual supervision.

Method: The approach freezes a MaxViT for stable visual patches, uses Bengali mbart-50 for decoding, and builds a lightweight bridge between modalities. It trains on LaBSE-verified EN–BN pairs and 110k bilingual-prompted synthetic images. The core is a tri-loss: Patch-Alignment Loss (PAL) aligns real and synthetic patch descriptors via decoder cross-attention; InfoNCE enforces global real–synthetic separation; Sinkhorn-based OT enforces balanced, fine-grained patch-level correspondences.

Result: Quantitative gains on Flickr30k-1k (BLEU-4 12.29, METEOR 27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14, BERTScore-F1 75.40). Outperforms strong CE baselines and narrows the real–synthetic centroid gap by 41%.

Conclusion: The PAL+InfoNCE+OT tri-loss synergy improves grounding and reduces spurious matches in Bengali captioning, demonstrating effective cross-lingual grounding with synthetic augmentation and patch-level alignment.

Abstract: Grounding vision--language models in low-resource languages remains
challenging, as they often produce fluent text about the wrong objects. This
stems from scarce paired data, translation pivots that break alignment, and
English-centric pretraining that ignores target-language semantics. We address
this with a compute-aware Bengali captioning pipeline trained on LaBSE-verified
EN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT
yields stable visual patches, a Bengali-native mBART-50 decodes, and a
lightweight bridge links the modalities. Our core novelty is a tri-loss
objective: Patch-Alignment Loss (PAL) aligns real and synthetic patch
descriptors using decoder cross-attention, InfoNCE enforces global
real--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained
patch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces
spurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR
27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,
BERTScore-F1 75.40), outperforming strong CE baselines and narrowing the
real--synthetic centroid gap by 41%.

</details>


### [24] [TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning](https://arxiv.org/abs/2509.18372)
*Reeshad Khan,John Gauch*

Main category: cs.CV

TL;DR: TinyBEV is a camera-only BEV framework that distills UniAD's full-stack capabilities into a lightweight 28M-parameter model for real-time, camera-based autonomous driving, achieving near full-stack performance with substantially reduced computation.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for resource-efficient, real-time, camera-only autonomous systems that retain full-stack capabilities (3D detection, HD-map segmentation, motion forecasting, occupancy prediction, and planning) by transferring knowledge from a large teacher to a compact student.

Method: Multi-stage distillation (feature-level, output-level, adaptive region-aware supervision) from UniAD into a BEV representation; model-agnostic; 28M backbone; supports 3D detection, HD-map segmentation, motion forecasting, occupancy, and planning; evaluated on nuScenes.

Result: Achieves 39.0 mAP for detection, 1.08 minADE for motion forecasting, 0.32 collision rate; runs at 11 FPS (5x faster than prior baselines) using only camera input; 78% parameter reduction vs UniAD.

Conclusion: Full-stack driving intelligence can be retained in resource-constrained camera-only systems, bridging the gap between large multi-modal models and deployment-ready real-time autonomy.

Abstract: We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework
that distills the full-stack capabilities of a large planning-oriented teacher
(UniAD [19]) into a compact, real-time student model. Unlike prior efficient
camera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the
complete autonomy stack 3D detection, HD-map segmentation, motion forecasting,
occupancy prediction, and goal-directed planning within a streamlined
28M-parameter backbone, achieving a 78% reduction in parameters over UniAD
[19]. Our model-agnostic, multi-stage distillation strategy combines
feature-level, output-level, and adaptive region-aware supervision to
effectively transfer high-capacity multi-modal knowledge to a lightweight BEV
representation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08
minADE for motion forecasting, and a 0.32 collision rate, while running 5x
faster (11 FPS) and requiring only camera input. These results demonstrate that
full-stack driving intelligence can be retained in resource-constrained
settings, bridging the gap between large-scale, multi-modal perception-planning
models and deployment-ready real-time autonomy.

</details>


### [25] [BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking](https://arxiv.org/abs/2509.18387)
*Thomas Gossard,Filip Radovic,Andreas Ziegler,Andrea Zell*

Main category: cs.CV

TL;DR: Proposes a motion-blur-aware labeling scheme for fast-ball detection in racket sports, releasing a center-of-blur label and blur attributes dataset; introduces BlurBall for joint ball-position and blur-attribute estimation with attention over multi-frame inputs, achieving state-of-the-art detection and better trajectory prediction.


<details>
  <summary>Details</summary>
Motivation: Motion blur and traditional leading-edge labeling obscure motion cues and introduce asymmetry, hindering reliable ball detection and trajectory estimation in fast sports like table tennis.

Method: 1) Replace traditional labeling with placing the ball at the center of the blur streak and annotate blur attributes. 2) Release a new table tennis ball detection dataset built with this convention. 3) Propose BlurBall, a model that jointly estimates ball position and blur attributes, incorporating attention mechanisms (e.g., Squeeze-and-Excitation) over multi-frame inputs to leverage temporal cues.

Result: The labeling convention consistently improves detection across various models. BlurBall with attention achieves state-of-the-art ball detection performance and yields more reliable trajectory predictions due to explicit blur cues.

Conclusion: Center-of-blur labeling plus blur-attribute modeling enhances detection accuracy and trajectory analytics in real-time sports applications, demonstrating the value of exploiting motion blur rather than suppressing it.

Abstract: Motion blur reduces the clarity of fast-moving objects, posing challenges for
detection systems, especially in racket sports, where balls often appear as
streaks rather than distinct points. Existing labeling conventions mark the
ball at the leading edge of the blur, introducing asymmetry and ignoring
valuable motion cues correlated with velocity. This paper introduces a new
labeling strategy that places the ball at the center of the blur streak and
explicitly annotates blur attributes. Using this convention, we release a new
table tennis ball detection dataset. We demonstrate that this labeling approach
consistently enhances detection performance across various models. Furthermore,
we introduce BlurBall, a model that jointly estimates ball position and motion
blur attributes. By incorporating attention mechanisms such as
Squeeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art
results in ball detection. Leveraging blur not only improves detection accuracy
but also enables more reliable trajectory prediction, benefiting real-time
sports analytics.

</details>


### [26] [MVP: Motion Vector Propagation for Zero-Shot Video Object Detection](https://arxiv.org/abs/2509.18388)
*Binhua Huang,Ni Wang,Wendong Yao,Soumyabrata Dev*

Main category: cs.CV

TL;DR: A training-free MVP pipeline reduces detector runs by querying Open-vocabulary detectors only on keyframes and propagating through video using compressed motion vectors, achieving strong zero-shot localization with label-free operation, and outperforming classic tracker methods while approaching framewise performance.


<details>
  <summary>Details</summary>
Motivation: Open-vocabulary detectors are accurate but expensive when run on every frame; need a label-free, training-free method to maintain coverage while reducing cost.

Method: Run OVLv2 on fixed-interval keyframes; propagate detections to intermediate frames using compressed-domain motion vectors via a 3x3 grid; apply translation/scale updates, area-growth check, optional single-class switch; no labels or fine-tuning; same prompt list for all methods.

Result: On ILSVRC2015-VID validation: mAP@0.5 = 0.609; mAP@[0.5:0.95] = 0.316; near framewise OVLv2-Large at loose IoU; outperforms MOSSE/KCF/CSRT trackers at mAP@0.5; supervised reference YOLOv12x reaches 0.631 at mAP@0.5 but requires labeled training; MVP is label-free and open-vocabulary; code available.

Conclusion: Compressed-domain propagation provides a practical, zero-shot, label-free way to reduce detector invocations in videos while preserving strong localization and open-vocabulary coverage.

Abstract: Running a large open-vocabulary (Open-vocab) detector on every video frame is
accurate but expensive. We introduce a training-free pipeline that invokes
OWLv2 only on fixed-interval keyframes and propagates detections to
intermediate frames using compressed-domain motion vectors (MV). A simple 3x3
grid aggregation of motion vectors provides translation and uniform-scale
updates, augmented with an area-growth check and an optional single-class
switch. The method requires no labels, no fine-tuning, and uses the same prompt
list for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),
our approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose
intersection-over-union (IoU) thresholds it remains close to framewise
OWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse
localization is largely preserved. Under the same keyframe schedule, MVP
outperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A
supervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled
training, whereas our method remains label-free and open-vocabulary. These
results indicate that compressed-domain propagation is a practical way to
reduce detector invocations while keeping strong zero-shot coverage in videos.
Our code and models are available at https://github.com/microa/MVP.

</details>


### [27] [Improving the color accuracy of lighting estimation models](https://arxiv.org/abs/2509.18390)
*Zitian Zhang,Joshua Urban Davis,Jeanne Phuong Anh Vu,Jiangtao Kuang,Jean-François Lalonde*

Main category: cs.CV

TL;DR: A pre-trained white balance network used as a preprocessing step improves color robustness in HDR lighting estimation from a single image without retraining, validated across three methods on a color-diverse HDR dataset.


<details>
  <summary>Details</summary>
Motivation: Color realism in AR requires accurate color handling; existing evaluations often conflate color with lighting attributes. Isolating color and testing lightweight adaptation can improve color fidelity without changing estimation models.

Method: Evaluate several adaptation strategies using a novel HDR dataset with diverse lighting colors; preprocess inputs with a pre-trained white balance network; apply to three state-of-the-art lighting estimation methods; no retraining.

Result: White balance preprocessing improved color robustness in all tested scenarios and outperformed other adaptation strategies, generalizing across the evaluated methods.

Conclusion: Simple input-space adaptation via white balance preprocessing can enhance color accuracy for HDR lighting estimation without retraining models, suggesting broader applicability to improve AR realism.

Abstract: Advances in high dynamic range (HDR) lighting estimation from a single image
have opened new possibilities for augmented reality (AR) applications.
Predicting complex lighting environments from a single input image allows for
the realistic rendering and compositing of virtual objects. In this work, we
investigate the color robustness of such methods -- an often overlooked yet
critical factor for achieving visual realism. While most evaluations conflate
color with other lighting attributes (e.g., intensity, direction), we isolate
color as the primary variable of interest. Rather than introducing a new
lighting estimation algorithm, we explore whether simple adaptation techniques
can enhance the color accuracy of existing models. Using a novel HDR dataset
featuring diverse lighting colors, we systematically evaluate several
adaptation strategies. Our results show that preprocessing the input image with
a pre-trained white balance network improves color robustness, outperforming
other strategies across all tested scenarios. Notably, this approach requires
no retraining of the lighting estimation model. We further validate the
generality of this finding by applying the technique to three state-of-the-art
lighting estimation methods from recent literature.

</details>


### [28] [Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models](https://arxiv.org/abs/2509.18405)
*Sourav Halder,Jinjun Tong,Xinyu Wu*

Main category: cs.CV

TL;DR: A training-free, zero-shot check field detector that uses a vision-language model and multimodal LLM to localize signature, MICR, amounts, payee/payor without labeled training data; evaluated on 110 checks, showing generalization and potential as a bootstrap dataset generator.


<details>
  <summary>Details</summary>
Motivation: Fraudulent check activity and the need for reliable field-level extraction; labeled, diverse datasets are scarce due to privacy/proprietary concerns; enabling rapid deployment with zero-shot methods addresses data bottlenecks.

Method: Combine VLM (vision-language model) with MLLM (multimodal large language model) to perform zero-shot field detection on checks; identify and localize components (signature, MICR, courtesy amount, legal amount, payee, payer) without task-specific training; use results to bootstrap labeled datasets for downstream detectors.

Result: On a hand-curated dataset of 110 checks across formats/layouts, the approach shows strong performance and generalization; demonstrates potential for real-time application and dataset generation.

Conclusion: A practical, training-free approach to check field detection that lowers deployment barriers and can seed high-quality labeled data for specialized models; invites further evaluation on larger, more diverse corpora and integration into fraud-detection pipelines.

Abstract: Checks remain a foundational instrument in the financial ecosystem,
facilitating substantial transaction volumes across institutions. However,
their continued use also renders them a persistent target for fraud,
underscoring the importance of robust check fraud detection mechanisms. At the
core of such systems lies the accurate identification and localization of
critical fields, such as the signature, magnetic ink character recognition
(MICR) line, courtesy amount, legal amount, payee, and payer, which are
essential for subsequent verification against reference checks belonging to the
same customer. This field-level detection is traditionally dependent on object
detection models trained on large, diverse, and meticulously labeled datasets,
a resource that is scarce due to proprietary and privacy concerns. In this
paper, we introduce a novel, training-free framework for automated check field
detection, leveraging the power of a vision language model (VLM) in conjunction
with a multimodal large language model (MLLM). Our approach enables zero-shot
detection of check components, significantly lowering the barrier to deployment
in real-world financial settings. Quantitative evaluation of our model on a
hand-curated dataset of 110 checks spanning multiple formats and layouts
demonstrates strong performance and generalization capability. Furthermore,
this framework can serve as a bootstrap mechanism for generating high-quality
labeled datasets, enabling the development of specialized real-time object
detection models tailored to institutional needs.

</details>


### [29] [Losing the Plot: How VLM responses degrade on imperfect charts](https://arxiv.org/abs/2509.18425)
*Philip Wootaek Shin,Jack Sampson,Vijaykrishnan Narayanan,Andres Marquez,Mahantesh Halappanavar*

Main category: cs.CV

TL;DR: VLMs are brittle on real-world charts; CHART NOISe benchmark and reverse-consistency probe reveal reliability gaps; baseline mitigations proposed.


<details>
  <summary>Details</summary>
Motivation: Real-world charts contain distortions; current benchmarks assume clean figures; need robust evaluation of chart reasoning and mitigation of hallucinations and overconfidence.

Method: Benchmark SOTA VLMs (ChatGPT 4o, Claude Sonnet 4, Gemini 2.5 Pro) on corrupted/occluded charts; introduce CHART NOISe dataset with corruptions, occlusions, and CSAT-style MC questions; propose prompt reverse inconsistency to detect self-contradiction; suggest mitigation strategies (quality filtering, occlusion detection).

Result: Found sharp performance drops under distortion and occlusion; increased hallucinations (value fabrication, trend misinterpretation, entity confusion); models remain overconfident with plausible but unsupported explanations; CHART NOISe unifies corruption, occlusion, and reverse inconsistency; baseline mitigation strategies proposed.

Conclusion: Establishes a rigorous testbed for chart understanding robustness; CHART NOISe released; highlights need for reliability improvements and further research into robustness strategies.

Abstract: Vision language models (VLMs) show strong results on chart understanding, yet
existing benchmarks assume clean figures and fact based queries. Real world
charts often contain distortions and demand reasoning beyond simple matching.
We evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp
performance drops under corruption or occlusion, with hallucinations such as
value fabrication, trend misinterpretation, and entity confusion becoming more
frequent. Models remain overconfident in degraded settings, generating
plausible but unsupported explanations.
  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,
and Reasoning Testing on Noisy and Occluded Input Selections), a dataset
combining chart corruptions, occlusions, and exam style multiple choice
questions inspired by Korea's CSAT English section. A key innovation is prompt
reverse inconsistency, where models contradict themselves when asked to confirm
versus deny the same statement. Our contributions are threefold: (1)
benchmarking state of the art VLMs, exposing systematic vulnerabilities in
chart reasoning; (2) releasing CHART NOISe, the first dataset unifying
corruption, occlusion, and reverse inconsistency; and (3) proposing baseline
mitigation strategies such as quality filtering and occlusion detection.
Together, these efforts establish a rigorous testbed for advancing robustness
and reliability in chart understanding.

</details>


### [30] [CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction](https://arxiv.org/abs/2509.18427)
*Xinyang Wu,Muheng Li,Xia Li,Orso Pusterla,Sairos Safai,Philippe C. Cattin,Antony J. Lomax,Ye Zhang*

Main category: cs.CV

TL;DR: A transformer-guided neural deformation framework for 4D-MRI that eliminates phase binning, enabling fast, accurate 3D reconstructions across respiratory states with improved efficiency.


<details>
  <summary>Details</summary>
Motivation: Phase-binning-based 4D-MRI struggles with temporal variability, workflow complexity, and computational load; a continuous, template-free approach could improve fidelity and efficiency for RT planning.

Method: Two networks: Spatial Anatomy Network (SAN) for continuous 3D anatomy, and Temporal Motion Network (TMN) guided by Transformer-derived 1D respiratory signals to produce temporally consistent deformation fields; integrates motion modeling with image reconstruction; replaces discrete sorting.

Result: On 19 volunteers' free-breathing data, accurately captures regular/irregular respiration, preserves vessel/bronchial continuity, high anatomical fidelity; training time ~15 min (vs ~5 hours); per-volume inference <1 second; reconstructs at any respiratory state; superior to conventional methods.

Conclusion: Template- and phase-free 4D-MRI reconstruction framework offering efficient, accurate 4D imaging with strong potential for 4D RT planning and real-time adaptive treatment.

Abstract: Four-dimensional MRI (4D-MRI) is an promising technique for capturing
respiratory-induced motion in radiation therapy planning and delivery.
Conventional 4D reconstruction methods, which typically rely on phase binning
or separate template scans, struggle to capture temporal variability,
complicate workflows, and impose heavy computational loads. We introduce a
neural representation framework that considers respiratory motion as a smooth,
continuous deformation steered by a 1D surrogate signal, completely replacing
the conventional discrete sorting approach. The new method fuses motion
modeling with image reconstruction through two synergistic networks: the
Spatial Anatomy Network (SAN) encodes a continuous 3D anatomical
representation, while a Temporal Motion Network (TMN), guided by
Transformer-derived respiratory signals, produces temporally consistent
deformation fields. Evaluation using a free-breathing dataset of 19 volunteers
demonstrates that our template- and phase-free method accurately captures both
regular and irregular respiratory patterns, while preserving vessel and
bronchial continuity with high anatomical fidelity. The proposed method
significantly improves efficiency, reducing the total processing time from
approximately five hours required by conventional discrete sorting methods to
just 15 minutes of training. Furthermore, it enables inference of each 3D
volume in under one second. The framework accurately reconstructs 3D images at
any respiratory state, achieves superior performance compared to conventional
methods, and demonstrates strong potential for application in 4D radiation
therapy planning and real-time adaptive treatment.

</details>


### [31] [An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects](https://arxiv.org/abs/2509.18451)
*Prithvi Raj Singh,Raju Gottumukkala,Anthony Maida*

Main category: cs.CV

TL;DR: Kalman-filter trackers struggle with fast-moving tiny objects like racquetballs; DeepOCSORT offers best accuracy among the evaluated methods but all trackers show notable tracking drift; ByteTrack is the fastest; overall error magnitudes are much higher than standard benchmarks, underscoring the need for specialized methods for fast, small targets.


<details>
  <summary>Details</summary>
Motivation: Unpredictable motion and small visual marks make precise tracking of fast-moving tiny objects difficult, a challenge with direct implications for sport robotics where lightweight, accurate perception and planning are desired. The study evaluates whether leading Kalman-filter-based trackers can handle such objects and at what speeds they operate.

Method: A comparative evaluation of five state-of-the-art Kalman-filter based trackers—OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT—on a custom dataset of 10,000 annotated racquetball frames captured at 720p–1280p. The analysis focuses on inference speed and per-image update frequency across four scenarios and reports ADE (in pixels) to compare accuracy.

Result: DeepOCSORT yields the lowest tracking error with an ADE of 31.15 pixels, while ByteTrack has a higher ADE of 114.3 pixels. ByteTrack is the fastest, with an average inference time of 26.6 ms compared to DeepOCSORT's 26.8 ms. All Kalman-filter trackers exhibit significant tracking drift with spatial errors of 3–11 cm (ADE values 31–114 px), indicating fundamental limitations in handling unpredictable motion patterns of fast-moving tiny objects.

Conclusion: Current Kalman-filter based tracking methods exhibit substantial limitations for fast-moving tiny objects like racquetballs, with error rates 3–4 times higher than standard object-tracking benchmarks. Substantial methodological advances are needed to develop reliable tracking for fast-moving tiny targets in sports robotics.

Abstract: Unpredictable movement patterns and small visual mark make precise tracking
of fast-moving tiny objects like a racquetball one of the challenging problems
in computer vision. This challenge is particularly relevant for sport robotics
applications, where lightweight and accurate tracking systems can improve robot
perception and planning capabilities. While Kalman filter-based tracking
methods have shown success in general object tracking scenarios, their
performance degrades substantially when dealing with rapidly moving objects
that exhibit irregular bouncing behavior. In this study, we evaluate the
performance of five state-of-the-art Kalman filter-based tracking
methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom
dataset containing 10,000 annotated racquetball frames captured at 720p-1280p
resolution. We focus our analysis on two critical performance factors:
inference speed and update frequency per image, examining how these parameters
affect tracking accuracy and reliability for fast-moving tiny objects. Our
experimental evaluation across four distinct scenarios reveals that DeepOCSORT
achieves the lowest tracking error with an average ADE of 31.15 pixels compared
to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest
processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.
However, our results show that all Kalman filter-based trackers exhibit
significant tracking drift with spatial errors ranging from 3-11cm (ADE values:
31-114 pixels), indicating fundamental limitations in handling the
unpredictable motion patterns of fast-moving tiny objects like racquetballs.
Our analysis demonstrates that current tracking approaches require substantial
improvements, with error rates 3-4x higher than standard object tracking
benchmarks, highlighting the need for specialized methodologies for fast-moving
tiny object tracking applications.

</details>


### [32] [MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition](https://arxiv.org/abs/2509.18473)
*Binhua Huang,Wendong Yao,Shaowu Chen,Guoxin Wang,Qingyuan Wang,Soumyabrata Dev*

Main category: cs.CV

TL;DR: MoCrop is a training-free, parameter-free module that uses H.264 motion vectors to produce a single clip-level crop across I-frames, improving action recognition efficiency in the compressed domain with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Improve video action recognition efficiency by exploiting readily available motion in compressed video, avoiding extra training or parameters, and enabling real-time deployment.

Method: A lightweight DM-MCS-AC pipeline: denoise & merge (DM), Monte Carlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix search. It selects a motion-dense crop from motion vectors and applies the same crop to all I-frames at inference; plugs into diverse backbones with no added parameters.

Result: On UCF101 with ResNet-50: +3.5% Top-1 at equal FLOPs (attention setting) or +2.4% Top-1 with 26.5% fewer FLOPs (efficiency setting). On CoViAR: 89.2% Top-1 at original cost; 88.5% Top-1 with compute reduced from 11.6 to 8.5 GFLOPs. General gains observed on MobileNet-V3, EfficientNet-B1, and Swin-B.

Conclusion: MoCrop is general, training-free, and practical for real-time deployment in the compressed domain, delivering consistent accuracy/compute gains across multiple backbones. Code is publicly available.

Abstract: We introduce MoCrop, a motion-aware adaptive cropping module for efficient
video action recognition in the compressed domain. MoCrop uses motion vectors
that are available in H.264 video to locate motion-dense regions and produces a
single clip-level crop that is applied to all I-frames at inference. The module
is training free, adds no parameters, and can be plugged into diverse
backbones. A lightweight pipeline that includes denoising & merge (DM), Monte
Carlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix
search yields robust crops with negligible overhead. On UCF101, MoCrop improves
accuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy
at equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer
FLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy
at the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6
to 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B
indicate strong generality and make MoCrop practical for real-time deployment
in the compressed domain. Our code and models are available at
https://github.com/microa/MoCrop.

</details>


### [33] [Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems](https://arxiv.org/abs/2509.18481)
*Xinyu Wang,Zikun Zhou,Yingjian Li,Xin An,Hongpeng Wang*

Main category: cs.CV

TL;DR: CAFC-SE: a codebook-based adaptive feature compression framework for edge-cloud analytics that uses vector quantization to map edge features to discrete indices, selectively transmitting them to the cloud. It preserves informative visual patterns under low bitrate, yielding better rate–accuracy trade-offs.


<details>
  <summary>Details</summary>
Motivation: Current approaches either compress images for analysis on reconstructed images or compress intermediate features with entropy models, but both struggle at low bitrate due to redundant details or highly skewed symbol distributions. A robust, low-bitrate feature compression method could improve analysis performance in edge-cloud systems.

Method: Edge-side vector quantization using a learned codebook to map continuous feature vectors to discrete indices; selective transmission of these indices to the cloud; includes a semantic enhancement component (CAFC-SE) to preserve semantically informative patterns under bitrate constraints.

Result: Extensive experiments show the proposed method achieves superior rate and accuracy, demonstrating reduced vulnerability to low-bitrate conditions and better analysis performance compared to baselines.

Conclusion: Codebook-based adaptive feature compression with semantic enhancement (CAFC-SE) effectively enables robust, low-bitrate edge-cloud analytics by quantizing features into informative discrete indices and transmitting them selectively to the cloud.

Abstract: Coding images for machines with minimal bitrate and strong analysis
performance is key to effective edge-cloud systems. Several approaches deploy
an image codec and perform analysis on the reconstructed image. Other methods
compress intermediate features using entropy models and subsequently perform
analysis on the decoded features. Nevertheless, these methods both perform
poorly under low-bitrate conditions, as they retain many redundant details or
learn over-concentrated symbol distributions. In this paper, we propose a
Codebook-based Adaptive Feature Compression framework with Semantic
Enhancement, named CAFC-SE. It maps continuous visual features to discrete
indices with a codebook at the edge via Vector Quantization (VQ) and
selectively transmits them to the cloud. The VQ operation that projects feature
vectors onto the nearest visual primitives enables us to preserve more
informative visual patterns under low-bitrate conditions. Hence, CAFC-SE is
less vulnerable to low-bitrate conditions. Extensive experiments demonstrate
the superiority of our method in terms of rate and accuracy.

</details>


### [34] [MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation](https://arxiv.org/abs/2509.18493)
*Md Mostafijur Rahman,Radu Marculescu*

Main category: cs.CV

TL;DR: An ultra-lightweight multi-kernel U-Net (MK-UNet) with MKDC and multi-faceted attention achieves state-of-the-art medical image segmentation with a tiny footprint.


<details>
  <summary>Details</summary>
Motivation: Enable real-time, high-fidelity medical image segmentation on resource-constrained devices by drastically reducing model size while improving accuracy compared to existing methods.

Method: Introduce MK-UNet, featuring a multi-kernel depth-wise convolution block (MKDC) to capture multi-resolution spatial relations, combined with channel, spatial, and grouped gated attention to emphasize salient features, all within a compact 0.316M-parameter network (0.314 GFLOPs).

Result: Outperforms state-of-the-art across six binary medical imaging benchmarks. Outperforms TransUNet by ~333x fewer parameters and ~123x fewer FLOPs; beats UNeXt by up to 6.7% Dice while using 4.7x fewer parameters; also surpasses MedT, CMUNeXt, EGE-UNet, and Rolling-UNet with substantially lower compute.

Conclusion: MK-UNet offers a highly efficient, accurate solution for medical image segmentation suitable for real-time diagnostics in resource-limited settings (e.g., point-of-care devices); implementation is publicly available at the provided GitHub repository.

Abstract: In this paper, we introduce MK-UNet, a paradigm shift towards
ultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image
segmentation. Central to MK-UNet is the multi-kernel depth-wise convolution
block (MKDC) we design to adeptly process images through multiple kernels,
while capturing complex multi-resolution spatial relationships. MK-UNet also
emphasizes the images salient features through sophisticated attention
mechanisms, including channel, spatial, and grouped gated attention. Our
MK-UNet network, with a modest computational footprint of only 0.316M
parameters and 0.314G FLOPs, represents not only a remarkably lightweight, but
also significantly improved segmentation solution that provides higher accuracy
over state-of-the-art (SOTA) methods across six binary medical imaging
benchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with
nearly 333$\times$ and 123$\times$ fewer parameters and FLOPs, respectively.
Similarly, when compared against UNeXt, MK-UNet exhibits superior segmentation
performance, improving the DICE score up to 6.7% margins while operating with
4.7$\times$ fewer #Params. Our MK-UNet also outperforms other recent
lightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with
much lower computational resources. This leap in performance, coupled with
drastic computational gains, positions MK-UNet as an unparalleled solution for
real-time, high-fidelity medical diagnostics in resource-limited settings, such
as point-of-care devices. Our implementation is available at
https://github.com/SLDGroup/MK-UNet.

</details>


### [35] [BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation](https://arxiv.org/abs/2509.18501)
*Maximilian Fehrentz,Alexander Winkler,Thomas Heiliger,Nazim Haouchine,Christian Heiliger,Nassir Navab*

Main category: cs.CV

TL;DR: BridgeSplat links intraoperative 3D reconstruction with preoperative CT by binding 3D Gaussian primitives to a CT mesh and jointly optimizing Gaussian parameters and mesh deformations under photometric supervision, enabling CT updates from monocular RGB during surgery.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between real-time surgical video (often monocular) and volumetric patient data (CT) to achieve deformable, CT-consistent surgical navigation and potential CT updates during procedures.

Method: Attach 3D Gaussians to a CT mesh and perform joint optimization of Gaussian parameters and mesh deformation guided by photometric loss from intraoperative RGB data. Parametrize each Gaussian with respect to its parent mesh triangle to enforce alignment with the mesh; deform Gaussians and propagate deformations back to update the CT. Demonstrate on visceral pig surgeries and synthetic liver data (monocular RGB) with code and data released.

Result: The approach yields sensible deformations of the preoperative CT that align with intraoperative monocular RGB observations, validated on animal experiments and synthetic human liver data.

Conclusion: BridgeSplat offers a practical CT-informed deformable registration framework for surgical navigation, enabling CT updates driven by intraoperative imagery and demonstrated effectiveness on realistic datasets, with accompanying open resources.

Abstract: We introduce BridgeSplat, a novel approach for deformable surgical navigation
that couples intraoperative 3D reconstruction with preoperative CT data to
bridge the gap between surgical video and volumetric patient data. Our method
rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian
parameters and mesh deformation through photometric supervision. By
parametrizing each Gaussian relative to its parent mesh triangle, we enforce
alignment between Gaussians and mesh and obtain deformations that can be
propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on
visceral pig surgeries and synthetic data of a human liver under simulation,
showing sensible deformations of the preoperative CT on monocular RGB data.
Code, data, and additional resources can be found at
https://maxfehrentz.github.io/ct-informed-splatting/ .

</details>


### [36] [Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment](https://arxiv.org/abs/2509.18502)
*Wenjie Liu,Hongmin Liu,Lixin Zhang,Bin Fan*

Main category: cs.CV

TL;DR: DGLE proposes a pseudo-label optimization framework for source-free domain adaptation in remote sensing semantic segmentation. It starts with a small set of high-quality seeds obtained via confidence filtering and super-resolution, then uses a diffusion model to propagate these seeds to a complete, high-quality pseudo-label set, improving self-training performance on the target domain.


<details>
  <summary>Details</summary>
Motivation: In SFDA for semantic segmentation, acquiring reliable pseudo-labels is challenging due to noise and the infeasibility of using source data. Existing methods often optimize the entire pseudo-label set, which is brittle to noise. The goal is to generate high-quality, complete pseudo-labels starting from a few reliable seeds to enhance target-domain adaptation.

Method: 1) Pseudo-label fusion: filter labels by confidence and enhance via super-resolution to obtain a small, high-quality seed set. 2) Diffusion-based propagation: apply a diffusion model to propagate seeds across the target data, denoising and modeling complex distributions to produce a complete, high-quality pseudo-label set.

Result: The approach improves pseudo-label quality and downstream target-domain performance by avoiding direct optimization of the full label set and leveraging diffusion to robustly fill in missing labels.

Conclusion: DGLE offers an effective SFDA strategy for semantic segmentation in remote sensing by coupling seed-based pseudo-label enrichment with diffusion-driven propagation, yielding better self-training signals for target-domain adaptation.

Abstract: Research on unsupervised domain adaptation (UDA) for semantic segmentation of
remote sensing images has been extensively conducted. However, research on how
to achieve domain adaptation in practical scenarios where source domain data is
inaccessible namely, source-free domain adaptation (SFDA) remains limited.
Self-training has been widely used in SFDA, which requires obtaining as many
high-quality pseudo-labels as possible to train models on target domain data.
Most existing methods optimize the entire pseudo-label set to obtain more
supervisory information. However, as pseudo-label sets often contain
substantial noise, simultaneously optimizing all labels is challenging. This
limitation undermines the effectiveness of optimization approaches and thus
restricts the performance of self-training. To address this, we propose a novel
pseudo-label optimization framework called Diffusion-Guided Label Enrichment
(DGLE), which starts from a few easily obtained high-quality pseudo-labels and
propagates them to a complete set of pseudo-labels while ensuring the quality
of newly generated labels. Firstly, a pseudo-label fusion method based on
confidence filtering and super-resolution enhancement is proposed, which
utilizes cross-validation of details and contextual information to obtain a
small number of high-quality pseudo-labels as initial seeds. Then, we leverage
the diffusion model to propagate incomplete seed pseudo-labels with irregular
distributions due to its strong denoising capability for randomly distributed
noise and powerful modeling capacity for complex distributions, thereby
generating complete and high-quality pseudo-labels. This method effectively
avoids the difficulty of directly optimizing the complete set of pseudo-labels,
significantly improves the quality of pseudo-labels, and thus enhances the
model's performance in the target domain.

</details>


### [37] [Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.18504)
*Jiaxin Dai,Xiang Xiang*

Main category: cs.CV

TL;DR: Hyperbolic space embedding via Poincaré ball for coarse-to-fine few-shot class-incremental learning (C2FSCIL), with hyperbolic contrastive loss, hyperbolic FC layers, and entropy-based hyperbolic augmentation, improving coarse and fine class accuracies on C2FSCIL benchmarks.


<details>
  <summary>Details</summary>
Motivation: Hierarchical data and incremental learning scenarios benefit from hyperbolic geometry, which better represents hierarchies than Euclidean space. The aim is to improve Coarse-To-Fine FSCIL by aligning feature space with the coarse-to-fine label structure and alleviating overfitting in few-shot regimes.

Method: Adopts the Knowe framework: contrastively learns coarse labels and freezes fine-class classifier weights in the embedding space. Embeds the feature extractor in hyperbolic space using the Poincaré ball. Introduces hyperbolic contrastive loss and hyperbolic fully-connected layers. Applies maximum entropy distribution in hyperbolic space to model the fine-class feature distribution and generate augmented features for training.

Result: Experiments on C2FSCIL benchmarks show improved accuracies for both coarse and fine classes, indicating the effectiveness of hyperbolic space for hierarchical, few-shot, and incremental learning tasks.

Conclusion: Embedding the feature extractor in hyperbolic space with the proposed hyperbolic losses, hyperbolic FC layers, and entropy-based augmentation enhances performance for coarse-to-fine FSCIL and helps mitigate overfitting in few-shot scenarios.

Abstract: In the field of machine learning, hyperbolic space demonstrates superior
representation capabilities for hierarchical data compared to conventional
Euclidean space. This work focuses on the Coarse-To-Fine Few-Shot
Class-Incremental Learning (C2FSCIL) task. Our study follows the Knowe
approach, which contrastively learns coarse class labels and subsequently
normalizes and freezes the classifier weights of learned fine classes in the
embedding space. To better interpret the "coarse-to-fine" paradigm, we propose
embedding the feature extractor into hyperbolic space. Specifically, we employ
the Poincar\'e ball model of hyperbolic space, enabling the feature extractor
to transform input images into feature vectors within the Poincar\'e ball
instead of Euclidean space. We further introduce hyperbolic contrastive loss
and hyperbolic fully-connected layers to facilitate model optimization and
classification in hyperbolic space. Additionally, to enhance performance under
few-shot conditions, we implement maximum entropy distribution in hyperbolic
space to estimate the probability distribution of fine-class feature vectors.
This allows generation of augmented features from the distribution to mitigate
overfitting during training with limited samples. Experiments on C2FSCIL
benchmarks show that our method effectively improves both coarse and fine class
accuracies.

</details>


### [38] [GeoRemover: Removing Objects and Their Causal Visual Artifacts](https://arxiv.org/abs/2509.18538)
*Zixin Zhu,Haoxiang Li,Xuelu Feng,He Wu,Chunming Qiao,Junsong Yuan*

Main category: cs.CV

TL;DR: A two-stage geometry-aware framework decoupling object removal into geometry removal and appearance rendering to erase objects and their causal artifacts, guided by a geometry-focused, preference-driven objective; achieves SOTA on two benchmarks; code released.


<details>
  <summary>Details</summary>
Motivation: Appearance-based object removal struggles with artifacts (shadows, reflections) due to ignoring causal links between object geometry and visual effects and suffers from strict or loose mask alignment leading to under- or over-erase.

Method: Stage 1: remove object from geometry (e.g., depth) with strictly mask-aligned supervision to enforce structure-aware editing. Stage 2: render RGB image conditioned on updated geometry, implicitly handling causal artifacts via geometry changes. A preference-driven objective with positive/negative sample pairs guides learning to remove objects and artifacts while avoiding new structural insertions.

Result: Extensive experiments show state-of-the-art performance in removing objects and artifacts on two benchmarks; code available at the provided URL.

Conclusion: Decoupling geometry removal from appearance rendering with geometry supervision yields robust editing with controllable, artifact-free removal; demonstrates the importance of causal reasoning in editing tasks.

Abstract: Towards intelligent image editing, object removal should eliminate both the
target object and its causal visual artifacts, such as shadows and reflections.
However, existing image appearance-based methods either follow strictly
mask-aligned training and fail to remove these causal effects which are not
explicitly masked, or adopt loosely mask-aligned strategies that lack
controllability and may unintentionally over-erase other objects. We identify
that these limitations stem from ignoring the causal relationship between an
object's geometry presence and its visual effects. To address this limitation,
we propose a geometry-aware two-stage framework that decouples object removal
into (1) geometry removal and (2) appearance rendering. In the first stage, we
remove the object directly from the geometry (e.g., depth) using strictly
mask-aligned supervision, enabling structure-aware editing with strong
geometric constraints. In the second stage, we render a photorealistic RGB
image conditioned on the updated geometry, where causal visual effects are
considered implicitly as a result of the modified 3D geometry. To guide
learning in the geometry removal stage, we introduce a preference-driven
objective based on positive and negative sample pairs, encouraging the model to
remove objects as well as their causal visual artifacts while avoiding new
structural insertions. Extensive experiments demonstrate that our method
achieves state-of-the-art performance in removing both objects and their
associated artifacts on two popular benchmarks. The code is available at
https://github.com/buxiangzhiren/GeoRemover.

</details>


### [39] [SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models](https://arxiv.org/abs/2509.18546)
*Yujia Liu,Dingquan Li,Tiejun Huang*

Main category: cs.CV

TL;DR: Proposes SEGA, a transferable black-box attack for No-Reference Image Quality Assessment models that uses Gaussian-smoothed gradients from multiple source models and a perturbation filter to enhance transferability and imperceptibility; evaluated on CLIVE with strong transfer success.


<details>
  <summary>Details</summary>
Motivation: NR-IQA models are vulnerable to adversarial perturbations, and white-box attacks do not translate well to real-world black-box settings. Improving transferability of attacks helps reveal vulnerabilities and guide defense design.

Method: A transferable Signed Ensemble Gaussian black-box Attack (SEGA): it approximates the target gradient by applying Gaussian smoothing to gradients from multiple source models and ensembling their smoothed gradients (signed), then applies a perturbation filter mask to remove perturbations that would be perceptible or ineffective, ensuring imperceptibility while enhancing transferability.

Result: Experiments on the CLIVE dataset show SEGA achieves superior transferability to unknown NR-IQA models compared to baselines, enabling successful transfer-based black-box attacks.

Conclusion: SEGA provides an effective framework for black-box NR-IQA attacks with high transferability, exposing model vulnerabilities and informing the development of robust NR-IQA defenses.

Abstract: No-Reference Image Quality Assessment (NR-IQA) models play an important role
in various real-world applications. Recently, adversarial attacks against
NR-IQA models have attracted increasing attention, as they provide valuable
insights for revealing model vulnerabilities and guiding robust system design.
Some effective attacks have been proposed against NR-IQA models in white-box
settings, where the attacker has full access to the target model. However,
these attacks often suffer from poor transferability to unknown target models
in more realistic black-box scenarios, where the target model is inaccessible.
This work makes the first attempt to address the challenge of low
transferability in attacking NR-IQA models by proposing a transferable Signed
Ensemble Gaussian black-box Attack (SEGA). The main idea is to approximate the
gradient of the target model by applying Gaussian smoothing to source models
and ensembling their smoothed gradients. To ensure the imperceptibility of
adversarial perturbations, SEGA further removes inappropriate perturbations
using a specially designed perturbation filter mask. Experimental results on
the CLIVE dataset demonstrate the superior transferability of SEGA, validating
its effectiveness in enabling successful transfer-based black-box attacks
against NR-IQA models.

</details>


### [40] [HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles](https://arxiv.org/abs/2509.18550)
*Mohammad Junayed Hasan,Nabeel Mohammed,Shafin Rahman,Philipp Koehn*

Main category: cs.CV

TL;DR: HadaSmileNet introduces a parameter-free Hadamard multiplicative fusion that directly combines transformer-based representations with physiologically grounded D-Markers for genuine vs posed smile emotion recognition, achieving state-of-the-art results with reduced parameters and simpler training.


<details>
  <summary>Details</summary>
Motivation: Distinguishing genuine from posed emotions is a core pattern recognition problem with broad data mining implications in social sciences, healthcare, and HCI. Existing multi-task learning approaches that fuse deep features with handcrafted D-Marker features suffer from computational inefficiency due to auxiliary task supervision and complex loss balancing.

Method: Propose HadaSmileNet, a feature fusion framework that uses parameter-free Hadamard multiplicative interactions to fuse transformer representations with D-Marker features. Conduct systematic evaluation of 15 fusion strategies and identify Hadamard fusion as optimal for direct feature interactions and efficiency.

Result: Achieves new state-of-the-art performance on four benchmarks: UvA-NEMO 88.7% (+0.8), MMI 99.7%, SPOS 98.5% (+0.7), BBC 100% (+5.0). Demonstrates 26% parameter reduction and simpler training versus multi-task approaches; visualization shows improved discriminative power from direct domain knowledge integration.

Conclusion: The framework delivers both effectiveness and efficiency, making it suitable for real-time multimedia data mining and affective computing applications. Hadamard multiplicative fusion provides the best balance of accuracy and computation by enabling direct interaction between deep representations and domain-grounded features.

Abstract: The distinction between genuine and posed emotions represents a fundamental
pattern recognition challenge with significant implications for data mining
applications in social sciences, healthcare, and human-computer interaction.
While recent multi-task learning frameworks have shown promise in combining
deep learning architectures with handcrafted D-Marker features for smile facial
emotion recognition, these approaches exhibit computational inefficiencies due
to auxiliary task supervision and complex loss balancing requirements. This
paper introduces HadaSmileNet, a novel feature fusion framework that directly
integrates transformer-based representations with physiologically grounded
D-Markers through parameter-free multiplicative interactions. Through
systematic evaluation of 15 fusion strategies, we demonstrate that Hadamard
multiplicative fusion achieves optimal performance by enabling direct feature
interactions while maintaining computational efficiency. The proposed approach
establishes new state-of-the-art results for deep learning methods across four
benchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS
(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational
analysis reveals 26 percent parameter reduction and simplified training
compared to multi-task alternatives, while feature visualization demonstrates
enhanced discriminative power through direct domain knowledge integration. The
framework's efficiency and effectiveness make it particularly suitable for
practical deployment in multimedia data mining applications that require
real-time affective computing capabilities.

</details>


### [41] [Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction](https://arxiv.org/abs/2509.18566)
*Xiaoting Yin,Hao Shi,Kailun Yang,Jiajun Zhai,Shangwei Guo,Lin Wang,Kaiwei Wang*

Main category: cs.CV

TL;DR: Unified event-guided 3D Gaussian Splatting for joint reconstruction and animation of dynamic humans and static scenes from a monocular event camera, achieving state-of-the-art results on blur-heavy benchmarks without external masks.


<details>
  <summary>Details</summary>
Motivation: Monocular RGB reconstructions struggle under fast motion due to motion blur; event cameras offer microsecond temporal resolution, providing complementary data to better capture fast dynamics. A unified, mask-free representation can simplify modeling and improve robustness.

Method: A single set of 3D Gaussians carries learnable semantic attributes; Gaussians labeled as human deform for animation while scene Gaussians stay static. An event-guided loss aligns brightness changes between consecutive renderings with the event stream to mitigate blur, enabling high-fidelity reconstruction from a monocular event input.

Result: On ZJU-MoCap-Blur and MMHPSD-Blur datasets, the method achieves state-of-the-art performance in PSNR/SSIM and reduces LPIPS, with especially pronounced gains for high-speed subjects, surpassing strong baselines without requiring external human masks.

Conclusion: The approach simplifies modeling by using a unified Gaussian set and removing the need for external masks, while the event-guided loss effectively leverages event data to improve local fidelity in fast motion, demonstrating strong potential for robust dynamic human-scene reconstruction from monocular event data.

Abstract: Reconstructing dynamic humans together with static scenes from monocular
videos remains difficult, especially under fast motion, where RGB frames suffer
from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond
temporal resolution, making them a superior sensing choice for dynamic human
reconstruction. Accordingly, we present a novel event-guided human-scene
reconstruction framework that jointly models human and scene from a single
monocular event camera via 3D Gaussian Splatting. Specifically, a unified set
of 3D Gaussians carries a learnable semantic attribute; only Gaussians
classified as human undergo deformation for animation, while scene Gaussians
stay static. To combat blur, we propose an event-guided loss that matches
simulated brightness changes between consecutive renderings with the event
stream, improving local fidelity in fast-moving regions. Our approach removes
the need for external human masks and simplifies managing separate Gaussian
sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers
state-of-the-art human-scene reconstruction, with notable gains over strong
baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.

</details>


### [42] [Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought](https://arxiv.org/abs/2509.18571)
*Yuhan Wang,Cheng Liu,Zihan Zhao,Weichao Wu*

Main category: cs.CV

TL;DR: Live-E2T proposes a real-time threat monitoring system that uses structured Human-Object-Interaction-Place tuples, online event deduplication, and a chain-of-thought–enhanced LLM to deliver accurate, efficient, and explainable threat analyses, outperforming state-of-the-art on XD-Violence and UCF-Crime.


<details>
  <summary>Details</summary>
Motivation: There is a trade-off between real-time performance and decision explainability in threat monitoring. Existing supervised or generative approaches struggle to satisfy both goals while maintaining robust performance. A compact, semantically focused representation plus effective deduplication and transparent reasoning is needed.

Method: 1) Decompose video frames into structured Human-Object-Interaction-Place (HOI-Place) semantic tuples to form a compact, informative representation. 2) Implement an online event deduplication/updating mechanism to filter spatio-temporal redundancies for real-time responsiveness. 3) Fine-tune a Large Language Model with a Chain-of-Thought strategy to reason over event sequences and generate transparent threat assessment reports.

Result: Extensive experiments on XD-Violence and UCF-Crime show that Live-E2T significantly outperforms state-of-the-art methods in threat detection accuracy, real-time efficiency, and explainability.

Conclusion: The framework demonstrates that combining a semantically structured representation, online deduplication, and LLM-based chain-of-thought reasoning yields superior real-time, explainable threat monitoring performance on benchmark datasets.

Abstract: Real-time threat monitoring identifies threatening behaviors in video streams
and provides reasoning and assessment of threat events through explanatory
text. However, prevailing methodologies, whether based on supervised learning
or generative models, struggle to concurrently satisfy the demanding
requirements of real-time performance and decision explainability. To bridge
this gap, we introduce Live-E2T, a novel framework that unifies these two
objectives through three synergistic mechanisms. First, we deconstruct video
frames into structured Human-Object-Interaction-Place semantic tuples. This
approach creates a compact, semantically focused representation, circumventing
the information degradation common in conventional feature compression. Second,
an efficient online event deduplication and updating mechanism is proposed to
filter spatio-temporal redundancies, ensuring the system's real time
responsiveness. Finally, we fine-tune a Large Language Model using a
Chain-of-Thought strategy, endow it with the capability for transparent and
logical reasoning over event sequences to produce coherent threat assessment
reports. Extensive experiments on benchmark datasets, including XD-Violence and
UCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art
methods in terms of threat detection accuracy, real-time efficiency, and the
crucial dimension of explainability.

</details>


### [43] [The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers](https://arxiv.org/abs/2509.18582)
*Daiqing Qi,Handong Zhao,Jing Shi,Simon Jenni,Yifei Fan,Franck Dernoncourt,Scott Cohen,Sheng Li*

Main category: cs.CV

TL;DR: Introduces PhotoCritique (a large-scale, expert-derived aesthetics dataset), PhotoEye (a language-guided multi-view fusion model), and PhotoBench (a professional benchmark) to advance aesthetic visual understanding in Multimodal Large Language Models (MLLMs); shows advantages over existing models on benchmarks including PhotoBench.


<details>
  <summary>Details</summary>
Motivation: Addresses the gap between general visual understanding (object detection, localization) and aesthetic visual understanding (color, lighting, composition) in MLLMs. Current models struggle with expert-level aesthetic analysis requiring photographic knowledge; a dedicated dataset, model, and benchmark are proposed to push the capability of MLLMs in real-world, professional contexts.

Method: 1) Build PhotoCritique from extensive discussions among professional photographers and enthusiasts to capture large-scale, diverse aesthetic expertise. 2) Propose PhotoEye, a language-guided multi-view vision fusion model to learn aesthetics from multiple perspectives. 3) Create PhotoBench as a comprehensive, professional benchmark for aesthetic visual understanding.

Result: Empirical results show PhotoEye achieves clear advantages over existing models on both standard benchmarks and the new PhotoBench, indicating improved aesthetic understanding and descriptive capability in professional contexts.

Conclusion: The work provides a foundation for high-level aesthetic reasoning in MLLMs through a specialized dataset, an architecture tailored for multi-view aesthetics, and a rigorous benchmark, enabling better evaluation and advancement in professional-image analysis.

Abstract: While editing directly from life, photographers have found it too difficult
to see simultaneously both the blue and the sky. Photographer and curator,
Szarkowski insightfully revealed one of the notable gaps between general and
aesthetic visual understanding: while the former focuses on identifying the
factual element in an image (sky), the latter transcends such object
identification, viewing it instead as an aesthetic component--a pure color
block (blue). Such fundamental distinctions between general (detection,
localization, etc.) and aesthetic (color, lighting, composition, etc.) visual
understanding present a significant challenge for Multimodal Large Language
Models (MLLMs). Although some recent works have made initial explorations, they
are often limited to general and basic aesthetic commonsense. As a result, they
frequently fall short in real-world scenarios (Fig. 1), which require extensive
expertise--including photographic techniques, photo pre/post-processing
knowledge, and more, to provide a detailed analysis and description. To
fundamentally enhance the aesthetics understanding of MLLMs, we first introduce
a novel dataset, PhotoCritique, derived from extensive discussions among
professional photographers and enthusiasts, and characterized by the large
scale, expertise, and diversity. Then, to better learn visual aesthetics from
PhotoCritique, we furthur propose a novel model, PhotoEye, featuring a
languageguided multi-view vision fusion mechanism to understand image
aesthetics from multiple perspectives. Finally, we present a novel benchmark,
PhotoBench, a comprehensive and professional benchmark for aesthetic visual
understanding. On existing benchmarks and PhotoBench, our model demonstrates
clear advantages over existing models.

</details>


### [44] [Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network](https://arxiv.org/abs/2509.18591)
*Pengchao Deng,Shengqi Chen*

Main category: cs.CV

TL;DR: XMem-based real-time tumor segmentation for MRI-guided radiotherapy; memory-augmented approach enables tracking across long cine-MRI; quantitative results are unavailable due to data loss, but preliminary tests show real-time capability and reasonable accuracy.


<details>
  <summary>Details</summary>
Motivation: Real-time MRI-guided radiotherapy requires precise, fast tumor segmentation across long cine-MRI sequences, often with limited annotated data. Temporal coherence and motion tracking present significant challenges for conventional segmentation models.

Method: Utilizes the XMem memory-augmented segmentation model to process long cine-MRI sequences, integrating memory mechanisms to maintain temporal context and track tumor motion in real time.

Result: No precise quantitative results can be reported because detailed experimental records were lost. Preliminary impressions during development indicate reasonable segmentation performance and satisfaction of the clinical real-time requirement.

Conclusion: The work contributes to improving the precision of tumor tracking during MRI-guided radiotherapy, potentially enhancing the accuracy and safety of cancer treatments.

Abstract: This paper presents an advanced tumor segmentation framework for real-time
MRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method
leverages the XMem model, a memory-augmented architecture, to segment tumors
across long cine-MRI sequences. The proposed system efficiently integrates
memory mechanisms to track tumor motion in real-time, achieving high
segmentation accuracy even under challenging conditions with limited annotated
data. Unfortunately, the detailed experimental records have been lost,
preventing us from reporting precise quantitative results at this stage.
Nevertheless, From our preliminary impressions during development, the
XMem-based framework demonstrated reasonable segmentation performance and
satisfied the clinical real-time requirement. Our work contributes to improving
the precision of tumor tracking during MRI-guided radiotherapy, which is
crucial for enhancing the accuracy and safety of cancer treatments.

</details>


### [45] [SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution](https://arxiv.org/abs/2509.18593)
*Xiaoman Wu,Lubin Gan,Siying Wu,Jing Zhang,Yunwei Ou,Xiaoyan Sun*

Main category: cs.CV

TL;DR: Proposes SSCM for multi-contrast MRI SR that combines dynamic spatial warping, semantic-aware token aggregation, and spatial-frequency fusion to improve cross-contrast alignment and high-frequency detail restoration, achieving state-of-the-art results with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: MC-MRI SR faces spatial-semantic inconsistencies due to anatomical misalignment, motion, and contrast differences between LR target and HR references. Conventional methods underutilize frequency-domain information and fail to model cross-contrast alignment adequately, leading to poor fine-grained reconstruction.

Method: 1) Dynamic Spatial Warping Module for inter-contrast spatial alignment to handle misalignments; 2) Semantic-Aware Token Aggregation Block to enforce long-range semantic consistency across contrasts; 3) Spatial-Frequency Fusion Block to restore fine structures by leveraging frequency information; trained and evaluated on public/private datasets.

Result: SSCM achieves state-of-the-art performance with fewer parameters, delivering spatially and semantically consistent reconstructions and improved high-frequency detail compared to baselines.

Conclusion: Integrating dynamic spatial warping, semantic-aware token aggregation, and spatial-frequency fusion yields a powerful framework for MC-MRI SR, balancing alignment, semantic coherence, and high-frequency restoration with efficient parameter usage.

Abstract: Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims
to enhance low-resolution (LR) contrasts leveraging high-resolution (HR)
references, shortening acquisition time and improving imaging efficiency while
preserving anatomical details. The main challenge lies in maintaining
spatial-semantic consistency, ensuring anatomical structures remain
well-aligned and coherent despite structural discrepancies and motion between
the target and reference images. Conventional methods insufficiently model
spatial-semantic consistency and underuse frequency-domain information, which
leads to poor fine-grained alignment and inadequate recovery of high-frequency
details. In this paper, we propose the Spatial-Semantic Consistent Model
(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast
spatial alignment, a Semantic-Aware Token Aggregation Block for long-range
semantic consistency, and a Spatial-Frequency Fusion Block for fine structure
restoration. Experiments on public and private datasets show that SSCM achieves
state-of-the-art performance with fewer parameters while ensuring spatially and
semantically consistent reconstructions.

</details>


### [46] [OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation](https://arxiv.org/abs/2509.18600)
*Zhuoxiao Chen,Hongyang Yu,Ying Xu,Yadan Luo,Long Duong,Yuan-Fang Li*

Main category: cs.CV

TL;DR: OraPO with FactScore enables RL-only single-stage training for radiology report generation under constrained budgets, achieving SOTA on CheXpert Plus with far less data.


<details>
  <summary>Details</summary>
Motivation: Reduce data/compute demands of radiology report generation pipelines while preserving clinical fidelity, and address rare/difficult cases by leveraging an oracle and fact-grounded rewards.

Method: OraPO converts failed GRPO explorations on rare/difficult studies into direct preference supervision via a lightweight oracle step. FactS extracts atomic clinical facts and checks their entailment against ground-truth labels to generate dense, interpretable sentence-level rewards.

Result: Significant improvement in learning efficiency on challenging cases; achieves new SOTA on CheXpert Plus with F1 of 0.341, using 2–3 orders of magnitude less training data and a small base vision-language model on modest hardware.

Conclusion: A compact, powerful framework that combines oracle-guided single-stage RL with fact-grounded rewards to deliver high performance in RRG under budget constraints.

Abstract: Radiology report generation (RRG) aims to automatically produce clinically
faithful reports from chest X-ray images. Prevailing work typically follows a
scale-driven paradigm, by multi-stage training over large paired corpora and
oversized backbones, making pipelines highly data- and compute-intensive. In
this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based
reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables
single-stage, RL-only training by converting failed GRPO explorations on rare
or difficult studies into direct preference supervision via a lightweight
oracle step. FactS grounds learning in diagnostic evidence by extracting atomic
clinical facts and checking entailment against ground-truth labels, yielding
dense, interpretable sentence-level rewards. Together, OraPO and FactS create a
compact and powerful framework that significantly improves learning efficiency
on clinically challenging cases, setting the new SOTA performance on the
CheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training
data using a small base VLM on modest hardware.

</details>


### [47] [Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation](https://arxiv.org/abs/2509.18602)
*Xu Liu,Yibo Lu,Xinxian Wang,Xinyu Wu*

Main category: cs.CV

TL;DR: AMSF enables controllable fusion of multiple reference styles in diffusion models without fine-tuning, via semantic-token decomposition injected into cross-attention and a similarity-aware re-weighting mechanism; scalable to multiple styles and outperforms SOTA.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of existing reference-based methods: they typically accept only a single style image and lack a principled way to balance multiple styles; AMSF aims for training-free, scalable, controllable multi-style generation.

Method: Encode all style images and textual hints with a semantic token decomposition module; inject these tokens into every cross-attention layer of a frozen diffusion model; employ a similarity-aware re-weighting module to recalibrate attention to each style component at every denoising step; no fine-tuning or external adapters.

Result: Qualitative and quantitative evaluations indicate AMSF yields multi-style fusion results that consistently outperform state-of-the-art approaches; the design scales seamlessly to two or more styles.

Conclusion: AMSF provides a practical, scalable solution for expressive multi-style diffusion generation without training, enabling balanced, user-controllable fusion of multiple reference styles.

Abstract: We propose Adaptive Multi-Style Fusion (AMSF), a reference-based
training-free framework that enables controllable fusion of multiple reference
styles in diffusion models. Most of the existing reference-based methods are
limited by (a) acceptance of only one style image, thus prohibiting hybrid
aesthetics and scalability to more styles, and (b) lack of a principled
mechanism to balance several stylistic influences. AMSF mitigates these
challenges by encoding all style images and textual hints with a semantic token
decomposition module that is adaptively injected into every cross-attention
layer of an frozen diffusion model. A similarity-aware re-weighting module then
recalibrates, at each denoising step, the attention allocated to every style
component, yielding balanced and user-controllable blends without any
fine-tuning or external adapters. Both qualitative and quantitative evaluations
show that AMSF produces multi-style fusion results that consistently outperform
the state-of-the-art approaches, while its fusion design scales seamlessly to
two or more styles. These capabilities position AMSF as a practical step toward
expressive multi-style generation in diffusion models.

</details>


### [48] [MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2509.18613)
*Yuzhi Wu,Li Xiao,Jun Liu,Guangfeng Jiang,XiangGen Xia*

Main category: cs.CV

TL;DR: A two-stage multi-level fusion framework (MLF-4DRCNet) for 3D object detection using 4D radar and camera, addressing radar sparsity with point-, scene-, and proposal-level fusion; achieves state-of-the-art results on VoD and TJ4DRadSet, rivaling LiDAR-based methods on VoD.


<details>
  <summary>Details</summary>
Motivation: 4D mmWave radar offers cost-effective, robust sensing but produces sparse/noisy point clouds, limiting standalone 3D detection. Existing BEV fusion methods borrowed from LiDAR-camera setups fail to handle radar-specific sparsity and incomplete geometry. There is a need for multi-level fusion that exploits radar geometry and rich image context to improve detection.

Method: A two-stage framework (MLF-4DRCNet) comprising: (1) Enhanced Radar Point Encoder (ERPE) that densifies radar points with 2D image instances and encodes into voxels via Triple-Attention Voxel Feature Encoder; (2) Hierarchical Scene Fusion Pooling (HSFP) that adaptively fuses multi-scale voxel features with 2D image features using deformable attention and pooling to capture scene context; (3) Proposal-Level Fusion Enhancement (PLFE) that refines region proposals by fusing image features and integrating with HSFP-pooled features. Performs point-level densitying, scene-level fusion, then proposal-level refinement in a two-stage pipeline.

Result: Experiments on VoD and TJ4DRadSet show state-of-the-art performance; notably, the method achieves results comparable to LiDAR-based models on the VoD dataset.

Conclusion: Multi-level fusion tailored to 4D radar geometry—point, scene, and proposal levels—effectively mitigates radar sparsity and yields superior 3D object detection performance in radar-camera fusion, marking a significant advance over traditional BEV fusion approaches.

Abstract: The emerging 4D millimeter-wave radar, measuring the range, azimuth,
elevation, and Doppler velocity of objects, is recognized for its
cost-effectiveness and robustness in autonomous driving. Nevertheless, its
point clouds exhibit significant sparsity and noise, restricting its standalone
application in 3D object detection. Recent 4D radar-camera fusion methods have
provided effective perception. Most existing approaches, however, adopt
explicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera
fusion, neglecting radar's inherent drawbacks. Specifically, they overlook the
sparse and incomplete geometry of radar point clouds and restrict fusion to
coarse scene-level integration. To address these problems, we propose
MLF-4DRCNet, a novel two-stage framework for 3D object detection via
multi-level fusion of 4D radar and camera images. Our model incorporates the
point-, scene-, and proposal-level multi-modal information, enabling
comprehensive feature representation. It comprises three crucial components:
the Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion
Pooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.
Operating at the point-level, ERPE densities radar point clouds with 2D image
instances and encodes them into voxels via the proposed Triple-Attention Voxel
Feature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D
image features using deformable attention to capture scene context and adopts
pooling to the fused features. PLFE refines region proposals by fusing image
features, and further integrates with the pooled features from HSFP.
Experimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets
demonstrate that MLF-4DRCNet achieves the state-of-the-art performance.
Notably, it attains performance comparable to LiDAR-based models on the VoD
dataset.

</details>


### [49] [Prompt-Guided Dual Latent Steering for Inversion Problems](https://arxiv.org/abs/2509.18619)
*Yichen Wu,Xu Liu,Chenxuan Zhao,Xinyu Wu*

Main category: cs.CV

TL;DR: PDLS is a training-free dual latent steering framework for diffusion inversion that uses a structural path plus a prompt-guided semantic path, solved with a closed-form LQR controller, improving both fidelity to the source and semantic alignment over single-latent baselines.


<details>
  <summary>Details</summary>
Motivation: Single-latent inversions of corrupted images into diffusion model latent spaces struggle to balance structural fidelity with semantic accuracy, often causing semantic drift and blurring. A method that preserves source structure while aligning semantic content is needed.

Method: PDLS decomposes the inversion into two streams: a structural path to preserve source integrity and a semantic path guided by a prompt. The dual guidance is formulated as an optimal control problem and solved in closed form via a Linear Quadratic Regulator (LQR). The controller dynamically steers the generative trajectory at each step. The approach is training-free and built on Rectified Flow models for stable inversion paths, avoiding per-image optimization.

Result: Extensive experiments on FFHQ-1K and ImageNet-1K across Gaussian deblurring, motion deblurring, super-resolution, and freeform inpainting show that PDLS yields reconstructions that are more faithful to the original image and better aligned with semantic information than single-latent baselines.

Conclusion: PDLS offers a training-free, dual-guidance inversion framework that improves both structural fidelity and semantic alignment, demonstrating versatility across diverse inversion tasks.

Abstract: Inverting corrupted images into the latent space of diffusion models is
challenging. Current methods, which encode an image into a single latent
vector, struggle to balance structural fidelity with semantic accuracy, leading
to reconstructions with semantic drift, such as blurred details or incorrect
attributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering
(PDLS), a novel, training-free framework built upon Rectified Flow models for
their stable inversion paths. PDLS decomposes the inversion process into two
complementary streams: a structural path to preserve source integrity and a
semantic path guided by a prompt. We formulate this dual guidance as an optimal
control problem and derive a closed-form solution via a Linear Quadratic
Regulator (LQR). This controller dynamically steers the generative trajectory
at each step, preventing semantic drift while ensuring the preservation of fine
detail without costly, per-image optimization. Extensive experiments on FFHQ-1K
and ImageNet-1K under various inversion tasks, including Gaussian deblurring,
motion deblurring, super-resolution and freeform inpainting, demonstrate that
PDLS produces reconstructions that are both more faithful to the original image
and better aligned with the semantic information than single-latent baselines.

</details>


### [50] [Learning neuroimaging models from health system-scale data](https://arxiv.org/abs/2509.18638)
*Yiwei Lyu,Samir Harake,Asadur Chowdury,Soumyanil Banerjee,Rachel Gologorsky,Shixuan Liu,Anna-Katharina Meissner,Akshay Rao,Chenhui Zhao,Akhil Kondepudi,Cheng Jiang,Xinhai Hou,Rushikesh S. Joshi,Volker Neuschmelting,Ashok Srinivasan,Dawn Kleindorfer,Brian Athey,Vikas Gulani,Aditya Pandey,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: Prima is a health-system–trained vision-language model for neuroimaging that achieves high diagnostic performance on MRI studies and improves radiology workflow, while addressing fairness across diverse systems.


<details>
  <summary>Details</summary>
Motivation: Rising MRI demand strains health systems, lengthening turnaround times and increasing clinician burnout, with disproportionate impact on low-resource and rural settings; scalable AI that can support clinical neuroimaging is urgently needed.

Method: Trained on over 220,000 MRI studies using a hierarchical vision architecture to capture general and transferable MRI features. Evaluated in a 1-year health-system-wide study with 30,000 MRI studies across 52 neurologic diagnoses, providing explainable differential diagnoses, worklist prioritization for radiologists, and clinical referral recommendations.

Result: Mean diagnostic AUC of 92.0 across 52 diagnoses, outperforming other state-of-the-art models; supports explainability, prioritization, and referral features across diverse patient demographics and MRI systems; demonstrates algorithmic fairness across sensitive groups.

Conclusion: Health-system-scale vision-language models like Prima have transformative potential for AI-driven neuroimaging, enabling faster, fairer, and more efficient care delivery across heterogeneous MRI systems.

Abstract: Neuroimaging is a ubiquitous tool for evaluating patients with neurological
diseases. The global demand for magnetic resonance imaging (MRI) studies has
risen steadily, placing significant strain on health systems, prolonging
turnaround times, and intensifying physician burnout \cite{Chen2017-bt,
Rula2024-qp-1}. These challenges disproportionately impact patients in
low-resource and rural settings. Here, we utilized a large academic health
system as a data engine to develop Prima, the first vision language model (VLM)
serving as an AI foundation for neuroimaging that supports real-world, clinical
MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a
hierarchical vision architecture that provides general and transferable MRI
features. Prima was tested in a 1-year health system-wide study that included
30K MRI studies. Across 52 radiologic diagnoses from the major neurologic
disorders, including neoplastic, inflammatory, infectious, and developmental
lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,
outperforming other state-of-the-art general and medical AI models. Prima
offers explainable differential diagnoses, worklist priority for radiologists,
and clinical referral recommendations across diverse patient demographics and
MRI systems. Prima demonstrates algorithmic fairness across sensitive groups
and can help mitigate health system biases, such as prolonged turnaround times
for low-resource populations. These findings highlight the transformative
potential of health system-scale VLMs and Prima's role in advancing AI-driven
healthcare.

</details>


### [51] [Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation](https://arxiv.org/abs/2509.18639)
*Yuanhuiyi Lyu,Chi Kit Wong,Chenfei Liao,Lutao Jiang,Xu Zheng,Zexin Lu,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: UiG (Understanding-in-Generation) proposes a framework to infuse the model's understanding into the generation process for text-to-image tasks via an Image Editing bridge, iteratively refining images and embedding understanding into editing instructions; shows ~3.92% gain on TIIF long-prompt benchmark; code provided.


<details>
  <summary>Details</summary>
Motivation: Current Chain-of-Thought approaches for unified text-to-image models separate understanding and generation, limiting their ability to use understanding to improve generation. UiG aims to tightly couple understanding with generation to address the deficiencies of generative capabilities.

Method: Introduce an Image Editing bridge that first verifies the generated image and then incorporates the model's understanding into editing instructions, followed by iterative refinements that gradually inject understanding into the generation process during image editing steps.

Result: Significant performance improvement in text-to-image generation compared to existing reasoning methods, e.g., 3.92% gain on the long-prompt setting of the TIIF benchmark.

Conclusion: UiG demonstrates that leveraging a unified model's understanding within an editing-guided generation loop can mitigate generative limitations and enhance image quality, with code available for reproducibility.

Abstract: Recent works have made notable advancements in enhancing unified models for
text-to-image generation through the Chain-of-Thought (CoT). However, these
reasoning methods separate the processes of understanding and generation, which
limits their ability to guide the reasoning of unified models in addressing the
deficiencies of their generative capabilities. To this end, we propose a novel
reasoning framework for unified models, Understanding-in-Generation (UiG),
which harnesses the robust understanding capabilities of unified models to
reinforce their performance in image generation. The core insight of our UiG is
to integrate generative guidance by the strong understanding capabilities
during the reasoning process, thereby mitigating the limitations of generative
abilities. To achieve this, we introduce "Image Editing" as a bridge to infuse
understanding into the generation process. Initially, we verify the generated
image and incorporate the understanding of unified models into the editing
instructions. Subsequently, we enhance the generated image step by step,
gradually infusing the understanding into the generation process. Our UiG
framework demonstrates a significant performance improvement in text-to-image
generation over existing text-to-image reasoning methods, e.g., a 3.92% gain on
the long prompt setting of the TIIF benchmark. The project code:
https://github.com/QC-LY/UiG

</details>


### [52] [Zero-shot Monocular Metric Depth for Endoscopic Images](https://arxiv.org/abs/2509.18642)
*Nicolas Toussaint,Emanuele Colleoni,Ricardo Sanchez-Matilla,Joshua Sutcliffe,Vanessa Thompson,Muhammad Asad,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: A benchmark and synthetic dataset for endoscopic depth estimation; highlights generalization gaps and shows fine-tuning with EndoSynth boosts performance on unseen real data.


<details>
  <summary>Details</summary>
Motivation: There is a lack of robust benchmarks and high-quality datasets for depth estimation in endoscopy, hindering generalization to clinical scenarios.

Method: Benchmark state-of-the-art monocular depth models (relative and metric) on unseen real endoscopic images; create EndoSynth synthetic dataset with ground-truth metric depth and segmentation masks; fine-tune depth foundation models on EndoSynth; release code, data, and weights.

Result: Fine-tuning with EndoSynth yields significant accuracy gains on most unseen real data; the benchmark reveals generalization patterns; EndoSynth and weights released.

Conclusion: Providing a benchmark and synthetic dataset accelerates endoscopic depth estimation research by improving generalization and offering resources for future work.

Abstract: Monocular relative and metric depth estimation has seen a tremendous boost in
the last few years due to the sharp advancements in foundation models and in
particular transformer based networks. As we start to see applications to the
domain of endoscopic images, there is still a lack of robust benchmarks and
high-quality datasets in that area. This paper addresses these limitations by
presenting a comprehensive benchmark of state-of-the-art (metric and relative)
depth estimation models evaluated on real, unseen endoscopic images, providing
critical insights into their generalisation and performance in clinical
scenarios. Additionally, we introduce and publish a novel synthetic dataset
(EndoSynth) of endoscopic surgical instruments paired with ground truth metric
depth and segmentation masks, designed to bridge the gap between synthetic and
real-world data. We demonstrate that fine-tuning depth foundation models using
our synthetic dataset boosts accuracy on most unseen real data by a significant
margin. By providing both a benchmark and a synthetic dataset, this work
advances the field of depth estimation for endoscopic images and serves as an
important resource for future research. Project page, EndoSynth dataset and
trained weights are available at https://github.com/TouchSurgery/EndoSynth.

</details>


### [53] [LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection](https://arxiv.org/abs/2509.18683)
*Lanhu Wu,Zilin Gao,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: LEAF-Mamba: a local-emphatic state-space model plus adaptive fusion for RGB-D SOD that achieves state-of-the-art performance with linear complexity and generalizes to RGB-T.


<details>
  <summary>Details</summary>
Motivation: CNNs' local receptive fields limit; Vision Transformers' quadratic cost; state-space models (SSMs) offer linear complexity for long-range dependencies but struggle with local semantics and cross-modality fusion in RGB-D SOD.

Method: 1) Local Emphatic SSM (LE-SSM) to capture multi-scale local dependencies for both RGB and depth modalities; 2) SSM-based Adaptive Fusion Module (AFM) for complementary cross-modality interaction and reliable cross-modality integration.

Result: LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in accuracy and efficiency; demonstrates strong generalization to RGB-T SOD.

Conclusion: LEAF-Mamba validates an effective, efficient multimodal SOD approach based on state-space models, with strong cross-modality fusion and generalization potential to related tasks.

Abstract: RGB-D salient object detection (SOD) aims to identify the most conspicuous
objects in a scene with the incorporation of depth cues. Existing methods
mainly rely on CNNs, limited by the local receptive fields, or Vision
Transformers that suffer from the cost of quadratic complexity, posing a
challenge in balancing performance and computational efficiency. Recently,
state space models (SSM), Mamba, have shown great potential for modeling
long-range dependency with linear complexity. However, directly applying SSM to
RGB-D SOD may lead to deficient local semantics as well as the inadequate
cross-modality fusion. To address these issues, we propose a Local Emphatic and
Adaptive Fusion state space model (LEAF-Mamba) that contains two novel
components: 1) a local emphatic state space module (LE-SSM) to capture
multi-scale local dependencies for both modalities. 2) an SSM-based adaptive
fusion module (AFM) for complementary cross-modality interaction and reliable
cross-modality integration. Extensive experiments demonstrate that the
LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in
both efficacy and efficiency. Moreover, our method can achieve excellent
performance on the RGB-T SOD task, proving a powerful generalization ability.

</details>


### [54] [Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification](https://arxiv.org/abs/2509.18692)
*Xinle Gao,Linghui Ye,Zhiyong Xiao*

Main category: cs.CV

TL;DR: A lightweight Vision Transformer-based food image classifier using Window Multi-Head Attention (WMHAM) and Spatial Attention (SAM) achieves high accuracy on Food-101 and Vireo Food-172 with substantially reduced parameters and FLOPs, enabling deployment on resource-constrained devices.


<details>
  <summary>Details</summary>
Motivation: Food-image classification is essential for automated quality control, safety, and smart agriculture, but ViT models are computationally heavy; there is a need for efficient alternatives for production environments.

Method: Integrates WMHAM to capture local and global context via window partitioning; SAM to emphasize discriminative spatial regions; lightweight architecture with fewer parameters and FLOPs; evaluated on Food-101 and Vireo Food-172.

Result: Accuracies: 95.24% (Food-101) and 94.33% (Vireo Food-172); notable reductions in parameters and FLOPs vs baselines; demonstrates good efficiency-performance balance.

Conclusion: The proposed WMHAM+SAM approach is suitable for resource-constrained deployment and supports automated quality control and safety supervision in the food industry; offers an effective trade-off between accuracy and efficiency.

Abstract: With the rapid development of society and continuous advances in science and
technology, the food industry increasingly demands higher production quality
and efficiency. Food image classification plays a vital role in enabling
automated quality control on production lines, supporting food safety
supervision, and promoting intelligent agricultural production. However, this
task faces challenges due to the large number of parameters and high
computational complexity of Vision Transformer models. To address these issues,
we propose a lightweight food image classification algorithm that integrates a
Window Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism
(SAM). The WMHAM reduces computational cost by capturing local and global
contextual features through efficient window partitioning, while the SAM
adaptively emphasizes key spatial regions to improve discriminative feature
representation. Experiments conducted on the Food-101 and Vireo Food-172
datasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,
respectively, while significantly reducing parameters and FLOPs compared with
baseline methods. These results confirm that the proposed approach achieves an
effective balance between computational efficiency and classification
performance, making it well-suited for deployment in resource-constrained
environments.

</details>


### [55] [OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery](https://arxiv.org/abs/2509.18693)
*Siyi Chen,Kai Wang,Weicong Pang,Ruiming Yang,Ziru Chen,Renjun Gao,Alexis Kai Hon Lau,Dasa Gu,Chenchen Zhang,Cheng Li*

Main category: cs.CV

TL;DR: OSDA is a three-stage, annotation-free framework for open-set land-cover discovery, segmentation, and description, combining a promptable SAM for discovery, a two-phase multimodal LLM for semantic attribution, and LLM-based evaluation.


<details>
  <summary>Details</summary>
Motivation: Open-set land-cover analysis requires detecting novel objects without labels and assigning semantic labels while enabling fine-grained localization; current methods struggle with scalable, label-free open-world interpretation in satellite imagery.

Method: Three-stage pipeline: (1) promptable fine-tuned segmentation model (SAM) for precise discovery and mask extraction; (2) two-phase fine-tuned multimodal large language model (MLLM) for semantic attribution and contextual description; (3) LLM-as-judge with manual scoring of MLLMs evaluation; architecture-agnostic and label-free.

Result: Demonstrates robust evaluation across diverse satellite imagery, achieving a blend of pixel-level accuracy and high-level semantic understanding; indicates scalability and interpretability for dynamic land-cover monitoring and automated cartographic updating.

Conclusion: OSDA provides a scalable, interpretable, annotation-free framework for open-set land-cover interpretation in remote sensing.

Abstract: Open-set land-cover analysis in remote sensing requires the ability to
achieve fine-grained spatial localization and semantically open categorization.
This involves not only detecting and segmenting novel objects without
categorical supervision but also assigning them interpretable semantic labels
through multimodal reasoning. In this study, we introduce OSDA, an integrated
three-stage framework for annotation-free open-set land-cover discovery,
segmentation, and description. The pipeline consists of: (1) precise discovery
and mask extraction with a promptable fine-tuned segmentation model (SAM), (2)
semantic attribution and contextual description via a two-phase fine-tuned
multimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring
of the MLLMs evaluation. By combining pixel-level accuracy with high-level
semantic understanding, OSDA addresses key challenges in open-world remote
sensing interpretation. Designed to be architecture-agnostic and label-free,
the framework supports robust evaluation across diverse satellite imagery
without requiring manual annotation. Our work provides a scalable and
interpretable solution for dynamic land-cover monitoring, showing strong
potential for automated cartographic updating and large-scale earth observation
analysis.

</details>


### [56] [Overview of PlantCLEF 2021: cross-domain plant identification](https://arxiv.org/abs/2509.18697)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: Assesses whether herbarium collections can boost automated plant ID in data-poor tropical regions via cross-domain learning, using LifeCLEF 2021 PlantID dataset focused on the Guiana Shield; training on hundreds of thousands of herbarium sheets with 5 trait values, testing on field photos; analyzes approaches and results across participating groups.


<details>
  <summary>Details</summary>
Motivation: To bridge data-poor biodiversity regions with extensive herbarium digitization, enabling better automated identification of flora by leveraging cross-domain data and trait metadata.

Method: Cross-domain classification with a training set of herbarium sheets (plus location, date, author, taxonomy) and 5 species traits; test set consists of field photos; evaluation summarizes the strategies and systems used by participants and analyzes results.

Result: The paper provides resources, evaluation framework, and a synthesis of approaches and systems used by participants, along with an analysis of the main results highlighting the potential and challenges of cross-domain plant identification using herbarium data.

Conclusion: Herbarium data and trait metadata can aid field-image identification in data-poor regions, but cross-domain gaps remain; the work establishes a dataset, evaluation protocol, and baseline analyses to drive future improvements.

Abstract: Automated plant identification has improved considerably thanks to recent
advances in deep learning and the availability of training data with more and
more field photos. However, this profusion of data concerns only a few tens of
thousands of species, mainly located in North America and Western Europe, much
less in the richest regions in terms of biodiversity such as tropical
countries. On the other hand, for several centuries, botanists have
systematically collected, catalogued and stored plant specimens in herbaria,
especially in tropical regions, and recent efforts by the biodiversity
informatics community have made it possible to put millions of digitised
records online. The LifeCLEF 2021 plant identification challenge (or "PlantCLEF
2021") was designed to assess the extent to which automated identification of
flora in data-poor regions can be improved by using herbarium collections. It
is based on a dataset of about 1,000 species mainly focused on the Guiana
Shield of South America, a region known to have one of the highest plant
diversities in the world. The challenge was evaluated as a cross-domain
classification task where the training set consisted of several hundred
thousand herbarium sheets and a few thousand photos to allow learning a
correspondence between the two domains. In addition to the usual metadata
(location, date, author, taxonomy), the training data also includes the values
of 5 morphological and functional traits for each species. The test set
consisted exclusively of photos taken in the field. This article presents the
resources and evaluations of the assessment carried out, summarises the
approaches and systems used by the participating research groups and provides
an analysis of the main results.

</details>


### [57] [AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping](https://arxiv.org/abs/2509.18699)
*Zedong Zhang,Ying Tai,Jianjun Qian,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: AGSwap achieves superior cross-category object fusion in text-to-image generation via Group-wise Embedding Swapping and Adaptive Group Updating; introduces COF dataset; outperforms SOTA methods like GPT-Image-1 on both simple and complex prompts.


<details>
  <summary>Details</summary>
Motivation: Cross-category fusion is desired but existing methods produce biased, visually chaotic, or semantically inconsistent results due to overlapping artifacts and the lack of a comprehensive benchmark. The authors address these gaps with a new method (AGSwap) and a large-scale dataset (COF).

Method: Two components: (1) Group-wise Embedding Swapping that fuses semantic attributes from different concepts through feature manipulation; (2) Adaptive Group Updating, a dynamic optimization guided by a balance evaluation score to ensure coherent synthesis. COF is a large-scale, hierarchically structured dataset built on ImageNet-1K and WordNet, with 95 superclasses, 10 subclasses each, enabling 451,250 unique fusion pairs.

Result: AGSwap outperforms state-of-the-art compositional T2I methods, including GPT-Image-1, using both simple and complex prompts. The COF dataset provides a robust benchmark for cross-category fusion tasks.

Conclusion: AGSwap offers an effective solution for cross-category object fusion in text-to-image generation, and the COF dataset supplies a scalable benchmark to advance future research in compositional T2I.

Abstract: Fusing cross-category objects to a single coherent object has gained
increasing attention in text-to-image (T2I) generation due to its broad
applications in virtual reality, digital media, film, and gaming. However,
existing methods often produce biased, visually chaotic, or semantically
inconsistent results due to overlapping artifacts and poor integration.
Moreover, progress in this field has been limited by the absence of a
comprehensive benchmark dataset. To address these problems, we propose
\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective
approach comprising two key components: (1) Group-wise Embedding Swapping,
which fuses semantic attributes from different concepts through feature
manipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism
guided by a balance evaluation score to ensure coherent synthesis.
Additionally, we introduce \textbf{Cross-category Object Fusion (COF)}, a
large-scale, hierarchically structured dataset built upon ImageNet-1K and
WordNet. COF includes 95 superclasses, each with 10 subclasses, enabling
451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap
outperforms state-of-the-art compositional T2I methods, including GPT-Image-1
using simple and complex prompts.

</details>


### [58] [Overview of LifeCLEF Plant Identification task 2019: diving into data deficient tropical countries](https://arxiv.org/abs/2509.18705)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: LifeCLEF 2019 Plant Identification challenges automated plant id in data-deprived tropical flora using a 10k-species dataset from the Guiana Shield and Northern Amazon; reports resources, approaches, and outcomes, and compares to tropical flora experts.


<details>
  <summary>Details</summary>
Motivation: Address the gap between the vast plant diversity (roughly 369k species) and the limited AI training data (tens of thousands), by evaluating automated identification in data-deficient, biodiverse regions and benchmarking against expert performance.

Method: Describes the challenge design, dataset, and evaluation protocol; surveys the resources and assessments of the PlantCLEF 2019 challenge; summarizes the approaches and systems used by participating groups; provides an analysis of the main outcomes.

Result: The paper presents the challenge resources and assessments, summarizes participating methods and systems, and analyzes the main outcomes of the PlantCLEF 2019 competition.

Conclusion: PlantCLEF 2019 offers a benchmark and insights for automated plant identification in data-deficient tropical regions, highlighting current strengths, weaknesses, and directions for future research, and providing resources to the community.

Abstract: Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data.
However, this profusion of data only concerns a few tens of thousands of
species, while the planet has nearly 369K. The LifeCLEF 2019 Plant
Identification challenge (or "PlantCLEF 2019") was designed to evaluate
automated identification on the flora of data deficient regions. It is based on
a dataset of 10K species mainly focused on the Guiana shield and the Northern
Amazon rainforest, an area known to have one of the greatest diversity of
plants and animals in the world. As in the previous edition, a comparison of
the performance of the systems evaluated with the best tropical flora experts
was carried out. This paper presents the resources and assessments of the
challenge, summarizes the approaches and systems employed by the participating
research groups, and provides an analysis of the main outcomes.

</details>


### [59] [RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images](https://arxiv.org/abs/2509.18711)
*Ke Li,Di Wang,Ting Wang,Fuyu Dong,Yiming Zhang,Luyao Zhang,Xiangyu Wang,Shaofeng Li,Quan Wang*

Main category: cs.CV

TL;DR: A training-free, zero-shot open-vocabulary remote sensing visual grounding framework (RSVG-ZeroOV) that uses frozen foundation models to localize objects with natural language queries, without task-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Overcome closed-set vocabularies and expensive fine-tuning in open-world remote sensing grounding by leveraging frozen generic foundation models in a zero-shot setting.

Method: Three-stage pipeline: (i) Overview uses a vision-language model (VLM) to extract cross-attention maps linking text queries to image regions; (ii) Focus leverages diffusion model priors to enrich structural and shape details missing from VLM; (iii) Evolve employs an attention evolution module to suppress irrelevant activations and produce purified segmentation masks, all without task-specific training.

Result: Extensive experiments show the proposed framework consistently outperforms existing weakly-supervised and zero-shot methods in open-vocabulary RSVG.

Conclusion: RSVG-ZeroOV demonstrates an efficient, scalable training-free approach for zero-shot open-vocabulary remote sensing grounding by effectively fusing frozen VLMs, diffusion priors, and an attention-evolution mechanism to produce accurate object masks without fine-tuning.

Abstract: Remote sensing visual grounding (RSVG) aims to localize objects in remote
sensing images based on free-form natural language expressions. Existing
approaches are typically constrained to closed-set vocabularies, limiting their
applicability in open-world scenarios. While recent attempts to leverage
generic foundation models for open-vocabulary RSVG, they overly rely on
expensive high-quality datasets and time-consuming fine-tuning. To address
these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework
that aims to explore the potential of frozen generic foundation models for
zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key
stages: (i) Overview: We utilize a vision-language model (VLM) to obtain
cross-attention\footnote[1]{In this paper, although decoder-only VLMs use
self-attention over all tokens, we refer to the image-text interaction part as
cross-attention to distinguish it from pure visual self-attention.}maps that
capture semantic correlations between text queries and visual regions. (ii)
Focus: By leveraging the fine-grained modeling priors of a diffusion model
(DM), we fill in gaps in structural and shape information of objects, which are
often overlooked by VLM. (iii) Evolve: A simple yet effective attention
evolution module is introduced to suppress irrelevant activations, yielding
purified segmentation masks over the referred objects. Without cumbersome
task-specific training, RSVG-ZeroOV offers an efficient and scalable solution.
Extensive experiments demonstrate that the proposed framework consistently
outperforms existing weakly-supervised and zero-shot methods.

</details>


### [60] [What Makes You Unique? Attribute Prompt Composition for Object Re-Identification](https://arxiv.org/abs/2509.18715)
*Yingquan Wang,Pingping Zhang,Chong Sun,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: An Attribute Prompt Composition (APC) framework uses a Semantic Attribute Dictionary and a Prompt Composition Module to create discriminative attribute-aware features, combined with a Fast-Slow Training Strategy to balance discrimination and generalization, achieving state-of-the-art results on conventional and Domain Generalized ReID datasets.


<details>
  <summary>Details</summary>
Motivation: Single-domain ReID models tend to overfit domain-specific cues, while cross-domain methods rely on normalization that can suppress identity-discriminative features. Leveraging textual semantics and the strong generalization of Vision-Language Models (VLMs) can jointly enhance discrimination and generalization for ReID.

Method: APC comprises a Semantic Attribute Dictionary (SAD), an over-complete attribute set that provides rich semantic descriptions, and a Prompt Composition Module (PCM) that adaptively selects and composes relevant attributes from SAD to generate discriminative attribute-aware features. To balance discrimination and generalization, a Fast-Slow Training Strategy (FSTS) is used, with a Fast Update Stream (FUS) that rapidly captures ReID-specific discriminative knowledge and a Slow Update Stream (SUS) that retains the generalizable knowledge from the pre-trained VLM, with mutual interaction between streams.

Result: Experimental evaluations on both conventional and Domain Generalized (DG) ReID datasets show that APC surpasses state-of-the-art methods in both discrimination and generalization.

Conclusion: APC demonstrates that integrating semantic attribute prompts with a dual-stream training regimen effectively enhances both discrimination and generalization in ReID, suggesting a promising direction for leveraging textual semantics and VLMs in practical DG ReID scenarios; the authors provide code at the given repository.

Abstract: Object Re-IDentification (ReID) aims to recognize individuals across
non-overlapping camera views. While recent advances have achieved remarkable
progress, most existing models are constrained to either single-domain or
cross-domain scenarios, limiting their real-world applicability. Single-domain
models tend to overfit to domain-specific features, whereas cross-domain models
often rely on diverse normalization strategies that may inadvertently suppress
identity-specific discriminative cues. To address these limitations, we propose
an Attribute Prompt Composition (APC) framework, which exploits textual
semantics to jointly enhance discrimination and generalization. Specifically,
we design an Attribute Prompt Generator (APG) consisting of a Semantic
Attribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an
over-complete attribute dictionary to provide rich semantic descriptions, while
PCM adaptively composes relevant attributes from SAD to generate discriminative
attribute-aware features. In addition, motivated by the strong generalization
ability of Vision-Language Models (VLM), we propose a Fast-Slow Training
Strategy (FSTS) to balance ReID-specific discrimination and generalizable
representation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)
to rapidly acquire ReID-specific discriminative knowledge and a Slow Update
Stream (SUS) to retain the generalizable knowledge inherited from the
pre-trained VLM. Through a mutual interaction, the framework effectively
focuses on ReID-relevant features while mitigating overfitting. Extensive
experiments on both conventional and Domain Generalized (DG) ReID datasets
demonstrate that our framework surpasses state-of-the-art methods, exhibiting
superior performances in terms of both discrimination and generalization. The
source code is available at https://github.com/AWangYQ/APC.

</details>


### [61] [Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment](https://arxiv.org/abs/2509.18717)
*Tong Zhang,Kuofeng Gao,Jiawang Bai,Leo Yu Zhang,Xin Yin,Zonghui Wang,Shouling Ji,Wenzhi Chen*

Main category: cs.CV

TL;DR: An optimal-transport based framework (OTCCLIP) reconstructs image-caption pairs using fine-grained features to defend CLIP against data poisoning/backdoor attacks and improves downstream robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: CLIP pretraining on web-crawled data is vulnerable to targeted data poisoning and backdoors; existing defenses based on global image-caption matching can mis-match at fine-grained level, harming training.

Method: Introduce OTCCLIP: construct a new OT-based distance between fine-grained visual and textual feature sets, re-assign captions per image using this distance; employ OT-based objectives to encourage inter- and intra-modal alignment to reduce mismatched pairs.

Result: OTCCLIP reduces attack success rates of poisoning attacks and significantly improves CLIP's zero-shot and linear probing performance on poisoned datasets compared with prior methods.

Conclusion: OTCCLIP provides a robust, fine-grained, transport-based defense for CLIP pretraining that preserves performance while mitigating poisoning risks.

Abstract: Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)
models are threatened by targeted data poisoning and backdoor attacks due to
massive training image-caption pairs crawled from the Internet. Previous
defense methods correct poisoned image-caption pairs by matching a new caption
for each image. However, the matching process relies solely on the global
representations of images and captions, overlooking fine-grained features of
visual and textual features. It may introduce incorrect image-caption pairs and
harm the CLIP pre-training. To address their limitations, we propose an Optimal
Transport-based framework to reconstruct image-caption pairs, named OTCCLIP. We
propose a new optimal transport-based distance measure between fine-grained
visual and textual feature sets and re-assign new captions based on the
proposed optimal transport distance. Additionally, to further reduce the
negative impact of mismatched pairs, we encourage the inter- and intra-modality
fine-grained alignment by employing optimal transport-based objective
functions. Our experiments demonstrate that OTCCLIP can successfully decrease
the attack success rates of poisoning attacks. Also, compared to previous
methods, OTCCLIP significantly improves CLIP's zero-shot and linear probing
performance trained on poisoned datasets.

</details>


### [62] [Knowledge Transfer from Interaction Learning](https://arxiv.org/abs/2509.18733)
*Yilin Gao,Kangyi Chen,Zhongxing Peng,Hengjie Lu,Shugong Xu*

Main category: cs.CV

TL;DR: Learning from Interactions (LFI) models dynamic cross-modal interaction patterns learned from VLMs to transfer knowledge to VFMs, via Interaction Queries and interaction-based supervision, achieving strong gains with efficient transfer.


<details>
  <summary>Details</summary>
Motivation: VFMs struggle to transfer knowledge from VLMs due to a representational gap and result-oriented training; capturing interaction dynamics can improve generalization across vision tasks.

Method: Two innovations: 1) Interaction Queries to preserve persistent relational structures across layers; 2) interaction-based supervision derived from cross-modal attention in VLMs; training VFMs to imitate these interactions.

Result: On TinyImageNet classification and COCO detection/segmentation, gains of 3.3 and 1.6 mAP/2.4 AP; zero-shot improvements on PACS (2.4) and VLCS (9.3); faster convergence and minimal parameter overhead; human evaluations show 2.7x higher semantic consistency than result-oriented baselines.

Conclusion: Modeling interactive processes enables more faithful knowledge transfer from VLMs to VFMs, improving cross-task and cross-domain generalization and cognitive alignment.

Abstract: Current visual foundation models (VFMs) face a fundamental limitation in
transferring knowledge from vision language models (VLMs), while VLMs excel at
modeling cross-modal interactions through unified representation spaces,
existing VFMs predominantly adopt result-oriented paradigms that neglect the
underlying interaction processes. This representational discrepancy hinders
effective knowledge transfer and limits generalization across diverse vision
tasks. We propose Learning from Interactions (LFI), a cognitive-inspired
framework that addresses this gap by explicitly modeling visual understanding
as an interactive process. Our key insight is that capturing the dynamic
interaction patterns encoded in pre-trained VLMs enables more faithful and
efficient knowledge transfer to VFMs. The approach centers on two technical
innovations, Interaction Queries, which maintain persistent relational
structures across network layers, and interaction-based supervision, derived
from the cross-modal attention mechanisms of VLMs. Comprehensive experiments
demonstrate consistent improvements across multiple benchmarks, achieving 3.3
and 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO
detection/segmentation respectively, with minimal parameter overhead and faster
convergence. The framework particularly excels in cross-domain settings,
delivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human
evaluations further confirm its cognitive alignment, outperforming
result-oriented methods by 2.7 times in semantic consistency metrics.

</details>


### [63] [HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection](https://arxiv.org/abs/2509.18738)
*Ruichao Hou,Xingyuan Li,Tongwei Ren,Dongming Zhou,Gangshan Wu,Jinde Cao*

Main category: cs.CV

TL;DR: HyPSAM is a prompt-driven RGB-T SOD framework that uses SAM for zero-shot refinement. It first builds high-quality initial saliency maps with a Dynamic Fusion Network (DFNet) to generate prompts, then refines those maps with a plug-and-play Refinement Network (P2RNet) using hybrid text/mask/box prompts. It achieves state-of-the-art results on three public RGB-T SOD datasets and can plug into other RGB-T SOD methods, highlighting the potential of prompt engineering.


<details>
  <summary>Details</summary>
Motivation: RGB-T salient object detection suffers from incomplete feature fusion and limited data, making precise boundaries and complete object delineation difficult. Leveraging SAM's zero-shot generalization via task-specific prompts can improve cross-modality fusion and saliency accuracy, while prompting methods offer flexible, plug-and-play improvements without retraining large models.

Method: DFNet (Dynamic Fusion Network) uses dynamic convolution and multi-branch decoding to enable adaptive cross-modality interaction and richer multi-modal features, producing initial saliency maps used as visual prompts. P2RNet (Plug-and-Play Refinement Network) refines SAM outputs through hybrid prompts: a text prompt ensures reliable modality input, while mask and box prompts provide precise localization, enabling SAM to refine saliency maps without task-specific fine-tuning.

Result: Extensive experiments on three public RGB-T SOD datasets show state-of-the-art performance. HyPSAM is versatile and can be integrated with different RGB-T SOD methods to achieve significant gains instead of requiring end-to-end retraining.

Conclusion: The work demonstrates the effectiveness of prompt engineering for RGB-T SOD, combining a dynamic fusion backbone with SAM-based refinement to produce accurate saliency boundaries and complete objects, and suggests broad potential for applying such prompt-guided strategies in multimodal saliency tasks.

Abstract: RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent
objects by integrating complementary information from RGB and thermal
modalities. However, learning the precise boundaries and complete objects
remains challenging due to the intrinsic insufficient feature fusion and the
extrinsic limitations of data scarcity. In this paper, we propose a novel
hybrid prompt-driven segment anything model (HyPSAM), which leverages the
zero-shot generalization capabilities of the segment anything model (SAM) for
RGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that
generates high-quality initial saliency maps as visual prompts. DFNet employs
dynamic convolution and multi-branch decoding to facilitate adaptive
cross-modality interaction, overcoming the limitations of fixed-parameter
kernels and enhancing multi-modal feature representation. Moreover, we propose
a plug-and-play refinement network (P2RNet), which serves as a general
optimization strategy to guide SAM in refining saliency maps by using hybrid
prompts. The text prompt ensures reliable modality input, while the mask and
box prompts enable precise salient object localization. Extensive experiments
on three public datasets demonstrate that our method achieves state-of-the-art
performance. Notably, HyPSAM has remarkable versatility, seamlessly integrating
with different RGB-T SOD methods to achieve significant performance gains,
thereby highlighting the potential of prompt engineering in this field. The
code and results of our method are available at:
https://github.com/milotic233/HyPSAM.

</details>


### [64] [TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing](https://arxiv.org/abs/2509.18743)
*Susmit Neogi*

Main category: cs.CV

TL;DR: TriFusion-AE proposes a multimodal cross-attention autoencoder that fuses textual priors, monocular depth maps from multi-view images, and LiDAR point clouds to improve denoising and reconstruction robustness against noise and adversarial perturbations. Evaluated on nuScenes-mini, it is model-agnostic and can plug into CNN-based point cloud autoencoders; shows strong robustness under heavy corruption.


<details>
  <summary>Details</summary>
Motivation: LiDAR-based perception is central to autonomous driving and robotics, but raw point clouds are highly vulnerable to noise, occlusion, and adversarial perturbations. Autoencoders help denoise, but their performance degrades in real-world challenging conditions. A multimodal approach that leverages semantic (text) cues and depth (images) can yield more robust representations.

Method: TriFusion-AE is a multimodal cross-attention autoencoder that integrates textual priors, monocular depth from multi-view images, and LiDAR point clouds. It aligns semantic cues from text, geometric depth features from images, and spatial structure from LiDAR to learn robust representations. The framework is model-agnostic and designed to work with any CNN-based point cloud autoencoder for joint representation learning.

Result: Under mild perturbations, gains are limited; under strong adversarial attacks and heavy noise, CNN-based autoencoders collapse without this fusion, whereas TriFusion-AE achieves significantly more robust reconstruction.

Conclusion: Multimodal cross-attention with textual priors and depth improves robustness of LiDAR-based autoencoders. The framework is adaptable to existing CNN-based point cloud AEs and is suitable for realistic low-data deployment scenarios.

Abstract: LiDAR-based perception is central to autonomous driving and robotics, yet raw
point clouds remain highly vulnerable to noise, occlusion, and adversarial
corruptions. Autoencoders offer a natural framework for denoising and
reconstruction, but their performance degrades under challenging real-world
conditions. In this work, we propose TriFusion-AE, a multimodal cross-attention
autoencoder that integrates textual priors, monocular depth maps from
multi-view images, and LiDAR point clouds to improve robustness. By aligning
semantic cues from text, geometric (depth) features from images, and spatial
structure from LiDAR, TriFusion-AE learns representations that are resilient to
stochastic noise and adversarial perturbations. Interestingly, while showing
limited gains under mild perturbations, our model achieves significantly more
robust reconstruction under strong adversarial attacks and heavy noise, where
CNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to
reflect realistic low-data deployment scenarios. Our multimodal fusion
framework is designed to be model-agnostic, enabling seamless integration with
any CNN-based point cloud autoencoder for joint representation learning.

</details>


### [65] [COLT: Enhancing Video Large Language Models with Continual Tool Usage](https://arxiv.org/abs/2509.18754)
*Yuyang Liu,Xinyuan Shi,Bang Yang,Peilin Zhou,Jiahua Dong,Long Chen,Ian Reid,Xiaondan Liang*

Main category: cs.CV

TL;DR: COLT enables open-source video LLMs to continuously acquire and manage tool usage from streaming tools via a learnable tool codebook, achieving state-of-the-art results on video benchmarks including VideoToolBench.


<details>
  <summary>Details</summary>
Motivation: Existing video LLM tool-use methods rely on fixed tool repositories or fixed instruction-tuning, struggle to adapt to evolving, streaming tools in real-world settings, and can forget previously learned tools.

Method: Introduce a learnable tool codebook as a memory for tool features; dynamically select relevant tools by measuring similarity between the user instruction and tool features within the codebook; train on a video-centric instruction-tuning dataset VideoToolBench to enable continuous tool-use learning without catastrophic forgetting.

Result: Empirical results on standard video LLM benchmarks and VideoToolBench show state-of-the-art performance, demonstrating effective continual tool acquisition and usage.

Conclusion: COLT enables continuous, memory-based tool usage for open-source video LLMs, handles streaming tool data, avoids forgetting, and advances the state of the field.

Abstract: The success of Large Language Models (LLMs) has significantly propelled the
research of video understanding. To harvest the benefits of well-trained expert
models (i.e., tools), video LLMs prioritize the exploration of tool usage
capabilities. Existing methods either prompt closed-source LLMs or employ the
instruction tuning paradigm for tool-use fine-tuning. These methods, however,
assume an established repository of fixed tools and struggle to generalize to
real-world environments where tool data is perpetually evolving and streaming
in. To this end, we propose to enhance open-source video LLMs with COntinuaL
Tool usage (termed COLT), which automatically acquires tool-use ability in a
successive tool stream without suffering 'catastrophic forgetting' of the past
learned tools. Specifically, our COLT incorporates a learnable tool codebook as
a tool-specific memory system. Then relevant tools are dynamically selected
based on the similarity between user instruction and tool features within the
codebook. To unleash the tool usage potential of video LLMs, we collect a
video-centric tool-use instruction tuning dataset VideoToolBench. Extensive
experiments on both previous video LLM benchmarks and the tool-use-specific
VideoToolBench dataset demonstrate the state-of-the-art performance of our
proposed COLT.

</details>


### [66] [FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation](https://arxiv.org/abs/2509.18759)
*Zhaorui Wang,Yi Gu,Deming Zhou,Renjing Xu*

Main category: cs.CV

TL;DR: FixingGS is a training-free framework that uses distillation of diffusion priors to enhance sparse-view 3D Gaussian Splatting reconstructions, coupled with adaptive progressive refinement, achieving superior quality and cross-view consistency.


<details>
  <summary>Details</summary>
Motivation: Reconstructing 3D scenes from sparse viewpoints leads to artifacts and under-constrained regions. Generative diffusion priors can help remove artifacts and inpaint missing content, but existing methods often lack multi-view consistency and produce blurred details. The goal is to leverage diffusion priors in a training-free, view-consistent way to improve 3DGS reconstructions.

Method: Introduce a distillation-based approach to extract accurate and cross-view coherent diffusion priors without training. Use an adaptive progressive enhancement scheme to refine under-constrained regions. Integrate with 3D Gaussian Splatting to remove artifacts and perform inpainting while maintaining multi-view consistency.

Result: Extensive experiments show that FixingGS surpasses state-of-the-art methods in visual quality and reconstruction performance for sparse-view 3DGS. The authors also indicate their code will be released publicly.

Conclusion: FixingGS demonstrates an effective, training-free strategy for leveraging diffusion priors to improve sparse-view 3DGS reconstructions, yielding better artifact removal, inpainting, and cross-view consistency; the adaptive progressive refinement further boosts results.

Abstract: Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in
3D reconstruction and novel view synthesis. However, reconstructing 3D scenes
from sparse viewpoints remains highly challenging due to insufficient visual
information, which results in noticeable artifacts persisting across the 3D
representation. To address this limitation, recent methods have resorted to
generative priors to remove artifacts and complete missing content in
under-constrained areas. Despite their effectiveness, these approaches struggle
to ensure multi-view consistency, resulting in blurred structures and
implausible details. In this work, we propose FixingGS, a training-free method
that fully exploits the capabilities of the existing diffusion model for
sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our
distillation approach, which delivers more accurate and cross-view coherent
diffusion priors, thereby enabling effective artifact removal and inpainting.
In addition, we propose an adaptive progressive enhancement scheme that further
refines reconstructions in under-constrained regions. Extensive experiments
demonstrate that FixingGS surpasses existing state-of-the-art methods with
superior visual quality and reconstruction performance. Our code will be
released publicly.

</details>


### [67] [Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models](https://arxiv.org/abs/2509.18763)
*Xijun Wang,Junyun Huang,Rayyan Abdalla,Chengyuan Zhang,Ruiqi Xian,Dinesh Manocha*

Main category: cs.CV

TL;DR: Bi-VLM introduces non-uniform, saliency-aware ultra-low-bit quantization (≤2 bits) for vision-language models by splitting weights into outlier/salient and multiple inlier subsets via Gaussian quantiles; demonstrates strong accuracy/efficiency gains and enables token pruning of visual tokens.


<details>
  <summary>Details</summary>
Motivation: Address the high computational cost and memory needs of vision-language models, especially in hardware-constrained environments, by enabling ultra-low-bit quantization.

Method: Non-uniform weight grouping based on Gaussian quantiles into salient/outlier and multiple inlier subsets; saliency-aware hybrid quantization with distinct constraints on scaler and binary matrices guided by saliency metrics and compression objectives; evaluation across multiple VLMs and exploration of token pruning on quantized models.

Result: Language-model component of Bi-VLM surpasses SOTA by 3%–47% on VQA tasks across four benchmarks and three models; overall VLM performance improves by 4%–45% over SOTA; token pruning reveals 90%–99% redundancy in image tokens, enabling further efficiency via visual-token pruning.

Conclusion: Saliency-guided, non-uniform ultra-low-bit quantization is effective for VLMs, delivering notable accuracy gains and enabling token pruning to reduce redundancy and improve efficiency in hardware-constrained deployments.

Abstract: We address the critical gap between the computational demands of
vision-language models and the possible ultra-low-bit weight precision
(bitwidth $\leq2$ bits) we can use for higher efficiency. Our work is motivated
by the substantial computational cost and memory requirements of VLMs, which
restrict their applicability in hardware-constrained environments. We propose
Bi-VLM, which separates model weights non-uniformly based on the Gaussian
quantiles. Our formulation groups the model weights into outlier (salient) and
multiple inlier (unsalient) subsets, ensuring that each subset contains a
proportion of weights corresponding to its quantile in the distribution. We
propose a saliency-aware hybrid quantization algorithm and use it to quantize
weights by imposing different constraints on the scaler and binary matrices
based on the saliency metric and compression objective. We have evaluated our
approach on different VLMs. For the language model part of the VLM, our Bi-VLM
outperforms the SOTA by 3%-47% on the visual question answering task in terms
of four different benchmarks and three different models. For the overall VLM,
our Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the
quantized models and observe that there is redundancy of image tokens 90% - 99%
in the quantized models. This helps us to further prune the visual tokens to
improve efficiency.

</details>


### [68] [DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision](https://arxiv.org/abs/2509.18765)
*Azad Singh,Deepak Mishra*

Main category: cs.CV

TL;DR: DiSSECT introduces a discrete self-supervised learning framework for medical imaging that uses multi-scale vector quantization to create a discrete bottleneck, reducing shortcut learning and enhancing transferability across tasks and domains with limited labeled data; it achieves strong classification and segmentation performance with minimal fine-tuning and high label efficiency.


<details>
  <summary>Details</summary>
Motivation: SSL methods in medical imaging often rely on architectural complexity, anatomy-specific priors, or heavy augmentations, leading to limited scalability and generalization. In modalities with high anatomical similarity (e.g., chest X-rays), models risk shortcut learning that captures spurious correlations rather than pathology. A scalable, data-efficient SSL method that emphasizes structure-aware representations is needed.

Method: DiSSECT integrates multi-scale vector quantization into the SSL pipeline to impose a discrete representation bottleneck. This module forces the learned features to be repeatable and structure-aware, suppressing view-specific or low-utility patterns. The approach aims to enhance cross-task/domain transfer and reduce reliance on extensive fine-tuning, by learning discrete codes that capture stable anatomical structure.

Result: The method yields strong performance on both classification and segmentation tasks, with minimal to no fine-tuning and high label efficiency in low-label settings. Validation across multiple public medical imaging datasets demonstrates robustness and generalizability compared with existing state-of-the-art SSL approaches.

Conclusion: Discrete self-supervision via multi-scale vector quantization provides a scalable, transferable representation for medical imaging. By constraining the representation space to discrete, structure-informed codes, the method mitigates shortcut learning and improves cross-task/domain transfer, especially in low-label regimes.

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for medical
image representation learning, particularly in settings with limited labeled
data. However, existing SSL methods often rely on complex architectures,
anatomy-specific priors, or heavily tuned augmentations, which limit their
scalability and generalizability. More critically, these models are prone to
shortcut learning, especially in modalities like chest X-rays, where anatomical
similarity is high and pathology is subtle. In this work, we introduce DiSSECT
-- Discrete Self-Supervision for Efficient Clinical Transferable
Representations, a framework that integrates multi-scale vector quantization
into the SSL pipeline to impose a discrete representational bottleneck. This
constrains the model to learn repeatable, structure-aware features while
suppressing view-specific or low-utility patterns, improving representation
transfer across tasks and domains. DiSSECT achieves strong performance on both
classification and segmentation tasks, requiring minimal or no fine-tuning, and
shows particularly high label efficiency in low-label regimes. We validate
DiSSECT across multiple public medical imaging datasets, demonstrating its
robustness and generalizability compared to existing state-of-the-art
approaches.

</details>


### [69] [Real-time Deer Detection and Warning in Connected Vehicles via Thermal Sensing and Deep Learning](https://arxiv.org/abs/2509.18779)
*Hemanth Puppala,Wayne Sarasua,Srinivas Biyaguda,Farhad Farzinpour,Mashrur Chowdhury*

Main category: cs.CV

TL;DR: A real-time deer detection and driver-warning system using thermal imaging, deep learning, and CV2X communication, trained on a 12k thermal deer image dataset. It achieves high accuracy (mAP 98.84%, precision 95.44%, recall 95.96%), robust in variable weather (thermal 88–92% detection when visible-light cameras under 60%), and latency under 100 ms from detection to alert, enabling vehicle-to-everything broadcasts to mitigate collisions.


<details>
  <summary>Details</summary>
Motivation: Deer-vehicle collisions cause widespread safety concerns in the US (≈2.1 million incidents/year, ~440 fatalities, ~59,000 injuries, ~$10B in damages) and deer populations are declining. There is a need for real-time detection and warning mechanisms that work under diverse conditions to reduce incidents.

Method: The system fuses thermal imaging, deep learning, and CV2X communication. It was trained and validated on a custom dataset of over 12,000 thermal deer images collected in Mars Hill, North Carolina. The approach yields end-to-end latency under 100 ms from detection to driver alert. High-probability detections trigger sensor data sharing to surrounding vehicles/RSUs via CV2X. Field tests demonstrated robust operation across weather conditions with thermal imaging maintaining 88–92% accuracy in challenging scenarios where visible cameras underperform (<60%).

Result: Performance on the dataset: 98.84% mean average precision, 95.44% precision, 95.96% recall. Thermal sensing maintained 88–92% detection accuracy under adverse conditions; visible-light cameras under 60% effectiveness. End-to-end latency (detection to alert) consistently under 100 ms. Field testing confirmed real-world viability and cross-vehicle information sharing.

Conclusion: The study demonstrates a viable technological pathway for reducing deer-vehicle collisions by combining thermal imaging with connected-vehicle communication, showing high accuracy, low latency, and robustness to weather, with field validation supporting deployment potential.

Abstract: Deer-vehicle collisions represent a critical safety challenge in the United
States, causing nearly 2.1 million incidents annually and resulting in
approximately 440 fatalities, 59,000 injuries, and 10 billion USD in economic
damages. These collisions also contribute significantly to declining deer
populations. This paper presents a real-time detection and driver warning
system that integrates thermal imaging, deep learning, and
vehicle-to-everything communication to help mitigate deer-vehicle collisions.
Our system was trained and validated on a custom dataset of over 12,000 thermal
deer images collected in Mars Hill, North Carolina. Experimental evaluation
demonstrates exceptional performance with 98.84 percent mean average precision,
95.44 percent precision, and 95.96 percent recall. The system was field tested
during a follow-up visit to Mars Hill and readily sensed deer providing the
driver with advanced warning. Field testing validates robust operation across
diverse weather conditions, with thermal imaging maintaining between 88 and 92
percent detection accuracy in challenging scenarios where conventional visible
light based cameras achieve less than 60 percent effectiveness. When a high
probability threshold is reached sensor data sharing messages are broadcast to
surrounding vehicles and roadside units via cellular vehicle to everything
(CV2X) communication devices. Overall, our system achieves end to end latency
consistently under 100 milliseconds from detection to driver alert. This
research establishes a viable technological pathway for reducing deer-vehicle
collisions through thermal imaging and connected vehicles.

</details>


### [70] [Towards Application Aligned Synthetic Surgical Image Synthesis](https://arxiv.org/abs/2509.18796)
*Danush Kumar Venkatesh,Stefanie Speidel*

Main category: cs.CV

TL;DR: SAADi aligns diffusion models to downstream objectives by fine-tuning with preferred/non-preferred synthetic images, improving surgical vision tasks when annotated data is scarce.


<details>
  <summary>Details</summary>
Motivation: Annotated surgical data are scarce and diffusion models tend to memorize data, leading to less diverse or less task-aligned samples. This hinders downstream surgical vision tasks like classification and segmentation.

Method: Create pairs of preferred and non-preferred synthetic images and perform lightweight fine-tuning of diffusion models to align image generation with downstream objectives; optionally iterate to refine synthetic samples.

Result: Classification gains of 7–9% and segmentation gains of 2–10% across three surgical datasets, with larger gains for underrepresented classes; iterative refinement yields an additional 4–10% improvement.

Conclusion: Task-aware alignment between generative data synthesis and downstream objectives is effective for mitigating data scarcity in surgical vision and outperforms baseline approaches.

Abstract: The scarcity of annotated surgical data poses a significant challenge for
developing deep learning systems in computer-assisted interventions. While
diffusion models can synthesize realistic images, they often suffer from data
memorization, resulting in inconsistent or non-diverse samples that may fail to
improve, or even harm, downstream performance. We introduce \emph{Surgical
Application-Aligned Diffusion} (SAADi), a new framework that aligns diffusion
models with samples preferred by downstream models. Our method constructs pairs
of \emph{preferred} and \emph{non-preferred} synthetic images and employs
lightweight fine-tuning of diffusion models to align the image generation
process with downstream objectives explicitly. Experiments on three surgical
datasets demonstrate consistent gains of $7$--$9\%$ in classification and
$2$--$10\%$ in segmentation tasks, with the considerable improvements observed
for underrepresented classes. Iterative refinement of synthetic samples further
boosts performance by $4$--$10\%$. Unlike baseline approaches, our method
overcomes sample degradation and establishes task-aware alignment as a key
principle for mitigating data scarcity and advancing surgical vision
applications.

</details>


### [71] [A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising](https://arxiv.org/abs/2509.18801)
*Kuang Xiaodong,Li Bingxuan,Li Yuan,Rao Fan,Ma Gege,Xie Qingguo,Mok Greta S P,Liu Huafeng,Zhu Wentao*

Main category: cs.CV

TL;DR: Neural KMDS-Net: a neural-enhanced, kernel-space sparse model for dynamic PET denoising that exploits inter-frame correlations and intra-frame structure to achieve high-quality temporal frames, outperforming baselines; code available.


<details>
  <summary>Details</summary>
Motivation: Dynamic PET frames suffer from low statistics in short frames, causing noise and degraded image quality. While deep learning helps denoise, integrating model-based structure with neural parameter estimation can improve fidelity and generalization.

Method: Introduce KMDS (kernel space-based multidimensional sparse) model leveraging inter-frame spatial correlations and intra-frame structure. Replace fixed parameter estimation with neural networks for adaptive optimization, forming the end-to-end Neural KMDS-Net for dynamic PET denoising.

Result: Extensive experiments on simulated and real data show strong denoising performance and outperformance of baseline methods, achieving high temporal and spatial resolution in dynamic PET.

Conclusion: Neural KMDS-Net effectively denoises dynamic PET frames and can enable higher temporal and spatial resolution; code is publicly available for reproducibility.

Abstract: Achieving high image quality for temporal frames in dynamic positron emission
tomography (PET) is challenging due to the limited statistic especially for the
short frames. Recent studies have shown that deep learning (DL) is useful in a
wide range of medical image denoising tasks. In this paper, we propose a
model-based neural network for dynamic PET image denoising. The inter-frame
spatial correlation and intra-frame structural consistency in dynamic PET are
used to establish the kernel space-based multidimensional sparse (KMDS) model.
We then substitute the inherent forms of the parameter estimation with neural
networks to enable adaptive parameters optimization, forming the end-to-end
neural KMDS-Net. Extensive experimental results from simulated and real data
demonstrate that the neural KMDS-Net exhibits strong denoising performance for
dynamic PET, outperforming previous baseline methods. The proposed method may
be used to effectively achieve high temporal and spatial resolution for dynamic
PET. Our source code is available at
https://github.com/Kuangxd/Neural-KMDS-Net/tree/main.

</details>


### [72] [Surgical Video Understanding with Label Interpolation](https://arxiv.org/abs/2509.18802)
*Garam Kim,Tae Kyeong Jeong,Juyoun Park*

Main category: cs.CV

TL;DR: Proposes a framework that uses optical flow-based label interpolation to propagate segmentation annotations from key frames to neighboring frames, enabling multi-task learning for surgical scene understanding in robot-assisted surgery by addressing temporal-spatial annotation imbalance.


<details>
  <summary>Details</summary>
Motivation: To achieve richer multi-task learning in surgical video understanding, overcoming the scarcity of pixel-level annotations and the temporal-spatial imbalance where long-term frame-level labels exist for all frames, but short-term labels (e.g., segmentation, action detection) exist only for key frames.

Method: Integrates optical-flow-based segmentation label interpolation with multi-task learning. Optical flow estimated from annotated key frames propagates labels to surrounding unlabeled frames, enriching spatial supervision and balancing temporal and spatial information for training of multiple tasks.

Result: Not reported in the abstract; the paper presents the framework and its intended improvements, but no empirical results are provided.

Conclusion: The approach targets temporal-spatial imbalance in multi-task learning for RAS and aims to improve surgical scene understanding efficiency and accuracy by leveraging interpolated labels from key frames.

Abstract: Robot-assisted surgery (RAS) has become a critical paradigm in modern
surgery, promoting patient recovery and reducing the burden on surgeons through
minimally invasive approaches. To fully realize its potential, however, a
precise understanding of the visual data generated during surgical procedures
is essential. Previous studies have predominantly focused on single-task
approaches, but real surgical scenes involve complex temporal dynamics and
diverse instrument interactions that limit comprehensive understanding.
Moreover, the effective application of multi-task learning (MTL) requires
sufficient pixel-level segmentation data, which are difficult to obtain due to
the high cost and expertise required for annotation. In particular, long-term
annotations such as phases and steps are available for every frame, whereas
short-term annotations such as surgical instrument segmentation and action
detection are provided only for key frames, resulting in a significant
temporal-spatial imbalance. To address these challenges, we propose a novel
framework that combines optical flow-based segmentation label interpolation
with multi-task learning. optical flow estimated from annotated key frames is
used to propagate labels to adjacent unlabeled frames, thereby enriching sparse
spatial supervision and balancing temporal and spatial information for
training. This integration improves both the accuracy and efficiency of
surgical scene understanding and, in turn, enhances the utility of RAS.

</details>


### [73] [Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation](https://arxiv.org/abs/2509.18824)
*Yanzuo Lu,Xin Xia,Manlin Zhang,Huafeng Kuang,Jianbin Zheng,Yuxi Ren,Xuefeng Xiao*

Main category: cs.CV

TL;DR: A unified acceleration framework called Hyper-Bagel that speeds up both multimodal understanding and generation by combining speculative decoding and multi-stage diffusion distillation, delivering significant speedups while preserving output quality.


<details>
  <summary>Details</summary>
Motivation: Unified multimodal models face high computational costs due to interleaved multimodal tokens and iterative diffusion and autoregressive decoding. There is a need for simultaneous acceleration of understanding and generation to enable real-time interactions.

Method: Divide-and-conquer approach using speculative decoding for next-token prediction during understanding and a multi-stage distillation process to accelerate diffusion denoising for generation; additional elements include adversarial distillation with human feedback learning and the development of 6-NFE and 1-NFE variants for progressively faster performance.

Result: Achieves over 2x speedup in multimodal understanding. For generation, a 6-NFE model yields ~16.67x speedup for text-to-image and ~22x for image editing while preserving output quality; a 1-NFE model enables near real-time interactive editing and generation.

Conclusion: Hyper-Bagel demonstrates that joint acceleration of both understanding and generation in unified multimodal models is feasible without sacrificing quality, enabling faster, cost-effective, and more responsive multimodal interactions through speculative decoding, staged diffusion distillation, and human-in-the-loop optimization.

Abstract: Unified multimodal models have recently attracted considerable attention for
their remarkable abilities in jointly understanding and generating diverse
content. However, as contexts integrate increasingly numerous interleaved
multimodal tokens, the iterative processes of diffusion denoising and
autoregressive decoding impose significant computational overhead. To address
this, we propose Hyper-Bagel, a unified acceleration framework designed to
simultaneously speed up both multimodal understanding and generation tasks. Our
approach uses a divide-and-conquer strategy, employing speculative decoding for
next-token prediction and a multi-stage distillation process for diffusion
denoising. The framework delivers substantial performance gains, achieving over
a 2x speedup in multimodal understanding. For generative tasks, our resulting
lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a
22x speedup in image editing, all while preserving the high-quality output of
the original model. We further develop a highly efficient 1-NFE model that
enables near real-time interactive editing and generation. By combining
advanced adversarial distillation with human feedback learning, this model
achieves ultimate cost-effectiveness and responsiveness, making complex
multimodal interactions seamless and instantaneous.

</details>


### [74] [Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography](https://arxiv.org/abs/2509.18839)
*Gianmarco Spinaci,Lukas Klic,Giovanni Colavizza*

Main category: cs.CV

TL;DR: Multimodal LLMs and VLMs can perform single-label classification of Christian iconography, often surpassing ResNet50 baselines; zero-shot accuracy improves with Iconclass descriptions, while few-shot often offers little gain; Wikidata is more challenging and sensitive to metadata; prompts show promise for metadata curation in digital humanities; future work includes prompt optimization and broader classification strategies.


<details>
  <summary>Details</summary>
Motivation: Assess whether general-purpose multimodal models can interpret iconography beyond supervised classifiers, enabling scalable metadata curation in digital humanities.

Method: Benchmark study on three Iconclass-supported datasets (ArtDL, ICONCLASS, Wikidata; top 10 classes) under three input modes (class labels, Iconclass descriptions, five-shot exemplars); compare to ResNet50 baselines.

Result: Gemini-2.5 Pro and GPT-4o outperform ResNet50 baselines; Siglip achieves highest accuracy on Wikidata; accuracy drops on Wikidata due to image size/metadata alignment; enriching prompts improves zero-shot; few-shot yields limited or no gains.

Conclusion: General-purpose multimodal LLMs are viable for classification in visually complex cultural heritage domains and can serve as metadata curation tools; future work on prompt optimization and expanding to other classification strategies/models.

Abstract: This study evaluates the capabilities of Multimodal Large Language Models
(LLMs) and Vision Language Models (VLMs) in the task of single-label
classification of Christian Iconography. The goal was to assess whether
general-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5,
can interpret the Iconography, typically addressed by supervised classifiers,
and evaluate their performance. Two research questions guided the analysis:
(RQ1) How do multimodal LLMs perform on image classification of Christian
saints? And (RQ2), how does performance vary when enriching input with
contextual information or few-shot exemplars? We conducted a benchmarking study
using three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and
Wikidata, filtered to include the top 10 most frequent classes. Models were
tested under three conditions: (1) classification using class labels, (2)
classification with Iconclass descriptions, and (3) few-shot learning with five
exemplars. Results were compared against ResNet50 baselines fine-tuned on the
same datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed
the ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset,
where Siglip reached the highest accuracy score, suggesting model sensitivity
to image size and metadata alignment. Enriching prompts with class descriptions
generally improved zero-shot performance, while few-shot learning produced
lower results, with only occasional and minimal increments in accuracy. We
conclude that general-purpose multimodal LLMs are capable of classification in
visually complex cultural heritage domains. These results support the
application of LLMs as metadata curation tools in digital humanities workflows,
suggesting future research on prompt optimization and the expansion of the
study to other classification strategies and models.

</details>


### [75] [ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized Graph Construction](https://arxiv.org/abs/2509.18840)
*Ismael Elsharkawi,Hossam Sharara,Ahmed Rafea*

Main category: cs.CV

TL;DR: Introduces Learnable Reparameterized Graph Construction (LRGC) for Vision Graph Neural Networks (ViG). It uses key–query attention between all node pairs and a differentiable soft-threshold reparameterization to select edges, enabling layer-wise learnable thresholds without hyperparameter search. The approach yields state-of-the-art performance on ImageNet-1k for ViG models of similar size.


<details>
  <summary>Details</summary>
Motivation: Current ViG graph construction relies on non-learning, fixed methods (e.g., k-NN, hypergraphs, fixed similarity thresholds) that can bias neighborhood selection and require manual hyperparameter tuning. A learnable, parameter-efficient graph construction could adapt to data and improve performance without manual search.

Method: LRGC computes key–query attention across all node pairs to derive candidate edges, then applies a soft-threshold reparameterization to determine edge presence. The thresholds are learnable and per-layer, making the graph construction differentiable and trainable end-to-end.

Result: ViG-LRGC outperforms state-of-the-art ViG models of similar sizes on ImageNet-1k.

Conclusion: LRGC provides a learnable, hyperparameter-free graph construction mechanism for Vision Graph NNs, enabling better neighborhood selection and improved accuracy on large-scale benchmarks while avoiding manual graph-building heuristics.

Abstract: Image Representation Learning is an important problem in Computer Vision.
Traditionally, images were processed as grids, using Convolutional Neural
Networks or as a sequence of visual tokens, using Vision Transformers.
Recently, Vision Graph Neural Networks (ViG) have proposed the treatment of
images as a graph of nodes; which provides a more intuitive image
representation. The challenge is to construct a graph of nodes in each layer
that best represents the relations between nodes and does not need a
hyper-parameter search. ViG models in the literature depend on
non-parameterized and non-learnable statistical methods that operate on the
latent features of nodes to create a graph. This might not select the best
neighborhood for each node. Starting from k-NN graph construction to HyperGraph
Construction and Similarity-Thresholded graph construction, these methods lack
the ability to provide a learnable hyper-parameter-free graph construction
method. To overcome those challenges, we present the Learnable Reparameterized
Graph Construction (LRGC) for Vision Graph Neural Networks. LRGC applies
key-query attention between every pair of nodes; then uses soft-threshold
reparameterization for edge selection, which allows the use of a differentiable
mathematical model for training. Using learnable parameters to select the
neighborhood removes the bias that is induced by any clustering or thresholding
methods previously introduced in the literature. In addition, LRGC allows
tuning the threshold in each layer to the training data since the thresholds
are learnable through training and are not provided as hyper-parameters to the
model. We demonstrate that the proposed ViG-LRGC approach outperforms
state-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark
dataset.

</details>


### [76] [Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions](https://arxiv.org/abs/2509.18847)
*Junhao Su,Yuanliang Wan,Junwei Yang,Hengyu Shi,Tianyang Han,Junfeng Luo,Yurui Qiu*

Main category: cs.CV

TL;DR: Structured reflection transforms the error-to-repair path into a trained, explicit step in tool-augmented LLMs, yielding fewer redundant calls and better multi-turn tool use by promoting diagnosis and executable follow-ups (Reflect, then Call, then Final).


<details>
  <summary>Details</summary>
Motivation: Current self-reflection methods rely on vague prompts or one-way reasoning and fail to prevent repeated mistakes in multi-turn tool interactions; there is a need for a trainable, explicit repair mechanism that can diagnose errors and propose executable corrections.

Method: Introduce Structured Reflection: the agent outputs a concise reflection that diagnoses the failure using evidence from the previous step and proposes a correct executable follow-up call. Training combines DAPO and GSPO objectives with tool-use-aware rewards, optimizing a Reflect→Call→Final sequence. Develop Tool-Reflection-Bench to programmatically verify structural validity, executability, parameter correctness, and result consistency. Train on mini-trajectory tasks (erroneous call → reflection → corrected call) with disjoint train/test splits. Evaluate on BFCL v3 and Tool-Reflection-Bench for multi-turn tool-call success and error recovery.

Result: The approach yields large gains in multi-turn tool-call success and error recovery, with reduced redundant calls, demonstrating that explicit, optimized reflection improves reliability and provides a reproducible failure-learning path for agents.

Conclusion: Explicitly modeling and optimizing reflection (Error diagnosis followed by executable repair) enhances the reliability of tool interaction and offers a reproducible framework for agents to learn from failure in multi-turn settings.

Abstract: Tool-augmented large language models (LLMs) are usually trained with
supervised imitation or coarse-grained reinforcement learning that optimizes
single tool calls. Current self-reflection practices rely on heuristic prompts
or one-way reasoning: the model is urged to 'think more' instead of learning
error diagnosis and repair. This is fragile in multi-turn interactions; after a
failure the model often repeats the same mistake. We propose structured
reflection, which turns the path from error to repair into an explicit,
controllable, and trainable action. The agent produces a short yet precise
reflection: it diagnoses the failure using evidence from the previous step and
then proposes a correct, executable follow-up call. For training we combine
DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing
the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce
Tool-Reflection-Bench, a lightweight benchmark that programmatically checks
structural validity, executability, parameter correctness, and result
consistency. Tasks are built as mini trajectories of erroneous call,
reflection, and corrected call, with disjoint train and test splits.
Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn
tool-call success and error recovery, and a reduction of redundant calls. These
results indicate that making reflection explicit and optimizing it directly
improves the reliability of tool interaction and offers a reproducible path for
agents to learn from failure.

</details>


### [77] [Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model](https://arxiv.org/abs/2509.18891)
*Xueyu Liu,Xiaoyi Zhang,Guangze Shi,Meilin Liu,Yexin Lai,Yongfei Wu,Mingqiang Wei*

Main category: cs.CV

TL;DR: A dual-agent reinforcement learning framework (Point Prompt Defender) automatically optimizes point prompts for Segment Anything Model (SAM) via an attacker/defender game in a dual-space graph, improving robustness and generalization of segmentation without retraining.


<details>
  <summary>Details</summary>
Motivation: Prompt quality critically affects SAM performance, yet current prompts rely on heuristics and manual design, hindering scalability and generalization across tasks and data.

Method: Build a task-agnostic point prompt environment by modeling image patches as nodes in a dual-space graph with edges capturing physical and semantic distances. An attacker selects disruptive prompts to degrade SAM; a defender suppresses them to restore accuracy. Both use Deep Q-Networks with rewards based on segmentation quality changes. At inference, only the defender is used to refine arbitrary coarse prompts.

Result: Empirical results show improved robustness and generalization of SAM segmentation with the defender framework, enabling plug-and-play prompt refinement across diverse tasks without retraining.

Conclusion: Point Prompt Defender offers a flexible, interpretable approach to prompt-based segmentation, providing automated, scalable prompt optimization and robustness against adversarial prompt perturbations.

Abstract: Prompt quality plays a critical role in the performance of the Segment
Anything Model (SAM), yet existing approaches often rely on heuristic or
manually crafted prompts, limiting scalability and generalization. In this
paper, we propose Point Prompt Defender, an adversarial reinforcement learning
framework that adopts an attack-for-defense paradigm to automatically optimize
point prompts. We construct a task-agnostic point prompt environment by
representing image patches as nodes in a dual-space graph, where edges encode
both physical and semantic distances. Within this environment, an attacker
agent learns to activate a subset of prompts that maximally degrade SAM's
segmentation performance, while a defender agent learns to suppress these
disruptive prompts and restore accuracy. Both agents are trained using Deep
Q-Networks with a reward signal based on segmentation quality variation. During
inference, only the defender is deployed to refine arbitrary coarse prompt
sets, enabling enhanced SAM segmentation performance across diverse tasks
without retraining. Extensive experiments show that Point Prompt Defender
effectively improves SAM's robustness and generalization, establishing a
flexible, interpretable, and plug-and-play framework for prompt-based
segmentation.

</details>


### [78] [SmartWilds: Multimodal Wildlife Monitoring Dataset](https://arxiv.org/abs/2509.18894)
*Jenna Kline,Anirudh Potlapally,Bharath Pillai,Tanishka Wani,Rugved Katole,Vedant Patil,Penelope Covey,Hari Subramoni,Tanya Berger-Wolf,Christopher Stewart*

Main category: cs.CV

TL;DR: SmartWilds releases a multimodal wildlife monitoring dataset (drone imagery, camera traps, and bioacoustics) to enable conservation-focused AI research; pilot data from The Wilds covers 4 days across 220 acres with multiple species, showcasing complementary modality strengths and establishing reproducible protocols; future expansions planned.


<details>
  <summary>Details</summary>
Motivation: Address fragmentation in wildlife monitoring by integrating multiple sensing modalities to improve land-use understanding, species detection, behavior, and habitat management, aiding endangered species research and conservation planning.

Method: Data collection of synchronized drone imagery, camera trap photos/videos, and bioacoustic recordings during Summer 2025 at The Wilds; a 4-day pilot across a 220-acre pasture with listed species; comparative analysis of modality performance; development of reproducible protocols and open datasets; groundwork for future GPS, citizen science data, and seasonal coverage.

Result: Demonstrates complementary strengths of modalities for land-use, detection, behavior, and habitat monitoring; provides open datasets and reproducible protocols; establishes a baseline for multimodal conservation CV research.

Conclusion: SmartWilds provides a foundational multimodal wildlife monitoring resource, enabling cross-modal research and planning, with planned future data expansions to broaden temporal and data modality coverage.

Abstract: We present the first release of SmartWilds, a multimodal wildlife monitoring
dataset. SmartWilds is a synchronized collection of drone imagery, camera trap
photographs and videos, and bioacoustic recordings collected during summer 2025
at The Wilds safari park in Ohio. This dataset supports multimodal AI research
for comprehensive environmental monitoring, addressing critical needs in
endangered species research, conservation ecology, and habitat management. Our
pilot deployment captured four days of synchronized monitoring across three
modalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,
Przewalski's horses, as well as species native to Ohio, including bald eagles,
white-tailed deer, and coyotes. We provide a comparative analysis of sensor
modality performance, demonstrating complementary strengths for landuse
patterns, species detection, behavioral analysis, and habitat monitoring. This
work establishes reproducible protocols for multimodal wildlife monitoring
while contributing open datasets to advance conservation computer vision
research. Future releases will include synchronized GPS tracking data from
tagged individuals, citizen science data, and expanded temporal coverage across
multiple seasons.

</details>


### [79] [RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing](https://arxiv.org/abs/2509.18897)
*Jiayu Wang,Ruizhi Wang,Jie Song,Haofei Zhang,Mingli Song,Zunlei Feng,Li Sun*

Main category: cs.CV

TL;DR: Introduces RS3DBench, a large remote-sensing image-depth-texture benchmark, and a diffusion-based depth estimation model; aims to advance 3D understanding in remote sensing with public release.


<details>
  <summary>Details</summary>
Motivation: Existing remote sensing datasets often lack comprehensive depth information or precise alignment between depth maps and imagery, hindering development of 3D vision and geographic AI.

Method: Construct RS3DBench with 54,951 image-depth pairs, pixel-level depth maps, and accompanying textual descriptions across diverse locales; develop a remotely sensed depth estimation model based on stable diffusion with multimodal fusion, achieving state-of-the-art performance on the dataset; provide code and data access.

Result: RS3DBench enables training and evaluation of 3D perception tasks on remote sensing data; the diffusion-based depth model attains state-of-the-art results on RS3DBench.

Conclusion: The work advances 3D visual perception models and geographic AI in remote sensing; plans to release dataset, models, and code publicly.

Abstract: In this paper, we introduce a novel benchmark designed to propel the
advancement of general-purpose, large-scale 3D vision models for remote sensing
imagery. While several datasets have been proposed within the realm of remote
sensing, many existing collections either lack comprehensive depth information
or fail to establish precise alignment between depth data and remote sensing
images. To address this deficiency, we present a visual Benchmark for 3D
understanding of Remotely Sensed images, dubbed RS3DBench. This dataset
encompasses 54,951 pairs of remote sensing images and pixel-level aligned depth
maps, accompanied by corresponding textual descriptions, spanning a broad array
of geographical contexts. It serves as a tool for training and assessing 3D
visual perception models within remote sensing image spatial understanding
tasks. Furthermore, we introduce a remotely sensed depth estimation model
derived from stable diffusion, harnessing its multimodal fusion capabilities,
thereby delivering state-of-the-art performance on our dataset. Our endeavor
seeks to make a profound contribution to the evolution of 3D visual perception
models and the advancement of geographic artificial intelligence within the
remote sensing domain. The dataset, models and code will be accessed on the
https://rs3dbench.github.io.

</details>


### [80] [DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring](https://arxiv.org/abs/2509.18898)
*Pengteng Li,Yunfan Lu,Pinhao Song,Weiyu Guo,Huizai Yao,F. Richard Yu,Hui Xiong*

Main category: cs.CV

TL;DR: DeblurSplat is the first SfM-free deblurring pipeline for 3D Gaussian Splatting using event cameras, obtaining accurate initial geometry directly from blurred frames via a pretrained dense-stereo module (DUSt3R) and leveraging event streams for fine-grained supervision, yielding high-fidelity novel views with improved rendering efficiency over current deblur methods.


<details>
  <summary>Details</summary>
Motivation: Motion blur hinders accurate 3D reconstruction and novel-view rendering, and relying on Structure-from-Motion (SfM) can propagate pose-estimation errors into geometry. The authors aim to eliminate intermediate pose estimation and use the high temporal sensitivity of event cameras to provide sharper supervision signals for scene reconstruction.

Method: Two-pronged approach: (1) use a pretrained dense-stereo module (DUSt3R) to directly generate accurate initial point clouds from blurred images without computing camera poses, avoiding error propagation from Pose to geometry; (2) integrate an event stream into the deblurring pipeline by decoding latent sharp images from event streams and blurred frames, providing fine-grained supervision for 3D Gaussian Splatting-based scene reconstruction (DeblurSplat).

Result: Extensive experiments show DeblurSplat delivers high-fidelity novel views and faster rendering compared with state-of-the-art (SOTA) methods in deblurred 3D-GS.

Conclusion: The SfM-free framework successfully reduces dependence on intermediate pose estimation, mitigates accumulated pose-geometry errors, and leverages event-based supervision to enhance reconstruction quality and rendering efficiency.

Abstract: In this paper, we propose the first Structure-from-Motion (SfM)-free
deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.
We address the motion-deblurring problem in two ways. First, we leverage the
pretrained capability of the dense stereo module (DUSt3R) to directly obtain
accurate initial point clouds from blurred images. Without calculating camera
poses as an intermediate result, we avoid the cumulative errors transfer from
inaccurate camera poses to the initial point clouds' positions. Second, we
introduce the event stream into the deblur pipeline for its high sensitivity to
dynamic change. By decoding the latent sharp images from the event stream and
blurred images, we can provide a fine-grained supervision signal for scene
reconstruction optimization. Extensive experiments across a range of scenes
demonstrate that DeblurSplat not only excels in generating high-fidelity novel
views but also achieves significant rendering efficiency compared to the SOTAs
in deblur 3D-GS.

</details>


### [81] [MoiréNet: A Compact Dual-Domain Network for Image Demoiréing](https://arxiv.org/abs/2509.18910)
*Shuwei Guo,Simin Luan,Yan Ke,Zeyd Boukhers,John See,Cong Yang*

Main category: cs.CV

TL;DR: MoiréNet is a CNN-based demoiréing framework that fuses frequency and spatial features using a Directional Frequency-Spatial Encoder and a Frequency-Spatial Adaptive Selector, achieving state-of-the-art results with high parameter efficiency.


<details>
  <summary>Details</summary>
Motivation: Display-camera moiré patterns cause severe, multi-scale artifacts that are difficult to remove with purely spatial methods; there is a need for architectures that effectively exploit frequency-domain cues and directional information for robust demoiréing, while remaining lightweight for resource-constrained settings.

Method: A U-Net style backbone (MoiréNet) augmented with: (1) a Directional Frequency-Spatial Encoder (DFSE) that uses directional difference convolutions to detect moiré orientation; and (2) a Frequency-Spatial Adaptive Selector (FSAS) that adaptively suppresses artifacts in feature maps with frequency-spatial cues; training and evaluation on public and active datasets; achieve parameter efficiency (~5.5M parameters).

Result: The approach achieves state-of-the-art demoiréing performance on multiple public and actively used datasets while using significantly fewer parameters (5.513M), indicating superior restoration quality with high efficiency.

Conclusion: MoiréNet offers an effective, lightweight solution for demoiréing that leverages frequency-spatial fusion and directional analysis, suitable for mobile and AR applications where computational resources are limited.

Abstract: Moir\'e patterns arise from spectral aliasing between display pixel lattices
and camera sensor grids, manifesting as anisotropic, multi-scale artifacts that
pose significant challenges for digital image demoir\'eing. We propose
Moir\'eNet, a convolutional neural U-Net-based framework that synergistically
integrates frequency and spatial domain features for effective artifact
removal. Moir\'eNet introduces two key components: a Directional
Frequency-Spatial Encoder (DFSE) that discerns moir\'e orientation via
directional difference convolution, and a Frequency-Spatial Adaptive Selector
(FSAS) that enables precise, feature-adaptive suppression. Extensive
experiments demonstrate that Moir\'eNet achieves state-of-the-art performance
on public and actively used datasets while being highly parameter-efficient.
With only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,
Moir\'eNet combines superior restoration quality with parameter efficiency,
making it well-suited for resource-constrained applications including
smartphone photography, industrial imaging, and augmented reality.

</details>


### [82] [Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation](https://arxiv.org/abs/2509.18912)
*Yunzhe Shen,Kai Peng,Leiye Liu,Wei Ji,Jingjing Li,Miao Zhang,Yongri Piao,Huchuan Lu*

Main category: cs.CV

TL;DR: Proposes Frequency-Aware Audio-Visual Segmentation (FAVS) that reframes AVS as a frequency-domain decomposition problem. It introduces a Frequency-Domain Enhanced Decomposer (FDED) for modality-specific semantics and a Synergistic Cross-Modal Consistency (SCMC) module with a mixture-of-experts for dynamic, coherent cross-modal fusion. Achieves state-of-the-art on three benchmarks; code to be released.


<details>
  <summary>Details</summary>
Motivation: Current AVS methods neglect intrinsic frequency-domain differences between audio and visual data—audio high-frequency noise vs. rich visual high-frequency details—leading to suboptimal segmentation.

Method: Two main components: (1) Frequency-Domain Enhanced Decomposer (FDED) employing a residual-based iterative frequency decomposition to separate modality-specific semantics and structural cues; (2) Synergistic Cross-Modal Consistency (SCMC) using a mixture-of-experts with dynamic routing to reinforce semantic consistency while preserving modality-specific features.

Result: Extensive experiments show state-of-the-art performance on three AVS benchmarks; qualitative visualizations corroborate the effectiveness of FDED and SCMC; code will be released upon acceptance.

Conclusion: Reframing AVS as a frequency-domain decomposition problem enables more effective integration of audio and visual cues. The FDED and SCMC modules address both modality-specific and cross-modal coherence, yielding superior performance and robust qualitative results.

Abstract: Audio-visual segmentation (AVS) plays a critical role in multimodal machine
learning by effectively integrating audio and visual cues to precisely segment
objects or regions within visual scenes. Recent AVS methods have demonstrated
significant improvements. However, they overlook the inherent frequency-domain
contradictions between audio and visual modalities--the pervasively interfering
noise in audio high-frequency signals vs. the structurally rich details in
visual high-frequency signals. Ignoring these differences can result in
suboptimal performance. In this paper, we rethink the AVS task from a deeper
perspective by reformulating AVS task as a frequency-domain decomposition and
recomposition problem. To this end, we introduce a novel Frequency-Aware
Audio-Visual Segmentation (FAVS) framework consisting of two key modules:
Frequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal
Consistency (SCMC) module. FDED module employs a residual-based iterative
frequency decomposition to discriminate modality-specific semantics and
structural features, and SCMC module leverages a mixture-of-experts
architecture to reinforce semantic consistency and modality-specific feature
preservation through dynamic expert routing. Extensive experiments demonstrate
that our FAVS framework achieves state-of-the-art performance on three
benchmark datasets, and abundant qualitative visualizations further verify the
effectiveness of the proposed FDED and SCMC modules. The code will be released
as open source upon acceptance of the paper.

</details>


### [83] [xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision](https://arxiv.org/abs/2509.18913)
*Nguyen Van Tu,Pham Nguyen Hai Long,Vo Hoai Viet*

Main category: cs.CV

TL;DR: A survey of four representative xAI approaches for visual perception (Saliency Maps, CBMs, Prototype-based methods, and Hybrid approaches), analyzing their mechanisms, strengths/limitations, and evaluation metrics to guide future research.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for image analysis are powerful but opaque; interpretable AI is needed to understand decisions and ensure reliability in critical applications.

Method: Literature survey of four categories of xAI methods in visual perception; analysis of underlying mechanisms, strengths and limitations, and evaluation metrics; synthesis to guide future research and applications.

Result: Provides a comprehensive overview of the four approaches, highlighting their trade-offs and informing researchers about current capabilities and gaps.

Conclusion: A structured overview that clarifies where each approach stands, offers guidance for future work, and underscores the need for standardized evaluation and human-aligned explanations in visual AI.

Abstract: Deep learning has become the de facto standard and dominant paradigm in image
analysis tasks, achieving state-of-the-art performance. However, this approach
often results in "black-box" models, whose decision-making processes are
difficult to interpret, raising concerns about reliability in critical
applications. To address this challenge and provide human a method to
understand how AI model process and make decision, the field of xAI has
emerged. This paper surveys four representative approaches in xAI for visual
perception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),
(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their
underlying mechanisms, strengths and limitations, as well as evaluation
metrics, thereby providing a comprehensive overview to guide future research
and applications.

</details>


### [84] [LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2509.18917)
*Amirhesam Aghanouri,Cristina Olaverri-Monreal*

Main category: cs.CV

TL;DR: A diffusion-based LiDAR denoising/augmentation method improves AV perception by handling noisy/sparse data; introduces new noise scheduling and time-step embeddings, evaluated on IAMCV and KITTI-360 with multiple metrics, showing strong performance.


<details>
  <summary>Details</summary>
Motivation: To mitigate noise and sparsity in real-world LiDAR data, enabling better 3D perception for autonomous vehicles and providing high-quality synthetic data for augmentation.

Method: A denoising diffusion probabilistic model (DDPM) enhanced with novel noise scheduling and time-step embedding techniques, generating synthetic point clouds conditioned on projections to improve realism and temporal awareness.

Result: Outperforms most existing baselines on the tested datasets across four metrics, demonstrating effective denoising, diverse and richly structured point clouds, and robustness to noise/sparsity.

Conclusion: DDPM with the proposed scheduling/embedding strategies can reliably augment LiDAR data and enhance AV perception tasks, though details on computational cost and generalization remain important considerations.

Abstract: Autonomous vehicles (AVs) are expected to revolutionize transportation by
improving efficiency and safety. Their success relies on 3D vision systems that
effectively sense the environment and detect traffic agents. Among sensors AVs
use to create a comprehensive view of surroundings, LiDAR provides
high-resolution depth data enabling accurate object detection, safe navigation,
and collision avoidance. However, collecting real-world LiDAR data is
time-consuming and often affected by noise and sparsity due to adverse weather
or sensor limitations. This work applies a denoising diffusion probabilistic
model (DDPM), enhanced with novel noise scheduling and time-step embedding
techniques to generate high-quality synthetic data for augmentation, thereby
improving performance across a range of computer vision tasks, particularly in
AV perception. These modifications impact the denoising process and the model's
temporal awareness, allowing it to produce more realistic point clouds based on
the projection. The proposed method was extensively evaluated under various
configurations using the IAMCV and KITTI-360 datasets, with four performance
metrics compared against state-of-the-art (SOTA) methods. The results
demonstrate the model's superior performance over most existing baselines and
its effectiveness in mitigating the effects of noisy and sparse LiDAR data,
producing diverse point clouds with rich spatial relationships and structural
detail.

</details>


### [85] [Advancing Metallic Surface Defect Detection via Anomaly-Guided Pretraining on a Large Industrial Dataset](https://arxiv.org/abs/2509.18919)
*Chuni Liu,Hongjie Li,Jiaqi Du,Yangyang Hou,Qian Sun,Lei Jin,Ke Xu*

Main category: cs.CV

TL;DR: Anomaly-Guided Self-Supervised Pretraining (AGSSP) is proposed to tackle domain shift in metallic surface defect detection by guiding self-supervised learning with anomaly priors. It uses a two-stage pretraining: (1) backbone pretraining from anomaly maps to emphasize defect-salient features; (2) detector pretraining with pseudo-defect boxes derived from those maps. A high-quality anomaly-map generation method and a 120k-image industrial dataset are introduced, plus two small pixel-level labeled defect datasets. Empirical results show consistent gains over ImageNet-pretrained baselines (up to ~10% mAP@0.5 and 11.4% mAP@0.5:0.95).


<details>
  <summary>Details</summary>
Motivation: Data scarcity and domain gap: pretraining on natural images (e.g., ImageNet) mismatches industrial defect data; naive self-supervised pretraining struggles to distinguish subtle defects from complex backgrounds. There is a need for anomaly-aware pretraining to align representations with localization tasks in industrial contexts.

Method: A two-stage framework: (1) pretrain backbone by distilling knowledge from anomaly maps to highlight defect-relevant features; (2) pretrain detector using pseudo-defect boxes derived from those maps. Also introduces a knowledge-enhanced anomaly-map generator and builds a large-scale industrial dataset (120k images), plus two small pixel-level labeled datasets for validation.

Result: AGSSP consistently improves defect detection performance across settings, achieving up to 10% improvement in mAP@0.5 and 11.4% in mAP@0.5:0.95 compared with ImageNet-based models.

Conclusion: AGSSP effectively narrows the domain gap for industrial defect detection, providing a practical pretraining paradigm that leverages anomaly priors; code, pretrained models, and datasets are released for reproducibility and broader validation.

Abstract: The pretraining-finetuning paradigm is a crucial strategy in metallic surface
defect detection for mitigating the challenges posed by data scarcity. However,
its implementation presents a critical dilemma. Pretraining on natural image
datasets such as ImageNet, faces a significant domain gap. Meanwhile, naive
self-supervised pretraining on in-domain industrial data is often ineffective
due to the inability of existing learning objectives to distinguish subtle
defect patterns from complex background noise and textures. To resolve this, we
introduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigm
that explicitly guides representation learning through anomaly priors. AGSSP
employs a two-stage framework: (1) it first pretrains the model's backbone by
distilling knowledge from anomaly maps, encouraging the network to capture
defect-salient features; (2) it then pretrains the detector using pseudo-defect
boxes derived from these maps, aligning it with localization tasks. To enable
this, we develop a knowledge-enhanced method to generate high-quality anomaly
maps and collect a large-scale industrial dataset of 120,000 images.
Additionally, we present two small-scale, pixel-level labeled metallic surface
defect datasets for validation. Extensive experiments demonstrate that AGSSP
consistently enhances performance across various settings, achieving up to a
10\% improvement in mAP@0.5 and 11.4\% in mAP@0.5:0.95 compared to
ImageNet-based models. All code, pretrained models, and datasets are publicly
available at https://clovermini.github.io/AGSSP-Dev/.

</details>


### [86] [Audio-Driven Universal Gaussian Head Avatars](https://arxiv.org/abs/2509.18924)
*Kartik Teotia,Helge Rhodin,Mohit Mendiratta,Hyeongwoo Kim,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: First generalizable audio-driven avatar system that maps audio to a universal head avatar latent space (UHAP) combining appearance and geometry, enabling realistic lip-sync and expressions across identities.


<details>
  <summary>Details</summary>
Motivation: Existing methods mostly map audio to geometric deformations and ignore audio-dependent appearance variations. There is a need for a universal, cross-identity avatar synthesis approach that generalizes to new subjects and captures both geometry and appearance, with efficient personalization.

Method: Introduce UHAP trained on cross-identity multi-view videos and neutral scans to capture identity-specific details. A universal speech model maps raw audio into the UHAP latent expression space, which encodes geometry and appearance changes. A monocular encoder enables lightweight regression of dynamic expression variations for new subjects, allowing a fine-tuning stage to focus on global appearance and geometry. Decoding the expression codes via UHAP yields avatars with lip-sync and nuanced expressions (eyebrows, gaze, mouth interior).

Result: The method yields highly realistic avatars with accurate lip synchronization and expressive details, and is claimed to be the first generalizable audio-driven avatar model that accounts for detailed appearance—outperforming geometry-only baselines in lip-sync accuracy, image quality, and perceptual realism.

Conclusion: The work demonstrates a unified approach to audio-driven avatar synthesis that integrates appearance and geometry through a universal head avatar prior, enabling generalizable, high-fidelity, lip-synced avatars across identities and subjects.

Abstract: We introduce the first method for audio-driven universal photorealistic
avatar synthesis, combining a person-agnostic speech model with our novel
Universal Head Avatar Prior (UHAP). UHAP is trained on cross-identity
multi-view videos. In particular, our UHAP is supervised with neutral scan
data, enabling it to capture the identity-specific details at high fidelity. In
contrast to previous approaches, which predominantly map audio features to
geometric deformations only while ignoring audio-dependent appearance
variations, our universal speech model directly maps raw audio inputs into the
UHAP latent expression space. This expression space inherently encodes, both,
geometric and appearance variations. For efficient personalization to new
subjects, we employ a monocular encoder, which enables lightweight regression
of dynamic expression variations across video frames. By accounting for these
expression-dependent changes, it enables the subsequent model fine-tuning stage
to focus exclusively on capturing the subject's global appearance and geometry.
Decoding these audio-driven expression codes via UHAP generates highly
realistic avatars with precise lip synchronization and nuanced expressive
details, such as eyebrow movement, gaze shifts, and realistic mouth interior
appearance as well as motion. Extensive evaluations demonstrate that our method
is not only the first generalizable audio-driven avatar model that can account
for detailed appearance modeling and rendering, but it also outperforms
competing (geometry-only) methods across metrics measuring lip-sync accuracy,
quantitative image quality, and perceptual realism.

</details>


### [87] [SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic Spines](https://arxiv.org/abs/2509.18926)
*Pamela Osuna-Vargas,Altug Kamacioglu,Dominik F. Aschauer,Petros E. Vlachos,Sercan Alipek,Jochen Triesch,Simon Rumpel,Matthias Kaschube*

Main category: cs.CV

TL;DR: A modular ML pipeline (SynapFlow) automates detection, depth-tracking, time-tracking, and feature extraction of dendritic spines in 3D+time two-photon microscopy data, validated on public and newly annotated datasets, with released code/data to establish a scalable baseline for spine dynamics analysis.


<details>
  <summary>Details</summary>
Motivation: Dendritic spines' size and dynamics are key to synaptic efficacy and learning/memory. 3D+time spine analyses are labor-intensive and challenging, hindering large-scale studies. An automated, end-to-end pipeline is needed for scalable, reproducible spine dynamics analysis.

Method: A modular pipeline combining a transformer-based detector, a depth-tracking component using spatial features, a time-tracking module enforcing spatial consistency to link spine detections across time, and a feature extraction unit to quantify spine properties. Validation on open-source labeled data and two newly published annotated datasets (one for detection/depth-tracking, one for time-tracking). Public release of data, code, and pre-trained weights.

Result: Demonstrates end-to-end automation for spine detection, depth-tracking, and time-tracking with publicly available datasets and datasets published by the authors. Provides a reproducible baseline for scalable spine-dynamics analysis in 3D+time microscopy.

Conclusion: The pipeline enables scalable, end-to-end analysis of dendritic spine dynamics, including a first-ever dataset for spine time-tracking, and establishes a baseline for future research and method improvements in 3D+time spine analysis.

Abstract: Dendritic spines are key structural components of excitatory synapses in the
brain. Given the size of dendritic spines provides a proxy for synaptic
efficacy, their detection and tracking across time is important for studies of
the neural basis of learning and memory. Despite their relevance, large-scale
analyses of the structural dynamics of dendritic spines in 3D+time microscopy
data remain challenging and labor-intense. Here, we present a modular machine
learning-based pipeline designed to automate the detection, time-tracking, and
feature extraction of dendritic spines in volumes chronically recorded with
two-photon microscopy. Our approach tackles the challenges posed by biological
data by combining a transformer-based detection module, a depth-tracking
component that integrates spatial features, a time-tracking module to associate
3D spines across time by leveraging spatial consistency, and a feature
extraction unit that quantifies biologically relevant spine properties. We
validate our method on open-source labeled spine data, and on two complementary
annotated datasets that we publish alongside this work: one for detection and
depth-tracking, and one for time-tracking, which, to the best of our knowledge,
is the first data of this kind. To encourage future research, we release our
data, code, and pre-trained weights at
https://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable,
end-to-end analysis of dendritic spine dynamics.

</details>


### [88] [No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning](https://arxiv.org/abs/2509.18938)
*Matheus Vinícius Todescato,Joel Luís Carbonera*

Main category: cs.CV

TL;DR: A zero-shot image classifier is built by a self-learning cycle that combines a vision-language model (VLM) with a pre-trained visual model using only class names and no labeled data. It uses confidence-based pseudo-labeling on test samples, with the VLM selecting high-confidence samples and the visual model enhancing their features to iteratively train a lightweight classifier. No VLM fine-tuning or large language models are used. It outperforms a baseline zero-shot method across ten datasets.


<details>
  <summary>Details</summary>
Motivation: To address the data scarcity in practical applications by reducing dependence on large labeled datasets. The work leverages vision-language priors and pre-trained visual representations to enable effective zero-shot classification without extensive supervision.

Method: During inference, a confidence-based pseudo-labeling loop selects high-confidence test samples by the VLM and enhances their features with a pre-trained visual model. A lightweight classifier is trained on-the-fly using these pseudo-labels, and the process iterates to refine the decision boundary. The VLM is not fine-tuned, and no large language models are required; only class names are used.

Result: Empirical evaluation on ten diverse datasets shows the proposed method surpasses baseline zero-shot approaches, demonstrating improved generalization and adaptability in data-scarce scenarios.

Conclusion: The study demonstrates that fusing a VLM with a pre-trained visual model in a self-learning cycle yields robust zero-shot image classification without labeled data, offering a practical path for adapting vision systems to new domains with minimal supervision.

Abstract: While deep learning, including Convolutional Neural Networks (CNNs) and
Vision Transformers (ViTs), has significantly advanced classification
performance, its typical reliance on extensive annotated datasets presents a
major obstacle in many practical scenarios where such data is scarce.
Vision-language models (VLMs) and transfer learning with pre-trained visual
models appear as promising techniques to deal with this problem. This paper
proposes a novel zero-shot image classification framework that combines a VLM
and a pre-trained visual model within a self-learning cycle. Requiring only the
set of class names and no labeled training data, our method utilizes a
confidence-based pseudo-labeling strategy to train a lightweight classifier
directly on the test data, enabling dynamic adaptation. The VLM identifies
high-confidence samples, and the pre-trained visual model enhances their visual
representations. These enhanced features then iteratively train the classifier,
allowing the system to capture complementary semantic and visual cues without
supervision. Notably, our approach avoids VLM fine-tuning and the use of large
language models, relying on the visual-only model to reduce the dependence on
semantic representation. Experimental evaluations on ten diverse datasets
demonstrate that our approach outperforms the baseline zero-shot method.

</details>


### [89] [Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting](https://arxiv.org/abs/2509.18956)
*Zijing Guo,Yunyang Zhao,Lin Wang*

Main category: cs.CV

TL;DR: Introduces MirrorScene3D and ReflectiveGS to exploit mirror reflections for improved 3D reconstruction in mirror-rich environments; achieves state-of-the-art results on a new benchmark.


<details>
  <summary>Details</summary>
Motivation: Mirrors cause view-dependent distortions and inconsistencies that degrade NeRF/3DGS performance. Reflections contain complementary information that can fill missing details, but current work largely ignores this potential.

Method: Constructs MirrorScene3D, a dataset with 1256 high-quality images and annotated mirror masks. Proposes ReflectiveGS, an extension of 3D Gaussian Splatting that treats mirror reflections as informative viewpoints rather than mere symmetry artifacts to enhance geometry and recover missing details.

Result: On MirrorScene3D, ReflectiveGS outperforms existing methods in SSIM, PSNR, LPIPS, and training speed.

Conclusion: Leveraging mirror reflections as informative cues can significantly improve 3D reconstruction in mirror-rich environments; the dataset and ReflectiveGS establish a new benchmark and baseline for future work.

Abstract: Mirror-containing environments pose unique challenges for 3D reconstruction
and novel view synthesis (NVS), as reflective surfaces introduce view-dependent
distortions and inconsistencies. While cutting-edge methods such as Neural
Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical
scenes, their performance deteriorates in the presence of mirrors. Existing
solutions mainly focus on handling mirror surfaces through symmetry mapping but
often overlook the rich information carried by mirror reflections. These
reflections offer complementary perspectives that can fill in absent details
and significantly enhance reconstruction quality. To advance 3D reconstruction
in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset
featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror
masks, providing a benchmark for evaluating reconstruction methods in
reflective settings. Building on this, we propose ReflectiveGS, an extension of
3D Gaussian Splatting that utilizes mirror reflections as complementary
viewpoints rather than simple symmetry artifacts, enhancing scene geometry and
recovering absent details. Experiments on MirrorScene3D show that
ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and
training speed, setting a new benchmark for 3D reconstruction in mirror-rich
environments.

</details>


### [90] [Generative data augmentation for biliary tract detection on intraoperative images](https://arxiv.org/abs/2509.18958)
*Cristina Iacono,Mariarosaria Meola,Federica Conte,Laura Mecozzi,Umberto Bracale,Pietro Falco,Fanny Ficuciello*

Main category: cs.CV

TL;DR: Proposes a deep-learning pipeline for biliary tract localization in laparoscopic cholecystectomy using YOLO, with dataset creation and GAN-based data augmentation; discusses results and ethical considerations.


<details>
  <summary>Details</summary>
Motivation: To reduce bile duct injuries and improve intraoperative visualization during laparoscopic cholecystectomy, which significantly affect patient quality of life and survival.

Method: Build and annotate a white-light image database, train a YOLO-based biliary tract detector, apply traditional data augmentation, and use a Generative Adversarial Network to generate synthetic training data; discuss experimental results and ethical considerations.

Result: Experimental results are discussed, but no specific quantitative metrics are provided in the abstract.

Conclusion: Deep-learning-based biliary tract localization shows promise for improving intraoperative visualization and potentially reducing bile duct injuries; GAN-based data augmentation is a feasible approach; ethical considerations are acknowledged.

Abstract: Cholecystectomy is one of the most frequently performed procedures in
gastrointestinal surgery, and the laparoscopic approach is the gold standard
for symptomatic cholecystolithiasis and acute cholecystitis. In addition to the
advantages of a significantly faster recovery and better cosmetic results, the
laparoscopic approach bears a higher risk of bile duct injury, which has a
significant impact on quality of life and survival. To avoid bile duct injury,
it is essential to improve the intraoperative visualization of the bile duct.
This work aims to address this problem by leveraging a deep-learning approach
for the localization of the biliary tract from white-light images acquired
during the surgical procedures. To this end, the construction and annotation of
an image database to train the Yolo detection algorithm has been employed.
Besides classical data augmentation techniques, the paper proposes Generative
Adversarial Network (GAN) for the generation of a synthetic portion of the
training dataset. Experimental results have been discussed along with ethical
considerations.

</details>


### [91] [Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images](https://arxiv.org/abs/2509.18973)
*Jiabao Chen,Shan Xiong,Jialin Peng*

Main category: cs.CV

TL;DR: Prompt-DAS is a promptable multitask framework for domain adaptive segmentation of organelle instances in electron microscopy. It supports unsupervised, weakly supervised, and interactive segmentation through flexible point prompts, without needing per-instance prompts like SAM. It introduces a center-point auxiliary task and prompt-guided contrastive learning, and achieves superior performance over UDA, WDA, and SAM-based methods on challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: Domain adaptive segmentation (DAS) for large-scale EM data requires annotation-efficient learning. SAM-like per-instance prompts are data-hungry and restrict flexibility. There is a need for a flexible framework that can operate with various prompt configurations (full, sparse, or no prompts) and support interactive testing while maintaining strong performance.

Method: We propose Prompt-DAS, a promptable multitask framework. It trains with multiple prompt configurations and is adaptable for unsupervised domain adaptation (UDA), weakly supervised domain adaptation (WDA), and interactive segmentation at test time. Unlike SAM, it does not require a prompt per object; it can use full prompts, sparse prompts, or no prompts, aided by an auxiliary center-point detection task. A novel prompt-guided contrastive learning objective is incorporated to enhance discriminative feature learning.

Result: Comprehensive experiments on challenging benchmarks show that Prompt-DAS outperforms existing UDA, WDA, and SAM-based approaches in domain adaptive segmentation of organelle instances in EM, demonstrating robustness to different prompt configurations and improved annotation efficiency.

Conclusion: Prompt-DAS provides a flexible, data-efficient solution for domain adaptive segmentation in EM. By combining a promptable multitask design with center-point supervision and prompt-guided contrastive learning, it enables unsupervised, weakly supervised, and interactive segmentation with competitive performance compared to SAM-based baselines.

Abstract: Domain adaptive segmentation (DAS) of numerous organelle instances from
large-scale electron microscopy (EM) is a promising way to enable
annotation-efficient learning. Inspired by SAM, we propose a promptable
multitask framework, namely Prompt-DAS, which is flexible enough to utilize any
number of point prompts during the adaptation training stage and testing stage.
Thus, with varying prompt configurations, Prompt-DAS can perform unsupervised
domain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well
as interactive segmentation during testing. Unlike the foundation model SAM,
which necessitates a prompt for each individual object instance, Prompt-DAS is
only trained on a small dataset and can utilize full points on all instances,
sparse points on partial instances, or even no points at all, facilitated by
the incorporation of an auxiliary center-point detection task. Moreover, a
novel prompt-guided contrastive learning is proposed to enhance discriminative
feature learning. Comprehensive experiments conducted on challenging benchmarks
demonstrate the effectiveness of the proposed approach over existing UDA, WDA,
and SAM-based approaches.

</details>


### [92] [VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction](https://arxiv.org/abs/2509.19002)
*Hao Wang,Eiki Murata,Lingfang Zhang,Ayako Sato,So Fukuda,Ziqi Yin,Wentao Hu,Keisuke Nakao,Yusuke Nakamura,Sebastian Zwirner,Yi-Chia Chen,Hiroyuki Otomo,Hiroki Ouchi,Daisuke Kawahara*

Main category: cs.CV

TL;DR: VIR-Bench introduces a 200-video benchmark for itinerary reconstruction to test geospatial-temporal intelligence in multimodal LMs; results show current models struggle; a travel-planning agent using VIR-Bench insights achieves better recommendations.


<details>
  <summary>Details</summary>
Motivation: To address the absence of benchmarks that evaluate long-range geospatial-temporal understanding in video-based multimodal LLMs, which is essential for embodied AI planning and navigation.

Method: Construct VIR-Bench with 200 travel videos focused on itinerary reconstruction; evaluate current state-of-the-art mult-modal LLMs on this task; conduct a case study by developing a prototype travel-planning agent that leverages VIR-Bench insights.

Result: State-of-the-art MLLMs underperform on VIR-Bench, highlighting the difficulty of long-distance spatiotemporal reasoning in videos; the travel-planning agent demonstrates markedly improved itinerary recommendations.

Conclusion: VIR-Bench provides an effective benchmark for geospatial-temporal intelligence in MLLMs and translates into concrete performance gains for user-facing travel planning applications.

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced video understanding capabilities, opening new
possibilities for practical applications. Yet current video benchmarks focus
largely on indoor scenes or short-range outdoor activities, leaving the
challenges associated with long-distance travel largely unexplored. Mastering
extended geospatial-temporal trajectories is critical for next-generation
MLLMs, underpinning real-world tasks such as embodied-AI planning and
navigation. To bridge this gap, we present VIR-Bench, a novel benchmark
consisting of 200 travel videos that frames itinerary reconstruction as a
challenging task designed to evaluate and push forward MLLMs'
geospatial-temporal intelligence. Experimental results reveal that
state-of-the-art MLLMs, including proprietary ones, struggle to achieve high
scores, underscoring the difficulty of handling videos that span extended
spatial and temporal scales. Moreover, we conduct an in-depth case study in
which we develop a prototype travel-planning agent that leverages the insights
gained from VIR-Bench. The agent's markedly improved itinerary recommendations
verify that our evaluation protocol not only benchmarks models effectively but
also translates into concrete performance gains in user-facing applications.

</details>


### [93] [Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards](https://arxiv.org/abs/2509.19003)
*Honghao Chen,Xingzhou Lou,Xiaokun Feng,Kaiqi Huang,Xinlong Wang*

Main category: cs.CV

TL;DR: Introduces a fine-grained step-level reasoning framework for vision-language models with a process reward model and reinforcement learning, enabling accurate evaluation of intermediate steps, improved performance, and scalable inference, plus a dataset PRM and code release.


<details>
  <summary>Details</summary>
Motivation: Current vision-language chain-of-thought methods rely on coarse reasoning and lack reliable evaluation of intermediate steps; there is a need for fine-grained rewards and scalable RL/inference to guide learning and assessment.

Method: Proposes a simple, transparent framework comprising (1) step-level reasoning data; (2) a process reward model (PRM) to judge step quality; (3) reinforcement learning training; and investigates inference-time scaling with fine-grained rewards; includes thorough ablation studies.

Result: Achieves strong baselines and consistent improvements on challenging vision-language benchmarks; provides thorough empirical analysis showing the impact of each component and the benefits of inference-time scaling; releases the PRM dataset and code.

Conclusion: The work establishes a solid baseline for vision-language reasoning with verifiable, stepwise reasoning, and offers practical insights and resources (PRM dataset/code) to support future multimodal reasoning research.

Abstract: Chain of thought reasoning has demonstrated remarkable success in large
language models, yet its adaptation to vision-language reasoning remains an
open challenge with unclear best practices. Existing attempts typically employ
reasoning chains at a coarse-grained level, which struggles to perform
fine-grained structured reasoning and, more importantly, are difficult to
evaluate the reward and quality of intermediate reasoning. In this work, we
delve into chain of step reasoning for vision-language models, enabling
assessing reasoning step quality accurately and leading to effective
reinforcement learning and inference-time scaling with fine-grained rewards. We
present a simple, effective, and fully transparent framework, including the
step-level reasoning data, process reward model (PRM), and reinforcement
learning training. With the proposed approaches, our models set strong
baselines with consistent improvements on challenging vision-language
benchmarks. More importantly, we conduct a thorough empirical analysis and
ablation study, unveiling the impact of each component and several intriguing
properties of inference-time scaling. We believe this paper serves as a
baseline for vision-language models and offers insights into more complex
multimodal reasoning. Our dataset, PRM, and code will be available at
https://github.com/baaivision/CoS.

</details>


### [94] [Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model](https://arxiv.org/abs/2509.19028)
*Ioannis Sarafis,Alexandros Papadopoulos,Anastasios Delopoulos*

Main category: cs.CV

TL;DR: Weakly supervised food image segmentation using ViT CAMs to prompt SAM; multi-mask SAM improves masks, evaluated on FoodSeg103 with 0.54 mIoU.


<details>
  <summary>Details</summary>
Motivation: Reduce annotation effort by combining CAM-based prompts with SAM, enabling segmentation with only image-level labels.

Method: Train Swin Transformer with image-level labels; generate class activation maps from ViT to prompt SAM; experiment with image preprocessing; produce single- and multi-mask SAM outputs.

Result: Average 2.4 masks per image (excluding background); multi-mask SAM achieves mIoU 0.54 on FoodSeg103.

Conclusion: This pipeline offers a practical tool to speed up food image annotation and can be integrated into nutrition apps; demonstrates effective weak supervision by fusing CAMs and SAM for food segmentation.

Abstract: In this paper, we propose a weakly supervised semantic segmentation approach
for food images which takes advantage of the zero-shot capabilities and
promptability of the Segment Anything Model (SAM) along with the attention
mechanisms of Vision Transformers (ViTs). Specifically, we use class activation
maps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable
for food image segmentation. The ViT model, a Swin Transformer, is trained
exclusively using image-level annotations, eliminating the need for pixel-level
annotations during training. Additionally, to enhance the quality of the
SAM-generated masks, we examine the use of image preprocessing techniques in
combination with single-mask and multi-mask SAM generation strategies. The
methodology is evaluated on the FoodSeg103 dataset, generating an average of
2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for
the multi-mask scenario. We envision the proposed approach as a tool to
accelerate food image annotation tasks or as an integrated component in food
and nutrition tracking applications.

</details>


### [95] [A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation](https://arxiv.org/abs/2509.19052)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: DyL-UNet is a dynamic, temporally-consistent segmentation framework for echocardiography that combines multiple Swin-Transformer branches with Cardiac Phase-Dynamics Attention guided by an Echo-Dynamics Graph to improve stability without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Echocardiographic segmentation suffers from frame-to-frame deformation and speckle noise, causing temporal jitter that harms clinical interpretability and functional estimates; a method is needed to achieve both high accuracy and temporal stability.

Method: DyL-UNet constructs an Echo-Dynamics Graph (EDG) via dynamic learning to capture motion information. It uses multiple Swin-Transformer-based encoder-decoder branches for single-frame processing and introduces Cardiac Phase-Dynamics Attention (CPDA) at skip connections, leveraging EDG features and cardiac-phase cues to enforce temporal consistency during segmentation.

Result: On CAMUS and EchoNet-Dynamic, DyL-UNet achieves segmentation accuracy comparable to existing methods while delivering superior temporal consistency.

Conclusion: Dynamic, temporally-consistent segmentation is viable for automated clinical echocardiography, potentially improving reliability of functional estimates without sacrificing per-frame accuracy.

Abstract: Accurate segmentation of cardiac anatomy in echocardiography is essential for
cardiovascular diagnosis and treatment. Yet echocardiography is prone to
deformation and speckle noise, causing frame-to-frame segmentation jitter. Even
with high accuracy in single-frame segmentation, temporal instability can
weaken functional estimates and impair clinical interpretability. To address
these issues, we propose DyL-UNet, a dynamic learning-based temporal
consistency U-Net segmentation architecture designed to achieve temporally
stable and precise echocardiographic segmentation. The framework constructs an
Echo-Dynamics Graph (EDG) through dynamic learning to extract dynamic
information from videos. DyL-UNet incorporates multiple Swin-Transformer-based
encoder-decoder branches for processing single-frame images. It further
introduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections,
which uses EDG-encoded dynamic features and cardiac-phase cues to enforce
temporal consistency during segmentation. Extensive experiments on the CAMUS
and EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation
accuracy comparable to existing methods while achieving superior temporal
consistency, providing a reliable solution for automated clinical
echocardiography.

</details>


### [96] [ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?](https://arxiv.org/abs/2509.19070)
*Zijian Ling,Han Zhang,Yazhuo Zhou,Jiahao Cui*

Main category: cs.CV

TL;DR: A benchmark (ColorBlindnessEval) to test Vision-Language Model robustness in Ishihara-like visuals; 500 images of numbers 0–99 with varied color patterns; evaluates 9 VLMs using Yes/No and open-ended prompts vs humans; reveals hallucination and robustness gaps; aims to improve reliability in critical real-world tasks.


<details>
  <summary>Details</summary>
Motivation: Address robustness gaps of Vision-Language Models in visually challenging, color-based tasks where accurate extraction of numerical information is essential.

Method: Construct 500 Ishihara-like images embedding numbers 0–99 with diverse color combinations; evaluate 9 VLMs using Yes/No and open-ended prompts; compare results to human participants.

Result: Found notable limitations in VLMs' ability to interpret numbers in adversarial contexts; hallucination issues observed; humans outperform models on the task.

Conclusion: ColorBlindnessEval provides a valuable benchmark for measuring and guiding improvements in the robustness and reliability of VLMs in complex visual environments requiring precise numeral recognition.

Abstract: This paper presents ColorBlindnessEval, a novel benchmark designed to
evaluate the robustness of Vision-Language Models (VLMs) in visually
adversarial scenarios inspired by the Ishihara color blindness test. Our
dataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with
varying color combinations, challenging VLMs to accurately recognize numerical
information embedded in complex visual patterns. We assess 9 VLMs using Yes/No
and open-ended prompts and compare their performance with human participants.
Our experiments reveal limitations in the models' ability to interpret numbers
in adversarial contexts, highlighting prevalent hallucination issues. These
findings underscore the need to improve the robustness of VLMs in complex
visual environments. ColorBlindnessEval serves as a valuable tool for
benchmarking and improving the reliability of VLMs in real-world applications
where accuracy is critical.

</details>


### [97] [WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction](https://arxiv.org/abs/2509.19073)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: Proposes WaveletGaussian for sparse-view 3D Gaussian reconstruction, reducing diffusion computation by performing diffusion in the wavelet domain's low-frequency LL subband and refining high-frequency details with a lightweight network, plus an online masking strategy to generate diffusion-training pairs, achieving competitive rendering with less training time.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting struggles in sparse views; existing diffusion-repair approaches are accurate but computationally heavy; need efficiency without sacrificing quality.

Method: Diffusion operates only on the low-resolution LL subband of wavelet-decomposed renders; high-frequency subbands refined by a lightweight network; online random masking strategy to curate training pairs for diffusion fine-tuning instead of leave-one-out; training on two benchmarks.

Result: Achieves competitive rendering quality while substantially reducing training time; validated on Mip-NeRF 360 and OmniObject3D.

Conclusion: WaveletGaussian provides an efficient, effective sparse-view 3D Gaussian reconstruction by combining wavelet-domain diffusion with high-frequency refinement and smarter training data curation; reduces computation while preserving quality.

Abstract: 3D Gaussian Splatting (3DGS) has become a powerful representation for
image-based object reconstruction, yet its performance drops sharply in
sparse-view settings. Prior works address this limitation by employing
diffusion models to repair corrupted renders, subsequently using them as pseudo
ground truths for later optimization. While effective, such approaches incur
heavy computation from the diffusion fine-tuning and repair steps. We present
WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object
reconstruction. Our key idea is to shift diffusion into the wavelet domain:
diffusion is applied only to the low-resolution LL subband, while
high-frequency subbands are refined with a lightweight network. We further
propose an efficient online random masking strategy to curate training pairs
for diffusion fine-tuning, replacing the commonly used, but inefficient,
leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360
and OmniObject3D, show WaveletGaussian achieves competitive rendering quality
while substantially reducing training time.

</details>


### [98] [3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference](https://arxiv.org/abs/2509.19082)
*Alexey Nekrasov,Ali Athar,Daan de Geus,Alexander Hermans,Bastian Leibe*

Main category: cs.CV

TL;DR: Sa2VA-i fixes training–inference inconsistencies in Sa2VA, yielding state-of-the-art results for referring video object segmentation across multiple benchmarks, with substantial gains and parity with larger models on MeViS; code released.


<details>
  <summary>Details</summary>
Motivation: Inconsistencies between training and inference in the original Sa2VA limit its performance on referring video segmentation; addressing these 

Method: Proposes Sa2VA-i, an improved version that rectifies training/inference issues and uses the same Sa2VA checkpoints for fair comparison; evaluates on MeViS, Ref-YT-VOS, Ref-DAVIS, and ReVOS, reporting substantial gains and showing parity with larger models on MeViS; provides code and updated models at the project repository.

Result: Sa2VA-i establishes new state of the art on multiple video benchmarks with gains up to +11.6 J&F on MeViS, +1.4 on Ref-YT-VOS, +3.3 on Ref-DAVIS, and +4.1 on ReVOS; the Sa2VA-i-1B model even matches the Sa2VA-26B model on MeViS using the same checkpoints; code and updated models released.

Conclusion: Highlights that seemingly minor implementation details can dramatically affect performance; the work provides actionable insights for the referring video segmentation field and offers open-source resources to reproduce and build upon the improvements.

Abstract: Sa2VA is a recent model for language-guided dense grounding in images and
video that achieves state-of-the-art results on multiple segmentation
benchmarks and that has become widely popular. However, we found that Sa2VA
does not perform according to its full potential for referring video object
segmentation tasks. We identify inconsistencies between training and inference
procedures as the key factor holding it back. To mitigate this issue, we
propose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and
improves the results. In fact, Sa2VA-i sets a new state of the art for multiple
video benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on
Ref-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA
checkpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the
original Sa2VA-26B model on the MeViS benchmark. We hope that this work will
show the importance of seemingly trivial implementation details and that it
will provide valuable insights for the referring video segmentation field. We
provide the code and updated models at https://github.com/kumuji/sa2va-i

</details>


### [99] [Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications](https://arxiv.org/abs/2509.19087)
*Ganesh Mallya,Yotam Gigi,Dahun Kim,Maxim Neumann,Genady Beryozkin,Tomer Shekel,Anelia Angelova*

Main category: cs.CV

TL;DR: A training-free zero-shot method to map multispectral inputs to RGB-trained generalist multimodal models, achieving strong zero-shot land-cover/land-use classification without retraining (demonstrated with Gemini2.5).


<details>
  <summary>Details</summary>
Motivation: Enable the use of powerful generalist multimodal models with non-RGB multispectral remote sensing data, reducing the need to train specialized multispectral models and expanding applicability of RGB-trained models.

Method: A training-free approach that adapts multispectral data to the RGB input space and injects domain-specific guidance as instructions to RGB-trained generalist multimodal models; demonstrated using Gemini2.5 in a zero-shot setting.

Result: Strong zero-shot performance gains on standard remote sensing benchmarks for land cover and land use classification; easy adaptation of Gemini2.5 to new multispectral inputs.

Conclusion: Geospatial professionals can leverage generalist multimodal models for non-standard, specialized inputs, gaining rich reasoning and context without additional model training.

Abstract: Multi-spectral imagery plays a crucial role in diverse Remote Sensing
applications including land-use classification, environmental monitoring and
urban planning. These images are widely adopted because their additional
spectral bands correlate strongly with physical materials on the ground, such
as ice, water, and vegetation. This allows for more accurate identification,
and their public availability from missions, such as Sentinel-2 and Landsat,
only adds to their value. Currently, the automatic analysis of such data is
predominantly managed through machine learning models specifically trained for
multi-spectral input, which are costly to train and support. Furthermore,
although providing a lot of utility for Remote Sensing, such additional inputs
cannot be used with powerful generalist large multimodal models, which are
capable of solving many visual problems, but are not able to understand
specialized multi-spectral signals.
  To address this, we propose a training-free approach which introduces new
multi-spectral data in a Zero-Shot-only mode, as inputs to generalist
multimodal models, trained on RGB-only inputs. Our approach leverages the
multimodal models' understanding of the visual space, and proposes to adapt to
inputs to that space, and to inject domain-specific information as instructions
into the model. We exemplify this idea with the Gemini2.5 model and observe
strong Zero-Shot performance gains of the approach on popular Remote Sensing
benchmarks for land cover and land use classification and demonstrate the easy
adaptability of Gemini2.5 to new inputs. These results highlight the potential
for geospatial professionals, working with non-standard specialized inputs, to
easily leverage powerful multimodal models, such as Gemini2.5, to accelerate
their work, benefiting from their rich reasoning and contextual capabilities,
grounded in the specialized sensor data.

</details>


### [100] [Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning](https://arxiv.org/abs/2509.19090)
*Guoxin Wang,Jun Zhao,Xinyi Liu,Yanbo Liu,Xuyang Cao,Chao Li,Zhuoyun Liu,Qintian Sun,Fangru Zhou,Haoqiang Xing,Zhenhong Yang*

Main category: cs.CV

TL;DR: Citrus-V is a unified multimodal medical foundation model that enables pixel-level lesion localization, structured reporting, and clinician-like reasoning by combining detection, segmentation, and multimodal chain-of-thought in a single framework, trained with a novel multimodal approach and open-source data, achieving superior benchmark performance.


<details>
  <summary>Details</summary>
Motivation: Current medical imaging models are narrow and require multiple specialized networks; while large-scale language and multimodal models show strong reasoning, real-world clinical use demands precise visual grounding, multimodal integration, and chain-of-thought reasoning.

Method: Develop Citrus-V architecture that integrates detection, segmentation, and multimodal chain-of-thought reasoning in a single framework. Introduce a novel multimodal training approach and release an open-source data suite covering reasoning, detection, segmentation, and document understanding tasks.

Result: Citrus-V outperforms existing open-source medical models and expert-level imaging systems across multiple benchmarks, delivering a unified pipeline from visual grounding to clinical reasoning and supporting precise lesion quantification, automated reporting, and reliable second opinions.

Conclusion: The work demonstrates the feasibility and value of a unified, reasoning-enabled medical foundation model for end-to-end clinical imaging workflows.

Abstract: Medical imaging provides critical evidence for clinical diagnosis, treatment
planning, and surgical decisions, yet most existing imaging models are narrowly
focused and require multiple specialized networks, limiting their
generalization. Although large-scale language and multimodal models exhibit
strong reasoning and multi-task capabilities, real-world clinical applications
demand precise visual grounding, multimodal integration, and chain-of-thought
reasoning. We introduce Citrus-V, a multimodal medical foundation model that
combines image analysis with textual reasoning. The model integrates detection,
segmentation, and multimodal chain-of-thought reasoning, enabling pixel-level
lesion localization, structured report generation, and physician-like
diagnostic inference in a single framework. We propose a novel multimodal
training approach and release a curated open-source data suite covering
reasoning, detection, segmentation, and document understanding tasks.
Evaluations demonstrate that Citrus-V outperforms existing open-source medical
models and expert-level imaging systems across multiple benchmarks, delivering
a unified pipeline from visual grounding to clinical reasoning and supporting
precise lesion quantification, automated reporting, and reliable second
opinions.

</details>


### [101] [Investigating Traffic Accident Detection Using Multimodal Large Language Models](https://arxiv.org/abs/2509.19096)
*Ilhan Skender,Kailin Tong,Selim Solmaz,Daniel Watzenig*

Main category: cs.CV

TL;DR: Zero-shot detection of traffic accidents from infrastructure images using multimodal LLMs; evaluated on CARLA DeepAccident; integrated with YOLO, Deep SORT, and SAM for improved prompts; Pixtral tops F1 (0.71) and recall (83%), Gemini 1.5/2.0 show precision gains with enhanced prompts but lower F1/recall, Gemma 3 is balanced; demonstrates potential of MLLMs with visual analytics for real-time traffic monitoring.


<details>
  <summary>Details</summary>
Motivation: Combat data scarcity for infrastructure-based accident detection and enable scalable, real-time monitoring by leveraging zero-shot capabilities of multimodal LLMs.

Method: Evaluate multiple MLLMs (Gemini 1.5/2.0, Gemma 3, Pixtral) on the simulated CARLA DeepAccident dataset without fine-tuning; enhance prompts with integrated visual analytics—YOLO for object detection, Deep SORT for tracking, and Segment Anything (SAM) for instance segmentation—to boost accuracy and explainability.

Result: Pixtral achieves the highest F1 of 0.71 with 83% recall; Gemini models gain precision with enhanced prompts (e.g., Gemini 1.5 up to 90%) but suffer F1 and recall losses; Gemma 3 offers the most balanced performance with minimal metric fluctuation.

Conclusion: Combining MLLMs with advanced visual analytics is a promising approach for automated, real-time traffic monitoring using infrastructure cameras, reducing reliance on large labeled datasets.

Abstract: Traffic safety remains a critical global concern, with timely and accurate
accident detection essential for hazard reduction and rapid emergency response.
Infrastructure-based vision sensors offer scalable and efficient solutions for
continuous real-time monitoring, facilitating automated detection of acci-
dents directly from captured images. This research investigates the zero-shot
capabilities of multimodal large language models (MLLMs) for detecting and
describing traffic accidents using images from infrastructure cameras, thus
minimizing reliance on extensive labeled datasets. Main contributions include:
(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,
explicitly addressing the scarcity of diverse, realistic, infrastructure-based
accident data through controlled simulations; (2) Comparative performance
analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent
identification and descriptive capabilities without prior fine-tuning; and (3)
Integration of advanced visual analytics, specifically YOLO for object
detection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for
instance segmentation, into enhanced prompts to improve model accuracy and
explainability. Key numerical results show Pixtral as the top performer with an
F1-score of 0.71 and 83% recall, while Gemini models gained precision with
enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and
recall losses. Gemma 3 offered the most balanced performance with minimal
metric fluctuation. These findings demonstrate the substantial potential of
integrating MLLMs with advanced visual analytics techniques, enhancing their
applicability in real-world automated traffic monitoring systems.

</details>


### [102] [Track-On2: Enhancing Online Point Tracking with Memory](https://arxiv.org/abs/2509.19115)
*Görkay Aydemir,Weidi Xie,Fatma Güney*

Main category: cs.CV

TL;DR: Track-On2 is a causal, transformer-based online long-term point tracker with memory that achieves state-of-the-art results on five benchmarks using synthetic training data.


<details>
  <summary>Details</summary>
Motivation: Robust long-term point tracking in video requires maintaining point identities across frames despite appearance changes, motion, and occlusion in an online, real-time setting. This demands causal processing and memory-based coherence, avoiding full-sequence access or iterative updates.

Method: Track-On2 extends Track-On with a simple, efficient transformer-based online architecture that uses a memory mechanism to maintain temporal coherence. It processes frames causally (no future frames) and performs coarse patch-level classification followed by refinement. It also investigates synthetic training setups to shape memory behavior and temporal robustness.

Result: Achieves state-of-the-art results across five synthetic and real-world benchmarks, surpassing prior online trackers and even strong offline methods that use bidirectional context, with improvements in both performance and efficiency.

Conclusion: Caused, memory-based architectures trained on synthetic data are effective and scalable for real-world online long-term point tracking, enabling real-time performance without access to future frames and reducing the need for costly online updates.

Abstract: In this paper, we consider the problem of long-term point tracking, which
requires consistent identification of points across video frames under
significant appearance changes, motion, and occlusion. We target the online
setting, i.e. tracking points frame-by-frame, making it suitable for real-time
and streaming applications. We extend our prior model Track-On into Track-On2,
a simple and efficient transformer-based model for online long-term tracking.
Track-On2 improves both performance and efficiency through architectural
refinements, more effective use of memory, and improved synthetic training
strategies. Unlike prior approaches that rely on full-sequence access or
iterative updates, our model processes frames causally and maintains temporal
coherence via a memory mechanism, which is key to handling drift and occlusions
without requiring future frames. At inference, we perform coarse patch-level
classification followed by refinement. Beyond architecture, we systematically
study synthetic training setups and their impact on memory behavior, showing
how they shape temporal robustness over long sequences. Through comprehensive
experiments, Track-On2 achieves state-of-the-art results across five synthetic
and real-world benchmarks, surpassing prior online trackers and even strong
offline methods that exploit bidirectional context. These results highlight the
effectiveness of causal, memory-based architectures trained purely on synthetic
data as scalable solutions for real-world point tracking. Project page:
https://kuis-ai.github.io/track_on2

</details>


### [103] [KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments](https://arxiv.org/abs/2509.19129)
*Adam Romlein,Benjamin X. Hou,Yuval Boss,Cynthia L. Christman,Stacie Koslovsky,Erin E. Moreland,Jason Parham,Anthony Hoogs*

Main category: cs.CV

TL;DR: KAMERA: a multi-camera, multi-spectral system for real-time detection of seals and polar bears in aerial surveys, achieving up to 80% faster processing and open-source release.


<details>
  <summary>Details</summary>
Motivation: Improve efficiency and accuracy of Arctic wildlife surveys by enabling synchronized, multi-spectral data capture, metadata-rich annotations, and georeferenced results.

Method: Design and calibration of a synchronized multi-camera, multi-spectral platform with real-time detection, comprehensive metadata annotation, and mapping of imagery/detections to a world plane; open-source release of software, models, and schematics.

Result: Up to 80% reduction in dataset processing time; multi-spectral detection capability; rich metadata; georeferenced mapping for area estimation; open-source release of software and schematics.

Conclusion: KAMERA provides a reusable, open framework for aerial wildlife detection and mapping that can inspire and accelerate similar efforts in the scientific community.

Abstract: We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral
synchronization and real-time detection of seals and polar bears. Utilized in
aerial surveys for ice-associated seals in the Bering, Chukchi, and Beaufort
seas around Alaska, KAMERA provides up to an 80% reduction in dataset
processing time over previous methods. Our rigorous calibration and hardware
synchronization enable using multiple spectra for object detection. All
collected data are annotated with metadata so they can be easily referenced
later. All imagery and animal detections from a survey are mapped onto a world
plane for accurate surveyed area estimates and quick assessment of survey
results. We hope KAMERA will inspire other mapping and detection efforts in the
scientific community, with all software, models, and schematics fully
open-sourced.

</details>


### [104] [NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and Dynamic Early-Exit](https://arxiv.org/abs/2509.19156)
*Maurf Hassan,Steven Davy,Muhammad Zawish,Owais Bin Zuber,Nouman Ashraf*

Main category: cs.CV

TL;DR: NeuCODEX is a neuromorphic co-inference framework that jointly reduces spatial and temporal redundancy using a learned spike-driven compression module and a dynamic early-exit mechanism, enabling edge-to-cloud SNN inference with dramatically lower data transfer and energy use and lower latency, while keeping accuracy loss under 2%.


<details>
  <summary>Details</summary>
Motivation: Spiking Neural Networks (SNNs) promise energy-efficient edge intelligence, but full edge inference incurs latency and energy penalties due to fixed high timesteps. Edge-cloud co-inference can help but introduces latency and data-transfer costs; there is a need to jointly optimize spatial and temporal redundancy to make practical, high-performance SNN deployment at the edge.

Method: NeuCODEX introduces a learned spike-driven compression module combined with a dynamic early-exit strategy within a neuromorphic co-inference architecture. It was evaluated on static image datasets (CIFAR-10 and Caltech) and neuromorphic streams (CIFAR10-DVS, N-Caltech), with prototyping on ResNet-18 and VGG-16 backbones in a real edge-to-cloud testbed.

Result: Data transfer is reduced by up to 2048x; edge energy consumption is reduced by over 90%; end-to-end latency is reduced by up to 3x compared to edge-only inference; accuracy loss is negligible, less than 2%.

Conclusion: NeuCODEX enables practical, high-performance SNN deployment in resource-constrained environments by effectively exploiting both spatial and temporal redundancies through spike-driven compression and adaptive early exit.

Abstract: Spiking Neural Networks (SNNs) offer significant potential for enabling
energy-efficient intelligence at the edge. However, performing full SNN
inference at the edge can be challenging due to the latency and energy
constraints arising from fixed and high timestep overheads. Edge-cloud
co-inference systems present a promising solution, but their deployment is
often hindered by high latency and feature transmission costs. To address these
issues, we introduce NeuCODEX, a neuromorphic co-inference architecture that
jointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a
learned spike-driven compression module to reduce data transmission and employs
a dynamic early-exit mechanism to adaptively terminate inference based on
output confidence. We evaluated NeuCODEX on both static images (CIFAR10 and
Caltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To
demonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16
backbones in a real edge-to-cloud testbed. Our proposed system reduces data
transfer by up to 2048x and edge energy consumption by over 90%, while reducing
end-to-end latency by up to 3x compared to edge-only inference, all with a
negligible accuracy drop of less than 2%. In doing so, NeuCODEX enables
practical, high-performance SNN deployment in resource-constrained
environments.

</details>


### [105] [RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions](https://arxiv.org/abs/2509.19165)
*Yun Wang,Junjie Hu,Junhui Hou,Chenghao Zhang,Renwei Yang,Dapeng Oliver Wu*

Main category: cs.CV

TL;DR: A robust self-supervised stereo matching framework (RoSe) for adverse weather, leveraging visual-foundation priors and scene-correspondence supervision, with synthetic weather-degraded data and a two-stage training (scene correspondence learning and adverse weather distillation), achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Self-supervised stereo performance degrades under adverse weather (night, rain, fog) due to degraded features and violations of photometric consistency; there is a need for robust representations and supervision that respect scene correspondence under challenging conditions.

Method: Inject robust priors from a visual foundation model into the CNN-based feature extractor; introduce scene correspondence priors to form robust supervisory signals beyond photometric loss; create synthetic stereo datasets with realistic weather degradations that preserve semantic context and disparity; propose a two-stage training paradigm: robust self-supervised scene correspondence learning and adverse weather distillation, aligning scene results between clean and adverse image pairs.

Result: Experiments show improved disparity estimation under adverse weather and outperform existing state-of-the-art self-supervised methods; code is released.

Conclusion: The proposed RoSe framework effectively mitigates weather-induced degradation in self-supervised stereo by combining foundation-model priors with scene-level supervision, offering versatile improvements and enabling robust disparity estimation in adverse conditions.

Abstract: Recent self-supervised stereo matching methods have made significant
progress, but their performance significantly degrades under adverse weather
conditions such as night, rain, and fog. We identify two primary weaknesses
contributing to this performance degradation. First, adverse weather introduces
noise and reduces visibility, making CNN-based feature extractors struggle with
degraded regions like reflective and textureless areas. Second, these degraded
regions can disrupt accurate pixel correspondences, leading to ineffective
supervision based on the photometric consistency assumption. To address these
challenges, we propose injecting robust priors derived from the visual
foundation model into the CNN-based feature extractor to improve feature
representation under adverse weather conditions. We then introduce scene
correspondence priors to construct robust supervisory signals rather than
relying solely on the photometric consistency assumption. Specifically, we
create synthetic stereo datasets with realistic weather degradations. These
datasets feature clear and adverse image pairs that maintain the same semantic
context and disparity, preserving the scene correspondence property. With this
knowledge, we propose a robust self-supervised training paradigm, consisting of
two key steps: robust self-supervised scene correspondence learning and adverse
weather distillation. Both steps aim to align underlying scene results from
clean and adverse image pairs, thus improving model disparity estimation under
adverse weather effects. Extensive experiments demonstrate the effectiveness
and versatility of our proposed solution, which outperforms existing
state-of-the-art self-supervised methods. Codes are available at
\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.

</details>


### [106] [YOLO-LAN: Precise Polyp Detection via Optimized Loss, Augmentations and Negatives](https://arxiv.org/abs/2509.19166)
*Siddharth Gupta,Jitin Singla*

Main category: cs.CV

TL;DR: A YOLO-based polyp detector, YOLO-LAN, uses M2IoU loss, data augmentations, and negative data to improve real-time colonoscopy polyp detection, achieving state-of-the-art mAP on Kvasir-seg and BKAI-IGH NeoPolyp datasets.


<details>
  <summary>Details</summary>
Motivation: Manual colonoscopy detection is inconsistent and prone to oversights. Deep learning object detection promises higher accuracy and real-time guidance during screening.

Method: Propose YOLO-LAN, a YOLO-based polyp detector trained with M2IoU loss, extensive data augmentations, and negative samples to mimic clinical conditions; evaluated on Kvasir-seg and BKAI-IGH NeoPolyp datasets using YOLOv1/YOLOv8 variants.

Result: Outperforms existing methods on both datasets, achieving mAP50 of 0.9619 and 0.9540 and mAP50:95 of 0.8599 and 0.8487, respectively, with high robustness to polyp size and precise localization.

Conclusion: YOLO-LAN delivers accurate, real-time polyp detection with robust localization, demonstrating clinical relevance for AI-assisted colorectal screening.

Abstract: Colorectal cancer (CRC), a lethal disease, begins with the growth of abnormal
mucosal cell proliferation called polyps in the inner wall of the colon. When
left undetected, polyps can become malignant tumors. Colonoscopy is the
standard procedure for detecting polyps, as it enables direct visualization and
removal of suspicious lesions. Manual detection by colonoscopy can be
inconsistent and is subject to oversight. Therefore, object detection based on
deep learning offers a better solution for a more accurate and real-time
diagnosis during colonoscopy. In this work, we propose YOLO-LAN, a YOLO-based
polyp detection pipeline, trained using M2IoU loss, versatile data
augmentations and negative data to replicate real clinical situations. Our
pipeline outperformed existing methods for the Kvasir-seg and BKAI-IGH NeoPolyp
datasets, achieving mAP$_{50}$ of 0.9619, mAP$_{50:95}$ of 0.8599 with YOLOv12
and mAP$_{50}$ of 0.9540, mAP$_{50:95}$ of 0.8487 with YOLOv8 on the Kvasir-seg
dataset. The significant increase is achieved in mAP$_{50:95}$ score, showing
the precision of polyp detection. We show robustness based on polyp size and
precise location detection, making it clinically relevant in AI-assisted
colorectal screening.

</details>


### [107] [The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware Video Segmentation via SeC](https://arxiv.org/abs/2509.19183)
*Mingqi Gao,Jingkun Chen,Yunqi Miao,Gengshen Wu,Zhijin Qin,Jungong Han*

Main category: cs.CV

TL;DR: Enhanced MOSEv2 semi-supervised VOS using long-term and concept-aware memory improves temporal continuity and distractor suppression; achieves top score (JF 39.89%) on MOSEv2 track.


<details>
  <summary>Details</summary>
Motivation: Tackle MOSEv2 challenges in semi-supervised VOS, notably occlusion, reappearance, and distractors; leverage memory to maintain temporal consistency and semantic priors.

Method: Adapted SeC (an enhanced SAM-2 framework) with long-term memory and concept-aware memory modules; analyze their effects on performance.

Result: JF score 39.89% on MOSEv2 test set; ranked 1st in LSVOS MOSEv2 track.

Conclusion: Memory-based strategies effectively address MOSEv2 core challenges and yield state-of-the-art performance.

Abstract: This technical report explores the MOSEv2 track of the LSVOS Challenge, which
targets complex semi-supervised video object segmentation. By analysing and
adapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of its
long-term memory and concept-aware memory, showing that long-term memory
preserves temporal continuity under occlusion and reappearance, while
concept-aware memory supplies semantic priors that suppress distractors;
together, these traits directly benefit several MOSEv2's core challenges. Our
solution achieves a JF score of 39.89% on the test set, ranking 1st in the
MOSEv2 track of the LSVOS Challenge.

</details>


### [108] [Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models](https://arxiv.org/abs/2509.19191)
*Yueyan Li,Chenggong Zhao,Zeyuan Zang,Caixia Yuan,Xiaojie Wang*

Main category: cs.CV

TL;DR: A dual-stream analysis of Vision-Language Models (what/where) reveals a shallow-to-deep two-stage object recognition process and a geometric structure for spatial positioning; introduces token-compression via a plug-and-play visual decoder and RoPE scaling to boost decoding efficiency and spatial reasoning; experimentally validates findings and formulates design principles for future architectures.


<details>
  <summary>Details</summary>
Motivation: Motivated by the mismatch between human parallel vision and serial VLM processing, and by the opacity of VLM internals. The goal is to dissect object recognition and spatial perception, derive interpretable mechanisms, and propose practical techniques to improve efficiency and reasoning in VLMs.

Method: Decompose VLM processing into two streams: (1) object recognition by converting images into text token maps and studying a shallow-to-deep two-stage progression (attribute recognition → semantic disambiguation); (2) spatial perception by deriving and empirically verifying the geometric structure of positional representations. Propose an instruction-agnostic token-compression algorithm via a plug-and-play visual decoder and apply RoPE scaling to enhance spatial reasoning. Validate hypotheses through rigorous experiments to extract design principles.

Result: Reveals a two-stage object-recognition process in VLMs (from attribute recognition to semantic disambiguation) and uncovers the geometric structure of positional representations for spatial perception. The proposed token-compression method and RoPE scaling improve decoding efficiency and spatial reasoning. Experimental results support the internal explanations and provide actionable design principles for future VLM architectures.

Conclusion: The work provides a deeper, interpretable understanding of VLM internals and offers concrete principles for building more capable future architectures, including efficiency-oriented decoding and improved spatial reasoning guidance.

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable performance across
a variety of real-world tasks. However, existing VLMs typically process visual
information by serializing images, a method that diverges significantly from
the parallel nature of human vision. Moreover, their opaque internal mechanisms
hinder both deeper understanding and architectural innovation. Inspired by the
dual-stream hypothesis of human vision, which distinguishes the "what" and
"where" pathways, we deconstruct the visual processing in VLMs into object
recognition and spatial perception for separate study. For object recognition,
we convert images into text token maps and find that the model's perception of
image content unfolds as a two-stage process from shallow to deep layers,
beginning with attribute recognition and culminating in semantic
disambiguation. For spatial perception, we theoretically derive and empirically
verify the geometric structure underlying the positional representation in
VLMs. Based on these findings, we introduce an instruction-agnostic token
compression algorithm based on a plug-and-play visual decoder to improve
decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.
Through rigorous experiments, our work validates these analyses, offering a
deeper understanding of VLM internals and providing clear principles for
designing more capable future architectures.

</details>


### [109] [Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions](https://arxiv.org/abs/2509.19203)
*Ioanna Ntinou,Alexandros Xenos,Yassine Ouali,Adrian Bulat,Georgios Tzimiropoulos*

Main category: cs.CV

TL;DR: Vision-free, text-only retrieval using VLLM-generated image descriptions can surpass traditional multimodal models, achieving state-of-the-art zero-shot performance with small models, while improving privacy and compositionality.


<details>
  <summary>Details</summary>
Motivation: Address limitations of contrastively-trained VLMs (e.g., CLIP): shallow language understanding, modality gap from dual-encoder designs, and privacy/expense concerns due to large web-scale training data. Seek a privacy-friendly, efficient alternative that reduces the reliance on raw images.

Method: Propose a vision-free, single-encoder retrieval pipeline that uses a text-to-text paradigm. Employ VLLM-generated structured image descriptions to replace raw images, enabling retrieval based on textual descriptions. Calibrate with a few hours on two GPUs. Release two compositionality benchmarks (subFlickr and subCOCO) derived from Flickr30k and COCO for generalization assessment.

Result: The vision-free retriever matches or surpasses traditional multimodal models and achieves state-of-the-art zero-shot performance on multiple retrieval and compositionality benchmarks, with models as small as 0.3B parameters.

Conclusion: A vision-free, text-centric retrieval approach is viable, privacy-friendly, and highly efficient, mitigating the modality gap and enhancing compositionality. The work also contributes new benchmarks (subFlickr, subCOCO) to better assess generalization and provides code for replication.

Abstract: Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have
become the standard approach for learning discriminative vision-language
representations. However, these models often exhibit shallow language
understanding, manifesting bag-of-words behaviour. These limitations are
reinforced by their dual-encoder design, which induces a modality gap.
Additionally, the reliance on vast web-collected data corpora for training
makes the process computationally expensive and introduces significant privacy
concerns. To address these limitations, in this work, we challenge the
necessity of vision encoders for retrieval tasks by introducing a vision-free,
single-encoder retrieval pipeline. Departing from the traditional text-to-image
retrieval paradigm, we migrate to a text-to-text paradigm with the assistance
of VLLM-generated structured image descriptions. We demonstrate that this
paradigm shift has significant advantages, including a substantial reduction of
the modality gap, improved compositionality, and better performance on short
and long caption queries, all attainable with only a few hours of calibration
on two GPUs. Additionally, substituting raw images with textual descriptions
introduces a more privacy-friendly alternative for retrieval. To further assess
generalisation and address some of the shortcomings of prior compositionality
benchmarks, we release two benchmarks derived from Flickr30k and COCO,
containing diverse compositional queries made of short captions, which we coin
subFlickr and subCOCO. Our vision-free retriever matches and often surpasses
traditional multimodal models. Importantly, our approach achieves
state-of-the-art zero-shot performance on multiple retrieval and
compositionality benchmarks, with models as small as 0.3B parameters. Code is
available at: https://github.com/IoannaNti/LexiCLIP

</details>


### [110] [Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs](https://arxiv.org/abs/2509.19207)
*Israfel Salazar,Desmond Elliott,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: There is a bidirectional link between compositionality and long-caption understanding in vision-language models: training for one improves the other, but gains depend on data quality and model design; high-quality long-caption data enables strong performance on both tasks.


<details>
  <summary>Details</summary>
Motivation: Understanding long, dense captions requires compositional reasoning about object attributes and relations. It is unclear whether improving compositionality helps long-caption understanding and vice versa, and how data quality and model design influence this interplay.

Method: Train and evaluate a spectrum of models targeted at either compositionality or long-caption understanding, and assess bidirectional transfer between these capabilities. Examine effects of data quality, amount of parameter updates, and design choices (e.g., freezing positional embeddings) on generalization, using dense, grounded descriptions to evaluate both tasks.

Result: A bidirectional relationship exists: compositional training enhances long-caption retrieval, and long-caption training promotes compositionality. However, gains are sensitive to data quality and model design. Poorly structured captions or limited updates hinder generalization; freezing positional embeddings does not improve compositional understanding.

Conclusion: Compositional understanding and long-caption understanding are intertwined and can be jointly learned from dense, grounded descriptions. High-quality, long-caption data enables strong performance in both tasks and provides practical guidance for improving VLM generalization.

Abstract: Contrastive vision-language models (VLMs) have made significant progress in
binding visual and textual information, but understanding long, dense captions
remains an open challenge. We hypothesize that compositionality, the capacity
to reason about object-attribute bindings and inter-object relationships, is
key to understanding longer captions. In this paper, we investigate the
interaction between compositionality and long-caption understanding, asking
whether training for one property enhances the other. We train and evaluate a
range of models that target each of these capabilities. Our results reveal a
bidirectional relationship: compositional training improves performance on
long-caption retrieval, and training on long captions promotes
compositionality. However, these gains are sensitive to data quality and model
design. We find that training on poorly structured captions, or with limited
parameter updates, fails to support generalization. Likewise, strategies that
aim at retaining general alignment, such as freezing positional embeddings, do
not improve compositional understanding. Overall, we find that compositional
understanding and long-caption understanding are intertwined capabilities that
can be jointly learned through training on dense, grounded descriptions.
Despite these challenges, we show that models trained on high-quality,
long-caption data can achieve strong performance in both tasks, offering
practical guidance for improving VLM generalization.

</details>


### [111] [Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data](https://arxiv.org/abs/2509.19208)
*Earl Ranario,Ismael Mayanja,Heesup Yun,Brian N. Bailey,J. Mason Earles*

Main category: cs.CV

TL;DR: Synthetic RGB data with a small set of real annotations and CycleGAN-turbo cross-modality alignment boost thermal plant/weed segmentation, yielding up to 22% relative improvement for weeds and 17% for plants over a full real-data baseline.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation in thermal imagery is hard due to low plant-weeds contrast, occlusions, and limited labeled data in outdoor field conditions.

Method: Train on 1,128 synthetic RGB images with crop/weed mixtures to generate segmentation masks. Evaluate adding as few as five real, manually segmented field images with various sampling strategies. Use CycleGAN-turbo to translate RGB to thermal for cross-modal alignment and robust template matching, enabling training with synthetic data plus limited real annotations.

Result: Combining all synthetic data with a small number of real annotations and cross-domain translation yielded up to 22% relative improvement for the weed class and 17% for the plant class over the full real-data baseline.

Conclusion: Synthetic data paired with limited real annotations and cross-domain generative modeling can substantially improve semantic segmentation performance in challenging, multi-domain field environments.

Abstract: Accurate plant segmentation in thermal imagery remains a significant
challenge for high throughput field phenotyping, particularly in outdoor
environments where low contrast between plants and weeds and frequent
occlusions hinder performance. To address this, we present a framework that
leverages synthetic RGB imagery, a limited set of real annotations, and
GAN-based cross-modality alignment to enhance semantic segmentation in thermal
images. We trained models on 1,128 synthetic images containing complex mixtures
of crop and weed plants in order to generate image segmentation masks for crop
and weed plants. We additionally evaluated the benefit of integrating as few as
five real, manually segmented field images within the training process using
various sampling strategies. When combining all the synthetic images with a few
labeled real images, we observed a maximum relative improvement of 22% for the
weed class and 17% for the plant class compared to the full real-data baseline.
Cross-modal alignment was enabled by translating RGB to thermal using
CycleGAN-turbo, allowing robust template matching without calibration. Results
demonstrated that combining synthetic data with limited manual annotations and
cross-domain translation via generative models can significantly boost
segmentation performance in complex field environments for multi-model imagery.

</details>


### [112] [HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus](https://arxiv.org/abs/2509.19218)
*Yunzhi Xu,Yushuang Ding,Hu Sun,Hongxi Zhang,Li Zhao*

Main category: cs.CV

TL;DR: HyKid is a public open-source dataset for pediatric hydrocephalus with 3D MRI segmentations including the choroid plexus; shows choroid plexus volume correlates with total CSF and achieves AUC 0.87 in a predictive model; serves as a benchmark.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of expert-annotated hydrocephalus datasets, especially with choroid plexus segmentation, and to provide a benchmark and potential biomarker discovery resource.

Method: Study includes 48 pediatric hydrocephalus patients with 3D MRIs (1 mm isotropic) reconstructed from low-resolution scans via a slice-to-volume algorithm. Manual corrections of segmentation for white matter, gray matter, lateral ventricle, external CSF, and choroid plexus by an experienced neurologist. Structured data extracted from radiology reports using a Retrieval-Augmented Generation framework. Assesses correlation between choroid plexus and CSF, and builds predictive model.

Result: Observed a strong correlation between choroid plexus volume and total CSF volume; predictive model achieves AUC = 0.87. HyKid provides a high-quality benchmark and highlights choroid plexus-related features in hydrocephalus assessment.

Conclusion: HyKid is a valuable public dataset for neuroimaging algorithm development and hydrocephalus evaluation, enabling exploration of choroid plexus as a biomarker; dataset is publicly available at Synapse.

Abstract: Evaluation of hydrocephalus in children is challenging, and the related
research is limited by a lack of publicly available, expert-annotated datasets,
particularly those with segmentation of the choroid plexus. To address this, we
present HyKid, an open-source dataset from 48 pediatric patients with
hydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was
reconstructed from routine low-resolution images using a slice-to-volume
algorithm. Manually corrected segmentations of brain tissues, including white
matter, grey matter, lateral ventricle, external CSF, and the choroid plexus,
were provided by an experienced neurologist. Additionally, structured data was
extracted from clinical radiology reports using a Retrieval-Augmented
Generation framework. The strong correlation between choroid plexus volume and
total CSF volume provided a potential biomarker for hydrocephalus evaluation,
achieving excellent performance in a predictive model (AUC = 0.87). The
proposed HyKid dataset provided a high-quality benchmark for neuroimaging
algorithms development, and it revealed the choroid plexus-related features in
hydrocephalus assessments. Our datasets are publicly available at
https://www.synapse.org/Synapse:syn68544889.

</details>


### [113] [MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation](https://arxiv.org/abs/2509.19227)
*Tongshuai Wu,Chao Lu,Ze Song,Yunlong Lin,Sizhe Fan,Xuemei Chen*

Main category: cs.CV

TL;DR: MsFIN proposes a three-layer network for dashcam accident anticipation that aggregates features at multiple temporal scales, uses Transformer-based interactions, and fuses multi-scale features; it outperforms single-scale baselines on DAD/DADA in correctness and earliness.


<details>
  <summary>Details</summary>
Motivation: Two main challenges persist in dashcam-based accident anticipation: modeling feature-level interactions among traffic participants (often occluded) and capturing asynchronous, multi-temporal behavioral cues preceding accidents; existing single-scale approaches struggle to capture these complexities.

Method: A three-layer architecture: (1) Multi-scale Feature Aggregation using a Multi-scale Module to extract short-term, mid-term, and long-term scene representations; (2) Temporal Feature Processing with Transformer-based interactions under causal constraints to model the sequential evolution of scene and object features; (3) Multi-scale Feature Post Fusion to fuse scene and object features across temporal scales into a comprehensive risk representation. Datasets DAD and DADA used; ablation studies validate contributions.

Result: MsFIN significantly outperforms state-of-the-art models with single-scale feature extraction in both prediction correctness and earliness. Ablation studies validate the effectiveness of each module.

Conclusion: The approach shows that multi-scale feature fusion and contextual interaction modeling improve early accident anticipation in dashcam videos; Transformer-based cross-scale interactions and causal temporal processing address occlusions and asynchronous cues, highlighting potential for safer proactive interventions.

Abstract: With the widespread deployment of dashcams and advancements in computer
vision, developing accident prediction models from the dashcam perspective has
become critical for proactive safety interventions. However, two key challenges
persist: modeling feature-level interactions among traffic participants (often
occluded in dashcam views) and capturing complex, asynchronous multi-temporal
behavioral cues preceding accidents. To deal with these two challenges, a
Multi-scale Feature Interaction Network (MsFIN) is proposed for early-stage
accident anticipation from dashcam videos. MsFIN has three layers for
multi-scale feature aggregation, temporal feature processing and multi-scale
feature post fusion, respectively. For multi-scale feature aggregation, a
Multi-scale Module is designed to extract scene representations at short-term,
mid-term and long-term temporal scales. Meanwhile, the Transformer architecture
is leveraged to facilitate comprehensive feature interactions. Temporal feature
processing captures the sequential evolution of scene and object features under
causal constraints. In the multi-scale feature post fusion stage, the network
fuses scene and object features across multiple temporal scales to generate a
comprehensive risk representation. Experiments on DAD and DADA datasets show
that MsFIN significantly outperforms state-of-the-art models with single-scale
feature extraction in both prediction correctness and earliness. Ablation
studies validate the effectiveness of each module in MsFIN, highlighting how
the network achieves superior performance through multi-scale feature fusion
and contextual interaction modeling.

</details>


### [114] [DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces](https://arxiv.org/abs/2509.19230)
*Tianshuo Zhang,Li Gao,Siran Peng,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: A continual learning framework for face forgery detection using a Developmental MoE with Real-LoRA and multiple Fake-LoRAs, enforcing orthogonal gradient directions to prevent forgetting while quickly adapting to new forgery types.


<details>
  <summary>Details</summary>
Motivation: Forgery techniques evolve rapidly; real faces are abundant and stable, while forgery types continually change. To defend against new forgeries, detectors must adapt with limited data/computation and avoid catastrophic forgetting of earlier types.

Method: A Developmental Mixture of Experts (MoE) where individual experts are LoRA adapters. Real-LoRA models real faces; multiple Fake-LoRAs capture incremental forgery types. Orthogonal subspace constraints ensure Fake-LoRAs learn non-conflicting directions; orthogonal gradients are integrated into their loss to prevent gradient interference during training across tasks.

Result: Experiments under datasets and manipulation-types incremental protocols demonstrate the method's effectiveness, showing improved adaptation to new forgery types while retaining performance on earlier types.

Conclusion: The proposed framework enables fast, scalable adaptation to evolving face forgery threats with minimal forgetting, by structuring knowledge as a developmental MoE with orthogonal learning constraints and specialized LoRA experts.

Abstract: The rise of realistic digital face generation and manipulation poses
significant social risks. The primary challenge lies in the rapid and diverse
evolution of generation techniques, which often outstrip the detection
capabilities of existing models. To defend against the ever-evolving new types
of forgery, we need to enable our model to quickly adapt to new domains with
limited computation and data while avoiding forgetting previously learned
forgery types. In this work, we posit that genuine facial samples are abundant
and relatively stable in acquisition methods, while forgery faces continuously
evolve with the iteration of manipulation techniques. Given the practical
infeasibility of exhaustively collecting all forgery variants, we frame face
forgery detection as a continual learning problem and allow the model to
develop as new forgery types emerge. Specifically, we employ a Developmental
Mixture of Experts (MoE) architecture that uses LoRA models as its individual
experts. These experts are organized into two groups: a Real-LoRA to learn and
refine knowledge of real faces, and multiple Fake-LoRAs to capture incremental
information from different forgery types. To prevent catastrophic forgetting,
we ensure that the learning direction of Fake-LoRAs is orthogonal to the
established subspace. Moreover, we integrate orthogonal gradients into the
orthogonal loss of Fake-LoRAs, preventing gradient interference throughout the
training process of each task. Experimental results under both the datasets and
manipulation types incremental protocols demonstrate the effectiveness of our
method.

</details>


### [115] [Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2509.19244)
*Shufan Li,Jiuxiang Gu,Kangning Liu,Zhe Lin,Zijun Wei,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: Lavida-O is a unified multi-modal Masked Diffusion Model that jointly understands and generates images, enabling high-res synthesis, object grounding, and editing with planning/self-reflection, and achieves state-of-the-art results while offering faster inference.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of prior multimodal diffusion language models that handle only simple image-level understanding tasks or low-resolution generation, and to create a single model that leverages image understanding to improve generation/editing, including high-resolution outputs.

Method: Proposes an Elastic Mixture-of-Transformer architecture, universal text conditioning, and stratified sampling, forming a unified multi-modal diffusion model that uses its understanding for planning and iterative self-reflection to enhance image generation and editing (up to 1024px).

Result: Achieves state-of-the-art on RefCOCO object grounding, GenEval text-to-image generation, and ImgEdit image editing, outperforming autoregressive and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, with faster inference.

Conclusion: Lavida-O demonstrates the feasibility of a unified multi-modal diffusion model that couples image understanding and generation through planning and self-reflection, delivering high-resolution capabilities and new tasks like object grounding and image editing while maintaining strong efficiency.

Abstract: We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)
capable of image understanding and generation tasks. Unlike existing multimodal
diffsion language models such as MMaDa and Muddit which only support simple
image-level understanding tasks and low-resolution image generation, Lavida-O
exhibits many new capabilities such as object grounding, image-editing, and
high-resolution (1024px) image synthesis. It is also the first unified MDM that
uses its understanding capabilities to improve image generation and editing
results through planning and iterative self-reflection. To allow effective and
efficient training and sampling, Lavida-O ntroduces many novel techniques such
as Elastic Mixture-of-Transformer architecture, universal text conditioning,
and stratified sampling. \ours~achieves state-of-the-art performance on a wide
range of benchmarks such as RefCOCO object grounding, GenEval text-to-image
generation, and ImgEdit image editing, outperforming existing autoregressive
and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while
offering considerable speedup at inference.

</details>


### [116] [ConViS-Bench: Estimating Video Similarity Through Semantic Concepts](https://arxiv.org/abs/2509.19245)
*Benedetta Liberatori,Alessandro Conti,Lorenzo Vaquero,Yiming Wang,Elisa Ricci,Paolo Rota*

Main category: cs.CV

TL;DR: ConViS introduces a concept-based video similarity task and a benchmark to enable interpretable, concept-level comparisons and language-guided evaluation of video similarity; it reveals concept-level differences in model alignment and enables new applications like concept-conditioned retrieval.


<details>
  <summary>Details</summary>
Motivation: Humans compare videos along multiple aspects (e.g., actions, settings, filming location). Current global similarity measures lack interpretability and fail to capture this multi-faceted reasoning; a concept-based framework and benchmark are needed to study and improve language-driven video understanding.

Method: Define the ConViS task: estimating similarity between video pairs across predefined semantic concepts. Build ConViS-Bench with carefully annotated video pairs, concept-level similarity scores, and textual descriptions of differences/similarities. Benchmark several state-of-the-art models to assess alignment with human judgments and analyze concept-wise difficulties.

Result: Results show significant variation in model performance across concepts; some concepts are more challenging for similarity estimation. ConViS-Bench exposes gaps between human judgments and model outputs and suggests that language-driven, concept-aware evaluation can guide better video understanding systems and enable applications like concept-conditioned retrieval.

Conclusion: ConViS-Bench is a valuable resource for advancing research in language-driven video understanding and for developing interpretable, concept-based video similarity methods.

Abstract: What does it mean for two videos to be similar? Videos may appear similar
when judged by the actions they depict, yet entirely different if evaluated
based on the locations where they were filmed. While humans naturally compare
videos by taking different aspects into account, this ability has not been
thoroughly studied and presents a challenge for models that often depend on
broad global similarity scores. Large Multimodal Models (LMMs) with video
understanding capabilities open new opportunities for leveraging natural
language in comparative video tasks. We introduce Concept-based Video
Similarity estimation (ConViS), a novel task that compares pairs of videos by
computing interpretable similarity scores across a predefined set of key
semantic concepts. ConViS allows for human-like reasoning about video
similarity and enables new applications such as concept-conditioned video
retrieval. To support this task, we also introduce ConViS-Bench, a new
benchmark comprising carefully annotated video pairs spanning multiple domains.
Each pair comes with concept-level similarity scores and textual descriptions
of both differences and similarities. Additionally, we benchmark several
state-of-the-art models on ConViS, providing insights into their alignment with
human judgments. Our results reveal significant performance differences on
ConViS, indicating that some concepts present greater challenges for estimating
video similarity. We believe that ConViS-Bench will serve as a valuable
resource for advancing research in language-driven video understanding.

</details>


### [117] [Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps](https://arxiv.org/abs/2509.19252)
*Gabriel Maldonado,Narges Rashvand,Armin Danesh Pazho,Ghazal Alinezhad Noghre,Vinit Katariya,Hamed Tabkhi*

Main category: cs.CV

TL;DR: Adversarially-refined VQ-GAN with dense motion tokenization compresses spatio-temporal heatmaps for human motion, outperforming dVAE on CMU Panoptic; reveals 2D motion can be represented with 128 tokens, while 3D requires 1024 tokens; code available.


<details>
  <summary>Details</summary>
Motivation: Continuous human motion understanding is high-dimensional and redundant; efficient compression and representation are crucial for scalable motion analysis.

Method: Proposes an adversarially-refined VQ-GAN with dense motion tokenization to compress spatio-temporal heatmaps, mitigating artifacts such as motion smear and temporal misalignment.

Result: Outperforms dVAE by 9.31% SSIM and reduces temporal instability by 37.1% on CMU Panoptic; 2D motion uses a compact 128-token vocabulary, 3D motion requires a larger 1024-token codebook; codebase available at GitHub.

Conclusion: Dense tokenization with adversarial refinement enables faithful reconstruction of motion and supports practical deployment and analysis of motion complexity across 2D/3D domains.

Abstract: Continuous human motion understanding remains a core challenge in computer
vision due to its high dimensionality and inherent redundancy. Efficient
compression and representation are crucial for analyzing complex motion
dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework
with dense motion tokenization for compressing spatio-temporal heatmaps while
preserving the fine-grained traces of human motion. Our approach combines dense
motion tokenization with adversarial refinement, which eliminates
reconstruction artifacts like motion smearing and temporal misalignment
observed in non-adversarial baselines. Our experiments on the CMU Panoptic
dataset provide conclusive evidence of our method's superiority, outperforming
the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.
Furthermore, our dense tokenization strategy enables a novel analysis of motion
complexity, revealing that 2D motion can be optimally represented with a
compact 128-token vocabulary, while 3D motion's complexity demands a much
larger 1024-token codebook for faithful reconstruction. These results establish
practical deployment feasibility across diverse motion analysis applications.
The code base for this work is available at
https://github.com/TeCSAR-UNCC/Pose-Quantization.

</details>


### [118] [Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies](https://arxiv.org/abs/2509.19258)
*Dheerendranath Battalapalli,Apoorva Safai,Maria Jaramillo,Hyemin Um,Gustavo Adalfo Pineda Ortiz,Ulas Bagci,Manmeet Singh Ahluwalia,Marwa Ismail,Pallavi Tiwari*

Main category: cs.CV

TL;DR: Graph-Radiomic Learning (GrRAiL) builds per-voxel radiomic profiles to form sub-region clusters within a lesion, then constructs weighted graphs to capture higher-order spatial relationships among clusters, aiming to distinguish malignant lesions from confounding pathologies on MRI. It was evaluated in 947 subjects across glioblastoma recurrence vs radiation effects, brain metastasis recurrence vs necrosis, and IPMN risk stratification, and consistently outperformed baselines (GNNs, texture/radiomics, and simple intensity graphs) with notable accuracy gains in cross-validation and on test sets.


<details>
  <summary>Details</summary>
Motivation: Rule-based or aggregate radiomic features treat lesions as a single ROI, missing complex spatial interactions among heterogenous subregions. Distinguishing malignant neoplasms from treatment- or pathology-related confounders requires capturing intra-lesional spatial organization of heterogeneity.

Method: Identify clusters of sub-regions using per-voxel radiomic measurements within ROIs, then compute graph-theoretic metrics to quantify spatial associations among these clusters, resulting in weighted graphs that encode higher-order intralesional heterogeneity (ILH). Evaluate GrRAiL in 947 subjects across three use cases (GBM recurrence vs pseudo-progression, brain metastasis recurrence vs radiotherapy necrosis, IPMN risk stratification) in a multi-institutional setting, comparing against Graph Neural Networks, texture/radiomic features, and intensity-graph analyses.

Result: GrRAiL consistently outperformed baselines across use cases. GBM: CV 89% and test 78% (>10% + over baselines). Brain mets: CV 84%, test 74% (>13% improvement). IPMN risk: CV 84%, test 75% (>10% improvement).

Conclusion: A graph-structured radiomic descriptor can capture higher-order spatial relationships within lesions, improving discrimination between malignant neoplasms and confounding pathologies on MRI across diverse clinical scenarios and datasets, with demonstrated cross-validation and external-test gains.

Abstract: A significant challenge in solid tumors is reliably distinguishing
confounding pathologies from malignant neoplasms on routine imaging. While
radiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,
many aggregate features across the region of interest (ROI) and miss complex
spatial relationships among varying intensity compositions. We present a new
Graph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional
heterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of
sub-regions using per-voxel radiomic measurements, then (2) computes
graph-theoretic metrics to quantify spatial associations among clusters. The
resulting weighted graphs encode higher-order spatial relationships within the
ROI, aiming to reliably capture ILH and disambiguate confounding pathologies
from malignancy. To assess efficacy and clinical feasibility, GrRAiL was
evaluated in n=947 subjects spanning three use cases: differentiating tumor
recurrence from radiation effects in glioblastoma (GBM; n=106) and brain
metastasis (n=233), and stratifying pancreatic intraductal papillary mucinous
neoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional
setting, GrRAiL consistently outperformed state-of-the-art baselines - Graph
Neural Networks (GNNs), textural radiomics, and intensity-graph analysis. In
GBM, cross-validation (CV) and test accuracies for recurrence vs
pseudo-progression were 89% and 78% with >10% test-accuracy gains over
comparators. In brain metastasis, CV and test accuracies for recurrence vs
radiation necrosis were 84% and 74% (>13% improvement). For IPMN risk
stratification, CV and test accuracies were 84% and 75%, showing >10%
improvement.

</details>


### [119] [Moving by Looking: Towards Vision-Driven Avatar Motion Generation](https://arxiv.org/abs/2509.19259)
*Markos Diomataris,Berat Mert Albaba,Giorgio Becherini,Partha Ghosh,Omid Taheri,Michael J. Black*

Main category: cs.CV

TL;DR: CLOPS presents a human avatar that relies solely on egocentric vision for perception and navigation, by decoupling motion learning from perception: a motion prior is learned from large motion capture data, and a Q-learning policy maps egocentric visuals to high-level commands for that prior. The result is human-like motion with obstacle avoidance, suggesting egocentric vision can enable realistic avatar behavior.


<details>
  <summary>Details</summary>
Motivation: Current human motion generation methods use task-specific perception that differs from human sensing, limiting realism. The authors argue for human-like perception (egocentric vision) to drive avatar motion and address dataset scale by decoupling motion learning from perception.

Method: 1) Train a motion prior on a large motion capture dataset to learn low-level motion skills. 2) Train a Q-learning policy to map egocentric visual inputs to high-level control commands for the motion prior, thereby connecting perception to motion.

Result: Empirical evidence that egocentric vision yields human-like motion characteristics, such as obstacle avoidance in the visual field, demonstrating the viability of perception-driven, human-like avatar behavior.

Conclusion: Equipping avatars with human-like sensors, especially egocentric vision, is promising for training agents that behave like humans; the proposed decoupled learning framework offers a scalable path toward lifelike avatar motion.

Abstract: The way we perceive the world fundamentally shapes how we move, whether it is
how we navigate in a room or how we interact with other humans. Current human
motion generation methods, neglect this interdependency and use task-specific
``perception'' that differs radically from that of humans. We argue that the
generation of human-like avatar behavior requires human-like perception.
Consequently, in this work we present CLOPS, the first human avatar that solely
uses egocentric vision to perceive its surroundings and navigate. Using vision
as the primary driver of motion however, gives rise to a significant challenge
for training avatars: existing datasets have either isolated human motion,
without the context of a scene, or lack scale. We overcome this challenge by
decoupling the learning of low-level motion skills from learning of high-level
control that maps visual input to motion. First, we train a motion prior model
on a large motion capture dataset. Then, a policy is trained using Q-learning
to map egocentric visual inputs to high-level control commands for the motion
prior. Our experiments empirically demonstrate that egocentric vision can give
rise to human-like motion characteristics in our avatars. For example, the
avatars walk such that they avoid obstacles present in their visual field.
These findings suggest that equipping avatars with human-like sensors,
particularly egocentric vision, holds promise for training avatars that behave
like humans.

</details>


### [120] [OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps](https://arxiv.org/abs/2509.19282)
*Bingnan Li,Chen-Yu Wang,Haiyang Xu,Xiang Zhang,Ethan Armand,Divyansh Srivastava,Xiaojun Shan,Zeyuan Chen,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: Introduce OverLayScore and OverLayBench to address challenging overlaps in layout-to-image generation; propose CreatiLayout-AM fine-tuned on amodal masks; aim for more robust, realistic evaluation and generation under overlap-rich layouts.


<details>
  <summary>Details</summary>
Motivation: Current layout-to-image methods struggle with layouts having heavy bounding-box overlap, especially large overlaps and indistinguishable overlapping instances. This leads to degraded generation quality and reveals biases in existing benchmarks that favor simpler, low-overlap cases. A systematic metric and diverse benchmark are needed to evaluate and guide progress under realistic conditions.

Method: 1) Define OverLayScore to quantify overlap complexity among bounding boxes. 2) Create OverLayBench with high-quality annotations and a balanced distribution across OverLayScore levels. 3) Introduce CreatiLayout-AM, a model fine-tuned on curated amodal mask data to improve overlap-aware generation.

Result: Qualitative and quantitative analyses show overlap factors degrade layout-to-image generation. OverLayBench provides a more challenging evaluation platform with a balanced OverLayScore distribution. CreatiLayout-AM represents an initial step toward better performance on complex overlaps, demonstrating the potential of amodal-mask-informed fine-tuning. Project link provided for dataset and benchmarks.

Conclusion: This work lays groundwork for more robust layout-to-image generation in realistic, overlap-rich scenarios by providing a dedicated overlap-complexity metric, a balanced benchmark, and an amodal-mask–tuned model, encouraging further research and development.

Abstract: Despite steady progress in layout-to-image generation, current methods still
struggle with layouts containing significant overlap between bounding boxes. We
identify two primary challenges: (1) large overlapping regions and (2)
overlapping instances with minimal semantic distinction. Through both
qualitative examples and quantitative analysis, we demonstrate how these
factors degrade generation quality. To systematically assess this issue, we
introduce OverLayScore, a novel metric that quantifies the complexity of
overlapping bounding boxes. Our analysis reveals that existing benchmarks are
biased toward simpler cases with low OverLayScore values, limiting their
effectiveness in evaluating model performance under more challenging
conditions. To bridge this gap, we present OverLayBench, a new benchmark
featuring high-quality annotations and a balanced distribution across different
levels of OverLayScore. As an initial step toward improving performance on
complex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a
curated amodal mask dataset. Together, our contributions lay the groundwork for
more robust layout-to-image generation under realistic and challenging
scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.

</details>


### [121] [Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation](https://arxiv.org/abs/2509.19296)
*Sherwin Bahmani,Tianchang Shen,Jiawei Ren,Jiahui Huang,Yifeng Jiang,Haithem Turki,Andrea Tagliasacchi,David B. Lindell,Zan Gojcic,Sanja Fidler,Huan Ling,Jun Gao,Xuanchi Ren*

Main category: cs.CV

TL;DR: Distill implicit 3D knowledge from video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation to enable text- or image-driven 3D scene synthesis without multi-view data, including dynamic scenes, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current 3D reconstruction methods rely on multi-view real data, which is often unavailable. 2D diffusion models are powerful but limited to 2D; there is a need for explicit 3D representations that can be trained without multi-view data for robotics and real-time applications.

Method: Introduce a self-distillation framework that augments an RGB decoder with a 3DGS decoder, supervised by the RGB output. The 3DGS branch is trained purely on synthetic data generated by video diffusion models. At inference, the model can synthesize 3D scenes from a text prompt or a single image and extends to dynamic 3D scene generation from monocular video.

Result: The framework achieves state-of-the-art performance for static and dynamic 3D scene generation and enables real-time rendering without requiring multi-view training data.

Conclusion: Implicit 3D knowledge contained in video diffusion models can be distilled into an explicit 3D Gaussian Splatting representation, enabling text- or image-driven, real-time 3D scene synthesis, including dynamic scenes, using synthetic data only.

Abstract: The ability to generate virtual environments is crucial for applications
ranging from gaming to physical AI domains such as robotics, autonomous
driving, and industrial AI. Current learning-based 3D reconstruction methods
rely on the availability of captured real-world multi-view data, which is not
always readily available. Recent advancements in video diffusion models have
shown remarkable imagination capabilities, yet their 2D nature limits the
applications to simulation where a robot needs to navigate and interact with
the environment. In this paper, we propose a self-distillation framework that
aims to distill the implicit 3D knowledge in the video diffusion models into an
explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for
multi-view training data. Specifically, we augment the typical RGB decoder with
a 3DGS decoder, which is supervised by the output of the RGB decoder. In this
approach, the 3DGS decoder can be purely trained with synthetic data generated
by video diffusion models. At inference time, our model can synthesize 3D
scenes from either a text prompt or a single image for real-time rendering. Our
framework further extends to dynamic 3D scene generation from a monocular input
video. Experimental results show that our framework achieves state-of-the-art
performance in static and dynamic 3D scene generation.

</details>


### [122] [VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction](https://arxiv.org/abs/2509.19297)
*Weijie Wang,Yeqing Chen,Zeyu Zhang,Hengyu Liu,Haoxiao Wang,Zhiyuan Feng,Wenkang Qin,Zheng Zhu,Donny Y. Chen,Bohan Zhuang*

Main category: cs.CV

TL;DR: VolSplat replaces pixel-aligned Gaussians with voxel-aligned Gaussians for multi-view 3D Gaussian Splatting, enabling robust, view-consistent, and density-adaptive reconstructions that achieve state-of-the-art novel-view synthesis on RealEstate10K and ScanNet.


<details>
  <summary>Details</summary>
Motivation: Pixel-aligned 2D-to-3D Gaussian prediction has notable limitations: heavy reliance on the number of input views, view-biased density distributions, and alignment errors due to occlusions or low texture. A voxel-aligned, 3D-driven approach promises better multi-view consistency and robustness.

Method: Introduce VolSplat, a multi-view feed-forward paradigm that predicts Gaussians directly from a predicted 3D voxel grid, replacing 2D pixel alignment with voxel alignment. Includes adaptive control of Gaussian density based on scene complexity for denser and more faithful Gaussian point clouds, improving geometric consistency and novel-view rendering.

Result: Empirical evaluation on RealEstate10K and ScanNet shows state-of-the-art performance, with more plausible and view-consistent Gaussian reconstructions; results, code, and models are available on the project page.

Conclusion: VolSplat offers a scalable, denser, and more robust feed-forward framework for 3D reconstruction and novel-view synthesis, reducing reliance on pixel-level alignment and enabling stronger multi-view consistency, paving the way for broader adoption and further research.

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective
solution for novel view synthesis. Existing methods predominantly rely on a
pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a
3D Gaussian. We rethink this widely adopted formulation and identify several
inherent limitations: it renders the reconstructed 3D models heavily dependent
on the number of input views, leads to view-biased density distributions, and
introduces alignment errors, particularly when source views contain occlusions
or low texture. To address these challenges, we introduce VolSplat, a new
multi-view feed-forward paradigm that replaces pixel alignment with
voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D
voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature
matching, ensuring robust multi-view consistency. Furthermore, it enables
adaptive control over Gaussian density based on 3D scene complexity, yielding
more faithful Gaussian point clouds, improved geometric consistency, and
enhanced novel-view rendering quality. Experiments on widely used benchmarks
including RealEstate10K and ScanNet demonstrate that VolSplat achieves
state-of-the-art performance while producing more plausible and view-consistent
Gaussian reconstructions. In addition to superior results, our approach
establishes a more scalable framework for feed-forward 3D reconstruction with
denser and more robust representations, paving the way for further research in
wider communities. The video results, code and trained models are available on
our project page: https://lhmd.top/volsplat.

</details>


### [123] [CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching](https://arxiv.org/abs/2509.19300)
*Chen Chen,Pengsheng Guo,Liangchen Song,Jiasen Lu,Rui Qian,Xinze Wang,Tsu-Jui Fu,Wei Liu,Yinfei Yang,Alex Schwing*

Main category: cs.CV

TL;DR: CAR-Flow introduces a lightweight, learned shift to condition the source/target in flow matching, reducing the learning burden and speeding training for conditional generative models; it improves ImageNet-256 FID with negligible parameter overhead.


<details>
  <summary>Details</summary>
Motivation: Conditional generative modeling with diffusion/flow requires learning both mass transport and conditional injection. This places a heavy burden on the model, motivating a simpler conditioning mechanism.

Method: Introduce Condition-Aware Reparameterization for Flow Matching (CAR-Flow): a small learned shift that conditions the source, target, or both distributions to relocate them, thereby shortening the probability path the model must learn.

Result: CAR-Flow reduces training difficulty and accelerates learning; on ImageNet-256 with SiT-XL/2 it lowers FID from 2.07 to 1.68 while adding under 0.6% parameters; synthetic data visuals quantify the effects.

Conclusion: A lightweight, condition-aware shift in flow matching can substantially ease conditional generation tasks, achieving better or comparable fidelity with minimal parameter overhead and faster training.

Abstract: Conditional generative modeling aims to learn a conditional data distribution
from samples containing data-condition pairs. For this, diffusion and
flow-based methods have attained compelling results. These methods use a
learned (flow) model to transport an initial standard Gaussian noise that
ignores the condition to the conditional data distribution. The model is hence
required to learn both mass transport and conditional injection. To ease the
demand on the model, we propose Condition-Aware Reparameterization for Flow
Matching (CAR-Flow) -- a lightweight, learned shift that conditions the source,
the target, or both distributions. By relocating these distributions, CAR-Flow
shortens the probability path the model must learn, leading to faster training
in practice. On low-dimensional synthetic data, we visualize and quantify the
effects of CAR. On higher-dimensional natural image data (ImageNet-256),
equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while
introducing less than 0.6% additional parameters.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [124] [PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies](https://arxiv.org/abs/2509.18282)
*Jesse Zhang,Marius Memmel,Kevin Kim,Dieter Fox,Jesse Thomason,Fabio Ramos,Erdem Bıyık,Abhishek Gupta,Anqi Li*

Main category: cs.RO

TL;DR: PEEK uses vision-language models to supply a unified keypoint representation that separates high-level planning (where and what) from low-level control (how), improving zero-shot generalization in robotic manipulation across architectures.


<details>
  <summary>Details</summary>
Motivation: Robotic manipulation generalizes poorly because systems must learn where to attend, what actions to take, and how to execute them; offloading high-level reasoning to powerful VLMs can free policies to focus on execution.

Method: Fine-tune vision-language models to predict a single, unified keypoint representation: (1) end-effector paths for actions and (2) task-relevant masks for attention. Overlay these annotations on robot observations to make the representation policy-agnostic and transferable. Build an automatic annotation pipeline that generates labeled data across 20+ robot datasets spanning 9 embodiments to enable scalable training. The approach yields a policy-agnostic representation that can be plugged into various architectures.

Result: PEEK yields strong zero-shot generalization: a 41.4x real-world improvement for a 3D policy trained only in simulation, and 2-3.5x gains for both large vision-language models and small manipulation policies across different architectures.

Conclusion: By outsourcing semantic/visual reasoning to VLMs, PEEK provides manipulation policies with minimal cues—where, what, and how—enhancing generalization and transferability across robots and policies.

Abstract: Robotic manipulation policies often fail to generalize because they must
simultaneously learn where to attend, what actions to take, and how to execute
them. We argue that high-level reasoning about where and what can be offloaded
to vision-language models (VLMs), leaving policies to specialize in how to act.
We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which
fine-tunes VLMs to predict a unified point-based intermediate representation:
1. end-effector paths specifying what actions to take, and 2. task-relevant
masks indicating where to focus. These annotations are directly overlaid onto
robot observations, making the representation policy-agnostic and transferable
across architectures. To enable scalable training, we introduce an automatic
annotation pipeline, generating labeled data across 20+ robot datasets spanning
9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot
generalization, including a 41.4x real-world improvement for a 3D policy
trained only in simulation, and 2-3.5x gains for both large VLAs and small
manipulation policies. By letting VLMs absorb semantic and visual complexity,
PEEK equips manipulation policies with the minimal cues they need--where, what,
and how. Website at https://peek-robot.github.io/.

</details>


### [125] [Fine-Tuning Robot Policies While Maintaining User Privacy](https://arxiv.org/abs/2509.18311)
*Benjamin A. Christie,Sagar Parekh,Dylan P. Losey*

Main category: cs.RO

TL;DR: PRoP is a model-agnostic framework for private, per-user personalization of robot policies. It uses per-user keys to transform network weights, enabling user-specific behavior when the correct key is used and reverting to baseline with an incorrect key, across imitation learning, RL, and classification. It preserves architecture and outperforms encoder-based privacy methods; code and videos are available.


<details>
  <summary>Details</summary>
Motivation: Personalizing general-purpose robot policies risks leaking users’ private preferences during fine-tuning. There is a need for effective personalization that preserves user privacy, enabling individuals to define their own preferences without revealing them to external agents.

Method: Assign each user a unique key that mathematically transforms the weights of the robot’s network. With the correct key, the policy aligns with that user’s preferences; with an incorrect key, the policy reverts to baseline behavior. This framework is model-agnostic and demonstrated across imitation learning, reinforcement learning, and classification tasks, while preserving the original architecture.

Result: Empirically, PRoP outperforms existing encoder-based approaches and demonstrates broad applicability across multiple model types (imitation learning, RL, classification).

Conclusion: PRoP provides a practical, architecture-preserving solution for private, personalized robot policies, enabling private customization without altering the policy’s structure; videos and code are available for evaluation.

Abstract: Recent works introduce general-purpose robot policies. These policies provide
a strong prior over how robots should behave -- e.g., how a robot arm should
manipulate food items. But in order for robots to match an individual person's
needs, users typically fine-tune these generalized policies -- e.g., showing
the robot arm how to make their own preferred dinners. Importantly, during the
process of personalizing robots, end-users leak data about their preferences,
habits, and styles (e.g., the foods they prefer to eat). Other agents can
simply roll-out the fine-tuned policy and see these personally-trained
behaviors. This leads to a fundamental challenge: how can we develop robots
that personalize actions while keeping learning private from external agents?
We here explore this emerging topic in human-robot interaction and develop
PRoP, a model-agnostic framework for personalized and private robot policies.
Our core idea is to equip each user with a unique key; this key is then used to
mathematically transform the weights of the robot's network. With the correct
key, the robot's policy switches to match that user's preferences -- but with
incorrect keys, the robot reverts to its baseline behaviors. We show the
general applicability of our method across multiple model types in imitation
learning, reinforcement learning, and classification tasks. PRoP is practically
advantageous because it retains the architecture and behaviors of the original
policy, and experimentally outperforms existing encoder-based approaches. See
videos and code here: https://prop-icra26.github.io.

</details>


### [126] [Haptic Communication in Human-Human and Human-Robot Co-Manipulation](https://arxiv.org/abs/2509.18327)
*Katherine H. Allen,Chris Rogers,Elaine S. Short*

Main category: cs.RO

TL;DR: Humans coordinate more fluently with other humans than with a robot in shared-object manipulation, with IMU data showing objective motion differences; results motivate robot designs that use anthropomorphic haptic signals to improve collaboration.


<details>
  <summary>Details</summary>
Motivation: To understand how haptic cues embedded in the motion of a shared object support joint action and to extend that understanding to robot partners, enabling more natural and effective human-robot collaboration.

Method: Participants formed a leader-follower dyad to jointly manipulate an object (human-human condition). The same participants then manipulated the object with a robot collaborator. Object motion was tracked with a low-cost IMU; intra-study and post-study questionnaires assessed fluency and accuracy. Comparative analysis examined differences between human-human and human-robot conditions.

Result: Human-human collaborations were significantly more fluent. IMU data captured objective differences in motion profiles between conditions, corroborated by subjective reports of accuracy and fluency.

Conclusion: The observed gaps motivate future work on robot assistants that can send and receive anthropomorphic haptic signals to improve physical collaboration.

Abstract: When a human dyad jointly manipulates an object, they must communicate about
their intended motion plans. Some of that collaboration is achieved through the
motion of the manipulated object itself, which we call "haptic communication."
In this work, we captured the motion of human-human dyads moving an object
together with one participant leading a motion plan about which the follower is
uninformed. We then captured the same human participants manipulating the same
object with a robot collaborator. By tracking the motion of the shared object
using a low-cost IMU, we can directly compare human-human shared manipulation
to the motion of those same participants interacting with the robot.
Intra-study and post-study questionnaires provided participant feedback on the
collaborations, indicating that the human-human collaborations are
significantly more fluent, and analysis of the IMU data indicates that it
captures objective differences in the motion profiles of the conditions. The
differences in objective and subjective measures of accuracy and fluency
between the human-human and human-robot trials motivate future research into
improving robot assistants for physical tasks by enabling them to send and
receive anthropomorphic haptic signals.

</details>


### [127] [The Landform Contextual Mesh: Automatically Fusing Surface and Orbital Terrain for Mars 2020](https://arxiv.org/abs/2509.18330)
*Marsette Vona*

Main category: cs.RO

TL;DR: A system that fuses rover imagery with orbital data into interactive 3D terrain meshes for Mars, enabling scientists and the public to visualize terrain and plan missions.


<details>
  <summary>Details</summary>
Motivation: To improve terrain interpretation and mission planning by combining high-resolution rover images with orbital context, providing both tactical and strategic visualization capabilities.

Method: Automatically generate contextual meshes for each rover location during mission ground data system processing, integrating thousands of Mars 2020 images with orbital elevation and color maps, and making them accessible to scientists via the Advanced Science Targeting Tool for Robotic Operations (ASTTRO); a subset is deployed on the public 'Explore with Perseverance' site.

Result: Interactive 3D terrain visualizations that fuse 2D and 3D data; mission scientists gain access for planning, with some datasets publicly accessible.

Conclusion: Effective fusion of rover and orbital data yields rich, explorable terrain visualizations that support planning and outreach in Mars exploration.

Abstract: The Landform contextual mesh fuses 2D and 3D data from up to thousands of
Mars 2020 rover images, along with orbital elevation and color maps from Mars
Reconnaissance Orbiter, into an interactive 3D terrain visualization.
Contextual meshes are built automatically for each rover location during
mission ground data system processing, and are made available to mission
scientists for tactical and strategic planning in the Advanced Science
Targeting Tool for Robotic Operations (ASTTRO). A subset of them are also
deployed to the "Explore with Perseverance" public access website.

</details>


### [128] [Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation](https://arxiv.org/abs/2509.18342)
*Rajitha de Silva,Jonathan Cox,James R. Heselden,Marija Popovic,Cesar Cadena,Riccardo Polvara*

Main category: cs.RO

TL;DR: Semantic particle filter for vineyard localization using stable object-level landmarks (vine trunks and poles), with semantic walls as pseudo-structural constraints to reduce row aliasing; uses adaptive noisy GPS priors in headland regions; outperforms AMCL and RTAB-Map in real vineyards.


<details>
  <summary>Details</summary>
Motivation: LiDAR-based localization in vineyards suffers from repetitive row geometry and perceptual aliasing, making accurate localization difficult. The work aims to achieve robust, row-consistent localization by leveraging stable semantic landmarks and structural constraints, plus global consistency in sparse semantics.

Method: A semantic particle filter integrates object-level detections of vine trunks and support poles as landmarks. Detected landmarks are projected to a bird's-eye view and fused with LiDAR scans to yield semantic observations. Semantic walls connect adjacent landmarks to create pseudo-structural constraints that mitigate row aliasing. A noisy GPS prior is adaptively applied in headland regions to maintain global consistency when semantic information is sparse.

Result: Real-world vineyard experiments show the approach localizes within the correct row, recovers from deviations that AMCL fails to handle, and outperforms vision-based SLAM (RTAB-Map).

Conclusion: The proposed framework improves robust localization in structured outdoor environments by combining semantic landmarks with pseudo-structural constraints and adaptive GPS priors, addressing perceptual aliasing and maintaining global consistency, with demonstrated superiority over AMCL and RTAB-Map in vineyards.

Abstract: Accurate localisation is critical for mobile robots in structured outdoor
environments, yet LiDAR-based methods often fail in vineyards due to repetitive
row geometry and perceptual aliasing. We propose a semantic particle filter
that incorporates stable object-level detections, specifically vine trunks and
support poles into the likelihood estimation process. Detected landmarks are
projected into a birds eye view and fused with LiDAR scans to generate semantic
observations. A key innovation is the use of semantic walls, which connect
adjacent landmarks into pseudo-structural constraints that mitigate row
aliasing. To maintain global consistency in headland regions where semantics
are sparse, we introduce a noisy GPS prior that adaptively supports the filter.
Experiments in a real vineyard demonstrate that our approach maintains
localisation within the correct row, recovers from deviations where AMCL fails,
and outperforms vision-based SLAM methods such as RTAB-Map.

</details>


### [129] [AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback](https://arxiv.org/abs/2509.18384)
*Yunhao Yang,Junyuan Hong,Gabriel Jacob Perin,Zhiwen Fan,Li Yin,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: LAD-VF is a fine-tuning-free framework that uses formal verification feedback to iteratively refine prompts for LLM-driven control, achieving >90% success in robot navigation/manipulation while avoiding model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Align LLM planning with safety/regulatory constraints in real-world robotics, addressing hallucinations and misalignment; reduce dependency on costly human labels or fine-tuning.

Method: Integrates a formal-verification-informed text loss into an LLM-AutoDiff-based prompt refinement loop, updating prompts (not model parameters) to satisfy formal constraints. Works with modular LLM architectures and provides auditable prompts.

Result: In experiments, improves task success rate from ~60% to >90% across navigation and manipulation tasks, demonstrating scalable, verifiable LLM control.

Conclusion: LAD-VF offers a scalable, interpretable route to trustworthy LLM-driven control by leveraging formal verification to guide prompt design rather than parameter updates.

Abstract: Large language models (LLMs) can translate natural language instructions into
executable action plans for robotics, autonomous driving, and other domains.
Yet, deploying LLM-driven planning in the physical world demands strict
adherence to safety and regulatory constraints, which current models often
violate due to hallucination or weak alignment. Traditional data-driven
alignment methods, such as Direct Preference Optimization (DPO), require costly
human labeling, while recent formal-feedback approaches still depend on
resource-intensive fine-tuning. In this paper, we propose LAD-VF, a
fine-tuning-free framework that leverages formal verification feedback for
automated prompt engineering. By introducing a formal-verification-informed
text loss integrated with LLM-AutoDiff, LAD-VF iteratively refines prompts
rather than model parameters. This yields three key benefits: (i) scalable
adaptation without fine-tuning; (ii) compatibility with modular LLM
architectures; and (iii) interpretable refinement via auditable prompts.
Experiments in robot navigation and manipulation tasks demonstrate that LAD-VF
substantially enhances specification compliance, improving success rates from
60% to over 90%. Our method thus presents a scalable and interpretable pathway
toward trustworthy, formally-verified LLM-driven control systems.

</details>


### [130] [Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections](https://arxiv.org/abs/2509.18407)
*Navya Tiwari,Joseph Vazhaeparampil,Victoria Preston*

Main category: cs.RO

TL;DR: A driver-assist framework for safe right-of-way at uncontrolled intersections is formulated as a POMDP, and four planners (FSM, QMDP, POMCP, DESPOT) are evaluated in a custom simulator. Probabilistic planners outperform the rule-based baseline, achieving up to 97.5% collision-free navigation under partial observability; POMCP emphasizes safety while DESPOT trades some safety for efficiency and real-time feasibility.


<details>
  <summary>Details</summary>
Motivation: Uncontrolled intersections cause a substantial share of crashes due to ambiguous right-of-way, occlusions, and unpredictable human behavior; there is a gap in retrofit driver-assist systems that account for uncertainty in decision making.

Method: Formulate the problem as a Partially Observable Markov Decision Process (POMDP) and compare four planners (deterministic FSM, QMDP, POMCP, DESPOT) using a custom simulation testbed with stochastic agents, pedestrians, occlusions, and adversarial scenarios.

Result: Probabilistic planners outperform the deterministic FSM baseline, achieving high rates of collision-free navigation under partial observability; POMCP prioritizes safety while DESPOT balances efficiency and runtime feasibility (and is more scalable).

Conclusion: Uncertainty-aware planning is important for driver assistance at uncontrolled intersections; future work should integrate sensor fusion and perception modules to enable real-time deployment in realistic traffic environments.

Abstract: Uncontrolled intersections account for a significant fraction of roadway
crashes due to ambiguous right-of-way rules, occlusions, and unpredictable
driver behavior. While autonomous vehicle research has explored
uncertainty-aware decision making, few systems exist to retrofit human-operated
vehicles with assistive navigation support. We present a driver-assist
framework for right-of-way reasoning at uncontrolled intersections, formulated
as a Partially Observable Markov Decision Process (POMDP). Using a custom
simulation testbed with stochastic traffic agents, pedestrians, occlusions, and
adversarial scenarios, we evaluate four decision-making approaches: a
deterministic finite state machine (FSM), and three probabilistic planners:
QMDP, POMCP, and DESPOT. Results show that probabilistic planners outperform
the rule-based baseline, achieving up to 97.5 percent collision-free navigation
under partial observability, with POMCP prioritizing safety and DESPOT
balancing efficiency and runtime feasibility. Our findings highlight the
importance of uncertainty-aware planning for driver assistance and motivate
future integration of sensor fusion and environment perception modules for
real-time deployment in realistic traffic environments.

</details>


### [131] [Latent Action Pretraining Through World Modeling](https://arxiv.org/abs/2509.18428)
*Bahey Tharwat,Yara Nasser,Ali Abouzeid,Ian Reid*

Main category: cs.RO

TL;DR: LAWM pretrains imitation learning models via self-supervised latent action learning from unlabeled video through world modeling, enabling efficient cross-task transfer and outperforming action-ground-truth baselines on LIBERO and real-world tests.


<details>
  <summary>Details</summary>
Motivation: Reduce reliance on large labeled action datasets and huge models; enable practical deployment by leveraging unlabeled video and world modeling to learn transferable latent actions.

Method: A model-agnostic LAWM framework that learns latent action representations from unlabeled video using world modeling to pretrain imitation learning models; accommodates robot or human action videos and emphasizes transfer across tasks, environments, and embodiments.

Result: Outperforms models trained with ground-truth robotics actions and similar pretraining methods on the LIBERO benchmark and real-world setup, while being significantly more efficient and practical for real-world deployment.

Conclusion: LAWM offers scalable, transferable self-supervised pretraining for imitation learning with latent actions, reducing data and model-size requirements and enabling effective real-world VLA deployment.

Abstract: Vision-Language-Action (VLA) models have gained popularity for learning
robotic manipulation tasks that follow language instructions. State-of-the-art
VLAs, such as OpenVLA and $\pi_{0}$, were trained on large-scale, manually
labeled action datasets collected through teleoperation. More recent
approaches, including LAPA and villa-X, introduce latent action representations
that enable unsupervised pretraining on unlabeled datasets by modeling abstract
visual changes between frames. Although these methods have shown strong
results, their large model sizes make deployment in real-world settings
challenging. In this work, we propose LAWM, a model-agnostic framework to
pretrain imitation learning models in a self-supervised way, by learning latent
action representations from unlabeled video data through world modeling. These
videos can be sourced from robot recordings or videos of humans performing
actions with everyday objects. Our framework is designed to be effective for
transferring across tasks, environments, and embodiments. It outperforms models
trained with ground-truth robotics actions and similar pretraining methods on
the LIBERO benchmark and real-world setup, while being significantly more
efficient and practical for real-world settings.

</details>


### [132] [PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction](https://arxiv.org/abs/2509.18447)
*Rishabh Madan,Jiawei Lin,Mahika Goel,Angchen Xie,Xiaoyu Liang,Marcus Lee,Justin Guo,Pranav N. Thakkar,Rohan Banerjee,Jose Barreiros,Kate Tsui,Tom Silver,Tapomayukh Bhattacharjee*

Main category: cs.RO

TL;DR: PrioriTouch is a learning-to-rank framework that prioritizes and executes multiple contact objectives in multi-contact physical human-robot interaction (pHRI), using hierarchical operational space control and simulation-in-the-loop data. It personalizes comfort thresholds from user studies to adapt to individual preferences, improving safety and task performance in caregiving and other multi-contact tasks.


<details>
  <summary>Details</summary>
Motivation: In pHRI, robots must adapt to individual human contact preferences. Whole-arm, multi-contact interactions create conflicting force requirements across body parts. In caregiving tasks, contact is frequent and varied, making trade-offs unavoidable and prioritization essential.

Method: PrioriTouch combines a novel learning-to-rank approach with hierarchical operational space control. It uses simulation-in-the-loop rollouts for data-efficient, safe exploration and incorporates personalized comfort thresholds derived from user studies into the ranking to guide control decisions.

Result: Through extensive simulations and real-world experiments, PrioriTouch demonstrates the ability to adapt to user contact preferences, maintain task performance, and enhance safety and comfort in multi-contact pHRI.

Conclusion: PrioriTouch provides a generalizable framework for prioritizing and executing multiple contact objectives in multi-contact scenarios, extending beyond caregiving to a broad range of pHRI tasks, with data-efficient learning and user-personalized thresholds.

Abstract: Physical human-robot interaction (pHRI) requires robots to adapt to
individual contact preferences, such as where and how much force is applied.
Identifying preferences is difficult for a single contact; with whole-arm
interaction involving multiple simultaneous contacts between the robot and
human, the challenge is greater because different body parts can impose
incompatible force requirements. In caregiving tasks, where contact is frequent
and varied, such conflicts are unavoidable. With multiple preferences across
multiple contacts, no single solution can satisfy all objectives--trade-offs
are inherent, making prioritization essential. We present PrioriTouch, a
framework for ranking and executing control objectives across multiple
contacts. PrioriTouch can prioritize from a general collection of controllers,
making it applicable not only to caregiving scenarios such as bed bathing and
dressing but also to broader multi-contact settings. Our method combines a
novel learning-to-rank approach with hierarchical operational space control,
leveraging simulation-in-the-loop rollouts for data-efficient and safe
exploration. We conduct a user study on physical assistance preferences, derive
personalized comfort thresholds, and incorporate them into PrioriTouch. We
evaluate PrioriTouch through extensive simulation and real-world experiments,
demonstrating its ability to adapt to user contact preferences, maintain task
performance, and enhance safety and comfort. Website:
https://emprise.cs.cornell.edu/prioritouch.

</details>


### [133] [Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands](https://arxiv.org/abs/2509.18455)
*Yunshuang Li,Yiyang Ling,Gaurav S. Sukhatme,Daniel Seita*

Main category: cs.RO

TL;DR: GD2P learns geometry-aware pre-contact hand poses for nonprehensile manipulation (pushing/pulling) with dexterous hands, using a diffusion model conditioned on object geometry and physics-filtered pose sampling to enable scalable dexterous manipulation.


<details>
  <summary>Details</summary>
Motivation: Dexterous hands offer rich contact modes that can stabilize and enable nonprehensile manipulation, but modeling the dynamics is hard and existing work relies on simple grippers; there is a need for geometry-aware, scalable methods.

Method: Generate diverse pre-contact hand poses via contact-guided sampling; filter with physics simulation; train a diffusion model conditioned on object geometry to predict viable poses; at test time sample poses and plan/execute pushing/pulling with standard motion planners.

Result: 840 real-world experiments on an Allegro hand show GD2P surpasses baselines; demonstrates cross-murface generality and applicability to a LEAP hand; provides a large dataset (1.3M hand poses across 2.3k objects) and open-source resources.

Conclusion: GD2P offers a scalable framework for training dexterous nonprehensile manipulation policies using geometry-aware hand poses, with broad applicability and reusable datasets across hand morphologies.

Abstract: Nonprehensile manipulation, such as pushing and pulling, enables robots to
move, align, or reposition objects that may be difficult to grasp due to their
geometry, size, or relationship to the robot or the environment. Much of the
existing work in nonprehensile manipulation relies on parallel-jaw grippers or
tools such as rods and spatulas. In contrast, multi-fingered dexterous hands
offer richer contact modes and versatility for handling diverse objects to
provide stable support over the objects, which compensates for the difficulty
of modeling the dynamics of nonprehensile manipulation. Therefore, we propose
Geometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile
manipulation with dexterous robotic hands. We study pushing and pulling by
framing the problem as synthesizing and learning pre-contact dexterous hand
poses that lead to effective manipulation. We generate diverse hand poses via
contact-guided sampling, filter them using physics simulation, and train a
diffusion model conditioned on object geometry to predict viable poses. At test
time, we sample hand poses and use standard motion planners to select and
execute pushing and pulling actions. We perform 840 real-world experiments with
an Allegro Hand, comparing our method to baselines. The results indicate that
GD2P offers a scalable route for training dexterous nonprehensile manipulation
policies. We further demonstrate GD2P on a LEAP Hand, highlighting its
applicability to different hand morphologies. Our pre-trained models and
dataset, including 1.3 million hand poses across 2.3k objects, will be
open-source to facilitate further research. Our project website is available
at: geodex2p.github.io.

</details>


### [134] [A Counterfactual Reasoning Framework for Fault Diagnosis in Robot Perception Systems](https://arxiv.org/abs/2509.18460)
*Haeyoon Han,Mahdi Taheri,Soon-Jo Chung,Fred Y. Hadaegh*

Main category: cs.RO

TL;DR: Counterfactual analytical redundancy for fault detection and isolation (FDI) in perception systems; combines passive belief updates with active causal bandit control using MCTS to maximize Effective Information, demonstrated on a space-robot with vision-based navigation.


<details>
  <summary>Details</summary>
Motivation: Perception faults depend on environment and can propagate through multi-stage pipelines. Physical redundancy (extra sensors) is costly; analytical redundancy via counterfactuals enables reliable FDI.

Method: Construct reliability tests as causal outcomes; use belief updates for passive FDI; formulate active FDI as a causal bandit problem; apply Monte Carlo Tree Search with UCB to select control inputs that maximize EI.

Result: Illustrated in a robot exploration scenario where the space robot adjusts attitude to increase EI and correctly isolates faults due to sensor damage, dynamic scenes, and perceptual degradation.

Conclusion: Counterfactual reasoning with analytical redundancy yields effective FDI without additional sensors; active FDI via causal bandits enhances fault isolation by probing informative states.

Abstract: Perception systems provide a rich understanding of the environment for
autonomous systems, shaping decisions in all downstream modules. Hence,
accurate detection and isolation of faults in perception systems is important.
Faults in perception systems pose particular challenges: faults are often tied
to the perceptual context of the environment, and errors in their multi-stage
pipelines can propagate across modules. To address this, we adopt a
counterfactual reasoning approach to propose a framework for fault detection
and isolation (FDI) in perception systems. As opposed to relying on physical
redundancy (i.e., having extra sensors), our approach utilizes analytical
redundancy with counterfactual reasoning to construct perception reliability
tests as causal outcomes influenced by system states and fault scenarios.
Counterfactual reasoning generates reliability test results under hypothesized
faults to update the belief over fault hypotheses. We derive both passive and
active FDI methods. While the passive FDI can be achieved by belief updates,
the active FDI approach is defined as a causal bandit problem, where we utilize
Monte Carlo Tree Search (MCTS) with upper confidence bound (UCB) to find
control inputs that maximize a detection and isolation metric, designated as
Effective Information (EI). The mentioned metric quantifies the informativeness
of control inputs for FDI. We demonstrate the approach in a robot exploration
scenario, where a space robot performing vision-based navigation actively
adjusts its attitude to increase EI and correctly isolate faults caused by
sensor damage, dynamic scenes, and perceptual degradation.

</details>


### [135] [Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task](https://arxiv.org/abs/2509.18463)
*Jannick van Buuren,Roberto Giglio,Loris Roveda,Luka Peternel*

Main category: cs.RO

TL;DR: Gaussian-noise reward mutations in PPO-based RL yield diverse robotic pouring skills in simulation.


<details>
  <summary>Details</summary>
Motivation: To induce diverse skill repertoires by perturbing reward weights, inspired by cost-benefit tradeoffs in human motor control.

Method: Apply Gaussian noise to the weights of reward terms (accuracy, time, effort) in a PPO RL framework. Experiments in NVIDIA Isaac Sim with a Franka Emika Panda manipulating a glass of liquid to pour into a container; systematic exploration of weight mutations.

Result: Learned policies exhibit a broad range of behaviors, from faithful pouring to novel tasks such as container rim cleaning, liquid mixing, and watering.

Conclusion: Reward function mutation is a promising approach to foster diverse skill learning and potential transferable competencies in robotic manipulation.

Abstract: This paper explores how deliberate mutations of reward function in
reinforcement learning can produce diversified skill variations in robotic
manipulation tasks, examined with a liquid pouring use case. To this end, we
developed a new reward function mutation framework that is based on applying
Gaussian noise to the weights of the different terms in the reward function.
Inspired by the cost-benefit tradeoff model from human motor control, we
designed the reward function with the following key terms: accuracy, time, and
effort. The study was performed in a simulation environment created in NVIDIA
Isaac Sim, and the setup included Franka Emika Panda robotic arm holding a
glass with a liquid that needed to be poured into a container. The
reinforcement learning algorithm was based on Proximal Policy Optimization. We
systematically explored how different configurations of mutated weights in the
rewards function would affect the learned policy. The resulting policies
exhibit a wide range of behaviours: from variations in execution of the
originally intended pouring task to novel skills useful for unexpected tasks,
such as container rim cleaning, liquid mixing, and watering. This approach
offers promising directions for robotic systems to perform diversified learning
of specific tasks, while also potentially deriving meaningful skills for future
tasks.

</details>


### [136] [RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain](https://arxiv.org/abs/2509.18466)
*Junnosuke Kamohara,Feiyang Wu,Chinmayee Wamorkar,Seth Hutchinson,Ye Zhao*

Main category: cs.RO

TL;DR: An RL-augmented MPC framework for bipedal locomotion on rough/slippery terrain that learns three components (dynamics, swing leg controller, gait frequency) to improve adaptability, validated in simulations across stairs, stepping stones, and low-friction surfaces; outperforms baseline MPC and RL.


<details>
  <summary>Details</summary>
Motivation: MPC-based locomotion struggles with poorly modeled terrain interactions, while RL-based policies lack formal constraint guarantees and need reward engineering. Combining them aims to achieve constraint-satisfying, robust locomotion over diverse terrains for bipeds.

Method: Extend single-rigid-body-dynamics-based MPC by parametrizing three components with RL: the system dynamics, the swing leg controller, and the gait frequency. Train/optimize these components within an RL-MPC framework and evaluate on a biped in NVIDIA IsaacLab across rough surfaces (stairs, stepping stones, low friction).

Result: The RL-augmented MPC yields significantly more adaptive and robust locomotion behaviors compared with baseline MPC and RL across tested terrains.

Conclusion: RL-augmented MPC can inherit the strengths of both approaches to handle rough, slippery terrains for bipedal robots; the framework demonstrates improved robustness on simulation platforms and suggests potential applicability to real robots.

Abstract: Model predictive control (MPC) has demonstrated effectiveness for humanoid
bipedal locomotion; however, its applicability in challenging environments,
such as rough and slippery terrain, is limited by the difficulty of modeling
terrain interactions. In contrast, reinforcement learning (RL) has achieved
notable success in training robust locomotion policies over diverse terrain,
yet it lacks guarantees of constraint satisfaction and often requires
substantial reward shaping. Recent efforts in combining MPC and RL have shown
promise of taking the best of both worlds, but they are primarily restricted to
flat terrain or quadrupedal robots. In this work, we propose an RL-augmented
MPC framework tailored for bipedal locomotion over rough and slippery terrain.
Our method parametrizes three key components of
single-rigid-body-dynamics-based MPC: system dynamics, swing leg controller,
and gait frequency. We validate our approach through bipedal robot simulations
in NVIDIA IsaacLab across various terrains, including stairs, stepping stones,
and low-friction surfaces. Experimental results demonstrate that our
RL-augmented MPC framework produces significantly more adaptive and robust
behaviors compared to baseline MPC and RL.

</details>


### [137] [Spatial Envelope MPC: High Performance Driving without a Reference](https://arxiv.org/abs/2509.18506)
*Siyuan Yu,Congkai Shen,Yufei Xi,James Dallas,Michael Thompson,John Subosits,Hiroshi Yasuda,Tulga Ersal*

Main category: cs.RO

TL;DR: An envelope-based MPC for autonomous driving enables high-performance operation without a predefined reference by modeling the drivable envelope, integrating dynamic feasibility and safety constraints, and blending reinforcement learning with optimization to approximate safe regions; validated in simulation and real-world tasks.


<details>
  <summary>Details</summary>
Motivation: Current planning/control largely relies on reference trajectories, which can be suboptimal or infeasible at dynamic limits. A real-time framework that respects vehicle dynamics and safety without a fixed reference is needed for high-performance autonomous driving.

Method: Develop a computationally efficient vehicle dynamics model and a continuously differentiable envelope formulation that captures the full drivable area. Integrate dynamic feasibility and safety constraints into a unified MPC. Address envelope planning by combining reinforcement learning with optimization to approximate the safe drivable region.

Result: Validations in simulations and real-world experiments show high performance across tasks (racing, emergency collision avoidance, off-road navigation), demonstrating scalability and broad applicability.

Conclusion: The framework provides a scalable, reference-free path to high-performance autonomous driving, with broad applicability across diverse driving scenarios.

Abstract: This paper presents a novel envelope based model predictive control (MPC)
framework designed to enable autonomous vehicles to handle high performance
driving across a wide range of scenarios without a predefined reference. In
high performance autonomous driving, safe operation at the vehicle's dynamic
limits requires a real time planning and control framework capable of
accounting for key vehicle dynamics and environmental constraints when
following a predefined reference trajectory is suboptimal or even infeasible.
State of the art planning and control frameworks, however, are predominantly
reference based, which limits their performance in such situations. To address
this gap, this work first introduces a computationally efficient vehicle
dynamics model tailored for optimization based control and a continuously
differentiable mathematical formulation that accurately captures the entire
drivable envelope. This novel model and formulation allow for the direct
integration of dynamic feasibility and safety constraints into a unified
planning and control framework, thereby removing the necessity for predefined
references. The challenge of envelope planning, which refers to maximally
approximating the safe drivable area, is tackled by combining reinforcement
learning with optimization techniques. The framework is validated through both
simulations and real world experiments, demonstrating its high performance
across a variety of tasks, including racing, emergency collision avoidance and
off road navigation. These results highlight the framework's scalability and
broad applicability across a diverse set of scenarios.

</details>


### [138] [LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA](https://arxiv.org/abs/2509.18576)
*Zeyi Kang,Liang He,Yanxin Zhang,Zuheng Ming,Kaixing Zhao*

Main category: cs.RO

TL;DR: LCMF is a lightweight cascaded attention framework with multi-level cross-modal parameter sharing in the Mamba module, enabling efficient fusion of heterogeneous modalities for robotics. It achieves strong VQA performance (74.29%) and competitive EQA video-task results with significantly reduced FLOPs (~4.35x) and modest parameter counts (image-text: 166.51M; video-text: 219M).


<details>
  <summary>Details</summary>
Motivation: Address the core challenges of multimodal fusion and computational efficiency in resource-constrained embodied AI/robotics, aiming for robust semantic alignment and generalization.

Method: Introduce lightweight LCMF cascaded attention with cross-attention and selective parameter-sharing SSMs embedded in the Mamba module, enabling multi-level cross-modal parameter sharing and efficient fusion of image-text and video-text modalities; evaluate on VQA and EQA video tasks.

Result: LCMF outperforms existing multimodal baselines in VQA (74.29% accuracy) and shows competitive mid-tier performance in EQA video tasks; achieves 4.35x FLOPs reduction relative to baselines with 166.51M (image-text) and 219M (video-text) parameters.

Conclusion: LCMF offers an efficient, generalizable solution for multimodal understanding in resource-constrained HRI scenarios, combining effective cross-modal fusion with low computational cost and strong decision-generalization capabilities.

Abstract: Multimodal semantic learning plays a critical role in embodied intelligence,
especially when robots perceive their surroundings, understand human
instructions, and make intelligent decisions. However, the field faces
technical challenges such as effective fusion of heterogeneous data and
computational efficiency in resource-constrained environments. To address these
challenges, this study proposes the lightweight LCMF cascaded attention
framework, introducing a multi-level cross-modal parameter sharing mechanism
into the Mamba module. By integrating the advantages of Cross-Attention and
Selective parameter-sharing State Space Models (SSMs), the framework achieves
efficient fusion of heterogeneous modalities and semantic complementary
alignment. Experimental results show that LCMF surpasses existing multimodal
baselines with an accuracy of 74.29% in VQA tasks and achieves competitive
mid-tier performance within the distribution cluster of Large Language Model
Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a
4.35-fold reduction in FLOPs relative to the average of comparable baselines
while using only 166.51M parameters (image-text) and 219M parameters
(video-text), providing an efficient solution for Human-Robot Interaction (HRI)
applications in resource-constrained scenarios with strong multimodal decision
generalization capabilities.

</details>


### [139] [VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation](https://arxiv.org/abs/2509.18592)
*Neel P. Bhatt,Yunhao Yang,Rohan Siva,Pranay Samineni,Daniel Milan,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: VLN-Zero introduces zero-shot neurosymbolic navigation by combining VLM-guided exploration to build compact scene graphs with a two-phase planner and cache-enabled execution, achieving faster, more robust unseen-environment navigation with fewer VLM calls.


<details>
  <summary>Details</summary>
Motivation: Rapid adaptation in unseen environments without exhaustive exploration or rigid policies; existing methods struggle with generalization and are computationally costly.

Method: Two-phase approach: (1) exploration: VLM-guided search using structured prompts to produce diverse, informative trajectories and compact symbolic scene graphs; (2) deployment: a neurosymbolic planner operating on the scene graph and observations to generate executable plans, aided by a cache that reuses previously computed task-location trajectories.

Result: Performs ~2x better in success rate than state-of-the-art zero-shot models, outperforms many fine-tuned baselines, reaches goals in about half the time, and reduces VLM calls by ~55% on average across diverse environments.

Conclusion: Combines rapid exploration, symbolic reasoning, and caching to improve robustness, scalability, and generalization in Vision-Language Navigation, mitigating computational inefficiency and overfitting of prior methods; demonstrates strong zero-shot performance.

Abstract: Rapid adaptation in unseen environments is essential for scalable real-world
autonomy, yet existing approaches rely on exhaustive exploration or rigid
navigation policies that fail to generalize. We present VLN-Zero, a two-phase
vision-language navigation framework that leverages vision-language models to
efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic
navigation. In the exploration phase, structured prompts guide VLM-based search
toward informative and diverse trajectories, yielding compact scene graph
representations. In the deployment phase, a neurosymbolic planner reasons over
the scene graph and environmental observations to generate executable plans,
while a cache-enabled execution module accelerates adaptation by reusing
previously computed task-location trajectories. By combining rapid exploration,
symbolic reasoning, and cache-enabled execution, the proposed framework
overcomes the computational inefficiency and poor generalization of prior
vision-language navigation methods, enabling robust and scalable
decision-making in unseen environments. VLN-Zero achieves 2x higher success
rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned
baselines, and reaches goal locations in half the time with 55% fewer VLM calls
on average compared to state-of-the-art models across diverse environments.
Codebase, datasets, and videos for VLN-Zero are available at:
https://vln-zero.github.io/.

</details>


### [140] [Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills](https://arxiv.org/abs/2509.18597)
*Yuan Meng,Zhenguo Sun,Max Fest,Xukun Li,Zhenshan Bing,Alois Knoll*

Main category: cs.RO

TL;DR: A human-in-the-loop framework that learns reusable skills from corrections and uses external memory plus Retrieval-Augmented Generation with hints to enable dynamic skill reuse, improving long-horizon robotic manipulation with higher success and efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome noise, fixed primitives, limited context windows, and forgetting in LLM-driven robotic code generation, and to enable reusable skills for long-horizon tasks through learning from corrections.

Method: Introduce a human-in-the-loop process that encodes corrections into reusable skills, backed by external memory and Retrieval-Augmented Generation with a hint mechanism for dynamic reuse; validate on Ravens, Franka Kitchen, MetaWorld, and real-world settings.

Result: Achieves a 0.93 success rate (up to 27% above baselines) and 42% efficiency improvement in correction rounds; capable of extremely long-horizon tasks such as 'build a house' requiring planning over about 20 primitives.

Conclusion: The framework robustly enables reusable skill learning and dynamic reuse for long-horizon robotic manipulation, improving generalization, correction efficiency, and resilience to forgetting.

Abstract: Large language models (LLMs)-based code generation for robotic manipulation
has recently shown promise by directly translating human instructions into
executable code, but existing methods remain noisy, constrained by fixed
primitives and limited context windows, and struggle with long-horizon tasks.
While closed-loop feedback has been explored, corrected knowledge is often
stored in improper formats, restricting generalization and causing catastrophic
forgetting, which highlights the need for learning reusable skills. Moreover,
approaches that rely solely on LLM guidance frequently fail in extremely
long-horizon scenarios due to LLMs' limited reasoning capability in the robotic
domain, where such issues are often straightforward for humans to identify. To
address these challenges, we propose a human-in-the-loop framework that encodes
corrections into reusable skills, supported by external memory and
Retrieval-Augmented Generation with a hint mechanism for dynamic reuse.
Experiments on Ravens, Franka Kitchen, and MetaWorld, as well as real-world
settings, show that our framework achieves a 0.93 success rate (up to 27%
higher than baselines) and a 42% efficiency improvement in correction rounds.
It can robustly solve extremely long-horizon tasks such as "build a house",
which requires planning over 20 primitives.

</details>


### [141] [End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning](https://arxiv.org/abs/2509.18608)
*Ana Luiza Mineiro,Francisco Affonso,Marcelo Becker*

Main category: cs.RO

TL;DR: End-to-end deep RL navigation that maps voxel-downsampled 3D LiDAR to vehicle controls, trained entirely in simulation; achieves 100% straight-row success; performance declines with row curvature; voxel downsampling reduces LiDAR input by 95.83%.


<details>
  <summary>Details</summary>
Motivation: GNSS unreliability, cluttered canopy rows, and variable lighting hinder autonomous navigation in agricultural fields. Existing approaches rely on labeled data or hand-engineered interfaces, creating data collection costs and limited robustness; a data-efficient, end-to-end policy could enable robust under-canopy navigation.

Method: Train a deep reinforcement learning policy that directly maps raw 3D LiDAR input to control commands. Use voxel-based downsampling to reduce input size by 95.83%, enabling efficient learning. The policy is trained entirely in a simulated environment without labeled datasets or manually designed control interfaces, and validated across varying row geometries.

Result: In simulation, the policy achieves a 100% success rate in straight-row plantations. Performance deteriorates gradually as row curvature increases, evaluated across different sinusoidal frequencies and amplitudes.

Conclusion: An end-to-end, data-efficient RL approach can effectively navigate under-canopy environments in simulation, with strong performance on straight rows and predictable degradation with curvature. Real-world transfer and handling more complex curvature remain avenues for future work.

Abstract: Reliable navigation in under-canopy agricultural environments remains a
challenge due to GNSS unreliability, cluttered rows, and variable lighting. To
address these limitations, we present an end-to-end learning-based navigation
system that maps raw 3D LiDAR data directly to control commands using a deep
reinforcement learning policy trained entirely in simulation. Our method
includes a voxel-based downsampling strategy that reduces LiDAR input size by
95.83%, enabling efficient policy learning without relying on labeled datasets
or manually designed control interfaces. The policy was validated in
simulation, achieving a 100% success rate in straight-row plantations and
showing a gradual decline in performance as row curvature increased, tested
across varying sinusoidal frequencies and amplitudes.

</details>


### [142] [PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving](https://arxiv.org/abs/2509.18609)
*Chengran Yuan,Zijian Lu,Zhanqi Zhang,Yimin Zhao,Zefan Huang,Shuo Sun,Jiawei Sun,Jiahui Li,Christina Dao Wen Lee,Dongen Li,Marcelo H. Ang Jr*

Main category: cs.RO

TL;DR: PIE is an end-to-end motion-planning framework that fuses perception and reasoning to model interactions and improve ego trajectory inference, achieving state-of-the-art NAVSIM performance without ensembles or data augmentation.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving pipelines struggle with scene understanding and decision-making; integrating perception, reasoning, and intention modeling can better capture interactions between the ego and surrounding agents and improve planning reliability.

Method: PIE fuses camera and LiDAR data via bidirectional Mamba fusion to mitigate multimodal data losses, uses a reasoning-enhanced decoder that combines Mamba with a Mixture-of-Experts to enable scene-consistent anchor selection and adaptive trajectory inference, and includes an action-motion interaction module that leverages predicted states of surrounding agents to refine ego planning. Evaluation is performed on the NAVSIM benchmark, without using ensemble methods or data augmentation.

Result: PIE achieves 88.9 PDM and 85.6 EPDM on NAVSIM, outperforming prior state-of-the-art methods. Extensive quantitative and qualitative analyses show the model can reliably generate feasible, high-quality ego trajectories.

Conclusion: PIE demonstrates that integrating advanced perception, reasoning, and intention modeling with a MoE-based decoder and robust multimodal fusion yields reliable, high-quality end-to-end ego trajectories and strong NAVSIM performance, indicating strong potential for real-world deployment.

Abstract: End-to-end motion planning is promising for simplifying complex autonomous
driving pipelines. However, challenges such as scene understanding and
effective prediction for decision-making continue to present substantial
obstacles to its large-scale deployment. In this paper, we present PIE, a
pioneering framework that integrates advanced perception, reasoning, and
intention modeling to dynamically capture interactions between the ego vehicle
and surrounding agents. It incorporates a bidirectional Mamba fusion that
addresses data compression losses in multimodal fusion of camera and LiDAR
inputs, alongside a novel reasoning-enhanced decoder integrating Mamba and
Mixture-of-Experts to facilitate scene-compliant anchor selection and optimize
adaptive trajectory inference. PIE adopts an action-motion interaction module
to effectively utilize state predictions of surrounding agents to refine ego
planning. The proposed framework is thoroughly validated on the NAVSIM
benchmark. PIE, without using any ensemble and data augmentation techniques,
achieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of
prior state-of-the-art methods. Comprehensive quantitative and qualitative
analyses demonstrate that PIE is capable of reliably generating feasible and
high-quality ego trajectories.

</details>


### [143] [SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones](https://arxiv.org/abs/2509.18610)
*Maximilian Adang,JunEn Low,Ola Shorinwa,Mac Schwager*

Main category: cs.RO

TL;DR: SINGER enables language-guided autonomous drone navigation on-board using a photorealistic language-embedded simulator (Gaussian Splatting), an RRT-inspired data generator, and a lightweight visuomotor policy, achieving strong zero-shot sim-to-real transfer and improved task performance in hardware with ~700k-1M demonstrations.


<details>
  <summary>Details</summary>
Motivation: Open-vocabulary autonomous drone navigation remains unsolved due to scarce large-scale demonstrations, real-time stabilization requirements, and lack of reliable external pose estimation. A onboard, language-conditioned approach is needed.

Method: (i) Photorealistic language-embedded flight simulator with Gaussian Splatting to generate data with minimal sim-to-real gap; (ii) an RRT-inspired multi-trajectory generation expert for collision-free navigation demonstrations; (iii) a lightweight end-to-end visuomotor policy for real-time closed-loop control.

Result: Hardware flight experiments show strong zero-shot sim-to-real transfer to unseen environments and unseen language-conditioned goal objects. When trained on ~700k–1M observation-action pairs, SINGER outperforms a velocity-controlled semantic guidance baseline by ~23.33% in query-reaching, ~16.67% better keeping the query in the field of view, and ~10% fewer collisions.

Conclusion: SINGER demonstrates robust open-vocabulary drone navigation using onboard sensing and compute, with strong sim-to-real transfer and scalability across unseen environments and language goals.

Abstract: Large vision-language models have driven remarkable progress in
open-vocabulary robot policies, e.g., generalist robot manipulation policies,
that enable robots to complete complex tasks specified in natural language.
Despite these successes, open-vocabulary autonomous drone navigation remains an
unsolved challenge due to the scarcity of large-scale demonstrations, real-time
control demands of drones for stabilization, and lack of reliable external pose
estimation modules. In this work, we present SINGER for language-guided
autonomous drone navigation in the open world using only onboard sensing and
compute. To train robust, open-vocabulary navigation policies, SINGER leverages
three central components: (i) a photorealistic language-embedded flight
simulator with minimal sim-to-real gap using Gaussian Splatting for efficient
data generation, (ii) an RRT-inspired multi-trajectory generation expert for
collision-free navigation demonstrations, and these are used to train (iii) a
lightweight end-to-end visuomotor policy for real-time closed-loop control.
Through extensive hardware flight experiments, we demonstrate superior
zero-shot sim-to-real transfer of our policy to unseen environments and unseen
language-conditioned goal objects. When trained on ~700k-1M observation action
pairs of language conditioned visuomotor data and deployed on hardware, SINGER
outperforms a velocity-controlled semantic guidance baseline by reaching the
query 23.33% more on average, and maintains the query in the field of view
16.67% more on average, with 10% fewer collisions.

</details>


### [144] [The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving](https://arxiv.org/abs/2509.18626)
*Jay Patrikar,Apoorva Sharma,Sushant Veer,Boyi Li,Sebastian Scherer,Marco Pavone*

Main category: cs.RO

TL;DR: A retrieval-based, ego-centric, crash-narrative grounded framework that uses accident reports to create a unified scene-action index for autonomous driving decisions, enhanced by agentic counterfactuals. Empirically improves calibration and decision quality on nuScenes.


<details>
  <summary>Details</summary>
Motivation: Addresses the gap that incident-free training data lack guidance near safety-performance boundaries; crash reports offer precise, contrastive evidence but are unstructured and misaligned with sensor views.

Method: Normalize crash narratives to ego-centric language; unify logs and crashes into a scene-action representation; perform precedent retrieval at decision time; extend with agentic counterfactuals to propose and evaluate alternatives before deciding.

Result: Precedent retrieval boosts calibration on nuScenes; recall of contextually preferred actions rises from 24% to 53%. The counterfactual variant preserves gains and sharpens decisions near risk.

Conclusion: Leveraging crash narratives in a unified retrieval framework with counterfactual reasoning can improve calibration and decision quality for autonomous driving, especially near safety boundaries.

Abstract: Learning-based autonomous driving systems are trained mostly on incident-free
data, offering little guidance near safety-performance boundaries. Real crash
reports contain precisely the contrastive evidence needed, but they are hard to
use: narratives are unstructured, third-person, and poorly grounded to sensor
views. We address these challenges by normalizing crash narratives to
ego-centric language and converting both logs and crashes into a unified
scene-action representation suitable for retrieval. At decision time, our
system adjudicates proposed actions by retrieving relevant precedents from this
unified index; an agentic counterfactual extension proposes plausible
alternatives, retrieves for each, and reasons across outcomes before deciding.
On a nuScenes benchmark, precedent retrieval substantially improves
calibration, with recall on contextually preferred actions rising from 24% to
53%. The counterfactual variant preserves these gains while sharpening
decisions near risk.

</details>


### [145] [Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training](https://arxiv.org/abs/2509.18631)
*Shuo Cheng,Liqian Ma,Zhenyang Chen,Ajay Mandlekar,Caelan Garrett,Danfei Xu*

Main category: cs.RO

TL;DR: A sim-and-real co-training framework for manipulation that learns a domain-invariant feature space by aligning joint distributions of observations and actions across sim and real domains using an OT-inspired loss, with unbalanced OT to handle data imbalance; achieves improved real-world performance and generalizes beyond simulated scenarios.


<details>
  <summary>Details</summary>
Motivation: Real-world data for robotic manipulation is expensive and slow to scale, while simulation provides abundant data but suffers from domain gaps. Aligning marginal observations is insufficient; aligning joint distributions of observations and actions across domains can provide a richer supervision signal. The approach uses an OT-inspired loss within a co-training framework and handles data imbalance with unbalanced OT to bridge sim-to-real gaps.

Method: A unified sim-and-real co-training framework that learns a domain-invariant, task-relevant feature space. It embeds an Optimal Transport–inspired loss to align the joint distributions of observations and actions across domains, extended with Unbalanced OT to accommodate more simulation data than real-world data, and trains on both simulated data and a limited set of real demonstrations.

Result: On challenging manipulation tasks, the method leverages abundant simulation data to achieve up to a 30% improvement in real-world success rate and can generalize to scenarios seen only in simulation.

Conclusion: The OT-guided joint distribution alignment within a sim-and-real co-training framework enables effective sim-to-real transfer with limited real data, producing robust, generalizable manipulation policies and suggesting broader applicability to robotics tasks where simulation is abundant.

Abstract: Behavior cloning has shown promise for robot manipulation, but real-world
demonstrations are costly to acquire at scale. While simulated data offers a
scalable alternative, particularly with advances in automated demonstration
generation, transferring policies to the real world is hampered by various
simulation and real domain gaps. In this work, we propose a unified
sim-and-real co-training framework for learning generalizable manipulation
policies that primarily leverages simulation and only requires a few real-world
demonstrations. Central to our approach is learning a domain-invariant,
task-relevant feature space. Our key insight is that aligning the joint
distributions of observations and their corresponding actions across domains
provides a richer signal than aligning observations (marginals) alone. We
achieve this by embedding an Optimal Transport (OT)-inspired loss within the
co-training framework, and extend this to an Unbalanced OT framework to handle
the imbalance between abundant simulation data and limited real-world examples.
We validate our method on challenging manipulation tasks, showing it can
leverage abundant simulation data to achieve up to a 30% improvement in the
real-world success rate and even generalize to scenarios seen only in
simulation.

</details>


### [146] [Number Adaptive Formation Flight Planning via Affine Deformable Guidance in Narrow Environments](https://arxiv.org/abs/2509.18636)
*Yuan Zhou,Jialiang Hou,Guangtong Xu,Fei Gao*

Main category: cs.RO

TL;DR: A deformable-virtual-structure based formation planning method enables adaptive reconfiguration with varying drone counts in narrow/cluttered environments, using PAAS (Lloyd partitioning + Hungarian assignment) and DVS-driven spatiotemporal trajectories, validated in simulation and real-world experiments.


<details>
  <summary>Details</summary>
Motivation: Maintaining formation when swarm size varies in constrained environments is challenging, causing slow convergence and loss of shape; a controllable method is needed to adapt to join/leave events while preserving safety and formation integrity.

Method: Combine Deformable Virtual Structures (DVS) with continuous spatiotemporal transformation. Use PAAS (Lloyd partitioning for uniform partitioning and Hungarian assignment) to satisfy safety distances and preserve formation shape for irregular geometries. Plan spatiotemporal trajectories with primitive-based path search and nonlinear trajectory optimization. Implement distributed trajectory planning using desired spatiotemporal positions, with collision avoidance and dynamic feasibility constraints.

Result: Allows up to 15% of drones to join or leave in cluttered environments, with rapid restoration of the desired formation in simulation. Demonstrates faster formation recovery and better environmental adaptability than state-of-the-art methods. Validated by real-world experiments.

Conclusion: The proposed DVS-guided formation planning provides robust, scalable adaptation to varying swarm sizes in narrow environments, achieving rapid recovery and strong resilience, supported by simulation and real-world tests.

Abstract: Formation maintenance with varying number of drones in narrow environments
hinders the convergence of planning to the desired configurations. To address
this challenge, this paper proposes a formation planning method guided by
Deformable Virtual Structures (DVS) with continuous spatiotemporal
transformation. Firstly, to satisfy swarm safety distance and preserve
formation shape filling integrity for irregular formation geometries, we employ
Lloyd algorithm for uniform $\underline{PA}$rtitioning and Hungarian algorithm
for $\underline{AS}$signment (PAAS) in DVS. Subsequently, a spatiotemporal
trajectory involving DVS is planned using primitive-based path search and
nonlinear trajectory optimization. The DVS trajectory achieves adaptive
transitions with respect to a varying number of drones while ensuring
adaptability to narrow environments through affine transformation. Finally,
each agent conducts distributed trajectory planning guided by desired
spatiotemporal positions within the DVS, while incorporating collision
avoidance and dynamic feasibility requirements. Our method enables up to 15\%
of swarm numbers to join or leave in cluttered environments while rapidly
restoring the desired formation shape in simulation. Compared to cutting-edge
formation planning method, we demonstrate rapid formation recovery capacity and
environmental adaptability. Real-world experiments validate the effectiveness
and resilience of our formation planning method.

</details>


### [147] [Do You Need Proprioceptive States in Visuomotor Policies?](https://arxiv.org/abs/2509.18644)
*Juntu Zhao,Wenbo Lu,Di Zhang,Yufeng Liu,Yushen Liang,Tianluo Zhang,Yifeng Cao,Junyuan Xie,Yingdong Hu,Shengjie Wang,Junliang Guo,Dequan Wang,Yang Gao*

Main category: cs.RO

TL;DR: A state-free visuomotor policy uses visual input only in a relative end-effector action space, yielding strong spatial generalization and cross-embodiment robustness compared to a state-based policy.


<details>
  <summary>Details</summary>
Motivation: Proprioceptive inputs cause overfitting to training trajectories, hindering spatial generalization across task variations and robot embodiments. The work seeks to rely on task-relevant visual information to improve generalization.

Method: Train a state-free policy that predicts actions from visual observations only, operating in a relative end-effector action space. Visual input comes from dual wide-angle wrist cameras to provide complete task-relevant information. Evaluations cover real-world tasks (pick-and-place, shirt-folding, whole-body manipulation) across multiple robot embodiments, focusing on spatial generalization and data efficiency.

Result: Significant improvement in spatial generalization: height generalization accuracy rises from 0% (state-based) to 85% (state-free); horizontal generalization rises from 6% to 64%. Additional gains in data efficiency and cross-embodiment adaptation.

Conclusion: Removing proprioceptive state input and using vision-based policies in a relative action space enhances spatial generalization and adaptability across robots, making such policies more practical for real-world deployment.

Abstract: Imitation-learning-based visuomotor policies have been widely used in robot
manipulation, where both visual observations and proprioceptive states are
typically adopted together for precise control. However, in this study, we find
that this common practice makes the policy overly reliant on the proprioceptive
state input, which causes overfitting to the training trajectories and results
in poor spatial generalization. On the contrary, we propose the State-free
Policy, removing the proprioceptive state input and predicting actions only
conditioned on visual observations. The State-free Policy is built in the
relative end-effector action space, and should ensure the full task-relevant
visual observations, here provided by dual wide-angle wrist cameras. Empirical
results demonstrate that the State-free policy achieves significantly stronger
spatial generalization than the state-based policy: in real-world tasks such as
pick-and-place, challenging shirt-folding, and complex whole-body manipulation,
spanning multiple robot embodiments, the average success rate improves from 0\%
to 85\% in height generalization and from 6\% to 64\% in horizontal
generalization. Furthermore, they also show advantages in data efficiency and
cross-embodiment adaptation, enhancing their practicality for real-world
deployment.

</details>


### [148] [SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer](https://arxiv.org/abs/2509.18648)
*Yarden As,Chengrui Qu,Benjamin Unger,Dongho Kang,Max van der Hart,Laixi Shi,Stelian Coros,Adam Wierman,Andreas Krause*

Main category: cs.RO

TL;DR: SPiDR provides a scalable, provably safe sim-to-real transfer for RL by using pessimistic domain randomization to enforce safety constraints under worst-case plausible sim-to-real gaps, validated on sim-to-sim benchmarks and two real robotic platforms while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The sim-to-real gap creates safety risks for deploying RL on real robots. While robust safe RL methods offer guarantees, they are often not scalable to large training regimes. Domain randomization is scalable but can yield unsafe behaviors in practice. There is a need for a method that combines scalable training with provable safety guarantees during real-world deployment.

Method: Introduce SPiDR (Sim-to-real via Pessimistic Domain Randomization). SPiDR models the sim-to-real gap as uncertainty within domain parameters and enforces safety constraints under a pessimistic (worst-case) realization within this uncertainty set. This pessimistic, domain-randomization-based safety enforcement is designed to be compatible with standard RL training pipelines, providing provable safety guarantees for transfer from simulation to the real world. The approach leverages domain randomization to capture gap uncertainty while constraining policy behavior to remain safe across plausible real-world variations.

Result: Empirical evaluation on both sim-to-sim benchmarks and two real-world robotic platforms shows SPiDR can guarantee safety despite the sim-to-real gap while preserving strong task performance. It outperforms naive domain randomization in safety metrics and demonstrates practical applicability due to its compatibility with existing training pipelines.

Conclusion: SPiDR offers a practical, scalable route to safe RL deployment across real robots by embedding pessimism about the sim-to-real gap into safety constraints. It provides provable guarantees and strong empirical performance, highlighting a promising direction for safe, scalable sim-to-real RL. Future work may address conservatism trade-offs, broader task domains, and tighter integration with diverse simulation-to-real gap models.

Abstract: Safety remains a major concern for deploying reinforcement learning (RL) in
real-world applications. Simulators provide safe, scalable training
environments, but the inevitable sim-to-real gap introduces additional safety
concerns, as policies must satisfy constraints in real-world conditions that
differ from simulation. To address this challenge, robust safe RL techniques
offer principled methods, but are often incompatible with standard scalable
training pipelines. In contrast, domain randomization, a simple and popular
sim-to-real technique, stands out as a promising alternative, although it often
results in unsafe behaviors in practice. We present SPiDR, short for
Sim-to-real via Pessimistic Domain Randomization -- a scalable algorithm with
provable guarantees for safe sim-to-real transfer. SPiDR uses domain
randomization to incorporate the uncertainty about the sim-to-real gap into the
safety constraints, making it versatile and highly compatible with existing
training pipelines. Through extensive experiments on sim-to-sim benchmarks and
two distinct real-world robotic platforms, we demonstrate that SPiDR
effectively ensures safety despite the sim-to-real gap while maintaining strong
performance.

</details>


### [149] [Distributionally Robust Safe Motion Planning with Contextual Information](https://arxiv.org/abs/2509.18666)
*Kaizer Rahaman,Simran Kumari,Ashish R. Hota*

Main category: cs.RO

TL;DR: A distributionally robust, context-aware collision-avoidance framework that uses conditional kernel mean embeddings in RKHS to model obstacle trajectories and integrates a robust constraint into receding-horizon planning, improving safety in simulations.


<details>
  <summary>Details</summary>
Motivation: To address collision avoidance under uncertainty by leveraging contextual information (ego dynamics) and handling distributional ambiguity in obstacle trajectories.

Method: Embed the conditional distribution of obstacle trajectories given the ego agent's motion into an RKHS via the conditional kernel mean embedding operator. Define an ambiguity set of distributions whose embeddings are within a distance from the empirical conditional mean embedding learned from past data. Formulate a distributionally robust collision-avoidance constraint and incorporate it into a receding-horizon (MPC) motion planning framework for the ego agent.

Result: Simulation experiments show the proposed method achieves better collision avoidance than approaches that ignore contextual information and/or distributional robustness across several challenging scenarios.

Conclusion: The framework effectively integrates context and distributional robustness into a tractable MPC formulation, yielding improved safety in dynamic, uncertain environments; future work may address computational scalability and real-world validation.

Abstract: We present a distributionally robust approach for collision avoidance by
incorporating contextual information. Specifically, we embed the conditional
distribution of future trajectory of the obstacle conditioned on the motion of
the ego agent in a reproducing kernel Hilbert space (RKHS) via the conditional
kernel mean embedding operator. Then, we define an ambiguity set containing all
distributions whose embedding in the RKHS is within a certain distance from the
empirical estimate of conditional mean embedding learnt from past data.
Consequently, a distributionally robust collision avoidance constraint is
formulated, and included in the receding horizon based motion planning
formulation of the ego agent. Simulation results show that the proposed
approach is more successful in avoiding collision compared to approaches that
do not include contextual information and/or distributional robustness in their
formulation in several challenging scenarios.

</details>


### [150] [N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout](https://arxiv.org/abs/2509.18671)
*Kaixin Chai,Hyunjun Lee,Joseph J. Lim*

Main category: cs.RO

TL;DR: N2M is a transition module that guides the robot to a preferable initial pose after reaching the task area, significantly improving manipulation success and showing data-efficient, robust performance across tasks.


<details>
  <summary>Details</summary>
Motivation: There is a misalignment between the navigation objective (reach the task area) and downstream manipulation performance, which depends on the initial pose. A module is needed to select an advantageous pose to improve task success.

Method: N2M uses ego-centric observations, operates in real-time, adapts to environmental changes, provides reliable predictions with high viewpoint robustness, and is broadly applicable across tasks, policies, and hardware. It acts as a transition after reaching the task area to move to a preferable initial pose for manipulation.

Result: In PnPCounterToCab, the average success rate increases from 3% (reachability baseline) to 54%. In Toybox Handover, N2M yields reliable predictions with only 15 data samples, demonstrating data efficiency and generalizability across unseen environments. Demonstrated in both simulation and real-world experiments.

Conclusion: N2M substantially improves downstream manipulation by aligning the navigation phase with a preferable initial pose, offering robustness, data efficiency, and broad applicability across tasks and hardware.

Abstract: In mobile manipulation, the manipulation policy has strong preferences for
initial poses where it is executed. However, the navigation module focuses
solely on reaching the task area, without considering which initial pose is
preferable for downstream manipulation. To address this misalignment, we
introduce N2M, a transition module that guides the robot to a preferable
initial pose after reaching the task area, thereby substantially improving task
success rates. N2M features five key advantages: (1) reliance solely on
ego-centric observation without requiring global or historical information; (2)
real-time adaptation to environmental changes; (3) reliable prediction with
high viewpoint robustness; (4) broad applicability across diverse tasks,
manipulation policies, and robot hardware; and (5) remarkable data efficiency
and generalizability. We demonstrate the effectiveness of N2M through extensive
simulation and real-world experiments. In the PnPCounterToCab task, N2M
improves the averaged success rate from 3% with the reachability-based baseline
to 54%. Furthermore, in the Toybox Handover task, N2M provides reliable
predictions even in unseen environments with only 15 data samples, showing
remarkable data efficiency and generalizability.

</details>


### [151] [3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space](https://arxiv.org/abs/2509.18676)
*Sangjun Noh,Dongwoo Nam,Kangmin Kim,Geonhyup Lee,Yeonguk Yu,Raeyoung Kang,Kyoobin Lee*

Main category: cs.RO

TL;DR: Introduces 3D Flow Diffusion Policy (3D FDP), using scene-level 3D flow as a structured intermediate representation within a diffusion framework to predict local motions and drive visuomotor policies that generalize across diverse objects and dynamics.


<details>
  <summary>Details</summary>
Motivation: Current visuomotor policies rely on global or object-centric features and often miss localized motion cues critical for precise, contact-rich manipulation, limiting generalization across objects and dynamics.

Method: Proposes 3D FDP that uses scene-level 3D flow as an intermediate representation; predicts temporal trajectories of sampled query points and conditions action generation on interaction-aware flows within a unified diffusion model, grounding manipulation in localized dynamics while allowing reasoning about broader scene consequences.

Result: Achieves state-of-the-art performance on MetaWorld across 50 tasks, excelling in medium and hard settings; validated on eight real-robot tasks where it outperforms prior baselines in contact-rich and non-prehensile scenarios.

Conclusion: 3D flow serves as a powerful structural prior for generalizable visuomotor policies, enabling more robust and versatile robotic manipulation; code and demonstrations are available online.

Abstract: Learning robust visuomotor policies that generalize across diverse objects
and interaction dynamics remains a central challenge in robotic manipulation.
Most existing approaches rely on direct observation-to-action mappings or
compress perceptual inputs into global or object-centric features, which often
overlook localized motion cues critical for precise and contact-rich
manipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework
that leverages scene-level 3D flow as a structured intermediate representation
to capture fine-grained local motion cues. Our approach predicts the temporal
trajectories of sampled query points and conditions action generation on these
interaction-aware flows, implemented jointly within a unified diffusion
architecture. This design grounds manipulation in localized dynamics while
enabling the policy to reason about broader scene-level consequences of
actions. Extensive experiments on the MetaWorld benchmark show that 3D FDP
achieves state-of-the-art performance across 50 tasks, particularly excelling
on medium and hard settings. Beyond simulation, we validate our method on eight
real-robot tasks, where it consistently outperforms prior baselines in
contact-rich and non-prehensile scenarios. These results highlight 3D flow as a
powerful structural prior for learning generalizable visuomotor policies,
supporting the development of more robust and versatile robotic manipulation.
Robot demonstrations, additional results, and code can be found at
https://sites.google.com/view/3dfdp/home.

</details>


### [152] [Query-Centric Diffusion Policy for Generalizable Robotic Assembly](https://arxiv.org/abs/2509.18686)
*Ziyi Xu,Haohong Lin,Shiqi Liu,Ding Zhao*

Main category: cs.RO

TL;DR: Proposes Query-centric Diffusion Policy (QDP), a hierarchical, query-guided diffusion framework linking high-level planning with low-level control for robotic assembly, achieving substantial gains in skill precision and long-horizon success, notably >50% improvement in insertion/screwing tasks over non-query baselines on FurnitureBench.


<details>
  <summary>Details</summary>
Motivation: To address the mismatch between high-level skill queries and low-level motor execution in contact-rich assembly, and to handle noise perturbations by leveraging task-relevant queries and perceptual data (point clouds) to improve robustness.

Method: Introduce QDP: a hierarchical policy that uses queries (objects, contact points, skill information) to identify task-relevant components and guide low-level policies. Utilizes diffusion-based policies conditioned on queries and 3D point cloud observations. Evaluations on FurnitureBench in sim and real world.

Result: QDP achieves higher skill precision and long-horizon success; in insertion and screwing tasks, skill-wise success rate improves by over 50% compared with baselines lacking structured queries.

Conclusion: Query-centric guidance can effectively bridge planning and control in assembly robotics, improving robustness to noise and complex interactions; paving the way for more capable generalist robots; future work could explore broader tasks and more sophisticated query formulations.

Abstract: The robotic assembly task poses a key challenge in building generalist robots
due to the intrinsic complexity of part interactions and the sensitivity to
noise perturbations in contact-rich settings. The assembly agent is typically
designed in a hierarchical manner: high-level multi-part reasoning and
low-level precise control. However, implementing such a hierarchical policy is
challenging in practice due to the mismatch between high-level skill queries
and low-level execution. To address this, we propose the Query-centric
Diffusion Policy (QDP), a hierarchical framework that bridges high-level
planning and low-level control by utilizing queries comprising objects, contact
points, and skill information. QDP introduces a query-centric mechanism that
identifies task-relevant components and uses them to guide low-level policies,
leveraging point cloud observations to improve the policy's robustness. We
conduct comprehensive experiments on the FurnitureBench in both simulation and
real-world settings, demonstrating improved performance in skill precision and
long-horizon success rate. In the challenging insertion and screwing tasks, QDP
improves the skill-wise success rate by over 50% compared to baselines without
structured queries.

</details>


### [153] [Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation](https://arxiv.org/abs/2509.18734)
*Nishant Doshi,Amey Sutvani,Sanket Gujar*

Main category: cs.RO

TL;DR: Reinforcement learning for a depth-camera-equipped virtual quadcopter to navigate urban environments and avoid collisions in simulation.


<details>
  <summary>Details</summary>
Motivation: Urban UAV navigation is challenged by GPS degradation, narrow spaces, and dynamic obstacles, necessitating onboard perception-based collision avoidance.

Method: Train a virtual quadcopter agent with a depth camera using reinforcement learning to learn collision-avoiding navigation in a simulated urban environment.

Result: Not specified in the abstract.

Conclusion: Not specified in the abstract.

Abstract: One of the challenges faced by Autonomous Aerial Vehicles is reliable
navigation through urban environments. Factors like reduction in precision of
Global Positioning System (GPS), narrow spaces and dynamically moving obstacles
make the path planning of an aerial robot a complicated task. One of the skills
required for the agent to effectively navigate through such an environment is
to develop an ability to avoid collisions using information from onboard depth
sensors. In this paper, we propose Reinforcement Learning of a virtual
quadcopter robot agent equipped with a Depth Camera to navigate through a
simulated urban environment.

</details>


### [154] [MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning](https://arxiv.org/abs/2509.18757)
*Omar Rayyan,John Abanes,Mahmoud Hafez,Anthony Tzes,Fares Abu-Dakka*

Main category: cs.RO

TL;DR: MV-UMI integrates third-person and egocentric views for handheld manipulation data to reduce domain shift; achieves ~47% gains on 3 tasks with ablation study.


<details>
  <summary>Details</summary>
Motivation: Imitation learning needs diverse, high-quality datasets; handheld grippers with wrist cameras lack broad scene context and suffer cross-embodiment domain shift; multi-view integration addresses this gap.

Method: Introduce MV-UMI framework combining third-person perspective with egocentric wrist camera; fusion mitigates scene-context limitations and preserves cross-embodiment advantages; includes ablation studies.

Result: Shows ~47% improvement in sub-tasks requiring broad scene understanding across 3 tasks; results support effectiveness of multi-view integration in handheld data collection.

Conclusion: MV-UMI broadens feasible manipulation tasks learnable from handheld systems while maintaining cross-embodiment benefits; demonstrates practical benefits for data collection in imitation learning.

Abstract: Recent advances in imitation learning have shown great promise for developing
robust robot manipulation policies from demonstrations. However, this promise
is contingent on the availability of diverse, high-quality datasets, which are
not only challenging and costly to collect but are often constrained to a
specific robot embodiment. Portable handheld grippers have recently emerged as
intuitive and scalable alternatives to traditional robotic teleoperation
methods for data collection. However, their reliance solely on first-person
view wrist-mounted cameras often creates limitations in capturing sufficient
scene contexts. In this paper, we present MV-UMI (Multi-View Universal
Manipulation Interface), a framework that integrates a third-person perspective
with the egocentric camera to overcome this limitation. This integration
mitigates domain shifts between human demonstration and robot deployment,
preserving the cross-embodiment advantages of handheld data-collection devices.
Our experimental results, including an ablation study, demonstrate that our
MV-UMI framework improves performance in sub-tasks requiring broad scene
understanding by approximately 47% across 3 tasks, confirming the effectiveness
of our approach in expanding the range of feasible manipulation tasks that can
be learned using handheld gripper systems, without compromising the
cross-embodiment advantages inherent to such systems.

</details>


### [155] [VGGT-DP: Generalizable Robot Control via Vision Foundation Models](https://arxiv.org/abs/2509.18778)
*Shijia Ge,Yinxin Zhang,Shuzhao Xie,Weixiang Zhang,Mingcai Zhou,Zhi Wang*

Main category: cs.RO

TL;DR: VGGT-DP is a visuomotor policy that fuses geometric priors from a pretrained 3D perception model with proprioceptive feedback, using frame-wise token reuse and random token pruning to achieve robust, low-latency control and outperform DP/DP3 on MetaWorld tasks.


<details>
  <summary>Details</summary>
Motivation: Current visual encoders often ignore structured geometry and proprioceptive cues, hindering spatial grounding and generalization. Incorporating biologically inspired proprioception and geometric priors can improve robustness and precision in manipulation tasks.

Method: Employ Visual Geometry Grounded Transformer (VGGT) as the visual encoder, add a proprioception-guided visual learning strategy to align perception with the robot's internal state, implement a frame-wise token reuse mechanism to compact multi-view tokens into an efficient spatial representation, and apply random token pruning to boost robustness and reduce overfitting.

Result: VGGT-DP significantly outperforms strong baselines such as DP and DP3, especially in precision-critical and long-horizon tasks, as demonstrated on challenging MetaWorld benchmarks.

Conclusion: Integrating geometric priors with proprioceptive feedback, along with efficiency techniques (token reuse and pruning), enhances spatial grounding and closed-loop control in visuomotor policies and yields strong generalization in complex manipulation tasks.

Abstract: Visual imitation learning frameworks allow robots to learn manipulation
skills from expert demonstrations. While existing approaches mainly focus on
policy design, they often neglect the structure and capacity of visual
encoders, limiting spatial understanding and generalization. Inspired by
biological vision systems, which rely on both visual and proprioceptive cues
for robust control, we propose VGGT-DP, a visuomotor policy framework that
integrates geometric priors from a pretrained 3D perception model with
proprioceptive feedback. We adopt the Visual Geometry Grounded Transformer
(VGGT) as the visual encoder and introduce a proprioception-guided visual
learning strategy to align perception with internal robot states, improving
spatial grounding and closed-loop control. To reduce inference latency, we
design a frame-wise token reuse mechanism that compacts multi-view tokens into
an efficient spatial representation. We further apply random token pruning to
enhance policy robustness and reduce overfitting. Experiments on challenging
MetaWorld tasks show that VGGT-DP significantly outperforms strong baselines
such as DP and DP3, particularly in precision-critical and long-horizon
scenarios.

</details>


### [156] [Human-Interpretable Uncertainty Explanations for Point Cloud Registration](https://arxiv.org/abs/2509.18786)
*Johannes A. Gaus,Loris Schneider,Yitian Shi,Jongseok Lee,Rania Rayyes,Rudolph Triebel*

Main category: cs.RO

TL;DR: Gaussian Process Concept Attribution (GP-CA) quantifies and explains registration uncertainty in point cloud alignment, using active learning to discover unknown error sources, achieving faster, more accurate, and robust registration with real-world robot demonstrations and failure-recovery behavior.


<details>
  <summary>Details</summary>
Motivation: Registration under uncertainty from sensor noise, pose-estimation errors, and partial overlap due to occlusion; ICP and similar methods can fail. There is a need to quantify uncertainty and attribute it to known error sources, as well as to discover new sources in the wild.

Method: Introduce Gaussian Process Concept Attribution (GP-CA) to model and attribute registration uncertainty to error sources. Leverage active learning to query informative instances and discover previously unknown uncertainty sources. Validate on three public datasets and a real-world robot experiment, with extensive ablations.

Result: GP-CA outperforms state-of-the-art methods in runtime, achieves high sample-efficiency through active learning, and attains higher accuracy. Validations across public datasets and real robot experiments demonstrate applicability, and ablations support design choices. Demonstrates useful failure-recovery behaviors in robotic perception.

Conclusion: GP-CA provides a practical, interpretable framework for robust point cloud registration under uncertainty, with active learning enabling discovery of new error sources and enabling failure-resilient robotic perception.

Abstract: In this paper, we address the point cloud registration problem, where
well-known methods like ICP fail under uncertainty arising from sensor noise,
pose-estimation errors, and partial overlap due to occlusion. We develop a
novel approach, Gaussian Process Concept Attribution (GP-CA), which not only
quantifies registration uncertainty but also explains it by attributing
uncertainty to well-known sources of errors in registration problems. Our
approach leverages active learning to discover new uncertainty sources in the
wild by querying informative instances. We validate GP-CA on three publicly
available datasets and in our real-world robot experiment. Extensive ablations
substantiate our design choices. Our approach outperforms other
state-of-the-art methods in terms of runtime, high sample-efficiency with
active learning, and high accuracy. Our real-world experiment clearly
demonstrates its applicability. Our video also demonstrates that GP-CA enables
effective failure-recovery behaviors, yielding more robust robotic perception.

</details>


### [157] [Application Management in C-ITS: Orchestrating Demand-Driven Deployments and Reconfigurations](https://arxiv.org/abs/2509.18793)
*Lukas Zanger,Bastian Lampe,Lennart Reiher,Lutz Eckstein*

Main category: cs.RO

TL;DR: A demand-driven, Kubernetes-based management framework for C-ITS that automates deployment, reconfiguration, updates, upgrades, and scaling of microservices on demand, integrating ROS 2 to handle dynamic demands and improve resource efficiency; demonstrated on collective environment perception with open-source code.


<details>
  <summary>Details</summary>
Motivation: In large-scale, dynamic C-ITS, efficient resource utilization and adaptable service orchestration are crucial as vehicles become more automated and rely on offboard services. Conventional static deployments struggle to cope with changing demands from multiple entities.

Method: Proposes a demand-driven application management framework built on Kubernetes and ROS 2. It reconciles evolving demands from various C-ITS actors and automates lifecycle operations (deploy, reconfigure, update, upgrade, scale) of microservices and external services (e.g., ecosystem models) on demand.

Result: Demonstrates the framework in a C-ITS use case of collective environment perception, showing feasibility of on-demand management and potential resource and network traffic reductions; source code is publicly available.

Conclusion: The approach addresses dynamic demand and resource challenges in large-scale C-ITS by enabling automated, on-demand management of microservices via Kubernetes and ROS 2, with practical demonstration and open-source implementation.

Abstract: Vehicles are becoming increasingly automated and interconnected, enabling the
formation of cooperative intelligent transport systems (C-ITS) and the use of
offboard services. As a result, cloud-native techniques, such as microservices
and container orchestration, play an increasingly important role in their
operation. However, orchestrating applications in a large-scale C-ITS poses
unique challenges due to the dynamic nature of the environment and the need for
efficient resource utilization. In this paper, we present a demand-driven
application management approach that leverages cloud-native techniques -
specifically Kubernetes - to address these challenges. Taking into account the
demands originating from different entities within the C-ITS, the approach
enables the automation of processes, such as deployment, reconfiguration,
update, upgrade, and scaling of microservices. Executing these processes on
demand can, for example, reduce computing resource consumption and network
traffic. A demand may include a request for provisioning an external supporting
service, such as a collective environment model. The approach handles changing
and new demands by dynamically reconciling them through our proposed
application management framework built on Kubernetes and the Robot Operating
System (ROS 2). We demonstrate the operation of our framework in the C-ITS use
case of collective environment perception and make the source code of the
prototypical framework publicly available at
https://github.com/ika-rwth-aachen/application_manager .

</details>


### [158] [DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation](https://arxiv.org/abs/2509.18830)
*Suzannah Wistreich,Baiyu Shi,Stephen Tian,Samuel Clarke,Michael Nath,Chengyi Xu,Zhenan Bao,Jiajun Wu*

Main category: cs.RO

TL;DR: DexSkin is a soft capacitive electronic skin for robots that provides localized, calibratable tactile sensing and enables learning-based manipulation with transfer across sensor instances.


<details>
  <summary>Details</summary>
Motivation: Human skin offers rich tactile sensing; mimicking this for dexterous robotic manipulation is challenging, requiring sensitive, conformable, calibratable sensing across complex geometries.

Method: Introduce DexSkin, a soft conformable capacitive e-skin; sensorize a pair of parallel gripper fingers to cover nearly all finger surfaces; calibrate for model transfer; evaluate on tasks via learning-from-demonstration and online RL; demonstrate transfer across sensors.

Result: DexSkin delivers sensitive, localized sensing across contoured surfaces, enables learning-based manipulation, enables transfer across sensor instances, and is applicable to online RL on real robots.

Conclusion: DexSkin is practical for real-world, contact-rich manipulation, with calibratability enabling cross-sensor transfer and learning-based control.

Abstract: Human skin provides a rich tactile sensing stream, localizing intentional and
unintentional contact events over a large and contoured region. Replicating
these tactile sensing capabilities for dexterous robotic manipulation systems
remains a longstanding challenge. In this work, we take a step towards this
goal by introducing DexSkin. DexSkin is a soft, conformable capacitive
electronic skin that enables sensitive, localized, and calibratable tactile
sensing, and can be tailored to varying geometries. We demonstrate its efficacy
for learning downstream robotic manipulation by sensorizing a pair of parallel
jaw gripper fingers, providing tactile coverage across almost the entire finger
surfaces. We empirically evaluate DexSkin's capabilities in learning
challenging manipulation tasks that require sensing coverage across the entire
surface of the fingers, such as reorienting objects in hand and wrapping
elastic bands around boxes, in a learning-from-demonstration framework. We then
show that, critically for data-driven approaches, DexSkin can be calibrated to
enable model transfer across sensor instances, and demonstrate its
applicability to online reinforcement learning on real robots. Our results
highlight DexSkin's suitability and practicality for learning real-world,
contact-rich manipulation. Please see our project webpage for videos and
visualizations: https://dex-skin.github.io/.

</details>


### [159] [Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation](https://arxiv.org/abs/2509.18865)
*Masato Kobayashi,Thanpimon Buamanee*

Main category: cs.RO

TL;DR: Bi-VLA extends bilateral imitation learning to multi-task scenarios by fusing vision and language. It uses leader-follower bilateral control data, visual features, and natural language instructions via SigLIP and FiLM-based fusion, and shows improved real-robot task performance over traditional bilateral imitation learning.


<details>
  <summary>Details</summary>
Motivation: Conventional bilateral control methods rely on task-specific models and single-task setups, limiting generality and scalability. There is a need for a unified framework that can handle multiple tasks by leveraging both visual information and natural language instructions.

Method: Utilizes leader-follower bilateral control data (joint angle, velocity, torque) augmented with visual features and natural language inputs. Integrates vision-language signals using SigLIP and FiLM-based fusion to generate actions. Tested on two task types: one requiring language cues and another differentiable by vision, with real-robot experiments.

Result: Bi-VLA successfully interprets vision-language cues and improves task success rates compared to conventional bilateral control-based imitation learning, demonstrating multi-task capability and enhanced versatility through vision-language fusion.

Conclusion: Bi-VLA addresses the single-task limitation of prior bilateral approaches and provides empirical evidence that combining vision and language enhances versatility in imitation learning. Real-world validation supports its effectiveness; further materials are available online.

Abstract: We propose Bilateral Control-Based Imitation Learning via Vision-Language
Fusion for Action Generation (Bi-VLA), a novel framework that extends bilateral
control-based imitation learning to handle more than one task within a single
model. Conventional bilateral control methods exploit joint angle, velocity,
torque, and vision for precise manipulation but require task-specific models,
limiting their generality. Bi-VLA overcomes this limitation by utilizing robot
joint angle, velocity, and torque data from leader-follower bilateral control
with visual features and natural language instructions through SigLIP and
FiLM-based fusion. We validated Bi-VLA on two task types: one requiring
supplementary language cues and another distinguishable solely by vision.
Real-robot experiments showed that Bi-VLA successfully interprets
vision-language combinations and improves task success rates compared to
conventional bilateral control-based imitation learning. Our Bi-VLA addresses
the single-task limitation of prior bilateral approaches and provides empirical
evidence that combining vision and language significantly enhances versatility.
Experimental results validate the effectiveness of Bi-VLA in real-world tasks.
For additional material, please visit the website:
https://mertcookimg.github.io/bi-vla/

</details>


### [160] [Lang2Morph: Language-Driven Morphological Design of Robotic Hands](https://arxiv.org/abs/2509.18937)
*Yanyuan Qiao,Kieran Gilday,Yutong Xie,Josie Hughes*

Main category: cs.RO

TL;DR: An LLM-driven pipeline (Lang2Morph) translates natural-language task descriptions into symbolic, OPH-compatible, 3D-printable robotic hand morphologies through Morphology Design and Selection/Refinement.


<details>
  <summary>Details</summary>
Motivation: Automated design of dexterous robotic hands remains compute-intensive, heuristic-driven, and often neglects dexterous, task-specific morphology. LLMs can leverage broad knowledge of human-object interactions for zero-shot design reasoning.

Method: Two-stage pipeline: Morphology Design maps tasks to semantic tags, structural grammars, and OPH-compatible parameters; Selection and Refinement evaluates candidates for semantic alignment and size compatibility, with optional LLM-guided refinement. Outputs task-specific, 3D-printable morphologies.

Result: Evaluated across varied tasks; Lang2Morph produced diverse, task-relevant morphologies, demonstrating feasibility of an LLM-based framework for task-conditioned robotic hand design.

Conclusion: LLMs offer zero-shot design reasoning to automate hand morphology generation; Lang2Morph is a promising first framework for task-conditioned robotic hand design, with potential for broader adoption and further benchmarking.

Abstract: Designing robotic hand morphologies for diverse manipulation tasks requires
balancing dexterity, manufacturability, and task-specific functionality. While
open-source frameworks and parametric tools support reproducible design, they
still rely on expert heuristics and manual tuning. Automated methods using
optimization are often compute-intensive, simulation-dependent, and rarely
target dexterous hands. Large language models (LLMs), with their broad
knowledge of human-object interactions and strong generative capabilities,
offer a promising alternative for zero-shot design reasoning. In this paper, we
present Lang2Morph, a language-driven pipeline for robotic hand design. It uses
LLMs to translate natural-language task descriptions into symbolic structures
and OPH-compatible parameters, enabling 3D-printable task-specific
morphologies. The pipeline consists of: (i) Morphology Design, which maps tasks
into semantic tags, structural grammars, and OPH-compatible parameters; and
(ii) Selection and Refinement, which evaluates design candidates based on
semantic alignment and size compatibility, and optionally applies LLM-guided
refinement when needed. We evaluate Lang2Morph across varied tasks, and results
show that our approach can generate diverse, task-relevant morphologies. To our
knowledge, this is the first attempt to develop an LLM-based framework for
task-conditioned robotic hand design.

</details>


### [161] [Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations](https://arxiv.org/abs/2509.18953)
*Hanqing Liu,Jiahuan Long,Junqi Wu,Jiacheng Hou,Huili Tang,Tingsong Jiang,Weien Zhou,Wen Yao*

Main category: cs.RO

TL;DR: Eva-VLA is a unified framework that systematically evaluates the robustness of Vision-Language-Action models by turning discrete physical variations into a continuous optimization problem; it decomposes variations into object 3D transformations, illumination, and adversarial patches, uses continuous black-box optimization to find worst-case scenarios, and reveals high fragility in OpenVLA models, highlighting gaps between lab success and real-world deployment.


<details>
  <summary>Details</summary>
Motivation: VLA models show promising capabilities but their robustness to real-world physical variations is underexplored, risking deployment in unpredictable environments; there is a need for reproducible characterization of variations and efficient discovery of worst-case scenarios without excessive data collection.

Method: Decompose variations into three domains (3D object transformations, illumination, adversarial patches); formulate a continuous black-box optimization framework that maps discrete variations to optimization parameters to search for worst-case conditions; apply Eva-VLA to OpenVLA models across multiple benchmarks.

Result: All variation types yield failure rates >60%, with object transformations causing up to 97.8% failure in long-horizon tasks across benchmarks; demonstrates significant vulnerability of VLA models; Eva-VLA provides a practical path to systematically assess and harden models.

Conclusion: Eva-VLA offers a practical, unified pathway to evaluate and improve robustness of VLA-based robotic manipulation, uncovering gaps between controlled success and real-world deployment and enabling targeted hardening against real-world variations.

Abstract: Vision-Language-Action (VLA) models have emerged as promising solutions for
robotic manipulation, yet their robustness to real-world physical variations
remains critically underexplored. To bridge this gap, we propose Eva-VLA, the
first unified framework that systematically evaluates the robustness of VLA
models by transforming discrete physical variations into continuous
optimization problems. However, comprehensively assessing VLA robustness
presents two key challenges: (1) how to systematically characterize diverse
physical variations encountered in real-world deployments while maintaining
evaluation reproducibility, and (2) how to discover worst-case scenarios
without prohibitive real-world data collection costs efficiently. To address
the first challenge, we decompose real-world variations into three critical
domains: object 3D transformations that affect spatial reasoning, illumination
variations that challenge visual perception, and adversarial patches that
disrupt scene understanding. For the second challenge, we introduce a
continuous black-box optimization framework that transforms discrete physical
variations into parameter optimization, enabling systematic exploration of
worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models
across multiple benchmarks reveal alarming vulnerabilities: all variation types
trigger failure rates exceeding 60%, with object transformations causing up to
97.8% failure in long-horizon tasks. Our findings expose critical gaps between
controlled laboratory success and unpredictable deployment readiness, while the
Eva-VLA framework provides a practical pathway for hardening VLA-based robotic
manipulation models against real-world deployment challenges.

</details>


### [162] [Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation](https://arxiv.org/abs/2509.18954)
*Minoo Dolatabadi,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi*

Main category: cs.RO

TL;DR: A data-driven framework predicts the 6-DoF ICP registration error covariance prior to matching, enabling Kalman-filter‑based LiDAR localization/SLAM without relying on a reference map, and improving accuracy and robustness on KITTI.


<details>
  <summary>Details</summary>
Motivation: ICP-based localization often fails in featureless or dynamic environments due to unmodeled uncertainty. Existing deep-learning localizability methods either depend on a map or yield binary localizability, which limits uncertainty modeling.

Method: A deep learning–based framework that estimates the ICP registration error covariance (6-DoF) for each LiDAR scan before matching. It operates without a reference map and integrates the predicted covariance into a Kalman filter to enhance localization and SLAM.

Result: Experiments on the KITTI dataset show that the method accurately predicts the registration covariance and, when used for localization with a pre-built map or SLAM, reduces localization errors and improves robustness.

Conclusion: This work demonstrates the feasibility and benefits of predicting ICP uncertainty to improve LiDAR-based localization/SLAM, enabling more reliable state estimation without a map and seamless Kalman-filter integration.

Abstract: LiDAR-based localization and SLAM often rely on iterative matching
algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align
sensor data with pre-existing maps or previous scans. However, ICP is prone to
errors in featureless environments and dynamic scenes, leading to inaccurate
pose estimation. Accurately predicting the uncertainty associated with ICP is
crucial for robust state estimation but remains challenging, as existing
approaches often rely on handcrafted models or simplified assumptions.
Moreover, a few deep learning-based methods for localizability estimation
either depend on a pre-built map, which may not always be available, or provide
a binary classification of localizable versus non-localizable, which fails to
properly model uncertainty. In this work, we propose a data-driven framework
that leverages deep learning to estimate the registration error covariance of
ICP before matching, even in the absence of a reference map. By associating
each LiDAR scan with a reliable 6-DoF error covariance estimate, our method
enables seamless integration of ICP within Kalman filtering, enhancing
localization accuracy and robustness. Extensive experiments on the KITTI
dataset demonstrate the effectiveness of our approach, showing that it
accurately predicts covariance and, when applied to localization using a
pre-built map or SLAM, reduces localization errors and improves robustness.

</details>


### [163] [Category-Level Object Shape and Pose Estimation in Less Than a Millisecond](https://arxiv.org/abs/2509.18979)
*Lorenzo Shaikewitz,Tim Nguyen,Luca Carlone*

Main category: cs.RO

TL;DR: A fast, certifiably globally optimal category-level shape and pose estimator from RGB-D that uses sparse learned keypoints and a linear active shape model; achieves ~100 μs per iteration and provides an outlier rejection-friendly pipeline; code released.


<details>
  <summary>Details</summary>
Motivation: Need robust, fast category-level pose estimation with minimal priors for robotics tasks.

Method: Detect sparse category-level keypoints with a learned front-end; model the object's unknown shape via a linear active shape model; pose and shape are estimated jointly by a MAP optimization over position, orientation (unit quaternions), and shape; expressed as an eigenvalue problem with eigenvector nonlinearities; solved efficiently using self-consistent field iteration; a Lagrange multiplier step provides a global optimality certificate.

Result: One iteration takes ~100 microseconds; demonstrates fast outlier rejection and performance on synthetic data, two public datasets, and a drone tracking scenario; code available at the provided GitHub link.

Conclusion: The approach delivers a fast, certifiably optimal solution for category-level shape and pose estimation with simple priors, suitable for real-time robotics tasks and robust to outliers.

Abstract: Object shape and pose estimation is a foundational robotics problem,
supporting tasks from manipulation to scene understanding and navigation. We
present a fast local solver for shape and pose estimation which requires only
category-level object priors and admits an efficient certificate of global
optimality. Given an RGB-D image of an object, we use a learned front-end to
detect sparse, category-level semantic keypoints on the target object. We
represent the target object's unknown shape using a linear active shape model
and pose a maximum a posteriori optimization problem to solve for position,
orientation, and shape simultaneously. Expressed in unit quaternions, this
problem admits first-order optimality conditions in the form of an eigenvalue
problem with eigenvector nonlinearities. Our primary contribution is to solve
this problem efficiently with self-consistent field iteration, which only
requires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector
pair at each iterate. Solving a linear system for the corresponding Lagrange
multipliers gives a simple global optimality certificate. One iteration of our
solver runs in about 100 microseconds, enabling fast outlier rejection. We test
our method on synthetic data and a variety of real-world settings, including
two public datasets and a drone tracking scenario. Code is released at
https://github.com/MIT-SPARK/Fast-ShapeAndPose.

</details>


### [164] [Pure Vision Language Action (VLA) Models: A Comprehensive Survey](https://arxiv.org/abs/2509.19012)
*Dapeng Zhang,Jin Sun,Chenghui Hu,Xiaoyan Wu,Zhenlong Yuan,Rui Zhou,Fei Shen,Qingguo Zhou*

Main category: cs.RO

TL;DR: A comprehensive survey of Vision Language Action (VLA) models, offering taxonomy, datasets, benchmarks, and future directions across multiple paradigms.


<details>
  <summary>Details</summary>
Motivation: VLA enables active robotic manipulation and decision-making, requiring a unified mapping of methods, datasets, and platforms to guide research.

Method: Systematic literature review of 300+ studies; classify VLA approaches into autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized; analyze motivations, strategies, implementations; summarize datasets, benchmarks, simulation platforms; synthesize key challenges and future directions.

Result: Provides a structured taxonomy and cross-paradigm analysis; maps applications, datasets, and platforms; identifies opportunities, challenges, and gaps for scalable, general-purpose VLA methods.

Conclusion: Field is rapidly evolving toward scalable, general-purpose VLA agents; future work should tackle evaluation, reliability, real-time decision-making, and integration with robotics platforms; the survey offers a consolidated view of state-of-the-art and directions.

Abstract: The emergence of Vision Language Action (VLA) models marks a paradigm shift
from traditional policy-based control to generalized robotics, reframing Vision
Language Models (VLMs) from passive sequence generators into active agents for
manipulation and decision-making in complex, dynamic environments. This survey
delves into advanced VLA methods, aiming to provide a clear taxonomy and a
systematic, comprehensive review of existing research. It presents a
comprehensive analysis of VLA applications across different scenarios and
classifies VLA approaches into several paradigms: autoregression-based,
diffusion-based, reinforcement-based, hybrid, and specialized methods; while
examining their motivations, core strategies, and implementations in detail. In
addition, foundational datasets, benchmarks, and simulation platforms are
introduced. Building on the current VLA landscape, the review further proposes
perspectives on key challenges and future directions to advance research in VLA
models and generalizable robotics. By synthesizing insights from over three
hundred recent studies, this survey maps the contours of this rapidly evolving
field and highlights the opportunities and challenges that will shape the
development of scalable, general-purpose VLA methods.

</details>


### [165] [Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion](https://arxiv.org/abs/2509.19023)
*Shuai Liu,Meng Cheng Lau*

Main category: cs.RO

TL;DR: ROM-GRL is a two-stage RL framework that uses a compact 4-DOF reduced-order model (ROM) to generate gait templates via PPO, then guides a full-body policy with SAC plus an adversarial discriminator to match the ROM’s gait distribution. It achieves stable, symmetric gaits at multiple speeds without human demonstrations or extensive reward shaping.


<details>
  <summary>Details</summary>
Motivation: Eliminate dependence on motion capture data and complex reward shaping for humanoid locomotion, and bridge reward-only RL with imitation-based approaches by distilling ROM guidance into high-dimensional policies.

Method: Stage 1: train a 4-DOF ROM with Proximal Policy Optimization to produce energy-efficient gait templates. Stage 2: train a full-body policy with Soft Actor-Critic augmented by an adversarial discriminator to align the five-dimensional gait feature distribution of the learned policy with the ROM demonstrations, using the ROM trajectories as guidance.

Result: ROM-GRL yielded stable, symmetric gaits with substantially lower tracking error than a pure-reward baseline at speeds of 1 m/s and 4 m/s, demonstrating effective distillation of lightweight ROM guidance into high-dimensional policies.

Conclusion: Distilling lightweight ROM guidance into high-dimensional policies enables versatile, naturalistic humanoid behaviors without human demonstrations, effectively bridging reward-only and imitation-based locomotion methods.

Abstract: We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a
two-stage reinforcement learning framework for humanoid walking that requires
no motion capture data or elaborate reward shaping. In the first stage, a
compact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via
Proximal Policy Optimization. This generates energy-efficient gait templates.
In the second stage, those dynamically consistent trajectories guide a
full-body policy trained with Soft Actor--Critic augmented by an adversarial
discriminator, ensuring the student's five-dimensional gait feature
distribution matches the ROM's demonstrations. Experiments at 1
meter-per-second and 4 meter-per-second show that ROM-GRL produces stable,
symmetric gaits with substantially lower tracking error than a pure-reward
baseline. By distilling lightweight ROM guidance into high-dimensional
policies, ROM-GRL bridges the gap between reward-only and imitation-based
locomotion methods, enabling versatile, naturalistic humanoid behaviors without
any human demonstrations.

</details>


### [166] [TacEva: A Performance Evaluation Framework For Vision-Based Tactile Sensors](https://arxiv.org/abs/2509.19037)
*Qingzheng Cong,Steven Oh,Wen Fan,Shan Luo,Kaspar Althoefer,Dandan Zhang*

Main category: cs.RO

TL;DR: TacEva is a standardized evaluation framework for Vision-Based Tactile Sensors (VBTSs) that defines metrics and a repeatable experimental pipeline to compare designs, enabling task-specific selection and performance-guided optimization.


<details>
  <summary>Details</summary>
Motivation: VBTSs vary in sensing mechanisms, size, and parameters, leading to large performance disparities and a lack of standardized metrics that hinder design choices and fine-tuning.

Method: Define a set of performance metrics capturing typical application characteristics, and design structured experimental pipelines for consistent, repeatable quantification. Apply the framework to multiple VBTS implementations with different sensing mechanisms.

Result: TacEva enables thorough, metric-driven evaluation across VBTS designs, providing quantitative indicators for each performance dimension and enabling task-based pre-selection and optimization guidance. The framework web resource lists existing VBTS evaluation methods and additional evaluations.

Conclusion: TacEva provides a comprehensive, standardized evaluation framework that facilitates fair comparison and optimization of VBTS designs across tasks, supporting reproducible experimentation and informed design decisions.

Abstract: Vision-Based Tactile Sensors (VBTSs) are widely used in robotic tasks because
of the high spatial resolution they offer and their relatively low
manufacturing costs. However, variations in their sensing mechanisms,
structural dimension, and other parameters lead to significant performance
disparities between existing VBTSs. This makes it challenging to optimize them
for specific tasks, as both the initial choice and subsequent fine-tuning are
hindered by the lack of standardized metrics. To address this issue, TacEva is
introduced as a comprehensive evaluation framework for the quantitative
analysis of VBTS performance. The framework defines a set of performance
metrics that capture key characteristics in typical application scenarios. For
each metric, a structured experimental pipeline is designed to ensure
consistent and repeatable quantification. The framework is applied to multiple
VBTSs with distinct sensing mechanisms, and the results demonstrate its ability
to provide a thorough evaluation of each design and quantitative indicators for
each performance dimension. This enables researchers to pre-select the most
appropriate VBTS on a task by task basis, while also offering
performance-guided insights into the optimization of VBTS design. A list of
existing VBTS evaluation methods and additional evaluations can be found on our
website: https://stevenoh2003.github.io/TacEva/

</details>


### [167] [ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation](https://arxiv.org/abs/2509.19047)
*Geonhyup Lee,Yeongjin Lee,Kangmin Kim,Seongju Lee,Sangjun Noh,Seunghyeok Back,Kyoobin Lee*

Main category: cs.RO

TL;DR: ManipForce collects high-frequency force-torque and RGB demonstrations with a handheld device and uses a Frequency-Aware Multimodal Transformer (FMT) to fuse asynchronous RGB and F/T data for diffusion-based policies, achieving strong performance on contact-rich tasks.


<details>
  <summary>Details</summary>
Motivation: Imitation learning for contact-rich manipulation commonly relies on vision-only data, but precise interaction forces and high-frequency signals are crucial for stable contact and high-precision assembly tasks.

Method: A handheld device (ManipForce) captures high-frequency F/T and RGB data during natural demonstrations. The Frequency-Aware Multimodal Transformer (FMT) encodes asynchronous RGB and F/T signals using frequency- and modality-aware embeddings and fuses them via bi-directional cross-attention within a transformer diffusion policy.

Result: Experiments on six real-world contact-rich tasks (e.g., gear assembly, box flipping, battery insertion) show an average success rate of 83% across tasks, outperforming RGB-only baselines. Ablation and frequency analyses indicate benefits from high-frequency F/T data and cross-modal integration, particularly for high-precision, stable-contact tasks.

Conclusion: Incorporating high-frequency force-torque data and multimodal fusion via cross-attention in a diffusion-policy framework substantially improves imitation learning for contact-rich manipulation, enabling robust, precise manipulation in real-world tasks.

Abstract: Contact-rich manipulation tasks such as precision assembly require precise
control of interaction forces, yet existing imitation learning methods rely
mainly on vision-only demonstrations. We propose ManipForce, a handheld system
designed to capture high-frequency force-torque (F/T) and RGB data during
natural human demonstrations for contact-rich manipulation. Building on these
demonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT).
FMT encodes asynchronous RGB and F/T signals using frequency- and
modality-aware embeddings and fuses them via bi-directional cross-attention
within a transformer diffusion policy. Through extensive experiments on six
real-world contact-rich manipulation tasks - such as gear assembly, box
flipping, and battery insertion - FMT trained on ManipForce demonstrations
achieves robust performance with an average success rate of 83% across all
tasks, substantially outperforming RGB-only baselines. Ablation and
sampling-frequency analyses further confirm that incorporating high-frequency
F/T data and cross-modal integration improves policy performance, especially in
tasks demanding high precision and stable contact.

</details>


### [168] [SlicerROS2: A Research and Development Module for Image-Guided Robotic Interventions](https://arxiv.org/abs/2509.19076)
*Laura Connolly,Aravind S. Kumar,Kapi Ketan Mehta,Lidia Al-Zogbi,Peter Kazanzides,Parvin Mousavi,Gabor Fichtinger,Axel Krieger,Junichi Tokuda,Russell H. Taylor,Simon Leonard,Anton Deguet*

Main category: cs.RO

TL;DR: Redesigned SlicerROS2 enables modular, Python-accessible, real-time imaging-guided robotics with four demonstration applications.


<details>
  <summary>Details</summary>
Motivation: To provide a standard, modular integration framework for medical robotics research by combining 3D Slicer and ROS, improving access to low-level features, Python APIs, and data transfer.

Method: Rebuild the SlicerROS2 module for modularity, expose low-level and Python APIs, enhance data transfer, and validate via four realistic image-guided robotics applications.

Result: A new SlicerROS2 design with improved modularity, feature access, Python integration, and data transfer; real-time robot loading/visualization demonstrated; four applications illustrate core capabilities.

Conclusion: The redesigned SlicerROS2 offers a versatile platform for image-guided robotics research, enabling broader experimentation and integration with medical imaging workflows.

Abstract: Image-guided robotic interventions involve the use of medical imaging in
tandem with robotics. SlicerROS2 is a software module that combines 3D Slicer
and robot operating system (ROS) in pursuit of a standard integration approach
for medical robotics research. The first release of SlicerROS2 demonstrated the
feasibility of using the C++ API from 3D Slicer and ROS to load and visualize
robots in real time. Since this initial release, we've rewritten and redesigned
the module to offer greater modularity, access to low-level features, access to
3D Slicer's Python API, and better data transfer protocols. In this paper, we
introduce this new design as well as four applications that leverage the core
functionalities of SlicerROS2 in realistic image-guided robotics scenarios.

</details>


### [169] [World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation](https://arxiv.org/abs/2509.19080)
*Zhennan Jiang,Kai Liu,Yuxin Qin,Shuai Tian,Yupeng Zheng,Mingcai Zhou,Chao Yu,Haoran Li,Dongbin Zhao*

Main category: cs.RO

TL;DR: Diffusion-based world models act as high-fidelity simulators to refine pre-trained policies entirely in imagined environments for robotic manipulation, enabling end-to-end policy optimization with fewer real-world interactions.


<details>
  <summary>Details</summary>
Motivation: Mitigate data scarcity in imitation learning, reduce cost and risk of real-world RL, and address the sim-to-real gap by using realistic diffusion-generated dynamics for policy refinement.

Method: Pre-train a diffusion world model on diverse, multi-task robotic dynamics; freeze the world model; refine policies entirely inside the imagined environment; use a two-hot action encoding for manipulation and diffusion backbones to improve fidelity.

Result: In simulation and real-world experiments, World4RL achieves high-fidelity environment modeling and consistent policy refinement, with significantly higher success rates than imitation learning and baseline methods.

Conclusion: World4RL enables end-to-end policy optimization within a diffusion-based world model, reducing or eliminating real-world interactions while delivering more robust manipulation policies.

Abstract: Robotic manipulation policies are commonly initialized through imitation
learning, but their performance is limited by the scarcity and narrow coverage
of expert data. Reinforcement learning can refine polices to alleviate this
limitation, yet real-robot training is costly and unsafe, while training in
simulators suffers from the sim-to-real gap. Recent advances in generative
models have demonstrated remarkable capabilities in real-world simulation, with
diffusion models in particular excelling at generation. This raises the
question of how diffusion model-based world models can be combined to enhance
pre-trained policies in robotic manipulation. In this work, we propose
World4RL, a framework that employs diffusion-based world models as
high-fidelity simulators to refine pre-trained policies entirely in imagined
environments for robotic manipulation. Unlike prior works that primarily employ
world models for planning, our framework enables direct end-to-end policy
optimization. World4RL is designed around two principles: pre-training a
diffusion world model that captures diverse dynamics on multi-task datasets and
refining policies entirely within a frozen world model to avoid online
real-world interactions. We further design a two-hot action encoding scheme
tailored for robotic manipulation and adopt diffusion backbones to improve
modeling fidelity. Extensive simulation and real-world experiments demonstrate
that World4RL provides high-fidelity environment modeling and enables
consistent policy refinement, yielding significantly higher success rates
compared to imitation learning and other baselines. More visualization results
are available at https://world4rl.github.io/.

</details>


### [170] [FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation](https://arxiv.org/abs/2509.19102)
*Hongli Xu,Lei Zhang,Xiaoyue Hu,Boyang Zhong,Kaixin Bai,Zoltán-Csaba Márton,Zhenshan Bing,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang*

Main category: cs.RO

TL;DR: FunCanon converts long-horizon manipulation tasks into sequences of action chunks (actor, verb, object) and uses functional object canonicalization with affordance-driven alignment from vision-language models to train a diffusion policy (FuncDiffuser). This yields improved generalization and sim2real transfer in manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: End-to-end, task-specific policies often fail to generalize beyond training distributions. There is a need for compositional, reusable action primitives and a shared functional frame of objects to enable cross-task generalization.

Method: Decompose tasks into action chunks (actor-verb-object). Apply functional object canonicalization to align objects into shared functional frames using affordances from large vision-language models. Train an object-centric and action-centric diffusion policy (FuncDiffuser) on the aligned data.

Result: The approach demonstrates category-level generalization, cross-task behavior reuse, and robust sim2real deployment.

Conclusion: Functional canonicalization provides a strong inductive bias that enables scalable imitation learning in complex manipulation domains, supporting reuse and generalization across tasks.

Abstract: General-purpose robotic skills from end-to-end demonstrations often leads to
task-specific policies that fail to generalize beyond the training
distribution. Therefore, we introduce FunCanon, a framework that converts
long-horizon manipulation tasks into sequences of action chunks, each defined
by an actor, verb, and object. These chunks focus policy learning on the
actions themselves, rather than isolated tasks, enabling compositionality and
reuse. To make policies pose-aware and category-general, we perform functional
object canonicalization for functional alignment and automatic manipulation
trajectory transfer, mapping objects into shared functional frames using
affordance cues from large vision language models. An object centric and action
centric diffusion policy FuncDiffuser trained on this aligned data naturally
respects object affordances and poses, simplifying learning and improving
generalization ability. Experiments on simulated and real-world benchmarks
demonstrate category-level generalization, cross-task behavior reuse, and
robust sim2real deployment, showing that functional canonicalization provides a
strong inductive bias for scalable imitation learning in complex manipulation
domains. Details of the demo and supplemental material are available on our
project website https://sites.google.com/view/funcanon.

</details>


### [171] [Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation](https://arxiv.org/abs/2509.19105)
*Sarvesh Prajapati,Ananya Trivedi,Nathaniel Hanson,Bruce Maxwell,Taskin Padir*

Main category: cs.RO

TL;DR: RS-Net maps RGB patches to spectral signatures to infer terrain labels and friction, enabling RGB-only sensing for planning and control in outdoor robots.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of physical interactions with terrain requires material info; spectral sensing provides this but is hardware-costly and compute-intensive. Bridging RGB sensing to spectral information aims to enable robust, low-cost perception for locomotion.

Method: Train RS-Net to predict spectral signatures from RGB patches, then map them to terrain labels and friction coefficients. Integrate these into a sampling-based planner for a wheeled robot and a friction-aware MPC for a quadruped, with training using spectral data and test-time RGB-only sensing.

Result: Feasibility demonstrated: RGB-derived spectral properties can be used within planning and control pipelines; code is publicly available for replication; the approach learns task-relevant physical properties during training and applies RGB-only at test time.

Conclusion: The framework reduces reliance on spectral hardware by learning task-relevant physical properties during training and operating with RGB sensing at test time, broadening accessibility of material-aware locomotion in outdoor robotics.

Abstract: Successful navigation in outdoor environments requires accurate prediction of
the physical interactions between the robot and the terrain. To this end,
several methods rely on geometric or semantic labels to classify traversable
surfaces. However, such labels cannot distinguish visually similar surfaces
that differ in material properties. Spectral sensors enable inference of
material composition from surface reflectance measured across multiple
wavelength bands. Although spectral sensing is gaining traction in robotics,
widespread deployment remains constrained by the need for custom hardware
integration, high sensor costs, and compute-intensive processing pipelines. In
this paper, we present RGB Image to Spectral Signature Neural Network (RS-Net),
a deep neural network designed to bridge the gap between the accessibility of
RGB sensing and the rich material information provided by spectral data. RS-Net
predicts spectral signatures from RGB patches, which we map to terrain labels
and friction coefficients. The resulting terrain classifications are integrated
into a sampling-based motion planner for a wheeled robot operating in outdoor
environments. Likewise, the friction estimates are incorporated into a
contact-force-based MPC for a quadruped robot navigating slippery surfaces.
Thus, we introduce a framework that learns the task-relevant physical property
once during training and thereafter relies solely on RGB sensing at test time.
The code is available at https://github.com/prajapatisarvesh/RS-Net.

</details>


### [172] [BiGraspFormer: End-to-End Bimanual Grasp Transformer](https://arxiv.org/abs/2509.19142)
*Kangmin Kim,Seunghyeok Back,Geonhyup Lee,Sangbeom Lee,Sangjun Noh,Kyoobin Lee*

Main category: cs.RO

TL;DR: BiGraspFormer is a unified end-to-end transformer framework that directly generates coordinated bimanual grasps from object point clouds using a Single-Guided Bimanual strategy, offering fast inference and superior performance in simulation and real-world tests.


<details>
  <summary>Details</summary>
Motivation: Current bimanual grasping methods either use single-arm grasping or split grasp generation and evaluation, causing coordination problems like collisions and unbalanced force; there's a need for end-to-end, coordinated bimanual grasping.

Method: Introduce Single-Guided Bimanual (SGB) strategy: a transformer decoder generates diverse single-grasp candidates; then specialized attention uses their features to jointly predict bimanual poses and quality scores, reducing 12-DoF search complexity; end-to-end BiGraspFormer.

Result: Comprehensive simulations and real-world validations show BiGraspFormer outperforms existing methods and runs at under 0.05 s inference.

Conclusion: BiGraspFormer effectively enables coordinated bimanual manipulation with an efficient end-to-end Transformer framework; code available.

Abstract: Bimanual grasping is essential for robots to handle large and complex
objects. However, existing methods either focus solely on single-arm grasping
or employ separate grasp generation and bimanual evaluation stages, leading to
coordination problems including collision risks and unbalanced force
distribution. To address these limitations, we propose BiGraspFormer, a unified
end-to-end transformer framework that directly generates coordinated bimanual
grasps from object point clouds. Our key idea is the Single-Guided Bimanual
(SGB) strategy, which first generates diverse single grasp candidates using a
transformer decoder, then leverages their learned features through specialized
attention mechanisms to jointly predict bimanual poses and quality scores. This
conditioning strategy reduces the complexity of the 12-DoF search space while
ensuring coordinated bimanual manipulation. Comprehensive simulation
experiments and real-world validation demonstrate that BiGraspFormer
consistently outperforms existing methods while maintaining efficient inference
speed (<0.05s), confirming the effectiveness of our framework. Code and
supplementary materials are available at https://sites.google.com/bigraspformer

</details>


### [173] [A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination](https://arxiv.org/abs/2509.19168)
*Mark Gonzales,Ethan Oh,Joseph Moore*

Main category: cs.RO

TL;DR: A receding-horizon, sampling-based planner uses cross-entropy to optimize multimodal policies, improving robustness and scalability for multi-robot planning with real-time feasibility.


<details>
  <summary>Details</summary>
Motivation: Mitigate local minima in planning, promote exploration of diverse strategy modes, and enable scalable, distributed coordination for multi-robot systems without centralized optimization.

Method: Use the cross-entropy method to optimize a multimodal policy under a common cost within a receding-horizon planner; extend to multi-robot settings where agents share diverse candidate policies to avoid deadlocks; aim to minimize a global objective without centralized optimization.

Result: Numerical simulations show higher success rates when employing multiple modes in trap environments and in multi-robot collision avoidance; hardware experiments confirm real-time feasibility and practical performance.

Conclusion: Multimodal policy optimization via cross-entropy within a receding-horizon planner yields robust, scalable, and real-time capable planning for multi-robot systems, with clear benefits in exploration, deadlock avoidance, and distributed coordination.

Abstract: In this paper, we present a receding-horizon, sampling-based planner capable
of reasoning over multimodal policy distributions. By using the cross-entropy
method to optimize a multimodal policy under a common cost function, our
approach increases robustness against local minima and promotes effective
exploration of the solution space. We show that our approach naturally extends
to multi-robot collision-free planning, enables agents to share diverse
candidate policies to avoid deadlocks, and allows teams to minimize a global
objective without incurring the computational complexity of centralized
optimization. Numerical simulations demonstrate that employing multiple modes
significantly improves success rates in trap environments and in multi-robot
collision avoidance. Hardware experiments further validate the approach's
real-time feasibility and practical performance.

</details>


### [174] [MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human Demonstration to Robotic Deployment Gap](https://arxiv.org/abs/2509.19169)
*Tianyu Wu,Xudong Han,Haoran Sun,Zishang Zhang,Bangchao Huang,Chaoyang Song,Fang Wan*

Main category: cs.RO

TL;DR: MagiClaw provides a dual-purpose, camera-equipped two-finger end-effector and data-collection tool that unifies handheld teleoperation and robotic policy deployment for manipulation.


<details>
  <summary>Details</summary>
Motivation: Domain gap between human demonstrations and robot execution due to sensing and morphology differences; need a versatile, hardware-consistent platform to collect rich, multimodal data for robust policy learning.

Method: Two-finger end-effector with Soft Polyhedral Networks on each finger, each with embedded camera to estimate 6-DoF forces and contact deformation; fuses SPN proprioception with exteroceptive data from an integrated iPhone (6D pose, RGB, LiDAR depth); data streamed via a custom iOS app for real-time teleoperation, offline learning, and mixed-reality control.

Result: System demonstrates unified architecture that lowers barrier to collecting high-fidelity, contact-rich datasets and accelerates development of generalizable manipulation policies.

Conclusion: MagiClaw offers hardware-consistent, multimodal sensing and software integration to bridge domain gaps, enabling scalable data collection and policy learning for manipulation tasks.

Abstract: The transfer of manipulation skills from human demonstration to robotic
execution is often hindered by a "domain gap" in sensing and morphology. This
paper introduces MagiClaw, a versatile two-finger end-effector designed to
bridge this gap. MagiClaw functions interchangeably as both a handheld tool for
intuitive data collection and a robotic end-effector for policy deployment,
ensuring hardware consistency and reliability. Each finger incorporates a Soft
Polyhedral Network (SPN) with an embedded camera, enabling vision-based
estimation of 6-DoF forces and contact deformation. This proprioceptive data is
fused with exteroceptive environmental sensing from an integrated iPhone, which
provides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS
application, MagiClaw streams synchronized, multi-modal data for real-time
teleoperation, offline policy learning, and immersive control via mixed-reality
interfaces. We demonstrate how this unified system architecture lowers the
barrier to collecting high-fidelity, contact-rich datasets and accelerates the
development of generalizable manipulation policies. Please refer to the iOS app
at https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.

</details>


### [175] [Proactive-reactive detection and mitigation of intermittent faults in robot swarms](https://arxiv.org/abs/2509.19246)
*Sinan Oğuz,Emanuele Garone,Marco Dorigo,Mary Katherine Heinrich*

Main category: cs.RO

TL;DR: Proactive-reactive fault detection and mitigation for intermittent faults in robot swarms with persistent networks (SoNS) using self-organized backup layers, a multiplex network, and one-shot likelihood ratio tests to detect faults and reroute communication, validated on formation control scenarios.


<details>
  <summary>Details</summary>
Motivation: Intermittent faults are hard to detect in traditional self-organizing ad-hoc swarm networks due to transient/topology changes. SoNS enables persistent networks that facilitate detection and resilience, improving reliability of swarm coordination.

Method: 1) Proactively create dynamic backup paths that adapt to topology and positioning. 2) Reactive fault detection via one-shot likelihood ratio tests comparing information along multiple paths in a multiplex network. 3) Self-organized rerouting of communication upon fault detection, until fault resolves. 4) Validation in scenarios with faulty positional data during formation control.

Result: High fault detection accuracy with low false positives; intermittent faults do not disrupt convergence to desired formations; effective communication rerouting maintains formation progression.

Conclusion: The proactive-reactive multiplex approach leveraging SoNS and self-organized backup layers offers robust detection and mitigation of intermittent faults in robot swarms, preserving formation control and reliability; the framework shows promise for general distributed systems with transient faults.

Abstract: Intermittent faults are transient errors that sporadically appear and
disappear. Although intermittent faults pose substantial challenges to
reliability and coordination, existing studies of fault tolerance in robot
swarms focus instead on permanent faults. One reason for this is that
intermittent faults are prohibitively difficult to detect in the fully
self-organized ad-hoc networks typical of robot swarms, as their network
topologies are transient and often unpredictable. However, in the recently
introduced self-organizing nervous systems (SoNS) approach, robot swarms are
able to self-organize persistent network structures for the first time, easing
the problem of detecting intermittent faults. To address intermittent faults in
robot swarms that have persistent networks, we propose a novel
proactive-reactive strategy to detection and mitigation, based on
self-organized backup layers and distributed consensus in a multiplex network.
Proactively, the robots self-organize dynamic backup paths before faults occur,
adapting to changes in the primary network topology and the robots' relative
positions. Reactively, robots use one-shot likelihood ratio tests to compare
information received along different paths in the multiplex network, enabling
early fault detection. Upon detection, communication is temporarily rerouted in
a self-organized way, until the detected fault resolves. We validate the
approach in representative scenarios of faulty positional data occurring during
formation control, demonstrating that intermittent faults are prevented from
disrupting convergence to desired formations, with high fault detection
accuracy and low rates of false positives.

</details>


### [176] [Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces](https://arxiv.org/abs/2509.19261)
*Kuanqi Cai,Chunfeng Wang,Zeqi Li,Haowen Yao,Weinan Chen,Luis Figueredo,Aude Billard,Arash Ajoudani*

Main category: cs.RO

TL;DR: An imitation-guided bimanual planning framework for seamless grasp transitions in dynamic environments, featuring sampling-based stable intersections in grasp manifolds and a hierarchical dual-stage motion planner (imitation-based global path generator + QP local planner) to improve stability, dexterity, and efficiency under external forces.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of smooth, adaptive grasp transitions under external forces and motion constraints, and the shortcomings of existing grasp transition strategies that neglect force variations and do not optimize motion performance.

Method: Introduce Strategies for Sampling Stable Intersections in Grasp Manifolds to enable seamless uni-manual/bi-manual transitions; propose a Hierarchical Dual-Stage Motion Architecture with an Imitation Learning-based Global Path Generator and a Quadratic Programming-driven Local Planner for real-time feasibility and obstacle avoidance.

Result: Demonstrates significant improvements in grasp transition efficiency and motion performance on force-intensive tasks; real-time capability and superior manipulability demonstrated in simulation results (video available).

Conclusion: The framework enhances stability and dexterity in dynamic manipulation by integrating efficient grasp transition strategies with motion optimization, reducing regrasping costs and enabling robust bimanual manipulation under external forces.

Abstract: Robotic manipulation in dynamic environments often requires seamless
transitions between different grasp types to maintain stability and efficiency.
However, achieving smooth and adaptive grasp transitions remains a challenge,
particularly when dealing with external forces and complex motion constraints.
Existing grasp transition strategies often fail to account for varying external
forces and do not optimize motion performance effectively. In this work, we
propose an Imitation-Guided Bimanual Planning Framework that integrates
efficient grasp transition strategies and motion performance optimization to
enhance stability and dexterity in robotic manipulation. Our approach
introduces Strategies for Sampling Stable Intersections in Grasp Manifolds for
seamless transitions between uni-manual and bi-manual grasps, reducing
computational costs and regrasping inefficiencies. Additionally, a Hierarchical
Dual-Stage Motion Architecture combines an Imitation Learning-based Global Path
Generator with a Quadratic Programming-driven Local Planner to ensure real-time
motion feasibility, obstacle avoidance, and superior manipulability. The
proposed method is evaluated through a series of force-intensive tasks,
demonstrating significant improvements in grasp transition efficiency and
motion performance. A video demonstrating our simulation results can be viewed
at
\href{https://youtu.be/3DhbUsv4eDo}{\textcolor{blue}{https://youtu.be/3DhbUsv4eDo}}.

</details>


### [177] [SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration](https://arxiv.org/abs/2509.19292)
*Yang Jin,Jun Lv,Han Xue,Wendi Chen,Chuan Wen,Cewu Lu*

Main category: cs.RO

TL;DR: SOE confines exploration to a learned task-relevant action manifold, via a compact latent representation, yielding safer, more diverse, and more sample-efficient policy improvement in robotic manipulation; it plugs into existing policies and supports human-guided exploration.


<details>
  <summary>Details</summary>
Motivation: Address action mode collapse and unsafe, random perturbations that hinder exploration in robotic manipulation; need safe, effective exploration that does not degrade base policy performance.

Method: Learn a compact latent representation of task-relevant factors and constrain exploration to the manifold of valid actions; deploy as a plug-in module compatible with any policy; structured latent space enables human-guided exploration.

Result: Outperforms prior methods in both simulation and real-world tasks, achieving higher task success rates, smoother and safer exploration, and superior sample efficiency.

Conclusion: On-manifold exploration is a principled approach for sample-efficient policy self-improvement in robotics, with SOE enabling safe, diverse exploration and human-guided control.

Abstract: Intelligent agents progress by continually refining their capabilities
through actively exploring environments. Yet robot policies often lack
sufficient exploration capability due to action mode collapse. Existing methods
that encourage exploration typically rely on random perturbations, which are
unsafe and induce unstable, erratic behaviors, thereby limiting their
effectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a
framework that enhances policy exploration and improvement in robotic
manipulation. SOE learns a compact latent representation of task-relevant
factors and constrains exploration to the manifold of valid actions, ensuring
safety, diversity, and effectiveness. It can be seamlessly integrated with
arbitrary policy models as a plug-in module, augmenting exploration without
degrading the base policy performance. Moreover, the structured latent space
enables human-guided exploration, further improving efficiency and
controllability. Extensive experiments in both simulation and real-world tasks
demonstrate that SOE consistently outperforms prior methods, achieving higher
task success rates, smoother and safer exploration, and superior sample
efficiency. These results establish on-manifold exploration as a principled
approach to sample-efficient policy self-improvement. Project website:
https://ericjin2002.github.io/SOE

</details>


### [178] [Residual Off-Policy RL for Finetuning Behavior Cloning Policies](https://arxiv.org/abs/2509.19301)
*Lars Ankile,Zhenyu Jiang,Rocky Duan,Guanya Shi,Pieter Abbeel,Anusha Nagabandi*

Main category: cs.RO

TL;DR: A residual learning framework that augments behavior cloning (BC) with lightweight per-step residual corrections learned via off-policy RL, enabling sample-efficient real-world RL on high-DoF robots and achieving state-of-the-art performance in vision-based tasks.


<details>
  <summary>Details</summary>
Motivation: BC provides strong priors from human demonstrations but is data-limited and task-specific; RL offers autonomous exploration but is sample-inefficient and risky for real robots, especially with sparse rewards. A hybrid approach aims to combine BC's bootstrapping with RL's refinement to enable practical, real-world manipulation.

Method: Treat the BC policy as a fixed black-box base and learn lightweight per-step residual corrections using sample-efficient off-policy RL trained with sparse binary rewards. This residual-RL is applied to high-DoF manipulation tasks in both simulation and the real world, including a first reported real-world RL training on a humanoid robot with dexterous hands.

Result: Demonstrates first successful real-world RL training on a humanoid robot with dexterous hands; achieves state-of-the-art performance on various vision-based manipulation tasks and shows the approach is a practical pathway for deploying RL in real-world robotics.

Conclusion: A residual BC+RL recipe provides a practical, data-efficient route to deploy RL on real robots by bootstrapping from BC and refining with lightweight off-policy residual learning, enabling high-DoF manipulation with sparse rewards.

Abstract: Recent advances in behavior cloning (BC) have enabled impressive visuomotor
control policies. However, these approaches are limited by the quality of human
demonstrations, the manual effort required for data collection, and the
diminishing returns from increasing offline data. In comparison, reinforcement
learning (RL) trains an agent through autonomous interaction with the
environment and has shown remarkable success in various domains. Still,
training RL policies directly on real-world robots remains challenging due to
sample inefficiency, safety concerns, and the difficulty of learning from
sparse rewards for long-horizon tasks, especially for high-degree-of-freedom
(DoF) systems. We present a recipe that combines the benefits of BC and RL
through a residual learning framework. Our approach leverages BC policies as
black-box bases and learns lightweight per-step residual corrections via
sample-efficient off-policy RL. We demonstrate that our method requires only
sparse binary reward signals and can effectively improve manipulation policies
on high-degree-of-freedom (DoF) systems in both simulation and the real world.
In particular, we demonstrate, to the best of our knowledge, the first
successful real-world RL training on a humanoid robot with dexterous hands. Our
results demonstrate state-of-the-art performance in various vision-based tasks,
pointing towards a practical pathway for deploying RL in the real world.
Project website: https://residual-offpolicy-rl.github.io

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [179] [A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services](https://arxiv.org/abs/2509.18101)
*Guanzhong Pan,Haibo Wang*

Main category: cs.AI

TL;DR: A framework to evaluate when on-prem open-source LLM deployment is cost-effective versus cloud subscriptions, using hardware, O&M costs, and performance benchmarks of open-source models to identify a breakeven usage point.


<details>
  <summary>Details</summary>
Motivation: Organizations seek to balance data privacy, vendor lock-in, and long-term costs; open-source on-prem deployment promises control and potential savings but requires careful cost assessment.

Method: Develop a cost-benefit analysis framework that (1) inventories hardware requirements and operating expenses for on-prem LLMs, (2) benchmarks performance of open-source models (e.g., Qwen, Llama, Mistral, etc.), and (3) compares total cost of ownership to cloud subscription fees to derive an estimated breakeven point based on usage and performance needs.

Result: The analysis yields a breakeven usage level where on-prem deployment becomes cheaper than cloud subscriptions for given performance targets, providing a practical planning tool for LLM strategy.

Conclusion: For certain workloads and performance requirements, on-prem open-source LLM deployment can be economically viable, and the presented framework helps organizations decide between cloud and on-prem options while considering data privacy and switching costs.

Abstract: Large language models (LLMs) are becoming increasingly widespread.
Organizations that want to use AI for productivity now face an important
decision. They can subscribe to commercial LLM services or deploy models on
their own infrastructure. Cloud services from providers such as OpenAI,
Anthropic, and Google are attractive because they provide easy access to
state-of-the-art models and are easy to scale. However, concerns about data
privacy, the difficulty of switching service providers, and long-term operating
costs have driven interest in local deployment of open-source models. This
paper presents a cost-benefit analysis framework to help organizations
determine when on-premise LLM deployment becomes economically viable compared
to commercial subscription services. We consider the hardware requirements,
operational expenses, and performance benchmarks of the latest open-source
models, including Qwen, Llama, Mistral, and etc. Then we compare the total cost
of deploying these models locally with the major cloud providers subscription
fee. Our findings provide an estimated breakeven point based on usage levels
and performance needs. These results give organizations a practical framework
for planning their LLM strategies.

</details>


### [180] [SPADE: A Large Language Model Framework for Soil Moisture Pattern Recognition and Anomaly Detection in Precision Agriculture](https://arxiv.org/abs/2509.18123)
*Yeonju Lee,Rui Qi Chen,Joseph Oboamah,Po Nien Su,Wei-zhen Liang,Yeyin Shi,Lu Gan,Yongsheng Chen,Xin Qiao,Jing Li*

Main category: cs.AI

TL;DR: SPADE presents a zero-shot, LLM-based framework for soil moisture time-series analysis by converting data to text and using prompt templates to detect irrigation events, estimate irrigation gains, and classify anomalies, yielding interpretable reports and improved performance on real-world data.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of threshold-based rules and data-hungry ML/DL models in soil moisture analysis: need for adaptable, interpretable, and annotation-efficient methods that can leverage domain knowledge and reasoning. LLMs offer instruction-following and reasoning to unify pattern and anomaly detection in a single framework.

Method: Convert soil moisture time-series into a textual representation. Use domain-informed prompts with ChatGPT-4.1 to perform zero-shot analysis that detects irrigation events, estimates net irrigation gains, detects/classifies anomalies, and generates structured reports. Evaluate on real-world sensor data from commercial and experimental farms across multiple crops in the United States.

Result: SPADE outperforms existing methods in anomaly detection (higher recall and F1) and accurately classifies anomaly types. It also achieves high precision and recall in detecting irrigation events, producing interpretable reports that summarize findings for actionable irrigation scheduling.

Conclusion: LLMs can serve as scalable, adaptable tools for precision agriculture by integrating qualitative knowledge with data-driven reasoning to monitor soil moisture and support improved irrigation decisions; SPADE demonstrates a versatile approach to time-series interpretation without task-specific annotations.

Abstract: Accurate interpretation of soil moisture patterns is critical for irrigation
scheduling and crop management, yet existing approaches for soil moisture
time-series analysis either rely on threshold-based rules or data-hungry
machine learning or deep learning models that are limited in adaptability and
interpretability. In this study, we introduce SPADE (Soil moisture Pattern and
Anomaly DEtection), an integrated framework that leverages large language
models (LLMs) to jointly detect irrigation patterns and anomalies in soil
moisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced
reasoning and instruction-following capabilities, enabling zero-shot analysis
without requiring task-specific annotation or fine-tuning. By converting
time-series data into a textual representation and designing domain-informed
prompt templates, SPADE identifies irrigation events, estimates net irrigation
gains, detects, classifies anomalies, and produces structured, interpretable
reports. Experiments were conducted on real-world soil moisture sensor data
from commercial and experimental farms cultivating multiple crops across the
United States. Results demonstrate that SPADE outperforms the existing method
in anomaly detection, achieving higher recall and F1 scores and accurately
classifying anomaly types. Furthermore, SPADE achieved high precision and
recall in detecting irrigation events, indicating its strong capability to
capture irrigation patterns accurately. SPADE's reports provide
interpretability and usability of soil moisture analytics. This study
highlights the potential of LLMs as scalable, adaptable tools for precision
agriculture, which is capable of integrating qualitative knowledge and
data-driven reasoning to produce actionable insights for accurate soil moisture
monitoring and improved irrigation scheduling from soil moisture time-series
data.

</details>


### [181] [Position Paper: Integrating Explainability and Uncertainty Estimation in Medical AI](https://arxiv.org/abs/2509.18132)
*Xiuyi Fan*

Main category: cs.AI

TL;DR: A conceptual framework (XUE) that fuses explainability with uncertainty quantification in medical AI, bridging XAI and UE to boost trust and clinical usefulness; provides mapping, challenges, directions, and guiding principles.


<details>
  <summary>Details</summary>
Motivation: Medical AI currently fails to clearly communicate uncertainty. XAI explains predictions but not confidence; UE offers confidence estimates but lacks interpretability. Bridging these gaps is essential for trustworthy adoption in real-world medicine.

Method: Systematically map medical uncertainty concepts to AI uncertainty concepts, identify key challenges in implementing XUE, and outline technical directions (multimodal uncertainty quantification, model-agnostic visualization techniques, uncertainty-aware decision support) along with guiding principles.

Result: Proposes the XUE framework and a roadmap: a conceptual integration of explainability with uncertainty quantification; provides a mapping, identified challenges, and directions for future research and practical guidelines.

Conclusion: To realize trustworthy medical AI, systems should not only generate reliable predictions but also articulate confidence levels in a clinically meaningful way. XUE offers a path to align explainability with uncertainty, facilitating adoption and real-world clinical reasoning.

Abstract: Uncertainty is a fundamental challenge in medical practice, but current
medical AI systems fail to explicitly quantify or communicate uncertainty in a
way that aligns with clinical reasoning. Existing XAI works focus on
interpreting model predictions but do not capture the confidence or reliability
of these predictions. Conversely, uncertainty estimation (UE) techniques
provide confidence measures but lack intuitive explanations. The disconnect
between these two areas limits AI adoption in medicine. To address this gap, we
propose Explainable Uncertainty Estimation (XUE) that integrates explainability
with uncertainty quantification to enhance trust and usability in medical AI.
We systematically map medical uncertainty to AI uncertainty concepts and
identify key challenges in implementing XUE. We outline technical directions
for advancing XUE, including multimodal uncertainty quantification,
model-agnostic visualization techniques, and uncertainty-aware decision support
systems. Lastly, we propose guiding principles to ensure effective XUE
realisation. Our analysis highlights the need for AI systems that not only
generate reliable predictions but also articulate confidence levels in a
clinically meaningful way. This work contributes to the development of
trustworthy medical AI by bridging explainability and uncertainty, paving the
way for AI systems that are aligned with real-world clinical complexities.

</details>


### [182] [HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics](https://arxiv.org/abs/2509.18168)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: Hierarchical Segment-Graph Memory (HSGM) enables scalable semantic parsing of ultra-long documents by decomposing input into segments, building local graphs, and aggregating a compact global memory for efficient query-driven reasoning.


<details>
  <summary>Details</summary>
Motivation: Semantic parsing of long documents faces quadratic growth in pairwise reasoning and prohibitive memory use; there is a need for incremental updates and efficient retrieval to support real-time, resource-constrained NLP.

Method: 1) Split input N into M segments of size k (k << N). 2) Build Local Semantic Graphs on each segment. 3) Extract summary nodes to form a Global Graph Memory. 4) Enable incremental updates: only newly arrived segments incur local graph construction and summary integration. 5) Use Hierarchical Query Processing: top-K retrieval over summary nodes to locate relevant segments, then perform fine-grained reasoning within their local graphs. 6) Theoretically reduce worst-case complexity from O(N^2) to O(N k + (N/k)^2) and derive Frobenius-norm bounds on approximation error from summarization and sparsification.

Result: Empirically, on long-document AMR parsing, segment-level semantic role labeling (OntoNotes), and legal event extraction, HSGM achieves 2–4× inference speedup, >60% peak memory reduction, and ≥95% of baseline accuracy.

Conclusion: HSGM enables scalable, accurate semantic modeling for ultra-long texts, unlocking real-time and resource-constrained NLP applications.

Abstract: Semantic parsing of long documents remains challenging due to quadratic
growth in pairwise composition and memory requirements. We introduce
\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that
decomposes an input of length $N$ into $M$ meaningful segments, constructs
\emph{Local Semantic Graphs} on each segment, and extracts compact
\emph{summary nodes} to form a \emph{Global Graph Memory}. HSGM supports
\emph{incremental updates} -- only newly arrived segments incur local graph
construction and summary-node integration -- while \emph{Hierarchical Query
Processing} locates relevant segments via top-$K$ retrieval over summary nodes
and then performs fine-grained reasoning within their local graphs.
  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to
$O\!\left(N\,k + (N/k)^2\right)$, with segment size $k \ll N$, and we derive
Frobenius-norm bounds on the approximation error introduced by node
summarization and sparsification thresholds. Empirically, on three benchmarks
-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),
and legal event extraction -- HSGM achieves \emph{2--4$\times$ inference
speedup}, \emph{$>60\%$ reduction} in peak memory, and \emph{$\ge 95\%$} of
baseline accuracy. Our approach unlocks scalable, accurate semantic modeling
for ultra-long texts, enabling real-time and resource-constrained NLP
applications.

</details>


### [183] [Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM](https://arxiv.org/abs/2509.18178)
*Ling Yue,Nithin Somasekharan,Tingwen Zhang,Yadi Cao,Shaowu Pan*

Main category: cs.AI

TL;DR: Foam-Agent automates the full OpenFOAM CFD workflow with a multi-agent system, enabling automated meshing, HPC script generation, and visualization from a natural-language prompt, achieving 88.2% task success on 110 benchmarks.


<details>
  <summary>Details</summary>
Motivation: CFD\'s steep learning curve and manual setup impede broader adoption; there is a need for end-to-end automation and reusable, composable tools.

Method: Multi-agent framework with a Meshing Agent that handles external meshes and generates geometries via Gmsh, automatic HPC submission script generation, and ParaView post-simulation visualization; Model Context Protocol (MCP) exposes core functions as discrete tools for flexible integration; Hierarchical Multi-Index RAG for precise context retrieval and dependency-aware generation; evaluated on 110 CFD tasks with Claude-code integration for exploratory workflows; open-source code at GitHub.

Result: Achieves 88.2% success rate with Claude 3.5 Sonnet, significantly outperforming MetaOpenFOAM at 55.5%; public code available at https://github.com/csml-rpi/Foam-Agent.

Conclusion: Demonstrates that specialized multi-agent systems can democratize complex CFD workflows by lowering expertise barriers and enabling end-to-end automation; a step toward broader adoption of automated scientific computing.

Abstract: Computational Fluid Dynamics (CFD) is an essential simulation tool in
engineering, yet its steep learning curve and complex manual setup create
significant barriers. To address these challenges, we introduce Foam-Agent, a
multi-agent framework that automates the entire end-to-end OpenFOAM workflow
from a single natural language prompt. Our key innovations address critical
gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:
Foam-Agent is the first system to manage the full simulation pipeline,
including advanced pre-processing with a versatile Meshing Agent capable of
handling external mesh files and generating new geometries via Gmsh, automatic
generation of HPC submission scripts, and post-simulation visualization via
ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,
the framework uses Model Context Protocol (MCP) to expose its core functions as
discrete, callable tools. This allows for flexible integration and use by other
agentic systems, such as Claude-code, for more exploratory workflows. 3.
High-Fidelity Configuration Generation: We achieve superior accuracy through a
Hierarchical Multi-Index RAG for precise context retrieval and a
dependency-aware generation process that ensures configuration consistency.
Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%
success rate with Claude 3.5 Sonnet, significantly outperforming existing
frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the
expertise barrier for CFD, demonstrating how specialized multi-agent systems
can democratize complex scientific computing. The code is public at
https://github.com/csml-rpi/Foam-Agent.

</details>


### [184] [Large Language Models and Operations Research: A Structured Survey](https://arxiv.org/abs/2509.18180)
*Yang Wang,Kai Li*

Main category: cs.AI

TL;DR: A survey on integrating large language models (LLMs) into operations research (OR), structured around three directions—automatic modeling, auxiliary optimization, and direct solving—reviewing benchmarks, domain applications, open issues, and future research directions.


<details>
  <summary>Details</summary>
Motivation: OR faces challenges with large-scale, dynamic, multi-constraint problems and relies on expert-crafted models and manual parameter tuning. LLMs offer semantic understanding and controllable generation to convert natural language into models, code, heuristics, and optimization strategies, potentially addressing these bottlenecks.

Method: A literature survey that organizes recent progress into three main directions (automatic modeling, auxiliary optimization, direct solving), reviews evaluation benchmarks and domain-specific applications, and identifies key open issues and future research avenues.

Result: A structured map of how LLMs are being integrated into OR, with a synthesis of methods, benchmarks, and applications, highlighting benefits and current limitations.

Conclusion: LLMs hold promise to advance OR by enhancing modeling, heuristics, and solving capability, but challenges remain in unstable semantic-to-structure mapping, fragmented progress, limited generalization, and insufficient evaluation; standardized benchmarks and robust evaluation frameworks are needed to guide future work.

Abstract: Operations research (OR) provides fundamental methodologies for complex
system decision-making, with established applications in transportation, supply
chain management, and production scheduling. Traditional approaches, which
depend on expert-based modeling and manual parameter adjustment, often face
challenges in handling large-scale, dynamic, and multi-constraint problems.
Recently, large language models (LLMs) have shown potential to address these
limitations through semantic understanding, structured generation, and
reasoning control. LLMs can translate natural language descriptions into
mathematical models or executable code, generate heuristics, evolve algorithms,
and directly tackle optimization tasks. This paper surveys recent progress on
the integration of LLMs into OR, organizing methods into three main directions:
automatic modeling, auxiliary optimization, and direct solving. It further
reviews evaluation benchmarks and domain-specific applications, and summarizes
key open issues such as unstable semantic-to-structure mapping, fragmented
research progress, limited generalization, and insufficient evaluation systems.
Finally, the survey outlines possible research avenues for advancing the role
of LLMs in OR.

</details>


### [185] [Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling](https://arxiv.org/abs/2509.18181)
*Mustafa Sameen,Xiaojian Zhang,Xilei Zhao*

Main category: cs.AI

TL;DR: SAPA leverages LLM-generated traveler personas and latent-variable scores to predict ridesourcing choice, achieving large PR-AUC gains over baselines.


<details>
  <summary>Details</summary>
Motivation: Address limited predictive accuracy in ridesourcing mode-choice models due to neglect of psychological factors and severe class imbalance (ridesourcing trips are a small fraction of daily travel).

Method: A hierarchical framework: (1) use an LLM to generate qualitative traveler personas from survey data; (2) train a propensity-score model on demographics/behavior enriched by personas; (3) use the LLM to assign quantitative scores to theory-driven latent variables (e.g., time/cost sensitivity); (4) a final classifier combines the propensity score, latent-variable scores and interactions, plus observable trip attributes to predict ridesourcing mode choice.

Result: SAPA significantly outperforms baselines, with up to 75.9% improvement in PR-AUC on a held-out test set, demonstrated on a large-scale, multi-year travel survey.

Conclusion: SAPA provides a powerful, transferable methodology and tool for accurately predicting ridesourcing mode choices and can be adapted to other applications involving latent-attitude modeling.

Abstract: Accurate modeling of ridesourcing mode choices is essential for designing and
implementing effective traffic management policies for reducing congestion,
improving mobility, and allocating resources more efficiently. Existing models
for predicting ridesourcing mode choices often suffer from limited predictive
accuracy due to their inability to capture key psychological factors, and are
further challenged by severe class imbalance, as ridesourcing trips comprise
only a small fraction of individuals' daily travel. To address these
limitations, this paper introduces the Synthesizing Attitudes, Predicting
Actions (SAPA) framework, a hierarchical approach that uses Large Language
Models (LLMs) to synthesize theory-grounded latent attitudes to predict
ridesourcing choices. SAPA first uses an LLM to generate qualitative traveler
personas from raw travel survey data and then trains a propensity-score model
on demographic and behavioral features, enriched by those personas, to produce
an individual-level score. Next, the LLM assigns quantitative scores to
theory-driven latent variables (e.g., time and cost sensitivity), and a final
classifier integrates the propensity score, latent-variable scores (with their
interaction terms), and observable trip attributes to predict ridesourcing mode
choice. Experiments on a large-scale, multi-year travel survey show that SAPA
significantly outperforms state-of-the-art baselines, improving ridesourcing
choice predictions by up to 75.9% in terms of PR-AUC on a held-out test set.
This study provides a powerful tool for accurately predicting ridesourcing mode
choices, and provides a methodology that is readily transferable to various
applications.

</details>


### [186] [An Outcome-Based Educational Recommender System](https://arxiv.org/abs/2509.18186)
*Nursultan Askarbekuly,Timur Fayzrakhmanov,Sladjan Babarogić,Ivan Luković*

Main category: cs.AI

TL;DR: An outcome-based educational recommender (OBER) embeds learning outcomes and assessments into the data schema to evaluate algorithms by mastery, not just clicks or ratings. In a two-week RCT with 5,700+ learners, collaborative filtering boosted retention while a fixed expert path yielded the highest mastery; OBER links business, relevance, and learning metrics from the same logs, enabling balance between relevance/engagement and learning outcomes and is technology-agnostic and extensible for future adaptive systems.


<details>
  <summary>Details</summary>
Motivation: Current educational recommender systems are optimized for click- or rating-based relevance, not for actual pedagogical impact. There is a need to evaluate and optimize systems by mastery and learning outcomes. Integrating learning objectives and assessments into the data schema enables direct measurement of learning impact and supports decision-making that balances engagement with mastery.

Method: Introduce OBER: a minimalist entity-relation data model, a log-driven mastery formula, and a plug-in architecture. Implemented in an e-learning system for a non-formal domain. Evaluated via a two-week randomized split test with over 5,700 learners comparing three strategies: fixed expert trajectory, collaborative filtering (CF), and knowledge-based (KB) filtering. Metrics derived from logs cover business, relevance, and learning outcomes, allowing method-agnostic evaluation.

Result: CF achieved higher retention, while the fixed expert path produced the highest mastery. Because all metrics come from the same logs, practitioners can balance relevance/engagement against mastery without extra testing, and the framework is readily extensible to future adaptive or context-aware recommenders.

Conclusion: OBER provides a unified, outcome-centered evaluation framework for educational recommenders. By embedding learning outcomes into the data schema and deriving all metrics from logs, it enables practitioners to weigh pedagogical impact against engagement, supports method-agnostic experimentation, and is readily extensible to future adaptive/context-aware systems.

Abstract: Most educational recommender systems are tuned and judged on click- or
rating-based relevance, leaving their true pedagogical impact unclear. We
introduce OBER-an Outcome-Based Educational Recommender that embeds learning
outcomes and assessment items directly into the data schema, so any algorithm
can be evaluated on the mastery it fosters. OBER uses a minimalist
entity-relation model, a log-driven mastery formula, and a plug-in
architecture. Integrated into an e-learning system in non-formal domain, it was
evaluated trough a two-week randomized split test with over 5 700 learners
across three methods: fixed expert trajectory, collaborative filtering (CF),
and knowledge-based (KB) filtering. CF maximized retention, but the fixed path
achieved the highest mastery. Because OBER derives business, relevance, and
learning metrics from the same logs, it lets practitioners weigh relevance and
engagement against outcome mastery with no extra testing overhead. The
framework is method-agnostic and readily extensible to future adaptive or
context-aware recommenders.

</details>


### [187] [MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation](https://arxiv.org/abs/2509.18198)
*Rui Liu,Zikang Wang,Peng Gao,Yu Shen,Pratap Tokekar,Ming Lin*

Main category: cs.AI

TL;DR: MMCD is a cross-modal, teacher-student framework for robust decision-making in connected autonomous driving that fuses ego and collaborative vehicle observations and remains effective when modalities are missing via cross-modal knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: Safety in accident-prone environments is hampered by limited sensors and occlusions; existing methods often require all modalities and vehicle connectivity during training and testing, which is impractical due to sensor or connectivity failures.

Method: Proposes MMCD that fuses multi-modal data from ego and collaborative vehicles; uses teacher-student structure with a teacher model trained on all modalities and a student model designed to operate with reduced modalities; employs cross-modal knowledge distillation to transfer knowledge from teacher to student, enabling robustness to missing modalities; evaluated on connected autonomous driving with ground vehicles and aerial-ground collaboration.

Result: Claims up to 20.7% improvement in driving safety, surpassing baselines in detecting potential accidents and making safe driving decisions.

Conclusion: MMCD demonstrates robust performance under modality absence and effective cross-modal distillation for connected autonomy; results suggest practical improvements for safety in multi-modal, multi-vehicle settings; project site provided for more details.

Abstract: Autonomous systems have advanced significantly, but challenges persist in
accident-prone environments where robust decision-making is crucial. A single
vehicle's limited sensor range and obstructed views increase the likelihood of
accidents. Multi-vehicle connected systems and multi-modal approaches,
leveraging RGB images and LiDAR point clouds, have emerged as promising
solutions. However, existing methods often assume the availability of all data
modalities and connected vehicles during both training and testing, which is
impractical due to potential sensor failures or missing connected vehicles. To
address these challenges, we introduce a novel framework MMCD (Multi-Modal
Collaborative Decision-making) for connected autonomy. Our framework fuses
multi-modal observations from ego and collaborative vehicles to enhance
decision-making under challenging conditions. To ensure robust performance when
certain data modalities are unavailable during testing, we propose an approach
based on cross-modal knowledge distillation with a teacher-student model
structure. The teacher model is trained with multiple data modalities, while
the student model is designed to operate effectively with reduced modalities.
In experiments on $\textit{connected autonomous driving with ground vehicles}$
and $\textit{aerial-ground vehicles collaboration}$, our method improves
driving safety by up to ${\it 20.7}\%$, surpassing the best-existing baseline
in detecting potential accidents and making safe driving decisions. More
information can be found on our website https://ruiiu.github.io/mmcd.

</details>


### [188] [Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and Counterfactual Explanations](https://arxiv.org/abs/2509.18215)
*Timotheus Kampik,Kristijonas Čyras,José Ruiz Alarcón*

Main category: cs.AI

TL;DR: Presents a formal method to explain how inferences change in Quantitative Bipolar Argumentation Frameworks by tracing strength inconsistencies among topic arguments; classifies explanations (sufficient, necessary, counterfactual) and provides a heuristic search with an implementation.


<details>
  <summary>Details</summary>
Motivation: To understand and justify why updates to a QBAF alter conclusions, and to provide actionable explanations that identify the arguments responsible for such changes.

Method: Trace the evolution of the partial order on argument strengths on topic arguments after each update; identify causes, classify explanations as sufficient/necessary/counterfactual; prove that explanations exist iff an update introduces strength inconsistency; design a heuristic search to find explanations; implement it.

Result: Establishes existence and types of explanations; links existence to updates causing strength inconsistency; provides a heuristic-based search method and an implementation.

Conclusion: The framework enables formal, explainable reasoning about inference changes in QBAFs, offering concrete explanations and a practical tool for practitioners to diagnose and communicate why conclusions shift.

Abstract: This paper presents a formal approach to explaining change of inference in
Quantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions
from a QBAF and updating the QBAF to then again draw conclusions (and so on),
our approach traces changes -- which we call strength inconsistencies -- in the
partial order over argument strengths that a semantics establishes on some
arguments of interest, called topic arguments. We trace the causes of strength
inconsistencies to specific arguments, which then serve as explanations. We
identify sufficient, necessary, and counterfactual explanations for strength
inconsistencies and show that strength inconsistency explanations exist if and
only if an update leads to strength inconsistency. We define a heuristic-based
approach to facilitate the search for strength inconsistency explanations, for
which we also provide an implementation.

</details>


### [189] [nDNA -- the Semantic Helix of Artificial Cognition](https://arxiv.org/abs/2509.18216)
*Amitava Das*

Main category: cs.AI

TL;DR: Proposes Neural DNA (nDNA), a semantic-genotypic fingerprint for AI foundation models that encodes latent identity via geometry (spectral curvature, thermodynamic length, belief vector field). Enables lineage tracing, inheritance, drift detection, and governance of artificial cognition; inaugurates Neural Genomics as a field.


<details>
  <summary>Details</summary>
Motivation: Benchmarks measure external behavior but miss the model’s internal cognitive structure. A geometry-first, coordinate-free fingerprint is proposed to capture latent identity, enable tracing of evolution across training and alignment, and support safer, more controllable AI development.

Method: Define three latent-geometry dimensions: spectral curvature (curvature of conceptual flow across layers), thermodynamic length (semantic effort to traverse representational transitions through layers), and belief vector field (semantic torsion fields guiding belief orientations). Read the model as semantic fluid dynamics; produce a geometry-based, coordinate-free nDNA fingerprint tied to input behavior. Use the fingerprint to trace lineages across pretraining, fine-tuning, alignment, pruning, distillation, and merging; measure inheritance between checkpoints and detect drift under new data/objectives; explore evolution of artificial cognition for comparison, risk diagnosis, and governance.

Result: A conceptual framework rather than empirical results. Introduces a semantic-genotypic readout (nDNA) and the field Neural Genomics, outlining how to inspect ancestry, mutation, and inheritance in model development and deployment.

Conclusion: nDNA offers a new lens to study and govern AI cognition, enabling lineage tracing and drift detection while highlighting the need for empirical validation and cross-disciplinary methods to operationalize these geometric probes.

Abstract: As AI foundation models grow in capability, a deeper question emerges: What
shapes their internal cognitive identity -- beyond fluency and output?
Benchmarks measure behavior, but the soul of a model resides in its latent
geometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic
representation that captures this latent identity through the intrinsic
geometry of belief. At its core, nDNA is synthesized from three principled and
indispensable dimensions of latent geometry: spectral curvature, which reveals
the curvature of conceptual flow across layers; thermodynamic length, which
quantifies the semantic effort required to traverse representational
transitions through layers; and belief vector field, which delineates the
semantic torsion fields that guide a model's belief directional orientations.
Like biological DNA, it encodes ancestry, mutation, and semantic inheritance,
found in finetuning and alignment scars, cultural imprints, and architectural
drift. In naming it, we open a new field: Neural Genomics, where models are not
just tools, but digital semantic organisms with traceable inner cognition.
  Modeling statement. We read AI foundation models as semantic fluid--dynamics:
meaning is transported through layers like fluid in a shaped conduit; nDNA is
the physics-grade readout of that flow -- a geometry-first measure of how
meaning is bent, paid for, and pushed -- yielding a stable, coordinate-free
neural DNA fingerprint tied to on-input behavior; with this fingerprint we
cross into biology: tracing lineages across pretraining, fine-tuning,
alignment, pruning, distillation, and merges; measuring inheritance between
checkpoints; detecting drift as traits shift under new data or objectives; and,
ultimately, studying the evolution of artificial cognition to compare models,
diagnose risks, and govern change over time.

</details>


### [190] [Similarity Field Theory: A Mathematical Framework for Intelligence](https://arxiv.org/abs/2509.18218)
*Kei-Sing Ng*

Main category: cs.AI

TL;DR: Proposes Similarity Field Theory (SFT), a formal framework modeling similarity as a field S over a universe of entities, with evolution, fibers, and a generative operator. It defines intelligence as generating new entities within a fiber, proves two structural theorems, and discusses interpreting large language models (LLMs) through this lens.


<details>
  <summary>Details</summary>
Motivation: Provide a foundational, mathematically rigorous language for describing how similarity relations shape the dynamics of intelligent systems, and to offer a lens for interpreting and probing LLMs and societal cognition.

Method: 1) Define S: U×U → [0,1] with S(E,E)=1, as a directed (not necessarily symmetric) field. 2) Model dynamics via Z_p=(X_p,S^(p)). 3) Define concepts K and associated fibers F_alpha(K) = {E | S(E,K) ≥ alpha}. 4) Use S_K(E)=S(E,K) and a generative operator G that produces new entities. 5) Define intelligence: G is intelligent w.r.t. K if it generates entities in the same fiber. 6) Prove (i) asymmetry blocks mutual inclusion; (ii) stability requires an anchor or confinement to a level set of f. 7) Discuss interpretive uses for LLMs and societal cognition.

Result: A formal theoretical framework: SFT formalizes similarity dynamics, fibers, and a generative intelligence notion; establishes two theorems ensuring constrained, interpretable evolution; positions LLMs as experimental probes within the framework.

Conclusion: SFT offers a foundational language to characterize, compare, and construct intelligent systems via similarity fields; it provides interpretability constraints and a platform to study LLMs and societal cognition through fiber-based generative dynamics.

Abstract: We posit that persisting and transforming similarity relations form the
structural basis of any comprehensible dynamic system. This paper introduces
Similarity Field Theory, a mathematical framework that formalizes the
principles governing similarity values among entities and their evolution. We
define: (1) a similarity field $S: U \times U \to [0,1]$ over a universe of
entities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed
relational field (asymmetry and non-transitivity are allowed); (2) the
evolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by
$p=0,1,2,\ldots$; (3) concepts $K$ as entities that induce fibers
$F_{\alpha}(K) = { E \in U \mid S(E,K) \ge \alpha }$, i.e., superlevel sets of
the unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that
produces new entities. Within this framework, we formalize a generative
definition of intelligence: an operator $G$ is intelligent with respect to a
concept $K$ if, given a system containing entities belonging to the fiber of
$K$, it generates new entities that also belong to that fiber. Similarity Field
Theory thus offers a foundational language for characterizing, comparing, and
constructing intelligent systems. We prove two theorems: (i) asymmetry blocks
mutual inclusion; and (ii) stability requires either an anchor coordinate or
eventual confinement within a level set of $f$. These results ensure that the
evolution of similarity fields is both constrained and interpretable,
culminating in an exploration of how the framework allows us to interpret large
language models and use them as experimental probes into societal cognition.

</details>


### [191] [Multimodal Health Risk Prediction System for Chronic Diseases via Vision-Language Fusion and Large Language Models](https://arxiv.org/abs/2509.18221)
*Dingxin Lu,Shurui Wu,Xinyi Huang*

Main category: cs.AI

TL;DR: VL-RiskFormer is a hierarchical visual-language multimodal Transformer with an LLM head for proactive health risk prediction, integrating cross-modal data and time-aware fusion, achieving AUROC 0.90 on MIMIC-IV.


<details>
  <summary>Details</summary>
Motivation: The growing global burden of chronic diseases and the heterogeneity of clinical data create a need for a unified multimodal AI framework that can proactively predict individual health risks.

Method: Four innovations: (i) pretraining with cross-modal comparison and fine-grained alignment of radiology images, fundus maps, and wearable photos with clinical narratives using momentum encoders and debiased InfoNCE losses; (ii) time fusion block integrating irregular visit sequences into a causal Transformer decoder via adaptive time interval position coding; (iii) disease ontology map adapter injecting ICD-10 codes into visual and textual channels in layers with a graph attention mechanism to infer comorbid patterns; (iv) an LLM inference head embedded in the top layer; evaluation on the MIMIC-IV longitudinal cohort.

Result: Average AUROC of 0.90 with an expected calibration error of 2.7% on the MIMIC-IV longitudinal cohort.

Conclusion: VL-RiskFormer demonstrates effective integration of multimodal data and ontology-informed inference, achieving strong predictive performance and calibration; this approach advances proactive health risk prediction and multimodal health AI.

Abstract: With the rising global burden of chronic diseases and the multimodal and
heterogeneous clinical data (medical imaging, free-text recordings, wearable
sensor streams, etc.), there is an urgent need for a unified multimodal AI
framework that can proactively predict individual health risks. We propose
VL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer
with a large language model (LLM) inference head embedded in its top layer. The
system builds on the dual-stream architecture of existing visual-linguistic
models (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with
cross-modal comparison and fine-grained alignment of radiological images,
fundus maps, and wearable device photos with corresponding clinical narratives
using momentum update encoders and debiased InfoNCE losses; (ii) a time fusion
block that integrates irregular visit sequences into the causal Transformer
decoder through adaptive time interval position coding; (iii) a disease
ontology map adapter that injects ICD-10 codes into visual and textual channels
in layers and infers comorbid patterns with the help of a graph attention
mechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an
average AUROC of 0.90 with an expected calibration error of 2.7 percent.

</details>


### [192] [From "What to Eat?" to Perfect Recipe: ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation](https://arxiv.org/abs/2509.18226)
*Yu Fu,Linyue Cai,Ruoyu Wu,Yong Zhao*

Main category: cs.AI

TL;DR: Hybrid ChefMind architecture for personalized recipe recommendation improves accuracy, relevance, and robustness against fuzzy queries by integrating Chain of Exploration, Knowledge Graph, Retrieval-Augmented Generation, and an LLM; achieves higher scores on Xiachufang dataset with low unprocessed query rate.


<details>
  <summary>Details</summary>
Motivation: Address fuzzy user intents, semantic accuracy, and detail coverage in recipe recommendations; enhance interpretability and robustness.

Method: CoE refines queries into structured conditions; KG provides semantic reasoning; RAG adds contextual culinary details; LLM integrates outputs into coherent recommendations. Evaluation against LLM-only, KG-only, RAG-only baselines on Xiachufang and manually annotated queries.

Result: Avg score 8.7 vs 6.4-6.7 for ablations; unprocessed queries 1.6%; improved accuracy, relevance, completeness, clarity.

Conclusion: Hybrid architecture effectively combines modules to boost recommendation quality and robustness for fuzzy demands; promising approach for restaurant/recipe recommender systems.

Abstract: Personalized recipe recommendation faces challenges in handling fuzzy user
intent, ensuring semantic accuracy, and providing sufficient detail coverage.
We propose ChefMind, a hybrid architecture combining Chain of Exploration
(CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large
Language Model (LLM). CoE refines ambiguous queries into structured conditions,
KG offers semantic reasoning and interpretability, RAG supplements contextual
culinary details, and LLM integrates outputs into coherent recommendations. We
evaluate ChefMind on the Xiachufang dataset and manually annotated queries,
comparing it with LLM-only, KG-only, and RAG-only baselines. Results show that
ChefMind achieves superior performance in accuracy, relevance, completeness,
and clarity, with an average score of 8.7 versus 6.4-6.7 for ablation models.
Moreover, it reduces unprocessed queries to 1.6%, demonstrating robustness in
handling fuzzy demands.

</details>


### [193] [An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems](https://arxiv.org/abs/2509.18229)
*Anthony Patera,Rohan Abeyaratne*

Main category: cs.AI

TL;DR: Proposes an N-Plus-1 GPT Agency that ensembles N independent problem-solve realizations and uses an Agent Compare aggregator to yield a recommended solution for mechanical engineering problems. It relies on Condorcet's Jury Theorem to argue high probability of correctness with large N and p>0.5, supports multiple model interpretations, and emphasizes transparency and pedagogy, differentiating from Grok Heavy.


<details>
  <summary>Details</summary>
Motivation: Single GPT solutions for mechanical engineering tasks can be unreliable (e.g., 85% success) and insufficient for education or practice. The work advocates a low-cost, ensemble-based, transparent analysis framework to improve reliability and educational value.

Method: Generate N independent Agent Solve realizations for a given problem; use Agent Compare to summarize, contrast, and select a Recommended Problem Solution; accommodate Secondary (Agent Compare) solutions representing alternative mathematical models or solution procedures; justify via Condorcet's Jury Theorem (p>0.5 and large N); compare to Grok Heavy, noting similarities and differences in emphasis (transparency and pedagogy).

Result: The abstract centers on a theoretical and design framework rather than empirical results. It argues that an ensemble approach can improve reliability for high per-solve accuracy and provides a mechanism to incorporate multiple interpretations; it also positions the method relative to Grok Heavy and highlights pedagogical transparency as a key strength.

Conclusion: An N-Plus-1 Agency provides a low-cost, reliable, and transparent initial analysis framework for mechanical engineering problems by leveraging ensemble reasoning and multiple models, with strong pedagogical value and Condorcet-based justification for correctness as N grows.

Abstract: Generative AI, and specifically GPT, can produce a remarkable solution to a
mechanical engineering analysis problem - but also, on occasion, a flawed
solution. For example, an elementary mechanics problem is solved flawlessly in
one GPT instance and incorrectly in a subsequent GPT instance, with a success
probability of only 85%. This unreliability renders "out-of-the-box" GPT
unsuitable for deployment in education or engineering practice. We introduce an
"N-Plus-1" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering
Problem Statements. Agency first launches N instantiations of Agent Solve to
yield N independent Proposed Problem Solution Realizations; Agency then invokes
Agent Compare to summarize and compare the N Proposed Problem Solution
Realizations and to provide a Recommended Problem Solution. We argue from
Condorcet's Jury Theorem that, for a Problem Statement characterized by
per-Solve success probability greater than 1/2 (and N sufficiently large), the
Predominant (Agent Compare) Proposed Problem Solution will, with high
probability, correspond to a Correct Proposed Problem Solution. Furthermore,
Agent Compare can also incorporate aspects of Secondary (Agent Compare)
Proposed Problem Solutions, in particular when the latter represent alternative
Problem Statement interpretations - different Mathematical Models - or
alternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a
commercial multi-agent model, show similarities in design and performance, but
also important differences in emphasis: our Agency focuses on transparency and
pedagogical value.

</details>


### [194] [Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces](https://arxiv.org/abs/2509.18230)
*Zihan Dong,Xinyu Fan,Zixiang Tang,Yunqing Li*

Main category: cs.AI

TL;DR: A lightweight, hierarchical reinforcement learning framework (ComputerAgent) for desktop automation uses a two-level option process (manager and subpolicy) with a triple-modal state encoder and early-stop meta-actions, enabling on-device inference with ~15M parameters. It achieves 92.1% success on simple tasks and 58.8% on hard tasks across 135 real-world tasks, matching/exceeding large MLLMs on simple cases while drastically reducing model size and inference time, demonstrating hierarchical RL as a practical alternative to monolithic MLLMs for computer control.


<details>
  <summary>Details</summary>
Motivation: Controlling desktop applications efficiently is challenging due to latency, sample inefficiency on long-horizon tasks, and the difficulty of on-device deployment for large models. A compact, scalable, and responsive approach is needed to automate OS-level tasks without relying on heavy multimodal LLMs.

Method: Formulates OS control as a two-level option process (manager oversees high-level decisions, subpolicy handles actions). Uses a triple-modal state encoder combining screenshots, task IDs, and numeric state to capture visual and contextual information. Introduces meta-actions with an early-stop mechanism to reduce wasted interactions. Employs a compact vision backbone and small policy networks for on-device inference (~15M parameters). Evaluated on 135 real-world desktop tasks.

Result: ComputerAgent achieves 92.1% success on simple tasks (<8 steps) and 58.8% on hard tasks (>=8 steps). It matches or exceeds 200B-parameter MLLM baselines on simple tasks while reducing model size by over 4 orders of magnitude and halving inference time.

Conclusion: Hierarchical RL provides a practical, scalable alternative to monolithic LLM-based automation for computer control, enabling effective on-device desktop automation with substantial efficiency gains.

Abstract: Controlling desktop applications via software remains a fundamental yet
under-served problem. Existing multi-modal large language models (MLLMs) ingest
screenshots and task instructions to generate keystrokes and mouse events, but
they suffer from prohibitive inference latency, poor sample efficiency on
long-horizon sparse-reward tasks, and infeasible on-device deployment. We
introduce a lightweight hierarchical reinforcement learning framework,
ComputerAgent, that formulates OS control as a two-level option process
(manager and subpolicy), employs a triple-modal state encoder (screenshot, task
ID, numeric state) to handle visual and contextual diversity, integrates
meta-actions with an early-stop mechanism to reduce wasted interactions, and
uses a compact vision backbone plus small policy networks for on-device
inference (15M parameters). On a suite of 135 real-world desktop tasks,
ComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on
hard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on
simple scenarios while reducing model size by over four orders of magnitude and
halving inference time. These results demonstrate that hierarchical RL offers a
practical, scalable alternative to monolithic MLLM-based automation for
computer control.

</details>


### [195] [The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks](https://arxiv.org/abs/2509.18234)
*Yu Gu,Jingjing Fu,Xiaodong Liu,Jeya Maria Jose Valanarasu,Noel Codella,Reuben Tan,Qianchu Liu,Ying Jin,Sheng Zhang,Jinyu Wang,Rui Wang,Lei Song,Guanghui Qin,Naoto Usuyama,Cliff Wong,Cheng Hao,Hohin Lee,Praneeth Sanapathi,Sarah Hilado,Bian Jiang,Javier Alvarez-Valle,Mu Wei,Jianfeng Gao,Eric Horvitz,Matt Lungren,Hoifung Poon,Paul Vozila*

Main category: cs.AI

TL;DR: Leaderboard medical benchmarks overstate AI readiness; frontier models show brittleness and shortcut learning under stress, calling into question real-world medical understanding and safety.


<details>
  <summary>Details</summary>
Motivation: Assess whether high benchmark scores reflect genuine medical competence or exploitations of benchmarks; stress-test models to evaluate robustness and alignment with real medical demands.

Method: Evaluate six flagship models across six widely used medical benchmarks, using stress tests (e.g., removing key inputs like images, prompt perturbations) and clinician-guided rubric evaluation to assess true medical understanding.

Result: Top benchmark scores conceal brittleness and shortcut learning; models can guess with missing inputs, flip answers with trivial prompts, and produce convincing yet flawed reasoning. Benchmarks vary in what they measure yet are treated interchangeably, masking failure modes and real-world readiness.

Conclusion: Medical benchmarks are insufficient alone to gauge readiness for clinical deployment; robust evaluation, accountability for reasoning quality, and alignment with authentic medical demands are essential for trusted healthcare AI.

Abstract: Large frontier models like GPT-5 now achieve top scores on medical
benchmarks. But our stress tests tell a different story. Leading systems often
guess correctly even when key inputs like images are removed, flip answers
under trivial prompt changes, and fabricate convincing yet flawed reasoning.
These aren't glitches; they expose how today's benchmarks reward test-taking
tricks over medical understanding. We evaluate six flagship models across six
widely used benchmarks and find that high leaderboard scores hide brittleness
and shortcut learning. Through clinician-guided rubric evaluation, we show that
benchmarks vary widely in what they truly measure yet are treated
interchangeably, masking failure modes. We caution that medical benchmark
scores do not directly reflect real-world readiness. If we want AI to earn
trust in healthcare, we must demand more than leaderboard wins and must hold
systems accountable for robustness, sound reasoning, and alignment with real
medical demands.

</details>


### [196] [Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints](https://arxiv.org/abs/2509.18382)
*Adarsha Balaji,Le Chen,Rajeev Thakur,Franck Cappello,Sandeep Madireddy*

Main category: cs.AI

TL;DR: Two compute-constrained strategies for reasoning models: length-controlled CoT via LCPO-based RL and model quantization, examining their safety-efficiency trade-offs.


<details>
  <summary>Details</summary>
Motivation: Test-time compute scaling can boost reasoning performance but at high computational cost; this work seeks affordable, safe alternatives to enable longer chain-of-thought under constraints.

Method: Two approaches: (1) Fine-tune reasoning models using a length controlled policy optimization (LCPO) reinforcement learning method to meet a user-defined CoT length; (2) apply quantization to maximize the generation of CoT sequences within a user-defined compute constraint; and study the resulting impact on model safety.

Result: The abstract does not report empirical results; it outlines proposed methods and aims to analyze the trade-off between computational efficiency and safety.

Conclusion: The work highlights a trade-off between efficiency and safety under compute constraints and suggests evaluating LCPO-based length control and quantization as means to manage this trade-off.

Abstract: Test-time compute scaling has demonstrated the ability to improve the
performance of reasoning language models by generating longer chain-of-thought
(CoT) sequences. However, this increase in performance comes with a significant
increase in computational cost. In this work, we investigate two compute
constraint strategies: (1) reasoning length constraint and (2) model
quantization, as methods to reduce the compute demand of reasoning models and
study their impact on their safety performance. Specifically, we explore two
approaches to apply compute constraints to reasoning models: (1) fine-tuning
reasoning models using a length controlled policy optimization (LCPO) based
reinforcement learning method to satisfy a user-defined CoT reasoning length,
and (2) applying quantization to maximize the generation of CoT sequences
within a user-defined compute constraint. Furthermore, we study the trade-off
between the computational efficiency and the safety of the model.

</details>


### [197] [Gödel Test: Can Large Language Models Solve Easy Conjectures?](https://arxiv.org/abs/2509.18383)
*Moran Feldman,Amin Karbasi*

Main category: cs.AI

TL;DR: A Gödel Test for LLMs: evaluating if a model can produce correct proofs for simple, previously unsolved conjectures; GPT-5 tested on five combinatorial optimization conjectures; mixed results show progress and notable limitations.


<details>
  <summary>Details</summary>
Motivation: To push beyond standard math problems and assess whether frontier LLMs can generate rigorous, correct proofs for new conjectures by synthesizing knowledge across papers; understand limitations in cross-paper reasoning and proof construction.

Method: For five conjectures, present 1–2 source papers; withheld original conjecture; analyze GPT-5's reasoning and proofs in detail; publicly report correctness and refutations; evaluate Problem 2's counterexample and overall cross-paper synthesis.

Result: Three easier problems: nearly correct solutions; Problem 2 produced a different valid solution and refuted a conjecture; Problem 4 failed due to needing cross-paper synthesis; Problem 5 proposed the same algorithm but failed in analysis; overall shows progress in routine reasoning and some originality but clear limits in cross-paper synthesis.

Conclusion: GPT-5 may be an early step toward frontier models passing the Gödel Test; results indicate meaningful progress and indicate where improvements are needed (cross-paper integration, proof verification, handling of multi-paper synthesis).

Abstract: Recent announcements from frontier AI model labs have highlighted strong
results on high-school and undergraduate math competitions. Yet it remains
unclear whether large language models can solve new, simple conjectures in more
advanced areas of mathematics. We propose the G\"odel Test: evaluating whether
a model can produce correct proofs for very simple, previously unsolved
conjectures. To this end, we study the performance of GPT-5 on five conjectures
in combinatorial optimization. For each problem, we provided one or two source
papers from which the conjecture arose, withheld our own conjecture, and then
assessed the model's reasoning in detail. On the three easier problems, GPT-5
produced nearly correct solutions; for Problem 2 it even derived a different
approximation guarantee that, upon checking, refuted our conjecture while
providing a valid solution. The model failed on Problem 4, which required
combining results from two papers. On Problem 5, a harder case without a
validated conjecture, GPT-5 proposed the same algorithm we had in mind but
failed in the analysis, suggesting the proof is more challenging than expected.
Although our sample is small, the results point to meaningful progress on
routine reasoning, occasional flashes of originality, and clear limitations
when cross-paper synthesis is required. GPT-5 may represent an early step
toward frontier models eventually passing the G\"odel Test.

</details>


### [198] [ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification](https://arxiv.org/abs/2509.18400)
*Pritish Yuvraj,Siva Devarakonda*

Main category: cs.AI

TL;DR: First HTS classification benchmark derived from CROSS; Atlas fine-tuned LLaMA-3.3-70B achieves 40% fully correct 10-digit classifications and 57.5% correct 6-digit classifications; cheaper and self-hostable than large baselines; dataset and model released; benchmark is challenging and invites further work.


<details>
  <summary>Details</summary>
Motivation: HTS classification is a critical bottleneck in global trade; misclassification can halt shipments and disrupt compliance. There is a need for a standardized benchmark and efficient models, with privacy-preserving, self-hosted options.

Method: Construct a benchmark for HTS code classification from the US CROSS dataset. Evaluate leading LLMs and a fine-tuned Atlas model (LLaMA-3.3-70B). Compare 10-digit and 6-digit accuracy, assess cost and self-hosting implications, and release both dataset and model to spur future work in retrieval, reasoning, and alignment.

Result: Atlas achieves 40% fully correct 10-digit classifications and 57.5% correct 6-digit classifications. Improvements over GPT-5-Thinking by 15 points and over Gemini-2.5-Pro-Thinking by 27.5 points. Atlas is roughly five times cheaper than GPT-5-Thinking and eight times cheaper than Gemini-2.5-Pro-Thinking, with self-hosting to protect data privacy.

Conclusion: HTS classification remains challenging, with only 40% 10-digit accuracy. The work establishes a new community benchmark, releasing dataset and model to foster future research in retrieval, reasoning, and alignment for high-stakes trade and compliance workflows.

Abstract: Accurate classification of products under the Harmonized Tariff Schedule
(HTS) is a critical bottleneck in global trade, yet it has received little
attention from the machine learning community. Misclassification can halt
shipments entirely, with major postal operators suspending deliveries to the
U.S. due to incomplete customs documentation. We introduce the first benchmark
for HTS code classification, derived from the U.S. Customs Rulings Online
Search System (CROSS). Evaluating leading LLMs, we find that our fine-tuned
Atlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit
classifications and 57.5 percent correct 6-digit classifications, improvements
of 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking.
Beyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and
eight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to
guarantee data privacy in high-stakes trade and compliance workflows. While
Atlas sets a strong baseline, the benchmark remains highly challenging, with
only 40 percent 10-digit accuracy. By releasing both dataset and model, we aim
to position HTS classification as a new community benchmark task and invite
future work in retrieval, reasoning, and alignment.

</details>


### [199] [Instruction-Following Evaluation in Function Calling for Large Language Models](https://arxiv.org/abs/2509.18420)
*Nikolai Skripko*

Main category: cs.AI

TL;DR: A new benchmark, IFEval-FC, tests strict formatting adherence in function calling for LLMs, revealing that even top models struggle with explicit formatting constraints; includes 750 test cases with formats embedded in JSON schemas; evaluation is algorithmic and fully reproducible; code/data are publicly available.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks check argument correctness but ignore strict formatting instructions embedded in parameter descriptions. Real-world AI agents require exact formats (e.g., quotes, date formats). A objective, scalable evaluation is needed to measure this capability.

Method: Encode verifiable formats directly within JSON schema descriptions for each test case, such as "values must not contain punctuation." Each test case includes a function description with an embedded required format for one input parameter and a corresponding user query. There are 750 test cases. Evaluation is algorithmic, ensuring objectivity, reproducibility, and scalability.

Result: State-of-the-art proprietary models (including GPT-5 and Claude 4.1 Opus) frequently fail to follow basic formatting rules, highlighting a practical limitation for real-world agent systems.

Conclusion: The benchmark highlights a gap in current models’ ability to adhere to explicit formatting instructions in function calling. It provides public code/data for broader use and future improvements in evaluating instruction-following in function calling.

Abstract: Function calling is a core capability of large language models, essential for
AI agents. Existing benchmarks such as the Berkeley Function Calling
Leaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench
(arXiv:2501.12851) evaluate argument correctness but do not test adherence to
format instructions embedded in parameter descriptions, such as enclosing
values in double quotes or using ISO date formats.
  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)
that assesses precise instruction following in function calling. IFEval-FC
encodes verifiable formats directly within JSON schema descriptions, for
example specifying that a value must not contain punctuation. It includes 750
test cases, each consisting of a function with an embedded format for one of
its input parameters and a corresponding user query. Evaluation is fully
algorithmic, ensuring objectivity, reproducibility, and scalability.
  Our results show that even state-of-the-art proprietary models, including
GPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,
highlighting a practical limitation for real-world agent systems. The complete
codebase and data are publicly available at
https://github.com/Skripkon/IFEval-FC.

</details>


### [200] [Memory-QA: Answering Recall Questions Based on Multimodal Memories](https://arxiv.org/abs/2509.18436)
*Hongda Jiang,Xinyuan Zhang,Siddhant Garg,Rishab Arora,Shiun-Zu Kuo,Jiayang Xu,Christopher Brossman,Yue Liu,Aaron Colak,Ahmed Aly,Anuj Kumar,Xin Luna Dong*

Main category: cs.AI

TL;DR: Introduces Memory-QA task and Pensieve pipeline to answer recall questions from stored multimodal memories; demonstrates gains on a multimodal benchmark, up to 14% QA accuracy over SOTA.


<details>
  <summary>Details</summary>
Motivation: Addresses practical need to reason over stored visual memories using temporal/location cues and multiple memories, enabling recall-based QA in real-world multimodal systems.

Method: Proposes Pensieve: memory-specific augmentation, time- and location-aware multi-signal retrieval, and multi-memory QA fine-tuning; accompanied by a multimodal benchmark illustrating real challenges.

Result: Pensieve surpasses state-of-the-art methods on the benchmark, achieving up to 14% improvements in QA accuracy.

Conclusion: Memory-aware retrieval and multi-memory QA training are effective for real-world recall tasks; the benchmark highlights core challenges and the pipeline offers a viable solution.

Abstract: We introduce Memory-QA, a novel real-world task that involves answering
recall questions about visual content from previously stored multimodal
memories. This task poses unique challenges, including the creation of
task-oriented memories, the effective utilization of temporal and location
information within memories, and the ability to draw upon multiple memories to
answer a recall question. To address these challenges, we propose a
comprehensive pipeline, Pensieve, integrating memory-specific augmentation,
time- and location-aware multi-signal retrieval, and multi-memory QA
fine-tuning. We created a multimodal benchmark to illustrate various real
challenges in this task, and show the superior performance of Pensieve over
state-of-the-art solutions (up to 14% on QA accuracy).

</details>


### [201] [FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning](https://arxiv.org/abs/2509.18527)
*Ziwen Chen,Zhong Wang*

Main category: cs.AI

TL;DR: FERA introduces an AI referee for foil fencing that fuses pose-based action recognition with rule-based reasoning, using 2D joint data and a 101-d feature set fed into a Transformer for move and blade classification, plus a distilled language model encoding right-of-way rules to decide scores and explanations. 5-fold cross-validation yields macro-F1 of 0.549, outperforming several baselines; not deployment-ready but showing promise for automated refereeing and coaching applications.


<details>
  <summary>Details</summary>
Motivation: Refereeing in fencing suffers from subjectivity, human error, bias, and limited access to practice environments. An AI assistant could improve consistency and availability of referee support and enable coaching.

Method: Extract 2D pose from video and normalize; compute a 101-dimensional kinematic feature vector; use a Transformer to perform multi-label classification of moves and blade actions; apply a distilled language model encoding right-of-way rules to determine exchange priority and scoring; provide decision plus explanation for each exchange; evaluate with Limited hand-labeled data using 5-fold cross-validation.

Result: Macro-F1 score of 0.549 on 5-fold cross-validation, outperforming baselines (TCN, BiLSTM, vanilla Transformer) with limited data.

Conclusion: FERA demonstrates a promising direction for automated referee assistance and coaching in foil fencing, showing that a combination of pose-based features and rule-based reasoning can achieve competitive performance with limited labeled data; further work needed before deployment.

Abstract: The sport of fencing, like many other sports, faces challenges in refereeing:
subjective calls, human errors, bias, and limited availability in practice
environments. We present FERA (Fencing Referee Assistant), a prototype AI
referee for foil fencing which integrates pose-based multi-label action
recognition and rule-based reasoning. FERA extracts 2D joint positions from
video, normalizes them, computes a 101-dimensional kinematic feature set, and
applies a Transformer for multi-label move and blade classification. To
determine priority and scoring, FERA applies a distilled language model with
encoded right-of-way rules, producing both a decision and an explanation for
each exchange. With limited hand-labeled data, a 5-fold cross-validation
achieves an average macro-F1 score of 0.549, outperforming multiple baselines,
including a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla
Transformer. While not ready for deployment, these results demonstrate a
promising path towards automated referee assistance in foil fencing and new
opportunities for AI applications, such as coaching in the field of fencing.

</details>


### [202] [LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs](https://arxiv.org/abs/2509.18557)
*Tom Pawelek,Raj Patel,Charlotte Crowell,Noorbakhsh Amiri,Sudip Mittal,Shahram Rahimi,Andy Perkins*

Main category: cs.AI

TL;DR: Proposes LLMZ+ prompt-whitelisting to secure agentic LLMs by filtering messages to predefined safe use cases, achieving zero false positives/negatives in experiments.


<details>
  <summary>Details</summary>
Motivation: Agentic AI presents high-value attack surface due to privileged data access and nondeterministic goal-directed behavior; traditional detection-based defenses struggle against jailbreaks and prompt injections; need a scalable, resilience-focused security approach that reduces operational overhead.

Method: Introduce LLMZ+ prompt whitelisting: enforce a context-specific policy so that only messages within predefined, safe use cases interact with the agentic LLM; evaluate resilience against jailbreaks and measure false positive/negative rates.

Result: LLMZ+ shows strong resilience against common jailbreak prompts; legitimate business communications remain uninterrupted; false positive and false negative rates reduced to 0 in the experimental setting.

Conclusion: A context-driven prompt whitelist can streamline security for agentic LLMs, enhancing long-term resilience and reducing resource requirements, and can complement existing detection-based defenses.

Abstract: Compared to traditional models, agentic AI represents a highly valuable
target for potential attackers as they possess privileged access to data
sources and API tools, which are traditionally not incorporated into classical
agents. Unlike a typical software application residing in a Demilitarized Zone
(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI
(only defining a final goal, leaving the path selection to LLM). This
characteristic introduces substantial security risk to both operational
security and information security. Most common existing defense mechanism rely
on detection of malicious intent and preventing it from reaching the LLM agent,
thus protecting against jailbreak attacks such as prompt injection. In this
paper, we present an alternative approach, LLMZ+, which moves beyond
traditional detection-based approaches by implementing prompt whitelisting.
Through this method, only contextually appropriate and safe messages are
permitted to interact with the agentic LLM. By leveraging the specificity of
context, LLMZ+ guarantees that all exchanges between external users and the LLM
conform to predefined use cases and operational boundaries. Our approach
streamlines the security framework, enhances its long-term resilience, and
reduces the resources required for sustaining LLM information security. Our
empirical evaluation demonstrates that LLMZ+ provides strong resilience against
the most common jailbreak prompts. At the same time, legitimate business
communications are not disrupted, and authorized traffic flows seamlessly
between users and the agentic LLM. We measure the effectiveness of approach
using false positive and false negative rates, both of which can be reduced to
0 in our experimental setting.

</details>


### [203] [Solving Math Word Problems Using Estimation Verification and Equation Generation](https://arxiv.org/abs/2509.18565)
*Mitchell Piehl,Dillon Wilson,Ananya Kalita,Jugal Kalita*

Main category: cs.AI

TL;DR: A decomposition-based prompting approach combined with an external solver and a verification loop to solve math word problems, achieving state-of-the-art results and introducing new datasets.


<details>
  <summary>Details</summary>
Motivation: LLMs often struggle with math word problems due to the need for multi-step reasoning and diverse mathematical abilities. The work aims to improve accuracy and generalization by guiding the model to decompose questions, verify solutions via estimation, and iteratively rectify errors.

Method: 1) Prompt the LLM to decompose a word problem into equations. 2) Use an external symbolic equation solver to obtain an answer. 3) Have the LLM re-solve the problem with the objective of estimating the correct answer (not exact solve) for verification, following a math-teacher inspired cue. 4) Compare estimation with the generated result; if mismatch, apply an iterative rectification loop to reach the correct answer.

Result: Achieves new state-of-the-art results on numeric and algebraic MWPs, improving the previous best by about 2 percentage points on average. Demonstrates satisfactory performance on trig MWPs (a task not previously attempted). Introduces two new datasets, SVAMPClean and Trig300, for broader testing of LLM reasoning on MWPs.

Conclusion: The approach validates that a decomposition-driven prompting plus external solving, coupled with a teacher-inspired verification step and iterative correction, can significantly enhance MWP solving, extend to trig domains, and provide valuable new benchmarks for evaluating LLMs' mathematical reasoning.

Abstract: Large Language Models (LLMs) excel at various tasks, including
problem-solving and question-answering. However, LLMs often find Math Word
Problems (MWPs) challenging because solving them requires a range of reasoning
and mathematical abilities with which LLMs seem to struggle. Recent efforts
have helped LLMs solve more complex MWPs with improved prompts. This study
proposes a novel method that initially prompts an LLM to create equations from
a decomposition of the question, followed by using an external symbolic
equation solver to produce an answer. To ensure the accuracy of the obtained
answer, inspired by an established recommendation of math teachers, the LLM is
instructed to solve the MWP a second time, but this time with the objective of
estimating the correct answer instead of solving it exactly. The estimation is
then compared to the generated answer to verify. If verification fails, an
iterative rectification process is employed to ensure the correct answer is
eventually found. This approach achieves new state-of-the-art results on
datasets used by prior published research on numeric and algebraic MWPs,
improving the previous best results by nearly two percent on average. In
addition, the approach obtains satisfactory results on trigonometric MWPs, a
task not previously attempted to the authors' best knowledge. This study also
introduces two new datasets, SVAMPClean and Trig300, to further advance the
testing of LLMs' reasoning abilities.

</details>


### [204] [Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents](https://arxiv.org/abs/2509.18633)
*Yara Mohajerani*

Main category: cs.AI

TL;DR: Geospatial ABM integrating climate hazards with evolutionary learning to study adaptive firm behavior, showing production convergence but higher prices under RCP8.5; open-source risk tool.


<details>
  <summary>Details</summary>
Motivation: Climate risk assessment needs to capture complex, spatially heterogeneous hazards and adaptive economic responses. The abstract motivates integrating geospatial hazard data with economic agents that learn and adapt to ongoing climate stress to better quantify both direct and cascading risks.

Method: Develop a Mesa-based geospatial ABM that integrates CLIMADA climate impact data, with evolutionary learning for firms (fitness-based selection and mutation) governing budget allocation, pricing, wages, and risk adaptation. Demonstrates riverine flood projections under RCP8.5 to 2100 and evaluates outcomes.

Result: Evolutionary adaptation enables firms to converge toward baseline production after decades of disruption. Systemic risks emerge through supply-chain linkages, with end-of-century goods prices rising by 5.6% under RCP8.5 compared to baseline.

Conclusion: An open-source framework for financial institutions and firms to quantify direct and cascading climate risks and to evaluate cost-effective adaptation strategies.

Abstract: Climate risk assessment requires modelling complex interactions between
spatially heterogeneous hazards and adaptive economic systems. We present a
novel geospatial agent-based model that integrates climate hazard data with
evolutionary learning for economic agents. Our framework combines Mesa-based
spatial modelling with CLIMADA climate impact assessment, introducing adaptive
learning behaviours that allow firms to evolve strategies for budget
allocation, pricing, wages, and risk adaptation through fitness-based selection
and mutation. We demonstrate the framework using riverine flood projections
under RCP8.5 until 2100, showing that evolutionary adaptation enables firms to
converge with baseline (no hazard) production levels after decades of
disruption due to climate stress. Our results reveal systemic risks where even
agents that are not directly exposed to floods face impacts through supply
chain disruptions, with the end-of-century average price of goods 5.6% higher
under RCP8.5 compared to the baseline. This open-source framework provides
financial institutions and companies with tools to quantify both direct and
cascading climate risks while evaluating cost-effective adaptation strategies.

</details>


### [205] [TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2509.18667)
*Qiao Xiao,Hong Ting Tsang,Jiaxin Bai*

Main category: cs.AI

TL;DR: TERAG reduces token cost in graph-based RAG by applying PPR-based retrieval to build compact, informative graphs, achieving around 80% of standard RAG accuracy with only 3–11% of output tokens.


<details>
  <summary>Details</summary>
Motivation: Graph-based RAG methods incur high LLM token costs during graph construction, hindering large-scale deployment. There is a need for cost-efficient approaches that maintain strong reasoning and factuality.

Method: TERAG integrates Personalized PageRank (PPR) during the retrieval phase (inspired by HippoRAG) to guide the construction of informative graphs with significantly fewer tokens, thereby lowering output token usage while preserving accuracy.

Result: Empirically, TERAG attains at least 80% of the accuracy of widely used graph-based RAG methods while consuming only 3–11% of the output tokens.

Conclusion: TERAG offers a simple, effective cost-saving framework for graph-based RAG, enabling more scalable deployment without substantial loss in performance; it highlights the value of PPR-guided graph construction in reducing LLM token usage.

Abstract: Graph-based Retrieval-augmented generation (RAG) has become a widely studied
approach for improving the reasoning, accuracy, and factuality of Large
Language Models. However, many existing graph-based RAG systems overlook the
high cost associated with LLM token usage during graph construction, hindering
large-scale adoption. To address this, we propose TERAG, a simple yet effective
framework designed to build informative graphs at a significantly lower cost.
Inspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the
retrieval phase, and we achieve at least 80% of the accuracy of widely used
graph-based RAG methods while consuming only 3%-11% of the output tokens.

</details>


### [206] [Implementation of airborne ML models with semantics preservation](https://arxiv.org/abs/2509.18681)
*Nicolas Valot,Louis Fabre,Benjamin Lesage,Ammar Mechouche,Claire Pagetti*

Main category: cs.AI

TL;DR: Introduces Machine Learning Model Description (MLMD) and a refined notion of semantics preservation to enable faithful replication and regulatory compliance of ML in airborne systems; demonstrated on multiple industrial use cases.


<details>
  <summary>Details</summary>
Motivation: Guarantee the safe operation of ML-based airborne systems and demonstrate compliance with EU aviation safety guidance (EASA, ED-324); clarify the distinction between the ML model and its unambiguous description to facilitate verification and certification.

Method: Define MLMD as an unambiguous, verifiable description of a machine learning model; refine the concept of semantics preservation to ensureaccurate replication of the model; apply these concepts to several industrial use cases to build and compare multiple target models.

Result: Provided definitions for MLMD and semantics preservation; proposed a practical framework for faithful model replication; demonstrated applicability by constructing and comparing several target models across multiple industrial use cases.

Conclusion: MLMD and semantics preservation offer a concrete approach to regulatory-ready ML in avionics, aiding replication, verification, and compliance with current and forthcoming guidance; further work should broaden adoption and integration with EASA/ED-324 workflows.

Abstract: Machine Learning (ML) may offer new capabilities in airborne systems.
However, as any piece of airborne systems, ML-based systems will be required to
guarantee their safe operation. Thus, their development will have to be
demonstrated to be compliant with the adequate guidance. So far, the European
Union Aviation Safety Agency (EASA) has published a concept paper and an
EUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level
objectives to confirm the ML model achieves its intended function and maintains
training performance in the target environment. The paper aims to clarify the
difference between an ML model and its corresponding unambiguous description,
referred to as the Machine Learning Model Description (MLMD). It then refines
the essential notion of semantics preservation to ensure the accurate
replication of the model. We apply our contributions to several industrial use
cases to build and compare several target models.

</details>


### [207] [Advances in Large Language Models for Medicine](https://arxiv.org/abs/2509.18690)
*Zhiyu Kan,Wensheng Gan,Zhenlian Qi,Philip S. Yu*

Main category: cs.AI

TL;DR: Systematic review of medical LLMs: surveys training methods, adaptation in healthcare, and applications; introduces a three-type taxonomy of medical LLMs by training and a two-category evaluation framework; discusses strengths/limitations and offers future directions.


<details>
  <summary>Details</summary>
Motivation: Rising impact of LLMs in medicine creates a need to synthesize current progress, understand training/adaptation challenges, and guide development and evaluation of medical LLMs.

Method: Comprehensive literature review and synthesis; categorizes medical LLMs into three training-based types; classifies evaluation approaches into two categories; analyzes current progress, strengths, limitations, and challenges.

Result: Proposes a structured framework for categorizing medical LLMs and evaluating them; identifies key gaps and practical recommendations to advance research and deployment in healthcare.

Conclusion: Medical LLMs show significant promise but require robust training methods, context-aware adaptation, rigorous evaluation, and governance; the paper provides guidance for researchers and practitioners to advance the field.

Abstract: Artificial intelligence (AI) technology has advanced rapidly in recent years,
with large language models (LLMs) emerging as a significant breakthrough. LLMs
are increasingly making an impact across various industries, with the medical
field standing out as the most prominent application area. This paper
systematically reviews the up-to-date research progress of LLMs in the medical
field, providing an in-depth analysis of training techniques for large medical
models, their adaptation in healthcare settings, related applications, as well
as their strengths and limitations. Furthermore, it innovatively categorizes
medical LLMs into three distinct types based on their training methodologies
and classifies their evaluation approaches into two categories. Finally, the
study proposes solutions to existing challenges and outlines future research
directions based on identified issues in the field of medical LLMs. By
systematically reviewing previous and advanced research findings, we aim to
highlight the necessity of developing medical LLMs, provide a deeper
understanding of their current state of development, and offer clear guidance
for subsequent research.

</details>


### [208] [Autonomous Data Agents: A New Opportunity for Smart Data](https://arxiv.org/abs/2509.18710)
*Yanjie Fu,Dongjie Wang,Wangyang Ying,Xiangliang Zhang,Huan Liu,Jian Pei*

Main category: cs.AI

TL;DR: DataAgents enable autonomous, end-to-end data-to-knowledge systems by combining LLM-based reasoning, task decomposition, action grounding, and tool use to transform diverse data into actionable knowledge; the paper surveys architecture, training, skills, and calls for benchmarks, privacy safeguards, and guardrails.


<details>
  <summary>Details</summary>
Motivation: As data scales in size and complexity, data requires intelligent, scalable processing. Aligning AI with data is crucial because data contains knowledge that AI learns from, yet data is often poorly structured for AI tasks. The question of how much knowledge can be packed into data through intensive operations motivates autonomous data agents.

Method: Propose DataAgents that integrate LLM reasoning with task decomposition, action reasoning and grounding, and tool calling to interpret data task descriptions, decompose tasks, plan workflows, generate/code groundings, and execute operations. Discuss architectural design, training strategies, and the new skills and capabilities enabled.

Result: DataAgents are presented as a paradigm shift enabling dynamic, scalable data workflows that perform collection, integration, preprocessing, selection, transformation, reweighing, augmentation, reprogramming, repairs, and retrieval, transforming complex/unstructured data into coherent knowledge. The paper outlines the convergence of agentic AI with data-to-knowledge systems and investigates design considerations.

Conclusion: DataAgents represent a paradigm shift toward autonomous data-to-knowledge systems. The authors call for research on workflow optimization, open datasets/benchmarks, privacy safeguards, scalability-efficiency balance, and trustworthy guardrails to prevent malicious actions.

Abstract: As data continues to grow in scale and complexity, preparing, transforming,
and analyzing it remains labor-intensive, repetitive, and difficult to scale.
Since data contains knowledge and AI learns knowledge from it, the alignment
between AI and data is essential. However, data is often not structured in ways
that are optimal for AI utilization. Moreover, an important question arises:
how much knowledge can we pack into data through intensive data operations?
Autonomous data agents (DataAgents), which integrate LLM reasoning with task
decomposition, action reasoning and grounding, and tool calling, can
autonomously interpret data task descriptions, decompose tasks into subtasks,
reason over actions, ground actions into python code or tool calling, and
execute operations. Unlike traditional data management and engineering tools,
DataAgents dynamically plan workflows, call powerful tools, and adapt to
diverse data tasks at scale. This report argues that DataAgents represent a
paradigm shift toward autonomous data-to-knowledge systems. DataAgents are
capable of handling collection, integration, preprocessing, selection,
transformation, reweighing, augmentation, reprogramming, repairs, and
retrieval. Through these capabilities, DataAgents transform complex and
unstructured data into coherent and actionable knowledge. We first examine why
the convergence of agentic AI and data-to-knowledge systems has emerged as a
critical trend. We then define the concept of DataAgents and discuss their
architectural design, training strategies, as well as the new skills and
capabilities they enable. Finally, we call for concerted efforts to advance
action workflow optimization, establish open datasets and benchmark ecosystems,
safeguard privacy, balance efficiency with scalability, and develop trustworthy
DataAgent guardrails to prevent malicious actions.

</details>


### [209] [Experience Scaling: Post-Deployment Evolution For Large Language Models](https://arxiv.org/abs/2509.18771)
*Xingkun Yin,Kaibin Huang,Dong In Kim,Hongyang Du*

Main category: cs.AI

TL;DR: Experience scaling enables continuous post-deployment learning for LLMs via autonomous environment interactions, distillation into compact reusable knowledge, and periodic content refresh; validated in simulated scenarios showing improved accuracy, stability over time, and better generalization to novel tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs trained on static data can saturate as human-generated text grows; further gains from scale face diminishing returns. A framework for ongoing learning from deployed experience aims to sustain and extend LLM capabilities.

Method: Capture raw interactions during deployment, distill them into compact reusable knowledge (summaries, rules, embeddings, retrieval content), and periodically refine stored content to preserve relevance and efficiency. Facilitate collaborative sharing of accumulated experience. Validate in simulated real-world scenarios: generalization to unseen but related tasks, repetitive queries, and over-saturated knowledge stores.

Result: Across tested settings, experience scaling improves accuracy, sustains performance over time, and maintains gains when applied to novel situations.

Conclusion: Structured post-deployment learning can extend LLM capabilities beyond static human-generated data, offering a scalable path for continued intelligence progress.

Abstract: Scaling model size, training data, and compute power have driven advances in
large language models (LLMs), but these approaches are reaching saturation as
human-generated text is exhausted and further gains diminish. We propose
experience scaling, a framework for continuous post-deployment evolution for
LLMs through autonomous interaction with the environment and collaborative
sharing of accumulated experience. The framework captures raw interactions,
distills them into compact, reusable knowledge, and periodically refines stored
content to preserve relevance and efficiency. We validate the framework in
simulated real-world scenarios involving generalization to previously unseen
but related tasks, repetitive queries, and over-saturated knowledge stores.
Across all settings, experience scaling improves accuracy, sustains performance
over time, and maintains gains when applied to novel situations. These results
demonstrate that structured post-deployment learning can extend LLM
capabilities beyond the limits of static human-generated data, offering a
scalable path for continued intelligence progress.

</details>


### [210] [The AGNTCY Agent Directory Service: Architecture and Implementation](https://arxiv.org/abs/2509.18787)
*Luca Muscariello,Vijoy Pandey,Ramiz Polic*

Main category: cs.AI

TL;DR: A formal architectural model for the Agent Directory Service (ADS), a distributed, verifiable registry for AI agent capabilities and provenance. Built atop OASF, a Kademlia-based DHT, and OCI/ORAS, it enables content-addressed storage, schema-driven extensibility, and Sigstore-based provenance for multi-dimensional discovery across heterogeneous MAS.


<details>
  <summary>Details</summary>
Motivation: The need for scalable, verifiable, and interoperable discovery of AI agent capabilities, metadata, and provenance across diverse multi-agent systems, with decoupled indexing and content location and strong provenance guarantees.

Method: Two-level mapping over a Kademlia-based DHT for capability indexing and content addressing; reuse of OCI/ORAS for artifact distribution; integration of Sigstore for provenance; use of the Open Agentic Schema Framework (OASF) for schema-driven extensibility; formal architectural model and descriptions of storage, discovery layers, and security/performance properties.

Result: Formal architectural model of ADS, descriptions of storage and discovery layers, and analysis of security and performance properties; positioning of ADS within the landscape of emerging agent registry and interoperability initiatives.

Conclusion: ADS provides efficient, verifiable, multi-dimensional discovery of AI agent capabilities and provenance, leveraging established standards and infrastructure to enable interoperability across heterogeneous MAS and future agent modalities.

Abstract: The Agent Directory Service (ADS) is a distributed directory for the
discovery of AI agent capabilities, metadata, and provenance. It leverages
content-addressed storage, hierarchical taxonomies, and cryptographic signing
to enable efficient, verifiable, and multi-dimensional discovery across
heterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema
Framework (OASF), ADS decouples capability indexing from content location
through a two-level mapping realized over a Kademlia-based Distributed Hash
Table (DHT). It reuses mature OCI / ORAS infrastructure for artifact
distribution, integrates Sigstore for provenance, and supports schema-driven
extensibility for emerging agent modalities (LLM prompt agents, MCP servers,
A2A-enabled components). This paper formalizes the architectural model,
describes storage and discovery layers, explains security and performance
properties, and positions ADS within the broader landscape of emerging agent
registry and interoperability initiatives.

</details>


### [211] [Bounded PCTL Model Checking of Large Language Model Outputs](https://arxiv.org/abs/2509.18836)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: LLMCHECKER uses PCTL model checking on alpha-k bounded LLM generation to verify properties (quality, biases, consistency) across several models, based on observing that only a small subset of top-k tokens dominate generation.


<details>
  <summary>Details</summary>
Motivation: Formal verification of LLM text generation properties (consistency, safety, bias, quality) is needed; existing methods lack a formal framework for probabilistic dynamics of token generation.

Method: Propose alpha-k bounded text generation focusing on the top-k tokens with maximal cumulative probability above threshold alpha; model generation as a process suitable for PCTL verification; verify properties against an initial string and chosen top-k tokens; accommodate different text evaluation metrics.

Result: Empirical evidence shows that only a limited number of tokens are typically chosen and vary across steps; demonstrates applicability of LLMCHECKER to Llama, Gemma, Mistral, Genstruct, and BERT; claims novelty as the first PCTL-based verification of LLM text generation consistency.

Conclusion: LLMCHECKER enables formal, probabilistic verification of LLM text generation properties under alpha-k bounding; reduces search space and supports cross-model verification for quality, bias, and other textual properties.

Abstract: In this paper, we introduce LLMCHECKER, a model-checking-based verification
method to verify the probabilistic computation tree logic (PCTL) properties of
an LLM text generation process. We empirically show that only a limited number
of tokens are typically chosen during text generation, which are not always the
same. This insight drives the creation of $\alpha$-$k$-bounded text generation,
narrowing the focus to the $\alpha$ maximal cumulative probability on the
top-$k$ tokens at every step of the text generation process. Our verification
method considers an initial string and the subsequent top-$k$ tokens while
accommodating diverse text quantification methods, such as evaluating text
quality and biases. The threshold $\alpha$ further reduces the selected tokens,
only choosing those that exceed or meet it in cumulative probability.
LLMCHECKER then allows us to formally verify the PCTL properties of
$\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in
several LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our
knowledge, this is the first time PCTL-based model checking has been used to
check the consistency of the LLM text generation process.

</details>


### [212] [Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning](https://arxiv.org/abs/2509.18846)
*Hong-Jie Dai,Zheng-Hao Li,An-Tai Lu,Bo-Tsz Shain,Ming-Ta Li,Tatheer Hussain Mir,Kuang-Te Wang,Min-I Su,Pei-Kang Liu,Ming-Ju Tsai*

Main category: cs.AI

TL;DR: A modular, data-refined framework for ICD-10-CM coding that selects base models, removes redundant data, and designs structured prompts; uses LLM-as-judge with Plackett-Luce to rank open-source LLMs; validates on Taiwanese hospital data; fine-tuned base models outperform baselines; more clinical sections boost accuracy; aims for scalable, deployment-ready automated coding.


<details>
  <summary>Details</summary>
Motivation: ICD coding is essential yet labor-intensive and error-prone. LLMs offer automation but suffer from base-model selection, data redundancy, and suboptimal contextual prompting. A principled framework can improve accuracy and enable real-world deployment.

Method: A modular framework combining: (1) LLM-as-judge evaluation protocol with Plackett-Luce aggregation to rank LLMs by ICD-10-CM code comprehension, (2) embedding-based similarity measures to assess semantic overlap, (3) redundancy-aware sampling to remove semantically duplicated discharge summaries, (4) structured discharge summaries from Taiwanese hospitals to study contextual effects, and (5) experiments on two institutional datasets comparing universal vs section-specific modelling paradigms with fine-tuned base models and open-source LLMs.

Result: The chosen base model, after fine-tuning, consistently outperforms baseline LLMs in both internal and external evaluations. Incorporating more clinical sections improves prediction performance. The framework demonstrates that open-source LLMs can yield a scalable, principled approach to ICD-10-CM coding.

Conclusion: A principled, scalable framework that combines informed model selection, data refinement, and context-aware prompting to enable practical ICD-10-CM code prediction using open-source LLMs, suitable for institution-ready deployment.

Abstract: Accurate International Classification of Diseases (ICD) coding is critical
for clinical documentation, billing, and healthcare analytics, yet it remains a
labour-intensive and error-prone task. Although large language models (LLMs)
show promise in automating ICD coding, their challenges in base model
selection, input contextualization, and training data redundancy limit their
effectiveness. We propose a modular framework for ICD-10 Clinical Modification
(ICD-10-CM) code prediction that addresses these challenges through principled
model selection, redundancy-aware data sampling, and structured input design.
The framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce
aggregation to assess and rank open-source LLMs based on their intrinsic
comprehension of ICD-10-CM code definitions. We introduced embedding-based
similarity measures, a redundancy-aware sampling strategy to remove
semantically duplicated discharge summaries. We leverage structured discharge
summaries from Taiwanese hospitals to evaluate contextual effects and examine
section-wise content inclusion under universal and section-specific modelling
paradigms. Experiments across two institutional datasets demonstrate that the
selected base model after fine-tuning consistently outperforms baseline LLMs in
internal and external evaluations. Incorporating more clinical sections
consistently improves prediction performance. This study uses open-source LLMs
to establish a practical and principled approach to ICD-10-CM code prediction.
The proposed framework provides a scalable, institution-ready solution for
real-world deployment of automated medical coding systems by combining informed
model selection, efficient data refinement, and context-aware prompting.

</details>


### [213] [MAPO: Mixed Advantage Policy Optimization](https://arxiv.org/abs/2509.18849)
*Wenke Huang,Quan Zhang,Yiyang Fang,Jian Liang,Xuankun Rong,Huanjin Yao,Guancheng Wan,Ke Liang,Wenwen He,Mingjun Li,Leszek Rutkowski,Mang Ye,Bo Du,Dacheng Tao*

Main category: cs.AI

TL;DR: MAPO adds dynamic, sample-aware reweighting of the GRPO advantage function to handle varying trajectory certainty, introducing advantage percent deviation for high-certainty trajectories and adaptive per-sample weighting to alleviate advantage reversion and mirror, improving reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To fix unstable and misallocated advantage in Group Relative Policy Optimization (GRPO) caused by fixed treatment of trajectories, addressing advantage reversion and mirror that hinder performance on reasoning tasks for foundation models.

Method: Introduce Mixed Advantage Policy Optimization (MAPO). Quantify trajectory certainty and define advantage percent deviation for high-certainty trajectories, then dynamically reweight the advantage function per sample to reflect trajectory certainty and sample characteristics, with ablation studies validating the approach.

Result: MAPO outperforms state-of-the-art methods and maintains robustness across different advantage variants, as demonstrated by comparative results and ablations against existing GRPO strategies.

Conclusion: MAPO offers a simple yet effective strategy to tailor the advantage function to per-sample characteristics, mitigating advantage reversion and mirror and improving GRPO performance on reasoning tasks; the approach is validated by ablations and comparisons with SOTA.

Abstract: Recent advances in reinforcement learning for foundation models, such as
Group Relative Policy Optimization (GRPO), have significantly improved the
performance of foundation models on reasoning tasks. Notably, the advantage
function serves as a central mechanism in GRPO for ranking the trajectory
importance. However, existing explorations encounter both advantage reversion
and advantage mirror problems, which hinder the reasonable advantage allocation
across different query samples. In this work, we propose an easy but effective
GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the
trajectory appears with different certainty and propose the advantage percent
deviation for samples with high-certainty trajectories. Furthermore, we
dynamically reweight the advantage function for samples with varying trajectory
certainty, thereby adaptively configuring the advantage function to account for
sample-specific characteristics. Comparison with related state-of-the-art
methods, along with ablation studies on different advantage variants, validates
the effectiveness of our approach.

</details>


### [214] [Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling](https://arxiv.org/abs/2509.18864)
*Yingxin Li,Jianbo Zhao,Xueyu Ren,Jie Tang,Wangjie You,Xu Chen,Kan Zhou,Chao Feng,Jiao Ran,Yuan Meng,Zhi Wang*

Main category: cs.AI

TL;DR: ProfileBench introduces an industrial benchmark for user profiling with LLMs, derived from real-world video platform data, and funding a two-stage confidence-driven framework (Conf-Profile) to enable label-free, reliable profiling. It synthesizes high-quality labels with confidence hints, uses confidence-weighted voting and calibration, aggregates results into a lightweight LLM, and further improves reasoning via confidence-guided unsupervised reinforcement learning. The result shows notable F1 gain (13.97) on Qwen3-8B.


<details>
  <summary>Details</summary>
Motivation: There is a lack of comprehensive benchmarks for LLM-based user profiling with real-world, heterogeneous data. Building such benchmarks is hard due to scarce ground-truth labels and noisy information, hindering reliable evaluation and model development.

Method: (ProfileBench) a real-world, industrial benchmark with heterogeneous user data and a profiling taxonomy; (Conf-Profile) a two-stage framework that first synthesizes labels from LLMs using confidence hints and applies confidence-weighted voting and calibration, then distills results into a lightweight LLM; further boosts reasoning with confidence-guided unsupervised reinforcement learning using confidence for difficulty filtering, quasi-ground truth voting, and reward weighting.

Result: Conf-Profile achieves substantial gains, reporting a 13.97 F1 improvement on Qwen3-8B, demonstrating the effectiveness of confidence-driven labeling, voting, calibration, and RL-based refinement.

Conclusion: The work advances reliable, scalable user profiling with LLMs by introducing a practical benchmark and a confidence-aware two-stage framework, achieving notable performance gains and paving the way for label-free yet accurate profiling in real-world settings.

Abstract: User profiling, as a core technique for user understanding, aims to infer
structural attributes from user information. Large Language Models (LLMs)
provide a promising avenue for user profiling, yet the progress is hindered by
the lack of comprehensive benchmarks. To bridge this gap, we propose
ProfileBench, an industrial benchmark derived from a real-world video platform,
encompassing heterogeneous user data and a well-structured profiling taxonomy.
However, the profiling task remains challenging due to the difficulty of
collecting large-scale ground-truth labels, and the heterogeneous and noisy
user information can compromise the reliability of LLMs. To approach label-free
and reliable user profiling, we propose a Confidence-driven Profile reasoning
framework Conf-Profile, featuring a two-stage paradigm. We first synthesize
high-quality labels by leveraging advanced LLMs with confidence hints, followed
by confidence-weighted voting for accuracy improvement and confidence
calibration for a balanced distribution. The multiple profile results,
rationales, and confidence scores are aggregated and distilled into a
lightweight LLM. We further enhance the reasoning ability via confidence-guided
unsupervised reinforcement learning, which exploits confidence for difficulty
filtering, quasi-ground truth voting, and reward weighting. Experimental
results demonstrate that Conf-Profile delivers substantial performance through
the two-stage training, improving F1 by 13.97 on Qwen3-8B.

</details>


### [215] [Memory in Large Language Models: Mechanisms, Evaluation and Evolution](https://arxiv.org/abs/2509.18868)
*Dianxing Zhang,Wendong Li,Kani Song,Jiaye Lu,Gang Li,Liuchun Yang,Sheng Li*

Main category: cs.AI

TL;DR: A comprehensive, auditable framework for LLM memory with taxonomy, governance, and multi-layered evaluation to enable reproducible, comparable research and safer deployment.


<details>
  <summary>Details</summary>
Motivation: Motivated by the need to standardize how LLM memory is defined, measured, and governed across heterogeneous pretraining, fine-tuning, and inference setups, ensuring comparability, privacy, and accountability.

Method: Proposes a four-part memory taxonomy (parametric, contextual, external, procedural/episodic) and a memory quadruple (location, persistence, write/access path, controllability). Introduces a three-setting protocol (parametric only, offline retrieval, online retrieval) to decouple capabilities from data availability. Builds a layered evaluation (parametric, contextual, external, procedural/episodic) and governance through leakage auditing and uncertainty reporting. Details an auditable loop for updating/forgetting (DMM Gov coordinating DAPT/TAPT, PEFT, model editing methods like ROME/MEND/MEMIT/SERAC, and RAG). Presents four testable propositions to ground comparison and progress.

Result: A reproducible, comparable coordinate system for LLM memory research and deployment, with integrated governance, leakage auditing, and an auditable update/forget mechanism; enabling standardized evaluation and safer deployment across varied settings.

Conclusion: The framework establishes a unified, testable, and governable foundation for researching and deploying LLM memory, promoting reproducibility, comparability, and responsible governance across different data regimes and time horizons.

Abstract: Under a unified operational definition, we define LLM memory as a persistent
state written during pretraining, finetuning, or inference that can later be
addressed and that stably influences outputs. We propose a four-part taxonomy
(parametric, contextual, external, procedural/episodic) and a memory quadruple
(location, persistence, write/access path, controllability). We link mechanism,
evaluation, and governance via the chain write -> read -> inhibit/update. To
avoid distorted comparisons across heterogeneous setups, we adopt a
three-setting protocol (parametric only, offline retrieval, online retrieval)
that decouples capability from information availability on the same data and
timeline. On this basis we build a layered evaluation: parametric (closed-book
recall, edit differential, memorization/privacy), contextual (position curves
and the mid-sequence drop), external (answer correctness vs snippet
attribution/faithfulness), and procedural/episodic (cross-session consistency
and timeline replay, E MARS+). The framework integrates temporal governance and
leakage auditing (freshness hits, outdated answers, refusal slices) and
uncertainty reporting via inter-rater agreement plus paired tests with
multiple-comparison correction. For updating and forgetting, we present DMM
Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),
and RAG to form an auditable loop covering admission thresholds, rollout,
monitoring, rollback, and change audits, with specs for timeliness, conflict
handling, and long-horizon consistency. Finally, we give four testable
propositions: minimum identifiability; a minimal evaluation card; causally
constrained editing with verifiable forgetting; and when retrieval with
small-window replay outperforms ultra-long-context reading. This yields a
reproducible, comparable, and governable coordinate system for research and
deployment.

</details>


### [216] [LongCat-Flash-Thinking Technical Report](https://arxiv.org/abs/2509.18883)
*Meituan LongCat Team,Anchun Gui,Bei Li,Bingyang Tao,Bole Zhou,Borun Chen,Chao Zhang,Chao Zhang,Chengcheng Han,Chenhui Yang,Chi Zhang,Chong Peng,Chuyu Zhang,Cong Chen,Fengcun Li,Gang Xu,Guoyuan Lin,Hao Jiang,Hao Liang,Haomin Fu,Haoxiang Ma,Hong Liu,Hongyan Hao,Hongyin Tang,Hongyu Zang,Hongzhi Ni,Hui Su,Jiahao Liu,Jiahuan Li,Jialin Liu,Jianfei Zhang,Jianhao Xu,Jianing Wang,Jiaqi Sun,Jiaqi Zhang,Jiarong Shi,Jiawei Yang,Jingang Wang,Jinrui Ding,Jun Kuang,Jun Xu,Ke He,Kefeng Zhang,Keheng Wang,Keqing He,Li Wei,Liang Shi,Lin Qiu,Lingbin Kong,Lingchuan Liu,Linsen Guo,Longfei An,Mai Xia,Meng Zhou,Mengshen Zhu,Peng Pei,Pengcheng Jia,Qi Gu,Qi Guo,Qiong Huang,Quan Chen,Quanchi Weng,Rongxiang Weng,Ruichen Shao,Rumei Li,Shanglin Lei,Shuai Du,Shuaikang Liu,Shuang Zhou,Shuhao Hu,Siyu Xu,Songshan Gong,Tao Liang,Tianhao Hu,Wei He,Wei Shi,Wei Wang,Wei Wu,Wei Zhuo,Weifeng Tang,Wenjie Shi,Wenlong Zhu,Xi Su,Xiangcheng Liu,Xiangyu Xi,Xiangzhou Huang,Xiao Liu,Xiaochen Jiang,Xiaowei Shi,Xiaowen Shi,Xiaoyu Li,Xin Chen,Xinyue Zhao,Xuan Huang,Xuemiao Zhang,Xuezhi Cao,Xunliang Cai,Yajie Zhang,Yang Chen,Yang Liu,Yang Liu,Yang Zheng,Yaoming Wang,Yaqi Huo,Yerui Sun,Yifan Lu,Yiyang Li,Youshao Xiao,Yuanzhe Lei,Yuchen Xie,Yueqing Sun,Yufei Zhang,Yuhuai Wei,Yulei Qian,Yunke Zhao,Yuqing Ding,Yuwei Jiang,Zhaohua Yang,Zhengyu Chen,Zhijian Liu,Zhikang Xia,Zhongda Su,Ziran Li,Ziwen Wang,Ziyuan Zhuang,Zongyu Wang,Zunyuan Yang*

Main category: cs.AI

TL;DR: Proposes LongCat-Flash-Thinking, a 560B open-source Mixture-of-Experts model for reasoning, trained via cold-start long chain-of-thought, domain-specific expert training fused into one model, and accelerated by DORA RL—achieving state-of-the-art open-source results and reduced agentic-token usage on AIME-25.


<details>
  <summary>Details</summary>
Motivation: Improve open-source reasoning capabilities and agentic AI efficiency by combining domain-specialized MoE experts with scalable RL, addressing both reasoning quality and token efficiency in agentic tasks.

Method: Cold-start long CoT data; domain-parallel MoE training decoupling optimization across domains (STEM, Code, Agentic) with subsequent fusion of experts; Dynamic Orchestration for Async rollout (DORA) enabling >3x speedups over synchronous RL on large accelerator fleets.

Result: Achieves state-of-the-art performance among open-source models on complex reasoning tasks; exhibits 64.5% reduction in average token consumption for agentic reasoning on AIME-25 without loss of accuracy.

Conclusion: Release of LongCat-Flash-Thinking to advance reasoning systems and agentic AI research; demonstrates practical benefits of domain-specific MoEs and asynchronous RL optimization for large-scale reasoning models.

Abstract: We present LongCat-Flash-Thinking, an efficient 560-billion-parameter
open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities
are cultivated through a meticulously crafted training process, beginning with
long Chain-of-Thought (CoT) data cold-start and culminating in large-scale
Reinforcement Learning (RL). We first employ a well-designed cold-start
training strategy, which significantly enhances the reasoning potential and
equips the model with specialized skills in both formal and agentic reasoning.
Then, a core innovation is our domain-parallel training scheme, which decouples
optimization across distinct domains (e.g., STEM, Code, Agentic) and
subsequently fuses the resulting expert models into a single, nearly
Pareto-optimal model. This entire process is powered by our Dynamic
ORchestration for Asynchronous rollout (DORA) system, a large-scale RL
framework that delivers a greater than threefold training speedup over
synchronous methods on tens of thousands of accelerators. As a result,
LongCat-Flash-Thinking achieves state-of-the-art performance among open-source
models on a suite of complex reasoning tasks. The model exhibits exceptional
efficiency in agentic reasoning, reducing average token consumption by 64.5%
(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We
release LongCat-Flash-Thinking to promote further advances in reasoning systems
and agentic AI research.

</details>


### [217] [How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective](https://arxiv.org/abs/2509.18905)
*Songsong Yu,Yuxin Chen,Hao Ju,Lianjie Jia,Fuxi Zhang,Shaofei Huang,Yuhan Wu,Rundi Cui,Binghao Ran,Zaibin Zhang,Zhedong Zheng,Zhipeng Zhang,Yifan Wang,Lin Song,Lijun Wang,Yanwei Li,Ying Shan,Huchuan Lu*

Main category: cs.AI

TL;DR: Systematic survey of Visual Spatial Reasoning in Vision-Language Models; introduces SIBench benchmark with ~20 datasets and 23 tasks; reveals a gap between perceptual abilities and higher-order spatial reasoning; provides roadmap for future research.


<details>
  <summary>Details</summary>
Motivation: Address the lack of understanding and benchmarking of Visual Spatial Reasoning in Vision-Language Models to advance embodied intelligence and autonomous systems.

Method: Literature review across input modalities, model architectures, training strategies, and reasoning mechanisms; defines a three-level spatial intelligence taxonomy (basic perception, spatial understanding, spatial planning); curates SIBench with ~20 open-source datasets across 23 tasks; empirical evaluation of state-of-the-art VLMs on perceptual vs. reasoning tasks.

Result: Empirical evidence shows strong perceptual capabilities but weak higher-order spatial reasoning; identifies gaps in numerical estimation, multi-view reasoning, temporal dynamics, and spatial imagination; provides a publicly accessible benchmark at the provided URL.

Conclusion: Spatial intelligence remains a major challenge; the work offers a systematic roadmap and a comprehensive benchmark to drive future research and progress in Visual Spatial Reasoning.

Abstract: Visual Spatial Reasoning (VSR) is a core human cognitive ability and a
critical requirement for advancing embodied intelligence and autonomous
systems. Despite recent progress in Vision-Language Models (VLMs), achieving
human-level VSR remains highly challenging due to the complexity of
representing and reasoning over three-dimensional space. In this paper, we
present a systematic investigation of VSR in VLMs, encompassing a review of
existing methodologies across input modalities, model architectures, training
strategies, and reasoning mechanisms. Furthermore, we categorize spatial
intelligence into three levels of capability, ie, basic perception, spatial
understanding, spatial planning, and curate SIBench, a spatial intelligence
benchmark encompassing nearly 20 open-source datasets across 23 task settings.
Experiments with state-of-the-art VLMs reveal a pronounced gap between
perception and reasoning, as models show competence in basic perceptual tasks
but consistently underperform in understanding and planning tasks, particularly
in numerical estimation, multi-view reasoning, temporal dynamics, and spatial
imagination. These findings underscore the substantial challenges that remain
in achieving spatial intelligence, while providing both a systematic roadmap
and a comprehensive benchmark to drive future research in the field. The
related resources of this study are accessible at
https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.

</details>


### [218] [Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning](https://arxiv.org/abs/2509.18942)
*Xiao Han,Zimo Zhao,Wanyu Wang,Maolin Wang,Zitao Liu,Yi Chang,Xiangyu Zhao*

Main category: cs.AI

TL;DR: DEAL combines Low-Rank Adaptation (LoRA) with a continuous fine-tuning strategy to enable continual, task- or domain-specific adaptation of LLMs while mitigating forgetting and improving data and resource efficiency in privacy-preserving settings.


<details>
  <summary>Details</summary>
Motivation: Conventional fine-tuning can cause catastrophic forgetting and is data-inefficient; retraining from scratch is often infeasible, and privacy-preserving, continual adaptation is desirable for real-world deployment.

Method: A framework (DEAL) that integrates LoRA adapters with a continuous fine-tuning strategy, featuring knowledge retention and adaptive parameter-update modules to mitigate forgetting and improve data efficiency while preserving privacy.

Result: Empirical evaluation on 15 diverse datasets shows DEAL consistently outperforms baseline methods, yielding significant gains in task accuracy and resource efficiency.

Conclusion: DEAL advances continual adaptation of LLMs by delivering better task performance with improved resource efficiency in privacy-preserving fine-tuning contexts.

Abstract: Recent advancements in Large Language Models (LLMs) have emphasized the
critical role of fine-tuning (FT) techniques in adapting LLMs to specific
tasks, especially when retraining from scratch is computationally infeasible.
Fine-tuning enables LLMs to leverage task- or domain-specific data, producing
models that more effectively meet the requirements of targeted applications.
However, con- ventional FT approaches often suffer from catastrophic forgetting
and suboptimal data efficiency, limiting their real-world applicability. To
address these challenges, this paper proposes DEAL, a novel framework that
integrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.
By incorporating knowledge retention and adaptive parameter update modules, the
framework mitigates the lim- itations of existing FT methods while maintaining
efficiency in privacy-preserving settings. Experiments on 15 diverse datasets
show that DEAL consistently outper- forms baseline methods, yielding
substantial gains in task accuracy and resource efficiency. These findings
demonstrate the potential of our approach to advance continual adaptation in
LLMs by enhancing task performance while improving resource efficiency.

</details>


### [219] [LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions](https://arxiv.org/abs/2509.18970)
*Xixun Lin,Yucheng Ning,Jingwen Zhang,Yan Dong,Yilong Liu,Yongxuan Wu,Xiaohua Qi,Nan Sun,Yanmin Shang,Pengfei Cao,Lixin Zou,Xu Chen,Chuan Zhou,Jia Wu,Shirui Pan,Bin Wang,Yanan Cao,Kai Chen,Songlin Hu,Li Guo*

Main category: cs.AI

TL;DR: A comprehensive survey of hallucinations in LLM-based agents, offering a taxonomy, a catalog of triggers, and a synthesis of mitigation/detection methods.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents promise human-like cognition and broad applications but suffer from hallucinations that threaten reliability; a systematic consolidation is needed to guide robust design.

Method: Analyze the complete agent workflow, propose a taxonomy of hallucinations by stage, identify 18 triggering causes, review and synthesize existing mitigation and detection approaches.

Result: A detailed taxonomy of agent hallucinations, a list of 18 triggering causes, and a consolidated review of mitigation and detection strategies from prior work.

Conclusion: The survey aims to spur future research toward more robust and reliable LLM-based agent systems.

Abstract: Driven by the rapid advancements of Large Language Models (LLMs), LLM-based
agents have emerged as powerful intelligent systems capable of human-like
cognition, reasoning, and interaction. These agents are increasingly being
deployed across diverse real-world applications, including student education,
scientific research, and financial analysis. However, despite their remarkable
potential, LLM-based agents remain vulnerable to hallucination issues, which
can result in erroneous task execution and undermine the reliability of the
overall system design. Addressing this critical challenge requires a deep
understanding and a systematic consolidation of recent advances on LLM-based
agents. To this end, we present the first comprehensive survey of
hallucinations in LLM-based agents. By carefully analyzing the complete
workflow of agents, we propose a new taxonomy that identifies different types
of agent hallucinations occurring at different stages. Furthermore, we conduct
an in-depth examination of eighteen triggering causes underlying the emergence
of agent hallucinations. Through a detailed review of a large number of
existing studies, we summarize approaches for hallucination mitigation and
detection, and highlight promising directions for future research. We hope this
survey will inspire further efforts toward addressing hallucinations in
LLM-based agents, ultimately contributing to the development of more robust and
reliable agent systems.

</details>


### [220] [From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system](https://arxiv.org/abs/2509.18980)
*Maxime Manderlier,Fabian Lecron,Olivier Vu Thanh,Nicolas Gillis*

Main category: cs.AI

TL;DR: LLMs can generate user-facing explanations from an interpretable matrix factorization recommender; a 326-person user study shows explanations are well received across transparency, effectiveness, persuasion, trust, and satisfaction, with modest differences between explanation strategies.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between interpretable mathematical models and user-understandable explanations; to evaluate whether LLM-generated explanations align with user needs rather than relying on automatic metrics.

Method: Use a constrained matrix factorization model where user types are explicit and item scores are on the same scale as observed ratings to keep representations interpretable; map these into natural language via carefully designed prompts to LLMs; generate multiple explanation types by varying the LLM input; conduct a user study with 326 participants evaluating five dimensions (transparency, effectiveness, persuasion, trust, satisfaction) and the recommendations; analyze quantitative differences and collect qualitative comments.

Result: All explanation types are generally well received; there are moderate statistical differences between strategies; user comments provide additional qualitative insights beyond the quantitative results.

Conclusion: LLMs can effectively translate interpretable recommender models into user-facing explanations; while multiple explanation strategies are viable, slight preferences emerge; emphasizes the value of user-centered evaluation to guide explanation design.

Abstract: We investigate whether large language models (LLMs) can generate effective,
user-facing explanations from a mathematically interpretable recommendation
model. The model is based on constrained matrix factorization, where user types
are explicitly represented and predicted item scores share the same scale as
observed ratings, making the model's internal representations and predicted
scores directly interpretable. This structure is translated into natural
language explanations using carefully designed LLM prompts. Many works in
explainable AI rely on automatic evaluation metrics, which often fail to
capture users' actual needs and perceptions. In contrast, we adopt a
user-centered approach: we conduct a study with 326 participants who assessed
the quality of the explanations across five key dimensions-transparency,
effectiveness, persuasion, trust, and satisfaction-as well as the
recommendations themselves.To evaluate how different explanation strategies are
perceived, we generate multiple explanation types from the same underlying
model, varying the input information provided to the LLM. Our analysis reveals
that all explanation types are generally well received, with moderate
statistical differences between strategies. User comments further underscore
how participants react to each type of explanation, offering complementary
insights beyond the quantitative results.

</details>


### [221] [Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)](https://arxiv.org/abs/2509.18986)
*Erik Penther,Michael Grohs,Jana-Rebecca Rehse*

Main category: cs.AI

TL;DR: Four remaining-time prediction approaches evaluated on a real-world outbound warehouse process; deep learning yields highest accuracy, while conventional boosting offers competitive accuracy with far lower computational requirements.


<details>
  <summary>Details</summary>
Motivation: Advance predictive process monitoring by benchmarking remaining-time forecasting methods on a large, real-world log; provide publicly available dataset.

Method: Empirical comparison of four remaining-time predictors on a novel event log with 169,523 traces from an aviation logistics outbound process; assessment includes accuracy and computational resource usage.

Result: Deep learning models achieve the highest remaining-time prediction accuracy; conventional boosting techniques provide competitive accuracy with substantially lower computational costs.

Conclusion: Trade-off between accuracy and resource usage; practical guidance for practitioners; dataset released publicly; results align with common ML patterns.

Abstract: Predictive process monitoring is a sub-domain of process mining which aims to
forecast the future of ongoing process executions. One common prediction target
is the remaining time, meaning the time that will elapse until a process
execution is completed. In this paper, we compare four different remaining time
prediction approaches in a real-life outbound warehouse process of a logistics
company in the aviation business. For this process, the company provided us
with a novel and original event log with 169,523 traces, which we can make
publicly available. Unsurprisingly, we find that deep learning models achieve
the highest accuracy, but shallow methods like conventional boosting techniques
achieve competitive accuracy and require significantly fewer computational
resources.

</details>


### [222] [Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action](https://arxiv.org/abs/2509.19030)
*Victoire Hervé,Henrik Warpefelt,Christoph Salge*

Main category: cs.AI

TL;DR: A player-centric framework for automatic decomposition of procedurally generated content using Landmarks, Monuments, and Beacons to align metrics with human experience.


<details>
  <summary>Details</summary>
Motivation: Current algorithmic evaluation of PCG often fails to capture the human experience, especially for composite artefacts. There is a need for metrics that reflect perceivability, evocativeness, and Call to Action, bridging humanities and game research to enable automated evaluation.

Method: Introduce nested concepts Landmarks, Monuments, and Beacons, grounded in a player-centric perspective and tied to perceivability, evocativeness, and Call to Action. These concepts are generic to games and can be identified and evaluated using existing techniques from both game studies/industry and AI research. The framework supports mixed-initiative and compositional PCG and aims to be applicable across genres.

Result: Proposes a systematic framework to locate and evaluate the salient sub-components of PCG artefacts, enabling automatic decomposition and evaluation of PCG content, with potential applicability beyond the emphasized domains.

Conclusion: Establishes a bridge between humanities-informed theory and technical game research, enabling better computational PCG evaluation and providing a path toward automated, cross-domain evaluation of procedurally generated artefacts.

Abstract: Algorithmic evaluation of procedurally generated content struggles to find
metrics that align with human experience, particularly for composite artefacts.
Automatic decomposition as a possible solution requires concepts that meet a
range of properties. To this end, drawing on Games Studies and Game AI
research, we introduce the nested concepts of \textit{Landmarks},
\textit{Monuments}, and \textit{Beacons}. These concepts are based on the
artefact's perceivability, evocativeness, and Call to Action, all from a
player-centric perspective. These terms are generic to games and usable across
genres. We argue that these entities can be found and evaluated with techniques
currently used in both research and industry, opening a path towards a fully
automated decomposition of PCG, and evaluation of the salient sub-components.
Although the work presented here emphasises mixed-initiative PCG and
compositional PCG, we believe it applies beyond those domains. With this
approach, we intend to create a connection between humanities and technical
game research and allow for better computational PCG evaluation

</details>


### [223] [Towards Causal Representation Learning with Observable Sources as Auxiliaries](https://arxiv.org/abs/2509.19058)
*Kwonho Kim,Heejeong Nam,Inwoo Hwang,Sanghack Lee*

Main category: cs.AI

TL;DR: Identifiability in causal representation learning is achieved by treating observable sources as auxiliary variables using volume-preserving encoders, enabling recovery of latent factors up to subspace-wise transformations and permutations; with multiple auxiliaries, a variable-selection scheme selects the most informative ones given a latent causal graph.


<details>
  <summary>Details</summary>
Motivation: Prevailing methods rely on externally provided conditioning variables, limiting applicability. If some system-driving latent factors are observable, they can serve as powerful auxiliaries to expand identifiability and practical recoverability.

Method: Propose a framework where observable sources act as auxiliaries and apply volume-preserving encoders to preserve information volumes. Introduce a variable-selection scheme to choose auxiliary variables that maximize latent-factor recoverability under knowledge of the latent causal graph. Validate on synthetic graph and image data.

Result: The approach yields identifiability of the entire latent variables up to subspace-wise transformations and permutations. Multiple known auxiliaries and the proposed selection procedure improve recoverability. Experimental results on synthetic datasets and images support effectiveness.

Conclusion: Extends identifiability boundaries in causal representation learning by leveraging observable sources as auxiliaries. The variable-selection mechanism helps practitioners pick the most informative auxiliaries given the latent graph, with empirical evidence on synthetic and image data.

Abstract: Causal representation learning seeks to recover latent factors that generate
observational data through a mixing function. Needing assumptions on latent
structures or relationships to achieve identifiability in general, prior works
often build upon conditional independence given known auxiliary variables.
However, prior frameworks limit the scope of auxiliary variables to be external
to the mixing function. Yet, in some cases, system-driving latent factors can
be easily observed or extracted from data, possibly facilitating
identification. In this paper, we introduce a framework of observable sources
being auxiliaries, serving as effective conditioning variables. Our main
results show that one can identify entire latent variables up to subspace-wise
transformations and permutations using volume-preserving encoders. Moreover,
when multiple known auxiliary variables are available, we offer a
variable-selection scheme to choose those that maximize recoverability of the
latent factors given knowledge of the latent causal graph. Finally, we
demonstrate the effectiveness of our framework through experiments on synthetic
graph and image data, thereby extending the boundaries of current approaches.

</details>


### [224] [Code Driven Planning with Domain-Adaptive Critic](https://arxiv.org/abs/2509.19077)
*Zikang Tian,Shaohui Peng,Du Huang,Jiaming Guo,Ruizhi Chen,Rui Zhang,Xishan Zhang,Yuxuan Guo,Zidong Du,Qi Guo,Ling Li,Yewen Pu,Xing Hu,Yunji Chen*

Main category: cs.AI

TL;DR: CoPiC uses high-level planning programs generated by LLMs and a domain-adaptive critic to optimize long-term rewards, reducing query costs and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: LLMs possess broad world knowledge but often mismatch environment-specific requirements, and frequent plan refinement via environmental feedback is costly and biased toward short-term rewards. There is a need to align planning with long-term objectives while reducing query costs.

Method: LLMs generate a diverse set of high-level planning programs to propose candidate plans. These candidates are iteratively produced/refined. A domain-adaptive critic—a trained evaluator—assesses candidates based on long-term reward alignment and selects the best one for execution.

Result: CoPiC outperforms AdaPlanner and Reflexion on ALFWorld, NetHack, and StarCraft II Unit Building, with an average 23.33% increase in success rate and a 91.27% reduction in query costs.

Conclusion: Using planning programs as the planner and a domain-adaptive critic as the estimator yields improved planning with substantial query-cost savings, demonstrating effective long-term reward alignment in LLM-driven planning.

Abstract: Large Language Models (LLMs) have been widely adopted as task planners for AI
agents in sequential decision-making problems, leveraging their extensive world
knowledge. However, the gap between their general knowledge and
environment-specific requirements often leads to inaccurate plans. To address
this, existing approaches rely on frequent LLM queries to iteratively refine
plans based on immediate environmental feedback, which incurs substantial query
costs. However, this refinement is typically guided by short-term environmental
feedback, limiting LLMs from developing plans aligned with long-term rewards.
We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of
relying on frequent queries, CoPiC employs LLMs to generate a diverse set of
high-level planning programs, which iteratively produce and refine candidate
plans. A trained domain-adaptive critic then evaluates these candidates and
selects the one most aligned with long-term rewards for execution. Using
high-level planning programs as planner and domain-adaptive critic as
estimator, CoPiC improves planning while significantly reducing query costs.
Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC
outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving
an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in
query costs.

</details>


### [225] [AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration](https://arxiv.org/abs/2509.19236)
*Chunhao Tian,Yutong Wang,Xuebo Liu,Zhexuan Wang,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: AgentInit introduces a MAS initialization framework that optimizes team structure through iterative agent collaboration, a Natural Language to Format standardization step, and Pareto-balanced team selection to balance diversity and task relevance. It claims consistent performance gains over state-of-the-art initializers and lower token usage, with strong transferability; code is publicly available.


<details>
  <summary>Details</summary>
Motivation: Current MAS initialization methods underemphasize the collaborative needs of agents in later stages of tasks. There is a need for a principled approach to compose teams that are both diverse and task-relevant to enhance efficiency and effectiveness.

Method: The approach combines (1) multi-round interactions and reflections during agent generation, (2) a Natural Language to Format mechanism for consistency and standardization, and (3) balanced team selection via Pareto principles to jointly optimize diversity and task relevance. Evaluations are conducted across various frameworks and tasks.

Result: AgentInit outperforms state-of-the-art initialization methods and predefined strategies, with reported improvements up to 1.2 and 1.6 (units not specified), and significantly reduced token consumption. The method demonstrates transferability to similar tasks and validates the effectiveness of its core components.

Conclusion: AgentInit is a reliable, adaptable MAS initialization method that improves collaboration efficiency and effectiveness, with strong transferability and practical code available for replication and adoption.

Abstract: Proper initialization is crucial for any system, particularly in multi-agent
systems (MAS), where it plays a pivotal role in determining both the system's
efficiency and effectiveness. However, existing MAS initialization methods do
not fully account for the collaborative needs of the generated agents in
subsequent stages. Inspired by the principles of effective team composition, we
propose AgentInit, which aims to optimize the structure of agent teams.
Specifically, in addition to multi-round interactions and reflections between
agents during agent generation, AgentInit incorporates a Natural Language to
Format mechanism to ensure consistency and standardization. Balanced team
selection strategies using Pareto principles are subsequently applied to
jointly consider agent team diversity and task relevance to promote effective
and efficient collaboration and enhance overall system performance. Experiments
show that AgentInit consistently outperforms state-of-the-art initialization
methods and pre-defined strategies across various frameworks and tasks,
achieving an overall performance improvement of up to 1.2 and 1.6,
respectively, while also significantly reducing token consumption. Further
analysis confirms its strong transferability to similar tasks and verifies the
effectiveness of its key components, demonstrating its capability and
adaptability as a reliable MAS initialization method. Source code and models
are available at https://github.com/1737423697/AgentInit.

</details>


### [226] [Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World](https://arxiv.org/abs/2509.19265)
*Saeed Almheiri,Rania Hossam,Mena Attia,Chenxi Wang,Preslav Nakov,Timothy Baldwin,Fajri Koto*

Main category: cs.AI

TL;DR: Cross-cultural transfer of commonsense reasoning in LLMs is feasible in the Arab world. A small number of culture-specific examples (12) from one country can improve cross-country performance by ~10% in multilingual models. Additionally, out-of-culture demonstrations (Indonesian and US contexts) can match or exceed in-culture alignment for MCQ reasoning, suggesting wider transferability and efficient adaptation for low-resource cultures.


<details>
  <summary>Details</summary>
Motivation: Address Western-centric biases in LLMs and evaluate whether alignment in one culture can transfer to others. The Arab world provides linguistic and historical commonalities with unique local differences, making it a valuable testbed for cross-cultural transfer in commonsense reasoning.

Method: Create a culturally grounded commonsense reasoning dataset covering 13 Arab countries. Evaluate lightweight alignment methods (in-context learning and demonstration-based reinforcement, termed DITTO) and compare against supervised fine-tuning and direct preference optimization. Use multilingual models to assess cross-cultural transfer and measure performance on cross-cultural tasks like MCQ reasoning.

Result: A small set of culture-specific examples from one country improves cross-country performance by about 10% on average. Out-of-culture demonstrations from Indonesia and the US can match or surpass in-culture alignment for MCQ reasoning, indicating cross-cultural transferability beyond the Arab world.

Conclusion: Efficient cross-cultural alignment is feasible and promising for adapting LLMs to low-resource cultural settings, enabling better performance across diverse cultural contexts with minimal culture-specific data.

Abstract: Large language models (LLMs) often reflect Western-centric biases, limiting
their effectiveness in diverse cultural contexts. Although some work has
explored cultural alignment, the potential for cross-cultural transfer, using
alignment in one culture to improve performance in others, remains
underexplored. This paper investigates cross-cultural transfer of commonsense
reasoning in the Arab world, where linguistic and historical similarities
coexist with local cultural differences. Using a culturally grounded
commonsense reasoning dataset covering 13 Arab countries, we evaluate
lightweight alignment methods such as in-context learning and
demonstration-based reinforcement (DITTO), alongside baselines like supervised
fine-tuning and direct preference optimization. Our results show that merely 12
culture-specific examples from one country can improve performance in others by
10\% on average, within multilingual models. In addition, we demonstrate that
out-of-culture demonstrations from Indonesia and US contexts can match or
surpass in-culture alignment for MCQ reasoning, highlighting cultural
commonsense transferability beyond the Arab world. These findings demonstrate
that efficient cross-cultural alignment is possible and offer a promising
approach to adapt LLMs to low-resource cultural settings.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [227] [Machine Learnability as a Measure of Order in Aperiodic Sequences](https://arxiv.org/abs/2509.18103)
*Jennifer Dodgson,Michael Joedhitya,Adith Ramdas,Surender Suresh Kumar,Adarsh Singh Chauhan,Akira Rafhael,Wang Mingshu,Nordine Lotfi*

Main category: cs.LG

TL;DR: An image-based ML model trained on blocks of the Ulam spiral reveals region-dependent regularity in prime patterns: blocks near 5×10^8 are more learnable than blocks for numbers below 2.5×10^7, suggesting higher-order regions exhibit easier-then-learning order and that ML can act as an experimental instrument in number theory with cryptographic relevance tied to strong/weak primes.


<details>
  <summary>Details</summary>
Motivation: Investigate the deterministic-vs-random duality of prime distributions and test whether an image-focused machine learning model can quantify local regularities in prime fields. Propose ML as a new experimental tool for number theory and cryptography-focused insights on prime patterns.

Method: Train image-based ML models on blocks extracted from two regions of the Ulam spiral: near 5×10^8 and region representing integers below 2.5×10^7. Compare models on pure accuracy, and analyze precision/recall breakdowns to infer region-specific classification strategies. Interpret results in light of density and equidistribution expectations and noise scaling with log x.

Result: Models trained on blocks from the 5×10^8 region achieve higher pure accuracy than those trained on blocks from the ≤2.5×10^7 region, implying more learnable order in the higher-region data. Precision/recall patterns suggest the model emphasizes prime-pattern detection at lower numbers in some regions and focus on eliminating composites at higher numbers in others.

Conclusion: The study demonstrates that ML can serve as an experimental instrument for number theory, capable of probing regional regularities in prime distributions. The findings align with conjectures that higher-order regions exhibit reduced local noise and that density/equidistribution dominate at scale, with potential cryptographic relevance through analysis of strong and weak primes.

Abstract: Research on the distribution of prime numbers has revealed a dual character:
deterministic in definition yet exhibiting statistical behavior reminiscent of
random processes. In this paper we show that it is possible to use an
image-focused machine learning model to measure the comparative regularity of
prime number fields at specific regions of an Ulam spiral. Specifically, we
demonstrate that in pure accuracy terms, models trained on blocks extracted
from regions of the spiral in the vicinity of 500m outperform models trained on
blocks extracted from the region representing integers lower than 25m. This
implies existence of more easily learnable order in the former region than in
the latter. Moreover, a detailed breakdown of precision and recall scores seem
to imply that the model is favouring a different approach to classification in
different regions of the spiral, focusing more on identifying prime patterns at
lower numbers and more on eliminating composites at higher numbers. This aligns
with number theory conjectures suggesting that at higher orders of magnitude we
should see diminishing noise in prime number distributions, with averages
(density, AP equidistribution) coming to dominate, while local randomness
regularises after scaling by log x. Taken together, these findings point toward
an interesting possibility: that machine learning can serve as a new
experimental instrument for number theory. Notably, the method shows potential
1 for investigating the patterns in strong and weak primes for cryptographic
purposes.

</details>


### [228] [Data Valuation and Selection in a Federated Model Marketplace](https://arxiv.org/abs/2509.18104)
*Wenqian Li,Youjia Yang,Ruoxi Jia,Yan Pang*

Main category: cs.LG

TL;DR: A Wasserstein-based, privacy-preserving estimator for data valuation and selection in Federated Learning within data marketplaces; uses neural scaling to extrapolate performance; validated across heterogeneous data scenarios.


<details>
  <summary>Details</summary>
Motivation: Trusted data marketplaces require reliable valuation and selection of data from heterogeneous, privacy-sensitive sources in Federated Learning; identifying high-quality data combinations is challenging without access to raw data.

Method: Develop a Wasserstein-distance–based estimator that predicts FL model performance on unseen data combinations; enables privacy-preserving, distributed computation of Wasserstein distance; analyzes compatibility between data heterogeneity and FL aggregation algorithms; leverages neural scaling laws to extrapolate performance from limited training; validated across diverse data-skew scenarios.

Result: The estimator consistently identifies high-performing data combinations across label-skew, mislabeled, and unlabeled scenarios, enabling effective data selection without full-scale training and improving reliability of FL-based model marketplaces.

Conclusion: A comprehensive, privacy-preserving framework for data valuation and selection in FL marketplaces that integrates Wasserstein-based performance extrapolation and neural-scaling insights, facilitating trustworthy and efficient data exchanges.

Abstract: In the era of Artificial Intelligence (AI), marketplaces have become
essential platforms for facilitating the exchange of data products to foster
data sharing. Model transactions provide economic solutions in data
marketplaces that enhance data reusability and ensure the traceability of data
ownership. To establish trustworthy data marketplaces, Federated Learning (FL)
has emerged as a promising paradigm to enable collaborative learning across
siloed datasets while safeguarding data privacy. However, effective data
valuation and selection from heterogeneous sources in the FL setup remain key
challenges. This paper introduces a comprehensive framework centered on a
Wasserstein-based estimator tailored for FL. The estimator not only predicts
model performance across unseen data combinations but also reveals the
compatibility between data heterogeneity and FL aggregation algorithms. To
ensure privacy, we propose a distributed method to approximate Wasserstein
distance without requiring access to raw data. Furthermore, we demonstrate that
model performance can be reliably extrapolated under the neural scaling law,
enabling effective data selection without full-scale training. Extensive
experiments across diverse scenarios, such as label skew, mislabeled, and
unlabeled sources, show that our approach consistently identifies
high-performing data combinations, paving the way for more reliable FL-based
model marketplaces.

</details>


### [229] [BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential Equations under Stochastic Demand](https://arxiv.org/abs/2509.18105)
*Nachiket N. Naik,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: Enforcing known structure improves forecasting under light-tailed or autocorrelated demand; UDE beats NODE in AR(1) and Gaussian regimes, but NODE’s flexibility wins for heavy-tailed shocks; guideline: apply structure when noise is modest and correlated, relax when extremes dominate.


<details>
  <summary>Details</summary>
Motivation: To quantify when structural bias in neural differential equations helps or hurts forecasting of the bullwhip effect, by comparing full NODE vs UDE across demand regimes, and to provide practical hybrid modeling guidance.

Method: Use a single-echelon testbed with three demand regimes (AR(1), i.i.d Gaussian, heavy-tailed lognormal); train on varying fractions of each trajectory; evaluate multi-step forecasts for inventory I, order rate O, and demand D; compare a fully learned NODE against a physics-informed UDE preserving conservation and order-up-to structure while learning a residual policy.

Result: Across structured regimes, UDE generalizes better: with 90% of the training horizon, inventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR1 and from 5.96 to 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the flexibility of NODE is better. Trends persist as training data shrinks, with NODE exhibiting phase drift in extrapolation while UDE remains stable but underreacts to rare spikes.

Conclusion: Enforce structure when noise is light-tailed or temporally correlated; relax structure when extreme events dominate. Beyond inventory control, the results offer guidance for hybrid modeling in scientific and engineering systems: enforce known structure when conservation laws and modest noise dominate, and relax structure to capture extremes in settings where rare events drive dynamics.

Abstract: We study learning of continuous-time inventory dynamics under stochastic
demand and quantify when structure helps or hurts forecasting of the bullwhip
effect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the
entire right-hand side against a physics-informed Universal Differential
Equation (UDE) that preserves conservation and order-up-to structure while
learning a small residual policy term. Classical supply chain models explain
the bullwhip through control/forecasting choices and information sharing, while
recent physics-informed and neural differential equation methods blend domain
constraints with learned components. It is unclear whether structural bias
helps or hinders forecasting under different demand regimes. We address this by
using a single-echelon testbed with three demand regimes - AR(1)
(autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done
on varying fractions of each trajectory, followed by evaluation of multi-step
forecasts for inventory I, order rate O, and demand D. Across the structured
regimes, UDE consistently generalizes better: with 90% of the training horizon,
inventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96
to 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the
flexibility of NODE is better. These trends persist as train18 ing data
shrinks, with NODE exhibiting phase drift in extrapolation while UDE remains
stable but underreacts to rare spikes. Our results provide concrete guidance:
enforce structure when noise is light-tailed or temporally correlated; relax
structure when extreme events dominate. Beyond inventory control, the results
offer guidance for hybrid modeling in scientific and engineering systems:
enforce known structure when conservation laws and modest noise dominate, and
relax structure to capture extremes in settings where rare events drive
dynamics.

</details>


### [230] [Model-Based Transfer Learning for Real-Time Damage Assessment of Bridge Networks](https://arxiv.org/abs/2509.18106)
*Elisa Tomassini,Enrique García-Macías,Filippo Ubertini*

Main category: cs.LG

TL;DR: Model-based transfer learning with neural network surrogates transfers knowledge between similar bridges for scalable structural health monitoring, validated on two bridges within a Bayesian damage inference framework.


<details>
  <summary>Details</summary>
Motivation: With widespread permanent monitoring data, there is a need to monitor large bridge networks efficiently. Transfer learning across similar structures can reuse learned damage patterns and reduce data costs.

Method: Train a neural network surrogate on one bridge and adapt it to another with similar characteristics to capture shared damage mechanisms. Integrate the transferred model into a Bayesian inference framework using modal features from monitoring data for continuous damage assessment.

Result: Transferred models demonstrated high sensitivity to damage location, severity, and extent and were validated with real data from two bridges. The approach enables real-time monitoring and cross-structure knowledge transfer.

Conclusion: Cross-structure transfer learning enhances scalable, generalizable monitoring and resilience at the network level by enabling smart, transferable damage models across bridges.

Abstract: The growing use of permanent monitoring systems has increased data
availability, offering new opportunities for structural assessment but also
posing scalability challenges, especially across large bridge networks.
Managing multiple structures requires tracking and comparing long-term
behaviour efficiently. To address this, knowledge transfer between similar
structures becomes essential. This study proposes a model-based transfer
learning approach using neural network surrogate models, enabling a model
trained on one bridge to be adapted to another with similar characteristics.
These models capture shared damage mechanisms, supporting a scalable and
generalizable monitoring framework. The method was validated using real data
from two bridges. The transferred model was integrated into a Bayesian
inference framework for continuous damage assessment based on modal features
from monitoring data. Results showed high sensitivity to damage location,
severity, and extent. This approach enhances real-time monitoring and enables
cross-structure knowledge transfer, promoting smart monitoring strategies and
improved resilience at the network level.

</details>


### [231] [AdaMixT: Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting](https://arxiv.org/abs/2509.18107)
*Huanyao Zhang,Jiaye Lin,Wentao Zhang,Haitao Yuan,Guoliang Li*

Main category: cs.LG

TL;DR: AdaMixT introduces an adaptive weighted mixture of multi-scale expert transformers for multivariate time series forecasting, combining patch-based multi-scale features from General Pre-trained Models (GPM) and Domain-specific Models (DSM) with a gating network to dynamically fuse expert predictions, achieving strong results on eight benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on fixed single-scale patches or lack robust multi-scale feature fusion, limiting ability to capture complex temporal patterns and generalize across diverse time-series domains.

Method: Constructs multiple patch scales to capture temporal granularity, leverages both General Pre-trained Models (GPM) and Domain-specific Models (DSM) for multi-scale feature extraction, and employs a gating network to dynamically assign weights to a set of expert transformers (mixture of experts) for adaptive multi-scale fusion.

Result: AdaMixT achieves consistent improvements across eight benchmarks (Weather, Traffic, Electricity, ILI, and four ETT datasets), demonstrating strong performance and practical applicability in real-world forecasting scenarios.

Conclusion: Adaptive multi-scale feature fusion with a gated mixture of expert transformers effectively enhances multivariate time series forecasting and generalization across diverse datasets.

Abstract: Multivariate time series forecasting involves predicting future values based
on historical observations. However, existing approaches primarily rely on
predefined single-scale patches or lack effective mechanisms for multi-scale
feature fusion. These limitations hinder them from fully capturing the complex
patterns inherent in time series, leading to constrained performance and
insufficient generalizability. To address these challenges, we propose a novel
architecture named Adaptive Weighted Mixture of Multi-Scale Expert Transformers
(AdaMixT). Specifically, AdaMixT introduces various patches and leverages both
General Pre-trained Models (GPM) and Domain-specific Models (DSM) for
multi-scale feature extraction. To accommodate the heterogeneity of temporal
features, AdaMixT incorporates a gating network that dynamically allocates
weights among different experts, enabling more accurate predictions through
adaptive multi-scale fusion. Comprehensive experiments on eight widely used
benchmarks, including Weather, Traffic, Electricity, ILI, and four ETT
datasets, consistently demonstrate the effectiveness of AdaMixT in real-world
scenarios.

</details>


### [232] [Solve it with EASE](https://arxiv.org/abs/2509.18108)
*Adam Viktorin,Tomas Kadavy,Jozef Kovac,Michal Pluhacek,Roman Senkerik*

Main category: cs.LG

TL;DR: EASE is an open-source, modular framework for iterative algorithmic solution generation with LLMs, integrating generation, testing, analysis, and evaluation in a reproducible feedback loop. It supports orchestrating multiple LLM roles (generator, analyst, evaluator), abstracts prompt design and model management, and aims to enable co-design of algorithms across domains.


<details>
  <summary>Details</summary>
Motivation: ToLower the barrier to building and evaluating LLM-driven algorithmic solutions by providing a transparent, extensible, and reproducible workflow that handles error handling, analysis, and quality assessment, while enabling multi-LLM collaboration.

Method: A modular architecture that orchestrates generation, testing, analysis, and evaluation within a feedback loop. Supports multiple LLMs in complementary roles (generator, analyst, evaluator) and abstracts prompt design and model management to reduce complexity and increase reproducibility.

Result: Conceptual framework and architecture for EASE; description of its capabilities and flow. No empirical results or benchmarks are reported in the abstract.

Conclusion: EASE offers a transparent, extensible platform for researchers and practitioners to co-design algorithms and other generative solutions across diverse domains.

Abstract: This paper presents EASE (Effortless Algorithmic Solution Evolution), an
open-source and fully modular framework for iterative algorithmic solution
generation leveraging large language models (LLMs). EASE integrates generation,
testing, analysis, and evaluation into a reproducible feedback loop, giving
users full control over error handling, analysis, and quality assessment. Its
architecture supports the orchestration of multiple LLMs in complementary
roles-such as generator, analyst, and evaluator. By abstracting the complexity
of prompt design and model management, EASE provides a transparent and
extensible platform for researchers and practitioners to co-design algorithms
and other generative solutions across diverse domains.

</details>


### [233] [Machine Learning-Based Classification of Vessel Types in Straits Using AIS Tracks](https://arxiv.org/abs/2509.18109)
*Jonatan Katz Nielsen*

Main category: cs.LG

TL;DR: A strait-scale ML pipeline classifies vessel types from AIS data using trajectory features; Random Forest with SMOTE achieves ~92% test accuracy across five classes, with bridge-position ratio and max SOG as key features; suggests improvements like DBSCAN-based segmentation and boosted ensembles.


<details>
  <summary>Details</summary>
Motivation: To enable safety oversight and combat IUU activity by automatically identifying vessel types from AIS trajectories, addressing data leakage concerns and enabling near real-time classification in busy straits.

Method: Preprocess 8 days of AIS data from the Bornholm Strait (Jan 22–30, 2025): forward/backward filling, outlier removal, per-MMSI track segmentation excluding stationary periods (>1 h); derive 31 trajectory-level features spanning kinematics (e.g., SOG statistics), temporal, geospatial (Haversine spans), and ship-shape attributes from AIS reference points (length, width, aspect ratio, bridge-position ratio); split data with grouped train/test by MMSI and 5-fold stratified CV to avoid leakage; compare tree-based models with SMOTE for class imbalance; report macro metrics and ROC-AUC; identify feature importances.

Result: Five-class accuracy on held-out test: 92.15% (macro-precision 94.11%, macro-recall 92.51%, macro-F1 93.27%); Random Forest with SMOTE performs best (ROC-AUC up to 0.9897 for one-vs-rest); bridge-position ratio and maximum SOG are top discriminators; most errors occur between cargo and tanker due to similar behavior; backfilling on unseen data demonstrates practical value; potential improvements include DBSCAN-based trip segmentation and gradient-boosted ensembles to better handle frequent-stop ferries.

Conclusion: Lightweight AIS-derived trajectory features enable real-time vessel type classification in straits; the approach demonstrates strong performance with practical applicability and can be enhanced with improved segmentation and boosted models to further lift accuracy and handle challenging cases.

Abstract: Accurate recognition of vessel types from Automatic Identification System
(AIS) tracks is essential for safety oversight and combating illegal,
unreported, and unregulated (IUU) activity. This paper presents a strait-scale,
machine-learning pipeline that classifies moving vessels using only AIS data.
We analyze eight days of historical AIS from the Danish Maritime Authority
covering the Bornholm Strait in the Baltic Sea (January 22-30, 2025). After
forward/backward filling voyage records, removing kinematic and geospatial
outliers, and segmenting per-MMSI tracks while excluding stationary periods
($\ge 1$ h), we derive 31 trajectory-level features spanning kinematics (e.g.,
SOG statistics), temporal, geospatial (Haversine distances, spans), and
ship-shape attributes computed from AIS A/B/C/D reference points (length,
width, aspect ratio, bridge-position ratio). To avoid leakage, we perform
grouped train/test splits by MMSI and use stratified 5-fold cross-validation.
Across five classes (cargo, tanker, passenger, high-speed craft, fishing;
N=1{,}910 trajectories; test=382), tree-based models dominate: a Random Forest
with SMOTE attains 92.15% accuracy (macro-precision 94.11%, macro-recall
92.51%, macro-F1 93.27%) on the held-out test set, while a tuned RF reaches
one-vs-rest ROC-AUC up to 0.9897. Feature-importance analysis highlights the
bridge-position ratio and maximum SOG as the most discriminative signals;
principal errors occur between cargo and tanker, reflecting similar transit
behavior. We demonstrate operational value by backfilling missing ship types on
unseen data and discuss improvements such as DBSCAN based trip segmentation and
gradient-boosted ensembles to handle frequent-stop ferries and further lift
performance. The results show that lightweight features over AIS trajectories
enable real-time vessel type classification in straits.

</details>


### [234] [Localized PCA-Net Neural Operators for Scalable Solution Reconstruction of Elliptic PDEs](https://arxiv.org/abs/2509.18110)
*Mrigank Dhingra,Romit Maulik,Adil Rasheed,Omer San*

Main category: cs.LG

TL;DR: Patch-based PCA-Net speeds up neural operator learning for PDEs by performing PCA locally within patches, with two patching strategies and optional refinements, achieving 3.7–4x speedups while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: Global PCA on high-dimensional PDE solution fields is costly; need scalable, data-driven operators for PDEs; patching reduces dimensionality per patch and enables efficient training.

Method: Decompose fields into patches; apply PCA inside each patch to obtain reduced representations; train neural operator in this PCA-reduced space. Compare two patching schemes: (1) local-to-global patch PCA (local patches mapped to a global reconstruction) and (2) local-to-local patch PCA (local patches mapped locally). Evaluate trade-offs between cost and accuracy. Refine the efficient method via (i) overlapping patches with a smoothing filter, (ii) a two-step CNN refinement process.

Result: Significant reduction in computational complexity; end-to-end time reduced by 3.7–4x versus global PCA; maintains high accuracy; patch-based PCA is advantageous for efficient operator learning in PDEs.

Conclusion: Patch-based PCA nets are a promising approach for efficient PDE operator learning, balancing efficiency and accuracy through patch-level PCA and optional refinements.

Abstract: Neural operator learning has emerged as a powerful approach for solving
partial differential equations (PDEs) in a data-driven manner. However,
applying principal component analysis (PCA) to high-dimensional solution fields
incurs significant computational overhead. To address this, we propose a
patch-based PCA-Net framework that decomposes the solution fields into smaller
patches, applies PCA within each patch, and trains a neural operator in the
reduced PCA space. We investigate two different patch-based approaches that
balance computational efficiency and reconstruction accuracy: (1)
local-to-global patch PCA, and (2) local-to-local patch PCA. The trade-off
between computational cost and accuracy is analyzed, highlighting the
advantages and limitations of each approach. Furthermore, within each approach,
we explore two refinements for the most computationally efficient method: (i)
introducing overlapping patches with a smoothing filter and (ii) employing a
two-step process with a convolutional neural network (CNN) for refinement. Our
results demonstrate that patch-based PCA significantly reduces computational
complexity while maintaining high accuracy, reducing end-to-end pipeline
processing time by a factor of 3.7 to 4 times compared to global PCA, thefore
making it a promising technique for efficient operator learning in PDE-based
systems.

</details>


### [235] [Prompt Optimization Meets Subspace Representation Learning for Few-shot Out-of-Distribution Detection](https://arxiv.org/abs/2509.18111)
*Faizul Rakib Sayem,Shahana Ibrahim*

Main category: cs.LG

TL;DR: A CoOp-based framework for open-world OOD detection in vision-language models that uses subspace representation learning to separate in-distribution (ID) and out-of-distribution (OOD) features by projecting ID features into a prompt-spanned subspace and ID-irrelevant features into an orthogonal null space, trained with an end-to-end criterion balancing OOD detection and ID accuracy.


<details>
  <summary>Details</summary>
Motivation: Open-world AI reliability hinges on effective OOD detection. While large vision-language models provide discriminative embeddings, existing prompt-learning OOD methods rely on softmax probabilities and do not exploit rich feature embeddings. This work aims to exploit VLM embeddings via subspace learning combined with prompt tuning to improve ID–OOD separability.

Method: A CoOp-based framework that integrates subspace representation learning with prompt tuning. It projects ID features into a subspace spanned by prompt vectors and maps ID-irrelevant features into an orthogonal null space. An end-to-end learning objective is designed to ensure strong OOD detection while preserving high ID classification accuracy.

Result: Experiments on real-world datasets demonstrate the effectiveness of the proposed approach, achieving improved ID–OOD separability and robust OOD detection while maintaining ID accuracy.

Conclusion: The study presents a novel integration of prompt-based learning with subspace representation for OOD detection in VLMs, offering improved open-world reliability without sacrificing in-distribution performance.

Abstract: The reliability of artificial intelligence (AI) systems in open-world
settings depends heavily on their ability to flag out-of-distribution (OOD)
inputs unseen during training. Recent advances in large-scale vision-language
models (VLMs) have enabled promising few-shot OOD detection frameworks using
only a handful of in-distribution (ID) samples. However, existing prompt
learning-based OOD methods rely solely on softmax probabilities, overlooking
the rich discriminative potential of the feature embeddings learned by VLMs
trained on millions of samples. To address this limitation, we propose a novel
context optimization (CoOp)-based framework that integrates subspace
representation learning with prompt tuning. Our approach improves ID-OOD
separability by projecting the ID features into a subspace spanned by prompt
vectors, while projecting ID-irrelevant features into an orthogonal null space.
To train such OOD detection framework, we design an easy-to-handle end-to-end
learning criterion that ensures strong OOD detection performance as well as
high ID classification accuracy. Experiments on real-world datasets showcase
the effectiveness of our approach.

</details>


### [236] [Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis](https://arxiv.org/abs/2509.18112)
*Sheng Wong,Ravi Shankar,Beth Albert,Gabriel Davis Jones*

Main category: cs.LG

TL;DR: Fine-tuned LLMs outperform foundation models and CTG-specific architectures in antepartum CTG analysis, suggesting a promising AI pathway for clinical fetal monitoring.


<details>
  <summary>Details</summary>
Motivation: CTG interpretation is highly subjective with variability in diagnostic accuracy; the study investigates whether foundation models and LLMs can effectively analyze long time-series CTG data, addressing a gap in existing research.

Method: Systematic benchmarking of time-series foundation models, large language models, and CTG-specific architectures on over 500 real-world antepartum CTG recordings with varying durations, comparing across modelling paradigms; emphasis on fine-tuning LLMs.

Result: Fine-tuned LLMs achieve superior performance compared with both foundation models and domain-specific CTG architectures.

Conclusion: Fine-tuned LLMs represent a promising alternative for clinical CTG interpretation and establish a foundation for future AI development in prenatal care.

Abstract: Foundation models (FMs) and large language models (LLMs) demonstrate
remarkable capabilities across diverse domains through training on massive
datasets. These models have demonstrated exceptional performance in healthcare
applications, yet their potential for electronic fetal monitoring
(EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating
fetal well-being, remains largely underexplored. Antepartum CTG interpretation
presents unique challenges due to the complex nature of fetal heart rate (FHR)
patterns and uterine activity, requiring sophisticated analysis of long
time-series data. The assessment of CTG is heavily based on subjective clinical
interpretation, often leading to variability in diagnostic accuracy and
deviation from timely pregnancy care. This study presents the first
comprehensive comparison of state-of-the-art AI approaches for automated
antepartum CTG analysis. We systematically compare time-series FMs and LLMs
against established CTG-specific architectures. Our evaluation encompasses over
500 CTG recordings of varying durations reflecting real-world clinical
recordings, providing robust performance benchmarks across different modelling
paradigms. Our results demonstrate that fine-tuned LLMs achieve superior
performance compared to both foundation models and domain-specific approaches,
offering a promising alternative pathway for clinical CTG interpretation. These
findings provide critical insights into the relative strengths of different AI
methodologies for fetal monitoring applications and establish a foundation for
future clinical AI development in prenatal care.

</details>


### [237] [A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU](https://arxiv.org/abs/2509.18114)
*Javed I. Khan an Henry Uwabor Moye*

Main category: cs.LG

TL;DR: A DPU-assisted framework using BlueField-3 to detect and mitigate multi-node GPU load imbalances during LLM tensor inference, enabling actionable feedback to controllers and schedulers.


<details>
  <summary>Details</summary>
Motivation: Decode-phase throughput and latency suffer from load skew across GPU shards in multi-node LLM inference; real-time detection/mitigation is needed.

Method: Propose a DPU-based monitoring framework that offloads telemetry collection and anomaly analysis to DPUs, analyzes GPU telemetry and inter-node communication patterns, and provides feedback to inference controllers/schedulers to address imbalances; aim to identify skew conditions, their performance impact, and assess trackability for mitigation.

Result: No empirical results are reported; the paper outlines a framework and three-fold goals for identifying skews, evaluating their performance impact, and assessing mitigation possibilities via a DPU network.

Conclusion: DPUs can enable real-time monitoring and mitigation of load imbalance in multi-node tensor-parallel inference, potentially improving throughput and latency; further evaluation is required to validate effectiveness and overhead.

Abstract: Autoregressive inference in large transformer-based language models (LLMs)
presents significant challenges for runtime efficiency, particularly during the
decode phase where load imbalance across GPU shards can cause throughput
degradation and latency spikes. A DPU-assisted framework leveraged by
BlueField-3 Data Processing Units can enable real-time detection and mitigation
of load imbalance in multi-node tensor-parallel inference. By offloading
monitoring tasks to the DPU and analyzing GPU telemetry and inter-node
communication patterns, the resulting system can provide actionable feedback to
inference controllers and schedulers. The goal of this study is three-fold i)
identify the reported skews/imbalances/pathological conditions that arise in
muti-GPU execution of a) LLM tensor computing (both during training and
inference), b) identify their impact on computational performance, and c) make
a critical assessment if those can be tracked for potential mitigation from a
DPU network.

</details>


### [238] [Towards Scalable and Structured Spatiotemporal Forecasting](https://arxiv.org/abs/2509.18115)
*Hongyi Chen,Xiucheng Li,Xinyang Chen,Jing Li,Kehai Chen,Liqiang Nie*

Main category: cs.LG

TL;DR: A Spatial Balance Attention (SBA) block for spatiotemporal forecasting balances local spatial proximity and global spatial correlation by partitioning the spatial graph into subgraphs; intra-subgraph attention learns local relations, while inter-subgraph attention enables global message passing via subgraph representations. A multiscale model grows subgraph scales for scalability. Claims state-of-the-art performance with low running costs, showing up to 7.7% improvement over baselines on real-world medium-to-large datasets.


<details>
  <summary>Details</summary>
Motivation: In spatiotemporal forecasting, capturing both local spatial proximity and global spatial dependencies is challenging. Existing methods often emphasize either local neighborhoods or global patterns. The proposed Spatial Balance Attention aims to integrate both to improve forecasting while remaining scalable.

Method: Partition the spatial graph into subgraphs. Use intra-subgraph attention to learn local spatial correlations within each subgraph. Aggregate nodes to form subgraph representations and perform inter-subgraph attention for message passing across subgraphs. Build a multiscale forecasting model by progressively increasing subgraph scales. Evaluate on real-world medium-to-large ST datasets, comparing against baseline models for efficacy and efficiency.

Result: Reported improvements up to 7.7% over baseline methods at low running costs on real-world datasets of medium to large sizes, indicating both accuracy gains and computational efficiency.

Conclusion: The Spatial Balance Attention block enables scalable, structured modeling of spatial relations in spatiotemporal forecasting by balancing local and global spatial dependencies; the approach is simple to implement and yields efficiency advantages while improving performance.

Abstract: In this paper, we propose a novel Spatial Balance Attention block for
spatiotemporal forecasting. To strike a balance between obeying spatial
proximity and capturing global correlation, we partition the spatial graph into
a set of subgraphs and instantiate Intra-subgraph Attention to learn local
spatial correlation within each subgraph; to capture the global spatial
correlation, we further aggregate the nodes to produce subgraph representations
and achieve message passing among the subgraphs via Inter-subgraph Attention.
Building on the proposed Spatial Balance Attention block, we develop a
multiscale spatiotemporal forecasting model by progressively increasing the
subgraph scales. The resulting model is both scalable and able to produce
structured spatial correlation, and meanwhile, it is easy to implement. We
evaluate its efficacy and efficiency against the existing models on real-world
spatiotemporal datasets from medium to large sizes. The experimental results
show that it can achieve performance improvements up to 7.7% over the baseline
methods at low running costs.

</details>


### [239] [Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization](https://arxiv.org/abs/2509.18116)
*Nathan Egbuna,Saatvik Gaur,Sunishchal Dev,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: Amortized Latent Steering (ALS) replaces costly latent-space optimization with a single offline-computed vector that nudges model activations toward successful generations, enabling 2–5× faster inference while matching/better results on GSM8K and MATH-500.


<details>
  <summary>Details</summary>
Motivation: Test-time latent optimization offers performance gains but incurs prohibitive per-query compute. A scalable, offline-calibrated approach could preserve reasoning benefits with constant-time inference, enabling production deployment.

Method: Compute the mean hidden-state difference between successful and unsuccessful generations. Use this difference as a steering direction to plug into the decoder: when decoding drifts from the success manifold, adjust activations toward it. This collapses iterative optimization into a single offline vector that is applied at inference with constant cost.

Result: ALS achieves 2–5× speedup over iterative latent optimization methods on GSM8K and MATH-500, while matching or surpassing greedy CoT and Self-Consistency baselines, yielding up to 101% improvement in the efficiency–accuracy trade-off.

Conclusion: Offline latent optimization captures much of the gains from latent reasoning methods, making such techniques viable for production deployment without sacrificing performance.

Abstract: Test-time optimization remains impractical at scale due to prohibitive
inference costs\textemdash techniques like iterative refinement and multi-step
verification can require $10$--$100\times$ more compute per query than standard
decoding. Latent space test-time optimization methods like LatentSeek offer a
more direct approach by steering hidden representations, but still demand
expensive per-query optimization loops with multiple backward passes. We
propose Amortized Latent Steering (ALS), which collapses this iterative
optimization into a single offline-computed vector applied at constant cost
during inference. ALS computes the mean difference between hidden states from
successful versus unsuccessful generations, then uses this direction to
calibrate the model's hidden representations: when decoding drifts away from
the success manifold, ALS nudges activations back toward it. Across GSM8K and
MATH-$500$ benchmarks, ALS achieves $2$--$5\times$ speedup over iterative
methods while matching or surpassing greedy Chain-of-Thought (CoT) and
Self-Consistency baselines, yielding up to 101\% improvement in
efficiency--accuracy trade-off. These results show that much of latent
optimization's benefit can be captured offline, making sophisticated reasoning
techniques viable for production deployment. Code is available
at~\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}

</details>


### [240] [Robust and continuous machine learning of usage habits to adapt digital interfaces to user needs](https://arxiv.org/abs/2509.18117)
*Eric Petit,Denis Chêne*

Main category: cs.LG

TL;DR: Bayesian online learning for personalized, adaptive interfaces that model individual user browsing habits and handle non-stationary environments.


<details>
  <summary>Details</summary>
Motivation: To design digital interfaces that adapt to individual users by learning their unique browsing habits from limited data and to cope with changing usage patterns, moving beyond group-level preferences.

Method: Online Bayesian inference to model per-user navigation behavior; builds a graphical task model with current-user usage statistics; online incremental learning that preserves prior knowledge (lifelong learning) and can acquire new tasks without forgetting.

Result: Simulations demonstrate effectiveness in both stationary and non-stationary environments; predictions are reliable even with little data, enabling adaptive interfaces that better support navigation and action.

Conclusion: This work provides a theoretical framework and demonstrates feasibility of adaptive systems that enhance user experience by personalizing navigation and interaction based on the current user’s behavior.

Abstract: The paper presents a machine learning approach to design digital interfaces
that can dynamically adapt to different users and usage strategies. The
algorithm uses Bayesian statistics to model users' browsing behavior, focusing
on their habits rather than group preferences. It is distinguished by its
online incremental learning, allowing reliable predictions even with little
data and in the case of a changing environment. This inference method generates
a task model, providing a graphical representation of navigation with the usage
statistics of the current user. The algorithm learns new tasks while preserving
prior knowledge. The theoretical framework is described, and simulations show
the effectiveness of the approach in stationary and non-stationary
environments. In conclusion, this research paves the way for adaptive systems
that improve the user experience by helping them to better navigate and act on
their interface.

</details>


### [241] [Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices](https://arxiv.org/abs/2509.18118)
*Marcelo Ribeiro,Diogo Costa,Gonçalo Moreira,Sandro Pinto,Tiago Gomes*

Main category: cs.LG

TL;DR: Extends L-SGD to RISC-V MCUs with an 8-bit quantized variant that dramatically reduces memory and speeds training with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: On-device training on resource-constrained MCUs is hard due to lack of GPUs/FPUs and privacy/connectivity concerns; Federated Learning and on-device training require efficient optimization; this work extends L-SGD from Arm to RISC-V MCUs to enable practical local training on emerging architectures.

Method: Port L-SGD to RISC-V MCUs; compare 32-bit floating-point training across Arm and RISC-V; introduce an 8-bit quantized L-SGD to mitigate hardware limitations; evaluate memory usage, training time, and accuracy on both platforms.

Result: 8-bit quantized L-SGD achieves about 4x reduction in memory usage and a 2.2x speedup in training time with negligible accuracy degradation; demonstrates feasibility of on-device training on RISC-V MCUs.

Conclusion: Quantization enables practical on-device training on RISC-V MCUs, bridging the hardware gap and expanding FL applicability to emerging architectures; future work could explore broader datasets and quantization strategies.

Abstract: Modern IoT devices increasingly rely on machine learning solutions to process
data locally. However, the lack of graphics processing units (GPUs) or
dedicated accelerators on most platforms makes on-device training largely
infeasible, often requiring cloud-based services to perform this task. This
procedure often raises privacy-related concerns, and creates dependency on
reliable and always-on connectivity. Federated Learning (FL) is a new trend
that addresses these issues by enabling decentralized and collaborative
training directly on devices, but it requires highly efficient optimization
algorithms. L-SGD, a lightweight variant of stochastic gradient descent, has
enabled neural network training on Arm Cortex-M Microcontroller Units (MCUs).
This work extends L-SGD to RISC-V-based MCUs, an open and emerging architecture
that still lacks robust support for on-device training. L-SGD was evaluated on
both Arm and RISC-V platforms using 32-bit floating-point arithmetic,
highlighting the performance impact of the absence of Floating-Point Units
(FPUs) in RISC-V MCUs. To mitigate these limitations, we introduce an 8-bit
quantized version of L-SGD for RISC-V, which achieves nearly 4x reduction in
memory usage and a 2.2x speedup in training time, with negligible accuracy
degradation.

</details>


### [242] [MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents](https://arxiv.org/abs/2509.18119)
*Yifan Xu,Xiao Liu,Xinghan Liu,Jiaqi Fu,Hanchen Zhang,Bohao Jing,Shudan Zhang,Yuting Wang,Wenyi Zhao,Yuxiao Dong*

Main category: cs.LG

TL;DR: MOBILERL is an online agentic RL framework for mobile GUI agents that uses ADAGRPO with difficulty-adaptive replay, failure curriculum filtering, and shortest-path reward adjustment to stabilize training and boost sample efficiency; achieves state-of-the-art success rates on AndroidWorld (75.8%) and AndroidLab (46.8%), and is open-sourced in THUDM/MobileRL.


<details>
  <summary>Details</summary>
Motivation: Mobile GUI RL faces a heavy-tailed task difficulty distribution and inefficient environment sampling, which hinder learning effective agents. A method that adapts to task difficulty could stabilize training and improve sample efficiency.

Method: Introduce MOBILERL with ADAGRPO: difficulty-adaptive positive replay, failure curriculum filtering, and shortest-path reward adjustment. Apply to two open models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base) to form MOBILERL-9B; evaluate on AndroidWorld and AndroidLab.

Result: Training stability improved and sample efficiency increased; MOBILERL-9B achieves strong performance with state-of-the-art results on AndroidWorld (75.8%) and AndroidLab (46.8%). Adoption in AutoGLM products and open-source release at THUDM/MobileRL.

Conclusion: MOBILERL provides a scalable framework for mobile GUI RL that can leverage large models and be deployed in real-world products; open-sourcing facilitates broader research and development.

Abstract: Building general-purpose graphical user interface (GUI) agents has become
increasingly promising with the progress in vision language models. However,
developing effective mobile GUI agents with reinforcement learning (RL) remains
challenging due to the heavy-tailed distribution of task difficulty and the
inefficiency of large-scale environment sampling. We present an online agentic
reinforcement learning framework MOBILERL to enhance GUI agents in mobile
environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)
algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and
failure curriculum filtering to adapt the model to different task difficulties.
We introduce the shortest path reward adjustment strategy to reshape rewards
concerning the task length in multi-turn agentic tasks. Those strategies
jointly stabilize RL training, improve sample efficiency, and generate strong
performance across diverse mobile apps and tasks. We apply MOBILERL to two open
models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B
model achieves state-of-the-art results in terms of success rates on both
AndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted
in the AutoGLM products, and also open-sourced at
https://github.com/THUDM/MobileRL.

</details>


### [243] [A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning](https://arxiv.org/abs/2509.18120)
*Thanh Linh Nguyen,Quoc-Viet Pham*

Main category: cs.LG

TL;DR: CoCoGen uses generative AI and game theory to enable coopetitive data generation in cross-silo federated learning (CFL), addressing heterogeneity and inter-organizational competition to improve social welfare.


<details>
  <summary>Details</summary>
Motivation: In CFL with privacy constraints, statistical heterogeneity and competitive market behavior reduce collaboration and welfare; there is a need for a framework that models both factors and guides data-sharing strategies.

Method: Model competition and heterogeneity via learning performance and utility-based formulations; treat each training round as a weighted potential game; derive GenAI-based data generation strategies to maximize social welfare.

Result: Empirical evaluation on Fashion-MNIST shows CoCoGen robustly outperforms baselines and reveals how varying heterogeneity and competition levels influence organizational behavior and welfare.

Conclusion: CoCoGen provides a practical framework integrating GenAI and game theory to enable coopetitive CFL, offering higher social welfare under heterogeneous and competitive settings and guiding incentive-compatible data-sharing designs.

Abstract: Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or
banks) to collaboratively train artificial intelligence (AI) models while
preserving data privacy by keeping data local. While prior work has primarily
addressed statistical heterogeneity across organizations, a critical challenge
arises from economic competition, where organizations may act as market rivals,
making them hesitant to participate in joint training due to potential utility
loss (i.e., reduced net benefit). Furthermore, the combined effects of
statistical heterogeneity and inter-organizational competition on
organizational behavior and system-wide social welfare remain underexplored. In
this paper, we propose CoCoGen, a coopetitive-compatible data generation
framework, leveraging generative AI (GenAI) and potential game theory to model,
analyze, and optimize collaborative learning under heterogeneous and
competitive settings. Specifically, CoCoGen characterizes competition and
statistical heterogeneity through learning performance and utility-based
formulations and models each training round as a weighted potential game. We
then derive GenAI-based data generation strategies that maximize social
welfare. Experimental results on the Fashion-MNIST dataset reveal how varying
heterogeneity and competition levels affect organizational behavior and
demonstrate that CoCoGen consistently outperforms baseline methods.

</details>


### [244] [Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters](https://arxiv.org/abs/2509.18124)
*Edmund Agyemang,Lawrence Agbota,Vincent Agbenyeavu,Peggy Akabuah,Bismark Bimpong,Christopher Attafuah*

Main category: cs.LG

TL;DR: Supervised ML predicts coffee ratings from textual and numeric review features using TF-IDF and feature selection; six models tested; ensemble methods and MLP outperform simpler models; data-driven approach complements expert cupping.


<details>
  <summary>Details</summary>
Motivation: Develop a data-driven approach to model consumer sensory evaluations and identify factors influencing coffee quality from reviews.

Method: Data preprocessing, text cleaning, TF-IDF feature extraction, SelectKBest feature selection; train six models (Decision Tree, KNN, MLP, Random Forest, Extra Trees, XGBoost) with tuned hyperparameters; evaluate with F1-score, G-mean, AUC.

Result: Ensemble methods (Extra Trees, Random Forest, XGBoost) and MLP achieve higher F1, G-mean, AUC than DT and KNN; feature selection and hyperparameter tuning are key for robust predictions.

Conclusion: Supports a data-driven framework for sensory product evaluation that complements traditional coffee cupping by experts.

Abstract: This study explores the application of supervised machine learning algorithms
to predict coffee ratings based on a combination of influential textual and
numerical attributes extracted from user reviews. Through careful data
preprocessing including text cleaning, feature extraction using TF-IDF, and
selection with SelectKBest, the study identifies key factors contributing to
coffee quality assessments. Six models (Decision Tree, KNearest Neighbors,
Multi-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained
and evaluated using optimized hyperparameters. Model performance was assessed
primarily using F1-score, Gmean, and AUC metrics. Results demonstrate that
ensemble methods (Extra Trees, Random Forest, and XGBoost), as well as
Multi-layer Perceptron, consistently outperform simpler classifiers (Decision
Trees and K-Nearest Neighbors) in terms of evaluation metrics such as F1
scores, G-mean and AUC. The findings highlight the essence of rigorous feature
selection and hyperparameter tuning in building robust predictive systems for
sensory product evaluation, offering a data driven approach to complement
traditional coffee cupping by expertise of trained professionals.

</details>


### [245] [NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment](https://arxiv.org/abs/2509.18125)
*Harsha Koduri*

Main category: cs.LG

TL;DR: NurseSchedRL is a PPO-based reinforcement learning framework for nurse-patient assignment that uses constrained action masking and attention-based state representations to handle skill heterogeneity, patient acuity, fatigue, and geography, achieving better efficiency, skill alignment, and fatigue reduction in simulation vs baselines.


<details>
  <summary>Details</summary>
Motivation: Healthcare systems must optimally allocate scarce nursing resources under multiple dynamic constraints (skill mix, patient acuity, fatigue, continuity, geography). Traditional optimization/heuristics struggle to adapt to arrivals and variable availability.

Method: Proximal Policy Optimization (PPO) with feasibility masks to enforce real-world constraints, structured state encoding, and attention-based representations of skills, fatigue, and geographical context; acts within dynamic nurse availability and patient arrivals.

Result: Simulation with realistic data shows NurseSchedRL improves scheduling efficiency, better alignment of nurse skills to patient needs, and reduced fatigue compared to baseline heuristics and unconstrained RL.

Conclusion: Reinforcement learning, with careful constraint handling and representation, has strong potential as decision support for complex, high-stakes healthcare workforce management.

Abstract: Healthcare systems face increasing pressure to allocate limited nursing
resources efficiently while accounting for skill heterogeneity, patient acuity,
staff fatigue, and continuity of care. Traditional optimization and heuristic
scheduling methods struggle to capture these dynamic, multi-constraint
environments. I propose NurseSchedRL, a reinforcement learning framework for
nurse-patient assignment that integrates structured state encoding, constrained
action masking, and attention-based representations of skills, fatigue, and
geographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with
feasibility masks to ensure assignments respect real-world constraints, while
dynamically adapting to patient arrivals and varying nurse availability. In
simulation with realistic nurse and patient data, NurseSchedRL achieves
improved scheduling efficiency, better alignment of skills to patient needs,
and reduced fatigue compared to baseline heuristic and unconstrained RL
approaches. These results highlight the potential of reinforcement learning for
decision support in complex, high-stakes healthcare workforce management.

</details>


### [246] [Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning](https://arxiv.org/abs/2509.18126)
*Bishal K C,Amr Hilal,Pawan Thapa*

Main category: cs.LG

TL;DR: FedAvg and FedAvgM can be used for anomaly detection in IoT-based EV charging stations, with FedAvgM offering better robustness under data and system heterogeneity; FedAvg can match centralized models under IID, but struggles with non-IID data and heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Address privacy and data-sensitivity concerns in IDS for EV charging networks by using Federated Learning to train across distributed IoT devices without sharing raw data, while handling real-world heterogeneity.

Method: Empirical evaluation of FL-based anomaly detection using FedAvg and FedAvgM on EV charging station data under IID and non-IID distributions and with system heterogeneity to assess convergence and detection accuracy.

Result: Under IID, FedAvg can outperform a centralized model with the same NN, but performance drops with non-IID data and heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous settings, achieving better convergence and higher anomaly detection accuracy.

Conclusion: FL is viable for privacy-preserving IDS in IoT-based EVCS, with FedAvgM as a promising approach to robustly handle heterogeneity and privacy concerns in real-world deployments.

Abstract: Federated Learning (FL) is a decentralized training framework widely used in
IoT ecosystems that preserves privacy by keeping raw data local, making it
ideal for IoT-enabled cyber-physical systems with sensing and communication
like Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric
Vehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle
infrastructure, securing these IoT-based charging stations against cyber
threats has become critical. Centralized Intrusion Detection Systems (IDS)
raise privacy concerns due to sensitive network and user data, making FL a
promising alternative. However, current FL-based IDS evaluations overlook
practical challenges such as system heterogeneity and non-IID data. To address
these challenges, we conducted experiments to evaluate the performance of
federated learning for anomaly detection in EV charging stations under system
and data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization
approaches, to analyze their effectiveness in anomaly detection. Under IID
settings, FedAvg achieves superior performance to centralized models using the
same neural network. However, performance degrades with non-IID data and system
heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous
settings, showing better convergence and higher anomaly detection accuracy. Our
results demonstrate that FL can handle heterogeneity in IoT-based EVCS without
significant performance loss, with FedAvgM as a promising solution for robust,
privacy-preserving EVCS security.

</details>


### [247] [Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework](https://arxiv.org/abs/2509.18127)
*Jiaqi Weng,Han Zheng,Hanyu Zhang,Qinqin He,Jialing Tao,Hui Xue,Zhixuan Chu,Xiting Wang*

Main category: cs.LG

TL;DR: Safe-SAIL proposes a framework to extract and interpret Sparse Autoencoder features in LLMs to identify safety-related neurons and scale interpretation, improving mechanistic safety analysis.


<details>
  <summary>Details</summary>
Motivation: Current safety research is limited to evaluating outputs or specific safety tasks; there is a need to uncover a diverse set of safety-relevant features in LLMs to understand and mitigate high-risk behaviors.

Method: Systematically identify SAEs with high concept-specific interpretability; explain safety-related neurons; introduce scalable strategies for feature explanation; plan to release a toolkit with SAE checkpoints and human-readable explanations.

Result: Framework and strategies established; evidence likely to yield interpretable safety concepts and practical scalability; the abstract outlines the approach but provides no empirical results.

Conclusion: Safe-SAIL can enhance mechanistic safety analysis of LLMs; a public toolkit will support empirical safety risk analysis and promote research on LLM safety.

Abstract: Increasing deployment of large language models (LLMs) in real-world
applications raises significant safety concerns. Most existing safety research
focuses on evaluating LLM outputs or specific safety tasks, limiting their
ability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)
facilitate interpretability research to clarify model behavior by explaining
single-meaning atomic features decomposed from entangled signals. jHowever,
prior applications on SAEs do not interpret features with fine-grained
safety-related con- cepts, thus inadequately addressing safety-critical
behaviors, such as generating toxic responses and violating safety regu-
lations. For rigorous safety analysis, we must extract a rich and diverse set
of safety-relevant features that effectively capture these high-risk behaviors,
yet face two challenges: identifying SAEs with the greatest potential for
generating safety concept-specific neurons, and the prohibitively high cost of
detailed feature explanation. In this paper, we pro- pose Safe-SAIL, a
framework for interpreting SAE features within LLMs to advance mechanistic
understanding in safety domains. Our approach systematically identifies SAE
with best concept-specific interpretability, explains safety-related neurons,
and introduces efficient strategies to scale up the in- terpretation process.
We will release a comprehensive toolkit including SAE checkpoints and
human-readable neuron ex- planations, which supports empirical analysis of
safety risks to promote research on LLM safety.

</details>


### [248] [Accounting for Uncertainty in Machine Learning Surrogates: A Gauss-Hermite Quadrature Approach to Reliability Analysis](https://arxiv.org/abs/2509.18128)
*Amirreza Tootchi,Xiaoping Du*

Main category: cs.LG

TL;DR: A Gauss-Hermite quadrature-based framework decouples epistemic uncertainty from aleatory uncertainty in ML surrogate-based reliability analysis, by computing conditional failure probabilities with FORM/SORM and integrating over model uncertainty, achieving accurate and efficient predictions.


<details>
  <summary>Details</summary>
Motivation: ML surrogates reduce computational cost but introduce epistemic (model) uncertainty. When combined with aleatory input uncertainty, reliability predictions can be biased. A method to decouple these nested uncertainties is needed for credible analyses.

Method: Compute conditional failure probabilities under aleatory uncertainty using First and Second Order Reliability Methods (FORM/SORM). Then integrate these probabilities across realizations of epistemic uncertainty using Gauss-Hermite quadrature.

Result: The approach maintains computational efficiency and yields more trustworthy reliability predictions than traditional methods that ignore model uncertainty, as demonstrated by three examples.

Conclusion: Gauss-Hermite quadrature provides an effective means to decouple nested epistemic and aleatory uncertainties in ML surrogate-based reliability analysis, leading to more credible predictions without excessive computational cost.

Abstract: Machine learning surrogates are increasingly employed to replace expensive
computational models for physics-based reliability analysis. However, their use
introduces epistemic uncertainty from model approximation errors, which couples
with aleatory uncertainty in model inputs, potentially compromising the
accuracy of reliability predictions. This study proposes a Gauss-Hermite
quadrature approach to decouple these nested uncertainties and enable more
accurate reliability analysis. The method evaluates conditional failure
probabilities under aleatory uncertainty using First and Second Order
Reliability Methods and then integrates these probabilities across realizations
of epistemic uncertainty. Three examples demonstrate that the proposed approach
maintains computational efficiency while yielding more trustworthy predictions
than traditional methods that ignore model uncertainty.

</details>


### [249] [Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model](https://arxiv.org/abs/2509.18130)
*Zijie Zhou,Huichen Ma*

Main category: cs.LG

TL;DR: A STL-decomposed time-series + GRU model for metro transfer passenger flow prediction, outperforming LSTM, GRU, and STL-LSTM baselines; validated on station data with notable MAPE improvements.


<details>
  <summary>Details</summary>
Motivation: Advance metro internal transfer passenger flow prediction accuracy and reliability for intelligent operation decisions by combining STL time-series decomposition with a GRU model and path-based feature extraction and outlier handling.

Method: Build a transfer flow time series from card swipe data using graph-based DFS to identify travel paths; apply STL to decompose into trend, seasonal, and residual components; remove/fill outliers in the residual via the 3-sigma rule; train a GRU model (Keras) on the processed data; compare against LSTM, GRU, and STL-LSTM baselines; evaluate on data from a specific metro station.

Result: STL-GRU achieved higher prediction accuracy than the baselines, with mean absolute percentage error (MAPE) reductions of at least 2.3 percentage points on weekdays (excluding Fridays), 1.36 pp on Fridays, and 6.42 pp on rest days.

Conclusion: The STL-GRU approach is an effective and superior method for metro transfer passenger flow prediction, offering improved accuracy and practical value for metro operation planning; the study demonstrates the benefit of integrating STL decomposition with GRU and path-aware preprocessing.

Abstract: In the metro intelligent transportation system, accurate transfer passenger
flow prediction is a key link in optimizing operation plans and improving
transportation efficiency. To further improve the theory of metro internal
transfer passenger flow prediction and provide more reliable support for
intelligent operation decisions, this paper innovatively proposes a metro
transfer passenger flow prediction model that integrates the Seasonal and Trend
decomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In
practical application, the model first relies on the deep learning library
Keras to complete the construction and training of the GRU model, laying the
foundation for subsequent prediction; then preprocesses the original metro card
swiping data, uses the graph-based depth-first search algorithm to identify
passengers' travel paths, and further constructs the transfer passenger flow
time series; subsequently adopts the STL time series decomposition algorithm to
decompose the constructed transfer passenger flow time series into trend
component, periodic component and residual component, and uses the 3{\sigma}
principle to eliminate and fill the outliers in the residual component, and
finally completes the transfer passenger flow prediction.Taking the transfer
passenger flow data of a certain metro station as the research sample, the
validity of the model is verified. The results show that compared with Long
Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of
STL time series decomposition method and Long Short-Term Memory (STL-LSTM), the
STL-GRU combined prediction model significantly improves the prediction
accuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays
and rest days, with the mean absolute percentage error (MAPE) of the prediction
results reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.

</details>


### [250] [Two ways to knowledge?](https://arxiv.org/abs/2509.18131)
*Jean-Michel Tucny,Abhisek Ganguly,Santosh Ansumali,Sauro Succi*

Main category: cs.LG

TL;DR: Transformer weights in two physical applications are random-like and show no direct link to underlying physics; ML and scientific inquiry may be complementary, but strict explainability remains elusive.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether transformer-based ML can reveal physically meaningful structure and to probe the limits of explaining learned representations in physics.

Method: Empirical analysis of weight matrices from transformers applied to two representative physical problems; discussion framed around comparisons to generalized path-integration concepts.

Result: Weight matrices exhibit random-like behavior with no clear correspondence to the physical/mathematical structure; the path-integration analogy offers partial intuition but does not resolve explainability tensions.

Conclusion: Cautions against gleaning knowledge without genuine insight; highlights hazards and the need for careful interpretation of ML results in physical science.

Abstract: It is shown that the weight matrices of transformer-based machine learning
applications to the solution of two representative physical applications show a
random-like character which bears no directly recognizable link to the physical
and mathematical structure of the physical problem under study. This suggests
that machine learning and the scientific method may represent two distinct and
potentially complementary paths to knowledge, even though a strict notion of
explainability in terms of direct correspondence between network parameters and
physical structures may remain out of reach. It is also observed that drawing a
parallel between transformer operation and (generalized) path-integration
techniques may account for the random-like nature of the weights, but still
does not resolve the tension with explainability. We conclude with some general
comments on the hazards of gleaning knowledge without the benefit of Insight.

</details>


### [251] [Self-Evolving LLMs via Continual Instruction Tuning](https://arxiv.org/abs/2509.18133)
*Le Huang,Jiazheng Kang,Cheng Hou,Zhe Zhao,Zhenxiang Yan,Chuan Shi,Ting Bai*

Main category: cs.LG

TL;DR: MoE-CL is a parameter-efficient dual-expert mixture-of-experts framework for continual instruction tuning of LLMs that uses per-task LoRA experts to prevent forgetting and a shared LoRA expert guided by a task-aware discriminator (GAN) to enable cross-task transfer; it demonstrates effective continual learning and cost reduction in industrial settings, validated on public and Tencent benchmarks and A/B tests.


<details>
  <summary>Details</summary>
Motivation: In real-world industrial settings, LLMs must continually learn under changing data distributions; existing CL methods (replay, isolation) suffer catastrophic forgetting and poor generalization, hindering self-evolution and deployment at scale.

Method: Propose MoE-CL with two LoRA experts: a dedicated per-task LoRA expert for task-specific knowledge (reducing forgetting) and a shared LoRA expert for cross-task transfer. Introduce a task-aware discriminator within a GAN to filter task-irrelevant noise from the shared path; adversarial training makes the shared expert pass only task-aligned information, yielding generalized representations while preserving task details in dedicated experts.

Result: Empirical validation on MTL5 and Tencent3 benchmarks shows effectiveness for continual instruction tuning; real-world A/B tests on Tencent Video platform show 15.3% reduction in manual review costs.

Conclusion: MoE-CL provides a practical, scalable solution for industrial continual learning with stable transfer and self-evolution in LLMs.

Abstract: In real-world industrial settings, large language models (LLMs) must learn
continually to keep pace with diverse and evolving tasks, requiring
self-evolution to refine knowledge under dynamic data distributions. However,
existing continual learning (CL) approaches, such as replay and parameter
isolation, often suffer from catastrophic forgetting: training on new tasks
degrades performance on earlier ones by overfitting to the new distribution and
weakening generalization.We propose MoE-CL, a parameter-efficient adversarial
mixture-of-experts framework for industrial-scale, self-evolving continual
instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated
LoRA expert per task to preserve task-specific knowledge via parameter
independence, mitigating forgetting; and (2) a shared LoRA expert to enable
cross-task transfer. To prevent transferring task-irrelevant noise through the
shared pathway, we integrate a task-aware discriminator within a GAN. The
discriminator encourages the shared expert to pass only task-aligned
information during sequential training. Through adversarial learning, the
shared expert acquires generalized representations that mimic the
discriminator, while dedicated experts retain task-specific details, balancing
knowledge retention and cross-task generalization and thereby supporting
self-evolution.Extensive experiments on the public MTL5 benchmark and an
industrial Tencent3 benchmark validate the effectiveness of MoE-CL for
continual instruction tuning. In real-world A/B testing for content compliance
review on the Tencent Video platform, MoE-CL reduced manual review costs by
15.3%. These results demonstrate that MoE-CL is practical for large-scale
industrial deployment where continual adaptation and stable transfer are
critical.

</details>


### [252] [A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization](https://arxiv.org/abs/2509.18134)
*Furan Xie,Bing Liu,Li Chai*

Main category: cs.LG

TL;DR: Proposes a privacy-preserving gradient-tracking method with decaying weights to remove leakage, proves convergence under time-varying step sizes, and validates on estimation and CNN training.


<details>
  <summary>Details</summary>
Motivation: Gradient tracking in distributed optimization can leak private information about agents. There is a need to design privacy-preserving algorithms that maintain convergence guarantees while mitigating privacy risks.

Method: Introduce a weighted gradient tracking approach using decaying weight factors to obfuscate gradient information. Analyze convergence under time-varying heterogeneous step sizes and mild assumptions, proving convergence to the optimal solution.

Result: The algorithm eliminates privacy leakage risk inherent in standard gradient tracking and converges precisely to the optimal solution under the stated conditions. Numerical experiments on a distributed estimation problem and distributed CNN training corroborate effectiveness.

Conclusion: The work demonstrates that privacy-preserving distributed optimization is feasible with gradient-tracking-based methods, offering a practical approach for secure multi-agent optimization and potential applicability to various distributed learning tasks.

Abstract: This paper investigates the privacy-preserving distributed optimization
problem, aiming to protect agents' private information from potential attackers
during the optimization process. Gradient tracking, an advanced technique for
improving the convergence rate in distributed optimization, has been applied to
most first-order algorithms in recent years. We first reveal the inherent
privacy leakage risk associated with gradient tracking. Building upon this
insight, we propose a weighted gradient tracking distributed privacy-preserving
algorithm, eliminating the privacy leakage risk in gradient tracking using
decaying weight factors. Then, we characterize the convergence of the proposed
algorithm under time-varying heterogeneous step sizes. We prove the proposed
algorithm converges precisely to the optimal solution under mild assumptions.
Finally, numerical simulations validate the algorithm's effectiveness through a
classical distributed estimation problem and the distributed training of a
convolutional neural network.

</details>


### [253] [SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.18135)
*Shaoxun Wang,Xingjun Zhang,Qianyang Li,Jiawei Cao,Zhendong Tan*

Main category: cs.LG

TL;DR: Presents SDGF, a dual-path graph fusion model for multivariate time series forecasting that combines a static, prior-based graph with a dynamically learned multi-scale graph via wavelet decomposition, fused through attention, and enhanced by a multi-kernel dilated convNet.


<details>
  <summary>Details</summary>
Motivation: Inter-series relationships in multivariate time series are both multi-scale and evolving over time; existing methods struggle to model stable long-term dependencies alongside scale-specific dynamic interactions.

Method: A static graph anchored by prior knowledge handles long-term dependencies; multi-level wavelet decomposition yields multi-scale features to construct an adaptive dynamic graph; an attention-gated module fuses static and dynamic graphs; a multi-kernel dilated convolutional network deepens temporal pattern modeling.

Result: Comprehensive experiments on widely used real-world benchmark datasets demonstrate the effectiveness of SDGF, outperforming baselines and validating the approach.

Conclusion: SDGF effectively captures multi-scale inter-series correlations, improving forecasting performance and offering a principled framework for leveraging both prior knowledge and learned scale-specific dynamics in multivariate time series.

Abstract: Inter-series correlations are crucial for accurate multivariate time series
forecasting, yet these relationships often exhibit complex dynamics across
different temporal scales. Existing methods are limited in modeling these
multi-scale dependencies and struggle to capture their intricate and evolving
nature. To address this challenge, this paper proposes a novel Static-Dynamic
Graph Fusion network (SDGF), whose core lies in capturing multi-scale
inter-series correlations through a dual-path graph structure learning
approach. Specifically, the model utilizes a static graph based on prior
knowledge to anchor long-term, stable dependencies, while concurrently
employing Multi-level Wavelet Decomposition to extract multi-scale features for
constructing an adaptively learned dynamic graph to capture associations at
different scales. We design an attention-gated module to fuse these two
complementary sources of information intelligently, and a multi-kernel dilated
convolutional network is then used to deepen the understanding of temporal
patterns. Comprehensive experiments on multiple widely used real-world
benchmark datasets demonstrate the effectiveness of our proposed model.

</details>


### [254] [From Parameters to Performance: A Data-Driven Study on LLM Structure and Development](https://arxiv.org/abs/2509.18136)
*Suqing Wang,Zuchao Li,Luohe Shi,Bo Du,Hai Zhao,Yun Li,Qianren Wang*

Main category: cs.LG

TL;DR: A large-scale dataset links open-source LLM structures to cross-benchmark performance; data mining reveals how design choices influence results, supported by mechanistic interpretability, with the dataset released for community use.


<details>
  <summary>Details</summary>
Motivation: Addresses the gap of systematic, data-driven understanding of how LLM architectural and training configurations affect performance across benchmarks, to guide future model development.

Method: Assembles a diverse dataset of open-source LLM structures and their benchmark performance; performs data-mining analysis to quantify structure-performance relationships; reviews historical development and explores future trends; corroborates findings with mechanistic interpretability techniques.

Result: Validated and quantified relationships between structural configurations and performance across benchmarks; identifies key configuration factors that influence performance; mechanistic interpretability provides supporting insights; dataset released at HuggingFace for public use.

Conclusion: Data-driven insights can guide targeted optimization and application of future LLMs; the released dataset serves as a resource for researchers and practitioners; anticipates evolving trends and further refinement of design heuristics.

Abstract: Large language models (LLMs) have achieved remarkable success across various
domains, driving significant technological advancements and innovations.
Despite the rapid growth in model scale and capability, systematic, data-driven
research on how structural configurations affect performance remains scarce. To
address this gap, we present a large-scale dataset encompassing diverse
open-source LLM structures and their performance across multiple benchmarks.
Leveraging this dataset, we conduct a systematic, data mining-driven analysis
to validate and quantify the relationship between structural configurations and
performance. Our study begins with a review of the historical development of
LLMs and an exploration of potential future trends. We then analyze how various
structural choices impact performance across benchmarks and further corroborate
our findings using mechanistic interpretability techniques. By providing
data-driven insights into LLM optimization, our work aims to guide the targeted
development and application of future models. We will release our dataset at
https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset

</details>


### [255] [LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods](https://arxiv.org/abs/2509.18137)
*Shaoheng Wang,Yao Lu,Yuqi Li,Yaxin Gao,Jiaqi Nie,Shanqing Yu,Yingli Tian,Qi Xuan*

Main category: cs.LG

TL;DR: A unified benchmark LoRALib standardizes data, hyperparameters, and evaluation for LoRA-MoE models, enabling fair cross-task comparisons; findings show LoRAMoE is best and task-relevant LoRA selection helps.


<details>
  <summary>Details</summary>
Motivation: Inconsistent evaluation standards across LoRA-MoE studies hinder fair benchmarking and progress; a unified benchmark across many tasks is needed to assess cross-task generalization and provide reliable baselines.

Method: Created LoRALib by collecting 40 downstream tasks, converting to a unified dataset format; fine-tuned using identical hyperparameters to produce 680 LoRA modules across 17 model architectures; evaluated 3 LoRA-MoE methods and various LoRA selection strategies using OpenCompass; released LoRA library and datasets.

Result: LoRAMoE method yielded the best performance on average; selecting LoRAs relevant to the target task further improves MoE performance.

Conclusion: LoRALib provides a standardized benchmark for fair comparison and future research; resources released publicly to foster reproducibility and progress in LoRA-MoE research.

Abstract: As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation
(LoRA) can save significant costs in storage and computing, but its strong
adaptability to a single task is often accompanied by insufficient cross-task
generalization capabilities. To improve this, existing work combines LoRA with
mixture-of-experts (MoE) to enhance the model's adaptability through expert
modules and routing mechanisms. However, existing LoRA-MoE methods lack unified
standards in models, datasets, hyperparameters, and evaluation methods, making
it difficult to conduct fair comparisons between different methods. To this
end, we proposed a unified benchmark named LoRALib. Specifically, we
standardized datasets from $40$ downstream tasks into a unified format,
fine-tuned them using the same hyperparameters and obtained $680$ LoRA modules
across $17$ model architectures. Based on this LoRA library, we conduct
large-scale experiments on $3$ representative LoRA-MoE methods and different
LoRA selection mechanisms using the open-sourced testing tool OpenCompass.
Extensive experiments show that LoRAMoE performs best, and that prioritizing
LoRAs relevant to the target task can further improve the performance of MoE.
We hope these findings will inspire future work. Our datasets and LoRA library
are available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset
and https://huggingface.co/YaoLuzjut/models.

</details>


### [256] [Rank-Induced PL Mirror Descent: A Rank-Faithful Second-Order Algorithm for Sleeping Experts](https://arxiv.org/abs/2509.18138)
*Tiantian Zhang*

Main category: cs.LG

TL;DR: Introduces RIPLM, a rank-faithful, variance-adaptive Plackett-Luce-based mirror-descent algorithm that updates in the rank-induced PL parameterization to preserve equivalence with the rank benchmark in sleeping-experts settings.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of previous approaches that optimize over expert identities by operating in a rank-induced distribution space, ensuring the theoretical equivalence with the rank benchmark and enabling variance-adaptive behavior.

Method: Develops Rank-Induced Plackett-Luce Mirror Descent (RIPLM) that updates directly in the rank-induced Plackett–Luce parameterization, ensuring all iterates remain within rank-induced distributions and preserving the rank-benchmark equivalence; applicable to sleeping experts.

Result: Claims that RIPLM is the first algorithm that is both rank-faithful and variance-adaptive in the sleeping experts setting.

Conclusion: RIPLM advances online ranking by working directly in the rank-induced PL space to maintain structural equivalences and enable variance-adaptive behavior.

Abstract: We introduce a new algorithm, \emph{Rank-Induced Plackett--Luce Mirror
Descent (RIPLM)}, which leverages the structural equivalence between the
\emph{rank benchmark} and the \emph{distributional benchmark} established in
\citet{BergamOzcanHsu2022}. Unlike prior approaches that operate on expert
identities, RIPLM updates directly in the \emph{rank-induced Plackett--Luce
(PL)} parameterization. This ensures that the algorithm's played distributions
remain within the class of rank-induced distributions at every round,
preserving the equivalence with the rank benchmark. To our knowledge, RIPLM is
the first algorithm that is both (i) \emph{rank-faithful} and (ii)
\emph{variance-adaptive} in the sleeping experts setting.

</details>


### [257] [Comparative Analysis of FOLD-SE vs. FOLD-R++ in Binary Classification and XGBoost in Multi-Category Classification](https://arxiv.org/abs/2509.18139)
*Akshay Murthy,Shawn Sebastian,Manil Shangle,Huaduo Wang,Sopam Dasgupta,Gopal Gupta*

Main category: cs.LG

TL;DR: FOLD-SE offers competitive accuracy with interpretable rules; outperforms FOLD-R++ in binary tasks with fewer rules, and in multiclass tasks it beats XGBoost on precision and efficiency while providing explainable rules.


<details>
  <summary>Details</summary>
Motivation: There is a growing need to balance accuracy, efficiency, and interpretability in ML. The paper compares rule-based classifiers (FOLD-SE and FOLD-R++) and a black-box baseline (XGBoost) to evaluate tradeoffs between explainability and performance.

Method: Experimental comparison on classification tasks: binary classification comparing FOLD-SE and FOLD-R++; multiclass classification comparing FOLD-SE with XGBoost. Metrics include accuracy, F1 score, and processing time. Datasets are from general classification collections.

Result: In binary classification, FOLD-SE outperforms FOLD-R++ by using fewer rules with only a minor reduction in accuracy and processing time. In multiclass tasks, FOLD-SE is more precise and significantly more efficient than XGBoost and also provides an interpretable rule set.

Conclusion: Rule-based models like FOLD-SE can bridge the gap between explainability and performance, offering viable, interpretable alternatives to black-box models across binary and multiclass classification tasks.

Abstract: Recently, the demand for Machine Learning (ML) models that can balance
accuracy, efficiency, and interpreability has grown significantly.
Traditionally, there has been a tradeoff between accuracy and explainability in
predictive models, with models such as Neural Networks achieving high accuracy
on complex datasets while sacrificing internal transparency. As such, new
rule-based algorithms such as FOLD-SE have been developed that provide tangible
justification for predictions in the form of interpretable rule sets. The
primary objective of this study was to compare FOLD-SE and FOLD-R++, both
rule-based classifiers, in binary classification and evaluate how FOLD-SE
performs against XGBoost, a widely used ensemble classifier, when applied to
multi-category classification. We hypothesized that because FOLD-SE can
generate a condensed rule set in a more explainable manner, it would lose
upwards of an average of 3 percent in accuracy and F1 score when compared with
XGBoost and FOLD-R++ in multiclass and binary classification, respectively. The
research used data collections for classification, with accuracy, F1 scores,
and processing time as the primary performance measures. Outcomes show that
FOLD-SE is superior to FOLD-R++ in terms of binary classification by offering
fewer rules but losing a minor percentage of accuracy and efficiency in
processing time; in tasks that involve multi-category classifications, FOLD-SE
is more precise and far more efficient compared to XGBoost, in addition to
generating a comprehensible rule set. The results point out that FOLD-SE is a
better choice for both binary tasks and classifications with multiple
categories. Therefore, these results demonstrate that rule-based approaches
like FOLD-SE can bridge the gap between explainability and performance,
highlighting their potential as viable alternatives to black-box models in
diverse classification tasks.

</details>


### [258] [A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders](https://arxiv.org/abs/2509.18140)
*Iram Wajahat,Amritpal Singh,Fazel Keshtkar,Syed Ahmad Chan Bukhari*

Main category: cs.LG

TL;DR: A predictive ML framework with gene-agnostic pathway mapping identifies T2DM risk in Pima Indians and suggests mechanistic targets and therapies.


<details>
  <summary>Details</summary>
Motivation: T2DM is a major global health burden, with worse impact on genetically predisposed groups; there is a need for interpretable, mechanism-informed predictions that bridge analytics with biology without requiring direct molecular data.

Method: Apply logistic regression and t-tests on the Pima Indian dataset to identify predictors; use PCA for dimensionality reduction; achieve 78.43% accuracy; develop a gene-agnostic pathway mapping linking predictors to signaling networks (insulin, AMPK, PPAR); validate via pathway enrichment analyses; propose therapeutic strategies.

Result: Model achieved 78.43% accuracy in predicting T2DM; provided mechanistic insights by mapping predictors to key signaling pathways; proposed therapeutic strategies including dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1 modulators, and phytochemicals; presented as a framework for interpretable and scalable precision medicine.

Conclusion: The work offers a framework that couples predictive ML with pathway-based interpretation to enable early detection and targeted intervention for metabolic disorders, especially in high-risk populations.

Abstract: Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent
a significant global health burden, disproportionately impacting genetically
predisposed populations such as the Pima Indians (a Native American tribe from
south central Arizona). This study introduces a novel machine learning (ML)
framework that integrates predictive modeling with gene-agnostic pathway
mapping to identify high-risk individuals and uncover potential therapeutic
targets. Using the Pima Indian dataset, logistic regression and t-tests were
applied to identify key predictors of T2DM, yielding an overall model accuracy
of 78.43%. To bridge predictive analytics with biological relevance, we
developed a pathway mapping strategy that links identified predictors to
critical signaling networks, including insulin signaling, AMPK, and PPAR
pathways. This approach provides mechanistic insights without requiring direct
molecular data. Building upon these connections, we propose therapeutic
strategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1
modulators, and phytochemical, further validated through pathway enrichment
analyses. Overall, this framework advances precision medicine by offering
interpretable and scalable solutions for early detection and targeted
intervention in metabolic disorders. The key contributions of this work are:
(1) development of an ML framework combining logistic regression and principal
component analysis (PCA) for T2DM risk prediction; (2) introduction of a
gene-agnostic pathway mapping approach to generate mechanistic insights; and
(3) identification of novel therapeutic strategies tailored for high-risk
populations.

</details>


### [259] [KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots](https://arxiv.org/abs/2509.18141)
*Yao Zhao,Haoyue Sun,Yantian Ding,Yanxun Xu*

Main category: cs.LG

TL;DR: Automated AI-based pipeline (KM-GPT) reconstructs IPD from Kaplan-Meier plots without manual digitization, achieving high accuracy and reproducibility with a web interface; validated on synthetic/real data and applied to gastric cancer immunotherapy meta-analysis.


<details>
  <summary>Details</summary>
Motivation: Manual digitization of KM plots is error-prone and not scalable; reconstructing IPD enables robust evidence synthesis and downstream subgroup analyses.

Method: KM-GPT integrates image preprocessing, multi-modal reasoning via GPT-5, and iterative IPD reconstruction; hybrid architecture for data extraction and validation; web interface and AI assistant for accessibility; evaluated on synthetic and real-world datasets; applied to gastric cancer immunotherapy meta-analysis.

Result: Consistently superior accuracy across datasets; demonstrates robustness and reproducibility; enables downstream evidence synthesis and biomarker-based subgroup analyses.

Conclusion: Automates IPD reconstruction from KM plots at scale, enabling more efficient and transparent evidence synthesis; provides a user-friendly platform; further validation and consideration of data quality, bias, and generalizability are warranted.

Abstract: Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots
provides valuable insights for evidence synthesis in clinical research.
However, existing approaches often rely on manual digitization, which is
error-prone and lacks scalability. To address these limitations, we develop
KM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD
directly from KM plots with high accuracy, robustness, and reproducibility.
KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered
by GPT-5, and iterative reconstruction algorithms to generate high-quality IPD
without manual input or intervention. Its hybrid reasoning architecture
automates the conversion of unstructured information into structured data flows
and validates data extraction from complex KM plots. To improve accessibility,
KM-GPT is equipped with a user-friendly web interface and an integrated AI
assistant, enabling researchers to reconstruct IPD without requiring
programming expertise. KM-GPT was rigorously evaluated on synthetic and
real-world datasets, consistently demonstrating superior accuracy. To
illustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer
immunotherapy trials, reconstructing IPD to facilitate evidence synthesis and
biomarker-based subgroup analyses. By automating traditionally manual processes
and providing a scalable, web-based solution, KM-GPT transforms clinical
research by leveraging reconstructed IPD to enable more informed downstream
analyses, supporting evidence-based decision-making.

</details>


### [260] [AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation](https://arxiv.org/abs/2509.18144)
*Yubo Yang,Yichen Zhu,Bo Jiang*

Main category: cs.LG

TL;DR: AdaSTI introduces an adaptive dependency model for diffusion-based spatio-temporal imputation, using a BiS4PI pre-imputation with a Spatio-Temporal Conditionalizer and a Noise-Aware Spatio-Temporal network to handle varying dependencies across diffusion steps, achieving substantial imputation error reductions on real data.


<details>
  <summary>Details</summary>
Motivation: Spatio-temporal data often contains missing values due to sensor issues. Diffusion models show strong performance, but existing methods propagate errors through conditional dependencies and ignore variability of dependencies at different diffusion steps. An adaptive dependency mechanism can improve imputation accuracy and robustness.

Method: Propose AdaSTI, combining (1) BiS4PI, a bi-directional S4-based pre-imputation network that generates an initial imputed result; (2) Spatio-Temporal Conditionalizer (STC) to extract conditional information from the pre-imputed data; (3) Noise-Aware Spatio-Temporal (NAST) network with gated attention to capture varying dependencies across diffusion steps; all within a diffusion-model framework for imputation.

Result: Experiments on three real-world datasets show AdaSTI outperforms existing methods in all settings, with up to 46.4% reduction in imputation error.

Conclusion: Adaptive dependency modeling within diffusion-based spatio-temporal imputation effectively mitigates error accumulation and leverages time-varying dependencies to boost imputation performance.

Abstract: Spatio-temporal data abounds in domain like traffic and environmental
monitoring. However, it often suffers from missing values due to sensor
malfunctions, transmission failures, etc. Recent years have seen continued
efforts to improve spatio-temporal data imputation performance. Recently
diffusion models have outperformed other approaches in various tasks, including
spatio-temporal imputation, showing competitive performance. Extracting and
utilizing spatio-temporal dependencies as conditional information is vital in
diffusion-based methods. However, previous methods introduce error accumulation
in this process and ignore the variability of the dependencies in the noisy
data at different diffusion steps. In this paper, we propose AdaSTI (Adaptive
Dependency Model in Diffusion-based Spatio-Temporal Imputation), a novel
spatio-temporal imputation approach based on conditional diffusion model.
Inside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model
for pre-imputation with the imputed result used to extract conditional
information by our designed Spatio-Temporal Conditionalizer (STC)network. We
also propose a Noise-Aware Spatio-Temporal (NAST) network with a gated
attention mechanism to capture the variant dependencies across diffusion steps.
Extensive experiments on three real-world datasets show that AdaSTI outperforms
existing methods in all the settings, with up to 46.4% reduction in imputation
error.

</details>


### [261] [Early Prediction of Multi-Label Care Escalation Triggers in the Intensive Care Unit Using Electronic Health Records](https://arxiv.org/abs/2509.18145)
*Syed Ahmad Chan Bukhari,Amritpal Singh,Shifath Hossain,Iram Wajahat*

Main category: cs.LG

TL;DR: A multi-label ICU deterioration prediction framework using the first 24 hours of data to predict care escalation triggers (CETs) across four domains (respiratory, hemodynamic, renal, neurological). CETs are defined via rule-based criteria applied to data at 24–72 hours in MIMIC-IV. The model (XGBoost) outperforms baselines with per-label F1 scores in the 0.62–0.76 range, relying on interpretable features (vital signs, labs, demographics) and avoiding complex time-series or NLP. 


<details>
  <summary>Details</summary>
Motivation: Traditional early warning systems (SOFA, MEWS) focus on single outcomes and do not capture the multi-dimensional nature of ICU deterioration. There is a need for multi-label, interpretable alerts that can trigger timely escalation based on a composite signature of physiology.

Method: Define four CETs via rule-based criteria using data from hours 24–72. Extract features from the first 24 hours including vital sign aggregates, laboratory values, and static demographics. Train and evaluate multiple classifiers on 85,242 ICU stays (80/20 split). The best model is XGBoost, assessed with per-label precision, recall, F1-score, and Hamming loss. Feature importance analyzed to identify key predictors (e.g., respiratory rate, BP, creatinine).

Result: XGBoost achieved per-label F1-scores: respiratory 0.66, hemodynamic 0.72, renal 0.76, neurologic 0.62, outperforming baseline models. Framework suggests practical potential for early, interpretable alerts without requiring time-series modeling or NLP.

Conclusion: Proposes a practical, multi-label warning framework that leverages common ICU data within the first 24 hours to predict CETs, enabling early escalation without complex modeling. Clinically relevant features align with CET definitions and offer interpretability.

Abstract: Intensive Care Unit (ICU) patients often present with complex, overlapping
signs of physiological deterioration that require timely escalation of care.
Traditional early warning systems, such as SOFA or MEWS, are limited by their
focus on single outcomes and fail to capture the multi-dimensional nature of
clinical decline. This study proposes a multi-label classification framework to
predict Care Escalation Triggers (CETs), including respiratory failure,
hemodynamic instability, renal compromise, and neurological deterioration,
using the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are
defined through rule-based criteria applied to data from hours 24 to 72 (for
example, oxygen saturation below 90, mean arterial pressure below 65 mmHg,
creatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale
score greater than 2). Features are extracted from the first 24 hours and
include vital sign aggregates, laboratory values, and static demographics. We
train and evaluate multiple classification models on a cohort of 85,242 ICU
stays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation
metrics include per-label precision, recall, F1-score, and Hamming loss.
XGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory,
0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration,
outperforming baseline models. Feature analysis shows that clinically relevant
parameters such as respiratory rate, blood pressure, and creatinine are the
most influential predictors, consistent with the clinical definitions of the
CETs. The proposed framework demonstrates practical potential for early,
interpretable clinical alerts without requiring complex time-series modeling or
natural language processing.

</details>


### [262] [ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks](https://arxiv.org/abs/2509.18147)
*Xinyu Mu,Hui Dou,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: ConceptFlow introduces concept attentions and conceptual pathways to trace concept emergence and propagation across CNN layers, enabling semantically grounded, hierarchical explanations.


<details>
  <summary>Details</summary>
Motivation: Existing concept-based interpretability neglects the semantic role of individual filters and the dynamic spread of concepts across layers, hindering faithful explanations.

Method: Two components: concept attentions (associate filters with relevant high-level concepts for localized interpretation) and conceptual pathways (a concept transition matrix modeling propagation/transform of concepts between filters across layers) to simulate the model's internal reasoning path.

Result: Experimental results show ConceptFlow yields semantically meaningful insights into model reasoning and validates the effectiveness of both components in explaining decisions.

Conclusion: Modeling hierarchical conceptual pathways provides deeper insights into CNNs' internal logic and supports more faithful, human-aligned explanations.

Abstract: Concept-based interpretability for Convolutional Neural Networks (CNNs) aims
to align internal model representations with high-level semantic concepts, but
existing approaches largely overlook the semantic roles of individual filters
and the dynamic propagation of concepts across layers. To address these
limitations, we propose ConceptFlow, a concept-based interpretability framework
that simulates the internal "thinking path" of a model by tracing how concepts
emerge and evolve across layers. ConceptFlow comprises two key components: (i)
concept attentions, which associate each filter with relevant high-level
concepts to enable localized semantic interpretation, and (ii) conceptual
pathways, derived from a concept transition matrix that quantifies how concepts
propagate and transform between filters. Together, these components offer a
unified and structured view of internal model reasoning. Experimental results
demonstrate that ConceptFlow yields semantically meaningful insights into model
reasoning, validating the effectiveness of concept attentions and conceptual
pathways in explaining decision behavior. By modeling hierarchical conceptual
pathways, ConceptFlow provides deeper insight into the internal logic of CNNs
and supports the generation of more faithful and human-aligned explanations.

</details>


### [263] [Sparse Training Scheme for Multimodal LLM](https://arxiv.org/abs/2509.18150)
*Kean Shi,Liang Chen,Haozhe Zhao,Baobao Chang*

Main category: cs.LG

TL;DR: Proposes Sparse Training Scheme (STS) for MLLMs to accelerate training via sparse representations.


<details>
  <summary>Details</summary>
Motivation: Training MLLMs is inefficient due to long multimodal inputs and underutilized inter-layer computations; improving training efficiency is critical.

Method: STS has two components: Visual Token Compressor to reduce visual token load and Layer Dynamic Skipper to dynamically skip unnecessary transformer layers during both forward and backward passes, applicable across diverse MLLM architectures.

Result: Extensive evaluation on multiple benchmarks shows the approach achieves training efficiency gains while maintaining effectiveness.

Conclusion: STS offers a broadly applicable training-focused framework to improve MLLM efficiency through sparse representations and dynamic layer skipping.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated outstanding
performance across a variety of domains. However, training MLLMs is often
inefficient due to the significantly longer input sequences introduced by
multimodal data and the low utilization of inter-layer computations. To address
this challenge, we shift the focus to the training process itself and propose a
novel training-efficient framework based on sparse representations, termed the
Sparse Training Scheme (STS). This scheme consists of two key components: the
Visual Token Compressor, which reduces the information load by compressing
visual tokens, and the Layer Dynamic Skipper, which mitigates the computational
overhead by dynamically skipping unnecessary layers in the language model
during both forward and backward passes. Our approach is broadly applicable to
diverse MLLM architectures and has been extensively evaluated on multiple
benchmarks, demonstrating its effectiveness and efficiency.

</details>


### [264] [HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork](https://arxiv.org/abs/2509.18151)
*Jindi Lv,Yuhao Zhou,Yuxin Tian,Qing Ye,Wentao Feng,Jiancheng Lv*

Main category: cs.LG

TL;DR: HyperNAS introduces a global encoding plus a shared hypernetwork with a dynamic adaptive multi-task loss to improve neural predictor-based NAS. It achieves strong few-shot performance and high accuracy with substantially fewer samples.


<details>
  <summary>Details</summary>
Motivation: Neural Architecture Search (NAS) is time-consuming; existing predictors struggle to generalize across diverse architectures due to limited expressive power for complex architecture relationships.

Method: 1) A global encoding scheme to capture macro-structure information; 2) a shared hypernetwork as an auxiliary task to improve learning of inter-architecture patterns; 3) a dynamic adaptive multi-task loss for training stability and personalized Pareto-front exploration; 4) extensive experiments across five search spaces including ViTs demonstrating few-shot gains.

Result: HyperNAS achieves state-of-the-art few-shot results, e.g., 97.60% top-1 on CIFAR-10 and 82.4% top-1 on ImageNet using at least 5.0x fewer samples.

Conclusion: HyperNAS effectively enhances architecture representation learning, enabling more efficient NAS with strong generalization, particularly in few-shot regimes.

Abstract: Time-intensive performance evaluations significantly impede progress in
Neural Architecture Search (NAS). To address this, neural predictors leverage
surrogate models trained on proxy datasets, allowing for direct performance
predictions for new architectures. However, these predictors often exhibit poor
generalization due to their limited ability to capture intricate relationships
among various architectures. In this paper, we propose HyperNAS, a novel neural
predictor paradigm for enhancing architecture representation learning. HyperNAS
consists of two primary components: a global encoding scheme and a shared
hypernetwork. The global encoding scheme is devised to capture the
comprehensive macro-structure information, while the shared hypernetwork serves
as an auxiliary task to enhance the investigation of inter-architecture
patterns. To ensure training stability, we further develop a dynamic adaptive
multi-task loss to facilitate personalized exploration on the Pareto front.
Extensive experiments across five representative search spaces, including ViTs,
demonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For
instance, HyperNAS strikes new state-of-the-art results, with 97.60\% top-1
accuracy on CIFAR-10 and 82.4\% top-1 accuracy on ImageNet, using at least
5.0$\times$ fewer samples.

</details>


### [265] [WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well Geological Interpretation](https://arxiv.org/abs/2509.18152)
*Zhenyu Qi,Qing Yu,Jichen Wang,Yun-Bo Zhao,Zerui Li,Wenjun Lv*

Main category: cs.LG

TL;DR: WLFM is a scalable foundation model for well-log interpretation trained on 1200 wells, achieving state-of-the-art porosity MSE and lithology accuracy with a three-stage pipeline (tokenization, self-supervised pretraining, few-shot adaptation). It shows emergent layer-awareness, a reusable geological vocabulary, and faithful masked-curve reconstruction, with room for improvement in shallow/ultra-deep intervals and potential boundary-detection extensions.


<details>
  <summary>Details</summary>
Motivation: Address challenges in well-log interpretation due to heterogeneous tool responses, noisy signals, and limited labeled data, by building a foundation model that is transferable, interpretable, and able to integrate multi-modal data.

Method: Three-stage approach: (1) tokenize log patches into geological tokens; (2) self-supervised pretraining with masked-token modeling and stratigraphy-aware contrastive learning; (3) multi-task adaptation with few-shot fine-tuning.

Result: Performance: porosity estimation MSE = 0.0041; lithology classification accuracy = 74.13%; with fine-tuning: MSE = 0.0038; accuracy = 78.10%. Additional findings include emergent layer-awareness, discovery of a reusable geological vocabulary, and faithful masked-curve reconstruction with some offsets in shallow and ultra-deep intervals; boundary detection not directly evaluated.

Conclusion: WLFM emerges as a scalable, interpretable, and transferable backbone for geological AI, enabling potential multi-modal integration of logs, seismic, and text data, with strong predictive performance and interpretability cues.

Abstract: Well-log interpretation is fundamental for subsurface characterization but
remains challenged by heterogeneous tool responses, noisy signals, and limited
labels. We propose WLFM, a foundation model pretrained on multi-curve logs from
1200 wells, comprising three stages: tokenization of log patches into
geological tokens, self-supervised pretraining with masked-token modeling and
stratigraphy-aware contrastive learning, and multi-task adaptation with
few-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines,
achieving 0.0041 MSE in porosity estimation and 74.13\% accuracy in lithology
classification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\%
accuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness,
learns a reusable geological vocabulary, and reconstructs masked curves with
reasonable fidelity, though systematic offsets are observed in shallow and
ultra-deep intervals. Although boundary detection is not explicitly evaluated
here, clustering analyses suggest strong potential for future extension. These
results establish WLFM as a scalable, interpretable, and transferable backbone
for geological AI, with implications for multi-modal integration of logs,
seismic, and textual data.

</details>


### [266] [A deep reinforcement learning platform for antibiotic discovery](https://arxiv.org/abs/2509.18153)
*Hanqun Cao,Marcelo D. T. Torres,Jingjie Zhang,Zijun Gao,Fang Wu,Chunbin Gu,Jure Leskovec,Yejin Choi,Cesar de la Fuente-Nunez,Guangyong Chen,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: ApexAmphion uses a 6.4B protein language model and reinforcement learning to de novo design antibiotic peptides, achieving 100% hit rate with nanomolar MICs and broad-spectrum activity, with lead molecules targeting the cytoplasmic membrane; the pipeline enables rapid, iterative optimization within hours.


<details>
  <summary>Details</summary>
Motivation: Antimicrobial resistance threatens up to 10 million deaths per year by 2050, underscoring urgent need for new antibiotics. A scalable, AI-driven design platform for peptide antibiotics is highly valuable.

Method: Fine-tune a 6.4-billion-parameter protein language model on curated antimicrobial peptide data; optimize designs with proximal policy optimization using a composite reward from a learned MIC classifier and differentiable physicochemical terms; validate in vitro on 100 designed peptides.

Result: All 100 designed peptides exhibited low MIC values (nanomolar range for some); 99 of 100 showed broad-spectrum antimicrobial activity against at least two clinically relevant bacteria; lead molecules killed bacteria primarily by potently targeting the cytoplasmic membrane.

Conclusion: The approach unifies generation, scoring, and multi-objective optimization into a single deep reinforcement learning pipeline, enabling rapid production of diverse, potent peptide antibiotics and iterative steering toward potency and developability within hours.

Abstract: Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths
annually by 2050, underscoring the urgent need for new antibiotics. Here we
present ApexAmphion, a deep-learning framework for de novo design of
antibiotics that couples a 6.4-billion-parameter protein language model with
reinforcement learning. The model is first fine-tuned on curated peptide data
to capture antimicrobial sequence regularities, then optimised with proximal
policy optimization against a composite reward that combines predictions from a
learned minimum inhibitory concentration (MIC) classifier with differentiable
physicochemical objectives. In vitro evaluation of 100 designed peptides showed
low MIC values (nanomolar range in some cases) for all candidates (100% hit
rate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial
activity against at least two clinically relevant bacteria. The lead molecules
killed bacteria primarily by potently targeting the cytoplasmic membrane. By
unifying generation, scoring and multi-objective optimization with deep
reinforcement learning in a single pipeline, our approach rapidly produces
diverse, potent candidates, offering a scalable route to peptide antibiotics
and a platform for iterative steering toward potency and developability within
hours.

</details>


### [267] [MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe](https://arxiv.org/abs/2509.18154)
*Tianyu Yu,Zefan Wang,Chongyi Wang,Fuwei Huang,Wenshuo Ma,Zhihui He,Tianchi Cai,Weize Chen,Yuxiang Huang,Yuanqian Zhao,Bokai Xu,Junbo Cui,Yingjing Xu,Liqing Ruan,Luoyuan Zhang,Hanyu Liu,Jingkun Tang,Hongyuan Liu,Qining Guo,Wenhao Hu,Bingxiang He,Jie Zhou,Jie Cai,Ji Qi,Zonghao Guo,Chi Chen,Guoyang Zeng,Yuxuan Li,Ganqu Cui,Ning Ding,Xu Han,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: MiniCPM-V 4.5 is an 8B multimodal LLM optimized for efficiency, deploying a 3D-Resampler architecture, unified document/text recognition learning, and a hybrid RL strategy. It achieves strong performance—surpassing GPT-4o-latest and larger models on OpenCompass—and state-of-the-art efficiency among sub-30B models on VideoMME (46.7% GPU memory, 8.7% inference time of Qwen2.5-VL 7B).


<details>
  <summary>Details</summary>
Motivation: The core bottlenecks in training and inference efficiency for Multimodal Large Language Models hinder accessibility and scalability. The paper aims to address efficiency while maintaining or improving performance, by (i) compact image/video encoding, (ii) unified handling of document knowledge and text recognition without heavy data engineering, and (iii) a training strategy that supports both short and long reasoning.

Method: Three main innovations: (1) a unified 3D-Resampler architecture for compact encoding of visual input (images/videos); (2) a unified learning paradigm that integrates document knowledge and text recognition without heavy data engineering; (3) a hybrid reinforcement learning strategy to enable proficient short- and long-range reasoning.

Result: On OpenCompass, MiniCPM-V 4.5 surpasses GPT-4o-latest and Qwen2.5-VL 72B. On VideoMME, among models under 30B, it achieves state-of-the-art performance with only 46.7% of GPU memory usage and 8.7% of the inference time of Qwen2.5-VL 7B.

Conclusion: The work demonstrates that an 8B MLLM with carefully designed architecture, data strategy, and training regime can deliver competitive or superior performance while achieving substantial efficiency gains, advancing accessibility and scalability of multimodal AI.

Abstract: Multimodal Large Language Models (MLLMs) are undergoing rapid progress and
represent the frontier of AI development. However, their training and inference
efficiency have emerged as a core bottleneck in making MLLMs more accessible
and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B
parameter model designed for high efficiency and strong performance. We
introduce three core improvements in model architecture, data strategy and
training method: a unified 3D-Resampler model architecture for highly compact
encoding over images and videos, a unified learning paradigm for document
knowledge and text recognition without heavy data engineering, and a hybrid
reinforcement learning strategy for proficiency in both short and long
reasoning modes. Comprehensive experimental results in OpenCompass evaluation
show that MiniCPM-V 4.5 surpasses widely used proprietary models such as
GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL
72B. Notably, the strong performance is achieved with remarkable efficiency.
For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves
state-of-the-art performance among models under 30B size, using just 46.7\% GPU
memory cost and 8.7\% inference time of Qwen2.5-VL 7B.

</details>


### [268] [Developing Training Procedures for Piecewise-linear Spline Activation Functions in Neural Networks](https://arxiv.org/abs/2509.18161)
*William H Patty*

Main category: cs.LG

TL;DR: Optimizing activation function shapes via nine training methodologies with parameterized linear B-spline activations yields significant accuracy gains over ReLU baselines, at the cost of increased training complexity and latency.


<details>
  <summary>Details</summary>
Motivation: Activation functions are typically fixed and hand-chosen; learnable activation shapes can improve parameter efficiency and accuracy by assigning more suitable activations to neurons.

Method: Nine training methodologies for dual-optimization of weights and activation shapes using parameterized linear B-spline activations, evaluated on feedforward nets (FNNs) and CNNs. Compare against traditional ReLU-based models.

Result: Experiments show up to 94% lower end-model error in FNNs and 51% lower in CNNs versus ReLU baselines, with gains offset by greater development/training complexity and end-model latency.

Conclusion: Dual-optimization of activation shapes is promising for performance gains but incurs higher training/development costs and latency; suggests further work on efficiency and broader activation families.

Abstract: Activation functions in neural networks are typically selected from a set of
empirically validated, commonly used static functions such as ReLU, tanh, or
sigmoid. However, by optimizing the shapes of a network's activation functions,
we can train models that are more parameter-efficient and accurate by assigning
more optimal activations to the neurons. In this paper, I present and compare 9
training methodologies to explore dual-optimization dynamics in neural networks
with parameterized linear B-spline activation functions. The experiments
realize up to 94% lower end model error rates in FNNs and 51% lower rates in
CNNs compared to traditional ReLU-based models. These gains come at the cost of
additional development and training complexity as well as end model latency.

</details>


### [269] [A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge](https://arxiv.org/abs/2509.18162)
*Meraryslan Meraliyev,Cemil Turan,Shirali Kadyrov*

Main category: cs.LG

TL;DR: A hybrid ALNS + reinforcement-learning approach for last-mile delivery with a truck and a drone under endurance constraints. It couples an ALNS-based truck tour with a small pointer/attention policy to schedule drone sorties, using a fast timeline simulator and feasibility masks. On Euclidean instances (N=50, E=0.7, R=0.1), it achieves a makespan of 5.203±0.093, beating ALNS (5.349±0.038) and closely matching NN (5.208±0.124). The RL method is 2.73% better than ALNS on average, and ties or beats NN on part of seeds, indicating robust improvement and balanced truck-wait trade-offs. A config-first implementation with replication utilities is provided.


<details>
  <summary>Details</summary>
Motivation: The problem addresses efficient last-mile delivery with one truck and one drone under strict endurance and recharge constraints. The goal is to minimize makespan by jointly optimizing truck routing and drone sortie scheduling, leveraging a hybrid learning approach that combines strong combinatorial optimization (ALNS) with a light neural policy to handle sequencing under feasibility constraints.

Method: An ALNS-based truck tour (with 2/3-opt and Or-opt) is coupled with a small pointer/attention policy that decodes launch–serve–rendezvous triplets. Feasibility masks enforce endurance and post-delivery recharge constraints. A fast, exact timeline simulator handles launch/recovery and computes true makespan, used by masked greedy/beam decoding. Experiments on Euclidean instances with N=50, E=0.7, R=0.1 compare against ALNS and NN.

Result: The hybrid RL solver achieves 5.203±0.093 makespan, outperforming ALNS (5.349±0.038) by 2.73% and matching NN (5.208±0.124) within 0.10%. Per-seed, RL never underperforms ALNS and ties/beats NN on two of three seeds. The method reveals a truck-wait trade-off and shows the learned scheduler balancing both components to minimize total completion time.

Conclusion: The proposed hybrid solver is effective for truck+drone last-mile delivery under endurance constraints, offering consistent improvement over a strong ALNS baseline and competitive performance relative to neural approaches, with a replication-ready config-first implementation.

Abstract: We study last-mile delivery with one truck and one drone under explicit
battery management: the drone flies at twice the truck speed; each sortie must
satisfy an endurance budget; after every delivery the drone recharges on the
truck before the next launch. We introduce a hybrid reinforcement learning (RL)
solver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a
small pointer/attention policy that schedules drone sorties. The policy decodes
launch--serve--rendezvous triplets with hard feasibility masks for endurance
and post-delivery recharge; a fast, exact timeline simulator enforces
launch/recovery handling and computes the true makespan used by masked
greedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and
$R{=}0.1$, the method achieves an average makespan of \textbf{5.203}$\pm$0.093,
versus \textbf{5.349}$\pm$0.038 for ALNS and \textbf{5.208}$\pm$0.124 for NN --
i.e., \textbf{2.73\%} better than ALNS on average and within \textbf{0.10\%} of
NN. Per-seed, the RL scheduler never underperforms ALNS on the same instance
and ties or beats NN on two of three seeds. A decomposition of the makespan
shows the expected truck--wait trade-off across heuristics; the learned
scheduler balances both to minimize the total completion time. We provide a
config-first implementation with plotting and significance-test utilities to
support replication.

</details>


### [270] [DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns](https://arxiv.org/abs/2509.18164)
*Ranfei Chen,Ming Chen*

Main category: cs.LG

TL;DR: DSFT is a simple Diffusion SFT approach that adjusts masking and loss to teach diffusion LLMs mathematical and logical patterns. It yields modest gains on small-scale data (roughly 5-10% on math tasks and ~2% on logical tasks) and is flexible to combine with pre-training, RL, or other methods; code is publicly available.


<details>
  <summary>Details</summary>
Motivation: Current training of diffusion-based LLMs emphasizes general knowledge and broad reasoning but lacks targeted learning of numerically sensitive mathematical and order-sensitive logical patterns. A specialized masking/learning strategy could improve pattern comprehension without overhauling existing pipelines.

Method: Introduce DSFT — a simple masking strategy and loss function adjustment designed to steer diffusion LLMs toward mathematical and logical patterns. The approach is modular and can be integrated with pre-training, RL, or other training plans and applied to models such as LLaDA and the Dream series.

Result: Empirical validation on models like LLaDA and the Dream series shows that DSFT with small-scale data yields improvements of about 5-10% on mathematical problems and approximately 2% on logical problems.

Conclusion: DSFT offers a lightweight, flexible masking-based training strategy to enhance pattern-specific learning in diffusion LLMs and can be combined with existing training methodologies to improve performance on mathematical and logical tasks; code is publicly available.

Abstract: Diffusion large language models (dLLMs) have emerged as a new architecture
following auto regressive models. Their denoising process offers a powerful
generative advantage, but they present significant challenges in learning and
understanding numerically sensitive mathematical and order-sensitive logical
tasks. Current training methods, including pre-training, fine-tuning, and
reinforcement learning, focus primarily on improving general knowledge
retention and reasoning abilities, but lack a comprehensive understanding of
mathematical and logical patterns. We propose DSFT, a simple yet effective
Diffusion SFT strategy, by adjusting the masking strategy and loss function,
guiding models to understand mathematical and logical patterns. This strategy
can be flexibly combined with pre-training, reinforcement learning, and other
training methods. Validated on models such as LLaDA and Dream series, we prove
that DSFT on small-scale data can achieve improvements of 5-10% and
approximately 2% on mathematical and logical problems, respectively. This
inspiring masking approach offers insights for future learning of specific
patterns, which can be easily and efficiently combined with other training
methods and applied to various dLLMs. Our code is publicly available at
https://anonymous.4open.science/r/DSFT-0FFB/

</details>


### [271] [MobiGPT: A Foundation Model for Mobile Wireless Networks](https://arxiv.org/abs/2509.18166)
*Xiaoqian Qi,Haoye Chai,Yong Li*

Main category: cs.LG

TL;DR: Proposes MobiGPT, a unified foundation model for forecasting three mobile data types (base station traffic, user app usage, and channel quality) using soft prompts and temporal masking. Demonstrates improved accuracy and zero-shot transfer on large real-world datasets.


<details>
  <summary>Details</summary>
Motivation: The rapid expansion of mobile networks creates a need for accurate, scalable forecasting across heterogeneous data (base station traffic, user behavior, channel quality). Current methods rely on bespoke models per data type, increasing complexity and deployment cost in large-scale networks. A unified foundation model could improve efficiency and generalization.

Method: Develop MobiGPT with a unified architecture. Employ soft-prompt learning to adapt to different data-type features and a temporal masking mechanism to guide three forecasting tasks: short-term prediction, long-term prediction, and distribution generation. Train and evaluate on real-world datasets with 100k+ samples to demonstrate multi-type forecasting performance and transferability.

Result: On real-world data (over 100k samples), MobiGPT improves forecasting accuracy by 27.37% (base station traffic), 20.08% (user app usage), and 7.27% (channel quality) compared with existing models. It also shows over 21.51% improvement in zero-/few-shot performance in unseen scenarios, indicating strong generalization and transferability.

Conclusion: A unified foundation-model approach for mobile data forecasting is feasible and advantageous. MobiGPT delivers accurate multi-type forecasts, reduces design complexity across heterogeneous networks, and exhibits strong transferability for unseen scenarios.

Abstract: With the rapid development of mobile communication technologies, future
mobile networks will offer vast services and resources for commuting,
production, daily life, and entertainment. Accurate and efficient forecasting
of mobile data (e.g., cell traffic, user behavior, channel quality) helps
operators monitor network state changes, orchestrate wireless resources, and
schedule infrastructure and users, thereby improving supply efficiency and
service quality. However, current forecasting paradigms rely on customized
designs with tailored models for exclusive data types. Such approaches increase
complexity and deployment costs under large-scale, heterogeneous networks
involving base stations, users, and channels. In this paper, we design a
foundation model for mobile data forecasting, MobiGPT, with a unified structure
capable of forecasting three data types: base station traffic, user app usage,
and channel quality. We propose a soft-prompt learning method to help the model
understand features of different data types, and introduce a temporal masking
mechanism to guide the model through three forecasting tasks: short-term
prediction, long-term prediction, and distribution generation, supporting
diverse optimization scenarios. Evaluations on real-world datasets with over
100,000 samples show that MobiGPT achieves accurate multi-type forecasting.
Compared to existing models, it improves forecasting accuracy by 27.37%,
20.08%, and 7.27%, reflecting strong generalization. Moreover, MobiGPT exhibits
superior zero/few-shot performance in unseen scenarios, with over 21.51%
improvement, validating its strong transferability as a foundation model.

</details>


### [272] [PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning](https://arxiv.org/abs/2509.18169)
*Hengbo Xiao,Jingyuan Fan,Xin Tong,Jingzhao Zhang,Chao Lu,Guannan He*

Main category: cs.LG

TL;DR: PiMoE endogenously integrates computation into neural networks via a trained mixture-of-experts, a text-to-computation module, and a router, enabling token-level iterative reasoning in a single chain-of-thought. It outperforms finetuned LLMs in accuracy and reduces latency, token usage, and GPU energy compared with multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack intrinsic high-precision numerical computation and interpretability for computation-heavy tasks. Relying on external tools or multi-agent systems adds communication overhead, inefficiency in multimodal reasoning, and scalability limits. There is a need for an integrated, efficient, and interpretable computation-and-reasoning mechanism within a single model.

Method: Train experts, a text-to-computation module, and a router separately. At inference, the router directs computation and reasoning at the token level, enabling iterative alternation within a single chain of thought without tool invocation. The approach is evaluated on two reasoning-computation tasks, comparing against LLM finetuning and multi-agent systems.

Result: PiMoE achieves higher accuracy than directly finetuning LLMs and shows significant improvements in response latency, token usage, and GPU energy consumption compared with mainstream multi-agent approaches.

Conclusion: PiMoE offers an efficient, interpretable, and scalable paradigm for next-generation scientific or industrial intelligent systems by tightly integrating computation and reasoning within a single neural architecture.

Abstract: Complex systems typically rely on high-precision numerical computation to
support decisions, but current large language models (LLMs) cannot yet
incorporate such computations as an intrinsic and interpretable capability with
existing architectures. Mainstream multi-agent approaches can leverage external
experts, but inevitably introduce communication overhead and suffer from
inefficient multimodal emergent capability and limited scalability. To this
end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and
inference architecture for integrating computation and reasoning. Instead of
the workflow paradigm of tool invocation, PiMoE endogenously integrates
computational capabilities into neural networks after separately training
experts, a text-to-computation module, and a router. At inference, the router
directs computation and reasoning at the token level, thereby enabling
iterative alternation within a single chain of thought. We evaluate PiMoE on
two reasoning-computation tasks against LLM finetuning and the multi-agent
system approaches. Results show that the PiMoE architecture achieves not only
higher accuracy than directly finetuning LLMs but also significant improvements
in response latency, token usage, and GPU energy consumption compared with
mainstream multi-agent approaches. PiMoE offers an efficient, interpretable,
and scalable paradigm for next-generation scientific or industrial intelligent
systems.

</details>


### [273] [FedIA: A Plug-and-Play Importance-Aware Gradient Pruning Aggregation Method for Domain-Robust Federated Graph Learning on Node Classification](https://arxiv.org/abs/2509.18171)
*Zhanting Zhou,KaHou Tam,Zeqin Wu,Pengzhao Sun,Jinbo Wang,Fengli Zhang*

Main category: cs.LG

TL;DR: FedIA presents a projection-first approach to federated graph learning under domain skew, denoising client updates before aggregation via a server-side top-ρ mask (~5% of coordinates) and an influence-regularised momentum to damp outliers. It achieves smoother convergence and higher final accuracy than baselines with negligible overhead, and preserves near-optimal convergence rate O(σ^2/√T).


<details>
  <summary>Details</summary>
Motivation: Domain skew across clients in Federated Graph Learning causes divergent or incompatible representations. Empirical analysis shows gradient noise dominates many dimensions due to domain-specific variance, making aggregation ineffective.

Method: A two-stage, plug-and-play pipeline: (i) server-side top-ρ mask that keeps only ~5% of coordinates with the most informative signals (projection-first denoising); (ii) an influence-regularised momentum weight to downweight outlier clients. No extra uplink traffic and negligible server memory.

Result: FedIA yields smoother, more stable convergence and higher final accuracy than nine strong baselines on both homogeneous (Twitch Gamers) and heterogeneous (Wikipedia) graphs.

Conclusion: A projection-first denoising strategy can effectively mitigate domain skew in federated graph learning, enabling stable convergence with minimal overhead; dynamic projection can preserve the near-optimal convergence rate.

Abstract: Federated Graph Learning (FGL) under domain skew -- as observed on platforms
such as \emph{Twitch Gamers} and multilingual \emph{Wikipedia} networks --
drives client models toward incompatible representations, rendering naive
aggregation both unstable and ineffective. We find that the culprit is not the
weighting scheme but the \emph{noisy gradient signal}: empirical analysis of
baseline methods suggests that a vast majority of gradient dimensions can be
dominated by domain-specific variance. We therefore shift focus from
"aggregation-first" to a \emph{projection-first} strategy that denoises client
updates \emph{before} they are combined. The proposed FedIA framework realises
this \underline{I}mportance-\underline{A}ware idea through a two-stage,
plug-and-play pipeline: (i) a server-side top-$\rho$ mask keeps only the most
informative about 5% of coordinates, and (ii) a lightweight
influence-regularised momentum weight suppresses outlier clients. FedIA adds
\emph{no extra uplink traffic and only negligible server memory}, making it
readily deployable. On both homogeneous (Twitch Gamers) and heterogeneous
(Wikipedia) graphs, it yields smoother, more stable convergence and higher
final accuracy than nine strong baselines. A convergence sketch further shows
that dynamic projection maintains the optimal
$\mathcal{O}(\sigma^{2}/\sqrt{T})$ rate.

</details>


### [274] [SBVR: Summation of BitVector Representation for Efficient LLM Quantization](https://arxiv.org/abs/2509.18172)
*Wonjun Bang,Jongseok Park,Hongseung Yu,Kyungmin Bin,Kyunghan Lee*

Main category: cs.LG

TL;DR: SBVR introduces Gaussian-like, non-uniform bitvector representations for LLM quantization, enabling fast inference with a custom CUDA kernel that performs matrix-vector multiplication directly in SBVR format. It aims to fix RTN and codebook limitations by matching weight distribution and improving speed.


<details>
  <summary>Details</summary>
Motivation: Mitigate deployment barriers of large-language-models by improving quantization accuracy and inference speed. RTN ignores Gaussian-like weight distributions; codebook methods suffer from random/strided memory access and cache issues. SBVR seeks a distribution-aware, hardware-friendly solution.

Method: Map weights to non-uniform representation points following the actual weight distribution (Gaussian-like). Use Summation of BitVector Representation (SBVR). Develop a custom CUDA kernel to perform matrix-vector multiplication directly in SBVR without decompression.

Result: Achieves state-of-the-art perplexity and accuracy on various models. Delivers 2.21x–3.04x end-to-end token-generation speedup over naive FP16 models in the 4-bit quantization regime.

Conclusion: SBVR provides a hardware-friendly, distribution-aware quantization approach that improves accuracy and speed, addressing limitations of RTN and codebook-based PTQ for LLMs, with a practical CUDA kernel enabling fast inference.

Abstract: With the advent of large language models (LLMs), numerous Post-Training
Quantization (PTQ) strategies have been proposed to alleviate deployment
barriers created by their enormous parameter counts. Quantization achieves
compression by limiting the number of representable points in the data.
Therefore, the key to achieving efficient quantization is selecting the optimal
combination of representation points, or codes, for the given data. Existing
PTQ solutions adopt two major approaches to this problem: Round-To-Nearest
(RTN)-based methods and codebook-based methods. RTN-based methods map LLM
weights onto uniformly distributed integer grids, failing to account for the
Gaussian-like weight distribution of LLM weights. Codebook-based methods
mitigate this issue by constructing distribution-aware codebooks; however, they
suffer from random and strided memory access patterns, resulting in degraded
inference speed that is exacerbated by the limited size of GPU L1 cache. To
overcome these limitations, we propose a novel LLM quantization method, SBVR
(Summation of BitVector Representation), that enables Gaussian-like code
representation in a hardware-friendly manner for fast inference. SBVR maps
weight values to non-uniform representation points whose distribution follows
the actual distribution of LLM weights, enabling more accurate compression.
Additionally, we design a custom CUDA kernel that allows matrix-vector
multiplication directly in the SBVR format without decompression, thereby
enabling high-performance execution of SBVR-compressed models. Our evaluations
of SBVR on various models demonstrate state-of-the-art perplexity and accuracy
benchmark performance while delivering a 2.21x- 3.04x end-to-end
token-generation speedup over naive FP16 models in the 4-bit quantization
regime.

</details>


### [275] [TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route](https://arxiv.org/abs/2509.18173)
*Hongyi Luo,Qing Cheng,Daniel Matos,Hari Krishna Gadi,Yanfeng Zhang,Lu Liu,Yongliang Wang,Niclas Zeller,Daniel Cremers,Liqiu Meng*

Main category: cs.LG

TL;DR: A large-scale benchmark evaluates geospatial route cognition in LLMs, introducing 36,000 routes across 12 cities, PathBuilder to translate NL instructions to routes and back, and an evaluation framework for 11 SOTA LLMs on route reversal. LLMs struggle with reverse routing, showing non-robust route generation and high confidence in incorrect answers; code/data released for reproducibility.


<details>
  <summary>Details</summary>
Motivation: To address the gap in assessing LLMs' geospatial reasoning, which has been hindered by non-quantifiable metrics and limited datasets. The work aims to quantify and benchmark how well LLMs can interpret, generate, and reverse geospatial routes using natural language.

Method: 1) Assemble a large-scale evaluation set of 36,000 routes in 12 global metropolises. 2) Develop PathBuilder, a tool to convert natural language instructions into navigation routes and convert routes back into NL instructions. 3) Propose a rigorous evaluation framework and metrics to assess 11 state-of-the-art LLMs on route reversal tasks.

Result: LLMs show limited ability in route reversal: most reverse routes neither return to the starting point nor match the optimal route. They exhibit low robustness in route generation and tend to be overconfident on incorrect outputs.

Conclusion: The benchmark exposes substantial gaps in LLMs’ geospatial route cognition and highlights the need for improved geospatial reasoning and robust evaluation methods. The authors provide code and data (TurnBack) to support further research and reproducibility.

Abstract: Humans can interpret geospatial information through natural language, while
the geospatial cognition capabilities of Large Language Models (LLMs) remain
underexplored. Prior research in this domain has been constrained by
non-quantifiable metrics, limited evaluation datasets and unclear research
hierarchies. Therefore, we propose a large-scale benchmark and conduct a
comprehensive evaluation of the geospatial route cognition of LLMs. We create a
large-scale evaluation dataset comprised of 36000 routes from 12 metropolises
worldwide. Then, we introduce PathBuilder, a novel tool for converting natural
language instructions into navigation routes, and vice versa, bridging the gap
between geospatial information and natural language. Finally, we propose a new
evaluation framework and metrics to rigorously assess 11 state-of-the-art
(SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs
exhibit limitation to reverse routes: most reverse routes neither return to the
starting point nor are similar to the optimal route. Additionally, LLMs face
challenges such as low robustness in route generation and high confidence for
their incorrect answers. Code\ \&\ Data available here:
\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}

</details>


### [276] [Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought](https://arxiv.org/abs/2509.18200)
*Yu Ti Huang*

Main category: cs.LG

TL;DR: Introduces COR benchmark and MCoT framework for egocentric-to-allocentric spatial reasoning in Traditional Chinese, achieving near-perfect accuracy and robustness in ASR/noisy conditions.


<details>
  <summary>Details</summary>
Motivation: Translating egocentric cues into allocentric directions is crucial in GPS-denied indoor settings; multimodal spatial reasoning with chain-of-thought is underexplored for non-English/ASR scenarios; need interpretable, resource-efficient navigation.

Method: Proposes a multimodal chain-of-thought (MCoT) framework that fuses ASR-transcribed speech with landmark coordinates in a three-step process: (1) extract spatial relations, (2) map coordinates to absolute directions, (3) infer user orientation; uses curriculum learning on Taiwan-LLM-13B-v2.0-Chat and evaluates on a Traditional Chinese COR benchmark.

Result: 100% orientation accuracy on clean transcripts; 98.1% with ASR transcripts; outperforms unimodal and non-structured baselines; robust to ASR errors, multilingual code-switching; maintains accuracy under cross-domain shifts and referential ambiguity.

Conclusion: Structured MCoT spatial reasoning is a promising path for interpretable, resource-efficient embodied navigation in non-English settings, enabling robust egocentric-to-allocentric orientation under realistic noise and linguistic variation.

Abstract: Conversational agents must translate egocentric utterances (e.g., "on my
right") into allocentric orientations (N/E/S/W). This challenge is particularly
critical in indoor or complex facilities where GPS signals are weak and
detailed maps are unavailable. While chain-of-thought (CoT) prompting has
advanced reasoning in language and vision tasks, its application to multimodal
spatial orientation remains underexplored. We introduce Conversational
Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese
conversational navigation projected from real-world environments, addressing
egocentric-to-allocentric reasoning in non-English and ASR-transcribed
scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which
integrates ASR-transcribed speech with landmark coordinates through a
structured three-step reasoning process: (1) extracting spatial relations, (2)
mapping coordinates to absolute directions, and (3) inferring user orientation.
A curriculum learning strategy progressively builds these capabilities on
Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of
resource-constrained settings. Experiments show that MCoT achieves 100%
orientation accuracy on clean transcripts and 98.1% with ASR transcripts,
substantially outperforming unimodal and non-structured baselines. Moreover,
MCoT demonstrates robustness under noisy conversational conditions, including
ASR recognition errors and multilingual code-switching. The model also
maintains high accuracy in cross-domain evaluation and resilience to linguistic
variation, domain shift, and referential ambiguity. These findings highlight
the potential of structured MCoT spatial reasoning as a path toward
interpretable and resource-efficient embodied navigation.

</details>


### [277] [Variational Task Vector Composition](https://arxiv.org/abs/2509.18208)
*Boyuan Zhang,Yingjun Du,Xiantong Zhen,Ling Shao*

Main category: cs.LG

TL;DR: Bayesian variational task vector composition with spike-and-slab sparsity and gated sampling to enable sample-specific, sparse, and interpretable task vector fusion that improves performance across datasets.


<details>
  <summary>Details</summary>
Motivation: Task vectors track fine-tuning changes and enable cross-task knowledge transfer, but existing methods are often task-level, high-variance, and non-sparse. There is structural redundancy in task vectors and a need for efficient, interpretable, sample-specific composition.

Method: Treat task-vector composition coefficients as latent variables in a Bayesian variational framework. Use a Spike-and-Slab prior to promote sparsity and keep only informative components. Introduce a gated sampling mechanism that filters coefficients by uncertainty and importance to construct a controllable posterior, yielding stable and interpretable component selection.

Result: The approach consistently outperforms existing methods across all datasets by selectively leveraging the most reliable and informative task-vector components, with improved stability, generalization, and interpretability due to reduced sampling variance.

Conclusion: This method establishes a new standard for efficient and effective task-vector composition, highlighting the practical value of selective, reliable component usage in task arithmetic.

Abstract: Task vectors capture how a model changes during fine-tuning by recording the
difference between pre-trained and task-specific weights. The composition of
task vectors, a key operator in task arithmetic, enables models to integrate
knowledge from multiple tasks without incurring additional inference costs. In
this paper, we propose variational task vector composition, where composition
coefficients are taken as latent variables and estimated in a Bayesian
inference framework. Unlike previous methods that operate at the task level,
our framework focuses on sample-specific composition. Motivated by the
observation of structural redundancy in task vectors, we introduce a
Spike-and-Slab prior that promotes sparsity and preserves only the most
informative components. To further address the high variance and sampling
inefficiency in sparse, high-dimensional spaces, we develop a gated sampling
mechanism that constructs a controllable posterior by filtering the composition
coefficients based on both uncertainty and importance. This yields a more
stable and interpretable variational framework by deterministically selecting
reliable task components, reducing sampling variance while improving
transparency and generalization. Experimental results demonstrate that our
method consistently outperforms existing approaches across all datasets by
selectively leveraging the most reliable and informative components in task
vectors. These findings highlight the practical value of our approach,
establishing a new standard for efficient and effective task vector
composition.

</details>


### [278] [MolPILE - large-scale, diverse dataset for molecular representation learning](https://arxiv.org/abs/2509.18353)
*Jakub Adamczyk,Jakub Poziemski,Franciszek Job,Mateusz Król,Maciej Makowski*

Main category: cs.LG

TL;DR: MolPILE is a large-scale, diverse, and curated dataset of 222M molecules from six databases, enabling improved molecular pretraining; retraining models on MolPILE improves generalization; aims to be an ImageNet-like resource for chemistry.


<details>
  <summary>Details</summary>
Motivation: The generalization of foundation models hinges on dataset size, diversity, and quality; existing small molecule datasets are insufficient for robust molecular representation learning; there is a need for a standardized, large-scale dataset akin to ImageNet in chemistry.

Method: Construct MolPILE by aggregating 222 million compounds from six large databases via an automated curation pipeline; analyze current pretraining datasets; retrain existing models on MolPILE to assess generalization improvements.

Result: Retraining models on MolPILE yields improvements in generalization performance; MolPILE provides a standardized resource for model training and advances toward an ImageNet-like dataset in molecular chemistry.

Conclusion: MolPILE addresses the need for a large, diverse, and well-curated pretraining resource in chemoinformatics, enabling better generalization and serving as a standard benchmark for molecular ML models.

Abstract: The size, diversity, and quality of pretraining datasets critically determine
the generalization ability of foundation models. Despite their growing
importance in chemoinformatics, the effectiveness of molecular representation
learning has been hindered by limitations in existing small molecule datasets.
To address this gap, we present MolPILE, large-scale, diverse, and rigorously
curated collection of 222 million compounds, constructed from 6 large-scale
databases using an automated curation pipeline. We present a comprehensive
analysis of current pretraining datasets, highlighting considerable
shortcomings for training ML models, and demonstrate how retraining existing
models on MolPILE yields improvements in generalization performance. This work
provides a standardized resource for model training, addressing the pressing
need for an ImageNet-like dataset in molecular chemistry.

</details>


### [279] [FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction](https://arxiv.org/abs/2509.18362)
*Yuxuan Cai,Xiaozhuan Liang,Xinghua Wang,Jin Ma,Haijin Liang,Jinwen Luo,Xinyu Zuo,Lisheng Duan,Yuyang Yin,Xi Chen*

Main category: cs.LG

TL;DR: FastMTP aligns multi-token prediction training with inference to accelerate autoregressive LLMs, achieving 2.03x speedups with lossless outputs and strong gains over vanilla MTP without heavy retraining.


<details>
  <summary>Details</summary>
Motivation: Autoregressive generation bottlenecks throughput; multi-token prediction (MTP) shows training benefits but limited inference acceleration potential.

Method: Fine-tune a single MTP head with position-shared weights on self-distilled data; train to capture dependencies across consecutive future tokens; maintain high acceptance rates across recursive drafts; integrate language-aware dynamic vocabulary compression to reduce drafting computation.

Result: On seven benchmarks, FastMTP yields ~2.03x speedup over standard next-token prediction with lossless outputs and outperforms vanilla MTP by ~82%; lightweight training and easy integration with existing inference frameworks.

Conclusion: FastMTP provides a practical, deployable solution to accelerate LLM inference by improving multi-step drafting quality and speculative decoding efficiency.

Abstract: As large language models (LLMs) become increasingly powerful, the sequential
nature of autoregressive generation creates a fundamental throughput bottleneck
that limits the practical deployment. While Multi-Token Prediction (MTP) has
demonstrated remarkable benefits for model training efficiency and performance,
its inherent potential for inference acceleration remains largely unexplored.
This paper introduces FastMTP, a simple yet effective method that improves
multi-step draft quality by aligning MTP training with its inference pattern,
significantly enhancing speculative decoding performance. Our approach
fine-tunes a single MTP head with position-shared weights on self-distilled
data, enabling it to capture dependencies among consecutive future tokens and
maintain high acceptance rates across multiple recursive draft steps. By
integrating language-aware dynamic vocabulary compression into the MTP head, we
further reduce computational overhead in the drafting process. Experimental
results across seven diverse benchmarks demonstrate that FastMTP achieves an
average of 2.03x speedup compared to standard next token prediction with
lossless output quality, outperforming vanilla MTP by 82%. FastMTP requires
only lightweight training and seamlessly integrates with existing inference
frameworks, offering a practical and rapidly deployable solution for
accelerating LLM inference.

</details>


### [280] [Multi-Worker Selection based Distributed Swarm Learning for Edge IoT with Non-i.i.d. Data](https://arxiv.org/abs/2509.18367)
*Zhuoyu Yao,Yue Wang,Songyang Zhang,Yingshu Li,Zhipeng Cai,Zhi Tian*

Main category: cs.LG

TL;DR: Study data heterogeneity in distributed swarm learning, propose M-DSL for selecting multiple workers in non-i.i.d settings, introduce a non-i.i.d degree metric, prove convergence, and show empirical gains over benchmarks.


<details>
  <summary>Details</summary>
Motivation: Non-i.i.d data in DSL degrades performance and training stability; lacking theory linking data heterogeneity to accuracy; need effective worker selection to improve global updates.

Method: Define a non-i.i.d degree metric to quantify dataset heterogeneity; design M-DSL algorithm to select multiple workers with significant contributions for updates; provide convergence analysis; conduct experiments on heterogeneous datasets under non-i.i.d settings.

Result: M-DSL yields performance gains and enhanced network intelligence compared to benchmarks; convergence results support stability; experimental validation across datasets.

Conclusion: M-DSL offers a principled, scalable approach to handle data heterogeneity in DSL, linking heterogeneity metrics to performance and guiding worker selection to improve global model updates.

Abstract: Recent advances in distributed swarm learning (DSL) offer a promising
paradigm for edge Internet of Things. Such advancements enhance data privacy,
communication efficiency, energy saving, and model scalability. However, the
presence of non-independent and identically distributed (non-i.i.d.) data pose
a significant challenge for multi-access edge computing, degrading learning
performance and diverging training behavior of vanilla DSL. Further, there
still lacks theoretical guidance on how data heterogeneity affects model
training accuracy, which requires thorough investigation. To fill the gap, this
paper first study the data heterogeneity by measuring the impact of non-i.i.d.
datasets under the DSL framework. This then motivates a new multi-worker
selection design for DSL, termed M-DSL algorithm, which works effectively with
distributed heterogeneous data. A new non-i.i.d. degree metric is introduced
and defined in this work to formulate the statistical difference among local
datasets, which builds a connection between the measure of data heterogeneity
and the evaluation of DSL performance. In this way, our M-DSL guides effective
selection of multiple works who make prominent contributions for global model
updates. We also provide theoretical analysis on the convergence behavior of
our M-DSL, followed by extensive experiments on different heterogeneous
datasets and non-i.i.d. data settings. Numerical results verify performance
improvement and network intelligence enhancement provided by our M-DSL beyond
the benchmarks.

</details>


### [281] [GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability](https://arxiv.org/abs/2509.18376)
*Burouj Armgaan,Eshan Jain,Harsh Pandey,Mahesh Chandran,Sayan Ranu*

Main category: cs.LG

TL;DR: GnnXemplar is a global GNN explainer that uses exemplars in embedding space and LLM-derived natural-language rules, optimizing coverage via reverse k-NN with a greedy algorithm, yielding better fidelity, scalability, and interpretability.


<details>
  <summary>Details</summary>
Motivation: Global explanations for GNNs are underdeveloped and struggle in large graphs with high-dimensional attributes and complex structure-attribute interactions; existing motif-based methods falter beyond small graphs; need scalable, human-friendly global explanations.

Method: Select representative exemplar nodes in the GNN embedding space; formulate coverage over reverse k-nearest neighbors; apply a greedy approximation to select exemplars; derive interpretable rules from neighborhoods using self-refining prompts with large language models.

Result: GnnXemplar outperforms existing global explainers in fidelity, scalability, and human interpretability across benchmarks; validated by a user study with 60 participants.

Conclusion: GnnXemplar demonstrates that exemplar-based global explanations combined with LLM-derived rule extraction can provide accurate, scalable, and interpretable explanations for GNN predictions in large real-world graphs.

Abstract: Graph Neural Networks (GNNs) are widely used for node classification, yet
their opaque decision-making limits trust and adoption. While local
explanations offer insights into individual predictions, global explanation
methods, those that characterize an entire class, remain underdeveloped.
Existing global explainers rely on motif discovery in small graphs, an approach
that breaks down in large, real-world settings where subgraph repetition is
rare, node attributes are high-dimensional, and predictions arise from complex
structure-attribute interactions. We propose GnnXemplar, a novel global
explainer inspired from Exemplar Theory from cognitive science. GnnXemplar
identifies representative nodes in the GNN embedding space, exemplars, and
explains predictions using natural language rules derived from their
neighborhoods. Exemplar selection is framed as a coverage maximization problem
over reverse k-nearest neighbors, for which we provide an efficient greedy
approximation. To derive interpretable rules, we employ a self-refining prompt
strategy using large language models (LLMs). Experiments across diverse
benchmarks show that GnnXemplar significantly outperforms existing methods in
fidelity, scalability, and human interpretability, as validated by a user study
with 60 participants.

</details>


### [282] [Graph Enhanced Trajectory Anomaly Detection](https://arxiv.org/abs/2509.18386)
*Jonathan Kabala Mbuya,Dieter Pfoser,Antonios Anastasopoulos*

Main category: cs.LG

TL;DR: GETAD is a graph-aware trajectory anomaly detector that fuses road-network topology, segment semantics, and historical patterns using a Graph Attention Network and a Transformer decoder, with a novel Confidence Weighted NLL scoring for robust anomaly detection, showing improvements on real and synthetic datasets, especially for road-constrained subtle anomalies.


<details>
  <summary>Details</summary>
Motivation: Existing trajectory anomaly methods treat movement as Euclidean sequences and ignore network connectivity; roads and segments impose constraints and semantics that can reveal anomalies not detectable in plain Euclidean space. Incorporating graph structure, topology, and semantics can improve detection accuracy and robustness in real-world, network-constrained environments.

Method: 1) Learn road-aware embeddings via a Graph Attention Network that captures physical attributes and transition behavior. 2) Augment embeddings with graph-based positional encodings reflecting road network layout. 3) Use a Transformer-based decoder to model sequential movement. 4) Train with a multiobjective loss combining autoregressive prediction and supervised link prediction to enforce realism and structural coherence. 5) Introduce Confidence Weighted Negative Log Likelihood (CW NLL) as an anomaly score emphasizing high-confidence deviations.

Result: GETAD achieves consistent improvements over existing methods, particularly in detecting subtle anomalies in road-constrained environments, on both real-world and synthetic datasets. The integration of graph structure and contextual semantics yields more precise and context-aware trajectory representations and anomaly detection.

Conclusion: Incorporating road-network topology, segment semantics, and historical patterns through graph-aware embeddings and network-informed positional encodings, coupled with a transformer-based sequence model and robust CW NLL scoring, enables more accurate and context-aware trajectory anomaly detection in road-constrained settings.

Abstract: Trajectory anomaly detection is essential for identifying unusual and
unexpected movement patterns in applications ranging from intelligent
transportation systems to urban safety and fraud prevention.
  Existing methods only consider limited aspects of the trajectory nature and
its movement space by treating trajectories as sequences of sampled locations,
with sampling determined by positioning technology, e.g., GPS, or by high-level
abstractions such as staypoints. Trajectories are analyzed in Euclidean space,
neglecting the constraints and connectivity information of the underlying
movement network, e.g., road or transit networks.
  The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework
tightly integrates road network topology, segment semantics, and historical
travel patterns to model trajectory data. GETAD uses a Graph Attention Network
to learn road-aware embeddings that capture both physical attributes and
transition behavior, and augments these with graph-based positional encodings
that reflect the spatial layout of the road network.
  A Transformer-based decoder models sequential movement, while a
multiobjective loss function combining autoregressive prediction and supervised
link prediction ensures realistic and structurally coherent representations.
  To improve the robustness of anomaly detection, we introduce Confidence
Weighted Negative Log Likelihood (CW NLL), an anomaly scoring function that
emphasizes high-confidence deviations.
  Experiments on real-world and synthetic datasets demonstrate that GETAD
achieves consistent improvements over existing methods, particularly in
detecting subtle anomalies in road-constrained environments. These results
highlight the benefits of incorporating graph structure and contextual
semantics into trajectory modeling, enabling more precise and context-aware
anomaly detection.

</details>


### [283] [Towards Provable Emergence of In-Context Reinforcement Learning](https://arxiv.org/abs/2509.18389)
*Jiuqi Wang,Rohan Chandra,Shangtong Zhang*

Main category: cs.LG

TL;DR: ICRL can stem from parametric pretraining: the paper argues that parameters that enable in-context RL are global minimizers of the pretraining loss, and provides a case study showing a Transformer pretrained for policy evaluation has a global minimizer that enables in-context TD learning.


<details>
  <summary>Details</summary>
Motivation: To understand why RL pretraining yields in-context adaptation without online parameter updates, and whether the existence of ICRL-capable parameters is tied to pretraining loss minimizers.

Method: A theoretical case study: analyze a Transformer pretrained for policy evaluation and prove that one of its global pretraining-loss minimizers can enable in-context temporal-difference learning.

Result: The paper proves that, in this setup, there exists a global minimizer of the pretraining loss that enables in-context TD learning, providing initial support for the hypothesis.

Conclusion: Supports the view that ICRL-capable parameters arise as pretraining-loss minimizers; suggests structuring pretraining objectives to favor such minimizers and motivates further generalization beyond the case study.

Abstract: Typically, a modern reinforcement learning (RL) agent solves a task by
updating its neural network parameters to adapt its policy to the task.
Recently, it has been observed that some RL agents can solve a wide range of
new out-of-distribution tasks without parameter updates after pretraining on
some task distribution. When evaluated in a new task, instead of making
parameter updates, the pretrained agent conditions its policy on additional
input called the context, e.g., the agent's interaction history in the new
task. The agent's performance increases as the information in the context
increases, with the agent's parameters fixed. This phenomenon is typically
called in-context RL (ICRL). The pretrained parameters of the agent network
enable the remarkable ICRL phenomenon. However, many ICRL works perform the
pretraining with standard RL algorithms. This raises the central question this
paper aims to address: Why can the RL pretraining algorithm generate network
parameters that enable ICRL? We hypothesize that the parameters capable of ICRL
are minimizers of the pretraining loss. This work provides initial support for
this hypothesis through a case study. In particular, we prove that when a
Transformer is pretrained for policy evaluation, one of the global minimizers
of the pretraining loss can enable in-context temporal difference learning.

</details>


### [284] [Development of Deep Learning Optimizers: Approaches, Concepts, and Update Rules](https://arxiv.org/abs/2509.18396)
*Doğay Altınel*

Main category: cs.LG

TL;DR: A comprehensive survey of deep learning optimizers, tracing the evolution from SGD to modern variants (Momentum, AdamW, Sophia, Muon), detailing update rules, concepts, hyperparameters, and open challenges to guide future work.


<details>
  <summary>Details</summary>
Motivation: To synthesize and compare the wide range of optimization algorithms used in deep learning, clarifying how they work, when they are effective, and what challenges remain.

Method: Literature-based review that examines optimizers individually in chronological order, presents their update rules and key concepts, discusses default hyperparameters, and highlights open problems in optimization.

Result: A structured, comparative resource that documents distinctive features and update mechanics of major optimizers, outlines typical defaults, and provides insights into current trends and limitations.

Conclusion: Offers a thorough resource on the optimizer landscape and delineates potential avenues for future research and development.

Abstract: Deep learning optimizers are optimization algorithms that enable deep neural
networks to learn. The effectiveness of learning is highly dependent on the
optimizer employed in the training process. Alongside the rapid advancement of
deep learning, a wide range of optimizers with different approaches have been
developed. This study aims to provide a review of various optimizers that have
been proposed and received attention in the literature. From Stochastic
gradient descent to the most recent ones such as Momentum, AdamW, Sophia, and
Muon in chronological order, optimizers are examined individually, and their
distinctive features are highlighted in the study. The update rule of each
optimizer is presented in detail, with an explanation of the associated
concepts and variables. The techniques applied by these optimizers, their
contributions to the optimization process, and their default hyperparameter
settings are also discussed. In addition, insights are offered into the open
challenges encountered in the optimization of deep learning models. Thus, a
comprehensive resource is provided both for understanding the current state of
optimizers and for identifying potential areas of future development.

</details>


### [285] [Explicit Path CGR: Maintaining Sequence Fidelity in Geometric Representations](https://arxiv.org/abs/2509.18408)
*Sarwan Ali*

Main category: cs.LG

TL;DR: Reversible, information-preserving Chaos Game Representation (R-CGR) that enables exact sequence reconstruction from geometric traces while achieving competitive classification performance and interpretable visuals.


<details>
  <summary>Details</summary>
Motivation: Traditional CGR loses sequence information during geometric mapping; a reversible representation is desired to preserve full sequence data and provide interpretable visualizations for bioinformatics.

Method: Develop Reverse-CGR with explicit path encoding and rational arithmetic precision control to store both position and character information at each step, enabling perfect sequence reconstruction.

Result: R-CGR achieves competitive performance on sequence classification tasks compared with standard sequence-based methods and provides interpretable geometric visualizations; it generates feature-rich images suitable for deep learning while preserving recoverable sequence data.

Conclusion: Information-preserving CGR enables accurate classification with full sequence recoverability, offering new interpretable avenues for bioinformatics analysis and potential broader applications.

Abstract: We present a novel information-preserving Chaos Game Representation (CGR)
method, also called Reverse-CGR (R-CGR), for biological sequence analysis that
addresses the fundamental limitation of traditional CGR approaches - the loss
of sequence information during geometric mapping. Our method introduces
complete sequence recovery through explicit path encoding combined with
rational arithmetic precision control, enabling perfect sequence reconstruction
from stored geometric traces. Unlike purely geometric approaches, our
reversibility is achieved through comprehensive path storage that maintains
both positional and character information at each step. We demonstrate the
effectiveness of R-CGR on biological sequence classification tasks, achieving
competitive performance compared to traditional sequence-based methods while
providing interpretable geometric visualizations. The approach generates
feature-rich images suitable for deep learning while maintaining complete
sequence information through explicit encoding, opening new avenues for
interpretable bioinformatics analysis where both accuracy and sequence recovery
are essential.

</details>


### [286] [Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors](https://arxiv.org/abs/2509.18433)
*Chang Liu,Ladda Thiamwong,Yanjie Fu,Rui Xie*

Main category: cs.LG

TL;DR: KANDI combines offline IRL with Kolmogorov-Arnold Networks for reward estimation and diffusion-based policies for action refinement within an Actor-Critic framework, applied to fall-risk reduction in older adults using wearable data; it outperforms state-of-the-art methods on D4RL and shows practical utility in a PEER fall-prevention intervention.


<details>
  <summary>Details</summary>
Motivation: Offline RL in healthcare faces challenges: defining direct rewards is hard, IRL struggles to infer accurate rewards, and aligning learned policies with observed human healthcare behavior is difficult. There is a need to promote physical activity in high-fall-risk older adults using real-world wearable data.

Method: Estimate reward functions by learning from expert free-living behavior of low-fall-risk older adults using Kolmogorov-Arnold Networks. Use diffusion-based policies within an Actor-Critic framework as a generative, offline policy refinement mechanism. Evaluate on wearable activity data from the PEER two-arm clinical trial and benchmark with D4RL.

Result: KANDI outperforms state-of-the-art offline RL methods on the D4RL benchmark and demonstrates practical applicability in a fall-risk intervention program for promoting physical activity among older adults, based on the PEER study data.

Conclusion: KANDI offers a viable solution to core offline RL challenges in healthcare—reward estimation from expert behavior and policy alignment with real-world healthcare actions—facilitating effective activity-promotion interventions for older adults at risk of falls.

Abstract: Utilizing offline reinforcement learning (RL) with real-world clinical data
is getting increasing attention in AI for healthcare. However, implementation
poses significant challenges. Defining direct rewards is difficult, and inverse
RL (IRL) struggles to infer accurate reward functions from expert behavior in
complex environments. Offline RL also encounters challenges in aligning learned
policies with observed human behavior in healthcare applications. To address
challenges in applying offline RL to physical activity promotion for older
adults at high risk of falls, based on wearable sensor activity monitoring, we
introduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse
Reinforcement Learning (KANDI). By leveraging the flexible function
approximation in Kolmogorov-Arnold Networks, we estimate reward functions by
learning free-living environment behavior from low-fall-risk older adults
(experts), while diffusion-based policies within an Actor-Critic framework
provide a generative approach for action refinement and efficiency in offline
RL. We evaluate KANDI using wearable activity monitoring data in a two-arm
clinical trial from our Physio-feedback Exercise Program (PEER) study,
emphasizing its practical application in a fall-risk intervention program to
promote physical activity among older adults. Additionally, KANDI outperforms
state-of-the-art methods on the D4RL benchmark. These results underscore
KANDI's potential to address key challenges in offline RL for healthcare
applications, offering an effective solution for activity promotion
intervention strategies in healthcare.

</details>


### [287] [MeshODENet: A Graph-Informed Neural Ordinary Differential Equation Neural Network for Simulating Mesh-Based Physical Systems](https://arxiv.org/abs/2509.18445)
*Kangzheng Liu,Leixin Ma*

Main category: cs.LG

TL;DR: MeshODENet blends Graph Neural Networks with Neural Ordinary Differential Equations to deliver stable, fast, long-term surrogates for mesh-based structural dynamics.


<details>
  <summary>Details</summary>
Motivation: Accelerate many-query simulations for discretized mesh systems by reducing computational cost, while addressing error accumulation and instability in autoregressive GNNs for long-term predictions.

Method: A general framework MeshODENet that combines GNN-based spatial reasoning with continuous-time neural ODEs to model dynamics; demonstrated on 1D and 2D elastic bodies undergoing large nonlinear deformations.

Result: Significant improvements in long-term predictive accuracy and stability over baselines, along with substantial computational speed-ups compared to traditional solvers.

Conclusion: A powerful, generalizable data-driven surrogate approach for rapid analysis and modeling of complex structural systems.

Abstract: The simulation of complex physical systems using a discretized mesh is a
cornerstone of applied mechanics, but traditional numerical solvers are often
computationally prohibitive for many-query tasks. While Graph Neural Networks
(GNNs) have emerged as powerful surrogate models for mesh-based data, their
standard autoregressive application for long-term prediction is often plagued
by error accumulation and instability. To address this, we introduce
MeshODENet, a general framework that synergizes the spatial reasoning of GNNs
with the continuous-time modeling of Neural Ordinary Differential Equations. We
demonstrate the framework's effectiveness and versatility on a series of
challenging structural mechanics problems, including one- and two-dimensional
elastic bodies undergoing large, non-linear deformations. The results
demonstrate that our approach significantly outperforms baseline models in
long-term predictive accuracy and stability, while achieving substantial
computational speed-ups over traditional solvers. This work presents a powerful
and generalizable approach for developing data-driven surrogates to accelerate
the analysis and modeling of complex structural systems.

</details>


### [288] [Fast Linear Solvers via AI-Tuned Markov Chain Monte Carlo-based Matrix Inversion](https://arxiv.org/abs/2509.18452)
*Anton Lebedev,Won Kyung Lee,Soumyadip Ghosh,Olha I. Yaman,Vassilis Kalantzis,Yingdong Lu,Tomasz Nowicki,Shashanka Ubaru,Lior Horesh,Vassil Alexandrov*

Main category: cs.LG

TL;DR: AI-driven tuning of MCMC-based preconditioners using a graph neural surrogate and Bayesian optimization to reduce iterations with half the search budget on ill-conditioned systems.


<details>
  <summary>Details</summary>
Motivation: Preconditioners speed Krylov subspace solvers for large sparse linear systems. MCMC-based preconditioning is promising but depends on tunable parameters; manual or grid search is costly and potentially impractical for varying matrices.

Method: A graph neural network surrogate predicts preconditioning speed from the system matrix A and MCMC parameters. A Bayesian acquisition function selects parameter sets that most likely minimize iterations. Evaluation on a previously unseen ill-conditioned system.

Result: The framework achieves better preconditioning with 50% of the search budget of conventional methods, and about a 10% reduction in iterations to convergence on an unseen ill-conditioned system.

Conclusion: AI-driven parameter tuning can efficiently integrate MCMC-based preconditioners into large-scale Krylov solvers, reducing tuning cost while improving convergence.

Abstract: Large, sparse linear systems are pervasive in modern science and engineering,
and Krylov subspace solvers are an established means of solving them. Yet
convergence can be slow for ill-conditioned matrices, so practical deployments
usually require preconditioners. Markov chain Monte Carlo (MCMC)-based matrix
inversion can generate such preconditioners and accelerate Krylov iterations,
but its effectiveness depends on parameters whose optima vary across matrices;
manual or grid search is costly. We present an AI-driven framework recommending
MCMC parameters for a given linear system. A graph neural surrogate predicts
preconditioning speed from $A$ and MCMC parameters. A Bayesian acquisition
function then chooses the parameter sets most likely to minimise iterations. On
a previously unseen ill-conditioned system, the framework achieves better
preconditioning with 50\% of the search budget of conventional methods,
yielding about a 10\% reduction in iterations to convergence. These results
suggest a route for incorporating MCMC-based preconditioners into large-scale
systems.

</details>


### [289] [GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting](https://arxiv.org/abs/2509.18457)
*Ebrahim Farahmand,Reza Rahimi Azghan,Nooshin Taheri Chatrudi,Velarie Yaa Ansu-Baidoo,Eric Kim,Gautham Krishna Gudur,Mohit Malu,Owen Krueger,Edison Thomaz,Giulia Pedrielli,Pavan Turaga,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: GluMind is a transformer-based multimodal framework for continual long-term glucose forecasting that uses cross-attention to fuse multimodal signals and multi-scale attention for long-range dependencies, along with a knowledge retention module to mitigate forgetting; evaluated on AIREADI with healthy, prediabetes, and T2D cohorts, achieving ~15% RMSE and ~9% MAE improvements over state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of continual, long-term blood glucose forecasting using multimodal data with varying sampling rates, while combating catastrophic forgetting and leveraging long-range temporal dependencies.

Method: A transformer-based architecture (GluMind) with parallel cross-attention and multi-scale attention. Cross-attention fuses glucose data with activity, stress, and heart rate; multi-scale attention captures long-range dependencies. A knowledge retention module mitigates forgetting in a continual learning setting. Evaluated on the AIREADI dataset across healthy, prediabetes, and type 2 diabetes cohorts, assessing performance stability as new cohorts are introduced.

Result: GluMind outperforms state-of-the-art forecasting models, achieving approximately 15% improvement in RMSE and 9% improvement in MAE across experiments.

Conclusion: GluMind demonstrates that combining multimodal cross-attention with multi-scale temporal attention and a knowledge retention mechanism yields robust continual glucose forecasting across diverse cohorts, indicating promise for personalized diabetes management in real-world, variable-sampling settings.

Abstract: This paper proposes GluMind, a transformer-based multimodal framework
designed for continual and long-term blood glucose forecasting. GluMind devises
two attention mechanisms, including cross-attention and multi-scale attention,
which operate in parallel and deliver accurate predictive performance.
Cross-attention effectively integrates blood glucose data with other
physiological and behavioral signals such as activity, stress, and heart rate,
addressing challenges associated with varying sampling rates and their adverse
impacts on robust prediction. Moreover, the multi-scale attention mechanism
captures long-range temporal dependencies. To mitigate catastrophic forgetting,
GluMind incorporates a knowledge retention technique into the transformer-based
forecasting model. The knowledge retention module not only enhances the model's
ability to retain prior knowledge but also boosts its overall forecasting
performance. We evaluate GluMind on the recently released AIREADI dataset,
which contains behavioral and physiological data collected from healthy people,
individuals with prediabetes, and those with type 2 diabetes. We examine the
performance stability and adaptability of GluMind in learning continuously as
new patient cohorts are introduced. Experimental results show that GluMind
consistently outperforms other state-of-the-art forecasting models, achieving
approximately 15% and 9% improvements in root mean squared error (RMSE) and
mean absolute error (MAE), respectively.

</details>


### [290] [Probabilistic Geometric Principal Component Analysis with application to neural data](https://arxiv.org/abs/2509.18469)
*Han-Lin Hsieh,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: PGPCA extends PPCA to model data distributed around nonlinear manifolds by incorporating manifold geometry, learning geometric coordinates, and using an EM algorithm; it reduces dimensionality and learns data distribution around/on the manifold, outperforming PPCA in simulations/brain data.


<details>
  <summary>Details</summary>
Motivation: PPCA and FA are linear models anchored in Euclidean space and struggle with data lying on nonlinear manifolds, which are common in neuroscience; there is a need for a probabilistic method that respects geometric structure and distinguishes manifold-aligned data from noise.

Method: Fit or learn the nonlinear manifold from data, derive a geometric coordinate system in addition to Euclidean coordinates, formulate a probabilistic model around the manifold, and develop a data-driven EM algorithm to estimate PGPCA parameters; provide a way to test whether the geometric coordinates better describe data than Euclidean ones.

Result: PGPCA effectively models data distributions around various manifolds and outperforms PPCA for such data; it enables testing of geometric versus Euclidean coordinates and supports dimensionality reduction and learning distributions around/on the manifold, demonstrated in simulations and brain data.

Conclusion: PGPCA generalizes PPCA to nonlinear geometries, offering a valuable framework for dimensionality reduction when data lie near or on manifolds; it enhances modeling of high-dimensional noisy data in neuroscience by incorporating manifold geometry.

Abstract: Dimensionality reduction is critical across various domains of science
including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a
prominent dimensionality reduction method that provides a probabilistic
approach unlike the deterministic approach of PCA and serves as a connection
between PCA and Factor Analysis (FA). Despite their power, PPCA and its
extensions are mainly based on linear models and can only describe the data in
a Euclidean coordinate system. However, in many neuroscience applications, data
may be distributed around a nonlinear geometry (i.e., manifold) rather than
lying in the Euclidean space. We develop Probabilistic Geometric Principal
Component Analysis (PGPCA) for such datasets as a new dimensionality reduction
algorithm that can explicitly incorporate knowledge about a given nonlinear
manifold that is first fitted from these data. Further, we show how in addition
to the Euclidean coordinate system, a geometric coordinate system can be
derived for the manifold to capture the deviations of data from the manifold
and noise. We also derive a data-driven EM algorithm for learning the PGPCA
model parameters. As such, PGPCA generalizes PPCA to better describe data
distributions by incorporating a nonlinear manifold geometry. In simulations
and brain data analyses, we show that PGPCA can effectively model the data
distribution around various given manifolds and outperforms PPCA for such data.
Moreover, PGPCA provides the capability to test whether the new geometric
coordinate system better describes the data than the Euclidean one. Finally,
PGPCA can perform dimensionality reduction and learn the data distribution both
around and on the manifold. These capabilities make PGPCA valuable for
enhancing the efficacy of dimensionality reduction for analysis of
high-dimensional data that exhibit noise and are distributed around a nonlinear
manifold.

</details>


### [291] [Discrete-time diffusion-like models for speech synthesis](https://arxiv.org/abs/2509.18470)
*Xiaozhou Tan,Minghui Zhao,Mattias Cross,Anton Ragni*

Main category: cs.LG

TL;DR: Discrete-time diffusion-like processes for speech generation can match continuous-time models in quality while improving training/inference efficiency and consistency.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of continuous-time diffusion models for speech: reliance on additive Gaussian noise, training/inference mismatch due to discretization, and potential inefficiency; explore discrete-time variants that enable more efficient and consistent training and sampling.

Method: Investigate diffusion-like discrete-time processes and propose several variants: additive Gaussian noise, multiplicative Gaussian noise, blurring noise, and a mixture of blurring and Gaussian noises; perform experiments comparing to continuous-time baselines on speech quality using subjective and objective metrics.

Result: Discrete-time variants achieve comparable speech quality to continuous-time models, with more efficient and consistent training and inference.

Conclusion: Discrete-time diffusion-like processes are a viable alternative for speech generation, enabling efficient training and sampling while maintaining quality; could inform future diffusion model design and reduce the gap between training and inference.

Abstract: Diffusion models have attracted a lot of attention in recent years. These
models view speech generation as a continuous-time process. For efficient
training, this process is typically restricted to additive Gaussian noising,
which is limiting. For inference, the time is typically discretized, leading to
the mismatch between continuous training and discrete sampling conditions.
Recently proposed discrete-time processes, on the other hand, usually do not
have these limitations, may require substantially fewer inference steps, and
are fully consistent between training/inference conditions. This paper explores
some diffusion-like discrete-time processes and proposes some new variants.
These include processes applying additive Gaussian noise, multiplicative
Gaussian noise, blurring noise and a mixture of blurring and Gaussian noises.
The experimental results suggest that discrete-time processes offer comparable
subjective and objective speech quality to their widely popular continuous
counterpart, with more efficient and consistent training and inference schemas.

</details>


### [292] [Individualized non-uniform quantization for vector search](https://arxiv.org/abs/2509.18471)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: NVQ introduces per-vector learned non-uniform vector quantizers using efficient nonlinearities, delivering higher accuracy with minimal computational cost for high-dimensional embeddings.


<details>
  <summary>Details</summary>
Motivation: Embedding vectors are high-dimensional and large, making memory/storage access and footprint expensive; efficient compression is needed for scalable, accurate vector search.

Method: Develop non-uniform vector quantizers based on novel parsimonious nonlinearities; each quantizer is independently learned for every indexed vector.

Result: NVQ achieves improved accuracy over state-of-the-art methods with only a minimal computational cost in experiments.

Conclusion: NVQ is computationally and spatially efficient in the high-fidelity regime, leveraging per-vector learned quantizers to enhance accuracy without increasing cost.

Abstract: Embedding vectors are widely used for representing unstructured data and
searching through it for semantically similar items. However, the large size of
these vectors, due to their high-dimensionality, creates problems for modern
vector search techniques: retrieving large vectors from memory/storage is
expensive and their footprint is costly. In this work, we present NVQ
(non-uniform vector quantization), a new vector compression technique that is
computationally and spatially efficient in the high-fidelity regime. The core
in NVQ is to use novel parsimonious and computationally efficient
nonlinearities for building non-uniform vector quantizers. Critically, these
quantizers are \emph{individually} learned for each indexed vector. Our
experimental results show that NVQ exhibits improved accuracy compared to the
state of the art with a minimal computational cost.

</details>


### [293] [SimpleFold: Folding Proteins is Simpler than You Think](https://arxiv.org/abs/2509.18480)
*Yuyang Wang,Jiarui Lu,Navdeep Jaitly,Josh Susskind,Miguel Angel Bautista*

Main category: cs.LG

TL;DR: SimpleFold is the first flow-matching protein folding model built solely from general-purpose transformer blocks, achieving competitive performance with state-of-the-art methods while enabling efficient ensemble predictions and deployment on consumer hardware, scaling to 3B parameters trained on ~9M distilled structures plus PDB data, challenging the need for domain-specific architectural components.


<details>
  <summary>Details</summary>
Motivation: To test whether domain-specific architectural modules in protein folding are essential or if a general-purpose generative framework can achieve comparable performance, enabling broader design space and easier deployment.

Method: Use a flow-matching objective (generative) with adaptive transformer blocks, training on 3B parameters; remove specialized modules (triangular updates, explicit pair reps, multi-objective domain-specific losses); include a structural term; train on ~9M distilled structures plus experimental PDB; evaluate on standard folding benchmarks and ensemble prediction.

Result: Achieves competitive performance on standard folding benchmarks; demonstrates strong ensemble prediction capability; efficient inference on consumer hardware; scalable approach.

Conclusion: Challenges the necessity of complex domain-specific designs for protein folding and opens a general-design space using standard transformers and flow-based objectives.

Abstract: Protein folding models have achieved groundbreaking results typically via a
combination of integrating domain knowledge into the architectural blocks and
training pipelines. Nonetheless, given the success of generative models across
different but related problems, it is natural to question whether these
architectural designs are a necessary condition to build performant models. In
this paper, we introduce SimpleFold, the first flow-matching based protein
folding model that solely uses general purpose transformer blocks. Protein
folding models typically employ computationally expensive modules involving
triangular updates, explicit pair representations or multiple training
objectives curated for this specific domain. Instead, SimpleFold employs
standard transformer blocks with adaptive layers and is trained via a
generative flow-matching objective with an additional structural term. We scale
SimpleFold to 3B parameters and train it on approximately 9M distilled protein
structures together with experimental PDB data. On standard folding benchmarks,
SimpleFold-3B achieves competitive performance compared to state-of-the-art
baselines, in addition SimpleFold demonstrates strong performance in ensemble
prediction which is typically difficult for models trained via deterministic
reconstruction objectives. Due to its general-purpose architecture, SimpleFold
shows efficiency in deployment and inference on consumer-level hardware.
SimpleFold challenges the reliance on complex domain-specific architectures
designs in protein folding, opening up an alternative design space for future
progress.

</details>


### [294] [Physics-informed time series analysis with Kolmogorov-Arnold Networks under Ehrenfest constraints](https://arxiv.org/abs/2509.18483)
*Abhijit Sen,Illya V. Lukin,Kurt Jacobs,Lev Kaplan,Andrii G. Sotnikov,Denys I. Bondar*

Main category: cs.LG

TL;DR: Physics-informed Kolmogorov Arnold Networks (KANs) are proposed to predict full quantum dynamical evolution efficiently, enforcing Ehrenfest theorems; they outperform data-hungry baselines and a Chain of KANs embeds temporal causality for time-series modeling.


<details>
  <summary>Details</summary>
Motivation: Modeling quantum dynamics in high-dimensional Hilbert spaces is computationally prohibitive with traditional methods, and existing neural architectures require large datasets with poor physical interpretability. A physics-informed, data-efficient approach is sought.

Method: Introduce Kolmogorov Arnold Networks (KANs) with physics-informed loss enforcing Ehrenfest theorems; train a Chain of KANs to embed temporal causality for time-series forecasting of quantum dynamics.

Result: KANs achieve superior accuracy with a fraction of the data (5.4%: 200 samples) versus Temporal Convolution Networks (3,700 samples); Chain of KANs further enhances time-series modeling while preserving physical consistency.

Conclusion: Physics-informed KANs offer a data-efficient, physically coherent alternative to black-box models for quantum dynamics, with the Chain of KANs effectively incorporating temporal causality.

Abstract: The prediction of quantum dynamical responses lies at the heart of modern
physics. Yet, modeling these time-dependent behaviors remains a formidable
challenge because quantum systems evolve in high-dimensional Hilbert spaces,
often rendering traditional numerical methods computationally prohibitive.
While large language models have achieved remarkable success in sequential
prediction, quantum dynamics presents a fundamentally different challenge:
forecasting the entire temporal evolution of quantum systems rather than merely
the next element in a sequence. Existing neural architectures such as recurrent
and convolutional networks often require vast training datasets and suffer from
spurious oscillations that compromise physical interpretability. In this work,
we introduce a fundamentally new approach: Kolmogorov Arnold Networks (KANs)
augmented with physics-informed loss functions that enforce the Ehrenfest
theorems. Our method achieves superior accuracy with significantly less
training data: it requires only 5.4 percent of the samples (200) compared to
Temporal Convolution Networks (3,700). We further introduce the Chain of KANs,
a novel architecture that embeds temporal causality directly into the model
design, making it particularly well-suited for time series modeling. Our
results demonstrate that physics-informed KANs offer a compelling advantage
over conventional black-box models, maintaining both mathematical rigor and
physical consistency while dramatically reducing data requirements.

</details>


### [295] [Hybrid Data can Enhance the Utility of Synthetic Data for Training Anti-Money Laundering Models](https://arxiv.org/abs/2509.18499)
*Rachel Chung,Pratyush Nidhi Sharma,Mikko Siponen,Rohit Vadodaria,Luke Smith*

Main category: cs.LG

TL;DR: Hybrid datasets that blend synthetically generated AML data with publicly available real-world features can preserve privacy while improving the performance of GNN-based AML models, addressing data access constraints.


<details>
  <summary>Details</summary>
Motivation: Privacy and confidentiality restrict access to real AML data for training. Purely synthetic data helps privacy but may reduce realism and utility. A hybrid approach aims to retain privacy while enhancing model performance.

Method: Create synthetic AML datasets and augment them with publicly available real-world features (hybrid data). Train graph neural networks on this augmented data and compare against models trained on purely synthetic data, evaluating privacy preservation and utility.

Result: Hybrid datasets preserve privacy and improve model utility (e.g., detection performance) compared to purely synthetic data, offering a practical pathway for financial institutions to enhance AML systems.

Conclusion: Using hybrid data that combines synthetic and public real-world features is a practical and beneficial strategy for AML model development under privacy constraints and can be extended with additional features and privacy assessments.

Abstract: Money laundering is a critical global issue for financial institutions.
Automated Anti-money laundering (AML) models, like Graph Neural Networks (GNN),
can be trained to identify illicit transactions in real time. A major issue for
developing such models is the lack of access to training data due to privacy
and confidentiality concerns. Synthetically generated data that mimics the
statistical properties of real data but preserves privacy and confidentiality
has been proposed as a solution. However, training AML models on purely
synthetic datasets presents its own set of challenges. This article proposes
the use of hybrid datasets to augment the utility of synthetic datasets by
incorporating publicly available, easily accessible, and real-world features.
These additions demonstrate that hybrid datasets not only preserve privacy but
also improve model utility, offering a practical pathway for financial
institutions to enhance AML systems.

</details>


### [296] [APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation](https://arxiv.org/abs/2509.18521)
*Yuzhen Zhou,Jiajun Li,Yusheng Su,Gowtham Ramesh,Zilin Zhu,Xiang Long,Chenyang Zhao,Jin Pan,Xiaodong Yu,Ze Wang,Kangrui Du,Jialian Wu,Ximeng Sun,Jiang Liu,Qiaolin Yu,Hao Chen,Zicheng Liu,Emad Barsoum*

Main category: cs.LG

TL;DR: APRIL introduces Active Partial Rollouts to reduce GPU idle time by over-provisioning and recycling incomplete rollout responses, achieving up to 44% rollout throughput gains and up to 8% higher final accuracy; framework- and hardware-agnostic.


<details>
  <summary>Details</summary>
Motivation: RL training for large LM systems is dominated by rollout generation (>90% of compute) and hampered by long-tail response lengths that waste GPU resources; improving rollout efficiency is key for scalable RL systems.

Method: During rollout, APRIL over-provisions requests, stops once a target number of responses is reached, and recycles unfinished responses for continuation in later steps. This prevents discarding rollouts and reduces GPU idle time. It is integrated into the slime RL framework and is hardware-agnostic, compatible with NVIDIA and AMD GPUs.

Result: Throughput improvement up to 44% across GRPO, DAPO, GSPO; faster convergence; final task accuracy increased by up to 8%.

Conclusion: APRIL unifies system-level optimizations and algorithmic changes to advance RL training efficiency for large LM RL pipelines and is readily adoptable across frameworks and hardware.

Abstract: Reinforcement learning (RL) has become a cornerstone in advancing large-scale
pre-trained language models (LLMs). Successive generations, including GPT-o
series, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale
RL training to enhance reasoning and coding capabilities. To meet the
community's growing RL needs, numerous RL frameworks have been proposed. Most
of these frameworks primarily rely on inference engines for rollout generation
and training engines for policy updates. However, RL training remains
computationally expensive, with rollout generation accounting for more than 90%
of total runtime. In addition, its efficiency is often constrained by the
long-tail distribution of rollout response lengths, where a few lengthy
responses stall entire batches, leaving GPUs idle and underutilized. As model
and rollout sizes continue to grow, this bottleneck increasingly limits
scalability. To address this challenge, we propose Active Partial Rollouts in
Reinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the
rollout phase, APRIL over-provisions rollout requests, terminates once the
target number of responses is reached, and recycles incomplete responses for
continuation in future steps. This strategy ensures that no rollouts are
discarded while substantially reducing GPU idle time. Experiments show that
APRIL improves rollout throughput by at most 44% across commonly used RL
algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8%
higher final accuracy across tasks. Moreover, APRIL is both framework and
hardware agnostic, already integrated into the slime RL framework, and
deployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies
system-level and algorithmic considerations in proposing APRIL, with the aim of
advancing RL training efficiency and inspiring further optimizations in RL
systems.

</details>


### [297] [Reverse-Complement Consistency for DNA Language Models](https://arxiv.org/abs/2509.18529)
*Mingqian Ma*

Main category: cs.LG

TL;DR: RCCR is a simple, model-agnostic fine-tuning objective that enforces consistency between a DNA sequence and its reverse complement, improving RC robustness across architectures and tasks without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: DNA's reverse-complement symmetry implies identical biological meaning for a sequence and its RC; current language models often flip predictions for RC pairs, undermining reliability and interpretability.

Method: Introduce Reverse-Complement Consistency Regularization (RCCR), a fine-tuning penalty that minimizes divergence between a model's outputs on a sequence and its reverse complement. Model-agnostic and applied during fine-tuning across backbones (Nucleotide Transformer, HyenaDNA, DNABERT-2) and tasks (classification, regression, profile prediction).

Result: RCCR substantially improves RC robustness by dramatically reducing prediction flips and errors, while maintaining or improving task accuracy compared with baselines such as RC data augmentation and test-time averaging.

Conclusion: Incorporating the biological prior of RC symmetry yields a robust, intrinsic, and computationally efficient fine-tuning recipe applicable across diverse genomics tasks and architectures.

Abstract: A fundamental property of DNA is that the reverse complement (RC) of a
sequence often carries identical biological meaning. However, state-of-the-art
DNA language models frequently fail to capture this symmetry, producing
inconsistent predictions for a sequence and its RC counterpart, which
undermines their reliability. In this work, we introduce Reverse-Complement
Consistency Regularization (RCCR), a simple and model-agnostic fine-tuning
objective that directly penalizes the divergence between a model's prediction
on a sequence and the aligned prediction on its reverse complement. We evaluate
RCCR across three diverse backbones (Nucleotide Transformer, HyenaDNA,
DNABERT-2) on a wide range of genomic tasks, including sequence classification,
scalar regression, and profile prediction. Our experiments show that RCCR
substantially improves RC robustness by dramatically reducing prediction flips
and errors, all while maintaining or improving task accuracy compared to
baselines such as RC data augmentation and test-time averaging. By integrating
a key biological prior directly into the learning process, RCCR produces a
single, intrinsically robust, and computationally efficient model fine-tuning
recipe for diverse biology tasks.

</details>


### [298] [Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts](https://arxiv.org/abs/2509.18542)
*Qi Wang,Hanyang Peng,Yue Yu*

Main category: cs.LG

TL;DR: Symphony-MoE enables assembling MoEs from heterogeneous, identically-architected but different pre-trained models by a two-stage harmony approach, achieving better multi-domain and out-of-distribution performance with minimal training.


<details>
  <summary>Details</summary>
Motivation: To increase expert diversity in MoEs beyond upcycled single-model backbones and address misalignment when combining disparate pre-trained sources.

Method: Two-stage framework: (1) training-free harmony that builds a shared backbone via layer-aware fusion and activates alignment to align parameters across experts; (2) a lightweight router training stage that coordinates the mixture.

Result: Empirically, Symphony-MoE successfully integrates experts from heterogeneous sources and significantly surpasses baselines on multi-domain tasks and out-of-distribution generalization.

Conclusion: A practical method to diversify MoEs by leveraging heterogeneous pre-trained models with minimal extra training, yielding improved generalization across domains.

Abstract: Mixture-of-Experts (MoE) models enable scalable performance by activating
large parameter sets sparsely, minimizing computational overhead. To circumvent
the prohibitive cost of training MoEs from scratch, recent work employs
upcycling, reusing a single pre-trained dense model by replicating its
feed-forward network (FFN) layers into experts. However, this limits expert
diversity, as all experts originate from a single pre-trained dense model. This
paper addresses this limitation by constructing powerful MoE models using
experts sourced from multiple identically-architected but disparate pre-trained
models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact
that these source models occupy disparate, dissonant regions of the parameter
space, making direct upcycling prone to severe performance degradation. To
overcome this, we propose Symphony-MoE, a novel two-stage framework designed to
harmonize these models into a single, coherent expert mixture. First, we
establish this harmony in a training-free manner: we construct a shared
backbone via a layer-aware fusion strategy and, crucially, alleviate parameter
misalignment among experts using activation-based functional alignment.
Subsequently, a single lightweight stage of router training coordinates the
entire architecture. Experiments demonstrate that our method successfully
integrates experts from heterogeneous sources, achieving an MoE model that
significantly surpasses baselines in multi-domain tasks and out-of-distribution
generalization.

</details>


### [299] [Global Minimizers of Sigmoid Contrastive Loss](https://arxiv.org/abs/2509.18552)
*Kiril Bangachev,Guy Bresler,Iliyas Noman,Yury Polyanskiy*

Main category: cs.LG

TL;DR: The paper offers a theoretical explanation for why trainable inverse temperature and bias in the sigmoid loss (as in SigLIP/SigLIP2) help contrastive pretraining. It introduces (m, b_rel)-Constellations to characterize zero-loss configurations, connects these to retrieval performance, modality gaps, and embedding dimension requirements, and proposes a reparameterization with explicit relative bias to improve training dynamics on synthetic data.


<details>
  <summary>Details</summary>
Motivation: Explain and justify the observed advantages of SigLIP-style sigmoid losses with trainable temperature and bias, reveal the geometric/combinatorial structures that enable successful retrieval, and provide practical guidance on when high-quality representations are achievable.

Method: Develop a theoretical framework around the sigmoid loss with trainable temperature and bias. Define and analyze (m, b_rel)-Constellations, a combinatorial object related to spherical codes, to characterize zero-loss configurations. Use this framework to justify SigLIP’s retrieval performance, explain the modality gap, and derive the embedding dimension needed for high-quality representations. Propose a reparameterization of the sigmoid loss with explicit relative bias and test its impact on training dynamics using synthetic data.

Result: A characterization of zero-loss configurations via (m, b_rel)-Constellations that links loss dynamics to retrieval success and modality differences. The theory identifies how temperature and bias influence the optimization landscape and provides dimensionality requirements. A reparameterization with explicit relative bias improves training dynamics on synthetic data.

Conclusion: The analysis provides a principled explanation for SigLIP's strengths and limitations, introduces constellations as a useful tool for understanding sigmoid-based contrastive learning, and offers a practical reparameterization to enhance training stability and performance on synthetic datasets.

Abstract: The meta-task of obtaining and aligning representations through contrastive
pretraining is steadily gaining importance since its introduction in CLIP and
ALIGN. In this paper we theoretically explain the advantages of synchronizing
with trainable inverse temperature and bias under the sigmoid loss, as
implemented in the recent SigLIP and SigLIP2 models of Google DeepMind.
Temperature and bias can drive the loss function to zero for a rich class of
configurations that we call $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations. $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations are a novel combinatorial object
related to spherical codes and are parametrized by a margin $\mathsf{m}$ and
relative bias $\mathsf{b}_{\mathsf{rel}}$. We use our characterization of
constellations to theoretically justify the success of SigLIP on retrieval, to
explain the modality gap present in SigLIP, and to identify the necessary
dimension for producing high-quality representations. Finally, we propose a
reparameterization of the sigmoid loss with explicit relative bias, which
improves training dynamics in experiments with synthetic data.

</details>


### [300] [Explainable Graph Neural Networks: Understanding Brain Connectivity and Biomarkers in Dementia](https://arxiv.org/abs/2509.18568)
*Niharika Tewari,Nguyen Linh Dan Le,Mujie Liu,Jing Ren,Ziqi Xu,Tabinda Sarwar,Veeky Baths,Feng Xia*

Main category: cs.LG

TL;DR: A comprehensive review of explainable graph neural networks (XGNNs) in dementia, covering applications across Alzheimer's, Parkinson's, mild cognitive impairment, and multi-disease diagnosis, with a taxonomy of explainability methods, clinical comparisons, and discussion of challenges and future directions including generalizability and LLM integration.


<details>
  <summary>Details</summary>
Motivation: Dementia's clinical and biological heterogeneity makes diagnosis and subtype differentiation difficult; while GNNs can model brain connectivity, their lack of robustness, limited data, and poor interpretability hinder clinical adoption. XGNNs aim to provide transparent, biomarker-relevant insights to support clinicians.

Method: Systematic literature review of XGNNs in dementia, development of a taxonomy of explainability methods tailored to dementia tasks, and comparative analysis of models in various clinical scenarios.

Result: Synthesis of current XGNN approaches in dementia, a structured taxonomy of explainability methods, benchmarking across diseases, identification of limitations (generalizability, data scarcity, domain gaps), and actionable recommendations for future research.

Conclusion: XGNNs offer a pathway to trustworthy, clinically meaningful dementia diagnostics via interpretable graph-based models, but challenges remain in generalizability, integration with emerging AI tools (e.g., LLMs), and broader validation to enable routine clinical deployment.

Abstract: Dementia is a progressive neurodegenerative disorder with multiple
etiologies, including Alzheimer's disease, Parkinson's disease, frontotemporal
dementia, and vascular dementia. Its clinical and biological heterogeneity
makes diagnosis and subtype differentiation highly challenging. Graph Neural
Networks (GNNs) have recently shown strong potential in modeling brain
connectivity, but their limited robustness, data scarcity, and lack of
interpretability constrain clinical adoption. Explainable Graph Neural Networks
(XGNNs) have emerged to address these barriers by combining graph-based
learning with interpretability, enabling the identification of disease-relevant
biomarkers, analysis of brain network disruptions, and provision of transparent
insights for clinicians. This paper presents the first comprehensive review
dedicated to XGNNs in dementia research. We examine their applications across
Alzheimer's disease, Parkinson's disease, mild cognitive impairment, and
multi-disease diagnosis. A taxonomy of explainability methods tailored for
dementia-related tasks is introduced, alongside comparisons of existing models
in clinical scenarios. We also highlight challenges such as limited
generalizability, underexplored domains, and the integration of Large Language
Models (LLMs) for early detection. By outlining both progress and open
problems, this review aims to guide future work toward trustworthy, clinically
meaningful, and scalable use of XGNNs in dementia research.

</details>


### [301] [Interaction Topological Transformer for Multiscale Learning in Porous Materials](https://arxiv.org/abs/2509.18573)
*Dong Chen,Jian Liu,Chun-Long Chen,Guo-Wei Wei*

Main category: cs.LG

TL;DR: Introduces Interaction Topological Transformer (ITT), a data-efficient, multi-scale framework for porous materials using interaction topology and a two-stage training (self-supervised pretraining on 0.6M unlabeled structures plus supervised fine-tuning) to achieve state-of-the-art predictions for adsorption, transport, and stability.


<details>
  <summary>Details</summary>
Motivation: Prediction of structure–property relationships in porous materials is challenged by multiscale complexity and sparse, uneven labeled data; current models struggle to generalize across material families.

Method: ITT constructs scale-aware features across structural, elemental, atomic, and pairwise-elemental levels via an interaction topology, integrated in a Transformer. Training proceeds in two stages: self-supervised pretraining on ~0.6 million unlabeled structures, followed by supervised fine-tuning.

Result: Achieves state-of-the-art, accurate, transferable predictions for adsorption, transport, and stability properties across diverse porous frameworks.

Conclusion: Offers a principled, scalable data-efficient path for learning-guided discovery in structurally and chemically diverse porous materials.

Abstract: Porous materials exhibit vast structural diversity and support critical
applications in gas storage, separations, and catalysis. However, predictive
modeling remains challenging due to the multiscale nature of structure-property
relationships, where performance is governed by both local chemical
environments and global pore-network topology. These complexities, combined
with sparse and unevenly distributed labeled data, hinder generalization across
material families. We propose the Interaction Topological Transformer (ITT), a
unified data-efficient framework that leverages novel interaction topology to
capture materials information across multiple scales and multiple levels,
including structural, elemental, atomic, and pairwise-elemental organization.
ITT extracts scale-aware features that reflect both compositional and
relational structure within complex porous frameworks, and integrates them
through a built-in Transformer architecture that supports joint reasoning
across scales. Trained using a two-stage strategy, i.e., self-supervised
pretraining on 0.6 million unlabeled structures followed by supervised
fine-tuning, ITT achieves state-of-the-art, accurate, and transferable
predictions for adsorption, transport, and stability properties. This framework
provides a principled and scalable path for learning-guided discovery in
structurally and chemically diverse porous materials.

</details>


### [302] [DS-Diffusion: Data Style-Guided Diffusion Model for Time-Series Generation](https://arxiv.org/abs/2509.18584)
*Mingchun Sun,Rongqiang Zhao,Jie Liu*

Main category: cs.LG

TL;DR: DS-Diffusion introduces a style-guided diffusion model for time series that avoids retraining for conditional guidance, reduces distributional bias via THD, and offers interpretable, style-indicative samples with flexible conditioning.


<details>
  <summary>Details</summary>
Motivation: Time series diffusion models struggle with retraining for specific conditioning, exhibit distributional bias, and have opaque inference. A flexible, bias-reducing, interpretable approach is needed.

Method: DS-Diffusion uses style-guided kernels to enable conditioning without retraining; time-information based hierarchical denoising (THD) to reduce distributional bias; samples reveal data style; evaluated on multiple public datasets.

Result: Outperforms state-of-the-art (e.g., ImagenTime) with reductions in predictive score by 5.56% and discriminative score by 61.55%; also reduces distributional bias and improves interpretability; no retraining required for targeted conditions.

Conclusion: DS-Diffusion achieves flexible, interpretable, and less biased time series generation without retraining, suggesting strong promise for conditional diffusion in time-series tasks.

Abstract: Diffusion models are the mainstream approach for time series generation
tasks. However, existing diffusion models for time series generation require
retraining the entire framework to introduce specific conditional guidance.
There also exists a certain degree of distributional bias between the generated
data and the real data, which leads to potential model biases in downstream
tasks. Additionally, the complexity of diffusion models and the latent spaces
leads to an uninterpretable inference process. To address these issues, we
propose the data style-guided diffusion model (DS-Diffusion). In the
DS-Diffusion, a diffusion framework based on style-guided kernels is developed
to avoid retraining for specific conditions. The time-information based
hierarchical denoising mechanism (THD) is developed to reduce the
distributional bias between the generated data and the real data. Furthermore,
the generated samples can clearly indicate the data style from which they
originate. We conduct comprehensive evaluations using multiple public datasets
to validate our approach. Experimental results show that, compared to the
state-of-the-art model such as ImagenTime, the predictive score and the
discriminative score decrease by 5.56% and 61.55%, respectively. The
distributional bias between the generated data and the real data is further
reduced, the inference process is also more interpretable. Moreover, by
eliminating the need to retrain the diffusion model, the flexibility and
adaptability of the model to specific conditions are also enhanced.

</details>


### [303] [Reflect before Act: Proactive Error Correction in Language Models](https://arxiv.org/abs/2509.18607)
*Qiuhai Zeng,Sarvesh Rajkumar,Di Wang,Narendra Gyanchandani,Wenbo Yan*

Main category: cs.LG

TL;DR: Introduce Reflect before Act (REBACT), a reflect-before-action step that curbs error accumulation in LLM-based decision-making, yielding substantial gains across ALFWorld, WebShop, and TextCraft with minimal edits.


<details>
  <summary>Details</summary>
Motivation: LLMs in interactive decision-making suffer from error accumulation and limited self-correction; a pre-action reflection mechanism can detect and correct errors before they propagate.

Method: Insert a 'reflect' step before each action, enabling immediate error correction using environment feedback; evaluated on ALFWorld, WebShop, and TextCraft with Claude3.5-sonnet as the base LLM; the reflect step requires only a few modification steps.

Result: Outperforms strong baselines with notable gains: WebShop 61% ( +24%), ALFWorld 98.51% ( +6.72%), TextCraft 99.5% ( +0.5% ). Demonstrates efficiency due to few modification steps.

Conclusion: REBACT provides robust, efficient self-correction and improved decision-making across diverse interactive environments; the approach is lightweight and generalizes beyond the tested domains.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
interactive decision-making tasks, but existing methods often struggle with
error accumulation and lack robust self-correction mechanisms. We introduce
"Reflect before Act" (REBACT), a novel approach that enhances LLM-based
decision-making by introducing a critical reflect step prior to taking the next
action. This approach allows for immediate error correction, ensuring smooth
action path and adaptibity to environment feedback. We evaluate REBACT on three
diverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results
demonstrate that REBACT significantly outperforms strong baselines, improving
success rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld
(achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using
Claude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's
performance improvements are achieved with only a few modification steps,
demonstrating its computational efficiency.

</details>


### [304] [Flow marching for a generative PDE foundation model](https://arxiv.org/abs/2509.18611)
*Zituo Chen,Sili Deng*

Main category: cs.LG

TL;DR: Introduces Flow Marching to build a generative PDE foundation model by learning a unified velocity field via flow matching, enabling uncertainty-aware generation and stable long-term rollouts; augments with P2VAE and Flow Marching Transformer (FMT) for efficient training on large PDE trajectory datasets.


<details>
  <summary>Details</summary>
Motivation: Deterministic Transformer-based PDE models have limited generative flexibility and struggle with long-term rollout stability and uncertainty quantification in complex dynamical systems.

Method: Flow Marching learns a velocity field by jointly sampling noise levels and physical time steps between adjacent states, transporting noisy states toward clean successors. Adds Physics-Pretrained VAE (P2VAE) for latent embedding and a Flow Marching Transformer (FMT) combining diffusion-forcing with latent temporal pyramids for efficiency (up to 15x faster than full video diffusion). Pretraining on ~2.5 million trajectories across 12 PDE families.

Result: Shows improved long-term rollout stability versus deterministic baselines, uncertainty-aware ensemble generations, and robust few-shot adaptation to unseen Kolmogorov turbulence; demonstrates substantial computational efficiency gains enabling large-scale pretraining.

Conclusion: Generative PDE foundation models—enabled by flow-based diffusion, latent representations, and efficient transformers—offer practical advantages for uncertainty quantification and scalable modeling of complex PDE-governed dynamics.

Abstract: Pretraining on large-scale collections of PDE-governed spatiotemporal
trajectories has recently shown promise for building generalizable models of
dynamical systems. Yet most existing PDE foundation models rely on
deterministic Transformer architectures, which lack generative flexibility for
many science and engineering applications. We propose Flow Marching, an
algorithm that bridges neural operator learning with flow matching motivated by
an analysis of error accumulation in physical dynamical systems, and we build a
generative PDE foundation model on top of it. By jointly sampling the noise
level and the physical time step between adjacent states, the model learns a
unified velocity field that transports a noisy current state toward its clean
successor, reducing long-term rollout drift while enabling uncertainty-aware
ensemble generations. Alongside this core algorithm, we introduce a
Physics-Pretrained Variational Autoencoder (P2VAE) to embed physical states
into a compact latent space, and an efficient Flow Marching Transformer (FMT)
that combines a diffusion-forcing scheme with latent temporal pyramids,
achieving up to 15x greater computational efficiency than full-length video
diffusion models and thereby enabling large-scale pretraining at substantially
reduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE
families and train suites of P2VAEs and FMTs at multiple scales. On downstream
evaluation, we benchmark on unseen Kolmogorov turbulence with few-shot
adaptation, demonstrate long-term rollout stability over deterministic
counterparts, and present uncertainty-stratified ensemble results, highlighting
the importance of generative PDE foundation models for real-world applications.

</details>


### [305] [HyperAdapt: Simple High-Rank Adaptation](https://arxiv.org/abs/2509.18629)
*Abel Gurung,Joseph Campbell*

Main category: cs.LG

TL;DR: HyperAdapt is a PEFT method that uses row- and column-wise scaling via diagonal matrices to create a high-rank update with only n+m trainable parameters for an n×m weight matrix, achieving comparable performance to full fine-tuning and state-of-the-art PEFT with far fewer parameters.


<details>
  <summary>Details</summary>
Motivation: To reduce the memory and compute burden of adapting large foundation models to specialized tasks by minimizing trainable parameters while preserving performance gains.

Method: HyperAdapt factorizes the adaptation of a pre-trained weight matrix into row- and column-wise diagonal scalings, enabling a high-rank update with only n+m trainable parameters, and provides theoretical rank bounds alongside empirical validation across tasks and models up to 14B parameters.

Result: Empirically, HyperAdapt consistently induces high-rank updates, and on benchmarks such as GLUE, arithmetic reasoning, and commonsense reasoning, it matches or nearly matches full fine-tuning and state-of-the-art PEFT with orders of magnitude fewer trainable parameters.

Conclusion: HyperAdapt offers an effective and parameter-efficient alternative to full fine-tuning and existing PEFT methods, delivering competitive performance with dramatically reduced trainable parameter counts.

Abstract: Foundation models excel across diverse tasks, but adapting them to
specialized applications often requires fine-tuning, an approach that is memory
and compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate
this by updating only a small subset of weights. In this paper, we introduce
HyperAdapt, a parameter-efficient fine-tuning method that significantly reduces
the number of trainable parameters compared to state-of-the-art methods like
LoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying
row- and column-wise scaling through diagonal matrices, thereby inducing a
high-rank update while requiring only $n+m$ trainable parameters for an $n
\times m$ matrix. Theoretically, we establish an upper bound on the rank of
HyperAdapt's updates, and empirically, we confirm that it consistently induces
high-rank transformations across model layers. Experiments on GLUE, arithmetic
reasoning, and commonsense reasoning benchmarks with models up to 14B
parameters demonstrate that HyperAdapt matches or nearly matches the
performance of full fine-tuning and state-of-the-art PEFT methods while using
orders of magnitude fewer trainable parameters.

</details>


### [306] [Subspace Clustering of Subspaces: Unifying Canonical Correlation Analysis and Subspace Clustering](https://arxiv.org/abs/2509.18653)
*Paris A. Karakasis,Nicholas D. Sidiropoulos*

Main category: cs.LG

TL;DR: Introduces SCoS, a framework to cluster tall matrices by their column spaces using Block Term Decomposition on a constructed tensor, with identifiability guarantees and scalable algorithms, outperforming existing subspace clustering in noisy hyperspectral data.


<details>
  <summary>Details</summary>
Motivation: Traditional subspace clustering focuses on vector data and may fail to exploit matrix-structured samples. This work targets clustering of matrix-valued samples by their column spaces (subspaces), addressing high-dimensional data where structure exists beyond individual vectors and enabling shared subspace discovery.

Method: Construct a third-order tensor from input matrices and apply Block Term Decomposition (BTD) to jointly estimate cluster memberships and partially shared subspaces. The approach includes identifiability results and scalable optimization schemes suitable for large datasets.

Result: First identifiability results for this formulation; demonstrated superior clustering accuracy and robustness on real-world hyperspectral imaging datasets, particularly under high noise and interference, compared with existing subspace clustering methods.

Conclusion: The proposed SCoS framework extends subspace clustering to matrix-valued samples, providing identifiability and scalable solutions; it shows strong potential for challenging high-dimensional applications where matrix structure carries important information.

Abstract: We introduce a novel framework for clustering a collection of tall matrices
based on their column spaces, a problem we term Subspace Clustering of
Subspaces (SCoS). Unlike traditional subspace clustering methods that assume
vectorized data, our formulation directly models each data sample as a matrix
and clusters them according to their underlying subspaces. We establish
conceptual links to Subspace Clustering and Generalized Canonical Correlation
Analysis (GCCA), and clarify key differences that arise in this more general
setting. Our approach is based on a Block Term Decomposition (BTD) of a
third-order tensor constructed from the input matrices, enabling joint
estimation of cluster memberships and partially shared subspaces. We provide
the first identifiability results for this formulation and propose scalable
optimization algorithms tailored to large datasets. Experiments on real-world
hyperspectral imaging datasets demonstrate that our method achieves superior
clustering accuracy and robustness, especially under high noise and
interference, compared to existing subspace clustering techniques. These
results highlight the potential of the proposed framework in challenging
high-dimensional applications where structure exists beyond individual data
vectors.

</details>


### [307] [Towards Rational Pesticide Design with Graph Machine Learning Models for Ecotoxicology](https://arxiv.org/abs/2509.18703)
*Jakub Adamczyk*

Main category: cs.LG

TL;DR: Presents ApisTox dataset and benchmarking across molecular ML methods for pesticide ecotoxicology, finding that medicinal-chemistry models often fail on agrochemicals, arguing for domain-specific benchmarks and models.


<details>
  <summary>Details</summary>
Motivation: Address ecotoxicology data gaps and the need for safer eco-friendly agrochemicals; existing ML models from drug discovery may not generalize to agrochemicals; create ApisTox to enable benchmarking.

Method: Create ApisTox, the largest curated dataset on pesticide toxicity to honey bees; evaluate multiple ML approaches for molecular graph classification (fingerprints, graph kernels, GNNs, pretrained transformers).

Result: Medicinal-chemistry ML methods often fail to generalize to agrochemicals; highlights need for domain-specific models and benchmarks.

Conclusion: Develop a comprehensive benchmarking suite and ML models tailored to pesticide discovery; emphasize domain-specific challenges and ecotoxicology emphasis.

Abstract: This research focuses on rational pesticide design, using graph machine
learning to accelerate the development of safer, eco-friendly agrochemicals,
inspired by in silico methods in drug discovery. With an emphasis on
ecotoxicology, the initial contributions include the creation of ApisTox, the
largest curated dataset on pesticide toxicity to honey bees. We conducted a
broad evaluation of machine learning (ML) models for molecular graph
classification, including molecular fingerprints, graph kernels, GNNs, and
pretrained transformers. The results show that methods successful in medicinal
chemistry often fail to generalize to agrochemicals, underscoring the need for
domain-specific models and benchmarks. Future work will focus on developing a
comprehensive benchmarking suite and designing ML models tailored to the unique
challenges of pesticide discovery.

</details>


### [308] [A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications](https://arxiv.org/abs/2509.18714)
*Zhenyu Tao,Wei Xu,Xiaohu You*

Main category: cs.LG

TL;DR: A formal Generalized Bisimulation Metric (GBSM) for pairs of MDPs is proposed, with rigorous properties and tight performance bounds for multi-MDP tasks such as policy transfer, state aggregation, and sampling-based estimation; it yields a closed-form sample complexity and is validated numerically.


<details>
  <summary>Details</summary>
Motivation: Extend the bisimulation metric to scenarios involving two MDPs to enable principled policy transfer and cross-MDP analysis, addressing gaps in theoretical guarantees for multi-MDP settings.

Method: Define GBSM over pairs of MDPs, prove three fundamental properties (symmetry, inter-MDP triangle inequality, and identical-state-space distance bounds), derive explicit, tighter bounds for policy transfer, state aggregation, and sampling-based estimation, and present a closed-form sample complexity; support with numerical experiments.

Result: Obtains explicit bounds tighter than standard BSM, provides closed-form sample complexity for estimation, and demonstrates practical effectiveness in multi-MDP contexts through simulations.

Conclusion: GBSM provides a rigorous, tighter theoretical framework for comparing and transferring policies across MDPs, enabling more reliable multi-MDP learning and transfer with practical estimation guarantees.

Abstract: The bisimulation metric (BSM) is a powerful tool for computing state
similarities within a Markov decision process (MDP), revealing that states
closer in BSM have more similar optimal value functions. While BSM has been
successfully utilized in reinforcement learning (RL) for tasks like state
representation learning and policy exploration, its application to multiple-MDP
scenarios, such as policy transfer, remains challenging. Prior work has
attempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis
of its mathematical properties has limited further theoretical progress. In
this work, we formally establish a generalized bisimulation metric (GBSM)
between pairs of MDPs, which is rigorously proven with the three fundamental
properties: GBSM symmetry, inter-MDP triangle inequality, and the distance
bound on identical state spaces. Leveraging these properties, we theoretically
analyse policy transfer, state aggregation, and sampling-based estimation in
MDPs, obtaining explicit bounds that are strictly tighter than those derived
from the standard BSM. Additionally, GBSM provides a closed-form sample
complexity for estimation, improving upon existing asymptotic results based on
BSM. Numerical results validate our theoretical findings and demonstrate the
effectiveness of GBSM in multi-MDP scenarios.

</details>


### [309] [LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection](https://arxiv.org/abs/2509.18719)
*Bo Qu,Zhurong Wang,Daisuke Yagi,Zhen Xu,Yang Zhao,Yinan Shan,Frank Zahradnik*

Main category: cs.LG

TL;DR: An LLM-augmented RL framework for e-commerce fraud detection; LLMs refine RL reward functions in a multi-step MDP, achieving better fraud detection and long-term robustness with real-world data.


<details>
  <summary>Details</summary>
Motivation: Reward design in industrial RL for fraud detection is complex and time-consuming; LLMs can provide reasoning and coding guidance to iteratively improve reward functions, enabling better performance and automation.

Method: Model transaction risk as a multi-step MDP; apply RL to optimize detection across payment stages; use LLMs to iteratively refine reward functions; zero-shot capability; evaluate on real-world data with long-term experiments.

Result: Improved fraud detection accuracy, robustness and resilience; zero-shot reward refinement demonstrated; long-term evaluations confirm effectiveness.

Conclusion: LLM-enhanced RL is a promising direction for industrial RL tasks like fraud detection, enabling automated reward engineering and improved performance.

Abstract: This paper presents a novel approach to e-commerce payment fraud detection by
integrating reinforcement learning (RL) with Large Language Models (LLMs). By
framing transaction risk as a multi-step Markov Decision Process (MDP), RL
optimizes risk detection across multiple payment stages. Crafting effective
reward functions, essential for RL model success, typically requires
significant human expertise due to the complexity and variability in design.
LLMs, with their advanced reasoning and coding capabilities, are well-suited to
refine these functions, offering improvements over traditional methods. Our
approach leverages LLMs to iteratively enhance reward functions, achieving
better fraud detection accuracy and demonstrating zero-shot capability.
Experiments with real-world data confirm the effectiveness, robustness, and
resilience of our LLM-enhanced RL framework through long-term evaluations,
underscoring the potential of LLMs in advancing industrial RL applications.

</details>


### [310] [Theory of periodic convolutional neural network](https://arxiv.org/abs/2509.18744)
*Yuqing Liu*

Main category: cs.LG

TL;DR: Periodic CNNs with periodic boundaries can approximate ridge functions depending on d-1 linear variables in d dimensions, establishing a sharp expressive power boundary that fails for d-2 or fewer variables; this architecture is promising for wrapped-domain problems and related fields.


<details>
  <summary>Details</summary>
Motivation: Fill gaps in CNN approximation theory by introducing periodic boundary conditions and proving a precise expressivity limit; identify problem domains where ridge-like structure with high intrinsic dimension is natural (e.g., wrapped domains, physics-informed learning, materials science).

Method: Introduce a periodic CNN architecture and prove an approximation theorem: periodic CNNs can approximate ridge functions of d-1 linear variables in R^d, while such approximation is impossible for ridge functions depending on d-2 or fewer variables; provide theoretical analysis of expressivity.

Result: Establishes a sharp characterization of the expressive power of periodic CNNs and demonstrates their suitability for wrapped-domain data and ridge-structured problems; contributes to the mathematical foundation of CNN approximation theory and suggests practical architectures with strong approximation capabilities.

Conclusion: Periodic CNNs offer a provably stronger expressivity for high-dimensional ridge functions than conventional settings, expanding the theoretical framework and pointing to practical applications in wrapped domains and high-intrinsic-dimension problems.

Abstract: We introduce a novel convolutional neural network architecture, termed the
\emph{periodic CNN}, which incorporates periodic boundary conditions into the
convolutional layers. Our main theoretical contribution is a rigorous
approximation theorem: periodic CNNs can approximate ridge functions depending
on $d-1$ linear variables in a $d$-dimensional input space, while such
approximation is impossible in lower-dimensional ridge settings ($d-2$ or fewer
variables). This result establishes a sharp characterization of the expressive
power of periodic CNNs. Beyond the theory, our findings suggest that periodic
CNNs are particularly well-suited for problems where data naturally admits a
ridge-like structure of high intrinsic dimension, such as image analysis on
wrapped domains, physics-informed learning, and materials science. The work
thus both expands the mathematical foundation of CNN approximation theory and
highlights a class of architectures with surprising and practically relevant
approximation capabilities.

</details>


### [311] [MOMEMTO: Patch-based Memory Gate Model in Time Series Foundation Model](https://arxiv.org/abs/2509.18751)
*Samuel Yoon,Jongwon Kim,Juyoung Ha,Young Myoung Ko*

Main category: cs.LG

TL;DR: MOMEMTO proposes a patch-based memory-augmented foundation model for time series anomaly detection to reduce over-generalization and enable cross-domain, multi-dataset fine-tuning, showing improved AUC and VUS across 23 datasets, especially in few-shot settings.


<details>
  <summary>Details</summary>
Motivation: Reconstruction-based anomaly detectors often over-generalize, reconstructing unseen anomalies. Memory-augmented approaches store normal-pattern prototypes but suffer high costs and have not been well integrated with time series foundation models. There is a need for efficient, cross-domain, few-shot capable solutions.

Method: Introduce MOMEMTO, a TFM with a patch-based memory module. Memory items are initialized from a pre-trained encoder, organized into patch-level units, and updated via an attention mechanism. The model supports multi-domain training to jointly fine-tune across multiple datasets, enabling a single model to capture normal patterns from different domains.

Result: Empirical evaluation on 23 univariate datasets shows MOMEMTO achieves higher AUC and VUS than baselines. It also enhances the backbone TFM, particularly in few-shot learning scenarios.

Conclusion: A patch-based memory-augmented TFM can mitigate over-generalization in time series anomaly detection while enabling efficient multi-domain training and improved few-shot performance, making MOMEMTO a strong single-model solution for cross-domain anomaly detection.

Abstract: Recently reconstruction-based deep models have been widely used for time
series anomaly detection, but as their capacity and representation capability
increase, these models tend to over-generalize, often reconstructing unseen
anomalies accurately. Prior works have attempted to mitigate this by
incorporating a memory architecture that stores prototypes of normal patterns.
Nevertheless, these approaches suffer from high training costs and have yet to
be effectively integrated with time series foundation models (TFMs). To address
these challenges, we propose \textbf{MOMEMTO}, a TFM for anomaly detection,
enhanced with a patch-based memory module to mitigate over-generalization. The
memory module is designed to capture representative normal patterns from
multiple domains and enables a single model to be jointly fine-tuned across
multiple datasets through a multi-domain training strategy. MOMEMTO initializes
memory items with latent representations from a pre-trained encoder, organizes
them into patch-level units, and updates them via an attention mechanism. We
evaluate our method using 23 univariate benchmark datasets. Experimental
results demonstrate that MOMEMTO, as a single model, achieves higher scores on
AUC and VUS metrics compared to baseline methods, and further enhances the
performance of its backbone TFM, particularly in few-shot learning scenarios.

</details>


### [312] [Diagonal Linear Networks and the Lasso Regularization Path](https://arxiv.org/abs/2509.18766)
*Raphaël Berthier*

Main category: cs.LG

TL;DR: Training diagonal linear networks mirrors the lasso path: time acts as inverse regularization parameter; exact under a monotone lasso path, otherwise approximate; supported by theory and simulations; with small initialization, converges to the min L1-norm predictor among minimizers.


<details>
  <summary>Details</summary>
Motivation: To rigorously understand implicit regularization in diagonal networks and relate it to the well-studied sparsity-inducing regularizer (lasso). This clarifies how training dynamics select among minimizers.

Method: Analyze the full training trajectory of diagonal linear networks and establish a connection to the lasso regularization path. The authors introduce a monotonicity assumption on the lasso path to obtain exact results and supplement with simulations to illustrate both exact and approximate regimes.

Result: The training from a small initialization converges to the linear predictor with minimal L1-norm among minimizers of the training loss. The full training trajectory closely follows the lasso regularization path, with training time acting as an inverse regularization parameter. The connection is exact under a monotone path and approximately correct in general.

Conclusion: This work deepens the understanding of implicit regularization in diagonal networks by giving a path-following interpretation of training dynamics. It identifies conditions under which the training process aligns with L1 regularization and suggests practical implications for leveraging path-based insights in model design and training.

Abstract: Diagonal linear networks are neural networks with linear activation and
diagonal weight matrices. Their theoretical interest is that their implicit
regularization can be rigorously analyzed: from a small initialization, the
training of diagonal linear networks converges to the linear predictor with
minimal 1-norm among minimizers of the training loss. In this paper, we deepen
this analysis showing that the full training trajectory of diagonal linear
networks is closely related to the lasso regularization path. In this
connection, the training time plays the role of an inverse regularization
parameter. Both rigorous results and simulations are provided to illustrate
this conclusion. Under a monotonicity assumption on the lasso regularization
path, the connection is exact while in the general case, we show an approximate
connection.

</details>


### [313] [Probabilistic Machine Learning for Uncertainty-Aware Diagnosis of Industrial Systems](https://arxiv.org/abs/2509.18810)
*Arman Mohammadi,Mattias Krysander,Daniel Jung,Erik Frisk*

Main category: cs.LG

TL;DR: Ensemble probabilistic learning is proposed to quantify uncertainty and boost reliability in data-driven, consistency-based fault diagnosis.


<details>
  <summary>Details</summary>
Motivation: To address the lack of calibrated confidence in deep neural fault models and reduce false alarms in safety-critical diagnostics.

Method: An ensemble probabilistic machine learning framework is integrated into a data-driven diagnostic pipeline to quantify predictive uncertainty and automate confidence-based decisions.

Result: Across multiple case studies with ablation and comparative analyses, the approach yields consistent improvements in diagnostic metrics.

Conclusion: Incorporating uncertainty quantification via ensemble probabilistic methods enhances reliability and decision-making in fault diagnosis systems.

Abstract: Deep neural networks has been increasingly applied in fault diagnostics,
where it uses historical data
  to capture systems behavior, bypassing the need for high-fidelity physical
models.
  However, despite their competence in prediction tasks, these models often
struggle with
  the evaluation of their confidence. This matter is particularly
  important in consistency-based diagnosis where decision logic is highly
sensitive to false alarms.
  To address this challenge, this work presents a diagnostic framework that
uses
  ensemble probabilistic machine learning to
  improve diagnostic characteristics of data driven consistency based diagnosis
  by quantifying and automating the prediction uncertainty.
  The proposed method is evaluated across several case studies using both
ablation
  and comparative analyses, showing consistent improvements across a range of
diagnostic metrics.

</details>


### [314] [Training-Free Data Assimilation with GenCast](https://arxiv.org/abs/2509.18811)
*Thomas Savary,François Rozet,Gilles Louppe*

Main category: cs.LG

TL;DR: Lightweight data assimilation using pre-trained diffusion models via particle filters; no extra training required; demonstrated on GenCast for global weather forecasts.


<details>
  <summary>Details</summary>
Motivation: To estimate dynamical system states from noisy observations efficiently by leveraging diffusion-model priors without retraining.

Method: Integrate particle filter framework with diffusion models pre-trained to emulate dynamics; use existing diffusion model forecasts as proposal or prior within filtering; apply to GenCast example; minimal additional training.

Result:  Shows feasibility and practicality of the approach; demonstration on GenCast indicates potential for improved assimilation with pre-trained priors.

Conclusion: The approach offers a general, training-free data assimilation pathway leveraging diffusion-models; applicable across meteorology, oceanography, robotics; invites further empirical validation and extension.

Abstract: Data assimilation is widely used in many disciplines such as meteorology,
oceanography, and robotics to estimate the state of a dynamical system from
noisy observations. In this work, we propose a lightweight and general method
to perform data assimilation using diffusion models pre-trained for emulating
dynamical systems. Our method builds on particle filters, a class of data
assimilation algorithms, and does not require any further training. As a
guiding example throughout this work, we illustrate our methodology on GenCast,
a diffusion-based model that generates global ensemble weather forecasts.

</details>


### [315] [Graph-based Clustering Revisited: A Relaxation of Kernel $k$-Means Perspective](https://arxiv.org/abs/2509.18826)
*Wenlong Lyu,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.LG

TL;DR: The paper critiques broad relaxations in graph-based clustering (spectral clustering, NMF, doubly stochastic normalization) and introduces LoRD, which relaxes only the orthonormal constraint to yield probabilistic clustering, and its extension B-LoRD with block-diagonal regularization. It shows orthogonality implies block diagonality under doubly stochastic constraints, and provides a linear convex reformulation via a class probability parameter. It proves gradient Lipschitz continuity, proposes a globally convergent projected gradient descent, and validates the approach with experiments; code is released at GitHub.


<details>
  <summary>Details</summary>
Motivation: Current graph-based clustering methods often relax multiple structural constraints (low-rank, nonnegativity, doubly stochastic, orthonormal) to achieve numerical feasibility. This broad relaxation can degrade clustering quality. The authors aim to preserve these constraints where feasible by relaxing only the orthonormality, thereby enabling probabilistic clustering that better respects the underlying data structure.

Method: 1) Propose LoRD: a low-rank, doubly stochastic clustering model that relaxes only the orthonormal constraint to obtain probabilistic cluster assignments. 2) Theoretically show the equivalence between orthogonality and block diagonality under the doubly stochastic constraint. 3) Introduce Block diagonal regularization into LoRD (B-LoRD) by maximizing the Frobenius norm to encourage block structure and improve performance. 4) Convert the non-convex doubly stochastic constraint into a linear convex constraint by introducing a class probability parameter, ensuring numerical solvability. 5) Prove gradient Lipschitz continuity for LoRD and B-LoRD, enabling a globally convergent projected gradient descent optimization algorithm. 6) Validate with extensive experiments and provide publicly available code.

Result: Theoretical contributions establish a connection between orthogonality and block diagonality under doubly stochastic constraints and provide a tractable convex reformulation via class probabilities. The optimization framework yields a globally convergent algorithm, and empirical results demonstrate effectiveness of LoRD and B-LoRD over existing relaxations in graph-based clustering.

Conclusion: LoRD (and its block-regularized variant B-LoRD) offer a principled alternative to existing relaxations by preserving key constraints and leveraging block structure to improve clustering. The approach is supported by theoretical guarantees (gradient Lipschitz continuity and convergence) and empirical validation, with code released for reproducibility.

Abstract: The well-known graph-based clustering methods, including spectral clustering,
symmetric non-negative matrix factorization, and doubly stochastic
normalization, can be viewed as relaxations of the kernel $k$-means approach.
However, we posit that these methods excessively relax their inherent low-rank,
nonnegative, doubly stochastic, and orthonormal constraints to ensure numerical
feasibility, potentially limiting their clustering efficacy. In this paper,
guided by our theoretical analyses, we propose \textbf{Lo}w-\textbf{R}ank
\textbf{D}oubly stochastic clustering (\textbf{LoRD}), a model that only
relaxes the orthonormal constraint to derive a probabilistic clustering
results. Furthermore, we theoretically establish the equivalence between
orthogonality and block diagonality under the doubly stochastic constraint. By
integrating \textbf{B}lock diagonal regularization into LoRD, expressed as the
maximization of the Frobenius norm, we propose \textbf{B-LoRD}, which further
enhances the clustering performance. To ensure numerical solvability, we
transform the non-convex doubly stochastic constraint into a linear convex
constraint through the introduction of a class probability parameter. We
further theoretically demonstrate the gradient Lipschitz continuity of our LoRD
and B-LoRD enables the proposal of a globally convergent projected gradient
descent algorithm for their optimization. Extensive experiments validate the
effectiveness of our approaches. The code is publicly available at
https://github.com/lwl-learning/LoRD.

</details>


### [316] [Shared-Weights Extender and Gradient Voting for Neural Network Expansion](https://arxiv.org/abs/2509.18842)
*Nikolas Chatzis,Ioannis Kordonis,Manos Theodosis,Petros Maragos*

Main category: cs.LG

TL;DR: Presents SWE and SVoD to expand networks during training while keeping new neurons active; achieved by coupling new neurons with existing ones and gradient-based neuron allocation, showing improved performance across four datasets.


<details>
  <summary>Details</summary>
Motivation: Expand neural capacity during training without retraining from scratch, but new neurons often become inactive and contribute little to performance.

Method: SWE (Shared-Weights Extender) couples new neurons with existing ones to ensure smooth integration; SVoD (Steepest Voting Distributor) gradient-based method to allocate neurons across layers during expansion.

Result: Empirical benchmarks on four datasets show reduced neuron inactivity and improved performance compared to other expansion methods and baselines.

Conclusion: SWE and SVoD jointly enable effective, activity-preserving network expansion, enabling capacity growth with better performance.

Abstract: Expanding neural networks during training is a promising way to augment
capacity without retraining larger models from scratch. However, newly added
neurons often fail to adjust to a trained network and become inactive,
providing no contribution to capacity growth. We propose the Shared-Weights
Extender (SWE), a novel method explicitly designed to prevent inactivity of new
neurons by coupling them with existing ones for smooth integration. In
parallel, we introduce the Steepest Voting Distributor (SVoD), a gradient-based
method for allocating neurons across layers during deep network expansion. Our
extensive benchmarking on four datasets shows that our method can effectively
suppress neuron inactivity and achieve better performance compared to other
expanding methods and baselines.

</details>


### [317] [NGRPO: Negative-enhanced Group Relative Policy Optimization](https://arxiv.org/abs/2509.18851)
*Gongrui Nan,Siye Chen,Jing Huang,Mengyu Lu,Dexun Wang,Chunmei Xie,Weiqi Xiong,Xianzhou Zeng,Qixuan Zhou,Yadong Li,Xingzhong Xu*

Main category: cs.LG

TL;DR: NGRPO tackles the zero-gradient issue in Group Relative Policy Optimization (GRPO) by converting homogeneous error signals into learning signals through Advantage Calibration and Asymmetric Clipping, yielding better mathematical reasoning in LLMs on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current RLVR methods like GRPO fail to learn from groups where all responses are uniformly correct or incorrect, causing zero or uninformative advantages and stalled learning; a robust method is needed to leverage homogeneous error signals.

Method: Introduce NGRPO with two components: (1) Advantage Calibration, which posits a virtual max-reward sample to adjust the reward distribution within a group, making nonzero advantages for homogeneous incorrect samples; (2) Asymmetric Clipping, which relaxes updates for positive samples while applying stricter constraints on negative samples to stabilize exploration from the calibrated advantages.

Result: Empirical evaluation on Qwen2.5-Math-7B shows NGRPO outperforms PPO, GRPO, DAPO, and PSR-NSR on math benchmarks MATH500, AMC23, and AIME2025, indicating stronger learning from homogeneous errors and improved mathematical reasoning.

Conclusion: NGRPO provides a robust pathway to learn from homogeneous error signals in RLVR, delivering stable and significant gains in mathematical reasoning; the work includes publicly available code.

Abstract: RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)
across various tasks. However, GRPO, a representative RLVR algorithm, suffers
from a critical limitation: when all responses within a group are either
entirely correct or entirely incorrect, the model fails to learn from these
homogeneous responses. This is particularly problematic for homogeneously
incorrect groups, where GRPO's advantage function yields a value of zero,
leading to null gradients and the loss of valuable learning signals. To
overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy
Optimization), an algorithm designed to convert homogeneous errors into robust
learning signals. First, NGRPO introduces Advantage Calibration. This mechanism
hypothesizes the existence of a virtual maximum-reward sample during advantage
calculation, thereby altering the mean and variance of rewards within a group
and ensuring that the advantages for homogeneously incorrect samples are no
longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the
update magnitude for positive samples while imposing stricter constraints on
that of negative samples. This serves to stabilize the exploration pressure
introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B
demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,
DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and
AIME2025. These results validate NGRPO's ability to learn from homogeneous
errors, leading to stable and substantial improvements in mathematical
reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.

</details>


### [318] [Exploring Heterophily in Graph-level Tasks](https://arxiv.org/abs/2509.18893)
*Qinhan Hou,Yilun Zheng,Xichun Zhang,Sitao Luan,Jing Tang*

Main category: cs.LG

TL;DR: First theoretical and empirical study of heterophily in graph-level learning; motif-based tasks require mixed-frequency dynamics; frequency-adaptive GNNs outperform frequency-dominated ones; provides architectural guidance.


<details>
  <summary>Details</summary>
Motivation: Heterophily's impact on graph-level tasks is unclear and understudied. This work aims to understand how graph-level labeling schemes—especially motif-based, local-structure labeling—behave under heterophily, addressing a gap beyond node-level analyses.

Method: Propose a taxonomy of graph-level labeling schemes and focus on motif-based local-structure labeling. Use energy-based gradient flow analysis to derive how motif objectives interact with spectral components. Validate through synthetic datasets with controlled heterophily and real-world molecular property prediction; compare frequency-adaptive vs frequency-dominated models.

Result: The theory shows motif objectives require mixed-frequency dynamics, not dominated by a single global frequency. Empirical results show frequency-adaptive models outperform frequency-dominated models under heterophily on both synthetic data and real molecular tasks.

Conclusion: Establishes a new theoretical understanding of heterophily in graph-level learning and provides architectural guidance for designing effective GNNs for motif-based graph-level tasks under heterophily.

Abstract: While heterophily has been widely studied in node-level tasks, its impact on
graph-level tasks remains unclear. We present the first analysis of heterophily
in graph-level learning, combining theoretical insights with empirical
validation. We first introduce a taxonomy of graph-level labeling schemes, and
focus on motif-based tasks within local structure labeling, which is a popular
labeling scheme. Using energy-based gradient flow analysis, we reveal a key
insight: unlike frequency-dominated regimes in node-level tasks, motif
detection requires mixed-frequency dynamics to remain flexible across multiple
spectral components. Our theory shows that motif objectives are inherently
misaligned with global frequency dominance, demanding distinct architectural
considerations. Experiments on synthetic datasets with controlled heterophily
and real-world molecular property prediction support our findings, showing that
frequency-adaptive model outperform frequency-dominated models. This work
establishes a new theoretical understanding of heterophily in graph-level
learning and offers guidance for designing effective GNN architectures.

</details>


### [319] [Enhancing the Effectiveness and Durability of Backdoor Attacks in Federated Learning through Maximizing Task Distinction](https://arxiv.org/abs/2509.18904)
*Zhaoxin Wang,Handing Wang,Cong Tian,Yaochu Jin*

Main category: cs.LG

TL;DR: A dynamic, min–max optimized backdoor trigger decouples the backdoor from the main task in federated learning, enhancing persistence under defenses.


<details>
  <summary>Details</summary>
Motivation: Fixed-pattern backdoors are vulnerable to dilution by honest updates and defenses; decoupling the backdoor from main tasks could create more robust, transferable attacks in FL.

Method: A bilevel optimization: inner layer maximizes the performance gap between poisoned and benign samples to isolate the backdoor's influence, while the outer loop adaptively injects triggers into local models; extends to CV and NLP; compares against six backdoor methods under six defense strategies.

Result: Demonstrates strong attack performance across tasks and defenses; can be integrated with existing backdoor techniques; shows persistence under standard FL defenses.

Conclusion: Decoupling the backdoor task via adaptive min–max triggers yields more robust, persistent backdoors in federated settings, highlighting a broader vulnerability in FL and the need for defense methods that address dynamic, task-decoupled backdoors.

Abstract: Federated learning allows multiple participants to collaboratively train a
central model without sharing their private data. However, this distributed
nature also exposes new attack surfaces. In particular, backdoor attacks allow
attackers to implant malicious behaviors into the global model while
maintaining high accuracy on benign inputs. Existing attacks usually rely on
fixed patterns or adversarial perturbations as triggers, which tightly couple
the main and backdoor tasks. This coupling makes them vulnerable to dilution by
honest updates and limits their persistence under federated defenses. In this
work, we propose an approach to decouple the backdoor task from the main task
by dynamically optimizing the backdoor trigger within a min-max framework. The
inner layer maximizes the performance gap between poisoned and benign samples,
ensuring that the contributions of benign users have minimal impact on the
backdoor. The outer process injects the adaptive triggers into the local model.
We evaluate our method on both computer vision and natural language tasks, and
compare it with six backdoor attack methods under six defense algorithms.
Experimental results show that our method achieves good attack performance and
can be easily integrated into existing backdoor attack techniques.

</details>


### [320] [Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning](https://arxiv.org/abs/2509.18930)
*Alex Schutz,Victor-Alexandru Darvariu,Efimia Panagiotaki,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

TL;DR: GNARL recasts neural algorithmic reasoning as a Markov decision process and RL, introducing a GNARL framework to translate NAR to RL for graph problems, achieving strong results including NP-hard problems and applicability without an expert algorithm.


<details>
  <summary>Details</summary>
Motivation: To overcome NAR's limitations: inability to guarantee valid solutions without post-processing, inability to accommodate multiple correct solutions, poor performance on NP-hard problems, and inapplicability to problems without known strong algorithms.

Method: Formulate learning trajectories as an MDP; develop GNARL framework to translate problem formulations from NAR to RL; propose a graph-oriented learning architecture compatible with various graph problems.

Result: High graph-accuracy on CLRS-30 problems; performance on NP-hard tasks matching or exceeding narrow NAR approaches; applicability even when no expert algorithm exists.

Conclusion: RL-based learning of algorithmic trajectories generalizes NAR, enabling robust solution construction for graph problems, including NP-hard instances and problems lacking known algorithms.

Abstract: Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks
to execute classic algorithms by supervised learning. Despite its successes,
important limitations remain: inability to construct valid solutions without
post-processing and to reason about multiple correct ones, poor performance on
combinatorial NP-hard problems, and inapplicability to problems for which
strong algorithms are not yet known. To address these limitations, we reframe
the problem of learning algorithm trajectories as a Markov Decision Process,
which imposes structure on the solution construction procedure and unlocks the
powerful tools of imitation and reinforcement learning (RL). We propose the
GNARL framework, encompassing the methodology to translate problem formulations
from NAR to RL and a learning architecture suitable for a wide range of
graph-based problems. We achieve very high graph accuracy results on several
CLRS-30 problems, performance matching or exceeding much narrower NAR
approaches for NP-hard problems and, remarkably, applicability even when
lacking an expert algorithm.

</details>


### [321] [Towards Privacy-Aware Bayesian Networks: A Credal Approach](https://arxiv.org/abs/2509.18949)
*Niccolò Rocchi,Fabio Stella,Cassio de Campos*

Main category: cs.LG

TL;DR: Credal networks offer privacy-aware probabilistic models by masking learned Bayesian networks instead of injecting noise, enabling meaningful inferences with tunable privacy-utility trade-offs; attacks are mitigated by hiding key learning information.


<details>
  <summary>Details</summary>
Motivation: Publicly released Bayesian networks risk privacy breaches via tracing attacks; standard privatization via noise degrades utility; need a method that preserves utility while mitigating attacks.

Method: Adapt tracing attacks to credal networks; show CNs can mask the underlying BN; identify which learning information must be concealed to prevent BN recovery; conduct numerical experiments varying CN hyperparameters to analyze privacy-utility trade-offs.

Result: CNs provide principled, practical privacy-aware models; they reduce success probability of tracing attacks while maintaining meaningful inferences; privacy gains tunable through CN hyperparameters; demonstrate feasibility.

Conclusion: CNs offer a viable alternative to noisy parameter perturbations for privacy in probabilistic graphical models; they balance privacy and utility; further work to optimize concealment strategies and evaluate broader attack models.

Abstract: Bayesian networks (BN) are probabilistic graphical models that enable
efficient knowledge representation and inference. These have proven effective
across diverse domains, including healthcare, bioinformatics and economics. The
structure and parameters of a BN can be obtained by domain experts or directly
learned from available data. However, as privacy concerns escalate, it becomes
increasingly critical for publicly released models to safeguard sensitive
information in training data. Typically, released models do not prioritize
privacy by design. In particular, tracing attacks from adversaries can combine
the released BN with auxiliary data to determine whether specific individuals
belong to the data from which the BN was learned. State-of-the-art protection
tecniques involve introducing noise into the learned parameters. While this
offers robust protection against tracing attacks, it significantly impacts the
model's utility, in terms of both the significance and accuracy of the
resulting inferences. Hence, high privacy may be attained at the cost of
releasing a possibly ineffective model. This paper introduces credal networks
(CN) as a novel solution for balancing the model's privacy and utility. After
adapting the notion of tracing attacks, we demonstrate that a CN enables the
masking of the learned BN, thereby reducing the probability of successful
attacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve
meaningful inferences while safeguarding privacy. Moreover, we identify key
learning information that must be concealed to prevent attackers from
recovering the underlying BN. Finally, we conduct a set of numerical
experiments to analyze how privacy gains can be modulated by tuning the CN
hyperparameters. Our results confirm that CNs provide a principled, practical,
and effective approach towards the development of privacy-aware probabilistic
graphical models.

</details>


### [322] [Lift What You Can: Green Online Learning with Heterogeneous Ensembles](https://arxiv.org/abs/2509.18962)
*Kirsten Köbschall,Sebastian Buschjäger,Raphael Fischer,Lisa Hartung,Stefan Kramer*

Main category: cs.LG

TL;DR: HEROS (heterogeneous online ensembles) introduces a green online learning framework for stream mining. It uses an MDP-based decision process to balance predictive performance with sustainability constraints and a novel zeta-policy that trains near-optimal models under reduced costs. Empirical evaluation on 11 benchmark datasets shows near-optimal performance with substantially lower resource usage, often outperforming competitors in accuracy while being more resource-friendly.


<details>
  <summary>Details</summary>
Motivation: Sustainability concerns in online ensemble learning: existing methods prioritise predictive accuracy and ignore computational costs, leading to inefficient, energy-intensive models. The work aims to enable green online learning.

Method: Initialize a pool of models with diverse hyperparameters. At each training step, select a subset of models to train under resource constraints. Model selection policies are derived from a Markov decision process; the zeta-policy focuses on training near-optimal models at reduced costs. The approach includes theoretical analysis (near-optimal guarantees) and empirical evaluation.

Result: Theoretical: the zeta-policy achieves near-optimal performance while using fewer resources than the best policy. Empirical: across 11 datasets, it delivers competitive accuracy and improved resource efficiency, sometimes surpassing competitors while being more resource-friendly.

Conclusion: HEROS advances green online learning by explicitly accounting for computational costs in ensemble training; the zeta-policy offers a practical, resource-aware path to near-optimal performance in streaming settings, with strong empirical validation.

Abstract: Ensemble methods for stream mining necessitate managing multiple models and
updating them as data distributions evolve. Considering the calls for more
sustainability, established methods are however not sufficiently considerate of
ensemble members' computational expenses and instead overly focus on predictive
capabilities. To address these challenges and enable green online learning, we
propose heterogeneous online ensembles (HEROS). For every training step, HEROS
chooses a subset of models from a pool of models initialized with diverse
hyperparameter choices under resource constraints to train. We introduce a
Markov decision process to theoretically capture the trade-offs between
predictive performance and sustainability constraints. Based on this framework,
we present different policies for choosing which models to train on incoming
data. Most notably, we propose the novel $\zeta$-policy, which focuses on
training near-optimal models at reduced costs. Using a stochastic model, we
theoretically prove that our $\zeta$-policy achieves near optimal performance
while using fewer resources compared to the best performing policy. In our
experiments across 11 benchmark datasets, we find empiric evidence that our
$\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating
highly accurate performance, in some cases even outperforming competitors, and
simultaneously being much more resource-friendly.

</details>


### [323] [Central Limit Theorems for Asynchronous Averaged Q-Learning](https://arxiv.org/abs/2509.18964)
*Xingtu Liu*

Main category: cs.LG

TL;DR: Presents finite-sample and functional CLTs for Polyak-Ruppert averaged Q-learning with asynchronous updates, quantifying distributional behavior via Wasserstein distance and Brownian-motion limits.


<details>
  <summary>Details</summary>
Motivation: To provide practical distributional guarantees for averaged Q-learning under asynchrony, bridging finite-sample behavior and theoretical asymptotics, and to understand how iterations, state-action space size, discount factor, and exploration quality affect variability.

Method: Derives a non-asymptotic central limit theorem in Wasserstein distance with explicit dependence on number of iterations, state-action space size, discount factor, and exploration quality; also proves a functional central limit theorem showing the partial-sum process converges weakly to a Brownian motion.

Result: Explicit non-asymptotic CLT bounds in Wasserstein distance and a functional CLT to Brownian motion for the averaged Q-learning estimates under asynchronous updates.

Conclusion: The work provides both finite-sample distributional guarantees and a diffusion (Brownian) limit for averaged Q-learning in asynchronous settings, enabling uncertainty quantification and insights into how system parameters influence fluctuations.

Abstract: This paper establishes central limit theorems for Polyak-Ruppert averaged
Q-learning under asynchronous updates. We present a non-asymptotic central
limit theorem, where the convergence rate in Wasserstein distance explicitly
reflects the dependence on the number of iterations, state-action space size,
the discount factor, and the quality of exploration. In addition, we derive a
functional central limit theorem, showing that the partial-sum process
converges weakly to a Brownian motion.

</details>


### [324] [Otters: An Energy-Efficient SpikingTransformer via Optical Time-to-First-Spike Encoding](https://arxiv.org/abs/2509.18968)
*Zhanglu Yan,Jiayi Mao,Qianhui Liu,Fanfan Li,Gang Pan,Tao Luo,Bowen Zhu,Weng-Fai Wong*

Main category: cs.LG

TL;DR: Repurpose device physics of optoelectronic synapses to perform TTFS computation directly, avoiding energy-heavy digital decay and weight multiplications, achieving energy-efficient SNNs with strong accuracy; introduces Otters hardware and a NN-to-SNN conversion for transformers, enabling high GLUE performance with 1.77x energy improvement.


<details>
  <summary>Details</summary>
Motivation: TTFS SNNs offer sparsity and potential energy savings, but practical implementations incur costly temporal decay computations and weight multiplications. This work seeks to harness intrinsic device physics to perform TTFS computation, and to bridge such neuromorphic primitives with modern architectures (transformers) via a conversion algorithm, to deliver energy-efficient, high-performance inference.

Method: Fabricate indium oxide optoelectronic synapses whose natural signal decay implements the TTFS temporal function. Treat the device’s analog output as the fused product of synaptic weight and temporal decay, eliminating digital multiply-accumulate. Develop a quantized NN-to-SNN conversion algorithm to map transformer-like architectures to the Otters TTFS SNN. Evaluate on seven GLUE datasets with energy measurements from a 22nm process, and provide a hardware-software co-design pipeline with open-source code.

Result: Achieves state-of-the-art accuracy across seven GLUE benchmarks and demonstrates a 1.77x energy efficiency improvement over prior leading SNNs. Provides comprehensive energy cost analysis (compute, data movement, memory access) and open-source tools/data.

Conclusion: Demonstrates a new paradigm for energy-efficient SNNs by directly translating device physics into computational primitives, enabling high-performance, low-energy inference in modern architectures through hardware-software co-design and NN-to-SNN conversion; opens doors for broader adoption of TTFS-based neuromorphic computation.

Abstract: Spiking neural networks (SNNs) promise high energy efficiency, particularly
with time-to-first-spike (TTFS) encoding, which maximizes sparsity by emitting
at most one spike per neuron. However, such energy advantage is often
unrealized because inference requires evaluating a temporal decay function and
subsequent multiplication with the synaptic weights. This paper challenges this
costly approach by repurposing a physical hardware `bug', namely, the natural
signal decay in optoelectronic devices, as the core computation of TTFS. We
fabricated a custom indium oxide optoelectronic synapse, showing how its
natural physical decay directly implements the required temporal function. By
treating the device's analog output as the fused product of the synaptic weight
and temporal decay, optoelectronic synaptic TTFS (named Otters) eliminates
these expensive digital operations. To use the Otters paradigm in complex
architectures like the transformer, which are challenging to train directly due
to the sparsity issue, we introduce a novel quantized neural network-to-SNN
conversion algorithm. This complete hardware-software co-design enables our
model to achieve state-of-the-art accuracy across seven GLUE benchmark datasets
and demonstrates a 1.77$\times$ improvement in energy efficiency over previous
leading SNNs, based on a comprehensive analysis of compute, data movement, and
memory access costs using energy measurements from a commercial 22nm process.
Our work thus establishes a new paradigm for energy-efficient SNNs, translating
fundamental device physics directly into powerful computational primitives. All
codes and data are open source.

</details>


### [325] [Learning From Simulators: A Theory of Simulation-Grounded Learning](https://arxiv.org/abs/2509.18990)
*Carson Dudley,Marisa Eisenberg*

Main category: cs.LG

TL;DR: SGNNs learn from synthetic mechanistic simulations, interpreting predictions via amortized Bayesian inference with a simulation prior. They converge to the Bayes-optimal predictor, offer generalization bounds under misspecification, and enable mechanistic, posterior-consistent explanations, with robust empirical performance in data-limited settings.


<details>
  <summary>Details</summary>
Motivation: Address the lack of formal theoretical grounding for SGNNs by providing a complete probabilistic foundation, error bounds under model misspecification, and interpretable attributions to simulated mechanisms.

Method: Theoretical analysis establishing SGNNs as amortized Bayesian inference under a simulation prior; proofs of convergence to Bayes-optimal predictor; derivation of generalization bounds under misspecification; formulation of mechanistic interpretability via attribution to simulated mechanisms; empirical validation through numerical experiments comparing to classical tools (e.g., AIC) and showing latent-parameter recovery and robustness.

Result: SGNNs are proven to converge to Bayes-optimal predictions under a simulation prior; generalization bounds hold under misspecification; they can learn unobservable quantities not accessible to empirical methods; they yield mechanistic attributions that are posterior-consistent; empirically they recover latent parameters, show robustness to mismatch, and beat classical methods (half the AIC error in a model-selection task).

Conclusion: SGNNs provide a principled, practical framework for scientific prediction in data-limited regimes, offering theoretical grounding, robustness to model misspecification, and interpretable, mechanism-based explanations.

Abstract: Simulation-Grounded Neural Networks (SGNNs) are predictive models trained
entirely on synthetic data from mechanistic simulations. They have achieved
state-of-the-art performance in domains where real-world labels are limited or
unobserved, but lack a formal underpinning.
  We present the foundational theory of simulation-grounded learning. We show
that SGNNs implement amortized Bayesian inference under a simulation prior and
converge to the Bayes-optimal predictor. We derive generalization bounds under
model misspecification and prove that SGNNs can learn unobservable scientific
quantities that empirical methods provably cannot. We also formalize a novel
form of mechanistic interpretability uniquely enabled by SGNNs: by attributing
predictions to the simulated mechanisms that generated them, SGNNs yield
posterior-consistent, scientifically grounded explanations.
  We provide numerical experiments to validate all theoretical predictions.
SGNNs recover latent parameters, remain robust under mismatch, and outperform
classical tools: in a model selection task, SGNNs achieve half the error of AIC
in distinguishing mechanistic dynamics. These results establish SGNNs as a
principled and practical framework for scientific prediction in data-limited
regimes.

</details>


### [326] [CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank Structure](https://arxiv.org/abs/2509.18993)
*Boao Kong,Junzhu Liang,Yuxi Liu,Renjia Deng,Kun Yuan*

Main category: cs.LG

TL;DR: CR-Net: a cross-layer low-rank residual network for efficient LLM pre-training that uses dual-path activations with low-rank differences and activation recomputation to reduce memory while preserving performance, achieving better results than existing low-rank methods across 60M–7B params.


<details>
  <summary>Details</summary>
Motivation: Existing low-rank approaches trade performance, add computation, and offer limited activation memory savings; need a parameter-efficient, memory-friendly solution for LLM pre-training.

Method: Propose CR-Net with a dual-path architecture that reconstructs layer activations from previous-layer outputs via low-rank residuals; employ an activation recomputation strategy to minimize memory; designed for cross-layer reuse of activation residuals.

Result: Pre-training experiments from 60M to 7B parameters show CR-Net consistently outperforms state-of-the-art low-rank methods while using fewer compute resources and less memory.

Conclusion: CR-Net successfully preserves high-rank information with minimal parameters, achieving efficient LLM pre-training and enabling stronger performance with lower resource demands across scales.

Abstract: Low-rank architectures have become increasingly important for efficient large
language model (LLM) pre-training, providing substantial reductions in both
parameter complexity and memory/computational demands. Despite these
advantages, current low-rank methods face three critical shortcomings: (1)
compromised model performance, (2) considerable computational overhead, and (3)
limited activation memory savings. To address these limitations, we propose
Cross-layer Low-Rank residual Network (CR-Net), an innovative
parameter-efficient framework inspired by our discovery that inter-layer
activation residuals possess low-rank properties. CR-Net implements this
insight through a dual-path architecture that efficiently reconstructs layer
activations by combining previous-layer outputs with their low-rank
differences, thereby maintaining high-rank information with minimal parameters.
We further develop a specialized activation recomputation strategy tailored for
CR-Net that dramatically reduces memory requirements. Extensive pre-training
experiments across model scales from 60M to 7B parameters demonstrate that
CR-Net consistently outperforms state-of-the-art low-rank frameworks while
requiring fewer computational resources and less memory.

</details>


### [327] [Theoretical Foundations of Representation Learning using Unlabeled Data: Statistics and Optimization](https://arxiv.org/abs/2509.18997)
*Pascal Esser,Maximilian Fleissner,Debarghya Ghoshdastidar*

Main category: cs.LG

TL;DR: Survey of recent theoretical advances in unsupervised representation learning from unlabeled data, outlining gaps in classical theories and summarizing the authors' contributions to bridging theory with modern self-supervised approaches.


<details>
  <summary>Details</summary>
Motivation: To understand why self-supervised and denoising/masked autoencoder representations work so well and to explain emergent behaviors by combining statistics and optimization.

Method: Literature review and synthesis of theoretical developments in unsupervised representation learning, with a focus on self-supervised and denoising-based methods; articulation of gaps and the authors’ contributions.

Result: A coherent overview and roadmap that connects existing theory to practice, highlighting open questions and positioning the authors' contributions within this landscape.

Conclusion: Bridging classical statistical optimization theories with modern deep learning requires new mathematical tools; the paper advocates this direction and outlines future research opportunities.

Abstract: Representation learning from unlabeled data has been extensively studied in
statistics, data science and signal processing with a rich literature on
techniques for dimension reduction, compression, multi-dimensional scaling
among others. However, current deep learning models use new principles for
unsupervised representation learning that cannot be easily analyzed using
classical theories. For example, visual foundation models have found tremendous
success using self-supervision or denoising/masked autoencoders, which
effectively learn representations from massive amounts of unlabeled data.
However, it remains difficult to characterize the representations learned by
these models and to explain why they perform well for diverse prediction tasks
or show emergent behavior. To answer these questions, one needs to combine
mathematical tools from statistics and optimization. This paper provides an
overview of recent theoretical advances in representation learning from
unlabeled data and mentions our contributions in this direction.

</details>


### [328] [Fully Learnable Neural Reward Machines](https://arxiv.org/abs/2509.19017)
*Hazem Dewidar,Elena Umili*

Main category: cs.LG

TL;DR: A fully learnable Neural Reward Machine framework that learns symbol grounding and the automaton end-to-end, enabling explainable, DRL-ready handling of non-Markovian tasks and outperforming RNN-based methods.


<details>
  <summary>Details</summary>
Motivation: Non-Markovian RL requires leveraging temporally extended objectives; symbolic formalisms provide structure but typically rely on predefined symbol grounding or prior task knowledge, limiting applicability.

Method: Introduce Fully Learnable Reward Machines (FLNRM) that jointly learn the Symbol Grounding (mapping raw observations to symbols) and the automaton, integrating with Deep RL; removes dependence on prior knowledge, offering explainable automata.

Result: Empirically, FLNRM integrated with DRL outperforms prior approaches based on Recurrent Neural Networks.

Conclusion: FLNRM makes non-Markovian RL more scalable and explainable by learning the symbolic representation and temporal structure directly, aligning with the strengths of DRL.

Abstract: Non-Markovian Reinforcement Learning (RL) tasks present significant
challenges, as agents must reason over entire trajectories of state-action
pairs to make optimal decisions. A common strategy to address this is through
symbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which
provide a structured way to express temporally extended objectives. However,
these approaches often rely on restrictive assumptions -- such as the
availability of a predefined Symbol Grounding (SG) function mapping raw
observations to high-level symbolic representations, or prior knowledge of the
temporal task. In this work, we propose a fully learnable version of Neural
Reward Machines (NRM), which can learn both the SG function and the automaton
end-to-end, removing any reliance on prior knowledge. Our approach is therefore
as easily applicable as classic deep RL (DRL) approaches, while being far more
explainable, because of the finite and compact nature of automata. Furthermore,
we show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL,
our method outperforms previous approaches based on Recurrent Neural Networks
(RNNs).

</details>


### [329] [OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval via Latent Space Alignment](https://arxiv.org/abs/2509.19018)
*Teng Xiao,Zuchao Li,Lefei Zhang*

Main category: cs.LG

TL;DR: OmniBridge is a unified, modular multimodal framework that uses a language-centric approach with latent alignment to handle vision-language understanding, generation, and retrieval within a single architecture, achieving competitive or state-of-the-art results with a two-stage training and latent-space alignment.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLMs often tackle tasks in isolation or require training LLMs from scratch, leading to high computational costs and limited cross-modal generalization. There is a need for a unified framework that reuses pretrained LLMs and aligns latent spaces across modalities.

Method: A language-centric design that reuses pretrained LLMs and adds a lightweight bidirectional latent alignment module. Two-stage decoupled training: (1) supervised fine-tuning and latent space alignment to align LLM behavior with multimodal reasoning, and (2) semantic-guided diffusion training to align cross-modal latent spaces via learnable query embeddings.

Result: OmniBridge achieves competitive or state-of-the-art performance across vision-language understanding, generation, and retrieval benchmarks. Latent space alignment proves effective for unifying multimodal modeling under a shared representation space.

Conclusion: The paper demonstrates the effectiveness of latent space alignment and a unified architecture for multimodal modeling. Code and models are released for reproducibility.

Abstract: Recent advances in multimodal large language models (LLMs) have led to
significant progress in understanding, generation, and retrieval tasks.
However, current solutions often treat these tasks in isolation or require
training LLMs from scratch, resulting in high computational costs and limited
generalization across modalities. In this work, we present OmniBridge, a
unified and modular multimodal framework that supports vision-language
understanding, generation, and retrieval within a unified architecture.
OmniBridge adopts a language-centric design that reuses pretrained LLMs and
introduces a lightweight bidirectional latent alignment module. To address the
challenge of task interference, we propose a two-stage decoupled training
strategy: supervised fine-tuning and latent space alignment for aligning LLM
behavior with multimodal reasoning, and semantic-guided diffusion training to
align cross-modal latent spaces via learnable query embeddings. Extensive
experiments across a wide range of benchmarks demonstrate that OmniBridge
achieves competitive or state-of-the-art performance in all three tasks.
Moreover, our results highlight the effectiveness of latent space alignment for
unifying multimodal modeling under a shared representation space. Code and
models are released at https://github.com/xiao-xt/OmniBridge.

</details>


### [330] [Improving Credit Card Fraud Detection through Transformer-Enhanced GAN Oversampling](https://arxiv.org/abs/2509.19032)
*Kashaf Ul Emaan*

Main category: cs.LG

TL;DR: A Transformer-augmented GAN for fraud data synthesis improves classifier performance on imbalanced credit card fraud data compared to SMOTE/CTGAN/TVAE and baseline classifiers.


<details>
  <summary>Details</summary>
Motivation: Credit card fraud datasets are highly imbalanced with frauds being rare. Traditional oversampling like SMOTE and some generative tabular models (CTGAN/TVAE) struggle to capture high-dimensional dependencies and realistic fraud patterns, limiting detection performance.

Method: A hybrid GAN with a Transformer encoder block to generate realistic fraudulent transactions. The generator learns adversarially while the Transformer captures rich feature interactions via self-attention. Evaluated on the Credit Card Fraud Detection dataset against conventional resampling strategies and classifiers (LR, RF, XGBoost, SVM).

Result: The Transformer-based GAN achieved substantial gains in recall, F1-score, and AUC over baselines and other generative resampling methods, indicating improved ability to model minority class and imbalanced data.

Conclusion: A Transformer-augmented GAN effectively overcomes limitations of existing oversampling and tabular generative models for fraud detection, producing diverse, high-quality minority samples that boost classifier performance in highly imbalanced, high-dimensional tabular data.

Abstract: Detection of credit card fraud is an acute issue of financial security
because transaction datasets are highly lopsided, with fraud cases being only a
drop in the ocean. Balancing datasets using the most popular methods of
traditional oversampling such as the Synthetic Minority Oversampling Technique
(SMOTE) generally create simplistic synthetic samples that are not readily
applicable to complex fraud patterns. Recent industry advances that include
Conditional Tabular Generative Adversarial Networks (CTGAN) and Tabular
Variational Autoencoders (TVAE) have demonstrated increased efficiency in
tabular synthesis, yet all these models still exhibit issues with
high-dimensional dependence modelling. Now we will present our hybrid approach
where we use a Generative Adversarial Network (GAN) with a Transformer encoder
block to produce realistic fraudulent transactions samples. The GAN
architecture allows training realistic generators adversarial, and the
Transformer allows the model to learn rich feature interactions by
self-attention. Such a hybrid strategy overcomes the limitations of SMOTE,
CTGAN, and TVAE by producing a variety of high-quality synthetic minority
classes samples. We test our algorithm on the publicly-available Credit Card
Fraud Detection dataset and compare it to conventional and generative
resampling strategies with a variety of classifiers, such as Logistic
Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and
Support Vector Machine (SVM). Findings indicate that our Transformer-based GAN
shows substantial gains in Recall, F1-score and Area Under the Receiver
Operating Characteristic Curve (AUC), which indicates that it is effective in
overcoming the severe class imbalance inherent in the task of fraud detection.

</details>


### [331] [Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks](https://arxiv.org/abs/2509.19044)
*Yang Li,Chenyu Wang,Tingrui Wang,Yongwei Wang,Haonan Li,Zhunga Liu,Quan Pan*

Main category: cs.LG

TL;DR: Latent diffusion-based black-box attacks (JAD) guided by cross-architecture attention distillation yield transferable, efficient adversarial examples across CNNs and ViTs.


<details>
  <summary>Details</summary>
Motivation: Black-box attacks struggle with limited transferability across architectures and high query costs; a method that generalizes across models and reduces querying is needed.

Method: JAD uses a latent diffusion model guided by attention maps distilled from both CNN and ViT models (joint attention distillation). It targets image regions commonly sensitive across architectures to craft perturbations, enabling architecture-agnostic transferability; the diffusion framework also reduces reliance on iterative queries for faster generation.

Result: Experimentally, JAD shows improved attack generalization, better cross-architecture transferability, and higher generation efficiency compared with existing black-box attack methods.

Conclusion: JAD provides a promising, architecture-agnostic and efficient paradigm for black-box adversarial attacks based on cross-model attention distillation and latent diffusion.

Abstract: Black-box adversarial attacks remain challenging due to limited access to
model internals. Existing methods often depend on specific network
architectures or require numerous queries, resulting in limited
cross-architecture transferability and high query costs. To address these
limitations, we propose JAD, a latent diffusion model framework for black-box
adversarial attacks. JAD generates adversarial examples by leveraging a latent
diffusion model guided by attention maps distilled from both a convolutional
neural network (CNN) and a Vision Transformer (ViT) models. By focusing on
image regions that are commonly sensitive across architectures, this approach
crafts adversarial perturbations that transfer effectively between different
model types. This joint attention distillation strategy enables JAD to be
architecture-agnostic, achieving superior attack generalization across diverse
models. Moreover, the generative nature of the diffusion framework yields high
adversarial sample generation efficiency by reducing reliance on iterative
queries. Experiments demonstrate that JAD offers improved attack
generalization, generation efficiency, and cross-architecture transferability
compared to existing methods, providing a promising and effective paradigm for
black-box adversarial attacks.

</details>


### [332] [Beyond Backpropagation: Exploring Innovative Algorithms for Energy-Efficient Deep Neural Network Training](https://arxiv.org/abs/2509.19063)
*Przemysław Spyra*

Main category: cs.LG

TL;DR: Mono-Forward (MF) BP-free training on MLPs outperforms backpropagation (BP) in accuracy and generalization, while reducing energy use (up to 41%) and training time (up to 34%). FF/CaFo are less efficient or less favorable; MF’s lean design underpins hardware-level efficiency. The work also provides a hardware-aware roadmap for energy-efficient DL using BP-free methods.


<details>
  <summary>Details</summary>
Motivation: To address the escalating computational and energy demands of deep neural networks driven by backpropagation by evaluating BP-free alternatives (FF, CaFo, MF) and quantifying their performance and energy characteristics.

Method: Implemented FF (MLP), CaFo (CNN), and MF (MLP) on their native architectures; compared against BP-trained baselines. Used Optuna for hyperparameter optimization; applied consistent early stopping based on validation performance. Evaluated energy with NVML and carbon footprint with CodeCarbon; conducted hardware-level analysis to explain efficiency differences.

Result: MF consistently achieves higher accuracy than BP on MLPs and shows better generalization by converging to a more favorable validation loss minimum. Energy consumption reduced by up to 41% and training time by up to 34% with MF. FF exhibits architectural inefficiencies, while MF demonstrates a lean design; not all BP-free methods are more memory-efficient.

Conclusion: The results argue that global optimization via BP is not strictly required for state-of-the-art performance; MF offers a compelling, energy-efficient BP-free alternative. The study provides a hardware-aware framework and roadmap for future energy-efficient DL research.

Abstract: The rising computational and energy demands of deep neural networks (DNNs),
driven largely by backpropagation (BP), challenge sustainable AI development.
This paper rigorously investigates three BP-free training methods: the
Forward-Forward (FF), Cascaded-Forward (CaFo), and Mono-Forward (MF)
algorithms, tracing their progression from foundational concepts to a
demonstrably superior solution.
  A robust comparative framework was established: each algorithm was
implemented on its native architecture (MLPs for FF and MF, a CNN for CaFo) and
benchmarked against an equivalent BP-trained model. Hyperparameters were
optimized with Optuna, and consistent early stopping criteria were applied
based on validation performance, ensuring all models were optimally tuned
before comparison.
  Results show that MF not only competes with but consistently surpasses BP in
classification accuracy on its native MLPs. Its superior generalization stems
from converging to a more favorable minimum in the validation loss landscape,
challenging the assumption that global optimization is required for
state-of-the-art results. Measured at the hardware level using the NVIDIA
Management Library (NVML) API, MF reduces energy consumption by up to 41% and
shortens training time by up to 34%, translating to a measurably smaller carbon
footprint as estimated by CodeCarbon.
  Beyond this primary result, we present a hardware-level analysis that
explains the efficiency gains: exposing FF's architectural inefficiencies,
validating MF's computationally lean design, and challenging the assumption
that all BP-free methods are inherently more memory-efficient. By documenting
the evolution from FF's conceptual groundwork to MF's synthesis of accuracy and
sustainability, this work offers a clear, data-driven roadmap for future
energy-efficient deep learning.

</details>


### [333] [Diffusion Bridge Variational Inference for Deep Gaussian Processes](https://arxiv.org/abs/2509.19078)
*Jian Xu,Qibin Zhao,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: DBVI extends DDVI by learning a data-dependent initial distribution for the reverse diffusion in diffusion-based variational inference for Deep Gaussian Processes, improving posterior quality and sample efficiency.


<details>
  <summary>Details</summary>
Motivation: DDVI uses a fixed unconditional start for the reverse diffusion, which poorly matches the true posterior and leads to inefficient inference and slow convergence. In DGPs, addressing the posterior gap is crucial for scalable, high-quality inference, especially with inducing variables.

Method: Introduce Diffusion Bridge Variational Inference (DBVI): replace the fixed start with a learnable, data-dependent initial distribution parameterized by an amortized neural network that operates on inducing inputs. Gradients from the ELBO progressively adapt the initialization. Maintain DDVI's Girsanov-based ELBOs and reverse-time SDEs, reinterpreting the prior via a Doob-bridged diffusion. Derive a tractable training objective and implement for scalable inference in large-scale DGPs.

Result: DBVI consistently outperforms DDVI and other variational baselines in predictive accuracy, convergence speed, and posterior quality across regression, classification, and image reconstruction tasks.

Conclusion: Learned, data-dependent initialization in diffusion-based variational inference reduces the posterior gap and improves inference efficiency for DGPs, while preserving mathematical elegance and scalability.

Abstract: Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian
modeling but pose substantial challenges for posterior inference, especially
over inducing variables. Denoising diffusion variational inference (DDVI)
addresses this by modeling the posterior as a time-reversed diffusion from a
simple Gaussian prior. However, DDVI's fixed unconditional starting
distribution remains far from the complex true posterior, resulting in
inefficient inference trajectories and slow convergence. In this work, we
propose Diffusion Bridge Variational Inference (DBVI), a principled extension
of DDVI that initiates the reverse diffusion from a learnable, data-dependent
initial distribution. This initialization is parameterized via an amortized
neural network and progressively adapted using gradients from the ELBO
objective, reducing the posterior gap and improving sample efficiency. To
enable scalable amortization, we design the network to operate on the inducing
inputs, which serve as structured, low-dimensional summaries of the dataset and
naturally align with the inducing variables' shape. DBVI retains the
mathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time
SDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We
derive a tractable training objective under this formulation and implement DBVI
for scalable inference in large-scale DGPs. Across regression, classification,
and image reconstruction tasks, DBVI consistently outperforms DDVI and other
variational baselines in predictive accuracy, convergence speed, and posterior
quality.

</details>


### [334] [Graph Neural Networks with Similarity-Navigated Probabilistic Feature Copying](https://arxiv.org/abs/2509.19084)
*Asela Hevapathige*

Main category: cs.LG

TL;DR: AxelGNN is a novel GNN architecture inspired by Axelrod's cultural dissemination model, introducing similarity-gated interactions, trait-level copying, and global polarization to tackle oversmoothing, heterophily, and feature granularity; it achieves strong performance across varied graph types.


<details>
  <summary>Details</summary>
Motivation: GNNs struggle with feature oversmoothing in deep networks, heterophilic relationships, and rigid indivisible feature vectors; a unified model that adapts convergence/divergence, allows fine-grained feature aggregation, and preserves node distinctiveness is needed.

Method: Propose AxelGNN with three components: (1) similarity-gated probabilistic interactions that adaptively promote convergence or divergence based on node similarity, (2) trait-level copying for segment-level feature aggregation, enabling fine-grained mixing of features, (3) global polarization to maintain multiple representation clusters; a bistable convergence dynamic enabling both homophily and heterophily handling within a single framework.

Result: Empirical evaluation on node classification and influence estimation benchmarks shows AxelGNN consistently matches or surpasses state-of-the-art GNNs across graphs with varying homophily/heterophily.

Conclusion: A unified, bistable GNN architecture inspired by cultural dissemination can address core limitations of existing GNNs and adapt to diverse graph structures; the framework's components collectively enable robust performance and representational diversity; potential future work includes scalability and interpretability studies.

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable success across
various graph-based tasks. However, they face some fundamental limitations:
feature oversmoothing can cause node representations to become
indistinguishable in deeper networks, they struggle to effectively manage
heterogeneous relationships where connected nodes differ significantly, and
they process entire feature vectors as indivisible units, which limits
flexibility. We seek to address these limitations. We propose AxelGNN, a novel
GNN architecture inspired by Axelrod's cultural dissemination model that
addresses these limitations through a unified framework. AxelGNN incorporates
similarity-gated probabilistic interactions that adaptively promote convergence
or divergence based on node similarity, implements trait-level copying
mechanisms for fine-grained feature aggregation at the segment level, and
maintains global polarization to preserve node distinctiveness across multiple
representation clusters. The model's bistable convergence dynamics naturally
handle both homophilic and heterophilic graphs within a single architecture.
Extensive experiments on node classification and influence estimation
benchmarks demonstrate that AxelGNN consistently outperforms or matches
state-of-the-art GNN methods across diverse graph structures with varying
homophily-heterophily characteristics.

</details>


### [335] [Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer Learning](https://arxiv.org/abs/2509.19098)
*Adrien Prevost,Timothee Mathieu,Odalric-Ambrym Maillard*

Main category: cs.LG

TL;DR: This work extends non-contextual stochastic bandits with transfer learning. It derives a problem-dependent asymptotic regret lower bound incorporating transfer parameters, and introduces KL-UCB-Transfer, an index policy that matches the bound in Gaussian settings. Empirical simulations show substantial regret reduction when source-target distributions are close, compared to a no-prior baseline.


<details>
  <summary>Details</summary>
Motivation: To reduce regret in multi-armed bandits by leveraging prior samples from related source distributions, within a transfer-learning framework. The aim is to quantify how prior information (samples and distance bounds) affects fundamental limits and to design an algorithm that achieves near-optimal performance under this setup.

Method: - Extend Lai-Robbins type asymptotic lower bounds to incorporate transfer parameters: distances between target and source distributions, the number of prior samples, and a known bound L_k on the distributional discrepancy. - Propose KL-UCB-Transfer, a KL-UCB-like index policy that uses the transferred information to guide exploration-exploitation. - Prove that KL-UCB-Transfer matches the new lower bound in the Gaussian case. - Validate via simulations comparing against a no-prior baseline, demonstrating gains when source and target are close.

Result: A problem-dependent asymptotic lower bound for transfer-enabled MABs is established, generalizing Lai-Robbins with transfer terms (d_k, L_k, N'_k). KL-UCB-Transfer is proposed and shown to be asymptotically optimal in Gaussian settings (matches the bound). Simulations indicate significant regret reductions over the no-prior baseline when source-target distributions are sufficiently close.

Conclusion: Transfer information through prior samples and known proximity bounds can improve learning efficiency in MABs. The KL-UCB-Transfer policy effectively exploits this information, achieving near-optimal performance under Gaussian assumptions and demonstrating practical gains when source-target distributions align closely.

Abstract: We study the non-contextual multi-armed bandit problem in a transfer learning
setting: before any pulls, the learner is given N'_k i.i.d. samples from each
source distribution nu'_k, and the true target distributions nu_k lie within a
known distance bound d_k(nu_k, nu'_k) <= L_k. In this framework, we first
derive a problem-dependent asymptotic lower bound on cumulative regret that
extends the classical Lai-Robbins result to incorporate the transfer parameters
(d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy that
matches this new bound in the Gaussian case. Finally, we validate our approach
via simulations, showing that KL-UCB-Transfer significantly outperforms the
no-prior baseline when source and target distributions are sufficiently close.

</details>


### [336] [Algorithms for Adversarially Robust Deep Learning](https://arxiv.org/abs/2509.19100)
*Alexander Robey*

Main category: cs.LG

TL;DR: A thesis on robust deep learning across adversarial vision, domain generalization, and LLM jailbreaking, introducing new methods and certifiable approaches, and reporting state-of-the-art generalization in several domains.


<details>
  <summary>Details</summary>
Motivation: Safety-critical deep learning demands robustness against adversarial inputs, distribution shifts, and malicious prompts; current gaps span computer vision robustness, cross-domain generalization, and language-model misuse, motivating a multi-area investigation.

Method: Three-part study: (1) adversarial robustness in computer vision with new results, training paradigms, and certification algorithms; (2) domain generalization with novel algorithms achieving strong performance on medical imaging, molecular identification, and image classification; (3) jailbreaking large language models with novel attacks and defenses to advance robustness of language-based agents.

Result: Provides improved robustness concepts and methods: certification-enabled robustness in vision; state-of-the-art generalization in listed domains; advancement toward robust LLM safety through new attacks and defenses; collectively, progress toward more robust AI systems across modalities.

Conclusion: Demonstrates that robust, trustworthy AI is increasingly feasible across vision, generalization under distribution shifts, and language models, and outlines directions for stronger guarantees and safer deployment.

Abstract: Given the widespread use of deep learning models in safety-critical
applications, ensuring that the decisions of such models are robust against
adversarial exploitation is of fundamental importance. In this thesis, we
discuss recent progress toward designing algorithms that exhibit desirable
robustness properties. First, we discuss the problem of adversarial examples in
computer vision, for which we introduce new technical results, training
paradigms, and certification algorithms. Next, we consider the problem of
domain generalization, wherein the task is to train neural networks to
generalize from a family of training distributions to unseen test
distributions. We present new algorithms that achieve state-of-the-art
generalization in medical imaging, molecular identification, and image
classification. Finally, we study the setting of jailbreaking large language
models (LLMs), wherein an adversarial user attempts to design prompts that
elicit objectionable content from an LLM. We propose new attacks and defenses,
which represent the frontier of progress toward designing robust language-based
agents.

</details>


### [337] [DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment](https://arxiv.org/abs/2509.19104)
*Sharan Sahu,Martin T. Wells*

Main category: cs.LG

TL;DR: Robust offline RLHF via DRO-REBEL with Wasserstein, KL, chi^2 divergences; reduces to relative-reward regression; provable estimation/minimax rates; scalable SGD algorithms; strong empirical robustness across tasks; reveals radius-coverage no-free-lunch trade-off.


<details>
  <summary>Details</summary>
Motivation: Address overoptimization in offline RLHF due to reward misspecification by introducing uncertainty-aware updates that are robust across ambiguity sets.

Method: Introduce a unified DRO-REBEL framework with three ambiguity sets (type-p Wasserstein, KL, chi^2). Use Fenchel duality to convert updates into a simple relative-reward regression. Develop practical SGD algorithms for each divergence: gradient regularization for Wasserstein, importance weighting for KL, and a fast 1-D dual solve for chi^2. Prove estimation and minimax rates under standard linear-reward and log-linear-policy with data-coverage; perform radius-coverage analysis. Empirically validate on Emotion Alignment, ArmoRM, and HH-Alignment for robustness across preference mixtures, model sizes, and data scales.

Result: Derives O(n^{-1/4}) estimation bounds with tighter constants than prior DRO-DPO; localized Rademacher analysis recovers minimax-optimal O(n^{-1/2}) rates; Wasserstein-DPO and KL-DPO also attain optimal rates. SGD algorithms provided for all three divergences; chi^2-REBEL shows strongest empirical performance across tasks; radius-coverage study confirms a no-free-lunch trade-off between rapid radius shrinking and empirical coverage.

Conclusion: DRO-REBEL delivers scalable, provably robust offline RLHF across multiple divergences, with clear trade-offs between radius and coverage. Practitioners can select the divergence and radius to balance robustness, data, and coverage; chi^2-REBEL often yields best empirical performance.

Abstract: Reinforcement learning with human feedback (RLHF) has become crucial for
aligning Large Language Models (LLMs) with human intent. However, existing
offline RLHF approaches suffer from overoptimization, where models overfit to
reward misspecification and drift from preferred behaviors observed during
training. We introduce DRO-REBEL, a unified family of robust REBEL updates with
type-$p$ Wasserstein, KL, and $\chi^2$ ambiguity sets. Using Fenchel duality,
each update reduces to a simple relative-reward regression, preserving
scalability and avoiding PPO-style clipping or auxiliary value networks. Under
standard linear-reward and log-linear policy classes with a data-coverage
condition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants
than prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$
rate via a localized Rademacher complexity analysis. The same analysis closes
the gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal
parametric rates. We derive practical SGD algorithms for all three divergences:
gradient regularization (Wasserstein), importance weighting (KL), and a fast
1-D dual solve ($\chi^2$). Experiments on Emotion Alignment, the large-scale
ArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong
worst-case robustness across unseen preference mixtures, model sizes, and data
scales, with $\chi^2$-REBEL showing consistently strong empirical performance.
A controlled radius--coverage study validates a no-free-lunch trade-off: radii
shrinking faster than empirical divergence concentration rates achieve
minimax-optimal parametric rates but forfeit coverage, while
coverage-guaranteeing radii incur $O(n^{-1/4})$ rates.

</details>


### [338] [Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation](https://arxiv.org/abs/2509.19112)
*Hugo Math,Rainer Lienhart*

Main category: cs.LG

TL;DR: CARGO is a scalable, domain-specific, two-stage causal discovery method for sparse, high-dimensional event sequences that infers per-sequence causal graphs with pretrained causal Transformers and then fuses them to recover global Markov boundaries, demonstrated on a large automotive dataset.


<details>
  <summary>Details</summary>
Motivation: Understanding causality in event sequences where outcomes arise from preceding events is critical across domains but challenging due to sparsity and high dimensionality; existing methods are often intractable at scale.

Method: Use two pretrained causal Transformers as domain-specific foundation models. For each sequence, infer a one-shot causal graph in parallel. Aggregate these graphs with adaptive frequency fusion to reconstruct the global Markov boundaries of labels, enabling efficient probabilistic reasoning without full-dataset conditional independence testing.

Result: Applied to a real-world automotive fault prediction dataset with over 29,100 unique event types and 474 imbalanced labels, showing CARGO can perform structured causal reasoning at scale.

Conclusion: CARGO enables scalable, parallelizable causal discovery for sparse, high-dimensional event sequences by combining sequence-level graphs into a global structure, offering efficient probabilistic reasoning without exhaustive unconditional independence testing.

Abstract: Understanding causality in event sequences where outcome labels such as
diseases or system failures arise from preceding events like symptoms or error
codes is critical. Yet remains an unsolved challenge across domains like
healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label
causal discovery method for sparse, high-dimensional event sequences comprising
of thousands of unique event types. Using two pretrained causal Transformers as
domain-specific foundation models for event sequences. CARGO infers in
parallel, per sequence one-shot causal graphs and aggregates them using an
adaptive frequency fusion to reconstruct the global Markov boundaries of
labels. This two-stage approach enables efficient probabilistic reasoning at
scale while bypassing the intractable cost of full-dataset conditional
independence testing. Our results on a challenging real-world automotive fault
prediction dataset with over 29,100 unique event types and 474 imbalanced
labels demonstrate CARGO's ability to perform structured reasoning.

</details>


### [339] [FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy Federated Learning in Healthcare AI](https://arxiv.org/abs/2509.19120)
*Ferdinand Kahenga,Antoine Bagula,Sajal K. Das,Patrick Sello*

Main category: cs.LG

TL;DR: FedFiTS is a trust- and fairness-aware selective FL framework that uses fitness-based client selection and slotted aggregation to improve convergence, robustness, and communication efficiency. It provides theoretical convergence and communication guarantees and shows empirical gains over FedAvg, FedRand, and FedPow on medical, vision, and tabular datasets.


<details>
  <summary>Details</summary>
Motivation: Address privacy-preserving learning in sensitive domains (e.g., healthcare) under non-IID data, unreliable clients, and adversarial manipulation by combining trust-aware aggregation with fairness-driven client selection and reduced communication.

Method: Three-phase participation: free-for-all training, natural selection, and slotted team participation; dynamic client scoring; adaptive thresholding; cohort-based scheduling; fitness-based client election; slotted aggregation; theoretical convergence analysis for convex and non-convex objectives; communication complexity analysis.

Result: Theoretical convergence bounds for both convex and non-convex objectives; reduced communication complexity relative to FedAvg and baselines; empirical gains over FedAvg, FedRand, and FedPow in accuracy, time-to-target, and resilience to poisoning across medical imaging (X-ray pneumonia), vision (MNIST, FMNIST), and tabular crop data (Crop Recommendation).

Conclusion: FedFiTS meaningfully advances scalable, secure FL by integrating trust-aware aggregation with fairness-driven client selection, making it well-suited for real-world healthcare and cross-domain deployments.

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for
privacy-preserving model training, yet deployments in sensitive domains such as
healthcare face persistent challenges from non-IID data, client unreliability,
and adversarial manipulation. This paper introduces FedFiTS, a trust and
fairness-aware selective FL framework that advances the FedFaSt line by
combining fitness-based client election with slotted aggregation. FedFiTS
implements a three-phase participation strategy-free-for-all training, natural
selection, and slotted team participation-augmented with dynamic client
scoring, adaptive thresholding, and cohort-based scheduling to balance
convergence efficiency with robustness. A theoretical convergence analysis
establishes bounds for both convex and non-convex objectives under standard
assumptions, while a communication-complexity analysis shows reductions
relative to FedAvg and other baselines. Experiments on diverse datasets-medical
imaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular
agricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently
outperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and
resilience to poisoning attacks. By integrating trust-aware aggregation with
fairness-oriented client selection, FedFiTS advances scalable and secure FL,
making it well suited for real-world healthcare and cross-domain deployments.

</details>


### [340] [Analysis on distribution and clustering of weight](https://arxiv.org/abs/2509.19122)
*Chunming Ye,Wenquan Tian,Yalan Gao,Songzhou Li*

Main category: cs.LG

TL;DR: Proposes two weight-characterization vectors (Standard-Deviation Vector and Clustering Vector) to analyze LLM weights; they distinguish models and show family similarities; LoRA tuning alters distribution but not correlations.


<details>
  <summary>Details</summary>
Motivation: To systematically capture distribution and correlation properties of weight matrices in LLMs to aid model comparison and understand fine-tuning effects.

Method: Standard-Deviation Vector: assume weights follow a normal distribution; compute normalized standard deviations of projection matrices. Clustering Vector: extract singular values from weight projection matrices and cluster with K-Means; group by matrix type to form the vector.

Result: The vectors can distinguish between different models and reveal within-family similarities; under LoRA fine-tuning, std-based distribution shifts with dataset, while clustering-based correlations remain consistent with the pre-trained model.

Conclusion: Two complementary vectors robustly characterize weight-side properties; useful for model comparison and understanding LoRA effects; clustering captures invariances across fine-tuning.

Abstract: The study on architecture and parameter characteristics remains the hot topic
in the research of large language models. In this paper we concern with the
characteristics of weight which are used to analyze the correlations and
differences between models. Two kinds of vectors-standard deviation vector and
clustering vector-are proposed to describe features of models. In the first
case, the weights are assumed to follow normal distribution. The standard
deviation values of projection matrices are normalized to form
Standard-Deviation Vector, representing the distribution characteristics of
models. In the second case, the singular values from each weight projection
matrix are extracted and grouped by K-Means algorithm. The grouped data with
the same type matrix are combined as Clustering Vector to represent the
correlation characteristics of models' weights. The study reveals that these
two vectors can effectively distinguish between different models and clearly
show the similarities among models of the same family. Moreover, after
conducting LoRA fine-tuning with different datasets and models, it is found
that the distribution of weights represented by standard deviation vector is
directly influenced by the dataset, but the correlations between different
weights represented by clustering vector remain unaffected and maintain a high
consistency with the pre-trained model.

</details>


### [341] [PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio](https://arxiv.org/abs/2509.19128)
*Alexandre Piché,Ehsan Kamaloo,Rafael Pardinas,Dzmitry Bahdanau*

Main category: cs.LG

TL;DR: PipelineRL enables asynchronous data generation with in-flight weight updates to improve RL for LLM training, achieving ~2x faster learning on 128 GPUs while maintaining on-policy data; open-source implementation provided.


<details>
  <summary>Details</summary>
Motivation: Scaling RL for LLMs is hampered by the need to keep accelerator utilization high without producing stale or off-policy data, which degrades RL algorithms.

Method: Introduce concurrent asynchronous data generation and model training with in-flight weight updates, allowing the generation engine to receive updated weights with minimal interruption during token generation, thereby improving hardware efficiency and data freshness.

Result: On long-form reasoning tasks with 128 H100 GPUs, PipelineRL achieves approximately 2x faster learning compared to conventional RL baselines while maintaining highly on-policy training data; scalable open-source implementation released.

Conclusion: PipelineRL provides a favorable trade-off between hardware efficiency and data on-policyness for RL in LLM training, enabling scalable, high-throughput RL workflows with preserved data freshness.

Abstract: Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning
capabilities of Large Language Models (LLMs). However, effectively scaling
these RL methods presents significant challenges, primarily due to the
difficulty in maintaining high AI accelerator utilization without generating
stale, off-policy data that harms common RL algorithms. This paper introduces
PipelineRL, an approach designed to achieve a superior trade-off between
hardware efficiency and data on-policyness for LLM training. PipelineRL employs
concurrent asynchronous data generation and model training, distinguished by
the novel in-flight weight updates. This mechanism allows the LLM generation
engine to receive updated model weights with minimal interruption during the
generation of token sequences, thereby maximizing both the accelerator
utilization and the freshness of training data. Experiments conducted on
long-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL
achieves approximately $\sim 2x$ faster learning compared to conventional RL
baselines while maintaining highly on-policy training data. A scalable and
modular open-source implementation of PipelineRL is also released as a key
contribution.

</details>


### [342] [GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding](https://arxiv.org/abs/2509.19135)
*Wenying Luo,Zhiyuan Lin,Wenhao Xu,Minghao Liu,Zhi Li*

Main category: cs.LG

TL;DR: GSTM-HMU is a four-part generative framework for mobility analysis that jointly models spatio-temporal semantics and behavior to improve next-location prediction, trajectory-user identification, and time estimation, with advantages in interpretability and generalization.


<details>
  <summary>Details</summary>
Motivation: To address the semantic and temporal complexity of human mobility data and improve predictive performance and interpretability beyond existing predictive models.

Method: Proposes STCE (spatio-temporal concept encoder), CTM (cognitive trajectory memory), LCB (lifestyle concept bank), and task-oriented generative heads. Trains and evaluates on Gowalla, WeePlace, Brightkite, and FourSquare for three tasks: next-location prediction, trajectory-user identification, and time estimation.

Result: Achieves consistent and substantial improvements over strong baselines across four real-world datasets and three tasks, demonstrating effectiveness in extracting semantic regularities and supporting robust, interpretable, and generalizable mobility intelligence.

Conclusion: Generative modeling offers a promising foundation for robust, interpretable, and generalizable human mobility systems, with GSTM-HMU advancing semantic-aware spatio-temporal representation learning.

Abstract: Human mobility traces, often recorded as sequences of check-ins, provide a
unique window into both short-term visiting patterns and persistent lifestyle
regularities. In this work we introduce GSTM-HMU, a generative spatio-temporal
framework designed to advance mobility analysis by explicitly modeling the
semantic and temporal complexity of human movement. The framework consists of
four key innovations. First, a Spatio-Temporal Concept Encoder (STCE)
integrates geographic location, POI category semantics, and periodic temporal
rhythms into unified vector representations. Second, a Cognitive Trajectory
Memory (CTM) adaptively filters historical visits, emphasizing recent and
behaviorally salient events in order to capture user intent more effectively.
Third, a Lifestyle Concept Bank (LCB) contributes structured human preference
cues, such as activity types and lifestyle patterns, to enhance
interpretability and personalization. Finally, task-oriented generative heads
transform the learned representations into predictions for multiple downstream
tasks. We conduct extensive experiments on four widely used real-world
datasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate
performance on three benchmark tasks: next-location prediction, trajectory-user
identification, and time estimation. The results demonstrate consistent and
substantial improvements over strong baselines, confirming the effectiveness of
GSTM-HMU in extracting semantic regularities from complex mobility data. Beyond
raw performance gains, our findings also suggest that generative modeling
provides a promising foundation for building more robust, interpretable, and
generalizable systems for human mobility intelligence.

</details>


### [343] [Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions](https://arxiv.org/abs/2509.19159)
*Qingfeng Lan,Gautham Vasan,A. Rupam Mahmood*

Main category: cs.LG

TL;DR: Activation function design, specifically gradient sparsity via elephant activation functions, reduces catastrophic forgetting in value-based RL, improving sample and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: Catastrophic forgetting is a long-standing challenge in RL; while algorithms to mitigate forgetting exist, the role of neural network architecture—especially activation functions and gradient sparsity—has been understudied. Understanding this could guide architecture choices that improve plasticity and retention.

Method: Investigate training dynamics of activation functions in RL; identify gradient sparsity as a key factor; propose a new class of elephant activations producing sparse outputs and sparse gradients; validate by replacing standard activations in value-based RL agents and evaluating forgetting resilience.

Result: Using elephant activations reduces forgetting in value-based RL and enhances sample and memory efficiency; significant resilience to catastrophic forgetting observed.

Conclusion: Activation function design, particularly inducing gradient sparsity, is a crucial architectural factor in forgetting. Elephant activations offer a practical approach to mitigate forgetting and improve RL efficiency.

Abstract: Catastrophic forgetting has remained a significant challenge for efficient
reinforcement learning for decades (Ring 1994, Rivest and Precup 2003). While
recent works have proposed effective methods to mitigate this issue, they
mainly focus on the algorithmic side. Meanwhile, we do not fully understand
what architectural properties of neural networks lead to catastrophic
forgetting. This study aims to fill this gap by studying the role of activation
functions in the training dynamics of neural networks and their impact on
catastrophic forgetting in reinforcement learning setup. Our study reveals
that, besides sparse representations, the gradient sparsity of activation
functions also plays an important role in reducing forgetting. Based on this
insight, we propose a new class of activation functions, elephant activation
functions, that can generate both sparse outputs and sparse gradients. We show
that by simply replacing classical activation functions with elephant
activation functions in the neural networks of value-based algorithms, we can
significantly improve the resilience of neural networks to catastrophic
forgetting, thus making reinforcement learning more sample-efficient and
memory-efficient.

</details>


### [344] [Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws](https://arxiv.org/abs/2509.19189)
*Binghui Li,Fengling Chen,Zixun Huang,Lean Wang,Lei Wu*

Main category: cs.LG

TL;DR: Introduces Functional Scaling Law (FSL) for SGD in teacher-student kernel regression to capture training dynamics and learning rate schedule effects; provides convolution-type term encoding LRS; supports three LRSs and data/compute regimes; offers theoretical justification for common LLM training practices and a practical surrogate model for loss curves.


<details>
  <summary>Details</summary>
Motivation: Current scaling-law work focuses on final-step loss and often ignores training dynamics and learning rate schedules (LRS). There is a need to understand how LRS shapes loss evolution during training to guide LLM pre-training more effectively.

Method: Model SGD as online kernel regression in a teacher-student setting and describe training via an intrinsic time SDE. Derive a Functional Scaling Law that expresses population risk evolution with an explicit convolution-type functional term capturing LRS effects. Analyze constant, exponential decay, and warmup-stable-decay LRS under data-limited and compute-limited regimes. Validate as a surrogate model for loss curves and provide theoretical support for empirical training practices.

Result: FSL characterizes how population risk evolves under SGD with general LRSs; LRS effects appear through a tractable convolution functional. Findings suggest higher-capacity models are more data- and compute-efficient, LR decay can improve training efficiency, and WSD-like schedules can outperform direct-decay schedules. FSL can be used to fit, predict, and optimize loss curves across model sizes (0.1B–1B).

Conclusion: FSL advances understanding of LLM pre-training dynamics by linking learning rate schedules to training loss evolution in a principled, tractable framework. It offers guidance for LRS design and supports practical use as a surrogate model for loss curve optimization.

Abstract: Scaling laws have played a cornerstone role in guiding the training of large
language models (LLMs). However, most existing works on scaling laws primarily
focus on the final-step loss, overlooking the loss dynamics during the training
process and, crucially, the impact of learning rate schedule (LRS). In this
paper, we aim to bridge this gap by studying a teacher-student kernel
regression setup trained via online stochastic gradient descent (SGD).
Leveraging a novel intrinsic time viewpoint and stochastic differential
equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),
which characterizes the evolution of population risk during the training
process for general LRSs. Remarkably, the impact of the LRSs is captured
through an explicit convolution-type functional term, making their effects
fully tractable. To illustrate the utility of FSL, we analyze three widely used
LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under
both data-limited and compute-limited regimes. We provide theoretical
justification for widely adopted empirical practices in LLMs pre-training such
as (i) higher-capacity models are more data- and compute-efficient; (ii)
learning rate decay can improve training efficiency; (iii) WSD-like schedules
can outperform direct-decay schedules. Lastly, we explore the practical
relevance of FSL as a surrogate model for fitting, predicting and optimizing
the loss curves in LLM pre-training, with experiments conducted across model
sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen
the understanding of LLM pre-training dynamics and provide insights for
improving large-scale model training.

</details>


### [345] [A Validation Strategy for Deep Learning Models: Evaluating and Enhancing Robustness](https://arxiv.org/abs/2509.19197)
*Abdul-Rauf Nuhu,Parham Kebria,Vahid Hemmati,Benjamin Lartey,Mahmoud Nabil Mahmoud,Abdollah Homaifar,Edward Tunstel*

Main category: cs.LG

TL;DR: Train-data driven robustness validation via weak-robust samples extracted from training data through local robustness analysis; uses these challenging instances to diagnose and guide improvements, improving reliability against adversarial and common corruptions.


<details>
  <summary>Details</summary>
Motivation: Traditional robustness validation relies on perturbed test sets; the approach seeks early, sensitive indicators of vulnerability from the training data itself by identifying 'weak robust' samples.

Method: Identify weakly robust samples from the training set using local robustness analysis; evaluate models on these samples to gain insights into vulnerabilities; use findings to drive targeted robustness improvements.

Result: Demonstrated on CIFAR-10, CIFAR-100, and ImageNet; robustness validation guided by weak robust samples leads to meaningful improvements under adversarial and common corruption scenarios.

Conclusion: Weak-robust-sample–driven validation offers a practical framework for assessing and enhancing robustness of deep models without relying solely on perturbed test data.

Abstract: Data-driven models, especially deep learning classifiers often demonstrate
great success on clean datasets. Yet, they remain vulnerable to common data
distortions such as adversarial and common corruption perturbations. These
perturbations can significantly degrade performance, thereby challenging the
overall reliability of the models. Traditional robustness validation typically
relies on perturbed test datasets to assess and improve model performance. In
our framework, however, we propose a validation approach that extracts "weak
robust" samples directly from the training dataset via local robustness
analysis. These samples, being the most susceptible to perturbations, serve as
an early and sensitive indicator of the model's vulnerabilities. By evaluating
models on these challenging training instances, we gain a more nuanced
understanding of its robustness, which informs targeted performance
enhancement. We demonstrate the effectiveness of our approach on models trained
with CIFAR-10, CIFAR-100, and ImageNet, highlighting how robustness validation
guided by weak robust samples can drive meaningful improvements in model
reliability under adversarial and common corruption scenarios.

</details>


### [346] [PPG-Distill: Efficient Photoplethysmography Signals Analysis via Foundation Model Distillation](https://arxiv.org/abs/2509.19215)
*Juntong Ni,Saurabh Kataria,Shengpu Tang,Carl Yang,Xiao Hu,Wei Jin*

Main category: cs.LG

TL;DR: PPG-Distill is a knowledge distillation framework that transfers both global and local knowledge (prediction, feature, and patch-level) to compact PPG models, using morphology and rhythm distillation to preserve waveform patterns and inter-patch temporal structure. It achieves up to 21.8% performance gains on heart rate estimation and atrial fibrillation detection, with 7x faster inference and 19x memory reduction, enabling efficient wearable PPG analysis.


<details>
  <summary>Details</summary>
Motivation: Deploying large PPG foundation models on resource-limited wearables is challenging due to computational, memory, and power constraints. There is a need to preserve local waveform morphology and inter-patch temporal structure while shrinking models.

Method: A knowledge distillation framework (PPG-Distill) that transfers both global and local knowledge through prediction-, feature-, and patch-level distillation. It introduces morphology distillation to preserve local waveform morphology and rhythm distillation to capture inter-patch temporal structures. The framework is evaluated on heart rate estimation and atrial fibrillation detection tasks, showing improved student performance and efficiency.

Result: On heart rate estimation and atrial fibrillation detection, the student model improves by up to 21.8% in performance. Inference is 7× faster and memory usage is reduced by 19×, enabling efficient PPG analysis on wearables.

Conclusion: PPG-Distill demonstrates that combining global and local knowledge distillation with targeted morphology and rhythm distillation can produce compact PPG models that retain essential waveform and temporal information, suitable for deployment on resource-constrained wearable devices.

Abstract: Photoplethysmography (PPG) is widely used in wearable health monitoring, yet
large PPG foundation models remain difficult to deploy on resource-limited
devices. We present PPG-Distill, a knowledge distillation framework that
transfers both global and local knowledge through prediction-, feature-, and
patch-level distillation. PPG-Distill incorporates morphology distillation to
preserve local waveform patterns and rhythm distillation to capture inter-patch
temporal structures. On heart rate estimation and atrial fibrillation
detection, PPG-Distill improves student performance by up to 21.8% while
achieving 7X faster inference and reducing memory usage by 19X, enabling
efficient PPG analysis on wearables

</details>


### [347] [FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity](https://arxiv.org/abs/2509.19220)
*Ferdinand Kahenga,Antoine Bagula,Patrick Sello,Sajal K. Das*

Main category: cs.LG

TL;DR: FedFusion unifies personalization, domain adaptation, and label-efficient learning to achieve robust, fair federated learning under heterogeneous, non-IID, and label-scarce conditions.


<details>
  <summary>Details</summary>
Motivation: Federated learning in practice faces heterogeneous feature spaces, severe non-IID data, and scarce labels across clients; there is a need to harmonize personalization with domain adaptation and label efficiency.

Method: FedFusion is a federated transfer-learning framework that uses DivEn encoders (DivEn, DivEn-mix, DivEn-c) for diversity and cluster awareness, confidence-filtered pseudo-labels from labelled teacher clients to guide learner clients via domain-adaptive transfer, and a similarity-weighted classifier coupling (with optional cluster-wise averaging) to maintain global coherence. It also features a frugal-labelling pipeline combining self-/semi-supervised pretext tasks with selective fine-tuning, reducing annotation demands while keeping raw data local. Personalised encoders are maintained for local data, and optional cluster-based averaging helps balance data-rich and minority clients.

Result: Across tabular and imaging benchmarks under IID, non-IID, and label-scarce regimes, FedFusion consistently outperforms state-of-the-art baselines in accuracy, robustness, and fairness while maintaining comparable communication and computation budgets.

Conclusion: Harmonising personalization, domain adaptation, and label efficiency is an effective recipe for robust federated learning under real-world constraints.

Abstract: Federated learning in practice must contend with heterogeneous feature
spaces, severe non-IID data, and scarce labels across clients. We present
FedFusion, a federated transfer-learning framework that unifies domain
adaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,
DivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via
confidence-filtered pseudo-labels and domain-adaptive transfer, while clients
maintain personalised encoders tailored to local data. To preserve global
coherence under heterogeneity, FedFusion employs similarity-weighted classifier
coupling (with optional cluster-wise averaging), mitigating dominance by
data-rich sites and improving minority-client performance. The frugal-labelling
pipeline combines self-/semi-supervised pretext training with selective
fine-tuning, reducing annotation demands without sharing raw data. Across
tabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,
FedFusion consistently outperforms state-of-the-art baselines in accuracy,
robustness, and fairness while maintaining comparable communication and
computation budgets. These results show that harmonising personalisation,
domain adaptation, and label efficiency is an effective recipe for robust
federated learning under real-world constraints.

</details>


### [348] [Video Killed the Energy Budget: Characterizing the Latency and Power Regimes of Open Text-to-Video Models](https://arxiv.org/abs/2509.19222)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: The paper analyzes latency and energy use in open-source text-to-video models, develops scaling laws for spatial resolution, temporal length, and denoising steps, validates predictions on WAN2.1-T2V, and compares six models to provide benchmarks and guidance for sustainable video generation.


<details>
  <summary>Details</summary>
Motivation: T2V models are computationally and energetically expensive, but detailed energy/latency scaling and cross-model benchmarks are lacking. Understanding these factors is essential for efficient deployment and environmental sustainability.

Method: Develop a compute-bound analytical model predicting how latency/energy scale with spatial resolution, temporal length, and denoising steps. Validate with fine-grained experiments on WAN2.1-T2V. Extend analysis to six diverse T2V models to compare runtime and energy under default settings.

Result: The model predicts quadratic growth with spatial and temporal dimensions and linear growth with the number of denoising steps. Experimental validation on WAN2.1-T2V supports these scaling laws. Across six models, runtime and energy profiles differ, but the paper provides a benchmark reference for sustainable deployment.

Conclusion: The work delivers practical guidelines and benchmarks to design and deploy more energy-efficient T2V systems, highlighting scaling trends and offering insights to reduce computational and energy costs in generative video workflows.

Abstract: Recent advances in text-to-video (T2V) generation have enabled the creation
of high-fidelity, temporally coherent clips from natural language prompts. Yet
these systems come with significant computational costs, and their energy
demands remain poorly understood. In this paper, we present a systematic study
of the latency and energy consumption of state-of-the-art open-source T2V
models. We first develop a compute-bound analytical model that predicts scaling
laws with respect to spatial resolution, temporal length, and denoising steps.
We then validate these predictions through fine-grained experiments on
WAN2.1-T2V, showing quadratic growth with spatial and temporal dimensions, and
linear scaling with the number of denoising steps. Finally, we extend our
analysis to six diverse T2V models, comparing their runtime and energy profiles
under default settings. Our results provide both a benchmark reference and
practical insights for designing and deploying more sustainable generative
video systems.

</details>


### [349] [Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation](https://arxiv.org/abs/2509.19233)
*Milad Leyli-abadi,Antoine Marot,Jérôme Picault*

Main category: cs.LG

TL;DR: Ablation study of hybrid physics-informed ML for power-flow prediction, comparing regularization-based constraints, unsupervised losses, and graph-based architectures, using a custom benchmarking pipeline (LIPS) to assess accuracy, physical compliance, industrial readiness, and out-of-distribution generalization; results illuminate trade-offs and reproducibility.


<details>
  <summary>Details</summary>
Motivation: Increase in renewable generation and cross-border electricity trading raises grid uncertainty and operational risk. Traditional solvers are accurate but too slow for near real-time use. Physics-informed/hybrid ML offers speed while respecting physical laws, but the best integration strategies are unclear.

Method: Systematic ablation across hybridization strategies: (i) incorporate physical constraints as regularization/unsupervised losses; (ii) test architectures from multilayer perceptrons to graph-based networks that can directly encode physics equations; (iii) evaluate within a custom benchmarking pipeline (LIPS) across four dimensions: accuracy, physical compliance, industrial readiness, and out-of-distribution generalization; all implementations are reproducible and available on GitHub.

Result: Findings indicate that the inclusion of physical knowledge affects performance across the four evaluation dimensions, highlighting trade-offs between accuracy and physical adherence as well as considerations for industrial deployment and OOD generalization; the study provides guidance on which hybridization choices yield favorable balances depending on the objective.

Conclusion: Hybridization strategies for physics-informed ML in power-flow prediction meaningfully shape performance trade-offs. The work offers actionable insights and a reproducible framework (LIPS) to help practitioners select and design surrogates that balance speed, physics compliance, reliability, and generalization.

Abstract: In the context of the energy transition, with increasing integration of
renewable sources and cross-border electricity exchanges, power grids are
encountering greater uncertainty and operational risk. Maintaining grid
stability under varying conditions is a complex task, and power flow simulators
are commonly used to support operators by evaluating potential actions before
implementation. However, traditional physical solvers, while accurate, are
often too slow for near real-time use. Machine learning models have emerged as
fast surrogates, and to improve their adherence to physical laws (e.g.,
Kirchhoff's laws), they are often trained with embedded constraints which are
also known as physics-informed or hybrid models. This paper presents an
ablation study to demystify hybridization strategies, ranging from
incorporating physical constraints as regularization terms or unsupervised
losses, and exploring model architectures from simple multilayer perceptrons to
advanced graph-based networks enabling the direct optimization of physics
equations. Using our custom benchmarking pipeline for hybrid models called
LIPS, we evaluate these models across four dimensions: accuracy, physical
compliance, industrial readiness, and out-of-distribution generalization. The
results highlight how integrating physical knowledge impacts performance across
these criteria. All the implementations are reproducible and provided in the
corresponding Github page.

</details>


### [350] [Stability and Generalization of Adversarial Diffusion Training](https://arxiv.org/abs/2509.19234)
*Hesam Hosseini,Ying Cao,Ali H. Sayed*

Main category: cs.LG

TL;DR: Stability-based generalization analysis for decentralized adversarial training under diffusion; derives a finite-sample bound showing generalization error grows with adversarial perturbation strength and number of training steps; validated empirically with logistic regression.


<details>
  <summary>Details</summary>
Motivation: Adversarial training improves robustness but induces robust overfitting and larger generalization gaps. While convergence of decentralized adversarial training has been studied, its generalization properties remain unexplored.

Method: Develop a stability-based framework for diffusion-based decentralized adversarial training with convex losses. Derive a finite-sample generalization bound that scales with perturbation amplitude and the number of training steps; apply to convex loss settings.

Result: The generalization bound indicates error grows with both perturbation strength and training steps, consistent with single-agent results but novel for decentralized (diffusion) settings. Logistic regression experiments validate the theoretical predictions.

Conclusion: Demonstrates that stability analysis can quantify generalization in decentralized adversarial training under diffusion; extends known single-agent results to decentralized networks and highlights trade-offs between robustness and generalization in such settings.

Abstract: Algorithmic stability is an established tool for analyzing generalization.
While adversarial training enhances model robustness, it often suffers from
robust overfitting and an enlarged generalization gap. Although recent work has
established the convergence of adversarial training in decentralized networks,
its generalization properties remain unexplored. This work presents a
stability-based generalization analysis of adversarial training under the
diffusion strategy for convex losses. We derive a bound showing that the
generalization error grows with both the adversarial perturbation strength and
the number of training steps, a finding consistent with single-agent case but
novel for decentralized settings. Numerical experiments on logistic regression
validate these theoretical predictions.

</details>


### [351] [What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT](https://arxiv.org/abs/2509.19284)
*Yunzhen Feng,Julia Kempe,Cheng Zhang,Parag Jain,Anthony Hartshorn*

Main category: cs.LG

TL;DR: Longer CoTs and more revisions do not improve accuracy; a graph-based metric (FSF) predicts correctness across models; removing failed branches and structure-aware test-time selection yield gains; effective CoTs fail less and are efficiently structured.


<details>
  <summary>Details</summary>
Motivation: To resolve what characterizes an effective chain-of-thought (CoT) in large reasoning models, moving beyond the belief that longer traces are better, by quantifying structure and failure points in CoT and testing interventions.

Method: Systematically evaluate ten large reasoning models on math and scientific reasoning; develop a graph view of CoT to measure structure; define Failed-Step Fraction (FSF) as the fraction of steps in abandoned branches; compare test-time metrics by ranking CoTs; edit CoTs to prune failed branches and measure impact on accuracy.

Result: Long CoTs and increased review correlate with lower accuracy. FSF consistently outpredicts length/review as a predictor of correctness across models. Pruning failed branches and structure-aware, test-time scaling yield accuracy gains (FSF-guided selection gives the largest pass@1 improvements).

Conclusion: Effective CoTs are those that fail less and leverage structure-aware strategies rather than indiscriminately producing longer traces; pruning failed branches and focusing on CoT structure improves reasoning performance.

Abstract: Large reasoning models (LRMs) spend substantial test-time compute on long
chain-of-thought (CoT) traces, but what *characterizes* an effective CoT
remains unclear. While prior work reports gains from lengthening CoTs and
increasing review (revisiting earlier steps) via appended *wait* tokens, recent
studies suggest that shorter thinking can outperform longer traces. We
therefore conduct a systematic evaluation across ten LRMs on math and
scientific reasoning. Contrary to the "longer-is-better" narrative, we find
that both naive CoT lengthening and increased review are associated with
*lower* accuracy.
  As CoT unfolds step by step, token-level metrics can conflate verbosity with
process quality. We introduce a graph view of CoT to extract structure and
identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of
steps in abandoned branches-that consistently outpredicts length and review
ratio for correctness across models. To probe causality, we design two
interventions. First, we rank candidate CoTs by each metric at test time, where
FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed
branches, which significantly improves accuracy, indicating that failed
branches bias subsequent reasoning. Taken together, these results characterize
effective CoTs as those that *fail less* and support *structure-aware*
test-time scaling over indiscriminately generating long CoT.

</details>
