<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 66]
- [cs.LG](#cs.LG) [Total: 70]
- [cs.CG](#cs.CG) [Total: 2]
- [cs.AI](#cs.AI) [Total: 38]
- [cs.RO](#cs.RO) [Total: 14]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Heatmap Regression without Soft-Argmax for Facial Landmark Detection](https://arxiv.org/abs/2508.14929)
*Chiao-An Yang,Raymond A. Yeh*

Main category: cs.CV

TL;DR: Revisits heatmap-based facial landmark regression by replacing Soft-argmax with a structured prediction training objective; shows state-of-the-art results on WFLW, COFW, 300W and 2.2x faster training.


<details>
  <summary>Details</summary>
Motivation: Soft-argmax is a common differentiable proxy for predicting landmark coordinates from heatmaps, but it may not be the best or only viable approach. The paper argues for a structured prediction framework to directly optimize landmark configurations, aiming for better training efficiency and potentially higher accuracy.

Method: Introduce a structured prediction objective for heatmap-based facial landmark detection that replaces the standard Soft-argmax training path. The objective aligns predictions with global facial structure via a structured loss, rather than relying on a differentiable argmax proxy.

Result: Achieves state-of-the-art performance on WFLW, COFW, and 300W benchmarks. Training converges roughly 2.2x faster while maintaining or improving accuracy. Code is released for replication.

Conclusion: Soft-argmax is not the sole viable route for heatmap-based landmark regression. A classic structured prediction objective can yield superior training efficiency and competitive accuracy, reinforcing the viability of non-Soft-argmax training in this domain.

Abstract: Facial landmark detection is an important task in computer vision with
numerous applications, such as head pose estimation, expression analysis, face
swapping, etc. Heatmap regression-based methods have been widely used to
achieve state-of-the-art results in this task. These methods involve computing
the argmax over the heatmaps to predict a landmark. Since argmax is not
differentiable, these methods use a differentiable approximation, Soft-argmax,
to enable end-to-end training on deep-nets. In this work, we revisit this
long-standing choice of using Soft-argmax and demonstrate that it is not the
only way to achieve strong performance. Instead, we propose an alternative
training objective based on the classic structured prediction framework.
Empirically, our method achieves state-of-the-art performance on three facial
landmark benchmarks (WFLW, COFW, and 300W), converging 2.2x faster during
training while maintaining better/competitive accuracy. Our code is available
here: https://github.com/ca-joe-yang/regression-without-softarg.

</details>


### [2] [Fast Graph Neural Network for Image Classification](https://arxiv.org/abs/2508.14958)
*Mustafa Mohammadi Gharasuie,Luis Rueda*

Main category: cs.CV

TL;DR: A graph-based image classification framework combining Graph Convolutional Networks with Voronoi/Delaunay graph refinements to improve accuracy and preprocessing efficiency, outperforming CNN baselines on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Images exhibit rich relational structure; CNNs on grids may miss nonlocal relations. Graph-based representations plus geometric triangulations aim to better capture spatial relations, potentially improving classification, especially in complex or fine-grained scenes.

Method: Represent images as graphs with pixels or regions as vertices; refine graphs using Delaunay triangulations and Voronoi diagrams; apply Graph Convolutional Networks for classification; evaluate on benchmark datasets with cross-validation; claim improved preprocessing efficiency and accuracy.

Result: Reported significant improvements in preprocessing efficiency and classification accuracy; surpasses state-of-the-art, particularly in challenging and fine-grained categories; validated via cross-validation.

Conclusion: Demonstrates the viability of integrating GCNs with Voronoi-based graph refinements for image classification and highlights broader potential of graph-based learning in computer vision and unstructured data analysis.

Abstract: The rapid progress in image classification has been largely driven by the
adoption of Graph Convolutional Networks (GCNs), which offer a robust framework
for handling complex data structures. This study introduces a novel approach
that integrates GCNs with Voronoi diagrams to enhance image classification by
leveraging their ability to effectively model relational data. Unlike
conventional convolutional neural networks (CNNs), our method represents images
as graphs, where pixels or regions function as vertices. These graphs are then
refined using corresponding Delaunay triangulations, optimizing their
representation. The proposed model achieves significant improvements in both
preprocessing efficiency and classification accuracy across various benchmark
datasets, surpassing state-of-the-art approaches, particularly in challenging
scenarios involving intricate scenes and fine-grained categories. Experimental
results, validated through cross-validation, underscore the effectiveness of
combining GCNs with Voronoi diagrams for advancing image classification. This
research not only presents a novel perspective on image classification but also
expands the potential applications of graph-based learning paradigms in
computer vision and unstructured data analysis.

</details>


### [3] [You Only Pose Once: A Minimalist's Detection Transformer for Monocular RGB Category-level 9D Multi-Object Pose Estimation](https://arxiv.org/abs/2508.14965)
*Hakjin Lee,Junghoon Seo,Jaehoon Sim*

Main category: cs.CV

TL;DR: YOPO is a single-stage RGB-only framework that unifies category-level 9-DoF pose estimation with 2D detection, using a transformer detector augmented with a pose head, translation module, and 6D-aware matching; achieves state-of-the-art on REAL275 and other benchmarks.


<details>
  <summary>Details</summary>
Motivation: To abolish the need for pseudo-depth, CAD models, and multi-stage pipelines by showing end-to-end RGB-only category-level pose estimation is possible.

Method: A single-stage, query-based transformer detector (YOPO) with a lightweight pose head, a bounding-box-conditioned translation module, and a 6D-aware Hungarian matching cost; trained end-to-end with RGB images and category-level pose labels.

Result: Sets new state of the art on three benchmarks; REAL275 IoU50 79.6%, and 10°/10cm 54.1%; surpasses previous RGB-only methods and closes much of the gap to RGB-D.

Conclusion: Demonstrates the feasibility and effectiveness of an RGB-only, unified detection-and-pose-estimation approach; provides a strong new baseline and code release.

Abstract: Accurately recovering the full 9-DoF pose of unseen instances within specific
categories from a single RGB image remains a core challenge for robotics and
automation. Most existing solutions still rely on pseudo-depth, CAD models, or
multi-stage cascades that separate 2D detection from pose estimation. Motivated
by the need for a simpler, RGB-only alternative that learns directly at the
category level, we revisit a longstanding question: Can object detection and
9-DoF pose estimation be unified with high performance, without any additional
data? We show that they can with our method, YOPO, a single-stage, query-based
framework that treats category-level 9-DoF estimation as a natural extension of
2D detection. YOPO augments a transformer detector with a lightweight pose
head, a bounding-box-conditioned translation module, and a 6D-aware Hungarian
matching cost. The model is trained end-to-end only with RGB images and
category-level pose labels. Despite its minimalist design, YOPO sets a new
state of the art on three benchmarks. On the REAL275 dataset, it achieves 79.6%
$\rm{IoU}_{50}$ and 54.1% under the $10^\circ$$10{\rm{cm}}$ metric, surpassing
prior RGB-only methods and closing much of the gap to RGB-D systems. The code,
models, and additional qualitative results can be found on our project.

</details>


### [4] [Paired-Sampling Contrastive Framework for Joint Physical-Digital Face Attack Detection](https://arxiv.org/abs/2508.14980)
*Andrei Balykin,Anvar Ganiev,Denis Kondranin,Kirill Polevoda,Nikolai Liudkevich,Artem Petrov*

Main category: cs.CV

TL;DR: Unified paired-sampling contrastive framework for detecting both physical and digital face spoofing, achieving strong accuracy with low compute.


<details>
  <summary>Details</summary>
Motivation: Combine physical (presentation) attacks and digital (forgery) attacks into a single detector to reduce complexity, latency, and vulnerability to combined attacks.

Method: Paired-Sampling Contrastive Framework that uses automatically matched pairs of genuine and attack selfies to learn modality-agnostic liveness cues; trains in a unified fashion for joint detection.

Result: ACER 2.10% on the 6th Face Anti-Spoofing Challenge Unified Physical-Digital Attack Detection benchmark; 4.46 GFLOPs; training in under one hour; outperforms prior solutions.

Conclusion: Demonstrates practicality for real-world deployment with a compact, unified detector; reduces system complexity and latency and increases resilience to combined spoofing vectors; code and pretrained models released.

Abstract: Modern face recognition systems remain vulnerable to spoofing attempts,
including both physical presentation attacks and digital forgeries.
Traditionally, these two attack vectors have been handled by separate models,
each targeting its own artifacts and modalities. However, maintaining distinct
detectors increases system complexity and inference latency and leaves systems
exposed to combined attack vectors. We propose the Paired-Sampling Contrastive
Framework, a unified training approach that leverages automatically matched
pairs of genuine and attack selfies to learn modality-agnostic liveness cues.
Evaluated on the 6th Face Anti-Spoofing Challenge Unified Physical-Digital
Attack Detection benchmark, our method achieves an average classification error
rate (ACER) of 2.10 percent, outperforming prior solutions. The framework is
lightweight (4.46 GFLOPs) and trains in under one hour, making it practical for
real-world deployment. Code and pretrained models are available at
https://github.com/xPONYx/iccv2025_deepfake_challenge.

</details>


### [5] [TAIGen: Training-Free Adversarial Image Generation via Diffusion Models](https://arxiv.org/abs/2508.15020)
*Susim Roy,Anubhooti Jain,Mayank Vatsa,Richa Singh*

Main category: cs.CV

TL;DR: TAIGen is a training-free black-box adversarial attack using diffusion models, requiring only 3–20 sampling steps and a selective RGB perturbation strategy to achieve high attack success with PSNR > 30 dB, while being ~10x faster than prior diffusion-based attacks.


<details>
  <summary>Details</summary>
Motivation: Diffusion models offer high-quality image generation but adversarial generation is expensive and slow. There is a need for training-free, black-box methods that can craft effective adversarial examples efficiently without full diffusion sampling.

Method: TAIGen injects perturbations during a limited mixing-step interval of unconditional diffusion sampling, using a selective RGB strategy: attention maps perturb the red channel while GradCAM-guided perturbations affect green and blue channels. This preserves image structure while maximizing misclassification, and requires only 3–20 steps from the diffusion model.

Result: On ImageNet with VGGNet as source, TAIGen achieves 70.6% attack success against ResNet, 80.8% against MNASNet, and 97.8% against ShuffleNet; PSNR > 30 dB across datasets; ~10× faster than existing diffusion-based attacks.

Conclusion: A training-free, black-box diffusion attack can produce high-quality adversarial images with minimal diffusion steps and a channel-aware perturbation strategy, achieving strong attack effectiveness and exposing vulnerabilities even against defenses that purify diffusion-generated content.

Abstract: Adversarial attacks from generative models often produce low-quality images
and require substantial computational resources. Diffusion models, though
capable of high-quality generation, typically need hundreds of sampling steps
for adversarial generation. This paper introduces TAIGen, a training-free
black-box method for efficient adversarial image generation. TAIGen produces
adversarial examples using only 3-20 sampling steps from unconditional
diffusion models. Our key finding is that perturbations injected during the
mixing step interval achieve comparable attack effectiveness without processing
all timesteps. We develop a selective RGB channel strategy that applies
attention maps to the red channel while using GradCAM-guided perturbations on
green and blue channels. This design preserves image structure while maximizing
misclassification in target models. TAIGen maintains visual quality with PSNR
above 30 dB across all tested datasets. On ImageNet with VGGNet as source,
TAIGen achieves 70.6% success against ResNet, 80.8% against MNASNet, and 97.8%
against ShuffleNet. The method generates adversarial examples 10x faster than
existing diffusion-based attacks. Our method achieves the lowest robust
accuracy, indicating it is the most impactful attack as the defense mechanism
is least successful in purifying the images generated by TAIGen.

</details>


### [6] [Reversible Unfolding Network for Concealed Visual Perception with Generative Refinement](https://arxiv.org/abs/2508.15027)
*Chunming He,Fengyang Xiao,Rihan Zhang,Chengyu Fang,Deng-Ping Fan,Sina Farsiu*

Main category: cs.CV

TL;DR: A reversible unfolding network RUN++ for concealed visual perception that unifies mask and RGB domains with diffusion-based refinement, using CORE, CARE, and FINE modules and a targeted Bernoulli diffusion on uncertain regions to improve robustness and efficiency.


<details>
  <summary>Details</summary>
Motivation: Extend reversible modeling from the mask domain to the RGB domain, address uncertainty more effectively, and leverage diffusion models for fine-detail restoration without full-image cost; provide a robust CVP framework under real-world degradations.

Method: Formulates CVP as an optimization problem and unfolds the iterative solution into a multi-stage network. CORE applies reversible modeling in the mask domain to identify core object regions; CARE extends the principle to the RGB domain for better foreground-background separation; FINE performs a final refinement via a targeted Bernoulli diffusion model that refines only the uncertain mask regions, guided by the unfolding prior; diffusion resolves remaining uncertainty, enabling efficient, focused refinement. The approach also introduces a bi-level optimization framework for robustness under degradations.

Result: Claims improved handling of uncertainty, reduced false positives/negatives, and efficiency through targeted diffusion that focuses on uncertain regions, plus enhanced robustness to real-world degradations; quantitative performance is not provided in the abstract.

Conclusion: RUN++ offers a principled, efficient framework for reversible CVP across mask and RGB domains and introduces a broader bi-level optimization paradigm to build robust CVP systems.

Abstract: Existing methods for concealed visual perception (CVP) often leverage
reversible strategies to decrease uncertainty, yet these are typically confined
to the mask domain, leaving the potential of the RGB domain underexplored. To
address this, we propose a reversible unfolding network with generative
refinement, termed RUN++. Specifically, RUN++ first formulates the CVP task as
a mathematical optimization problem and unfolds the iterative solution into a
multi-stage deep network. This approach provides a principled way to apply
reversible modeling across both mask and RGB domains while leveraging a
diffusion model to resolve the resulting uncertainty. Each stage of the network
integrates three purpose-driven modules: a Concealed Object Region Extraction
(CORE) module applies reversible modeling to the mask domain to identify core
object regions; a Context-Aware Region Enhancement (CARE) module extends this
principle to the RGB domain to foster better foreground-background separation;
and a Finetuning Iteration via Noise-based Enhancement (FINE) module provides a
final refinement. The FINE module introduces a targeted Bernoulli diffusion
model that refines only the uncertain regions of the segmentation mask,
harnessing the generative power of diffusion for fine-detail restoration
without the prohibitive computational cost of a full-image process. This unique
synergy, where the unfolding network provides a strong uncertainty prior for
the diffusion model, allows RUN++ to efficiently direct its focus toward
ambiguous areas, significantly mitigating false positives and negatives.
Furthermore, we introduce a new paradigm for building robust CVP systems that
remain effective under real-world degradations and extend this concept into a
broader bi-level optimization framework.

</details>


### [7] [GasTwinFormer: A Hybrid Vision Transformer for Livestock Methane Emission Segmentation and Dietary Classification in Optical Gas Imaging](https://arxiv.org/abs/2508.15057)
*Toqi Tahamid Sarker,Mohamed Embaby,Taminul Islam,Amer AbuGhazaleh,Khaled R Ahmed*

Main category: cs.CV

TL;DR: GasTwinFormer is a lightweight hybrid transformer for real-time methane emission segmentation and dietary classification in optical gas imaging, achieving high accuracy with a small model on a new beef cattle dataset.


<details>
  <summary>Details</summary>
Motivation: Livestock methane accounts for a large share of anthropogenic methane; automated, real-time monitoring is critical for effective climate mitigation and management.

Method: Introduces GasTwinFormer with a Mix Twin encoder that alternates spatially-reduced global attention and locally-grouped attention, plus a lightweight LR-ASPP decoder for multi-scale feature aggregation. Supports simultaneous methane segmentation and dietary classification in a unified framework. Builds the first comprehensive beef cattle methane emission dataset (11,694 annotated frames across three diets).

Result: Segmentation: 74.47% mIoU and 83.63% mF1. Efficiency: 3.348M parameters, 3.428 GFLOPs, 114.9 FPS. Diet classification: 100% accuracy. Dataset enables diet-emission correlation analysis. Extensive ablations validate architectural components.

Conclusion: GasTwinFormer offers a practical solution for real-time livestock emission monitoring and introduces a valuable dataset for diet-emission studies, demonstrating strong performance with efficiency suitable for field deployment.

Abstract: Livestock methane emissions represent 32% of human-caused methane production,
making automated monitoring critical for climate mitigation strategies. We
introduce GasTwinFormer, a hybrid vision transformer for real-time methane
emission segmentation and dietary classification in optical gas imaging through
a novel Mix Twin encoder alternating between spatially-reduced global attention
and locally-grouped attention mechanisms. Our architecture incorporates a
lightweight LR-ASPP decoder for multi-scale feature aggregation and enables
simultaneous methane segmentation and dietary classification in a unified
framework. We contribute the first comprehensive beef cattle methane emission
dataset using OGI, containing 11,694 annotated frames across three dietary
treatments. GasTwinFormer achieves 74.47% mIoU and 83.63% mF1 for segmentation
while maintaining exceptional efficiency with only 3.348M parameters, 3.428G
FLOPs, and 114.9 FPS inference speed. Additionally, our method achieves perfect
dietary classification accuracy (100%), demonstrating the effectiveness of
leveraging diet-emission correlations. Extensive ablation studies validate each
architectural component, establishing GasTwinFormer as a practical solution for
real-time livestock emission monitoring. Please see our project page at
gastwinformer.github.io.

</details>


### [8] [CurveFlow: Curvature-Guided Flow Matching for Image Generation](https://arxiv.org/abs/2508.15093)
*Yan Luo,Drake Du,Hao Huang,Yi Fang,Mengyu Wang*

Main category: cs.CV

TL;DR: CurveFlow introduces curvature-aware non-linear trajectory learning for flow-based text-to-image generation, achieving state-of-the-art semantic alignment on COCO by regularizing trajectory curvature.


<details>
  <summary>Details</summary>
Motivation: Rectified flow models use linear trajectories with zero curvature, which can push samples through low-probability regions and misalign with complex captions. Curvature might drive better instruction compliance.

Method: Propose CurveFlow, a flow-matching framework that enforces smooth, non-linear trajectories via a curvature regularization term; penalizes abrupt changes in intrinsic dynamics while guiding the flow path with curvature cues; evaluated on MS COCO 2014/2017.

Result: Achieves state-of-the-art text-to-image generation on COCO; outperforms standard rectified flows and non-linear baselines (e.g., Rectified Diffusion); notable gains in semantic metrics (BLEU, METEOR, ROUGE, CLAIR); code released.

Conclusion: Curvature-aware trajectory modeling substantially enhances semantic fidelity and instruction compliance without sacrificing image quality.

Abstract: Existing rectified flow models are based on linear trajectories between data
and noise distributions. This linearity enforces zero curvature, which can
inadvertently force the image generation process through low-probability
regions of the data manifold. A key question remains underexplored: how does
the curvature of these trajectories correlate with the semantic alignment
between generated images and their corresponding captions, i.e., instructional
compliance? To address this, we introduce CurveFlow, a novel flow matching
framework designed to learn smooth, non-linear trajectories by directly
incorporating curvature guidance into the flow path. Our method features a
robust curvature regularization technique that penalizes abrupt changes in the
trajectory's intrinsic dynamics.Extensive experiments on MS COCO 2014 and 2017
demonstrate that CurveFlow achieves state-of-the-art performance in
text-to-image generation, significantly outperforming both standard rectified
flow variants and other non-linear baselines like Rectified Diffusion. The
improvements are especially evident in semantic consistency metrics such as
BLEU, METEOR, ROUGE, and CLAIR. This confirms that our curvature-aware modeling
substantially enhances the model's ability to faithfully follow complex
instructions while simultaneously maintaining high image quality. The code is
made publicly available at
https://github.com/Harvard-AI-and-Robotics-Lab/CurveFlow.

</details>


### [9] [HiRQA: Hierarchical Ranking and Quality Alignment for Opinion-Unaware Image Quality Assessment](https://arxiv.org/abs/2508.15130)
*Vaishnav Ramesh,Haining Wang,Md Jahidul Islam*

Main category: cs.CV

TL;DR: HiRQA is a self-supervised, no-reference IQA framework using hierarchical ranking and quality alignment to learn quality-aware embeddings from synthetic distortions, achieving SOTA generalization to authentic degradations and enabling a fast HiRQA-S variant (3.5 ms per image).


<details>
  <summary>Details</summary>
Motivation: NR-IQA suffers from dataset biases and reliance on subjective labels, which harms generalization. A pristine-reference-free, opinion-unaware approach with robust cross-distortion generalization is needed.

Method: A self-supervised framework that combines higher-order ranking loss over distortion pairs, an embedding distance loss aligning feature distances with perceptual differences, and a training-time contrastive alignment loss guided by structured textual prompts. Trains only on synthetic distortions and predicts quality scores using only the input image. A lightweight HiRQA-S variant provides real-time inference (~3.5 ms/image).

Result: HiRQA generalizes effectively to authentic degradations (lens flare, haze, motion blur, low-light) and achieves state-of-the-art performance on both synthetic and authentic IQA benchmarks, with strong generalization and scalability.

Conclusion: HiRQA offers a no-reference, opinion-unaware, hierarchical quality representation that generalizes from synthetic to real-world distortions, and delivers scalable, real-time inference via HiRQA-S.

Abstract: Despite significant progress in no-reference image quality assessment
(NR-IQA), dataset biases and reliance on subjective labels continue to hinder
their generalization performance. We propose HiRQA, Hierarchical Ranking and
Quality Alignment), a self-supervised, opinion-unaware framework that offers a
hierarchical, quality-aware embedding through a combination of ranking and
contrastive learning. Unlike prior approaches that depend on pristine
references or auxiliary modalities at inference time, HiRQA predicts quality
scores using only the input image. We introduce a novel higher-order ranking
loss that supervises quality predictions through relational ordering across
distortion pairs, along with an embedding distance loss that enforces
consistency between feature distances and perceptual differences. A
training-time contrastive alignment loss, guided by structured textual prompts,
further enhances the learned representation. Trained only on synthetic
distortions, HiRQA generalizes effectively to authentic degradations, as
demonstrated through evaluation on various distortions such as lens flare,
haze, motion blur, and low-light conditions. For real-time deployment, we
introduce \textbf{HiRQA-S}, a lightweight variant with an inference time of
only 3.5 ms per image. Extensive experiments across synthetic and authentic
benchmarks validate HiRQA's state-of-the-art (SOTA) performance, strong
generalization ability, and scalability.

</details>


### [10] [Reliable Multi-view 3D Reconstruction for `Just-in-time' Edge Environments](https://arxiv.org/abs/2508.15158)
*Md. Nurul Absur,Abhinav Kumar,Swastik Brahma,Saptarshi Debroy*

Main category: cs.CV

TL;DR: Portfolio-theory–inspired edge resource management for reliable multi-view 3D reconstruction under spatiotemporal disruptions, solved with a genetic algorithm.


<details>
  <summary>Details</summary>
Motivation: Emergency response, tactical, and public-safety applications require near-real-time multi-view reconstruction in ad-hoc edge environments. Spatiotemporal disruptions cause camera failures and degrade reconstruction quality, and existing strategies lack reliability guarantees.

Method: Formulates camera selection and edge resource allocation as a portfolio-optimization problem using portfolio theory to balance reconstruction quality against disruption risk; solves the optimization with a genetic algorithm that converges quickly in realistic settings; evaluates on public and customized 3D datasets.

Result: Demonstrates improved reliability of 3D reconstruction under correlated disruptions compared with traditional baselines; the GA-based solution achieves fast convergence and practical performance on realistic datasets.

Conclusion: A practical, fast-converging approach to guarantee reconstruction quality under adverse, dynamic edge conditions, enabling reliable multi-view 3D reconstruction in ad-hoc edge deployments.

Abstract: Multi-view 3D reconstruction applications are revolutionizing critical use
cases that require rapid situational-awareness, such as emergency response,
tactical scenarios, and public safety. In many cases, their near-real-time
latency requirements and ad-hoc needs for compute resources necessitate
adoption of `Just-in-time' edge environments where the system is set up on the
fly to support the applications during the mission lifetime. However,
reliability issues can arise from the inherent dynamism and operational
adversities of such edge environments, resulting in spatiotemporally correlated
disruptions that impact the camera operations, which can lead to sustained
degradation of reconstruction quality. In this paper, we propose a novel
portfolio theory inspired edge resource management strategy for reliable
multi-view 3D reconstruction against possible system disruptions. Our proposed
methodology can guarantee reconstruction quality satisfaction even when the
cameras are prone to spatiotemporally correlated disruptions. The portfolio
theoretic optimization problem is solved using a genetic algorithm that
converges quickly for realistic system settings. Using publicly available and
customized 3D datasets, we demonstrate the proposed camera selection strategy's
benefits in guaranteeing reliable 3D reconstruction against traditional
baseline strategies, under spatiotemporal disruptions.

</details>


### [11] [XDR-LVLM: An Explainable Vision-Language Large Model for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2508.15168)
*Masato Ito,Kaito Tanaka,Keisuke Matsuda,Aya Nakayama*

Main category: cs.CV

TL;DR: XDR-LVLM uses Vision-Language LLMs with a medical encoder and multi-task prompting to provide explainable diabetic retinopathy diagnosis and natural-language reports, achieving strong accuracy and interpretability on the DDR dataset.


<details>
  <summary>Details</summary>
Motivation: To tackle the black-box nature of deep learning DR models and supply transparent, clinically meaningful explanations that improve trust and decision-making.

Method: A modular framework combining a specialized Medical Vision Encoder, LVLM Core, and multi-task prompt engineering with multi-stage fine-tuning to extract pathological features from fundus images and generate diagnostic reports that include DR severity, key pathologies (hemorrhages, exudates, microaneurysms), and explanatory links between features and diagnosis.

Result: On the DDR dataset, disease diagnosis achieves Balanced Accuracy 84.55% and F1 score 79.92%; concept detection reaches 77.95% BACC and 66.88% F1. Human evaluations report high fluency, accuracy, and clinical utility of the explanations.

Conclusion: XDR-LVLM demonstrates that explainable, high-performance DR diagnosis is feasible, providing robust, interpretable insights that bridge automated analysis with clinical needs and potentially enhance adoption in clinical workflow.

Abstract: Diabetic Retinopathy (DR) is a major cause of global blindness, necessitating
early and accurate diagnosis. While deep learning models have shown promise in
DR detection, their black-box nature often hinders clinical adoption due to a
lack of transparency and interpretability. To address this, we propose XDR-LVLM
(eXplainable Diabetic Retinopathy Diagnosis with LVLM), a novel framework that
leverages Vision-Language Large Models (LVLMs) for high-precision DR diagnosis
coupled with natural language-based explanations. XDR-LVLM integrates a
specialized Medical Vision Encoder, an LVLM Core, and employs Multi-task Prompt
Engineering and Multi-stage Fine-tuning to deeply understand pathological
features within fundus images and generate comprehensive diagnostic reports.
These reports explicitly include DR severity grading, identification of key
pathological concepts (e.g., hemorrhages, exudates, microaneurysms), and
detailed explanations linking observed features to the diagnosis. Extensive
experiments on the Diabetic Retinopathy (DDR) dataset demonstrate that XDR-LVLM
achieves state-of-the-art performance, with a Balanced Accuracy of 84.55% and
an F1 Score of 79.92% for disease diagnosis, and superior results for concept
detection (77.95% BACC, 66.88% F1). Furthermore, human evaluations confirm the
high fluency, accuracy, and clinical utility of the generated explanations,
showcasing XDR-LVLM's ability to bridge the gap between automated diagnosis and
clinical needs by providing robust and interpretable insights.

</details>


### [12] [MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion](https://arxiv.org/abs/2508.15169)
*Xuyang Chen,Zhijun Zhai,Kaixuan Zhou,Zengmao Wang,Jianan He,Dong Wang,Yanfeng Zhang,mingwei Sun,Rüdiger Westermann,Konrad Schindler,Liqiu Meng*

Main category: cs.CV

TL;DR: MeSS uses a three-stage diffusion-based pipeline with 3D Gaussian splatting to synthesize textured, geometry-aligned outdoor city scenes from mesh priors, achieving improved cross-view consistency and enabling relighting/styling.


<details>
  <summary>Details</summary>
Motivation: There is a need for realistic textures on city mesh models for virtual urban navigation and autonomous driving. Diffusion models struggle with 3D consistency and adhering to predefined camera paths, so a geometry-aware approach is desirable.

Method: Three-stage pipeline: (1) Cascaded Outpainting ControlNets to generate geometrically consistent sparse views; (2) AGInpaint to propagate denser intermediate views; (3) GCAlign to remove global inconsistencies (e.g., exposure). Concurrent 3D Gaussian Splatting (3DGS) reconstructs a scene by placing Gaussian balls on the mesh surface. The synthesized scene can be relit and style-transferred.

Result: The approach outperforms existing methods in geometric alignment and generation quality, producing texture-rich outdoor scenes aligned to the city mesh priors.

Conclusion: MeSS effectively integrates geometry priors with diffusion-based generation to produce high-quality, style-consistent outdoor scenes, with 3DGS enabling rendering in diverse styles through relighting and style transfer.

Abstract: Mesh models have become increasingly accessible for numerous cities; however,
the lack of realistic textures restricts their application in virtual urban
navigation and autonomous driving. To address this, this paper proposes MeSS
(Meshbased Scene Synthesis) for generating high-quality, styleconsistent
outdoor scenes with city mesh models serving as the geometric prior. While
image and video diffusion models can leverage spatial layouts (such as depth
maps or HD maps) as control conditions to generate street-level perspective
views, they are not directly applicable to 3D scene generation. Video diffusion
models excel at synthesizing consistent view sequences that depict scenes but
often struggle to adhere to predefined camera paths or align accurately with
rendered control videos. In contrast, image diffusion models, though unable to
guarantee cross-view visual consistency, can produce more geometry-aligned
results when combined with ControlNet. Building on this insight, our approach
enhances image diffusion models by improving cross-view consistency. The
pipeline comprises three key stages: first, we generate geometrically
consistent sparse views using Cascaded Outpainting ControlNets; second, we
propagate denser intermediate views via a component dubbed AGInpaint; and
third, we globally eliminate visual inconsistencies (e.g., varying exposure)
using the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting
(3DGS) scene is reconstructed by initializing Gaussian balls on the mesh
surface. Our method outperforms existing approaches in both geometric alignment
and generation quality. Once synthesized, the scene can be rendered in diverse
styles through relighting and style transfer techniques.

</details>


### [13] [Adversarial Agent Behavior Learning in Autonomous Driving Using Deep Reinforcement Learning](https://arxiv.org/abs/2508.15207)
*Arjun Srinivasan,Anubhav Paras,Aniket Bera*

Main category: cs.CV

TL;DR: Proposes a learning-based adversary to cause failure scenarios for rule-based agents in RL for autonomous driving, and shows the adversary reduces the rule-based agents' cumulative rewards.


<details>
  <summary>Details</summary>
Motivation: In safety-critical settings like autonomous driving, accurately modeling surrounding agents is essential. Existing rule-based/ IDM models may be brittle, so it is important to study adversarial behaviors that can exploit these models to reveal vulnerabilities.

Method: Introduce a learning-based method to derive adversarial behavior targeting rule-based agents (modeled by IDM/behavior strategies). Evaluate the adversarial agent against all rule-based agents and measure the resulting change in cumulative reward.

Result: The adversarial agent successfully reduces the cumulative reward of the rule-based agents, indicating vulnerabilities in current behavior modeling.

Conclusion: Adversarial strategies reveal weaknesses in rule-based surrounding-agent models; robust modeling and adversarially-aware training or testing should be pursued to improve safety in RL for autonomous driving.

Abstract: Existing approaches in reinforcement learning train an agent to learn desired
optimal behavior in an environment with rule based surrounding agents. In
safety critical applications such as autonomous driving it is crucial that the
rule based agents are modelled properly. Several behavior modelling strategies
and IDM models are used currently to model the surrounding agents. We present a
learning based method to derive the adversarial behavior for the rule based
agents to cause failure scenarios. We evaluate our adversarial agent against
all the rule based agents and show the decrease in cumulative reward.

</details>


### [14] [DyMorph-B2I: Dynamic and Morphology-Guided Binary-to-Instance Segmentation for Renal Pathology](https://arxiv.org/abs/2508.15208)
*Leiyue Zhao,Yuechen Yang,Yanfan Zhu,Haichun Yang,Yuankai Huo,Paul D. Simonson,Kenji Ikemura,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: Dynamic, morphology-guided binary-to-instance segmentation (DyMorph-B2I) for renal pathology that combines watershed, skeletonization, and morphological operations with adaptive refinement and class-specific hyperparameters to convert semantic masks into reliable instance segmentations; achieves superior separation and morphometric accuracy, with code released.


<details>
  <summary>Details</summary>
Motivation: Need for accurate instance-level segmentation of renal functional units; semantic masks limit downstream morphometrics; renal tissue exhibits diverse morphologies and connectivity that hinder classical post-processing.

Method: Unified framework integrating watershed, skeletonization, morphologic operations; adaptive geometric refinement; class-specific hyperparameter tuning; systematic parameter optimization to separate adherent/heterogeneous structures.

Result: DyMorph-B2I outperforms individual classical methods and naive combinations on instance separation, enabling more accurate morphometric analyses.

Conclusion: The pipeline advances renal pathology workflows by providing robust, adaptable binary-to-instance segmentation; code available at GitHub.

Abstract: Accurate morphological quantification of renal pathology functional units
relies on instance-level segmentation, yet most existing datasets and automated
methods provide only binary (semantic) masks, limiting the precision of
downstream analyses. Although classical post-processing techniques such as
watershed, morphological operations, and skeletonization, are often used to
separate semantic masks into instances, their individual effectiveness is
constrained by the diverse morphologies and complex connectivity found in renal
tissue. In this study, we present DyMorph-B2I, a dynamic, morphology-guided
binary-to-instance segmentation pipeline tailored for renal pathology. Our
approach integrates watershed, skeletonization, and morphological operations
within a unified framework, complemented by adaptive geometric refinement and
customizable hyperparameter tuning for each class of functional unit. Through
systematic parameter optimization, DyMorph-B2I robustly separates adherent and
heterogeneous structures present in binary masks. Experimental results
demonstrate that our method outperforms individual classical approaches and
na\"ive combinations, enabling superior instance separation and facilitating
more accurate morphometric analysis in renal pathology workflows. The pipeline
is publicly available at: https://github.com/ddrrnn123/DyMorph-B2I.

</details>


### [15] [STAGNet: A Spatio-Temporal Graph and LSTM Framework for Accident Anticipation](https://arxiv.org/abs/2508.15216)
*Vipooshan Vipulananthan,Kumudu Mohottala,Kavindu Chinthana,Nimsara Paramulla,Charith D Chitraranjan*

Main category: cs.CV

TL;DR: STAGNet uses enhanced spatio-temporal features and a recurrent aggregator to predict accidents from dash-cam video, outperforming state-of-the-art graph networks on three public datasets in both within-dataset and cross-dataset evaluations.


<details>
  <summary>Details</summary>
Motivation: Accident prediction is crucial for road safety and requires low-cost, easily deployable solutions. Dash-cam only systems are attractive but challenging; the work aims to improve predictive performance using video alone, without dependence on LiDAR/radar/GPS.

Method: Introduce STAGNet, a model that learns richer spatio-temporal features from dash-cam videos and aggregates them with a recurrent network, surpassing graph neural network baselines. Evaluated on three public datasets with cross-validation and cross-dataset tests, using metrics such as average precision and mean time-to-collision.

Result: STAGNet achieves higher average precision and MTTC values than prior methods across both within-dataset and cross-dataset evaluations on three public datasets.

Conclusion: Enhanced spatio-temporal feature extraction combined with recurrent aggregation yields improved accident prediction from dash-cam video, offering a cost-effective and generalizable approach for ADAS.

Abstract: Accident prediction and timely warnings play a key role in improving road
safety by reducing the risk of injury to road users and minimizing property
damage. Advanced Driver Assistance Systems (ADAS) are designed to support human
drivers and are especially useful when they can anticipate potential accidents
before they happen. While many existing systems depend on a range of sensors
such as LiDAR, radar, and GPS, relying solely on dash-cam video input presents
a more challenging but a more cost-effective and easily deployable solution. In
this work, we incorporate better spatio-temporal features and aggregate them
through a recurrent network to improve upon state-of-the-art graph neural
networks for predicting accidents from dash-cam videos. Experiments using three
publicly available datasets show that our proposed STAGNet model achieves
higher average precision and mean time-to-collision values than previous
methods, both when cross-validated on a given dataset and when trained and
tested on different datasets.

</details>


### [16] [Collaborative Multi-Modal Coding for High-Quality 3D Generation](https://arxiv.org/abs/2508.15228)
*Ziang Cao,Zhaoxi Chen,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: TriMM is a feed-forward 3D-native generator that learns from multiple modalities (RGB, RGBD, and point clouds) via collaborative multi-modal coding and a triplane latent diffusion model, achieving high-quality 3D assets with limited data.


<details>
  <summary>Details</summary>
Motivation: 3D content is inherently multi-modal; existing 3D generators are often single-modality or 3D-only, underutilizing complementary information from RGB, depth, and point clouds.

Method: TriMM introduces collaborative multi-modal coding to fuse modality-specific features while preserving their strengths, adds auxiliary 2D and 3D supervision, and uses a triplane latent diffusion model conditioned on the multi-modal code to generate 3D assets.

Result: Experiments across multiple datasets show TriMM achieves competitive performance with models trained on large-scale data despite limited training data; RGB-D data and other modalities can be incorporated, validating the approach’s data-efficiency and versatility.

Conclusion: Multi-modal integration in 3D generation enhances texture and geometry quality with higher data efficiency; TriMM demonstrates a scalable framework for leveraging multiple modalities in 3D generation.

Abstract: 3D content inherently encompasses multi-modal characteristics and can be
projected into different modalities (e.g., RGB images, RGBD, and point clouds).
Each modality exhibits distinct advantages in 3D asset modeling: RGB images
contain vivid 3D textures, whereas point clouds define fine-grained 3D
geometries. However, most existing 3D-native generative architectures either
operate predominantly within single-modality paradigms-thus overlooking the
complementary benefits of multi-modality data-or restrict themselves to 3D
structures, thereby limiting the scope of available training datasets. To
holistically harness multi-modalities for 3D modeling, we present TriMM, the
first feed-forward 3D-native generative model that learns from basic
multi-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMM
first introduces collaborative multi-modal coding, which integrates
modality-specific features while preserving their unique representational
strengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced to
raise the robustness and performance of multi-modal coding. 3) Based on the
embedded multi-modal code, TriMM employs a triplane latent diffusion model to
generate 3D assets of superior quality, enhancing both the texture and the
geometric detail. Extensive experiments on multiple well-known datasets
demonstrate that TriMM, by effectively leveraging multi-modality, achieves
competitive performance with models trained on large-scale datasets, despite
utilizing a small amount of training data. Furthermore, we conduct additional
experiments on recent RGB-D datasets, verifying the feasibility of
incorporating other multi-modal datasets into 3D generation.

</details>


### [17] [Center-Oriented Prototype Contrastive Clustering](https://arxiv.org/abs/2508.15231)
*Shihao Dong,Xiaotong Zhou,Yuhui Zheng,Huiying Xu,Xinzhong Zhu*

Main category: cs.CV

TL;DR: A center-oriented prototype contrastive clustering method with soft prototype weighting and dual consistency to reduce inter-class conflicts and prototype drift, achieving state-of-the-art results on five datasets.


<details>
  <summary>Details</summary>
Motivation: Contrastive clustering often struggles with conflicts between classes and prototype drift when using hard prototypes; need center-aligned prototypes to improve clustering accuracy.

Method: Proposes a two-module framework: (1) soft prototype contrastive module that weights prototypes by the probability of a sample belonging to a cluster center to reduce inter-class conflicts; (2) dual consistency learning module that enforces transformation-invariance and compact intra-cluster neighborhoods to stabilize prototypes.

Result: Empirical evaluation on five datasets shows the method outperforms state-of-the-art methods; code released.

Conclusion: Center-oriented prototype contrastive clustering effectively mitigates prototype drift and class conflicts, yielding robust clustering with better semantic alignment; the approach is practical and reproducible.

Abstract: Contrastive learning is widely used in clustering tasks due to its
discriminative representation. However, the conflict problem between classes is
difficult to solve effectively. Existing methods try to solve this problem
through prototype contrast, but there is a deviation between the calculation of
hard prototypes and the true cluster center. To address this problem, we
propose a center-oriented prototype contrastive clustering framework, which
consists of a soft prototype contrastive module and a dual consistency learning
module. In short, the soft prototype contrastive module uses the probability
that the sample belongs to the cluster center as a weight to calculate the
prototype of each category, while avoiding inter-class conflicts and reducing
prototype drift. The dual consistency learning module aligns different
transformations of the same sample and the neighborhoods of different samples
respectively, ensuring that the features have transformation-invariant semantic
information and compact intra-cluster distribution, while providing reliable
guarantees for the calculation of prototypes. Extensive experiments on five
datasets show that the proposed method is effective compared to the SOTA. Our
code is published on https://github.com/LouisDong95/CPCC.

</details>


### [18] [AeroDuo: Aerial Duo for UAV-based Vision and Language Navigation](https://arxiv.org/abs/2508.15232)
*Ruipu Wu,Yige Zhang,Jinyu Chen,Linjiang Huang,Shifeng Zhang,Xu Zhou,Liang Wang,Si Liu*

Main category: cs.CV

TL;DR: Introduce DuAl-VLN, a dual-altitude UAV joint task for navigation, with HaL-13k dataset and AeroDuo framework enabling high-altitude reasoning via Pilot-LLM and low-altitude precise grounding with a lightweight policy; minimal inter-UAV communication.


<details>
  <summary>Details</summary>
Motivation: VLN for UAVs is challenging due to long trajectories and maneuverability; dual-altitude collaboration provides complementary perspectives (global reasoning vs local precision) while keeping the policy space manageable and reducing human intervention.

Method: Define a Dual-Altitude UAV Collaborative VLN (DuAl-VLN) task; build HaL-13k dataset with 13,838 high-low collaborative trajectories and target-oriented instructions; propose AeroDuo framework where high-altitude UAV uses a multimodal LLM (Pilot-LLM) for target reasoning and low-altitude UAV uses a multi-stage lightweight policy for navigation and grounding; exchange of only minimal coordinate information.

Result: HaL-13k dataset and the DuAl-VLN task are introduced; AeroDuo demonstrates how dual perspectives can be integrated to handle unseen maps and objects via evaluation with unseen maps and object validation; provides a foundation for generalization in UAV-VLN with limited communication.

Conclusion: Dual-altitude collaboration synergizes global reasoning and local navigation to address UAV-VLN challenges, enabling robust, generalizable navigation in outdoor environments with reduced human input and efficient communication.

Abstract: Aerial Vision-and-Language Navigation (VLN) is an emerging task that enables
Unmanned Aerial Vehicles (UAVs) to navigate outdoor environments using natural
language instructions and visual cues. However, due to the extended
trajectories and complex maneuverability of UAVs, achieving reliable UAV-VLN
performance is challenging and often requires human intervention or overly
detailed instructions. To harness the advantages of UAVs' high mobility, which
could provide multi-grained perspectives, while maintaining a manageable motion
space for learning, we introduce a novel task called Dual-Altitude UAV
Collaborative VLN (DuAl-VLN). In this task, two UAVs operate at distinct
altitudes: a high-altitude UAV responsible for broad environmental reasoning,
and a low-altitude UAV tasked with precise navigation. To support the training
and evaluation of the DuAl-VLN, we construct the HaL-13k, a dataset comprising
13,838 collaborative high-low UAV demonstration trajectories, each paired with
target-oriented language instructions. This dataset includes both unseen maps
and an unseen object validation set to systematically evaluate the model's
generalization capabilities across novel environments and unfamiliar targets.
To consolidate their complementary strengths, we propose a dual-UAV
collaborative VLN framework, AeroDuo, where the high-altitude UAV integrates a
multimodal large language model (Pilot-LLM) for target reasoning, while the
low-altitude UAV employs a lightweight multi-stage policy for navigation and
target grounding. The two UAVs work collaboratively and only exchange minimal
coordinate information to ensure efficiency.

</details>


### [19] [Pretrained Diffusion Models Are Inherently Skipped-Step Samplers](https://arxiv.org/abs/2508.15233)
*Wenju Xu*

Main category: cs.CV

TL;DR: Skipped-step sampling enables faster, Markovian diffusion sampling without changing the training objective, by bypassing multiple denoising steps and integrating with DDIM, yielding high-quality results with far fewer steps.


<details>
  <summary>Details</summary>
Motivation: Address the efficiency gap in diffusion models by showing that the original (Markovian) diffusion process can achieve competitive sampling speed without resorting to non-Markovian variants like DDIM.

Method: Introduce skipped-step sampling that bypasses multiple intermediate denoising steps in the iterative generation process. Derive this mechanism from the standard diffusion training objective (no changes to training). Combine the technique with DDIM. Validate on pretrained models (OpenAI ADM, Stable Diffusion, Open Sora).

Result: The approach achieves high-quality generation with significantly reduced sampling steps across multiple pretrained diffusion models, demonstrating that accelerated, yet principled, sampling is feasible in a Markovian setting.

Conclusion: Skipped-step sampling reveals an intrinsic, Markovian pathway to accelerated diffusion sampling, enabling faster generation without altering the training objective, and complements DDIM to further boost efficiency.

Abstract: Diffusion models have been achieving state-of-the-art results across various
generation tasks. However, a notable drawback is their sequential generation
process, requiring long-sequence step-by-step generation. Existing methods,
such as DDIM, attempt to reduce sampling steps by constructing a class of
non-Markovian diffusion processes that maintain the same training objective.
However, there remains a gap in understanding whether the original diffusion
process can achieve the same efficiency without resorting to non-Markovian
processes. In this paper, we provide a confirmative answer and introduce
skipped-step sampling, a mechanism that bypasses multiple intermediate
denoising steps in the iterative generation process, in contrast with the
traditional step-by-step refinement of standard diffusion inference. Crucially,
we demonstrate that this skipped-step sampling mechanism is derived from the
same training objective as the standard diffusion model, indicating that
accelerated sampling via skipped-step sampling via a Markovian way is an
intrinsic property of pretrained diffusion models. Additionally, we propose an
enhanced generation method by integrating our accelerated sampling technique
with DDIM. Extensive experiments on popular pretrained diffusion models,
including the OpenAI ADM, Stable Diffusion, and Open Sora models, show that our
method achieves high-quality generation with significantly reduced sampling
steps.

</details>


### [20] [Comp-X: On Defining an Interactive Learned Image Compression Paradigm With Expert-driven LLM Agent](https://arxiv.org/abs/2508.15243)
*Yixin Gao,Xin Li,Xiaohan Pan,Runsen Feng,Bingchen Li,Yunpeng Qi,Yiting Lu,Zhengxue Cheng,Zhibo Chen,Jörn Ostermann*

Main category: cs.CV

TL;DR: Comp-X introduces an LLM-driven interactive image compression framework that unifies multiple coding modes, enables interactive coding with expert-guided learning, and provides IIC-bench for evaluation; it achieves efficient understanding and competitive compression under a single framework, signaling AGI potential in image compression.


<details>
  <summary>Details</summary>
Motivation: Conventional image codecs offer limited modes and rely on manual mode selection, creating a usability gap for non-experts; an intelligent, interactive solution could democratize image compression and adapt to diverse tasks.

Method: Three innovations: (i) multi-functional coding framework unifying varied coding modes (perception-oriented, variable coding, spatial bit allocation); (ii) interactive coding agent using augmented in-context learning with coding expert feedback to guide request interpretation, mode selection, and tool use; (iii) IIC-bench benchmark with diverse user requests and expert annotations to evaluate intelligent interactivity in image compression.

Result: Experiments show Comp-X understands coding requests efficiently, demonstrates strong textual interaction, and maintains compression performance comparable to traditional frameworks when using a single framework.

Conclusion: This work points toward AGI in image compression, demonstrating that an LLM-powered, interactive approach can generalize across coding tasks while preserving efficiency.

Abstract: We present Comp-X, the first intelligently interactive image compression
paradigm empowered by the impressive reasoning capability of large language
model (LLM) agent. Notably, commonly used image codecs usually suffer from
limited coding modes and rely on manual mode selection by engineers, making
them unfriendly for unprofessional users. To overcome this, we advance the
evolution of image coding paradigm by introducing three key innovations: (i)
multi-functional coding framework, which unifies different coding modes of
various objective/requirements, including human-machine perception, variable
coding, and spatial bit allocation, into one framework. (ii) interactive coding
agent, where we propose an augmented in-context learning method with coding
expert feedback to teach the LLM agent how to understand the coding request,
mode selection, and the use of the coding tools. (iii) IIC-bench, the first
dedicated benchmark comprising diverse user requests and the corresponding
annotations from coding experts, which is systematically designed for
intelligently interactive image compression evaluation. Extensive experimental
results demonstrate that our proposed Comp-X can understand the coding requests
efficiently and achieve impressive textual interaction capability. Meanwhile,
it can maintain comparable compression performance even with a single coding
framework, providing a promising avenue for artificial general intelligence
(AGI) in image compression.

</details>


### [21] [Normal and Abnormal Pathology Knowledge-Augmented Vision-Language Model for Anomaly Detection in Pathology Images](https://arxiv.org/abs/2508.15256)
*Jinsol Song,Jiamu Wang,Anh Tien Nguyen,Keunho Byeon,Sangjeong Ahn,Sung Hak Lee,Jin Tae Kwak*

Main category: cs.CV

TL;DR: Ano-NAViLa is a vision-language anomaly detection model for pathology that uses a knowledge-augmented normal/abnormal framework with a lightweight MLP, achieving state-of-the-art anomaly detection and localization on two lymph node datasets, with interpretable image-text associations.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection in computational pathology suffers from limited disease data, heterogeneity of tissue structures, computational constraints, and lack of interpretability; existing industrial methods don't translate well to pathology.

Method: A pre-trained vision-language backbone with a trainable lightweight MLP; integrates normal and abnormal pathology knowledge; leverages image-text associations for interpretability.

Result: State-of-the-art performance in anomaly detection and localization on two lymph node datasets across organs, outperforming competing models.

Conclusion: Ano-NAViLa provides robust, interpretable anomaly detection in pathology under limited data and variability, enabling better localization and understanding via image-text alignment.

Abstract: Anomaly detection in computational pathology aims to identify rare and scarce
anomalies where disease-related data are often limited or missing. Existing
anomaly detection methods, primarily designed for industrial settings, face
limitations in pathology due to computational constraints, diverse tissue
structures, and lack of interpretability. To address these challenges, we
propose Ano-NAViLa, a Normal and Abnormal pathology knowledge-augmented
Vision-Language model for Anomaly detection in pathology images. Ano-NAViLa is
built on a pre-trained vision-language model with a lightweight trainable MLP.
By incorporating both normal and abnormal pathology knowledge, Ano-NAViLa
enhances accuracy and robustness to variability in pathology images and
provides interpretability through image-text associations. Evaluated on two
lymph node datasets from different organs, Ano-NAViLa achieves the
state-of-the-art performance in anomaly detection and localization,
outperforming competing models.

</details>


### [22] [RATopo: Improving Lane Topology Reasoning via Redundancy Assignment](https://arxiv.org/abs/2508.15272)
*Han Li,Shaofei Huang,Longfei Xu,Yulu Gao,Beipeng Mu,Si Liu*

Main category: cs.CV

TL;DR: RATopo introduces redundancy in lane topology reasoning by reconfiguring the Transformer decoder to preserve multiple lane predictions, enabling one-to-many assignments and diverse cross-attention, yielding consistent topology gains across models.


<details>
  <summary>Details</summary>
Motivation: Current one-to-one supervision in first-detect-then-reason pipelines limits topology reasoning quality due to a narrow supervision signal; richer, geometry-diverse supervision is needed.

Method: Swap cross-attention and self-attention in the Transformer decoder to keep redundant lane predictions prior to suppression, and deploy multiple parallel cross-attention blocks with independent parameters to boost diversity of detected lanes.

Result: Empirical evaluation on OpenLane-V2 shows RATopo is model-agnostic and improves both lane-lane and lane-traffic topology performance when integrated into existing topology reasoning frameworks.

Conclusion: RATopo provides a plug-in redundancy assignment strategy that enhances topology reasoning by enabling one-to-many predictions and diverse attention, leading to consistent gains across frameworks.

Abstract: Lane topology reasoning plays a critical role in autonomous driving by
modeling the connections among lanes and the topological relationships between
lanes and traffic elements. Most existing methods adopt a
first-detect-then-reason paradigm, where topological relationships are
supervised based on the one-to-one assignment results obtained during the
detection stage. This supervision strategy results in suboptimal topology
reasoning performance due to the limited range of valid supervision. In this
paper, we propose RATopo, a Redundancy Assignment strategy for lane Topology
reasoning that enables quantity-rich and geometry-diverse topology supervision.
Specifically, we restructure the Transformer decoder by swapping the
cross-attention and self-attention layers. This allows redundant lane
predictions to be retained before suppression, enabling effective one-to-many
assignment. We also instantiate multiple parallel cross-attention blocks with
independent parameters, which further enhances the diversity of detected lanes.
Extensive experiments on OpenLane-V2 demonstrate that our RATopo strategy is
model-agnostic and can be seamlessly integrated into existing topology
reasoning frameworks, consistently improving both lane-lane and lane-traffic
topology performance.

</details>


### [23] [DesignCLIP: Multimodal Learning with CLIP for Design Patent Understanding](https://arxiv.org/abs/2508.15297)
*Zhu Wang,Homaira Huda Shomee,Sathya N. Ravi,Sourav Medya*

Main category: cs.CV

TL;DR: A CLIP-based multimodal framework, DesignCLIP, enhances design patent classification and retrieval by leveraging generated captions, multi-view images, and class-aware contrastive learning, outperforming baselines and SOTA.


<details>
  <summary>Details</summary>
Motivation: Patent design images often lack complete visual/semantic context, leading to ambiguities in prior art searches; CLIP-style vision-language models offer a path to more reliable, AI-driven patent analysis.

Method: DesignCLIP uses a CLIP backbone with class-aware classification and contrastive learning, generates detailed captions for patent images, and employs multi-view image learning on a large-scale U.S. design patent dataset, targeting classification and retrieval tasks and exploring multimodal patent retrieval.

Result: DesignCLIP consistently outperforms baseline and SOTA models across design patent classification and retrieval tasks; multimodal retrieval shows potential to broaden sources of inspiration; codebase is available.

Conclusion: Multimodal, CLIP-based design patent analysis is a promising direction that improves accuracy and versatility in patent analysis, with practical utility for prior art searches and creativity support; a large-scale dataset and code are available to advance this line.

Abstract: In the field of design patent analysis, traditional tasks such as patent
classification and patent image retrieval heavily depend on the image data.
However, patent images -- typically consisting of sketches with abstract and
structural elements of an invention -- often fall short in conveying
comprehensive visual context and semantic information. This inadequacy can lead
to ambiguities in evaluation during prior art searches. Recent advancements in
vision-language models, such as CLIP, offer promising opportunities for more
reliable and accurate AI-driven patent analysis. In this work, we leverage CLIP
models to develop a unified framework DesignCLIP for design patent applications
with a large-scale dataset of U.S. design patents. To address the unique
characteristics of patent data, DesignCLIP incorporates class-aware
classification and contrastive learning, utilizing generated detailed captions
for patent images and multi-views image learning. We validate the effectiveness
of DesignCLIP across various downstream tasks, including patent classification
and patent retrieval. Additionally, we explore multimodal patent retrieval,
which provides the potential to enhance creativity and innovation in design by
offering more diverse sources of inspiration. Our experiments show that
DesignCLIP consistently outperforms baseline and SOTA models in the patent
domain on all tasks. Our findings underscore the promise of multimodal
approaches in advancing patent analysis. The codebase is available here:
https://anonymous.4open.science/r/PATENTCLIP-4661/README.md.

</details>


### [24] [TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification](https://arxiv.org/abs/2508.15298)
*Darya Taratynova,Alya Almsouti,Beknur Kalmakhanbet,Numan Saeed,Mohammad Yaqub*

Main category: cs.CV

TL;DR: Temporal Prompt Alignment (TPA) integrates temporal modeling, prompt-aware contrastive learning, and uncertainty quantification to classify fetal CHD in ultrasound videos, achieving state-of-the-art accuracy and improved calibration.


<details>
  <summary>Details</summary>
Motivation: Image noise and probe positioning variability hinder CHD detection; existing ML approaches underutilize temporal information, are often binary, and lack reliable calibration. A temporally-aware, calibrated, multi-class approach is needed for ultrasound video CHD classification.

Method: Extract frame-level features from video subclips with an image encoder; employ a trainable temporal extractor to capture heart motion; align video representations with class-specific text prompts using a margin-hinge contrastive loss; incorporate a Conditional Variational Autoencoder Style Modulation (CVAESM) that learns a latent style vector to modulate embeddings and quantify prediction uncertainty; evaluate on a private CHD dataset and EchoNet-Dynamic for systolic dysfunction.

Result: TPA achieves macro F1 of 85.40% for CHD diagnosis; reduces expected calibration error (ECE) by 5.38% and adaptive ECE by 6.8%; on EchoNet-Dynamic's three-class task, macro F1 improves from 53.89% to 58.62% (4.73-point gain).

Conclusion: TPA demonstrates that combining temporal modeling, prompt-aware contrastive learning, and uncertainty quantification yields improved diagnostic performance and better calibration in ultrasound video CHD classification.

Abstract: Congenital heart defect (CHD) detection in ultrasound videos is hindered by
image noise and probe positioning variability. While automated methods can
reduce operator dependence, current machine learning approaches often neglect
temporal information, limit themselves to binary classification, and do not
account for prediction calibration. We propose Temporal Prompt Alignment (TPA),
a method leveraging foundation image-text model and prompt-aware contrastive
learning to classify fetal CHD on cardiac ultrasound videos. TPA extracts
features from each frame of video subclips using an image encoder, aggregates
them with a trainable temporal extractor to capture heart motion, and aligns
the video representation with class-specific text prompts via a margin-hinge
contrastive loss. To enhance calibration for clinical reliability, we introduce
a Conditional Variational Autoencoder Style Modulation (CVAESM) module, which
learns a latent style vector to modulate embeddings and quantifies
classification uncertainty. Evaluated on a private dataset for CHD detection
and on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPA
achieves state-of-the-art macro F1 scores of 85.40% for CHD diagnosis, while
also reducing expected calibration error by 5.38% and adaptive ECE by 6.8%. On
EchoNet-Dynamic's three-class task, it boosts macro F1 by 4.73% (from 53.89% to
58.62%). Temporal Prompt Alignment (TPA) is a framework for fetal congenital
heart defect (CHD) classification in ultrasound videos that integrates temporal
modeling, prompt-aware contrastive learning, and uncertainty quantification.

</details>


### [25] [BasketLiDAR: The First LiDAR-Camera Multimodal Dataset for Professional Basketball MOT](https://arxiv.org/abs/2508.15299)
*Ryunosuke Hayashi,Kohei Torimi,Rokuto Nagata,Kazuma Ikeda,Ozora Sako,Taichi Nakamura,Masaki Tani,Yoshimitsu Aoki,Kentaro Yoshioka*

Main category: cs.CV

TL;DR: BasketLiDAR introduces the first multimodal basketball MOT dataset (BasketLiDAR) combining LiDAR with synchronized multi-view cameras, and a real-time MOT framework for basketball that uses LiDAR alone or fusion with cameras to achieve real-time performance and robust tracking under occlusions; dataset includes 4,445 frames and 3,105 IDs with synchronized IDs across three LiDARs and three cameras; results show real-time operation and improved tracking, especially under occlusion.


<details>
  <summary>Details</summary>
Motivation: Real-time 3D trajectory tracking in sports is challenging due to the inherent 2D nature of video data, heavy occlusions, and the need for accurate 3D reconstruction; basketball is a particularly difficult MOT scenario with rapid, dense player motion.

Method: Develop BasketLiDAR dataset with synchronized LiDAR point clouds and multi-view camera footage in professional basketball; propose a novel MOT framework with two pipelines: (1) a real-time LiDAR-only tracking pipeline; (2) a multimodal LiDAR-camera fused pipeline; ensure synchronized IDs across three LiDARs and three cameras; evaluate on 5-on-5 and 3-on-3 game data; provide complete 3D positions and IDs for each player.

Result: The approach achieves real-time operation and superior tracking performance under occlusion, surpassing camera-only methods; the dataset with 4,445 frames and 3,105 IDs is provided by request.

Conclusion: BasketLiDAR is the first multimodal dataset in sports MOT and enables real-time, robust 3D tracking for basketball; the framework reduces computational costs and improves occlusion handling, enabling advances in tactical analysis, performance evaluation, and spectator experience; dataset availability facilitates future research.

Abstract: Real-time 3D trajectory player tracking in sports plays a crucial role in
tactical analysis, performance evaluation, and enhancing spectator experience.
Traditional systems rely on multi-camera setups, but are constrained by the
inherently two-dimensional nature of video data and the need for complex 3D
reconstruction processing, making real-time analysis challenging. Basketball,
in particular, represents one of the most difficult scenarios in the MOT field,
as ten players move rapidly and complexly within a confined court space, with
frequent occlusions caused by intense physical contact.
  To address these challenges, this paper constructs BasketLiDAR, the first
multimodal dataset in the sports MOT field that combines LiDAR point clouds
with synchronized multi-view camera footage in a professional basketball
environment, and proposes a novel MOT framework that simultaneously achieves
improved tracking accuracy and reduced computational cost. The BasketLiDAR
dataset contains a total of 4,445 frames and 3,105 player IDs, with fully
synchronized IDs between three LiDAR sensors and three multi-view cameras. We
recorded 5-on-5 and 3-on-3 game data from actual professional basketball
players, providing complete 3D positional information and ID annotations for
each player. Based on this dataset, we developed a novel MOT algorithm that
leverages LiDAR's high-precision 3D spatial information. The proposed method
consists of a real-time tracking pipeline using LiDAR alone and a multimodal
tracking pipeline that fuses LiDAR and camera data. Experimental results
demonstrate that our approach achieves real-time operation, which was difficult
with conventional camera-only methods, while achieving superior tracking
performance even under occlusion conditions. The dataset is available upon
request at: https://sites.google.com/keio.jp/keio-csg/projects/basket-lidar

</details>


### [26] [First RAG, Second SEG: A Training-Free Paradigm for Camouflaged Object Detection](https://arxiv.org/abs/2508.15313)
*Wutao Liu,YiDan Wang,Pan Gao*

Main category: cs.CV

TL;DR: A training-free COD method that uses Retrieval-Augmented Generation to create coarse prompt masks, refined by SAM-based segmentation (SAM2), enabling competitive performance on camouflaged object detection using a personal laptop.


<details>
  <summary>Details</summary>
Motivation: Camouflaged object detection is hard due to high similarity between objects and background. Traditional methods rely on heavy training and large compute; even foundation models like SAM need fine-tuning or high-quality prompts. There is a need for a lightweight, training-free approach that reduces computational demands.

Method: A two-stage framework: (1) Retrieval-Augmented Generation (RAG) builds a compact retrieval database via unsupervised clustering and uses retrieved features to generate pseudo-labels (coarse masks) as prompts; (2) SAM-based segmentation (SEG) refines these prompts to produce precise masks (SAM2). The approach is training-free and aims for efficiency on standard hardware.

Result: Extensive experiments on benchmark COD datasets show that RAG-SEG achieves performance on par with or better than state-of-the-art methods while running on a personal laptop. The method demonstrates computational efficiency and practicality, with additional analysis in the Appendix addressing limitations, extension to salient object detection, and potential improvements.

Conclusion: RAG-SEG offers a practical, training-free COD solution by decoupling retrieval-based prompt generation from SAM-based refinement, achieving competitive accuracy with reduced computational burden and broad potential for extension to related vision tasks.

Abstract: Camouflaged object detection (COD) poses a significant challenge in computer
vision due to the high similarity between objects and their backgrounds.
Existing approaches often rely on heavy training and large computational
resources. While foundation models such as the Segment Anything Model (SAM)
offer strong generalization, they still struggle to handle COD tasks without
fine-tuning and require high-quality prompts to yield good performance.
However, generating such prompts manually is costly and inefficient. To address
these challenges, we propose \textbf{First RAG, Second SEG (RAG-SEG)}, a
training-free paradigm that decouples COD into two stages: Retrieval-Augmented
Generation (RAG) for generating coarse masks as prompts, followed by SAM-based
segmentation (SEG) for refinement. RAG-SEG constructs a compact retrieval
database via unsupervised clustering, enabling fast and effective feature
retrieval. During inference, the retrieved features produce pseudo-labels that
guide precise mask generation using SAM2. Our method eliminates the need for
conventional training while maintaining competitive performance. Extensive
experiments on benchmark COD datasets demonstrate that RAG-SEG performs on par
with or surpasses state-of-the-art methods. Notably, all experiments are
conducted on a \textbf{personal laptop}, highlighting the computational
efficiency and practicality of our approach. We present further analysis in the
Appendix, covering limitations, salient object detection extension, and
possible improvements.

</details>


### [27] [VideoEraser: Concept Erasure in Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.15314)
*Naen Xu,Jinghuai Zhang,Changjiang Li,Zhi Chen,Chunyi Zhou,Qingming Li,Tianyu Du,Shouling Ji*

Main category: cs.CV

TL;DR: VideoEraser is a training-free, plug-and-play framework that uses SPEA and ARNG to suppress undesirable concepts in text-to-video diffusion models, achieving a 46% average reduction of problematic content across four erasure tasks.


<details>
  <summary>Details</summary>
Motivation: The rapid rise of text-to-video diffusion models raises privacy, copyright, and safety concerns due to training on unauthorised personal identities, artistic works, and harmful materials, risking uncontrolled generation and distribution.

Method: A two-stage approach: (1) Selective Prompt Embedding Adjustment (SPEA) to modify prompts to avoid undesired concepts, and (2) Adversarial-Resilient Noise Guidance (ARNG) to guide diffusion outputs away from undesired content. The framework is training-free and plug-and-play with representative T2V diffusion models.

Result: Extensive evaluations across four tasks—object erasure, artistic style erasure, celebrity erasure, and explicit content erasure—show VideoEraser outperforms baselines in efficacy, integrity, fidelity, robustness, and generalizability, achieving an average 46% reduction of undesirable content across tasks.

Conclusion: VideoEraser provides an effective, generalizable safety layer for T2V diffusion models without retraining, enabling safer deployment. The approach demonstrates strong practical potential for mitigating misuse in video generation.

Abstract: The rapid growth of text-to-video (T2V) diffusion models has raised concerns
about privacy, copyright, and safety due to their potential misuse in
generating harmful or misleading content. These models are often trained on
numerous datasets, including unauthorized personal identities, artistic
creations, and harmful materials, which can lead to uncontrolled production and
distribution of such content. To address this, we propose VideoEraser, a
training-free framework that prevents T2V diffusion models from generating
videos with undesirable concepts, even when explicitly prompted with those
concepts. Designed as a plug-and-play module, VideoEraser can seamlessly
integrate with representative T2V diffusion models via a two-stage process:
Selective Prompt Embedding Adjustment (SPEA) and Adversarial-Resilient Noise
Guidance (ARNG). We conduct extensive evaluations across four tasks, including
object erasure, artistic style erasure, celebrity erasure, and explicit content
erasure. Experimental results show that VideoEraser consistently outperforms
prior methods regarding efficacy, integrity, fidelity, robustness, and
generalizability. Notably, VideoEraser achieves state-of-the-art performance in
suppressing undesirable content during T2V generation, reducing it by 46% on
average across four tasks compared to baselines.

</details>


### [28] [Predicting Road Crossing Behaviour using Pose Detection and Sequence Modelling](https://arxiv.org/abs/2508.15336)
*Subhasis Dasgupta,Preetam Saha,Agniva Roy,Jaydip Sen*

Main category: cs.CV

TL;DR: GRU-based sequence modeling better predicts pedestrian crossing intent from pose sequences than LSTM, while 1D CNN provides faster inference; an end-to-end video→pose→prediction framework is feasible.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles require early and reliable prediction of pedestrians' crossing intentions to ensure safety and smooth navigation.

Method: Experimental study comparing three sequence models (GRU, LSTM, and 1D CNN) on pose-detection outputs derived from video, forming an end-to-end framework from video to crossing-prediction.

Result: GRU outperforms LSTM in predicting crossing intent; 1D CNN offers the fastest inference time; integrated pipeline from video to pose to sequence modeling is feasible.

Conclusion: The study shows that the choice of sequence model significantly affects prediction performance, with GRU providing better accuracy than LSTM and 1D CNN offering speed advantages; an end-to-end video-to-pose-to-prediction framework is viable for predicting road-crossing intents in autonomous driving systems.

Abstract: The world is constantly moving towards AI based systems and autonomous
vehicles are now reality in different parts of the world. These vehicles
require sensors and cameras to detect objects and maneuver according to that.
It becomes important to for such vehicles to also predict from a distant if a
person is about to cross a road or not. The current study focused on predicting
the intent of crossing the road by pedestrians in an experimental setup. The
study involved working with deep learning models to predict poses and sequence
modelling for temporal predictions. The study analysed three different sequence
modelling to understand the prediction behaviour and it was found out that GRU
was better in predicting the intent compared to LSTM model but 1D CNN was the
best model in terms of speed. The study involved video analysis, and the output
of pose detection model was integrated later on to sequence modelling
techniques for an end-to-end deep learning framework for predicting road
crossing intents.

</details>


### [29] [RCDINO: Enhancing Radar-Camera 3D Object Detection with DINOv2 Semantic Features](https://arxiv.org/abs/2508.15353)
*Olga Matykina,Dmitry Yudin*

Main category: cs.CV

TL;DR: RCDINO fuses radar with DINOv2-based semantic features to boost 3D object detection, achieving state-of-the-art radar-camera results on nuScenes (56.4 NDS, 48.1 mAP) while staying compatible with standard architectures.


<details>
  <summary>Details</summary>
Motivation: Need for robust multimodal fusion in 3D detection for autonomous systems; leveraging large pretrained vision models (DINOv2) to enrich visual features from cameras before fusion with radar.

Method: A multimodal transformer-based model that enriches visual backbone features by injecting semantically rich DINOv2 representations and fusing with radar data, preserving baseline architecture compatibility.

Result: On nuScenes, achieves 56.4 NDS and 48.1 mAP, state-of-the-art among radar-camera models; code at GitHub.

Conclusion: Demonstrates the effectiveness of combining pretrained semantic representations with multimodal fusion for improved radar-camera 3D detection; maintains compatibility and provides open-source implementation.

Abstract: Three-dimensional object detection is essential for autonomous driving and
robotics, relying on effective fusion of multimodal data from cameras and
radar. This work proposes RCDINO, a multimodal transformer-based model that
enhances visual backbone features by fusing them with semantically rich
representations from the pretrained DINOv2 foundation model. This approach
enriches visual representations and improves the model's detection performance
while preserving compatibility with the baseline architecture. Experiments on
the nuScenes dataset demonstrate that RCDINO achieves state-of-the-art
performance among radar-camera models, with 56.4 NDS and 48.1 mAP. Our
implementation is available at https://github.com/OlgaMatykina/RCDINO.

</details>


### [30] [An Empirical Study on How Video-LLMs Answer Video Questions](https://arxiv.org/abs/2508.15360)
*Chenhui Gou,Ziyu Ma,Zicheng Duan,Haoyu He,Feng Chen,Akide Liu,Bohan Zhuang,Jianfei Cai,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: Video-LLMs are analyzed via three attention knockout variants across layer windows to uncover internal mechanisms, revealing a two-stage early perceptual encoding followed by higher-level reasoning, with critical outlier layers in the middle, and a strong reliance on language-guided retrieval for spatial-temporal modeling; insights enable reduced attention computation.


<details>
  <summary>Details</summary>
Motivation: Understand the internal workings of Video-LLMs beyond performance gains, enabling interpretability and efficiency.

Method: Three knockout variants (Video Temporal Knockout, Video Spatial Knockout, Language-to-Video Knockout) applied over different layer windows; global and fine-grained settings to study information flow and layer importance.

Result: Global setting: early layers handle perceptual encoding; two-stage processing. Fine-grained setting: some intermediate layers act as outsized critical outliers. Spatial-temporal modeling relies more on language-guided retrieval than on self-attention among video tokens. Findings can guide reduction of attention computation.

Conclusion: First systematic study of internal processing in Video-LLMs; provides interpretability and efficiency implications for future Video-LLM research.

Abstract: Taking advantage of large-scale data and pretrained language models, Video
Large Language Models (Video-LLMs) have shown strong capabilities in answering
video questions. However, most existing efforts focus on improving performance,
with limited attention to understanding their internal mechanisms. This paper
aims to bridge this gap through a systematic empirical study. To interpret
existing VideoLLMs, we adopt attention knockouts as our primary analytical tool
and design three variants: Video Temporal Knockout, Video Spatial Knockout, and
Language-to-Video Knockout. Then, we apply these three knockouts on different
numbers of layers (window of layers). By carefully controlling the window of
layers and types of knockouts, we provide two settings: a global setting and a
fine-grained setting. Our study reveals three key findings: (1) Global setting
indicates Video information extraction primarily occurs in early layers,
forming a clear two-stage process -- lower layers focus on perceptual encoding,
while higher layers handle abstract reasoning; (2) In the fine-grained setting,
certain intermediate layers exert an outsized impact on video question
answering, acting as critical outliers, whereas most other layers contribute
minimally; (3) In both settings, we observe that spatial-temporal modeling
relies more on language-guided retrieval than on intra- and inter-frame
self-attention among video tokens, despite the latter's high computational
cost. Finally, we demonstrate that these insights can be leveraged to reduce
attention computation in Video-LLMs. To our knowledge, this is the first work
to systematically uncover how Video-LLMs internally process and understand
video content, offering interpretability and efficiency perspectives for future
research.

</details>


### [31] [Transfer learning optimization based on evolutionary selective fine tuning](https://arxiv.org/abs/2508.15367)
*Jacinto Colan,Ana Davila,Yasuhisa Hasegawa*

Main category: cs.CV

TL;DR: BioTune is an evolutionary adaptive fine-tuning method that selectively fine-tunes a subset of layers in a pretrained model using an evolutionary algorithm, aiming to improve transfer learning efficiency. It achieves competitive or better accuracy with fewer trainable parameters across nine image classification datasets, outperforming methods like AutoRGN and LoRA.


<details>
  <summary>Details</summary>
Motivation: To reduce computational cost and overfitting in fine-tuning large pretrained vision models by identifying which layers to adjust for a given target task.

Method: An evolutionary algorithm searches for an optimal subset of layers to fine-tune. The model is fine-tuned only on the selected layers, yielding improved efficiency and comparable or better accuracy relative to full fine-tuning and other methods.

Result: BioTune achieves competitive or improved accuracy and efficiency across nine diverse image classification datasets, with fewer trainable parameters than full fine-tuning and existing methods like AutoRGN and LoRA.

Conclusion: Selective, evolutionary fine-tuning of a subset of layers can enhance transfer learning efficiency without sacrificing performance, enabling more scalable adaptation of large pretrained models to diverse tasks.

Abstract: Deep learning has shown substantial progress in image analysis. However, the
computational demands of large, fully trained models remain a consideration.
Transfer learning offers a strategy for adapting pre-trained models to new
tasks. Traditional fine-tuning often involves updating all model parameters,
which can potentially lead to overfitting and higher computational costs. This
paper introduces BioTune, an evolutionary adaptive fine-tuning technique that
selectively fine-tunes layers to enhance transfer learning efficiency. BioTune
employs an evolutionary algorithm to identify a focused set of layers for
fine-tuning, aiming to optimize model performance on a given target task.
Evaluation across nine image classification datasets from various domains
indicates that BioTune achieves competitive or improved accuracy and efficiency
compared to existing fine-tuning methods such as AutoRGN and LoRA. By
concentrating the fine-tuning process on a subset of relevant layers, BioTune
reduces the number of trainable parameters, potentially leading to decreased
computational cost and facilitating more efficient transfer learning across
diverse data characteristics and distributions.

</details>


### [32] [Image-Conditioned 3D Gaussian Splat Quantization](https://arxiv.org/abs/2508.15372)
*Xinshuang Liu,Runfa Blark Li,Keito Suzuki,Truong Nguyen*

Main category: cs.CV

TL;DR: ICGS-Quantizer enables ultra-compressed 3D Gaussian Splatting with image-conditioned decoding and shared codebooks, achieving kilobyte storage and better adaptability than current methods.


<details>
  <summary>Details</summary>
Motivation: To overcome megabyte-scale limitations for large-scale 3DGS and lack of mechanisms to adapt archives to post-archival scene changes.

Method: Jointly trains encoding, quantization, and decoding of 3DGS using inter-Gaussian/inter-attribute correlations; uses shared codebooks across training scenes; decoding is conditioned on decoding-time images; per-scene codebooks are avoided; codes are learned to be effective for conditional decoding.

Result: Achieves storage reduction to kilobyte range while preserving visual fidelity; outperforms state-of-the-art in compression efficiency and adaptability; validated on 3D scene compression and updating; code/data will be released on GitHub.

Conclusion: ICGS-Quantizer offers a scalable, adaptable solution for archival 3DGS that supports large scenes and changes over time, with practical deployment via shared codebooks and image-conditioned decoding.

Abstract: 3D Gaussian Splatting (3DGS) has attracted considerable attention for
enabling high-quality real-time rendering. Although 3DGS compression methods
have been proposed for deployment on storage-constrained devices, two
limitations hinder archival use: (1) they compress medium-scale scenes only to
the megabyte range, which remains impractical for large-scale scenes or
extensive scene collections; and (2) they lack mechanisms to accommodate scene
changes after long-term archival. To address these limitations, we propose an
Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer) that substantially
enhances compression efficiency and provides adaptability to scene changes
after archiving. ICGS-Quantizer improves quantization efficiency by jointly
exploiting inter-Gaussian and inter-attribute correlations and by using shared
codebooks across all training scenes, which are then fixed and applied to
previously unseen test scenes, eliminating the overhead of per-scene codebooks.
This approach effectively reduces the storage requirements for 3DGS to the
kilobyte range while preserving visual fidelity. To enable adaptability to
post-archival scene changes, ICGS-Quantizer conditions scene decoding on images
captured at decoding time. The encoding, quantization, and decoding processes
are trained jointly, ensuring that the codes, which are quantized
representations of the scene, are effective for conditional decoding. We
evaluate ICGS-Quantizer on 3D scene compression and 3D scene updating.
Experimental results show that ICGS-Quantizer consistently outperforms
state-of-the-art methods in compression efficiency and adaptability to scene
changes. Our code, model, and data will be publicly available on GitHub.

</details>


### [33] [DriveSplat: Decoupled Driving Scene Reconstruction with Geometry-enhanced Partitioned Neural Gaussians](https://arxiv.org/abs/2508.15376)
*Cong Wang,Xianda Guo,Wenbo Xu,Wei Tian,Ruiqi Song,Chenming Zhang,Lingxi Li,Long Chen*

Main category: cs.CV

TL;DR: DriveSplat introduces neural Gaussian representations with dynamic-static decoupling and region-wise initialization for driving scenes, using deformable Gaussians and depth/normal priors to improve geometry and novel-view synthesis, achieving state-of-the-art results on Waymo and KITTI.


<details>
  <summary>Details</summary>
Motivation: Driving scenarios contain fast-moving dynamic objects with a dominant static background, which challenges 3D reconstruction. Existing decode- and fitting-based Gaussian methods decouple dynamics but neglect robust background geometry and rely on per-view fitting, limiting rendering quality and geometric accuracy.

Method: Propose DriveSplat: a neural Gaussian representation with dynamic-static decoupling. Use region-wise voxel initialization (near, middle, far) to reflect mostly linear motion in driving viewpoints. Introduce deformable neural Gaussians to model non-rigid dynamic actors, with a learnable deformation network. Supervise geometry with depth and normal priors from pre-trained models to improve structure accuracy.

Result: Evaluations on Waymo and KITTI datasets show state-of-the-art performance in novel-view synthesis for driving scenes.

Conclusion: DriveSplat improves geometric accuracy and rendering robustness for driving scenarios by combining dynamic-static decoupling, region-aware initialization, deformable Gaussians, and geometric priors, demonstrating strong performance on standard driving benchmarks.

Abstract: In the realm of driving scenarios, the presence of rapidly moving vehicles,
pedestrians in motion, and large-scale static backgrounds poses significant
challenges for 3D scene reconstruction. Recent methods based on 3D Gaussian
Splatting address the motion blur problem by decoupling dynamic and static
components within the scene. However, these decoupling strategies overlook
background optimization with adequate geometry relationships and rely solely on
fitting each training view by adding Gaussians. Therefore, these models exhibit
limited robustness in rendering novel views and lack an accurate geometric
representation. To address the above issues, we introduce DriveSplat, a
high-quality reconstruction method for driving scenarios based on neural
Gaussian representations with dynamic-static decoupling. To better accommodate
the predominantly linear motion patterns of driving viewpoints, a region-wise
voxel initialization scheme is employed, which partitions the scene into near,
middle, and far regions to enhance close-range detail representation.
Deformable neural Gaussians are introduced to model non-rigid dynamic actors,
whose parameters are temporally adjusted by a learnable deformation network.
The entire framework is further supervised by depth and normal priors from
pre-trained models, improving the accuracy of geometric structures. Our method
has been rigorously evaluated on the Waymo and KITTI datasets, demonstrating
state-of-the-art performance in novel-view synthesis for driving scenarios.

</details>


### [34] [DIO: Refining Mutual Information and Causal Chain to Enhance Machine Abstract Reasoning Ability](https://arxiv.org/abs/2508.15387)
*Ruizhuo Song,Beiming Yuan*

Main category: cs.CV

TL;DR: A study on solving Raven's Progressive Matrices (RPM) using causal-chain analysis, showing that the DIO baseline's mutual-information objective fails to capture human-like abstract reasoning and proposing three progressive improvements.


<details>
  <summary>Details</summary>
Motivation: RPMs are a benchmark for abstract reasoning in deep learning; current models struggle with abstract reasoning tasks. MI-based objectives and lower-bound tightness hinder learning of true causal reasoning, and MI does not inherently capture subject–object causality.

Method: Adopts a causal-chain modeling perspective: image -> abstract attributes -> progressive attribute patterns -> pattern consistency -> correct answer; designs the DIO baseline network; analyzes why the MI objective (maximizing the variational lower bound of mutual information) fails to enforce human-like reasoning; introduces three progressive improvement methods.

Result: Experiments indicate that maximizing the variational lower bound of MI does not induce the intended human-like reasoning due to tight lower-bound issues and the inadequacy of MI to encode causal relations. The paper therefore proposes three improvement methods to address these limitations.

Conclusion: The work highlights a gap between statistical MI-based optimization and causal reasoning in RPM tasks and outlines three steps to realign deep models with human-like abstract reasoning capabilities.

Abstract: Despite the outstanding performance of current deep learning models across
various domains, their fundamental bottleneck in abstract reasoning remains
unresolved. To address this challenge, the academic community has introduced
Raven's Progressive Matrices (RPM) problems as an authoritative benchmark for
evaluating the abstract reasoning capabilities of deep learning algorithms,
with a focus on core intelligence dimensions such as abstract reasoning,
pattern recognition, and complex problem-solving. Therefore, this paper centers
on solving RPM problems, aiming to contribute to enhancing the abstract
reasoning abilities of machine intelligence. Firstly, this paper adopts a
``causal chain modeling'' perspective to systematically analyze the complete
causal chain in RPM tasks: image $\rightarrow$ abstract attributes
$\rightarrow$ progressive attribute patterns $\rightarrow$ pattern consistency
$\rightarrow$ correct answer. Based on this analysis, the network architecture
of the baseline model DIO is designed. However, experiments reveal that the
optimization objective formulated for DIO, namely maximizing the variational
lower bound of mutual information between the context and the correct option,
fails to enable the model to genuinely acquire the predefined human reasoning
logic. This is attributed to two main reasons: the tightness of the lower bound
significantly impacts the effectiveness of mutual information maximization, and
mutual information, as a statistical measure, does not capture the causal
relationship between subjects and objects. To overcome these limitations, this
paper progressively proposes three improvement methods:

</details>


### [35] [Spiking Variational Graph Representation Inference for Video Summarization](https://arxiv.org/abs/2508.15389)
*Wenrui Li,Wei Han,Liang-Jian Deng,Ruiqin Xiong,Xiaopeng Fan*

Main category: cs.CV

TL;DR: Proposes SpiVG: a Spiking Variational Graph network for video summarization that uses SNN-based keyframe extraction, dynamic graph reasoning, and variational reconstruction to handle global temporal dependencies and noise in multi-channel features, achieving state-of-the-art performance on SumMe, TVSum, VideoXum, QFVS.


<details>
  <summary>Details</summary>
Motivation: Current video summarization methods struggle to capture global temporal dependencies and are sensitive to noise during multi-channel feature fusion, limiting semantic coherence and efficiency.

Method: Keyframe extractor based on Spiking Neural Networks for autonomous keyframe learning; Dynamic Aggregation Graph Reasoner for fine-grained contextual reasoning separating object consistency from semantic coherence; Variational Inference Reconstruction Module using ELBO and posterior regularization to model latent multi-channel feature distributions and mitigate noise.

Result: SpiVG outperforms existing methods on multiple datasets (SumMe, TVSum, VideoXum, QFVS); code and pre-trained models are publicly available.

Conclusion: SpiVG demonstrates improved information density and reduced computational complexity for video summarization, with robust performance across datasets and a modular framework that addresses global temporal dependencies and noisy multi-channel fusion.

Abstract: With the rise of short video content, efficient video summarization
techniques for extracting key information have become crucial. However,
existing methods struggle to capture the global temporal dependencies and
maintain the semantic coherence of video content. Additionally, these methods
are also influenced by noise during multi-channel feature fusion. We propose a
Spiking Variational Graph (SpiVG) Network, which enhances information density
and reduces computational complexity. First, we design a keyframe extractor
based on Spiking Neural Networks (SNN), leveraging the event-driven computation
mechanism of SNNs to learn keyframe features autonomously. To enable
fine-grained and adaptable reasoning across video frames, we introduce a
Dynamic Aggregation Graph Reasoner, which decouples contextual object
consistency from semantic perspective coherence. We present a Variational
Inference Reconstruction Module to address uncertainty and noise arising during
multi-channel feature fusion. In this module, we employ Evidence Lower Bound
Optimization (ELBO) to capture the latent structure of multi-channel feature
distributions, using posterior distribution regularization to reduce
overfitting. Experimental results show that SpiVG surpasses existing methods
across multiple datasets such as SumMe, TVSum, VideoXum, and QFVS. Our codes
and pre-trained models are available at https://github.com/liwrui/SpiVG.

</details>


### [36] [From Linearity to Non-Linearity: How Masked Autoencoders Capture Spatial Correlations](https://arxiv.org/abs/2508.15404)
*Anthony Bisulco,Rahul Ramesh,Randall Balestriero,Pratik Chaudhari*

Main category: cs.CV

TL;DR: MAEs' hyperparameters shape the spatial scale of learned features: linear MAEs reveal how masking ratio and patch size bias toward short- vs long-range correlations; nonlinear MAEs adapt to spatial statistics beyond second-order; provides practical hyperparameter guidance.


<details>
  <summary>Details</summary>
Motivation: To understand how MAE hyperparameters influence the kinds of spatial correlations learned and how this affects downstream performance, bridging theory and practical tuning.

Method: The authors analytically derive feature learning in a linear MAE to relate masking ratio and patch size to captured spatial correlations; they extend the analysis to non-linear MAEs to show adaptation to dataset spatial statistics beyond second-order.

Result: Masking ratio and patch size selectively promote features that capture short- and long-range spatial correlations; nonlinear MAEs adapt representations to the dataset's spatial statistics beyond second-order; implications for hyperparameter selection.

Conclusion: The study provides actionable insights for choosing MAE hyperparameters by aligning them with the desired spatial correlation structure in the data, reducing trial-and-error in practice.

Abstract: Masked Autoencoders (MAEs) have emerged as a powerful pretraining technique
for vision foundation models. Despite their effectiveness, they require
extensive hyperparameter tuning (masking ratio, patch size, encoder/decoder
layers) when applied to novel datasets. While prior theoretical works have
analyzed MAEs in terms of their attention patterns and hierarchical latent
variable models, the connection between MAE hyperparameters and performance on
downstream tasks is relatively unexplored. This work investigates how MAEs
learn spatial correlations in the input image. We analytically derive the
features learned by a linear MAE and show that masking ratio and patch size can
be used to select for features that capture short- and long-range spatial
correlations. We extend this analysis to non-linear MAEs to show that MAE
representations adapt to spatial correlations in the dataset, beyond
second-order statistics. Finally, we discuss some insights on how to select MAE
hyper-parameters in practice.

</details>


### [37] [Bidirectional Temporal Information Propagation for Moving Infrared Small Target Detection](https://arxiv.org/abs/2508.15415)
*Dengyan Luo,Yanping Xiang,Hu Wang,Luping Ji. Shuai Li,Mao Ye*

Main category: cs.CV

TL;DR: Bidirectional temporal propagation (BIRD) for moving infrared small target detection; uses Local Temporal Motion Fusion (LTMF) and Global Temporal Motion Fusion (GTMF) in forward/backward branches and an STF loss; achieves state-of-the-art performance with fast inference.


<details>
  <summary>Details</summary>
Motivation: Sliding-window multi-frame approaches ignore global temporal information and can incur redundant computation; joint optimization over the entire video could improve both accuracy and efficiency.

Method: Introduce LTMF to model local spatio-temporal dependencies between a frame and its two neighbors, followed by GTMF to integrate global propagation features; run in both forward and backward propagation branches; fuse bidirectional features and feed to detection head; train with standard detection loss plus Spatio-Temporal Fusion (STF) loss for joint optimization over the video clip.

Result: Empirical results indicate state-of-the-art detection accuracy and fast inference speed.

Conclusion: BIRD effectively leverages both local and global temporal information, outperforming sliding-window methods and offering a computationally efficient solution for infrared small target detection.

Abstract: Moving infrared small target detection is broadly adopted in infrared search
and track systems, and has attracted considerable research focus in recent
years. The existing learning-based multi-frame methods mainly aggregate the
information of adjacent frames in a sliding window fashion to assist the
detection of the current frame. However, the sliding-window-based methods do
not consider joint optimization of the entire video clip and ignore the global
temporal information outside the sliding window, resulting in redundant
computation and sub-optimal performance. In this paper, we propose a
Bidirectional temporal information propagation method for moving InfraRed small
target Detection, dubbed BIRD. The bidirectional propagation strategy
simultaneously utilizes local temporal information of adjacent frames and
global temporal information of past and future frames in a recursive fashion.
Specifically, in the forward and backward propagation branches, we first design
a Local Temporal Motion Fusion (LTMF) module to model local spatio-temporal
dependency between a target frame and its two adjacent frames. Then, a Global
Temporal Motion Fusion (GTMF) module is developed to further aggregate the
global propagation feature with the local fusion feature. Finally, the
bidirectional aggregated features are fused and input into the detection head
for detection. In addition, the entire video clip is jointly optimized by the
traditional detection loss and the additional Spatio-Temporal Fusion (STF)
loss. Extensive experiments demonstrate that the proposed BIRD method not only
achieves the state-of-the-art performance but also shows a fast inference
speed.

</details>


### [38] [The Impact of Image Resolution on Face Detection: A Comparative Analysis of MTCNN, YOLOv XI and YOLOv XII models](https://arxiv.org/abs/2507.23341)
*Ahmet Can Ömercikoğlu,Mustafa Mansur Yönügül,Pakize Erdoğmuş*

Main category: cs.CV

TL;DR: Resolution profoundly affects face detector performance; YOLOv11 achieves the best accuracy at larger input sizes, YOLOv12 shows a slight edge in recall, and MTCNN provides strong landmark localization but is slower for real-time use.


<details>
  <summary>Details</summary>
Motivation: To understand how input resolution impacts accuracy and robustness of leading deep learning-based face detectors, guiding model choice under real-world constraints.

Method: Systematic evaluation on the WIDER FACE dataset across three detectors (YOLOv11, YOLOv12, MTCNN) and three input resolutions (160x160, 320x320, 640x640), using metrics such as precision, recall, mAP50, mAP50-95, and inference time.

Result: YOLOv11 generally outperforms YOLOv12 and MTCNN in detection accuracy, especially at higher resolutions; YOLOv12 shows slightly better recall; MTCNN is competitive for landmark localization but trails in real-time inference speed.

Conclusion: The study offers actionable guidance for selecting resolution-aware face detectors under varying operational constraints and highlights the importance of aligning input resolution with the desired accuracy-speed tradeoffs.

Abstract: Face detection is a crucial component in many AI-driven applications such as
surveillance, biometric authentication, and human-computer interaction.
However, real-world conditions like low-resolution imagery present significant
challenges that degrade detection performance. In this study, we systematically
investigate the impact of input resolution on the accuracy and robustness of
three prominent deep learning-based face detectors: YOLOv11, YOLOv12, and
MTCNN. Using the WIDER FACE dataset, we conduct extensive evaluations across
multiple image resolutions (160x160, 320x320, and 640x640) and assess each
model's performance using metrics such as precision, recall, mAP50, mAP50-95,
and inference time. Results indicate that YOLOv11 outperforms YOLOv12 and MTCNN
in terms of detection accuracy, especially at higher resolutions, while YOLOv12
exhibits slightly better recall. MTCNN, although competitive in landmark
localization, lags in real-time inference speed. Our findings provide
actionable insights for selecting resolution-aware face detection models
suitable for varying operational constraints.

</details>


### [39] [A Curated Dataset and Deep Learning Approach for Minor Dent Detection in Vehicles](https://arxiv.org/abs/2508.15431)
*Danish Zia Baig,Mohsin Kamal*

Main category: cs.CV

TL;DR: YOLOv8m-t42 detects microscopic car surface dents with high precision/recall and real-time performance, outperforming YOLOv8m-t4 despite longer convergence.


<details>
  <summary>Details</summary>
Motivation: Automate and speed up car damage inspection to detect tiny surface flaws that manual methods miss.

Method: Create a bespoke annotated dataset of car surfaces under varying lighting, angles, and textures; train YOLOv8m and variants YOLOv8m-t4 and YOLOv8m-t42 with real-time data augmentation; evaluate using mAP, precision, recall, F1; compare models; report PR curves.

Result: YOLOv8m-t42 achieved precision 0.86, recall 0.84, F1 0.85; YOLOv8m-t4 achieved 0.81, 0.79, 0.80; mAP@0.5:0.95 reduced to 0.20 but stabilized at 0.60 for YOLOv8m-t42; PR area 0.88 vs 0.82; overall better accuracy and robustness; suitable for real-time dent detection.

Conclusion: YOLOv8m-t42 provides higher accuracy and robustness for microscopic dent detection in real-world conditions, making it more appropriate for practical deployment though it converges more slowly.

Abstract: Conventional car damage inspection techniques are labor-intensive, manual,
and frequently overlook tiny surface imperfections like microscopic dents.
Machine learning provides an innovative solution to the increasing demand for
quicker and more precise inspection methods. The paper uses the YOLOv8 object
recognition framework to provide a deep learning-based solution for
automatically detecting microscopic surface flaws, notably tiny dents, on car
exteriors. Traditional automotive damage inspection procedures are manual,
time-consuming, and frequently unreliable at detecting tiny flaws. To solve
this, a bespoke dataset containing annotated photos of car surfaces under
various lighting circumstances, angles, and textures was created. To improve
robustness, the YOLOv8m model and its customized variants, YOLOv8m-t4 and
YOLOv8m-t42, were trained employing real-time data augmentation approaches.
Experimental results show that the technique has excellent detection accuracy
and low inference latency, making it suited for real-time applications such as
automated insurance evaluations and automobile inspections. Evaluation
parameters such as mean Average Precision (mAP), precision, recall, and
F1-score verified the model's efficacy. With a precision of 0.86, recall of
0.84, and F1-score of 0.85, the YOLOv8m-t42 model outperformed the YOLOv8m-t4
model (precision: 0.81, recall: 0.79, F1-score: 0.80) in identifying
microscopic surface defects. With a little reduced mAP@0.5:0.95 of 0.20, the
mAP@0.5 for YOLOv8m-t42 stabilized at 0.60. Furthermore, YOLOv8m-t42's PR curve
area was 0.88, suggesting more consistent performance than YOLOv8m-t4 (0.82).
YOLOv8m-t42 has greater accuracy and is more appropriate for practical dent
detection applications, even though its convergence is slower.

</details>


### [40] [Aligning Moments in Time using Video Queries](https://arxiv.org/abs/2508.15439)
*Yogesh Kumar,Uday Agarwal,Manish Gupta,Anand Mishra*

Main category: cs.CV

TL;DR: Introduces MATR, a transformer-based Vid2VidMR model with dual-stage alignment and self-supervised pretraining, achieving major gains on ActivityNet-VRL and SportsMoments.


<details>
  <summary>Details</summary>
Motivation: The task requires precise semantic frame-level alignment and modeling of complex dependencies between query and target videos; existing methods struggle with boundary precision and contextual reasoning.

Method: MATR uses a transformer that conditions target video representations on query features through dual-stage sequence alignment to capture correlations; includes foreground/background classification and boundary prediction heads; equipped with self-supervised pretraining by localizing random clips within videos.

Result: Achieves significant gains: 13.1% R@1 and 8.1% mIoU on ActivityNet-VRL; 14.7% R@1 and 14.4% mIoU on SportsMoments, versus strong baselines.

Conclusion: MATR effectively models semantic context and temporal details for precise moment localization; self-supervised pretraining provides strong initialization and substantial performance gains across datasets.

Abstract: Video-to-video moment retrieval (Vid2VidMR) is the task of localizing unseen
events or moments in a target video using a query video. This task poses
several challenges, such as the need for semantic frame-level alignment and
modeling complex dependencies between query and target videos. To tackle this
challenging problem, we introduce MATR (Moment Alignment TRansformer), a
transformer-based model designed to capture semantic context as well as the
temporal details necessary for precise moment localization. MATR conditions
target video representations on query video features using dual-stage sequence
alignment that encodes the required correlations and dependencies. These
representations are then used to guide foreground/background classification and
boundary prediction heads, enabling the model to accurately identify moments in
the target video that semantically match with the query video. Additionally, to
provide a strong task-specific initialization for MATR, we propose a
self-supervised pre-training technique that involves training the model to
localize random clips within videos. Extensive experiments demonstrate that
MATR achieves notable performance improvements of 13.1% in R@1 and 8.1% in mIoU
on an absolute scale compared to state-of-the-art methods on the popular
ActivityNet-VRL dataset. Additionally, on our newly proposed dataset,
SportsMoments, MATR shows a 14.7% gain in R@1 and a 14.4% gain in mIoU on an
absolute scale over strong baselines.

</details>


### [41] [Enhancing Novel View Synthesis from extremely sparse views with SfM-free 3D Gaussian Splatting Framework](https://arxiv.org/abs/2508.15457)
*Zongqi He,Hanmin Li,Kin-Chung Chan,Yushen Zuo,Hao Xie,Zhe Xiao,Jun Xiao,Kin-Man Lam*

Main category: cs.CV

TL;DR: An SfM-free 3D Gaussian Splatting framework for extremely sparse views that jointly estimates camera poses and reconstructs 3D scenes, using a dense stereo module for initialization, a coherent view interpolation module for supervision, and multi-scale regularization to boost geometry and rendering quality, achieving state-of-the-art results with as few as two training views.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3DGS) typically needs dense multi-view inputs with accurate camera poses; when views are extremely sparse, SfM-based initialization fails, leading to poor geometry and rendering. There is a need for an SfM-free approach that can operate under extreme sparsity.

Method: Introduce a dense stereo module to progressively estimate camera poses and build a global dense point cloud for initialization. Employ a coherent view interpolation module to generate viewpoint-consistent supervision signals from training view pairs. Apply multi-scale Laplacian consistent regularization and adaptive spatial-aware multi-scale geometry regularization to improve geometric fidelity and render quality.

Result: The method significantly outperforms state-of-the-art 3DGS-based approaches under extremely sparse views, achieving a 2.75dB PSNR improvement using only 2 training views, with minimal distortion and preserved high-frequency details in rendered images.

Conclusion: An SfM-free 3DGS framework can robustly handle extremely sparse-view scenarios by jointly estimating poses and reconstructing scenes, supported by coherent view supervision and strong multi-scale geometry regularization, leading to superior visual quality and quantitative performance.

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated remarkable real-time
performance in novel view synthesis, yet its effectiveness relies heavily on
dense multi-view inputs with precisely known camera poses, which are rarely
available in real-world scenarios. When input views become extremely sparse,
the Structure-from-Motion (SfM) method that 3DGS depends on for initialization
fails to accurately reconstruct the 3D geometric structures of scenes,
resulting in degraded rendering quality. In this paper, we propose a novel
SfM-free 3DGS-based method that jointly estimates camera poses and reconstructs
3D scenes from extremely sparse-view inputs. Specifically, instead of SfM, we
propose a dense stereo module to progressively estimates camera pose
information and reconstructs a global dense point cloud for initialization. To
address the inherent problem of information scarcity in extremely sparse-view
settings, we propose a coherent view interpolation module that interpolates
camera poses based on training view pairs and generates viewpoint-consistent
content as additional supervision signals for training. Furthermore, we
introduce multi-scale Laplacian consistent regularization and adaptive
spatial-aware multi-scale geometry regularization to enhance the quality of
geometrical structures and rendered content. Experiments show that our method
significantly outperforms other state-of-the-art 3DGS-based approaches,
achieving a remarkable 2.75dB improvement in PSNR under extremely sparse-view
conditions (using only 2 training views). The images synthesized by our method
exhibit minimal distortion while preserving rich high-frequency details,
resulting in superior visual quality compared to existing techniques.

</details>


### [42] [LGMSNet: Thinning a medical image segmentation model via dual-level multiscale fusion](https://arxiv.org/abs/2508.15476)
*Chengqi Dong,Fenghe Tang,Rongge Mao,Xinpei Gao,S. Kevin Zhou*

Main category: cs.CV

TL;DR: LGMSNet is a lightweight, local-global dual multiscale segmentation framework that uses heterogeneous intra-layer kernels and sparse transformer-convolution branches to achieve state-of-the-art accuracy with low computational cost, including strong zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Resource-constrained clinical settings require efficient, accurate segmentation models. Existing lightweight models often sacrifice performance and rarely leverage attention mechanisms, while channel redundancy under identical convolutions impedes effective feature extraction.

Method: LGMSNet introduces a local-global dual multiscale design with heterogeneous intra-layer kernels to extract local high-frequency information and mitigate channel redundancy, plus sparse transformer-convolution branches to capture global information.

Result: Outperforms existing state-of-the-art methods on six public datasets and demonstrates strong zero-shot generalization on four unseen datasets; code is publicly available.

Conclusion: LGMSNet shows strong potential for real-world deployment in resource-limited medical imaging and offers a viable path for combining efficiency with global context awareness.

Abstract: Medical image segmentation plays a pivotal role in disease diagnosis and
treatment planning, particularly in resource-constrained clinical settings
where lightweight and generalizable models are urgently needed. However,
existing lightweight models often compromise performance for efficiency and
rarely adopt computationally expensive attention mechanisms, severely
restricting their global contextual perception capabilities. Additionally,
current architectures neglect the channel redundancy issue under the same
convolutional kernels in medical imaging, which hinders effective feature
extraction. To address these challenges, we propose LGMSNet, a novel
lightweight framework based on local and global dual multiscale that achieves
state-of-the-art performance with minimal computational overhead. LGMSNet
employs heterogeneous intra-layer kernels to extract local high-frequency
information while mitigating channel redundancy. In addition, the model
integrates sparse transformer-convolutional hybrid branches to capture
low-frequency global information. Extensive experiments across six public
datasets demonstrate LGMSNet's superiority over existing state-of-the-art
methods. In particular, LGMSNet maintains exceptional performance in zero-shot
generalization tests on four unseen datasets, underscoring its potential for
real-world deployment in resource-limited medical scenarios. The whole project
code is in https://github.com/cq-dong/LGMSNet.

</details>


### [43] [MExECON: Multi-view Extended Explicit Clothed humans Optimized via Normal integration](https://arxiv.org/abs/2508.15500)
*Fulden Ece Uğur,Rafael Redondo,Albert Barreiro,Stefan Hristov,Roger Marí*

Main category: cs.CV

TL;DR: MExECON introduces a multi-view 3D reconstruction pipeline for clothed human avatars by jointly fitting a single SMPL-X model across views and refining with normal maps, achieving better fidelity without retraining and competitive few-shot performance.


<details>
  <summary>Details</summary>
Motivation: Single-view methods struggle with geometry, pose accuracy, and clothing detail; leveraging multiple views can enforce multi-view consistency and provide richer surface information while avoiding extensive network retraining.

Method: Joint Multi-view Body Optimization (JMBO) fits one SMPL-X body model across all input views to enforce cross-view consistency, uses this body as a low-frequency prior to guide surface reconstruction, and adds fine details via normal map integration from front and back views without retraining networks.

Result: Experiments show MExECON improves fidelity over the single-view ECON baseline and is competitive with modern few-shot 3D reconstruction methods.

Conclusion: Multi-view optimization combined with normal-map-based detail integration yields higher-quality clothed human avatars without requiring additional training, validating the approach against single-view baselines and few-shot methods.

Abstract: This work presents MExECON, a novel pipeline for 3D reconstruction of clothed
human avatars from sparse multi-view RGB images. Building on the single-view
method ECON, MExECON extends its capabilities to leverage multiple viewpoints,
improving geometry and body pose estimation. At the core of the pipeline is the
proposed Joint Multi-view Body Optimization (JMBO) algorithm, which fits a
single SMPL-X body model jointly across all input views, enforcing multi-view
consistency. The optimized body model serves as a low-frequency prior that
guides the subsequent surface reconstruction, where geometric details are added
via normal map integration. MExECON integrates normal maps from both front and
back views to accurately capture fine-grained surface details such as clothing
folds and hairstyles. All multi-view gains are achieved without requiring any
network re-training. Experimental results show that MExECON consistently
improves fidelity over the single-view baseline and achieves competitive
performance compared to modern few-shot 3D reconstruction methods.

</details>


### [44] [Task-Generalized Adaptive Cross-Domain Learning for Multimodal Image Fusion](https://arxiv.org/abs/2508.15505)
*Mengyu Wang,Zhenyu Liu,Kun Li,Yu Wang,Yuwei Wang,Yanyan Wei,Fei Wang*

Main category: cs.CV

TL;DR: AdaSFFuse is a task-general multimodal image fusion framework that uses adaptive wavelet-based frequency decoupling (AdaWAT) and Spatial-Frequency Mamba Blocks to robustly fuse diverse modalities across spatial and frequency domains, achieving superior results with low computation on IVF, MFF, MEF, and MIF.


<details>
  <summary>Details</summary>
Motivation: Current MMIF approaches suffer from modality misalignment, loss of high-frequency details, and task-specific limitations, motivating a general, efficient fusion framework that can adapt across domains and modalities.

Method: AdaSFFuse introduces Adaptive Approximate Wavelet Transform (AdaWAT) to adaptively decouple frequency components per modality and scene, plus Spatial-Frequency Mamba Blocks that fuse features across spatial and frequency domains using learnable mappings for robust cross-domain fusion.

Result: Extensive experiments on four MMIF tasks (IVF, MFF, MEF, MIF) show superior fusion performance with low computational cost and a compact network, validating robustness and efficiency.

Conclusion: AdaSFFuse achieves improved alignment and integration of multimodal features, mitigates frequency loss, and preserves detail across diverse MMIF tasks, offering a strong performance–efficiency balance and generalizability, with code to be released.

Abstract: Multimodal Image Fusion (MMIF) aims to integrate complementary information
from different imaging modalities to overcome the limitations of individual
sensors. It enhances image quality and facilitates downstream applications such
as remote sensing, medical diagnostics, and robotics. Despite significant
advancements, current MMIF methods still face challenges such as modality
misalignment, high-frequency detail destruction, and task-specific limitations.
To address these challenges, we propose AdaSFFuse, a novel framework for
task-generalized MMIF through adaptive cross-domain co-fusion learning.
AdaSFFuse introduces two key innovations: the Adaptive Approximate Wavelet
Transform (AdaWAT) for frequency decoupling, and the Spatial-Frequency Mamba
Blocks for efficient multimodal fusion. AdaWAT adaptively separates the high-
and low-frequency components of multimodal images from different scenes,
enabling fine-grained extraction and alignment of distinct frequency
characteristics for each modality. The Spatial-Frequency Mamba Blocks
facilitate cross-domain fusion in both spatial and frequency domains, enhancing
this process. These blocks dynamically adjust through learnable mappings to
ensure robust fusion across diverse modalities. By combining these components,
AdaSFFuse improves the alignment and integration of multimodal features,
reduces frequency loss, and preserves critical details. Extensive experiments
on four MMIF tasks -- Infrared-Visible Image Fusion (IVF), Multi-Focus Image
Fusion (MFF), Multi-Exposure Image Fusion (MEF), and Medical Image Fusion (MIF)
-- demonstrate AdaSFFuse's superior fusion performance, ensuring both low
computational cost and a compact network, offering a strong balance between
performance and efficiency. The code will be publicly available at
https://github.com/Zhen-yu-Liu/AdaSFFuse.

</details>


### [45] [ExtraGS: Geometric-Aware Trajectory Extrapolation with Uncertainty-Guided Generative Priors](https://arxiv.org/abs/2508.15529)
*Kaiyuan Tan,Yingying Shen,Haohui Zhu,Zhiwei Zhan,Shan Zhao,Mingfei Tu,Hongcheng Luo,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye*

Main category: cs.CV

TL;DR: ExtraGS introduces a holistic extrapolation framework that fuses geometric priors with generative priors for extrapolated driving views. It uses Road Surface Gaussian (RSG) built on a hybrid Gaussian-SDF representation and Far Field Gaussians (FFG) with learnable scaling, plus self-supervised uncertainty via spherical harmonics to selectively apply priors. Demonstrates improved realism and geometric consistency across datasets and setups while preserving fidelity on the original trajectory.


<details>
  <summary>Details</summary>
Motivation: Current extrapolated driving view methods rely on generative priors, which can produce geometrically inconsistent and over-smoothed results. There is a need to consistently integrate geometric cues with generative priors to synthesize realistic, artifact-free extrapolated views.

Method: Introduce Road Surface Gaussian (RSG) using a hybrid Gaussian-SDF representation to model road surfaces; implement Far Field Gaussians (FFG) with learnable scaling to efficiently handle distant objects; develop a self-supervised uncertainty estimation framework based on spherical harmonics to identify extrapolation artifacts and selectively fuse generative priors only where needed.

Result: Extensive experiments across multiple datasets and multi-camera configurations show that ExtraGS improves realism and geometric consistency of extrapolated views and preserves high fidelity along the original trajectory when using diverse generative priors.

Conclusion: ExtraGS demonstrates that integrating geometric priors, a novel RSG/FFG representation, and uncertainty-guided selective use of generative priors yields more realistic and geometrically coherent extrapolated driving views without sacrificing fidelity on the original path.

Abstract: Synthesizing extrapolated views from recorded driving logs is critical for
simulating driving scenes for autonomous driving vehicles, yet it remains a
challenging task. Recent methods leverage generative priors as pseudo ground
truth, but often lead to poor geometric consistency and over-smoothed
renderings. To address these limitations, we propose ExtraGS, a holistic
framework for trajectory extrapolation that integrates both geometric and
generative priors. At the core of ExtraGS is a novel Road Surface Gaussian(RSG)
representation based on a hybrid Gaussian-Signed Distance Function (SDF)
design, and Far Field Gaussians (FFG) that use learnable scaling factors to
efficiently handle distant objects. Furthermore, we develop a self-supervised
uncertainty estimation framework based on spherical harmonics that enables
selective integration of generative priors only where extrapolation artifacts
occur. Extensive experiments on multiple datasets, diverse multi-camera setups,
and various generative priors demonstrate that ExtraGS significantly enhances
the realism and geometric consistency of extrapolated views, while preserving
high fidelity along the original trajectory.

</details>


### [46] [Multi-Object Sketch Animation with Grouping and Motion Trajectory Priors](https://arxiv.org/abs/2508.15535)
*Guotao Liang,Juncheng Hu,Ximing Xing,Jing Zhang,Qian Yu*

Main category: cs.CV

TL;DR: A two-stage GroupSketch framework using interactive grouping and a Group-based Displacement Network to animate multi-object sketches with improved temporal consistency.


<details>
  <summary>Details</summary>
Motivation: Existing sketch animation approaches are limited to single-object scenarios or suffer from temporal instability and poor generalization when handling multiple interacting objects.

Method: Two-stage pipeline: Stage 1 Motion Initialization — interactive division of input into semantic groups and keyframe-based interpolation to generate a coarse animation. Stage 2 Motion Refinement — Group-based Displacement Network (GDN) predicts group-specific displacement fields to refine the coarse animation, enhanced by Context-conditioned Feature Enhancement (CCFE) and leveraging priors from a text-to-video model to improve temporal consistency.

Result: Experimental results show substantial improvements over baselines in producing high-quality, temporally consistent animations for complex, multi-object sketches, validating the effectiveness and generalization of the approach.

Conclusion: GroupSketch broadens practical applications of vector sketch animation by robustly handling multi-object interactions and complex motion, thanks to the two-stage design and GDN with CCFE.

Abstract: We introduce GroupSketch, a novel method for vector sketch animation that
effectively handles multi-object interactions and complex motions. Existing
approaches struggle with these scenarios, either being limited to single-object
cases or suffering from temporal inconsistency and poor generalization. To
address these limitations, our method adopts a two-stage pipeline comprising
Motion Initialization and Motion Refinement. In the first stage, the input
sketch is interactively divided into semantic groups and key frames are
defined, enabling the generation of a coarse animation via interpolation. In
the second stage, we propose a Group-based Displacement Network (GDN), which
refines the coarse animation by predicting group-specific displacement fields,
leveraging priors from a text-to-video model. GDN further incorporates
specialized modules, such as Context-conditioned Feature Enhancement (CCFE), to
improve temporal consistency. Extensive experiments demonstrate that our
approach significantly outperforms existing methods in generating high-quality,
temporally consistent animations for complex, multi-object sketches, thus
expanding the practical applications of sketch animation.

</details>


### [47] [D3FNet: A Differential Attention Fusion Network for Fine-Grained Road Structure Extraction in Remote Perception Systems](https://arxiv.org/abs/2508.15537)
*Chang Liu,Yang Xu,Tamas Sziranyi*

Main category: cs.CV

TL;DR: D3FNet presents a dilated, dual-stream attention-based network for fine-grained, narrow road segmentation, achieving state-of-the-art IoU and recall on challenging benchmarks by integrating differential attention dilation, dual-path decoding, and multi-scale dilations.


<details>
  <summary>Details</summary>
Motivation: Narrow, occluded, and low-contrast roads pose significant segmentation challenges in high-resolution remote sensing, requiring fine-grained feature discrimination and robust connectivity, beyond traditional road segmentation models.

Method: Introduces three core innovations built on D-LinkNet: (1) Differential Attention Dilation Extraction (DADE) at the bottleneck to enhance subtle road signals while suppressing background; (2) Dual-stream Decoding Fusion Mechanism (DDFM) that fuses original and attention-modulated features for precise spatial detail and strong semantic context; (3) a multi-scale dilation scheme with rates 1, 3, 5, 9 to reduce gridding artifacts and improve continuity of narrow roads.

Result: On DeepGlobe and CHN6-CUG benchmarks, D3FNet achieves superior IoU and recall on challenging road regions, outperforming state-of-the-art baselines; ablation studies confirm the complementary effects of attention-guided encoding and dual-path decoding.

Conclusion: D3FNet provides a robust solution for fine-grained, narrow road extraction in complex remote sensing and cooperative perception contexts, effectively handling occlusions, low contrast, and fragmented road topology.

Abstract: Extracting narrow roads from high-resolution remote sensing imagery remains a
significant challenge due to their limited width, fragmented topology, and
frequent occlusions. To address these issues, we propose D3FNet, a Dilated
Dual-Stream Differential Attention Fusion Network designed for fine-grained
road structure segmentation in remote perception systems. Built upon the
encoder-decoder backbone of D-LinkNet, D3FNet introduces three key
innovations:(1) a Differential Attention Dilation Extraction (DADE) module that
enhances subtle road features while suppressing background noise at the
bottleneck; (2) a Dual-stream Decoding Fusion Mechanism (DDFM) that integrates
original and attention-modulated features to balance spatial precision with
semantic context; and (3) a multi-scale dilation strategy (rates 1, 3, 5, 9)
that mitigates gridding artifacts and improves continuity in narrow road
prediction. Unlike conventional models that overfit to generic road widths,
D3FNet specifically targets fine-grained, occluded, and low-contrast road
segments. Extensive experiments on the DeepGlobe and CHN6-CUG benchmarks show
that D3FNet achieves superior IoU and recall on challenging road regions,
outperforming state-of-the-art baselines. Ablation studies further verify the
complementary synergy of attention-guided encoding and dual-path decoding.
These results confirm D3FNet as a robust solution for fine-grained narrow road
extraction in complex remote and cooperative perception scenarios.

</details>


### [48] [Backpropagation-Free Test-Time Adaptation via Probabilistic Gaussian Alignment](https://arxiv.org/abs/2508.15568)
*Youjia Zhang,Youngeun Kim,Young-Geun Choi,Hongyeob Kim,Huiling Liu,Sungeun Hong*

Main category: cs.CV

TL;DR: ADAPT is a backpropagation-free test-time adaptation method that models class-conditional Gaussian likelihoods with gradually updated class means and a shared covariance, enabling closed-form inference without source data or gradient updates, guided by CLIP priors and a historical knowledge bank; reports state-of-the-art results and scalable robustness across distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Current TTA approaches largely rely on backpropagation or iterative optimization, which hurts scalability and real-time deployment. There is a shortage of explicit modeling of class-conditional feature distributions, which is essential for calibrated predictions, especially when source data and test-time supervision are unavailable.

Method: Formulate TTA as Gaussian probabilistic inference by representing class-conditional likelihoods with means updated over time and a shared covariance. This yields closed-form, training-free inference. Regularize to mitigate likelihood bias using CLIP-based priors and a historical knowledge bank. No source data, no gradient updates, and no full access to target data; suitable for online and transductive settings.

Result: Extensive experiments across diverse benchmarks indicate state-of-the-art performance under distribution shifts, with improved scalability and robustness.

Conclusion: ADAPT delivers a scalable, backprop-free TTA framework that leverages Gaussian probabilistic modeling and external priors to produce calibrated, reliable predictions without source data, enabling effective deployment in real-time and restricted-access scenarios.

Abstract: Test-time adaptation (TTA) enhances the zero-shot robustness under
distribution shifts by leveraging unlabeled test data during inference. Despite
notable advances, several challenges still limit its broader applicability.
First, most methods rely on backpropagation or iterative optimization, which
limits scalability and hinders real-time deployment. Second, they lack explicit
modeling of class-conditional feature distributions. This modeling is crucial
for producing reliable decision boundaries and calibrated predictions, but it
remains underexplored due to the lack of both source data and supervision at
test time. In this paper, we propose ADAPT, an Advanced Distribution-Aware and
backPropagation-free Test-time adaptation method. We reframe TTA as a Gaussian
probabilistic inference task by modeling class-conditional likelihoods using
gradually updated class means and a shared covariance matrix. This enables
closed-form, training-free inference. To correct potential likelihood bias, we
introduce lightweight regularization guided by CLIP priors and a historical
knowledge bank. ADAPT requires no source data, no gradient updates, and no full
access to target data, supporting both online and transductive settings.
Extensive experiments across diverse benchmarks demonstrate that our method
achieves state-of-the-art performance under a wide range of distribution shifts
with superior scalability and robustness.

</details>


### [49] [High-Frequency First: A Two-Stage Approach for Improving Image INR](https://arxiv.org/abs/2508.15582)
*Sumit Kumar Dam,Mrityunjoy Gain,Eui-Nam Huh,Choong Seon Hong*

Main category: cs.CV

TL;DR: A two-stage training strategy for implicit neural representations that uses a neighbor-aware soft mask to overweight high-frequency, locally varying pixels, encouraging early focus on fine details and improving reconstruction quality while complementing existing INR methods.


<details>
  <summary>Details</summary>
Motivation: Neural networks exhibit a spectral bias that favors low-frequency components, hindering high-frequency detail capture in implicit neural representations (INRs). Prior fixes focused on architecture or activations; this work seeks to steer training to emphasize high-frequency content.

Method: Train INRs in two stages: (1) apply a neighbor-aware soft mask that assigns higher weights to pixels with strong local variation to promote early focus on fine details, and (2) switch to full-image training to finalize optimization. The mask adaptively weights pixels during training.

Result: Empirical results show consistent improvements in reconstruction quality and indicate the approach complements existing INR methods. It is a pioneering attempt to incorporate frequency-aware pixel importance into image INR.

Conclusion: Training-time pixel weighting offers a new avenue to mitigate spectral bias in INRs, suggesting broader applicability for improving high-frequency content capture in implicit representations.

Abstract: Implicit Neural Representations (INRs) have emerged as a powerful alternative
to traditional pixel-based formats by modeling images as continuous functions
over spatial coordinates. A key challenge, however, lies in the spectral bias
of neural networks, which tend to favor low-frequency components while
struggling to capture high-frequency (HF) details such as sharp edges and fine
textures. While prior approaches have addressed this limitation through
architectural modifications or specialized activation functions, we propose an
orthogonal direction by directly guiding the training process. Specifically, we
introduce a two-stage training strategy where a neighbor-aware soft mask
adaptively assigns higher weights to pixels with strong local variations,
encouraging early focus on fine details. The model then transitions to
full-image training. Experimental results show that our approach consistently
improves reconstruction quality and complements existing INR methods. As a
pioneering attempt to assign frequency-aware importance to pixels in image INR,
our work offers a new avenue for mitigating the spectral bias problem.

</details>


### [50] [Fast globally optimal Truncated Least Squares point cloud registration with fixed rotation axis](https://arxiv.org/abs/2508.15613)
*Ivo Ivanov,Carsten Markgraf*

Main category: cs.CV

TL;DR: A fast linear-time convex relaxation and a contractor accelerate provably global TLS-based point-cloud registration with outliers, achieving global optimality for 2x3D registration (rotation given) in under 0.5s and greatly exceeding SDP-based methods; full 6DoF remains open.


<details>
  <summary>Details</summary>
Motivation: Global optimality in truncated least squares (TLS) for point-cloud registration with high outlier rates is computationally expensive with semidefinite programming (SDP). There is a need for faster, provably optimal approaches that scale to practical problem sizes.

Method: Introduce a linear-time convex relaxation and a contractor method to accelerate Branch-and-Bound (BnB). When the rotation axis is provided, the method regresses two 3D point clouds with 100 points to provable global optimality in under 0.5 seconds; benchmarked against the SDP solver STRIDE for the rotation-only TLS problem. The work also provides a formal proof of global optimality and empirical adversarial tests.

Result: Registration of 100 points with a known rotation axis reaches provable global optimality in under 0.5s. The approach is ~100× faster than STRIDE for rotation-only TLS and offers a formal global-optimality guarantee. It demonstrates empirical evidence of global optimality via adversarial instances with near-global minima.

Conclusion: The paper delivers a fast, provably globally optimal approach for rotation-only TLS point-cloud registration, significantly accelerating over SDP-based methods; full 6DoF remains unresolved, but the method shows strong potential and theoretical validation.

Abstract: Recent results showed that point cloud registration with given
correspondences can be made robust to outlier rates of up to 95\% using the
truncated least squares (TLS) formulation. However, solving this combinatorial
optimization problem to global optimality is challenging. Provably globally
optimal approaches using semidefinite programming (SDP) relaxations take
hundreds of seconds for 100 points. In this paper, we propose a novel linear
time convex relaxation as well as a contractor method to speed up Branch and
Bound (BnB). Our solver can register two 3D point clouds with 100 points to
provable global optimality in less than half a second when the axis of rotation
is provided. Although it currently cannot solve the full 6DoF problem, it is
two orders of magnitude faster than the state-of-the-art SDP solver STRIDE when
solving the rotation-only TLS problem. In addition to providing a formal proof
for global optimality, we present empirical evidence of global optimality using
adversarial instances with local minimas close to the global minimum.

</details>


### [51] [Multi-perspective monitoring of wildlife and human activities from camera traps and drones with deep learning models](https://arxiv.org/abs/2508.15629)
*Hao Chen,Fang Qiu,Li An,Douglas Stow,Eve Bohnett,Haitao Lyu,Shuang Tian*

Main category: cs.CV

TL;DR: A multiperspective monitoring framework uses camera traps and drone imagery with deep learning to map wildlife and human activity distributions, identify overlaps, and locate human–wildlife conflict zones in Chitwan National Park, Nepal.


<details>
  <summary>Details</summary>
Motivation: To understand the spatial distribution of wildlife and human activities for evaluating human–wildlife interactions and informing conservation planning and conflict mitigation, using complementary sensing modalities and automated detection.

Method: Field data were collected Feb–Jul 2022 in CNP and surrounding areas using visible/near-infrared camera traps and thermal infrared drones. Datasets were built for training/testing deep learning models to automatically identify wildlife and human activities. Drone thermal imagery provided an aerial perspective. YOLOv11s was evaluated and achieved top performance; an enhanced Faster RCNN was applied to drone data. Spatial pattern analysis identified wildlife and human activity hotspots and overlapping zones to delineate potential conflict.

Result: YOLOv11s achieved precision 96.2%, recall 92.3%, and mAP50 values of 96.7% (camera-trap imagery) and 81.3% (drone-thermal detections). Drone-based thermal imagery analyzed with an enhanced Faster RCNN offered a complementary aerial viewpoint. Spatial analysis revealed clear hotspots for wildlife and human activities with overlapping areas indicating potential conflict.

Conclusion: Integrating multiperspective monitoring with automated object detection enhances wildlife surveillance and landscape management, enabling identification of hotspots and potential conflict zones to inform conservation planning.

Abstract: Wildlife and human activities are key components of landscape systems.
Understanding their spatial distribution is essential for evaluating human
wildlife interactions and informing effective conservation planning.
Multiperspective monitoring of wildlife and human activities by combining
camera traps and drone imagery. Capturing the spatial patterns of their
distributions, which allows the identification of the overlap of their activity
zones and the assessment of the degree of human wildlife conflict. The study
was conducted in Chitwan National Park (CNP), Nepal, and adjacent regions.
Images collected by visible and nearinfrared camera traps and thermal infrared
drones from February to July 2022 were processed to create training and testing
datasets, which were used to build deep learning models to automatic identify
wildlife and human activities. Drone collected thermal imagery was used for
detecting targets to provide a multiple monitoring perspective. Spatial pattern
analysis was performed to identify animal and resident activity hotspots and
delineation potential human wildlife conflict zones. Among the deep learning
models tested, YOLOv11s achieved the highest performance with a precision of
96.2%, recall of 92.3%, mAP50 of 96.7%, and mAP50 of 81.3%, making it the most
effective for detecting objects in camera trap imagery. Drone based thermal
imagery, analyzed with an enhanced Faster RCNN model, added a complementary
aerial viewpoint for camera trap detections. Spatial pattern analysis
identified clear hotspots for both wildlife and human activities and their
overlapping patterns within certain areas in the CNP and buffer zones
indicating potential conflict. This study reveals human wildlife conflicts
within the conserved landscape. Integrating multiperspective monitoring with
automated object detection enhances wildlife surveillance and landscape
management.

</details>


### [52] [When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding](https://arxiv.org/abs/2508.15641)
*Pengcheng Fang,Yuxia Chen,Rui Guo*

Main category: cs.CV

TL;DR: Grounded VideoDiT introduces three innovations—DTL encoder, object-grounded representations, and discrete temporal tokens—that improve temporal grounding and entity alignment in Video LLMs, achieving state-of-the-art results on Charades STA, NExT GQA, and various VideoQA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current Video LLMs struggle with precise temporal localization and entity-level grounding: timestamps are implicit, frame-level continuity is weak, and language–vision alignment can drift away from queried entities. This paper targets robust temporal grounding and entity binding.

Method: 1) Diffusion Temporal Latent (DTL) encoder for sharper temporal boundaries and consistency. 2) Object-grounded representations that bind query entities to localized visual evidence. 3) Mixed token scheme with discrete temporal tokens for explicit timestamp modeling to enable fine-grained temporal reasoning.

Result: Achieves state-of-the-art results on Charades STA, NExT GQA, and multiple VideoQA benchmarks, demonstrating improved grounding robustness and temporal understanding.

Conclusion: Grounded VideoDiT advances temporal grounding and entity alignment in Video LLMs, enabling precise timestamped reasoning and robust grounding across diverse video QA tasks.

Abstract: Understanding videos requires more than answering open ended questions, it
demands the ability to pinpoint when events occur and how entities interact
across time. While recent Video LLMs have achieved remarkable progress in
holistic reasoning, they remain coarse in temporal perception: timestamps are
encoded only implicitly, frame level features are weak in capturing continuity,
and language vision alignment often drifts from the entities of interest. In
this paper, we present Grounded VideoDiT, a Video LLM designed to overcome
these limitations by introducing three key innovations. First, a Diffusion
Temporal Latent (DTL) encoder enhances boundary sensitivity and maintains
temporal consistency. Second, object grounded representations explicitly bind
query entities to localized visual evidence, strengthening alignment. Third, a
mixed token scheme with discrete temporal tokens provides explicit timestamp
modeling, enabling fine grained temporal reasoning. Together, these designs
equip Grounded VideoDiT with robust grounding capabilities, as validated by
state of the art results on Charades STA, NExT GQA, and multiple VideoQA
benchmarks.

</details>


### [53] [Weakly-Supervised Learning for Tree Instances Segmentation in Airborne Lidar Point Clouds](https://arxiv.org/abs/2508.15646)
*Swann Emilien Céleste Destouches,Jesse Lahaye,Laurent Valentin Jospin,Jan Skaloud*

Main category: cs.CV

TL;DR: Weakly supervised ALS tree instance segmentation: human-rated quality signals guide a rating model to fine-tune an initial segmentation, yielding 34% more correctly identified tree instances and fewer non-tree predictions, but struggles with small trees and cluttered surroundings.


<details>
  <summary>Details</summary>
Motivation: Reduce labeling burden and data variability challenges in tree instance segmentation from airborne laser scanning by leveraging human quality judgments to supervise learning without requiring fully labeled instance masks.

Method: 1) Start from an initial segmentation produced by a non-finetuned model or a closed-form algorithm. 2) A human operator provides quality ratings for these segmentations. 3) Train a rating model to classify segmentations into the same class labels as used by humans. 4) Finetune the segmentation model using feedback from the rating model. 5) Evaluate improvements in tree instance identification.

Result: The approach improves correctly identified tree instances by about 34% and reduces non-tree predictions, demonstrating gains from weak supervision. However, performance degrades in sparsely forested regions with very small trees (<2 m) or in complex surroundings containing shrubs or boulders that can be confused with trees.

Conclusion: Weakly supervised learning via quality-rated feedback can substantially boost segmentation performance while reducing labeling costs, but additional work is needed to address challenging data regimes such as small trees and clutter.

Abstract: Tree instance segmentation of airborne laser scanning (ALS) data is of utmost
importance for forest monitoring, but remains challenging due to variations in
the data caused by factors such as sensor resolution, vegetation state at
acquisition time, terrain characteristics, etc. Moreover, obtaining a
sufficient amount of precisely labeled data to train fully supervised instance
segmentation methods is expensive. To address these challenges, we propose a
weakly supervised approach where labels of an initial segmentation result
obtained either by a non-finetuned model or a closed form algorithm are
provided as a quality rating by a human operator. The labels produced during
the quality assessment are then used to train a rating model, whose task is to
classify a segmentation output into the same classes as specified by the human
operator. Finally, the segmentation model is finetuned using feedback from the
rating model. This in turn improves the original segmentation model by 34\% in
terms of correctly identified tree instances while considerably reducing the
number of non-tree instances predicted. Challenges still remain in data over
sparsely forested regions characterized by small trees (less than two meters in
height) or within complex surroundings containing shrubs, boulders, etc. which
can be confused as trees where the performance of the proposed method is
reduced.

</details>


### [54] [Towards a 3D Transfer-based Black-box Attack via Critical Feature Guidance](https://arxiv.org/abs/2508.15650)
*Shuchao Pang,Zhenghan Chen,Shen Zhang,Liming Lu,Siyuan Liang,Anan Du,Yongbin Zhou*

Main category: cs.CV

TL;DR: A transfer-based black-box attack (CFG) boosts adversarial success on 3D point clouds by focusing on features that are crucial across network architectures, while constraining perturbations for imperceptibility; it outperforms prior attacks on ModelNet40 and ScanObjectNN.


<details>
  <summary>Details</summary>
Motivation: In real-world settings, attacker has no access to target model parameters or outputs. Since critical features for classification appear consistent across DNN architectures, exploiting these features should transfer better across models.

Method: Critical Feature Guidance (CFG): compute feature importance across extracted features, regularize the adversarial point-cloud search to favor corrupting critical features shared by diverse architectures, and enforce a maximum perturbation constraint within the loss to maintain imperceptibility.

Result: CFG significantly improves transferability and outperforms state-of-the-art transfer-based attack methods on ModelNet40 and ScanObjectNN datasets.

Conclusion: CFG demonstrates that cross-architecture-consistent critical features can be effectively exploited to enhance transfer-based black-box attacks on 3D point clouds, highlighting a potential robustness gap and guiding defense considerations.

Abstract: Deep neural networks for 3D point clouds have been demonstrated to be
vulnerable to adversarial examples. Previous 3D adversarial attack methods
often exploit certain information about the target models, such as model
parameters or outputs, to generate adversarial point clouds. However, in
realistic scenarios, it is challenging to obtain any information about the
target models under conditions of absolute security. Therefore, we focus on
transfer-based attacks, where generating adversarial point clouds does not
require any information about the target models. Based on our observation that
the critical features used for point cloud classification are consistent across
different DNN architectures, we propose CFG, a novel transfer-based black-box
attack method that improves the transferability of adversarial point clouds via
the proposed Critical Feature Guidance. Specifically, our method regularizes
the search of adversarial point clouds by computing the importance of the
extracted features, prioritizing the corruption of critical features that are
likely to be adopted by diverse architectures. Further, we explicitly constrain
the maximum deviation extent of the generated adversarial point clouds in the
loss function to ensure their imperceptibility. Extensive experiments conducted
on the ModelNet40 and ScanObjectNN benchmark datasets demonstrate that the
proposed CFG outperforms the state-of-the-art attack methods by a large margin.

</details>


### [55] [MapKD: Unlocking Prior Knowledge with Cross-Modal Distillation for Efficient Online HD Map Construction](https://arxiv.org/abs/2508.15653)
*Ziyang Yan,Ruikai Li,Zhiyong Cui,Bohan Li,Han Jiang,Yilong Ren,Aoyong Li,Zhenning Li,Sijia Wen,Haiyang Yu*

Main category: cs.CV

TL;DR: MapKD introduces a teacher-coach-student distillation framework to enable a light-weight vision-centric model for online HD map construction by transferring knowledge from multimodal models with map priors, using TGPD and MSRD; achieves significant performance gains and faster inference on nuScenes.


<details>
  <summary>Details</summary>
Motivation: Reduce reliance on stale offline SD/HD maps and heavy multi-modal sensor chains; lower computational overhead while preserving accuracy by distilling knowledge into a vision-only student; bridge cross-modal gaps with a coach and simulated LiDAR.

Method: Multi-level cross-modal distillation (Teacher-Coach-Student). Teacher: camera-LiDAR fusion model with SD/HD map priors. Coach: vision-centric model with priors and simulated LiDAR to bridge transfer gap. Student: lightweight vision-based model. Two distillation strategies: Token-Guided 2D Patch Distillation (TGPD) for BEV feature alignment; Masked Semantic Response Distillation (MSRD) for semantic guidance.

Result: On nuScenes, the student model gains +6.68 mIoU and +10.94 mAP, while inference speed is accelerated. Code available at the provided GitHub link.

Conclusion: MapKD effectively transfers rich, multi-modal knowledge into a compact vision-only model for online HD map construction, reducing reliance on offline maps and heavy sensors while delivering competitive accuracy and speed. This approach shows promise for real-time autonomous driving and paves the way for further cross-modal distillation research.

Abstract: Online HD map construction is a fundamental task in autonomous driving
systems, aiming to acquire semantic information of map elements around the ego
vehicle based on real-time sensor inputs. Recently, several approaches have
achieved promising results by incorporating offline priors such as SD maps and
HD maps or by fusing multi-modal data. However, these methods depend on stale
offline maps and multi-modal sensor suites, resulting in avoidable
computational overhead at inference. To address these limitations, we employ a
knowledge distillation strategy to transfer knowledge from multimodal models
with prior knowledge to an efficient, low-cost, and vision-centric student
model. Specifically, we propose MapKD, a novel multi-level cross-modal
knowledge distillation framework with an innovative Teacher-Coach-Student (TCS)
paradigm. This framework consists of: (1) a camera-LiDAR fusion model with
SD/HD map priors serving as the teacher; (2) a vision-centric coach model with
prior knowledge and simulated LiDAR to bridge the cross-modal knowledge
transfer gap; and (3) a lightweight vision-based student model. Additionally,
we introduce two targeted knowledge distillation strategies: Token-Guided 2D
Patch Distillation (TGPD) for bird's eye view feature alignment and Masked
Semantic Response Distillation (MSRD) for semantic learning guidance. Extensive
experiments on the challenging nuScenes dataset demonstrate that MapKD improves
the student model by +6.68 mIoU and +10.94 mAP while simultaneously
accelerating inference speed. The code is available
at:https://github.com/2004yan/MapKD2026.

</details>


### [56] [CM2LoD3: Reconstructing LoD3 Building Models Using Semantic Conflict Maps](https://arxiv.org/abs/2508.15672)
*Franz Hanke,Antonia Bieringer,Olaf Wysocki,Boris Jutzi*

Main category: cs.CV

TL;DR: CM2LoD3 introduces an automated LoD3 building reconstruction by segmenting Conflict Maps (CMs) generated from ray-to-model-prior analysis, using a Semantic Conflict Map Generator (SCMG) to train on synthetic CMs and optionally fuse textured model segmentation with CMs via confidence scores to improve accuracy, achieving 61% segmentation performance with uncertainty-aware texture fusion.


<details>
  <summary>Details</summary>
Motivation: LoD3 models provide facade details (windows, doors, openings) missing in LoD1/LoD2 but are costly to produce manually, hindering scalable, automated large-scale 3D city modeling for urban planning, digital twins, and disaster management.

Method: The approach, CM2LoD3, reconstructs LoD3 buildings by leveraging Conflict Maps (CMs) from ray-to-model-prior analysis. It semantically segments real-world CMs and synthetically generated CMs from a Semantic Conflict Map Generator (SCMG). It further fuses segmentation of textured models with CMs using confidence scores to boost segmentation performance, leading to improved reconstruction of building openings.

Result: Experimental results demonstrate effective segmentation and reconstruction of building openings, with a reported 61% performance when fusing textured-model segmentation with CM-based segmentation under uncertainty-aware fusion.

Conclusion: CM2LoD3 advances automated LoD3 model reconstruction, enabling scalable and efficient 3D city modeling by combining CM-based segmentation with texture fusion, moving towards automated, large-scale LoD3 reconstruction.

Abstract: Detailed 3D building models are crucial for urban planning, digital twins,
and disaster management applications. While Level of Detail 1 (LoD)1 and LoD2
building models are widely available, they lack detailed facade elements
essential for advanced urban analysis. In contrast, LoD3 models address this
limitation by incorporating facade elements such as windows, doors, and
underpasses. However, their generation has traditionally required manual
modeling, making large-scale adoption challenging. In this contribution,
CM2LoD3, we present a novel method for reconstructing LoD3 building models
leveraging Conflict Maps (CMs) obtained from ray-to-model-prior analysis.
Unlike previous works, we concentrate on semantically segmenting real-world CMs
with synthetically generated CMs from our developed Semantic Conflict Map
Generator (SCMG). We also observe that additional segmentation of textured
models can be fused with CMs using confidence scores to further increase
segmentation performance and thus increase 3D reconstruction accuracy.
Experimental results demonstrate the effectiveness of our CM2LoD3 method in
segmenting and reconstructing building openings, with the 61% performance with
uncertainty-aware fusion of segmented building textures. This research
contributes to the advancement of automated LoD3 model reconstruction, paving
the way for scalable and efficient 3D city modeling. Our project is available:
https://github.com/InFraHank/CM2LoD3

</details>


### [57] [LLM-empowered Dynamic Prompt Routing for Vision-Language Models Tuning under Long-Tailed Distributions](https://arxiv.org/abs/2508.15688)
*Yongju Jia,Jiarui Ma,Xiangxian Li,Baiqiao Zhang,Xianhui Cao,Juan Liu,Yulong Bian*

Main category: cs.CV

TL;DR: MDPR introduces a multi-dimensional dynamic prompt routing framework to mitigate class-imbalance bias in vision-language model fine-tuning by building a five-dimensional semantic knowledge base and routing prompts via dynamic alignment and logits fusion, achieving competitive long-tailed performance with low overhead.


<details>
  <summary>Details</summary>
Motivation: Long-tailed class distributions and pretraining biases hamper VLM fine-tuning; existing LLM-enhanced methods often ignore inherent class imbalance, risking biased downstream performance.

Method: Construct a five-dimensional visual-semantic knowledge base for classes; during fine-tuning, apply a dynamic routing mechanism that aligns global class representations, retrieves optimal prompts, balances fine-grained semantics, and fuses logits to yield stable predictions.

Result: MDPR achieves results comparable to SOTA on CIFAR-LT, ImageNet-LT, and Places-LT; ablation studies show semantic library benefits for tail classes; dynamic routing incurs minimal computational overhead.

Conclusion: MDPR provides a flexible and efficient enhancement for VLM fine-tuning under data imbalance, addressing pretraining bias and improving tail-class stability through multi-dimensional dynamic prompt routing.

Abstract: Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated
impressive capability in visual tasks, but their fine-tuning often suffers from
bias in class-imbalanced scene. Recent works have introduced large language
models (LLMs) to enhance VLM fine-tuning with supplementing semantic
information. However, they often overlook inherent class imbalance in VLMs'
pre-training, which may lead to bias accumulation in downstream tasks. To
address this problem, this paper proposes a Multi-dimensional Dynamic Prompt
Routing (MDPR) framework. MDPR constructs a comprehensive knowledge base for
classes, spanning five visual-semantic dimensions. During fine-tuning, the
dynamic routing mechanism aligns global visual classes, retrieves optimal
prompts, and balances fine-grained semantics, yielding stable predictions
through logits fusion. Extensive experiments on long-tailed benchmarks,
including CIFAR-LT, ImageNet-LT, and Places-LT, demonstrate that MDPR achieves
comparable results with current SOTA methods. Ablation studies further confirm
the effectiveness of our semantic library for tail classes, and show that our
dynamic routing incurs minimal computational overhead, making MDPR a flexible
and efficient enhancement for VLM fine-tuning under data imbalance.

</details>


### [58] [StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding](https://arxiv.org/abs/2508.15717)
*Yanlai Yang,Zhuokai Zhao,Satya Narayan Shukla,Aashu Singh,Shlok Kumar Mishra,Lizhu Zhang,Mengye Ren*

Main category: cs.CV

TL;DR: StreamMem introduces a query-agnostic, fixed-size KV cache for streaming video understanding, compressing the KV via attention between visual tokens and generic query tokens, enabling memory-efficient long-video QA without prior knowledge of questions; achieves state-of-the-art compression for this setting and is competitive with query-aware methods across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Long video understanding in multimodal LLMs is hindered by the need to store and attend to large visual context using KV caches. Existing visual compression either requires encoding the entire context upfront or access to the questions in advance, which is impractical for streaming long videos and multi-turn dialog.

Method: Propose StreamMem: a streaming, query-agnostic KV cache mechanism. It encodes new video frames as they arrive and compresses the KV cache by leveraging attention scores between visual tokens and generic query tokens, while preserving a fixed-size memory structure suitable for memory-constrained, long-video QA.

Result: Empirical evaluation on three long video understanding benchmarks and two streaming video QA benchmarks shows that StreamMem achieves state-of-the-art performance in query-agnostic KV cache compression and is competitive with query-aware compression approaches.

Conclusion: StreamMem enables efficient, memory-safe long-video understanding in streaming settings by providing a fixed-size, query-agnostic KV cache with attention-based compression, delivering strong performance gains and practicality for long-context MLLMs.

Abstract: Multimodal large language models (MLLMs) have made significant progress in
visual-language reasoning, but their ability to efficiently handle long videos
remains limited. Despite recent advances in long-context MLLMs, storing and
attending to the key-value (KV) cache for long visual contexts incurs
substantial memory and computational overhead. Existing visual compression
methods require either encoding the entire visual context before compression or
having access to the questions in advance, which is impractical for long video
understanding and multi-turn conversational settings. In this work, we propose
StreamMem, a query-agnostic KV cache memory mechanism for streaming video
understanding. Specifically, StreamMem encodes new video frames in a streaming
manner, compressing the KV cache using attention scores between visual tokens
and generic query tokens, while maintaining a fixed-size KV memory to enable
efficient question answering (QA) in memory-constrained, long-video scenarios.
Evaluation on three long video understanding and two streaming video question
answering benchmarks shows that StreamMem achieves state-of-the-art performance
in query-agnostic KV cache compression and is competitive with query-aware
compression approaches.

</details>


### [59] [WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception](https://arxiv.org/abs/2508.15720)
*Zhiheng Liu,Xueqing Deng,Shoufa Chen,Angtian Wang,Qiushan Guo,Mingfei Han,Zeyue Xue,Mengzhao Chen,Ping Luo,Linjie Yang*

Main category: cs.CV

TL;DR: WorldWeaver is a long-horizon video-generation framework that jointly models RGB frames and perceptual conditions, uses a depth-based memory bank to curb drift, and employs segmented noise scheduling to improve temporal consistency and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address persistent temporal drift and structural errors in long videos produced by RGB-only generative models, which accumulate errors over time.

Method: A unified representation that jointly predicts perceptual conditions and color from a single model, a depth-enabled memory bank to preserve contextual information, and segmented noise scheduling during training for prediction groups; applicable to both diffusion- and rectified-flow-based video models.

Result: Demonstrates reduced temporal drift and higher fidelity in long-horizon video generation across diffusion and rectified-flow frameworks, with clearer motion dynamics and more stable object structure over extended sequences.

Conclusion: WorldWeaver provides a robust, memory-augmented, multi-modal approach for stable long video synthesis, improving temporal consistency and quality while reducing computational cost.

Abstract: Generative video modeling has made significant strides, yet ensuring
structural and temporal consistency over long sequences remains a challenge.
Current methods predominantly rely on RGB signals, leading to accumulated
errors in object structure and motion over extended durations. To address these
issues, we introduce WorldWeaver, a robust framework for long video generation
that jointly models RGB frames and perceptual conditions within a unified
long-horizon modeling scheme. Our training framework offers three key
advantages. First, by jointly predicting perceptual conditions and color
information from a unified representation, it significantly enhances temporal
consistency and motion dynamics. Second, by leveraging depth cues, which we
observe to be more resistant to drift than RGB, we construct a memory bank that
preserves clearer contextual information, improving quality in long-horizon
video generation. Third, we employ segmented noise scheduling for training
prediction groups, which further mitigates drift and reduces computational
cost. Extensive experiments on both diffusion- and rectified flow-based models
demonstrate the effectiveness of WorldWeaver in reducing temporal drift and
improving the fidelity of generated videos.

</details>


### [60] [Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered All-in-SAM Model](https://arxiv.org/abs/2508.15751)
*Xueyuan Li,Can Cui,Ruining Deng,Yucheng Tang,Quan Liu,Tianyuan Yao,Shunxing Bao,Naweed Chowdhury,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: All-in-SAM combines Segment Anything Model with molecular-powered learning to enable annotation-friendly, semantic-focused nuclei segmentation and cell classification, using a SAM adapter and MOCL to improve accuracy and generalizability, especially under limited pixel-level annotations.


<details>
  <summary>Details</summary>
Motivation: General vision foundation models struggle with fine-grained semantic segmentation in pathology. The paper aims to bridge this gap by injecting molecular-level guidance and adaptive customization into SAM, reducing annotation burden and improving segmentation/classification in nuclei/cells.

Method: A full-stack approach: (1) annotation-engaging lay annotators via molecular-empowered learning to reduce pixel-level labeling; (2) learning-adapting SAM to emphasize specific semantics using a SAM adapter; (3) refinement-enhancing segmentation via Molecular-Oriented Corrective Learning (MOCL). Evaluated on in-house and public datasets for cell classification performance.

Result: Experimental results show significant improvements in cell classification across datasets, with robustness to varying annotation quality, demonstrating improved accuracy and generalization.

Conclusion: The approach lowers annotation workload, extends access to precise biomedical image analysis in resource-limited settings, and advances automated pathology image analysis.

Abstract: Purpose: Recent developments in computational pathology have been driven by
advances in Vision Foundation Models, particularly the Segment Anything Model
(SAM). This model facilitates nuclei segmentation through two primary methods:
prompt-based zero-shot segmentation and the use of cell-specific SAM models for
direct segmentation. These approaches enable effective segmentation across a
range of nuclei and cells. However, general vision foundation models often face
challenges with fine-grained semantic segmentation, such as identifying
specific nuclei subtypes or particular cells. Approach: In this paper, we
propose the molecular-empowered All-in-SAM Model to advance computational
pathology by leveraging the capabilities of vision foundation models. This
model incorporates a full-stack approach, focusing on: (1) annotation-engaging
lay annotators through molecular-empowered learning to reduce the need for
detailed pixel-level annotations, (2) learning-adapting the SAM model to
emphasize specific semantics, which utilizes its strong generalizability with
SAM adapter, and (3) refinement-enhancing segmentation accuracy by integrating
Molecular-Oriented Corrective Learning (MOCL). Results: Experimental results
from both in-house and public datasets show that the All-in-SAM model
significantly improves cell classification performance, even when faced with
varying annotation quality. Conclusions: Our approach not only reduces the
workload for annotators but also extends the accessibility of precise
biomedical image analysis to resource-limited settings, thereby advancing
medical diagnostics and automating pathology image analysis.

</details>


### [61] [Waver: Wave Your Way to Lifelike Video Generation](https://arxiv.org/abs/2508.15761)
*Yifu Zhang,Hao Yang,Yuqi Zhang,Yifei Hu,Fengda Zhu,Chuang Lin,Xiaofeng Mei,Yi Jiang,Zehuan Yuan,Bingyue Peng*

Main category: cs.CV

TL;DR: Waver is a unified, high‑performance foundation model for image and video generation that supports T2V, I2V, and T2I. It can generate 5–10 s videos at 720p (upscaled to 1080p), uses a Hybrid Stream DiT architecture, and employs a data curation pipeline plus an MLLM‑based video quality filter. It achieves strong motion and temporal consistency, ranks Top 3 on T2V and I2V leaderboards, and ships open‑source resources and recipes.


<details>
  <summary>Details</summary>
Motivation: There is a need for a single, efficient framework that can coherently handle image and video generation across multiple modalities (T2V, I2V, T2I) with high motion quality and temporal stability. Improving data quality, alignment across modalities, and providing practical training/inference guidance can accelerate progress in video generation and democratize access to high-quality models.

Method: Introduce Waver with Hybrid Stream DiT architecture to improve modality alignment and faster training. Build a comprehensive data curation pipeline and train an MLLM-based video quality model to filter high-quality samples. Provide detailed training and inference recipes. Generate 5–10 s 720p videos (upscaled to 1080p) and support T2V, I2V, and T2I in a unified framework.

Result: Waver demonstrates strong motion capture and temporal consistency in video synthesis. It ranks among the Top 3 on both T2V and I2V leaderboards at Artificial Analysis (data current to 2025-07-30 10:00 GMT+8), outperforming existing open-source models and matching or surpassing certain commercial solutions.

Conclusion: The report contributes a practical, high‑quality, unified approach to video generation, including novel architecture, curation and quality-filtering methods, and actionable training/inference guidance aimed at advancing the field and lowering barriers to high‑quality video generation research.

Abstract: We present Waver, a high-performance foundation model for unified image and
video generation. Waver can directly generate videos with durations ranging
from 5 to 10 seconds at a native resolution of 720p, which are subsequently
upscaled to 1080p. The model simultaneously supports text-to-video (T2V),
image-to-video (I2V), and text-to-image (T2I) generation within a single,
integrated framework. We introduce a Hybrid Stream DiT architecture to enhance
modality alignment and accelerate training convergence. To ensure training data
quality, we establish a comprehensive data curation pipeline and manually
annotate and train an MLLM-based video quality model to filter for the
highest-quality samples. Furthermore, we provide detailed training and
inference recipes to facilitate the generation of high-quality videos. Building
on these contributions, Waver excels at capturing complex motion, achieving
superior motion amplitude and temporal consistency in video synthesis. Notably,
it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial
Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming
existing open-source models and matching or surpassing state-of-the-art
commercial solutions. We hope this technical report will help the community
more efficiently train high-quality video generation models and accelerate
progress in video generation technologies. Official page:
https://github.com/FoundationVision/Waver.

</details>


### [62] [ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling](https://arxiv.org/abs/2508.15767)
*Jinhyung Park,Javier Romero,Shunsuke Saito,Fabian Prada,Takaaki Shiratori,Yichen Xu,Federica Bogo,Shoou-I Yu,Kris Kitani,Rawal Khirodkar*

Main category: cs.CV

TL;DR: ATLAS is a high-fidelity 3D human body model grounded in the skeleton, learned from 600k high-resolution scans (240 synchronized cameras). It decouples shape and skeleton bases for greater expressivity and controllability, employing non-linear pose correctives that outperform linear models in capturing complex poses.


<details>
  <summary>Details</summary>
Motivation: Prevailing parametric body models rely on limited data diversity and linear bases, and couple outer soft tissue with an internal skeleton. This entanglement hampers accurate pose/shape modeling and prevents independent control of body height and bone lengths. A decoupled, skeleton-grounded representation is needed for higher fidelity and controllability.

Method: Explicitly decouple shape and skeleton bases by grounding the mesh representation in the human skeleton. Train on 600k high-resolution scans from 240 synchronized cameras. Use non-linear pose correctives to better capture complex poses, allowing keypoint fitting independent of external soft-tissue characteristics.

Result: ATLAS achieves more accurate fits for unseen subjects across diverse poses than prior methods. Quantitative results show non-linear pose correctives better capture complex poses than linear models, and the decoupled representation yields enhanced shape expressivity and controllable body attributes.

Conclusion: ATLAS provides a more accurate, controllable, and expressive body model by decoupling shape and skeleton, enabling independent control of height and bone lengths and improved pose modeling through non-linear corrections.

Abstract: Parametric body models offer expressive 3D representation of humans across a
wide range of poses, shapes, and facial expressions, typically derived by
learning a basis over registered 3D meshes. However, existing human mesh
modeling approaches struggle to capture detailed variations across diverse body
poses and shapes, largely due to limited training data diversity and
restrictive modeling assumptions. Moreover, the common paradigm first optimizes
the external body surface using a linear basis, then regresses internal
skeletal joints from surface vertices. This approach introduces problematic
dependencies between internal skeleton and outer soft tissue, limiting direct
control over body height and bone lengths. To address these issues, we present
ATLAS, a high-fidelity body model learned from 600k high-resolution scans
captured using 240 synchronized cameras. Unlike previous methods, we explicitly
decouple the shape and skeleton bases by grounding our mesh representation in
the human skeleton. This decoupling enables enhanced shape expressivity,
fine-grained customization of body attributes, and keypoint fitting independent
of external soft-tissue characteristics. ATLAS outperforms existing methods by
fitting unseen subjects in diverse poses more accurately, and quantitative
evaluations show that our non-linear pose correctives more effectively capture
complex poses compared to linear models.

</details>


### [63] [SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass](https://arxiv.org/abs/2508.15769)
*Yanxu Meng,Haoning Wu,Ya Zhang,Weidi Xie*

Main category: cs.CV

TL;DR: SceneGen is a single-pass, optimization-free framework that generates multiple 3D assets with geometry and texture from a single scene image and object masks, featuring a novel feature aggregation module and a position head; extendable to multi-image inputs.


<details>
  <summary>Details</summary>
Motivation: Efficiently synthesize multiple 3D assets within a scene for VR/AR and embodied AI, addressing scalability and spatial consistency without heavy optimization or asset retrieval.

Method: Propose SceneGen architecture: input scene image and masks; feature aggregation module combining local/global cues from visual and geometric encoders; position head for relative asset placement; end-to-end feedforward generation of assets and their locations; trained on single-image data; can extend to multi-image inputs at inference.

Result: Quantitative and qualitative evaluations show efficiency and robust generation; improved results when using multi-image inputs; code and model will be released publicly.

Conclusion: Offers a new paradigm for high-quality 3D content generation with potential practical impact on downstream tasks.

Abstract: 3D content generation has recently attracted significant research interest
due to its applications in VR/AR and embodied AI. In this work, we address the
challenging task of synthesizing multiple 3D assets within a single scene
image. Concretely, our contributions are fourfold: (i) we present SceneGen, a
novel framework that takes a scene image and corresponding object masks as
input, simultaneously producing multiple 3D assets with geometry and texture.
Notably, SceneGen operates with no need for optimization or asset retrieval;
(ii) we introduce a novel feature aggregation module that integrates local and
global scene information from visual and geometric encoders within the feature
extraction module. Coupled with a position head, this enables the generation of
3D assets and their relative spatial positions in a single feedforward pass;
(iii) we demonstrate SceneGen's direct extensibility to multi-image input
scenarios. Despite being trained solely on single-image inputs, our
architectural design enables improved generation performance with multi-image
inputs; and (iv) extensive quantitative and qualitative evaluations confirm the
efficiency and robust generation abilities of our approach. We believe this
paradigm offers a novel solution for high-quality 3D content generation,
potentially advancing its practical applications in downstream tasks. The code
and model will be publicly available at: https://mengmouxu.github.io/SceneGen.

</details>


### [64] [Visual Autoregressive Modeling for Instruction-Guided Image Editing](https://arxiv.org/abs/2508.15772)
*Qingyang Mao,Qi Cai,Yehao Li,Yingwei Pan,Mingyue Cheng,Ting Yao,Qi Liu,Tao Mei*

Main category: cs.CV

TL;DR: VAREdit reframes image editing as a next-scale prediction task in a visual autoregressive (VAR) framework, using a Scale-Aligned Reference (SAR) to condition coarse target features on source image tokens. It achieves better editing adherence and speed than diffusion-based methods, with a 512×512 edit in 1.2s and 2.2× speedup over UltraEdit.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based editing often entangles edited regions with global image context, causing spurious changes and weak adherence to editing instructions. Autoregressive image modeling offers causal, compositional advantages but requires effective cross-scale conditioning to guide coarse-to-fine predictions.

Method: VAREdit treats editing as next-scale feature prediction conditioned on source image features and text. It introduces Scale-Aligned Reference (SAR), injecting scale-matched conditioning into the first self-attention layer to bridge guidance from fine source features to coarser target features, enabling accurate edits across scales.

Result: On standard benchmarks, VAREdit outperforms leading diffusion-based editors by 30%+ GPT-Balance score. It edits 512×512 images in about 1.2 seconds, 2.2× faster than a similarly sized UltraEdit. Code is available at the project link.

Conclusion: VAREdit demonstrates that a visual autoregressive approach with scale-aligned conditioning can surpass diffusion-based editors in both fidelity to editing instructions and efficiency, offering a compelling alternative for efficient, precise image editing.

Abstract: Recent advances in diffusion models have brought remarkable visual fidelity
to instruction-guided image editing. However, their global denoising process
inherently entangles the edited region with the entire image context, leading
to unintended spurious modifications and compromised adherence to editing
instructions. In contrast, autoregressive models offer a distinct paradigm by
formulating image synthesis as a sequential process over discrete visual
tokens. Their causal and compositional mechanism naturally circumvents the
adherence challenges of diffusion-based methods. In this paper, we present
VAREdit, a visual autoregressive (VAR) framework that reframes image editing as
a next-scale prediction problem. Conditioned on source image features and text
instructions, VAREdit generates multi-scale target features to achieve precise
edits. A core challenge in this paradigm is how to effectively condition the
source image tokens. We observe that finest-scale source features cannot
effectively guide the prediction of coarser target features. To bridge this
gap, we introduce a Scale-Aligned Reference (SAR) module, which injects
scale-matched conditioning information into the first self-attention layer.
VAREdit demonstrates significant advancements in both editing adherence and
efficiency. On standard benchmarks, it outperforms leading diffusion-based
methods by 30\%+ higher GPT-Balance score. Moreover, it completes a
$512\times512$ editing in 1.2 seconds, making it 2.2$\times$ faster than the
similarly sized UltraEdit. The models are available at
https://github.com/HiDream-ai/VAREdit.

</details>


### [65] [Scaling Group Inference for Diverse and High-Quality Generation](https://arxiv.org/abs/2508.15773)
*Gaurav Parmar,Or Patashnik,Daniil Ostashev,Kuan-Chieh Wang,Kfir Aberman,Srinivasa Narasimhan,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: A scalable group inference method for generative models that jointly selects high-quality, diverse groups of outputs by formulating candidate outputs as graph nodes in a quadratic assignment problem, with progressive pruning to scale to large candidate sets; applicable across text-to-image, image-to-image, prompting, and video generation.


<details>
  <summary>Details</summary>
Motivation: In real applications, users often view multiple outputs per prompt. Independent sampling yields redundant results and limited exploration. Existing inference-time guidance focuses on single-sample quality and does not optimize a coherent group. There is a need for a scalable method to jointly optimize a diverse, high-quality group of outputs.

Method: Model candidates as graph nodes and formulate subset selection as a quadratic integer programming problem. The unary term captures individual sample quality, while the binary term encourages diversity across the selected group. To scale, progressively prune the candidate set using intermediate predictions, enabling handling of large candidate pools. The framework is designed to be plug-and-play across different generative modalities.

Result: Empirical evaluations show that the proposed method improves both group diversity and quality compared to independent sampling baselines and recent inference algorithms. The approach generalizes across tasks such as text-to-image, image-to-image, image prompting, and video generation, demonstrating the ability to treat multiple outputs as a cohesive group rather than independent samples.

Conclusion: The group inference method offers a scalable, effective solution for producing diverse, high-quality groups of outputs from generative models. By jointly optimizing quality and diversity with scalable pruning, it enhances user choice and exploration across multiple modalities.

Abstract: Generative models typically sample outputs independently, and recent
inference-time guidance and scaling algorithms focus on improving the quality
of individual samples. However, in real-world applications, users are often
presented with a set of multiple images (e.g., 4-8) for each prompt, where
independent sampling tends to lead to redundant results, limiting user choices
and hindering idea exploration. In this work, we introduce a scalable group
inference method that improves both the diversity and quality of a group of
samples. We formulate group inference as a quadratic integer assignment
problem: candidate outputs are modeled as graph nodes, and a subset is selected
to optimize sample quality (unary term) while maximizing group diversity
(binary term). To substantially improve runtime efficiency, we progressively
prune the candidate set using intermediate predictions, allowing our method to
scale up to large candidate sets. Extensive experiments show that our method
significantly improves group diversity and quality compared to independent
sampling baselines and recent inference algorithms. Our framework generalizes
across a wide range of tasks, including text-to-image, image-to-image, image
prompting, and video generation, enabling generative models to treat multiple
outputs as cohesive groups rather than independent samples.

</details>


### [66] [CineScale: Free Lunch in High-Resolution Cinematic Visual Generation](https://arxiv.org/abs/2508.15774)
*Haonan Qiu,Ning Yu,Ziqi Huang,Paul Debevec,Ziwei Liu*

Main category: cs.CV

TL;DR: CineScale enables higher-resolution visual generation for diffusion-models without fine-tuning, with variants for image-to-video, video-to-video generation and LoRA-assisted tuning; achieves 8k image generation without fine-tuning and 4k video with minimal LoRA fine-tuning, validated by experiments.


<details>
  <summary>Details</summary>
Motivation: High-resolution generation is hampered by training-resolution limits, data scarcity, and computation; existing tuning-free approaches still produce artifacts and repetitive patterns due to high-frequency information when generating beyond training resolution.

Method: CineScale is an inference paradigm with dedicated variants for different video generation architectures. It supports high-resolution I2V and V2V synthesis on top of open-source frameworks, designed to tackle high-frequency info and error accumulation by adjusting inference-time strategy rather than model fine-tuning.

Result: Extensive experiments show superiority in extending high-resolution generation capabilities for both image and video models; 8k images without fine-tuning; 4k videos with minimal LoRA fine-tuning; samples available online.

Conclusion: CineScale broadens the scope of high-resolution generation for image and video diffusion models without heavy fine-tuning, mitigating high-frequency artifacts and enabling state-of-the-art high-res outputs across I2V, V2V, T2I/T2V tasks.

Abstract: Visual diffusion models achieve remarkable progress, yet they are typically
trained at limited resolutions due to the lack of high-resolution data and
constrained computation resources, hampering their ability to generate
high-fidelity images or videos at higher resolutions. Recent efforts have
explored tuning-free strategies to exhibit the untapped potential
higher-resolution visual generation of pre-trained models. However, these
methods are still prone to producing low-quality visual content with repetitive
patterns. The key obstacle lies in the inevitable increase in high-frequency
information when the model generates visual content exceeding its training
resolution, leading to undesirable repetitive patterns deriving from the
accumulated errors. In this work, we propose CineScale, a novel inference
paradigm to enable higher-resolution visual generation. To tackle the various
issues introduced by the two types of video generation architectures, we
propose dedicated variants tailored to each. Unlike existing baseline methods
that are confined to high-resolution T2I and T2V generation, CineScale broadens
the scope by enabling high-resolution I2V and V2V synthesis, built atop
state-of-the-art open-source video generation frameworks. Extensive experiments
validate the superiority of our paradigm in extending the capabilities of
higher-resolution visual generation for both image and video models.
Remarkably, our approach enables 8k image generation without any fine-tuning,
and achieves 4k video generation with only minimal LoRA fine-tuning. Generated
video samples are available at our website:
https://eyeline-labs.github.io/CineScale/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [67] [Learning to Drive Ethically: Embedding Moral Reasoning into Autonomous Driving](https://arxiv.org/abs/2508.14926)
*Dianzhao Li,Ostap Okhrin*

Main category: cs.LG

TL;DR: A hierarchical Safe RL framework for autonomous driving that optimizes ethical risk (collision probability and harm severity) at decision level with dynamic prioritized replay, and uses polynomial planning with PID/Stanley at execution level to yield smooth trajectories. Validated on real-world traffic data, it outperforms baselines in reducing ethical risk while preserving driving performance; claims novelty in real-world ethical decision-making via Safe RL.


<details>
  <summary>Details</summary>
Motivation: To embed robust ethical reasoning into autonomous driving, balancing safety and efficiency, especially for rare high-risk events and in mixed-traffic environments with pedestrians and cyclists.

Method: Decision level: Safe RL agent trained with a composite ethical risk cost (collision probability + harm severity) to generate high-level motion targets. Learning enhancement: dynamic Prioritized Experience Replay emphasizes rare, high-risk events. Execution level: polynomial path planning plus PID and Stanley controllers to convert targets into smooth, feasible trajectories.

Result: The approach outperforms baseline methods in reducing ethical risk while maintaining driving performance on real-world traffic datasets containing diverse actors (vehicles, cyclists, pedestrians). It is presented as the first real-world Safe RL study for ethical decision-making in autonomous driving.

Conclusion: Combining formal control theory with data-driven Safe RL can advance ethically accountable autonomy in complex, human-mixed traffic environments.

Abstract: Autonomous vehicles hold great promise for reducing traffic fatalities and
improving transportation efficiency, yet their widespread adoption hinges on
embedding robust ethical reasoning into routine and emergency maneuvers. Here,
we present a hierarchical Safe Reinforcement Learning (Safe RL) framework that
explicitly integrates moral considerations with standard driving objectives. At
the decision level, a Safe RL agent is trained using a composite ethical risk
cost, combining collision probability and harm severity, to generate high-level
motion targets. A dynamic Prioritized Experience Replay mechanism amplifies
learning from rare but critical, high-risk events. At the execution level,
polynomial path planning coupled with Proportional-Integral-Derivative (PID)
and Stanley controllers translates these targets into smooth, feasible
trajectories, ensuring both accuracy and comfort. We train and validate our
approach on rich, real-world traffic datasets encompassing diverse vehicles,
cyclists, and pedestrians, and demonstrate that it outperforms baseline methods
in reducing ethical risk and maintaining driving performance. To our knowledge,
this is the first study of ethical decision-making for autonomous vehicles via
Safe RL in real-world scenarios. Our results highlight the potential of
combining formal control theory and data-driven learning to advance ethically
accountable autonomy in complex, human-mixed traffic environments.

</details>


### [68] [Cohort-Aware Agents for Individualized Lung Cancer Risk Prediction Using a Retrieval-Augmented Model Selection Framework](https://arxiv.org/abs/2508.14940)
*Chongyu Qu,Allen J. Luna,Thomas Z. Li,Junchao Zhu,Junlin Guo,Juming Xiong,Kim L. Sandler,Bennett A. Landman,Yuankai Huo*

Main category: cs.LG

TL;DR: A two-stage retrieval-and-reasoning agent dynamically selects the best lung cancer risk model for each patient by retrieving the most relevant cohort via FAISS and asking an LLM to pick among eight models, enabling cohort-aware personalized prediction across nine real-world cohorts.


<details>
  <summary>Details</summary>
Motivation: To tackle substantial cross-cohort variability in model performance and the lack of a single best model, the work aims to tailor risk prediction to the individual patient by leveraging cohort knowledge and modern AI tools.

Method: Stage 1: FAISS-based cohort retrieval across nine diverse cohorts using patient CT and structured metadata. Stage 2: An LLM is prompted with the retrieved cohort and its performance metrics to recommend the optimal algorithm from eight models (Mayo, Brock, TDVIT, DLSTM, Liao, Sybil, DLS, DLI). The pipeline combines retrieval with reasoning to select a model per patient.

Result: The abstract presents a conceptual framework with no reported quantitative results; it claims dynamic, cohort-aware model selection and practical path toward individualized risk assessment.

Conclusion: The proposed agent enables flexible, cohort-driven model selection for real-world, personalized lung cancer risk prediction, potentially improving accuracy across diverse clinical populations.

Abstract: Accurate lung cancer risk prediction remains challenging due to substantial
variability across patient populations and clinical settings -- no single model
performs best for all cohorts. To address this, we propose a personalized lung
cancer risk prediction agent that dynamically selects the most appropriate
model for each patient by combining cohort-specific knowledge with modern
retrieval and reasoning techniques. Given a patient's CT scan and structured
metadata -- including demographic, clinical, and nodule-level features -- the
agent first performs cohort retrieval using FAISS-based similarity search
across nine diverse real-world cohorts to identify the most relevant patient
population from a multi-institutional database. Second, a Large Language Model
(LLM) is prompted with the retrieved cohort and its associated performance
metrics to recommend the optimal prediction algorithm from a pool of eight
representative models, including classical linear risk models (e.g., Mayo,
Brock), temporally-aware models (e.g., TDVIT, DLSTM), and multi-modal computer
vision-based approaches (e.g., Liao, Sybil, DLS, DLI). This two-stage agent
pipeline -- retrieval via FAISS and reasoning via LLM -- enables dynamic,
cohort-aware risk prediction personalized to each patient's profile. Building
on this architecture, the agent supports flexible and cohort-driven model
selection across diverse clinical populations, offering a practical path toward
individualized risk assessment in real-world lung cancer screening.

</details>


### [69] [Structure-Aware Temporal Modeling for Chronic Disease Progression Prediction](https://arxiv.org/abs/2508.14942)
*Jiacheng Hu,Bo Zhang,Ting Xu,Haifeng Yang,Min Gao*

Main category: cs.LG

TL;DR: A unified GNN-Transformer framework for Parkinson's disease progression prediction that fuses structural relationships among multimodal symptoms with temporal dynamics using a structure-aware fusion gate, improving accuracy and personalized trajectory modeling.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of symptom evolution complexity and insufficient temporal dependency modeling in PD progression prediction by integrating structural perception with temporal modeling.

Method: Construct a graph of multimodal clinical symptoms to capture structural relationships using graph neural networks; represent semantic dependencies via graph-based encodings; apply a Transformer to model dynamic temporal features; introduce a structure-aware gating mechanism to dynamically fuse structural encodings with temporal features; implement a multi-component pipeline (graph construction, temporal encoding, prediction output) and evaluate on real-world longitudinal PD data.

Result: On real-world longitudinal PD data, the approach outperforms mainstream models in AUC, RMSE, and IPW-F1, with better discrimination of progression stages and enhanced capture of personalized symptom trajectories; demonstrates strong generalization and scalability to chronic progressive diseases.

Conclusion: A unified, scalable framework that integrates structural perception and temporal modeling to improve PD progression prediction and potentially support intelligent modeling of chronic progressive diseases.

Abstract: This study addresses the challenges of symptom evolution complexity and
insufficient temporal dependency modeling in Parkinson's disease progression
prediction. It proposes a unified prediction framework that integrates
structural perception and temporal modeling. The method leverages graph neural
networks to model the structural relationships among multimodal clinical
symptoms and introduces graph-based representations to capture semantic
dependencies between symptoms. It also incorporates a Transformer architecture
to model dynamic temporal features during disease progression. To fuse
structural and temporal information, a structure-aware gating mechanism is
designed to dynamically adjust the fusion weights between structural encodings
and temporal features, enhancing the model's ability to identify key
progression stages. To improve classification accuracy and stability, the
framework includes a multi-component modeling pipeline, consisting of a graph
construction module, a temporal encoding module, and a prediction output layer.
The model is evaluated on real-world longitudinal Parkinson's disease data. The
experiments involve comparisons with mainstream models, sensitivity analysis of
hyperparameters, and graph connection density control. Results show that the
proposed method outperforms existing approaches in AUC, RMSE, and IPW-F1
metrics. It effectively distinguishes progression stages and improves the
model's ability to capture personalized symptom trajectories. The overall
framework demonstrates strong generalization and structural scalability,
providing reliable support for intelligent modeling of chronic progressive
diseases such as Parkinson's disease.

</details>


### [70] [HHNAS-AM: Hierarchical Hybrid Neural Architecture Search using Adaptive Mutation Policies](https://arxiv.org/abs/2508.14946)
*Anurag Tripathi,Ajeet Kumar Singh,Rajsabi Surya,Aum Gupta,Sahiinii Lemaina Veikho,Dorien Herremans,Sudhir Bisane*

Main category: cs.LG

TL;DR: Hierarchical Hybrid Neural Architecture Search with Adaptive Mutation Policies (HHNAS-AM) for text classification networks; uses template-driven, hierarchical search with Q-learning guided mutations; achieves notable gains (8% test accuracy on Spider) and consistent high-performing architectures.


<details>
  <summary>Details</summary>
Motivation: NAS in text classification faces enormous, often redundant search spaces and flat architectures; a structured, adaptive search strategy is needed to efficiently navigate the space and discover effective text representations.

Method: Introduce architectural templates to organize the search space; propose a hierarchical hybrid search framework; apply adaptive mutation policies via Q-learning to guide exploration; model is fully probabilistic to encourage exploration; evaluated on text-centric tasks (db_id prediction and Spider).

Result: Consistently discovers high-performing architectures across experiments; 8% improvement in test accuracy on the Spider dataset compared with existing baselines.

Conclusion: HHNAS-AM demonstrates that template-guided hierarchical NAS with adaptive mutations can effectively navigate large text-architecture search spaces, yielding improved performance and robust search behavior; the probabilistic approach supports comprehensive exploration.

Abstract: Neural Architecture Search (NAS) has garnered significant research interest
due to its capability to discover architectures superior to manually designed
ones. Learning text representation is crucial for text classification and other
language-related tasks. The NAS model used in text classification does not have
a Hybrid hierarchical structure, and there is no restriction on the
architecture structure, due to which the search space becomes very large and
mostly redundant, so the existing RL models are not able to navigate the search
space effectively. Also, doing a flat architecture search leads to an
unorganised search space, which is difficult to traverse. For this purpose, we
propose HHNAS-AM (Hierarchical Hybrid Neural Architecture Search with Adaptive
Mutation Policies), a novel approach that efficiently explores diverse
architectural configurations. We introduce a few architectural templates to
search on which organise the search spaces, where search spaces are designed on
the basis of domain-specific cues. Our method employs mutation strategies that
dynamically adapt based on performance feedback from previous iterations using
Q-learning, enabling a more effective and accelerated traversal of the search
space. The proposed model is fully probabilistic, enabling effective
exploration of the search space. We evaluate our approach on the database id
(db_id) prediction task, where it consistently discovers high-performing
architectures across multiple experiments. On the Spider dataset, our method
achieves an 8% improvement in test accuracy over existing baselines.

</details>


### [71] [Linear Preference Optimization: Decoupled Gradient Control via Absolute Regularization](https://arxiv.org/abs/2508.14947)
*Rui Wang,Qianguo Sun,Chao Song,Junlong Wu,Tianrong Chen,Zhiyun Zeng,Yu Li*

Main category: cs.LG

TL;DR: LPO improves on DPO by decoupling optimization dynamics via absolute-difference loss, stabilizing training with an offset constraint and positive regularization, and suppressing unwanted rejection through gradient separation with a tunable coefficient; shows consistent gains across text, math, and TTS tasks; code released.


<details>
  <summary>Details</summary>
Motivation: DPO is simple and stable but susceptible to overfitting and collapse; a more robust, tunable alignment method is needed to preserve quality while reducing degeneration.

Method: Three innovations: 1) gradient decoupling by using an absolute-difference loss instead of log-sigmoid to isolate optimization dynamics; 2) stability via an offset constraint plus a positive regularization term to preserve chosen response quality; 3) controllable rejection suppression using gradient separation with a tunable coefficient that regulates decline of rejection probability.

Result: Extensive experiments show LPO consistently improves performance on general text tasks, math tasks, and text-to-speech tasks; claims robustness and tunability; source code, models, and data released.

Conclusion: LPO provides a robust, tunable preference-alignment paradigm that overcomes DPO shortcomings; supports broader applicability across modalities; open-source resources accompany the work.

Abstract: DPO (Direct Preference Optimization) has become a widely used offline
preference optimization algorithm due to its simplicity and training stability.
However, DPO is prone to overfitting and collapse. To address these challenges,
we propose Linear Preference Optimization (LPO), a novel alignment framework
featuring three key innovations. First, we introduce gradient decoupling by
replacing the log-sigmoid function with an absolute difference loss, thereby
isolating the optimization dynamics. Second, we improve stability through an
offset constraint combined with a positive regularization term to preserve the
chosen response quality. Third, we implement controllable rejection suppression
using gradient separation with straightforward estimation and a tunable
coefficient that linearly regulates the descent of the rejection probability.
Through extensive experiments, we demonstrate that LPO consistently improves
performance on various tasks, including general text tasks, math tasks, and
text-to-speech (TTS) tasks. These results establish LPO as a robust and tunable
paradigm for preference alignment, and we release the source code, models, and
training data publicly.

</details>


### [72] [Large Foundation Model for Ads Recommendation](https://arxiv.org/abs/2508.14948)
*Shangyu Zhang,Shijie Quan,Zhongren Wang,Junwei Pan,Tianqu Zhuang,Bo Fu,Yilong Sun,Jieying Lin,Jushuo Chen,Xiaotian Li,Zhixiang Feng,Xian Hu,Huiting Deng,Hua Lu,Jinpeng Wang,Boqi Dai,Xiaoyu Chen,Bin Hu,Lili Huang,Yanwen Wu,Yeshou Cai,Qi Zhou,Huang Tang,Chunfeng Yang,Chengguo Yin,Tingyu Jiang,Lifeng Wang,Shudong Huang,Dapeng Liu,Lei Xiao,Haijie Gu,Shu-Tao Xia,Jie Jiang*

Main category: cs.LG

TL;DR: A comprehensive all-representation, multi-granularity transfer framework (LFM4Ads) for ads recommendation that transfers URs, IRs, and CRs from a foundation model, with layer-optimized extraction and multi-granularity adapters/modules, deployed at Tencent achieving a 2.45% GMV lift platform-wide.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of prior methods that use only user representations and single-transfer granularity. Seeks to leverage all representations (URs, IRs, CRs) and bridge upstream-downstream gaps to improve transferability and performance in ads recommendations.

Method: Transfer all representations (URs, IRs, CRs) from a pre-trained foundation model. For CRs, identify the optimal extraction layer and aggregate them into transferable coarse-grained forms. Employ multi-granularity mechanisms: non-linear adapters for feature-level transfer, Isomorphic Interaction Module for module-level transfer, and Standalone Retrieval for model-level transfer.

Result: Industrial deployment in Tencent’s platform, handling tens of billions of daily samples with terabyte-scale model parameters and billions of sparse embeddings across about 2000 features. Since Q4 2024 deployment, 10+ production launches across ad scenarios (e.g., Weixin Moments, Channels) with an overall GMV lift of 2.45% across the platform, implying hundreds of millions in estimated annual revenue increases.

Conclusion: Transferring all representations with multi-granularity mechanisms yields tangible gains in ads recommendation quality and revenue at industrial scale, validating the approach’s effectiveness and scalability.

Abstract: Online advertising relies on accurate recommendation models, with recent
advances using pre-trained large-scale foundation models (LFMs) to capture
users' general interests across multiple scenarios and tasks. However, existing
methods have critical limitations: they extract and transfer only user
representations (URs), ignoring valuable item representations (IRs) and
user-item cross representations (CRs); and they simply use a UR as a feature in
downstream applications, which fails to bridge upstream-downstream gaps and
overlooks more transfer granularities. In this paper, we propose LFM4Ads, an
All-Representation Multi-Granularity transfer framework for ads recommendation.
It first comprehensively transfers URs, IRs, and CRs, i.e., all available
representations in the pre-trained foundation model. To effectively utilize the
CRs, it identifies the optimal extraction layer and aggregates them into
transferable coarse-grained forms. Furthermore, we enhance the transferability
via multi-granularity mechanisms: non-linear adapters for feature-level
transfer, an Isomorphic Interaction Module for module-level transfer, and
Standalone Retrieval for model-level transfer. LFM4Ads has been successfully
deployed in Tencent's industrial-scale advertising platform, processing tens of
billions of daily samples while maintaining terabyte-scale model parameters
with billions of sparse embedding keys across approximately two thousand
features. Since its production deployment in Q4 2024, LFM4Ads has achieved 10+
successful production launches across various advertising scenarios, including
primary ones like Weixin Moments and Channels. These launches achieve an
overall GMV lift of 2.45% across the entire platform, translating to estimated
annual revenue increases in the hundreds of millions of dollars.

</details>


### [73] [Quantum Long Short-term Memory with Differentiable Architecture Search](https://arxiv.org/abs/2508.14955)
*Samuel Yen-Chi Chen,Prayag Tiwari*

Main category: cs.LG

TL;DR: End-to-end differentiable optimization of both VQC parameters and architecture in DiffQAS-QLSTM leads to improved performance on quantum sequence learning over handcrafted baselines.


<details>
  <summary>Details</summary>
Motivation: There is strong interest in QML for sequential data, but designing effective VQCs is hard and often task-specific; an end-to-end approach could jointly optimize parameters and architecture for scalable adaptive quantum sequence models.

Method: Introduce DiffQAS-QLSTM, an end-to-end differentiable framework that jointly optimizes VQC parameters and architecture selection during training for quantum recurrent models (QLSTM).

Result: DiffQAS-QLSTM consistently achieves lower loss than handcrafted baselines across diverse test settings.

Conclusion: This framework enables scalable, adaptive quantum sequence learning by integrating architecture search with parameter optimization.

Abstract: Recent advances in quantum computing and machine learning have given rise to
quantum machine learning (QML), with growing interest in learning from
sequential data. Quantum recurrent models like QLSTM are promising for
time-series prediction, NLP, and reinforcement learning. However, designing
effective variational quantum circuits (VQCs) remains challenging and often
task-specific. To address this, we propose DiffQAS-QLSTM, an end-to-end
differentiable framework that optimizes both VQC parameters and architecture
selection during training. Our results show that DiffQAS-QLSTM consistently
outperforms handcrafted baselines, achieving lower loss across diverse test
settings. This approach opens the door to scalable and adaptive quantum
sequence learning.

</details>


### [74] [CuMoLoS-MAE: A Masked Autoencoder for Remote Sensing Data Reconstruction](https://arxiv.org/abs/2508.14957)
*Anurup Naskar,Nathanael Zhixin Wong,Sara Shamekh*

Main category: cs.LG

TL;DR: A curriculum-guided masked autoencoder with Monte Carlo ensemble (CuMoLoS-MAE) restores fine-scale atmospheric features from noisy remote sensing data, learns a data-driven prior, and provides pixel-wise uncertainty via posterior predictive estimation.


<details>
  <summary>Details</summary>
Motivation: Remote-sensing atmospheric profiles suffer from low SNR, range folding, and discontinuities. Conventional gap-filling blurs fine details and standard deep models lack uncertainty estimates; there is a need for high-fidelity reconstructions with quantified confidence to improve diagnostics and data assimilation.

Method: CuMoLoS-MAE uses a Vision Transformer-based masked autoencoder trained with a mask-ratio curriculum that forces reconstruction from progressively sparser context. During inference, it performs Monte Carlo estimation over random mask realizations to approximate the posterior predictive, yielding a mean reconstruction and per-pixel uncertainty maps.

Result: The approach achieves high-fidelity restoration of fine-scale features (updraft/downdraft cores, shear lines, small vortices), learns a data-driven atmospheric prior, and provides calibrated per-pixel uncertainty. It supports enhanced convection diagnostics, real-time data assimilation, and improved long-term climate reanalysis.

Conclusion: A novel DL workflow that combines curriculum-guided masked autoencoding with Monte Carlo masking to deliver accurate atmospheric reconstructions with uncertainty quantification, enabling improved diagnostics and assimilation in atmospheric remote sensing.

Abstract: Accurate atmospheric profiles from remote sensing instruments such as Doppler
Lidar, Radar, and radiometers are frequently corrupted by low-SNR (Signal to
Noise Ratio) gates, range folding, and spurious discontinuities. Traditional
gap filling blurs fine-scale structures, whereas deep models lack confidence
estimates. We present CuMoLoS-MAE, a Curriculum-Guided Monte Carlo Stochastic
Ensemble Masked Autoencoder designed to (i) restore fine-scale features such as
updraft and downdraft cores, shear lines, and small vortices, (ii) learn a
data-driven prior over atmospheric fields, and (iii) quantify pixel-wise
uncertainty. During training, CuMoLoS-MAE employs a mask-ratio curriculum that
forces a ViT decoder to reconstruct from progressively sparser context. At
inference, we approximate the posterior predictive by Monte Carlo over random
mask realisations, evaluating the MAE multiple times and aggregating the
outputs to obtain the posterior predictive mean reconstruction together with a
finely resolved per-pixel uncertainty map. Together with high-fidelity
reconstruction, this novel deep learning-based workflow enables enhanced
convection diagnostics, supports real-time data assimilation, and improves
long-term climate reanalysis.

</details>


### [75] [Aura-CAPTCHA: A Reinforcement Learning and GAN-Enhanced Multi-Modal CAPTCHA System](https://arxiv.org/abs/2508.14976)
*Joydeep Chandra,Prabal Manhas,Ramanjot Kaur,Rashi Sahay*

Main category: cs.LG

TL;DR: Aura-CAPTCHA uses GAN-generated dynamic image challenges, RL-based difficulty tuning, and LLM-generated prompts to create a multi-modal CAPTCHA that improves resilience against AI-based bypassing, achieving 92% human success and 10% bot bypass in real-world traffic.


<details>
  <summary>Details</summary>
Motivation: Traditional CAPTCHAs are increasingly vulnerable to AI-based bypasses (OCR, adversarial image processing). There is a need for robust, scalable, multi-modal CAPTCHAs that adapt to user behavior while remaining accessible.

Method: GANs generate dynamic image challenges; RL tunes difficulty based on incorrect attempts, response time, and suspicious behavior; LLMs produce text and audio prompts. Visual challenges: 3x3 grid with at least three correct images. Audio challenges mix randomized numbers and words in a single task. Evaluated on real-world traffic with measured human success and bot bypass rates.

Result: 97?

Conclusion: Aura-CAPTCHA provides a robust, scalable multi-modal CAPTCHA leveraging GANs, RL, and LLMs to enhance resilience to automation while remaining accessible. Real-world results show high human success and low bot bypass, outperforming traditional CAPTCHA systems.

Abstract: Aura-CAPTCHA was developed as a multi-modal CAPTCHA system to address
vulnerabilities in traditional methods that are increasingly bypassed by AI
technologies, such as Optical Character Recognition (OCR) and adversarial image
processing. The design integrated Generative Adversarial Networks (GANs) for
generating dynamic image challenges, Reinforcement Learning (RL) for adaptive
difficulty tuning, and Large Language Models (LLMs) for creating text and audio
prompts. Visual challenges included 3x3 grid selections with at least three
correct images, while audio challenges combined randomized numbers and words
into a single task. RL adjusted difficulty based on incorrect attempts,
response time, and suspicious user behavior. Evaluations on real-world traffic
demonstrated a 92% human success rate and a 10% bot bypass rate, significantly
outperforming existing CAPTCHA systems. The system provided a robust and
scalable approach for securing online applications while remaining accessible
to users, addressing gaps highlighted in previous research.

</details>


### [76] [Generative Neural Operators of Log-Complexity Can Simultaneously Solve Infinitely Many Convex Programs](https://arxiv.org/abs/2508.14995)
*Anastasis Kratsios,Ariel Neufeld,Philipp Schmocker*

Main category: cs.LG

TL;DR: GEOs are finite-dimensional deep equilibrium operators designed to uniformly approximate solutions to a family of convex optimization problems over infinite-dimensional spaces, with logarithmic growth in network size relative to accuracy, bridging theory-practice gap in neural operators.


<details>
  <summary>Details</summary>
Motivation: Address the gap between worst-case universal approximation bounds (which imply many parameters) and practical evidence of efficiency in neural operators by focusing on a concrete, realizable class (generative equilibrium operators) for convex problems in Hilbert spaces.

Method: Introduce Generative Equilibrium Operators (GEOs) built from finite-dimensional deep equilibrium layers. Inputs are smooth convex loss functions on a separable Hilbert space X; outputs are approximate solutions to the corresponding optimization problem. Prove uniform approximation of the solutions for losses lying in suitable infinite-dimensional compact sets, with rank/depth/width growing only logarithmically in 1/ε. Validate theory and trainability on three applications: nonlinear PDEs, stochastic optimal control, and liquidity-constrained hedging in finance.

Result: Theoretical result: uniform approximation with logarithmic growth in model complexity. Empirical validation across three domains—nonlinear PDEs, stochastic control, and finance hedging—demonstrating trainability and practical performance.

Conclusion: GEOs can close the theory-practice gap for a class of neural operators by providing scalable, finite-dimensional, trainable models that uniformly approximate solutions to a broad family of convex optimization problems in infinite-dimensional spaces.

Abstract: Neural operators (NOs) are a class of deep learning models designed to
simultaneously solve infinitely many related problems by casting them into an
infinite-dimensional space, whereon these NOs operate. A significant gap
remains between theory and practice: worst-case parameter bounds from universal
approximation theorems suggest that NOs may require an unrealistically large
number of parameters to solve most operator learning problems, which stands in
direct opposition to a slew of experimental evidence. This paper closes that
gap for a specific class of {NOs}, generative {equilibrium operators} (GEOs),
using (realistic) finite-dimensional deep equilibrium layers, when solving
families of convex optimization problems over a separable Hilbert space $X$.
Here, the inputs are smooth, convex loss functions on $X$, and outputs are the
associated (approximate) solutions to the optimization problem defined by each
input loss.
  We show that when the input losses lie in suitable infinite-dimensional
compact sets, our GEO can uniformly approximate the corresponding solutions to
arbitrary precision, with rank, depth, and width growing only logarithmically
in the reciprocal of the approximation error. We then validate both our
theoretical results and the trainability of GEOs on three applications: (1)
nonlinear PDEs, (2) stochastic optimal control problems, and (3) hedging
problems in mathematical finance under liquidity constraints.

</details>


### [77] [Quantized Neural Networks for Microcontrollers: A Comprehensive Review of Methods, Platforms, and Applications](https://arxiv.org/abs/2508.15008)
*Hamza A. Abushahla,Dara Varam,Ariel J. N. Panopio,Mohamed I. AlHajri*

Main category: cs.LG

TL;DR: A hardware-centric survey on QNN quantization for TinyML, detailing techniques, trade-offs, frameworks, platforms, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: Efficient deployment of quantized neural networks on microcontrollers and embedded devices under strict resource limits, requiring balancing accuracy, latency, memory, and energy through hardware-software co-design.

Method: Systematic literature review of quantization techniques; survey of software frameworks and hardware platforms; analysis of trade-offs between model performance and hardware constraints; synthesis of guidelines and future directions.

Result: Provides a structured taxonomy of quantization techniques for embedded deployment, comparative analysis of software frameworks and hardware targets, and discussion of practical trade-offs, design guidelines, gaps, and future research directions.

Conclusion: Quantized neural networks are essential for TinyML; success hinges on integrated hardware–software optimization, with ongoing advances in quantization methods, tooling, and platform support to meet memory, compute, and energy constraints.

Abstract: The deployment of Quantized Neural Networks (QNNs) on resource-constrained
devices, such as microcontrollers, has introduced significant challenges in
balancing model performance, computational complexity and memory constraints.
Tiny Machine Learning (TinyML) addresses these issues by integrating
advancements across machine learning algorithms, hardware acceleration, and
software optimization to efficiently run deep neural networks on embedded
systems. This survey presents a hardware-centric introduction to quantization,
systematically reviewing essential quantization techniques employed to
accelerate deep learning models for embedded applications. In particular,
further emphasis is put on critical trade-offs among model performance and
hardware capabilities. The survey further evaluates existing software
frameworks and hardware platforms designed specifically for supporting QNN
execution on microcontrollers. Moreover, we provide an analysis of the current
challenges and an outline of promising future directions in the rapidly
evolving domain of QNN deployment.

</details>


### [78] [TOAST: Fast and scalable auto-partitioning based on principled static analysis](https://arxiv.org/abs/2508.15010)
*Sami Alabed,Dominik Grewe,Norman Alexander Rink,Timur Sitdikov,Agnieszka Swietlik,Dimitrios Vytiniotis,Daniel Belov*

Main category: cs.LG

TL;DR: A static compiler analysis plus Monte Carlo Tree Search for hardware-aware partitioning of large ML models across distributed accelerators, outperforming industrial baselines and fully automated.


<details>
  <summary>Details</summary>
Motivation: Auto-partitioners struggle with out-of-memory and slowness due to exploring an exponentially large search space; restricting the search often yields infeasible or suboptimal partitions.

Method: A static compiler analysis identifies tensor dimensions requiring identical sharding and partitioning conflicts to resolve, then uses Monte Carlo Tree Search to explore feasible partitionings within this reduced decision space.

Result: System significantly outperforms state-of-the-art industrial methods across diverse hardware platforms and model architectures, discovering previously unknown, superior partitionings.

Conclusion: The approach provides a fully automated, scalable solution for partitioning large models across distributed accelerators, mitigating memory constraints and search time while improving performance.

Abstract: Partitioning large machine learning models across distributed accelerator
systems is a complex process, requiring a series of interdependent decisions
that are further complicated by internal sharding ambiguities. Consequently,
existing auto-partitioners often suffer from out-of-memory errors or are
prohibitively slow when exploring the exponentially large space of possible
partitionings. To mitigate this, they artificially restrict the search space,
but this approach frequently yields infeasible solutions that violate device
memory constraints or lead to sub-optimal performance.
  We propose a system that combines a novel static compiler analysis with a
Monte Carlo Tree Search. Our analysis constructs an efficient decision space by
identifying (i) tensor dimensions requiring identical sharding, and (ii)
partitioning "conflicts" that require resolution.
  Our system significantly outperforms state-of-the-art industrial methods
across diverse hardware platforms and model architectures, discovering
previously unknown, superior solutions, and the process is fully automated even
for complex and large models.

</details>


### [79] [Fragment-Wise Interpretability in Graph Neural Networks via Molecule Decomposition and Contribution Analysis](https://arxiv.org/abs/2508.15015)
*Sebastian Musiał,Bartosz Zieliński,Tomasz Danel*

Main category: cs.LG

TL;DR: SEAL is an interpretable GNN for molecules that attributes predictions to chemically meaningful subgraphs by fragment-based attribution and limited inter-fragment message passing, achieving superior attribution accuracy and human-aligned explanations.


<details>
  <summary>Details</summary>
Motivation: Molecular property prediction with GNNs is powerful but often opaque. There is a need for reliable, human-aligned explanations of how individual atoms/substructures drive predictions, and existing explainers struggle due to entangled message passing.

Method: Decompose input graphs into chemically relevant fragments. Train an attribution model (attribution learning) to estimate the causal influence of fragments on the output. Modify the GNN architecture to explicitly reduce inter-fragment message passing, aligning fragment-level contributions with predictions.

Result: SEAL outperforms baseline explainability methods on synthetic and real molecular datasets across attribution metrics and human-aligned interpretability; a user study with domain experts shows explanations are more intuitive and trustworthy.

Conclusion: SEAL advances the interpretability of molecular GNNs by enabling substructure-level explanations with improved trustworthiness, suggesting a promising path toward transparent and actionable molecular modeling.

Abstract: Graph neural networks have demonstrated remarkable success in predicting
molecular properties by leveraging the rich structural information encoded in
molecular graphs. However, their black-box nature reduces interpretability,
which limits trust in their predictions for important applications such as drug
discovery and materials design. Furthermore, existing explanation techniques
often fail to reliably quantify the contribution of individual atoms or
substructures due to the entangled message-passing dynamics. We introduce SEAL
(Substructure Explanation via Attribution Learning), a new interpretable graph
neural network that attributes model predictions to meaningful molecular
subgraphs. SEAL decomposes input graphs into chemically relevant fragments and
estimates their causal influence on the output. The strong alignment between
fragment contributions and model predictions is achieved by explicitly reducing
inter-fragment message passing in our proposed model architecture. Extensive
evaluations on synthetic benchmarks and real-world molecular datasets
demonstrate that SEAL outperforms other explainability methods in both
quantitative attribution metrics and human-aligned interpretability. A user
study further confirms that SEAL provides more intuitive and trustworthy
explanations to domain experts. By bridging the gap between predictive
performance and interpretability, SEAL offers a promising direction for more
transparent and actionable molecular modeling.

</details>


### [80] [Twin-Boot: Uncertainty-Aware Optimization via Online Two-Sample Bootstrapping](https://arxiv.org/abs/2508.15019)
*Carlos Stein Brito*

Main category: cs.LG

TL;DR: Twin-Bootstrap Gradient Descent trains two identical networks on bootstrap samples with a periodic mean-reset to stay in the same basin; their divergence estimates local, within-basin uncertainty and is used to adaptively regularize weights toward flatter minima, improving calibration and generalization with interpretable uncertainty maps.


<details>
  <summary>Details</summary>
Motivation: Uncertainty estimates are essential but hard to obtain in deep learning, especially in overparameterized or low-data regimes. Traditional bootstrapping is impractical for deep nets and fails in non-convex landscapes where optima vary across runs.

Method: Two identical models are trained in parallel on independent bootstrap samples. A periodic mean-reset keeps both trajectories in the same basin. The divergence between the models serves as a local uncertainty measure, which is then used to adaptively sample weights, providing data-driven regularization that biases toward flatter minima.

Result: The approach improves calibration and generalization in deep neural networks and complex high-dimensional inverse problems and yields interpretable uncertainty maps.

Conclusion: Twin-Boot offers a practical framework that integrates uncertainty estimation into optimization, yielding better-calibrated predictions, improved generalization, and interpretable uncertainty quantification through training-time adaptive regularization.

Abstract: Standard gradient descent methods yield point estimates with no measure of
confidence. This limitation is acute in overparameterized and low-data regimes,
where models have many parameters relative to available data and can easily
overfit. Bootstrapping is a classical statistical framework for uncertainty
estimation based on resampling, but naively applying it to deep learning is
impractical: it requires training many replicas, produces post-hoc estimates
that cannot guide learning, and implicitly assumes comparable optima across
runs - an assumption that fails in non-convex landscapes. We introduce
Twin-Bootstrap Gradient Descent (Twin-Boot), a resampling-based training
procedure that integrates uncertainty estimation into optimization. Two
identical models are trained in parallel on independent bootstrap samples, and
a periodic mean-reset keeps both trajectories in the same basin so that their
divergence reflects local (within-basin) uncertainty. During training, we use
this estimate to sample weights in an adaptive, data-driven way, providing
regularization that favors flatter solutions. In deep neural networks and
complex high-dimensional inverse problems, the approach improves calibration
and generalization and yields interpretable uncertainty maps.

</details>


### [81] [Nonlinear Federated System Identification](https://arxiv.org/abs/2508.15025)
*Omkar Tupe,Max Hartman,Lav R. Varshney,Saurav Prakash*

Main category: cs.LG

TL;DR: Federated learning for linearly-parameterized nonlinear system identification improves convergence with more clients; the rate gap to centralized is a constant that depends on the feature map, which can be chosen to boost excitation in nonlinear settings.


<details>
  <summary>Details</summary>
Motivation: Assess whether federated nonlinear system identification can match or surpass centralized methods and how the number of clients affects convergence; explore how feature map choice influences excitation and performance.

Method: Theoretical analysis establishing convergence rates for federated vs centralized identification in the nonlinear, linearly-parameterized setting; shows the rate gap is a constant determined by the feature map φ. Proposes selecting φ to increase excitation in the nonlinear regime. Experimental validation on physical systems with i.i.d. control inputs and random perturbations, using real-analytic feature functions (polynomials, trigonometric terms) modeling pendulum/quadrotor dynamics; analyzes convergence under different noise levels and data distributions.

Result: Convergence of each client improves as the number of participating clients grows, with federated learning delivering a consistent gain over isolated learning. In both linear and nonlinear cases, the rates differ only by a constant that depends on φ; by choosing φ to enhance excitation, the nonlinear setting can achieve stronger performance.

Conclusion: Federated nonlinear system identification offers practical gains over isolated approaches, with feature-map design playing a critical role in achieving excitation and improving convergence. The framework validates applicability to real nonlinear physical systems and shows robustness to varying data distributions and noise.

Abstract: We consider federated learning of linearly-parameterized nonlinear systems.
We establish theoretical guarantees on the effectiveness of federated nonlinear
system identification compared to centralized approaches, demonstrating that
the convergence rate improves as the number of clients increases. Although the
convergence rates in the linear and nonlinear cases differ only by a constant,
this constant depends on the feature map $\phi$, which can be carefully chosen
in the nonlinear setting to increase excitation and improve performance. We
experimentally validate our theory in physical settings where client devices
are driven by i.i.d. control inputs and control policies exhibiting i.i.d.
random perturbations, ensuring non-active exploration. Experiments use
trajectories from nonlinear dynamical systems characterized by real-analytic
feature functions, including polynomial and trigonometric components,
representative of physical systems including pendulum and quadrotor dynamics.
We analyze the convergence behavior of the proposed method under varying noise
levels and data distributions. Results show that federated learning
consistently improves convergence of any individual client as the number of
participating clients increases.

</details>


### [82] [Rethinking the Potential of Layer Freezing for Efficient DNN Training](https://arxiv.org/abs/2508.15033)
*Chence Yang,Ci Zhang,Lei Lu,Qitao Tan,Sheng Li,Ao Li,Xulong Tang,Shaoyi Huang,Jinzhen Wang,Guoming Li,Jundong Li,Xiaoming Zhai,Jin Lu,Geng Yuan*

Main category: cs.LG

TL;DR: This work proposes a caching-based layer-freezing approach for DNN training that avoids forward passes through frozen layers by storing their outputs as a dataset, and introduces similarity-aware channel augmentation and progressive compression to manage augmentation effects and storage overhead, achieving large training cost reductions with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: As neural networks and datasets grow, training cost rises dramatically. Traditional layer-freezing still requires forward propagation through frozen layers to produce feature maps, limiting cost reductions. Caching feature maps seems straightforward but faces key issues: how to apply augmentations to cached feature maps, and substantial storage overhead. This work identifies these challenges and provides a systematic solution to address them.

Method: Propose similarity-aware channel augmentation that caches channels with high augmentation sensitivity to improve accuracy with minimal extra storage. Integrate lossy data compression into layer freezing via progressive compression, increasing compression rates as more layers become frozen to reduce storage costs. Conduct comprehensive evaluation of freezing and compression strategies to guide practical application and optimization.

Result: Demonstrates significant reductions in training cost while maintaining model accuracy, with only minor time overhead. Provides extensive evaluation and practical insights for effectively applying freezing and compression strategies in DNN training.

Conclusion: Offers a practical, systematic solution to cache-based layer-freezing challenges, enabling efficient DNN training at scale. The approach provides guidance on when and how to apply augmentation and compression to maximize training efficiency while preserving accuracy.

Abstract: With the growing size of deep neural networks and datasets, the computational
costs of training have significantly increased. The layer-freezing technique
has recently attracted great attention as a promising method to effectively
reduce the cost of network training. However, in traditional layer-freezing
methods, frozen layers are still required for forward propagation to generate
feature maps for unfrozen layers, limiting the reduction of computation costs.
To overcome this, prior works proposed a hypothetical solution, which caches
feature maps from frozen layers as a new dataset, allowing later layers to
train directly on stored feature maps. While this approach appears to be
straightforward, it presents several major challenges that are severely
overlooked by prior literature, such as how to effectively apply augmentations
to feature maps and the substantial storage overhead introduced. If these
overlooked challenges are not addressed, the performance of the caching method
will be severely impacted and even make it infeasible. This paper is the first
to comprehensively explore these challenges and provides a systematic solution.
To improve training accuracy, we propose \textit{similarity-aware channel
augmentation}, which caches channels with high augmentation sensitivity with a
minimum additional storage cost. To mitigate storage overhead, we incorporate
lossy data compression into layer freezing and design a \textit{progressive
compression} strategy, which increases compression rates as more layers are
frozen, effectively reducing storage costs. Finally, our solution achieves
significant reductions in training cost while maintaining model accuracy, with
a minor time overhead. Additionally, we conduct a comprehensive evaluation of
freezing and compression strategies, providing insights into optimizing their
application for efficient DNN training.

</details>


### [83] [Robust Estimation Under Heterogeneous Corruption Rates](https://arxiv.org/abs/2508.15051)
*Syomantak Chaudhuri,Jerry Li,Thomas A. Courtade*

Main category: cs.LG

TL;DR: The authors study robust estimation under known, non-identical sample-wise corruption rates and derive minimax rates for several tasks; they show a thresholding phenomenon where samples with high corruption are discarded, and the rates depend on the corruption-rate distribution (up to a sqrt(d) gap in some multivariate problems).


<details>
  <summary>Details</summary>
Motivation: Heterogeneous corruption arises in distributed/federated learning, crowdsourcing, and sensor networks; existing robust methods assume uniform or worst-case corruption and miss structural heterogeneity; there is a need for minimax characterizations that incorporate heterogeneous corruption patterns.

Method: Derive minimax rates for mean estimation under heterogeneous Bernoulli-type corruption for univariate Gaussian and multivariate bounded distributions; for multivariate Gaussian mean estimation and linear regression in higher dimensions, establish minimax rates for squared error up to a factor of sqrt(d); use analysis showing that an adaptive sample discard strategy (thresholding) achieves optimal rates, with the threshold determined by the empirical distribution of corruption rates.

Result: Tight minimax rates are established for mean estimation under heterogeneous corruption patterns for univariate and multivariate distributions. For multivariate Gaussian mean estimation and linear regression, the minimax rate is shown to hold up to a factor sqrt(d). The results reveal a threshold phenomenon: beyond a certain corruption level, optimal estimators discard samples, and this threshold is dictated by the observed corruption-rate distribution.

Conclusion: Heterogeneous corruption materially affects the information in the data; optimal estimators should adaptively discard highly corrupted samples according to the corruption-rate distribution, yielding minimax rates that depend on that distribution and, in high dimensions, incur a sqrt(d) loss factor. These insights guide the design of robust estimators in heterogeneous data environments.

Abstract: We study the problem of robust estimation under heterogeneous corruption
rates, where each sample may be independently corrupted with a known but
non-identical probability. This setting arises naturally in distributed and
federated learning, crowdsourcing, and sensor networks, yet existing robust
estimators typically assume uniform or worst-case corruption, ignoring
structural heterogeneity. For mean estimation for multivariate bounded
distributions and univariate gaussian distributions, we give tight minimax
rates for all heterogeneous corruption patterns. For multivariate gaussian mean
estimation and linear regression, we establish the minimax rate for squared
error up to a factor of $\sqrt{d}$, where $d$ is the dimension. Roughly, our
findings suggest that samples beyond a certain corruption threshold may be
discarded by the optimal estimators -- this threshold is determined by the
empirical distribution of the corruption rates given.

</details>


### [84] [Enhancing Optimizer Stability: Momentum Adaptation of The NGN Step-size](https://arxiv.org/abs/2508.15071)
*Rustem Islamov,Niccolo Ajroldi,Antonio Orvieto,Aurelien Lucchi*

Main category: cs.LG

TL;DR: NGN-M: a momentum-based NGN step-size adaptation that matches state-of-the-art optimizers while being more robust to step-size hyperparameter choices; achieves O(1/√K) convergence under weaker assumptions.


<details>
  <summary>Details</summary>
Motivation: Hyperparameter sensitivity of momentum/adaptive optimizers, especially step-size, leads to costly tuning. There is a need for algorithms that perform well across a wide range of step-sizes.

Method: Introduce NGN-M, a momentum-based extension of the NGN step-size method. Theoretical analysis shows standard 1/√K convergence under weaker conditions (no interpolation, no bounded gradients/iterates). Empirical experiments compare NGN-M against other optimizers, highlighting robustness to step-size choice.

Result: NGN-M achieves performance comparable to or better than state-of-the-art optimizers and demonstrates enhanced robustness to step-size hyperparameters in practice.

Conclusion: NGN-M provides stable and competitive optimization with reduced sensitivity to step-size, broadening practical applicability of adaptive/momentum-based methods.

Abstract: Modern optimization algorithms that incorporate momentum and adaptive
step-size offer improved performance in numerous challenging deep learning
tasks. However, their effectiveness is often highly sensitive to the choice of
hyperparameters, especially the step-size. Tuning these parameters is often
difficult, resource-intensive, and time-consuming. Therefore, recent efforts
have been directed toward enhancing the stability of optimizers across a wide
range of hyperparameter choices [Schaipp et al., 2024]. In this paper, we
introduce an algorithm that matches the performance of state-of-the-art
optimizers while improving stability to the choice of the step-size
hyperparameter through a novel adaptation of the NGN step-size method [Orvieto
and Xiao, 2024]. Specifically, we propose a momentum-based version (NGN-M) that
attains the standard convergence rate of $\mathcal{O}(1/\sqrt{K})$ under less
restrictive assumptions, without the need for interpolation condition or
assumptions of bounded stochastic gradients or iterates, in contrast to
previous approaches. Additionally, we empirically demonstrate that the
combination of the NGN step-size with momentum results in enhanced robustness
to the choice of the step-size hyperparameter while delivering performance that
is comparable to or surpasses other state-of-the-art optimizers.

</details>


### [85] [Wormhole Dynamics in Deep Neural Networks](https://arxiv.org/abs/2508.15086)
*Yen-Lung Lai,Zhe Jin*

Main category: cs.LG

TL;DR: Overparameterized DNNs exhibit a collapse of the output feature space that initially improves generalization but eventually leads to degeneracy where distinct inputs map to the same output, causing zero loss; a proposed 'wormhole' solution bypasses this degeneracy and links fooling examples to meaningful labels, offering a new view on shortcut learning.


<details>
  <summary>Details</summary>
Motivation: Understand generalization and fooling examples from a theoretical, maximum-likelihood perspective, avoiding reliance on gradient-based optimization and explicit labels.

Method: An analytical framework based on maximum likelihood estimation; analysis of overparameterized regimes, output-space collapse and degeneracy; introduction of a 'wormhole' solution to bypass degeneracy in fooling examples.

Result: Demonstrates that output-space collapse can improve generalization but deeper networks induce degeneracy; the wormhole solution resolves this by reconciling fooling inputs with meaningful labels, offering a novel lens on shortcut learning.

Conclusion: Provides deeper theoretical insights into DNN generalization, identifies the limits of overparameterized regimes, and suggests future work in learning dynamics for unsupervised settings to bridge theory and practice.

Abstract: This work investigates the generalization behavior of deep neural networks
(DNNs), focusing on the phenomenon of "fooling examples," where DNNs
confidently classify inputs that appear random or unstructured to humans. To
explore this phenomenon, we introduce an analytical framework based on maximum
likelihood estimation, without adhering to conventional numerical approaches
that rely on gradient-based optimization and explicit labels. Our analysis
reveals that DNNs operating in an overparameterized regime exhibit a collapse
in the output feature space. While this collapse improves network
generalization, adding more layers eventually leads to a state of degeneracy,
where the model learns trivial solutions by mapping distinct inputs to the same
output, resulting in zero loss. Further investigation demonstrates that this
degeneracy can be bypassed using our newly derived "wormhole" solution. The
wormhole solution, when applied to arbitrary fooling examples, reconciles
meaningful labels with random ones and provides a novel perspective on shortcut
learning. These findings offer deeper insights into DNN generalization and
highlight directions for future research on learning dynamics in unsupervised
settings to bridge the gap between theory and practice.

</details>


### [86] [Evaluating Sparse Autoencoders for Monosemantic Representation](https://arxiv.org/abs/2508.15094)
*Moghis Fereidouni,Muhammad Umair Haider,Peizhong Ju,A. B. Siddique*

Main category: cs.LG

TL;DR: Sparse autoencoders (SAEs) reduce polysemanticity and improve concept separability in LLM neurons compared to base models, but higher sparsity can hurt downstream performance; concept-level interventions like APP enable more precise targeted suppression.


<details>
  <summary>Details</summary>
Motivation: Address polysemanticity in large language models by quantifying monosemanticity with a Jensen-Shannon-based separability score and systematically comparing SAEs to base models; evaluate practical concept-level interventions.

Method: Evaluate Gemma-2-2B with multiple SAE variants over five benchmarks; compute a fine-grained concept separability score via Jensen-Shannon distance; test two intervention strategies (full neuron masking and partial suppression); propose Attenuation via Posterior Probabilities (APP) using concept-conditioned activation distributions for targeted suppression; compare against existing methods.

Result: SAEs reduce polysemanticity and increase concept separability relative to base models; however, greater sparsity does not consistently improve separability and can impair downstream performance; partial suppression affords more precise concept-level control; APP outperforms existing approaches for targeted concept removal.

Conclusion: SAEs offer interpretability benefits with caveats about sparsity levels; achieving a balance is important; APP provides a strong, targeted intervention for removing specific concepts, suggesting a practical path for controllable, monosemantic activation in LLMs.

Abstract: A key barrier to interpreting large language models is polysemanticity, where
neurons activate for multiple unrelated concepts. Sparse autoencoders (SAEs)
have been proposed to mitigate this issue by transforming dense activations
into sparse, more interpretable features. While prior work suggests that SAEs
promote monosemanticity, there has been no quantitative comparison with their
base models. This paper provides the first systematic evaluation of SAEs
against base models concerning monosemanticity. We introduce a fine-grained
concept separability score based on the Jensen-Shannon distance, which captures
how distinctly a neuron's activation distributions vary across concepts. Using
Gemma-2-2B and multiple SAE variants across five benchmarks, we show that SAEs
reduce polysemanticity and achieve higher concept separability. However,
greater sparsity of SAEs does not always yield better separability and often
impairs downstream performance. To assess practical utility, we evaluate
concept-level interventions using two strategies: full neuron masking and
partial suppression. We find that, compared to base models, SAEs enable more
precise concept-level control when using partial suppression. Building on this,
we propose Attenuation via Posterior Probabilities (APP), a new intervention
method that uses concept-conditioned activation distributions for targeted
suppression. APP outperforms existing approaches in targeted concept removal.

</details>


### [87] [Hydra: A 1.6B-Parameter State-Space Language Model with Sparse Attention, Mixture-of-Experts, and Memory](https://arxiv.org/abs/2508.15099)
*Siddharth Chaudhary,Bennett Browning*

Main category: cs.LG

TL;DR: A blueprint for a modular, hybrid long-context LLM architecture (Hydra) at ~1.6B params, combining SSM backbones, intermittent sparse attention, chunk-level MoE routing, and dual memory systems; toy-scale results show feasibility but not full-scale performance, highlighting future empirical validation.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of efficient, scalable, long-context language modeling by proposing a modular architecture that blends structured state-space modeling, selective attention, mixture-of-experts, and persistent memory; provide a blueprint to guide empirical exploration and future validation of end-task gains at scale.

Method: Propose an architectural design: Mamba-style Structured State Space Model backbone with intermittent sparse global attention, chunk-level MoE feed-forward routing, and dual memories (workspace + factual PKM). Formalize component interfaces, provide parameter and complexity accounting, and outline a staged curriculum to stably activate parts. Include toy-scale prototype measurements (tens of millions of parameters on synthetic data) to demonstrate feasibility and qualitative scaling (e.g., long-context throughput crossover, controllable expert routing) rather than full-scale performance claims.

Result: Toy-scale prototype measurements on synthetic data with tens of millions of parameters demonstrate feasibility of implementation and show qualitative scaling trends (long-context throughput crossover, controllable expert routing). No claims of competitive full-scale performance.

Conclusion: Hydra is positioned as a blueprint to stimulate empirical follow-up for modular, input-adaptive long-context LMs. Explicit open risks (training complexity, memory utilization, specialization dynamics) are acknowledged, and validating end-task gains at target scale remains future work.

Abstract: We present Hydra as an architectural proposal for hybrid long-context
language models that combine conditional computation, long-context memory
mechanisms, and sparse mixture-of-experts within an approximately 1.6B
parameter design envelope. Hydra integrates a Mamba-style Structured State
Space Model (SSM) backbone with intermittent sparse global attention,
chunk-level MoE feed-forward routing, and dual (workspace plus factual PKM)
memories. We formalize the component interfaces, give transparent parameter and
complexity accounting, and outline a staged curriculum intended to stably
activate the parts. We accompany the specification with illustrative toy-scale
prototype measurements (tens of millions of parameters on synthetic data) whose
sole purpose is to demonstrate implementation feasibility and qualitative
scaling behaviors (for example, long-context throughput crossover and
controllable expert routing), not to claim competitive full-scale performance.
We explicitly delineate assumptions and open risks (training complexity, memory
utilization, specialization dynamics) and position Hydra as a blueprint to
stimulate empirical follow-up rather than a finished system. By combining SSM
efficiency, selective sparse attention, MoE capacity, and learnable memory,
Hydra sketches a path toward modular, input-adaptive long-context language
models; validating end-task gains at target scale remains future work.

</details>


### [88] [Side Effects of Erasing Concepts from Diffusion Models](https://arxiv.org/abs/2508.15124)
*Shaswati Saha,Sourajit Saha,Manas Gaur,Tejas Gokhale*

Main category: cs.LG

TL;DR: CETs for erasing target concepts in text-to-image models are fragile and easy to circumvent; the authors introduce Side Effect Evaluation (SEE) to measure robustness and side effects, finding issues like attribute leakage and attention shifts; they release data, code, and tools to advance robust concept erasure.


<details>
  <summary>Details</summary>
Motivation: With growing concerns about privacy, copyright, and safety in text-to-image generation, there is a need to evaluate and improve concept erasure techniques beyond simply removing a target concept, ensuring negligible impact on related concepts and preventing side-channel leakage.

Method: Propose Side Effect Evaluation (SEE) benchmark consisting of hierarchical and compositional prompts that describe objects and attributes. Develop an automated evaluation pipeline to quantify CET side effects across three axes: impact on neighboring concepts, evasion of targets, and attribute leakage. Conduct experiments showing CETs can be circumvented via superclass-subclass relationships and semantically similar prompts, and analyze attention behavior.

Result: CETs are vulnerable to circumvention; SEE reveals side effects including attribute leakage and non-intuitive attention patterns (concentration or dispersal) when erasing concepts; the work provides a dataset, code, and tooling for robust evaluation.

Conclusion: Encourages building more robust CETs and provides resources (dataset, code, tools) to facilitate future research on reliable concept erasure and safety in T2I models.

Abstract: Concerns about text-to-image (T2I) generative models infringing on privacy,
copyright, and safety have led to the development of Concept Erasure Techniques
(CETs).
  The goal of an effective CET is to prohibit the generation of undesired
``target'' concepts specified by the user, while preserving the ability to
synthesize high-quality images of the remaining concepts.
  In this work, we demonstrate that CETs can be easily circumvented and present
several side effects of concept erasure.
  For a comprehensive measurement of the robustness of CETs, we present Side
Effect Evaluation (\see), an evaluation benchmark that consists of hierarchical
and compositional prompts that describe objects and their attributes.
  This dataset and our automated evaluation pipeline quantify side effects of
CETs across three aspects: impact on neighboring concepts, evasion of targets,
and attribute leakage.
  Our experiments reveal that CETs can be circumvented by using
superclass-subclass hierarchy and semantically similar prompts, such as
compositional variants of the target. We show that CETs suffer from attribute
leakage and counterintuitive phenomena of attention concentration or dispersal.
  We release our dataset, code, and evaluation tools to aid future work on
robust concept erasure.

</details>


### [89] [Towards Source-Free Machine Unlearning](https://arxiv.org/abs/2508.15127)
*Sk Miraj Ahmed,Umit Yigit Basaran,Dripta S. Raychaudhuri,Arindam Dutta,Rohit Kundu,Fahim Faisal Niloy,Basak Guler,Amit K. Roy-Chowdhury*

Main category: cs.LG

TL;DR: Proposes a source-free unlearning method that estimates the Hessian of the remaining data to enable zero-shot unlearning with theoretical guarantees and maintained performance on the remaining data.


<details>
  <summary>Details</summary>
Motivation: The need to remove private or copyrighted information from trained models without access to the original training data, due to privacy/copyright regulations and practicality of data access.

Method: Estimating the Hessian of the unknown remaining training data and using this estimate to perform unlearning. The method enables zero-shot unlearning and provides theoretical guarantees on unlearning performance while preserving performance on the remaining data.

Result: Extensive experiments across diverse datasets validating the efficacy and robustness of the proposed source-free unlearning approach.

Conclusion: The work enables practical source-free unlearning by leveraging Hessian estimation to achieve efficient, theoretically grounded zero-shot unlearning with maintained accuracy on remaining data.

Abstract: As machine learning becomes more pervasive and data privacy regulations
evolve, the ability to remove private or copyrighted information from trained
models is becoming an increasingly critical requirement. Existing unlearning
methods often rely on the assumption of having access to the entire training
dataset during the forgetting process. However, this assumption may not hold
true in practical scenarios where the original training data may not be
accessible, i.e., the source-free setting. To address this challenge, we focus
on the source-free unlearning scenario, where an unlearning algorithm must be
capable of removing specific data from a trained model without requiring access
to the original training dataset. Building on recent work, we present a method
that can estimate the Hessian of the unknown remaining training data, a crucial
component required for efficient unlearning. Leveraging this estimation
technique, our method enables efficient zero-shot unlearning while providing
robust theoretical guarantees on the unlearning performance, while maintaining
performance on the remaining data. Extensive experiments over a wide range of
datasets verify the efficacy of our method.

</details>


### [90] [Universal Reinforcement Learning in Coalgebras: Asynchronous Stochastic Computation via Conduction](https://arxiv.org/abs/2508.15128)
*Sridhar Mahadevan*

Main category: cs.LG

TL;DR: A category-theoretic generalization of reinforcement learning (URL) that uses coinduction, universal coalgebras, and topos theory to model RL and dynamical systems as universal coalgebras, enabling asynchronous, distributed fixed-point computations via final coalgebras.


<details>
  <summary>Details</summary>
Motivation: To unify reinforcement learning with powerful abstract frameworks (category theory, coalgebras, topos theory) to study convergence, fixed points, and distributed/asynchronous computation in RL, and to extend RL models (MDPs, PSRs, LDSs) within a universal coalgebraic setting.

Method: Survey and formal synthesis: treat RL with categories and functors; relate asynchronous distributed minimization to Bertsekas–Tsitsiklis framework; model the space of RL algorithms as a functor category whose codomain is a topos; characterize RL dynamical models (MDPs, POMDPs, PSRs, LDSs) as coalgebras; generalize the fixed-point problem to finding the final coalgebra asynchronously in parallel.

Result: Establishes a cohesive mathematical framework that reinterprets RL constructs as coalgebraic/topos-theoretic objects; connects metric coinduction to asynchronous convergence; shows how the algorithmic landscape for RL can be organized as a functor category with a topos, and extends these ideas to a broad class of universal coalgebras.

Conclusion: URL provides a unifying, high-level framework that integrates RL with coinductive and coalgebraic methods, enabling asynchronous, distributed computation and offering new avenues for convergence analysis and model generalization across RL variants.

Abstract: In this paper, we introduce a categorial generalization of RL, termed
universal reinforcement learning (URL), building on powerful mathematical
abstractions from the study of coinduction on non-well-founded sets and
universal coalgebras, topos theory, and categorial models of asynchronous
parallel distributed computation. In the first half of the paper, we review the
basic RL framework, illustrate the use of categories and functors in RL,
showing how they lead to interesting insights. In particular, we also introduce
a standard model of asynchronous distributed minimization proposed by Bertsekas
and Tsitsiklis, and describe the relationship between metric coinduction and
their proof of the Asynchronous Convergence Theorem. The space of algorithms
for MDPs or PSRs can be modeled as a functor category, where the co-domain
category forms a topos, which admits all (co)limits, possesses a subobject
classifier, and has exponential objects. In the second half of the paper, we
move on to universal coalgebras. Dynamical system models, such as Markov
decision processes (MDPs), partially observed MDPs (POMDPs), a predictive state
representation (PSRs), and linear dynamical systems (LDSs) are all special
types of coalgebras. We describe a broad family of universal coalgebras,
extending the dynamic system models studied previously in RL. The core problem
in finding fixed points in RL to determine the exact or approximate (action)
value function is generalized in URL to determining the final coalgebra
asynchronously in a parallel distributed manner.

</details>


### [91] [Towards Reliable and Generalizable Differentially Private Machine Learning (Extended Version)](https://arxiv.org/abs/2508.15141)
*Wenxuan Bao,Vincent Bindschaedler*

Main category: cs.LG

TL;DR: An R+R study assessing 11 state-of-the-art differentially private ML techniques. Findings are mixed: some methods reproduce, others fail outside initial conditions. DP-specific randomness adds variability. The paper offers best-practice guidelines for scientifically valid DPML evaluation.


<details>
  <summary>Details</summary>
Motivation: DPML literature reports SoTA results but lacks consensus on which techniques are genuinely reliable due to heterogeneity in codebases, datasets, methodologies, and architectures. A systematic reproducibility and replicability assessment is needed.

Method: Conduct a reproducibility and replicability (R+R) experiment on 11 recent SoTA DPML techniques from the literature. Reimplement and evaluate them across broader conditions beyond their original experiments, accounting for extra randomness introduced by DP noise, and analyze challenges unique to DPML reproducibility.

Result: The results are mixed: some DPML techniques hold up under re-evaluation, while others falter outside their original experimental conditions. DP noise adds extra randomness, complicating direct comparisons and replication. The study surfaces variability due to codebase heterogeneity and methodological differences.

Conclusion: Provides insights and best-practice guidelines to obtain scientifically valid and reliable results in DPML. Highlights DP-specific reproducibility challenges and proposes recommendations for robust evaluation, benchmarking, and reporting to improve reliability in DPML research.

Abstract: There is a flurry of recent research papers proposing novel differentially
private machine learning (DPML) techniques. These papers claim to achieve new
state-of-the-art (SoTA) results and offer empirical results as validation.
However, there is no consensus on which techniques are most effective or if
they genuinely meet their stated claims. Complicating matters, heterogeneity in
codebases, datasets, methodologies, and model architectures make direct
comparisons of different approaches challenging.
  In this paper, we conduct a reproducibility and replicability (R+R)
experiment on 11 different SoTA DPML techniques from the recent research
literature. Results of our investigation are varied: while some methods stand
up to scrutiny, others falter when tested outside their initial experimental
conditions. We also discuss challenges unique to the reproducibility of DPML,
including additional randomness due to DP noise, and how to address them.
Finally, we derive insights and best practices to obtain scientifically valid
and reliable results.

</details>


### [92] [A Robust BERT-Based Deep Learning Model for Automated Cancer Type Extraction from Unstructured Pathology Reports](https://arxiv.org/abs/2508.15149)
*Minh Tran,Jeffery C. Chan,Min Li Huang,Maya Kansara,John P. Grady,Christine E. Napier,Subotheni Thavaneswaran,Mandy L. Ballinger,David M. Thomas,Frank P. Lin*

Main category: cs.LG

TL;DR: Fine-tuning a RoBERTa model on pathology reports improves automated extraction of cancer type information for precision oncology; achieves F1-Bertscore 0.98 and 80.61% exact match, outperforming a baseline and Mistral 7B, with potential for scalable integration into molecular tumor boards.


<details>
  <summary>Details</summary>
Motivation: Clinical information extraction from electronic medical records is labor-intensive and requires expert curation. There is a need for scalable, accurate NLP methods to support precision oncology research and decision-making.

Method: Fine-tune a domain-specific RoBERTa model on pathology reports to extract cancer types. Evaluate against a baseline model and the Mistral 7B large language model, reporting F1-Bertscore and exact-match metrics.

Result: The RoBERTa-based model achieved F1_Bertscore of 0.98 and exact-match accuracy of 80.61%, outperforming both the baseline model and the Mistral 7B LLM.

Conclusion: Domain-specific fine-tuning for oncology information extraction can yield highly accurate, scalable solutions that integrate into workflows such as molecular tumor boards, advancing efficient data extraction for precision medicine.

Abstract: The accurate extraction of clinical information from electronic medical
records is particularly critical to clinical research but require much trained
expertise and manual labor. In this study we developed a robust system for
automated extraction of the specific cancer types for the purpose of supporting
precision oncology research. from pathology reports using a fine-tuned RoBERTa
model. This model significantly outperformed the baseline model and a Large
Language Model, Mistral 7B, achieving F1_Bertscore 0.98 and overall exact match
of 80.61%. This fine-tuning approach demonstrates the potential for scalability
that can integrate seamlessly into the molecular tumour board process.
Fine-tuning domain-specific models for precision tasks in oncology, may pave
the way for more efficient and accurate clinical information extraction.

</details>


### [93] [SafeLLM: Unlearning Harmful Outputs from Large Language Models against Jailbreak Attacks](https://arxiv.org/abs/2508.15182)
*Xiangman Li,Xiaodong Wu,Qi Li,Jianbing Ni,Rongxing Lu*

Main category: cs.LG

TL;DR: SafeLLM is an unlearning-based defense against jailbreaks that detects unsafe outputs, traces harmful knowledge to FFN activations, and applies constrained optimization to forget harmful substructures, preserving general language abilities across Vicuna, LLaMA, and GPT-J. It outperforms standard defenses and remains robust to unseen attacks.


<details>
  <summary>Details</summary>
Motivation: Jailbreak prompts threaten LLM safety by bypassing alignment. There is a need to irreversibly forget harmful knowledge while preserving fluency and general capabilities, offering scalable safety improvements beyond conventional fine-tuning or PPO objectives.

Method: Three-stage pipeline: (1) dynamic unsafe output detection using a hybrid approach that combines external classifiers with model-internal evaluations; (2) token-level tracing of harmful content through FFN activations to locate responsible knowledge; (3) constrained optimization to suppress unsafe behavior and neutralize identified FFN substructures, achieving targeted forgetting without degrading general performance.

Result: Empirical evaluations on Vicuna, LLaMA, and GPT-J across multiple jailbreak benchmarks show substantial reductions in attack success rates while maintaining high general-purpose performance. SafeLLM offers stronger safety guarantees, more precise control over harmful behavior, and robustness to unseen attacks compared to standard defenses like supervised fine-tuning or direct preference optimization.

Conclusion: Unlearning-based approaches are a promising direction for scalable and effective LLM safety, with SafeLLM demonstrating targeted, irreversible forgetting of harmful knowledge while preserving linguistic fluency and general capabilities.

Abstract: Jailbreak attacks pose a serious threat to the safety of Large Language
Models (LLMs) by crafting adversarial prompts that bypass alignment mechanisms,
causing the models to produce harmful, restricted, or biased content. In this
paper, we propose SafeLLM, a novel unlearning-based defense framework that
unlearn the harmful knowledge from LLMs while preserving linguistic fluency and
general capabilities. SafeLLM employs a three-stage pipeline: (1) dynamic
unsafe output detection using a hybrid approach that integrates external
classifiers with model-internal evaluations; (2) token-level harmful content
tracing through feedforward network (FFN) activations to localize harmful
knowledge; and (3) constrained optimization to suppress unsafe behavior without
degrading overall model quality. SafeLLM achieves targeted and irreversible
forgetting by identifying and neutralizing FFN substructures responsible for
harmful generation pathways. Extensive experiments on prominent LLMs (Vicuna,
LLaMA, and GPT-J) across multiple jailbreak benchmarks show that SafeLLM
substantially reduces attack success rates while maintaining high
general-purpose performance. Compared to standard defense methods such as
supervised fine-tuning and direct preference optimization, SafeLLM offers
stronger safety guarantees, more precise control over harmful behavior, and
greater robustness to unseen attacks. Moreover, SafeLLM maintains the general
performance after the harmful knowledge unlearned. These results highlight
unlearning as a promising direction for scalable and effective LLM safety.

</details>


### [94] [Revisiting Pre-processing Group Fairness: A Modular Benchmarking Framework](https://arxiv.org/abs/2508.15193)
*Brodie Oldfield,Ziqi Xu,Sevvandi Kandanaarachchi*

Main category: cs.LG

TL;DR: FairPrep is a modular, extensible benchmarking framework for fairness-aware pre-processing on tabular data, built on AIF360, enabling standardized, reproducible evaluation of fairness interventions and utility across datasets and models.


<details>
  <summary>Details</summary>
Motivation: As ML systems are increasingly used in high-stakes decisions, bias in outcomes is a critical concern. Pre-processing methods, despite advantages like model-agnosticism and privacy benefits, have received less attention and lack standardized evaluation tools. There is a need for a unified benchmark to advance data-level fairness research.

Method: Develop FairPrep as an extensible framework on top of AIF360; modular design to integrate datasets, fairness interventions, and predictive models; batch-processing interface for efficient experimentation; automatic reporting of fairness and utility metrics; standardized pipelines to enable reproducible evaluations.

Result: Introduction of a new benchmarking framework that standardizes evaluation of fairness-aware pre-processing on tabular data, enabling efficient experimentation, reproducible reporting, and a practical foundation for advancing data-level fairness research.

Conclusion: FairPrep addresses a key gap in fairness benchmarking by providing a practical, reproducible platform for evaluating pre-processing fairness techniques and supporting data-level bias mitigation research.

Abstract: As machine learning systems become increasingly integrated into high-stakes
decision-making processes, ensuring fairness in algorithmic outcomes has become
a critical concern. Methods to mitigate bias typically fall into three
categories: pre-processing, in-processing, and post-processing. While
significant attention has been devoted to the latter two, pre-processing
methods, which operate at the data level and offer advantages such as
model-agnosticism and improved privacy compliance, have received comparatively
less focus and lack standardised evaluation tools. In this work, we introduce
FairPrep, an extensible and modular benchmarking framework designed to evaluate
fairness-aware pre-processing techniques on tabular datasets. Built on the
AIF360 platform, FairPrep allows seamless integration of datasets, fairness
interventions, and predictive models. It features a batch-processing interface
that enables efficient experimentation and automatic reporting of fairness and
utility metrics. By offering standardised pipelines and supporting reproducible
evaluations, FairPrep fills a critical gap in the fairness benchmarking
landscape and provides a practical foundation for advancing data-level fairness
research.

</details>


### [95] [Frequency-adaptive tensor neural networks for high-dimensional multi-scale problems](https://arxiv.org/abs/2508.15198)
*Jizu Huang,Rukang You,Tao Zhou*

Main category: cs.LG

TL;DR: A frequency-adaptive tensor neural network (TNN) framework is proposed to enhance high-frequency feature capture in high-dimensional multi-scale problems by combining Fourier analysis, random Fourier features, and DFT-based frequency extraction along tensor components, addressing the curse of dimensionality and improving robustness as validated by extensive experiments.


<details>
  <summary>Details</summary>
Motivation: TNNs, like conventional neural networks, suffer from the Frequency Principle, hindering accurate recovery of high-frequency solution features in high-dimensional, multi-scale problems. The work aims to enhance expressivity and training dynamics by leveraging Fourier techniques and the tensor structure.

Method: 1) Analyze TNN training dynamics via Fourier analysis. 2) Introduce random Fourier features to boost high-frequency representation. 3) Exploit tensor structure to extract frequency features by applying the Discrete Fourier Transform to one-dimensional component functions, mitigating dimensionality curse. 4) Propose a frequency-adaptive TNN algorithm built on these ideas. 5) Validate with extensive numerical experiments.

Result: The proposed framework improves the ability of TNNs to capture high-frequency components in high-dimensional problems, reduces the curse of dimensionality, and demonstrates robustness and effectiveness through extensive numerical experiments.

Conclusion: Frequency-adaptive TNNs significantly enhance expressivity and performance for complex multi-scale, high-dimensional problems, offering a practical and robust approach validated by numerical studies.

Abstract: Tensor neural networks (TNNs) have demonstrated their superiority in solving
high-dimensional problems. However, similar to conventional neural networks,
TNNs are also influenced by the Frequency Principle, which limits their ability
to accurately capture high-frequency features of the solution. In this work, we
analyze the training dynamics of TNNs by Fourier analysis and enhance their
expressivity for high-dimensional multi-scale problems by incorporating random
Fourier features. Leveraging the inherent tensor structure of TNNs, we further
propose a novel approach to extract frequency features of high-dimensional
functions by performing the Discrete Fourier Transform to one-dimensional
component functions. This strategy effectively mitigates the curse of
dimensionality. Building on this idea, we propose a frequency-adaptive TNNs
algorithm, which significantly improves the ability of TNNs in solving complex
multi-scale problems. Extensive numerical experiments are performed to validate
the effectiveness and robustness of the proposed frequency-adaptive TNNs
algorithm.

</details>


### [96] [SleepDIFFormer: Sleep Stage Classification via Multivariate Differential Transformer](https://arxiv.org/abs/2508.15215)
*Benjamin Wei Hao Chin,Yuin Torng Yew,Haocheng Wu,Lanxin Liang,Chow Khuen Chan,Norita Mohd Zain,Siti Balqis Samdin,Sim Kuan Goh*

Main category: cs.LG

TL;DR: Introduces SleepDIFFormer, a multivariate differential transformer for joint EEG-EOG sleep-stage classification with cross-domain alignment, achieving state-of-the-art results across five datasets and offering interpretable differential attention.


<details>
  <summary>Details</summary>
Motivation: Manual EEG/EOG sleep staging is time-consuming and error-prone, and existing ML/DL models struggle to generalize due to non-stationarity and variability across datasets and subjects. A joint EEG-EOG representation with domain-invariant features could improve generalization to unseen data.

Method: Proposes Multivariate Differential Transformer Architecture (MDTA) and SleepDIFFormer for time-series EEG and EOG inputs. Uses cross-domain alignment to learn a domain-invariant joint representation, mitigates spatial/temporal attention noise, and performs feature distribution alignment. Includes ablation studies and interpretation of differential attention weights. Code is released publicly.

Result: Evaluated on five sleep staging datasets, achieving state-of-the-art performance. Ablation analyses confirm the contribution of the differential attention mechanism and the importance of the learned joint EEG-EOG representations for accurate sleep stage classification.

Conclusion: SleepDIFFormer generalizes well to unseen datasets and advances automated sleep stage classification, with potential for improved sleep quality assessment. Public code enables replication and adaptation.

Abstract: Classification of sleep stages is essential for assessing sleep quality and
diagnosing sleep disorders such as insomnia. However, manual inspection of EEG
characteristics for each stage is time-consuming and prone to human error.
Although machine learning and deep learning methods have been actively
developed, they continue to face challenges from the non-stationarity and
variability of electroencephalography (EEG) and electrooculography (EOG)
signals, often leading to poor generalization on unseen datasets. This research
proposed a Sleep Stage Classification method by developing Multivariate
Differential Transformer (SleepDIFFormer) for joint EEG and EOG representation
learning. Specifically, SleepDIFFormer was developed to process EEG and EOG
signals using our Multivariate Differential Transformer Architecture (MDTA) for
time series, trained with cross-domain alignment. Our method mitigated spatial
and temporal attention noise while learning a domain-invariant joint EEG-EOG
representation through feature distribution alignment, thereby enabling
generalization to unseen target datasets. Empirically, we evaluated our method
on five different sleep staging datasets and compared it with existing
approaches, achieving state-of-the-art performance. We also conducted thorough
ablation analyses of SleepDIFFormer and interpreted the differential attention
weights, highlighting their relevance to characteristic sleep EEG patterns.
These findings have implications for advancing automated sleep stage
classification and its application to sleep quality assessment. Our source code
is publicly available at https://github.com/Ben1001409/SleepDIFFormer

</details>


### [97] [See Beyond a Single View: Multi-Attribution Learning Leads to Better Conversion Rate Prediction](https://arxiv.org/abs/2508.15217)
*Sishuo Chen,Zhangming Chan,Xiang-Rong Sheng,Lei Zhang,Sheng Chen,Chenghuan Hou,Han Zhu,Jian Xu,Bo Zheng*

Main category: cs.LG

TL;DR: Introduces Multi-Attribution Learning (MAL) for CVR prediction that jointly leverages multiple attribution signals to improve model performance and deployment compatibility.


<details>
  <summary>Details</summary>
Motivation: Single-attribution labels waste complementary information from other attribution perspectives; industrial systems rely on a specific attribution metric, but integrating multiple signals can better capture conversion patterns and improve label quality.

Method: MAL comprises two components: (1) Attribution Knowledge Aggregator (AKA) as a multi-task learner that fuses knowledge from diverse attribution labels; (2) Primary Target Predictor (PTP) that produces well-calibrated CVR aligned with the system attribution metric. A training strategy CAT uses the Cartesian product of all attribution label combinations to provide enriched supervision for the aggregator.

Result: Offline GAUC improved by +0.51% over single-attribution baselines; Online ROI increased by +2.6%.

Conclusion: MAL effectively leverages multi-attribution signals to enhance CVR prediction and aligns training with industrial attribution metrics, offering gains in both offline and online evaluations over traditional single-attribution models.

Abstract: Conversion rate (CVR) prediction is a core component of online advertising
systems, where the attribution mechanisms-rules for allocating conversion
credit across user touchpoints-fundamentally determine label generation and
model optimization. While many industrial platforms support diverse attribution
mechanisms (e.g., First-Click, Last-Click, Linear, and Data-Driven Multi-Touch
Attribution), conventional approaches restrict model training to labels from a
single production-critical attribution mechanism, discarding complementary
signals in alternative attribution perspectives.
  To address this limitation, we propose a novel Multi-Attribution Learning
(MAL) framework for CVR prediction that integrates signals from multiple
attribution perspectives to better capture the underlying patterns driving user
conversions. Specifically, MAL is a joint learning framework consisting of two
core components: the Attribution Knowledge Aggregator (AKA) and the Primary
Target Predictor (PTP). AKA is implemented as a multi-task learner that
integrates knowledge extracted from diverse attribution labels. PTP, in
contrast, focuses on the task of generating well-calibrated conversion
probabilities that align with the system-optimized attribution metric (e.g.,
CVR under the Last-Click attribution), ensuring direct compatibility with
industrial deployment requirements. Additionally, we propose CAT, a novel
training strategy that leverages the Cartesian product of all attribution label
combinations to generate enriched supervision signals. This design
substantially enhances the performance of the attribution knowledge aggregator.
Empirical evaluations demonstrate the superiority of MAL over
single-attribution learning baselines, achieving +0.51% GAUC improvement on
offline metrics. Online experiments demonstrate that MAL achieved a +2.6%
increase in ROI (Return on Investment).

</details>


### [98] [Locally Pareto-Optimal Interpretations for Black-Box Machine Learning Models](https://arxiv.org/abs/2508.15220)
*Aniruddha Joshi,Supratik Chakraborty,S Akshay,Shetal Shah,Hazem Torfah,Sanjit Seshia*

Main category: cs.LG

TL;DR: A scalable framework for Pareto-optimal interpretations with local guarantees, integrating multi-objective search (e.g., MO-MCTS) with SAT-based local optimality checks to approximate globally optimal trade-offs between accuracy and explainability.


<details>
  <summary>Details</summary>
Motivation: Explainable AI requires balancing accuracy and interpretability; existing multi-objective methods either lack formal Pareto guarantees or are computationally intractable for exploring the Pareto front. This work aims to provide scalable, provable local optimality guarantees in the space of interpretations.

Method: 1) Generate a set of Pareto-optimal interpretation candidates using a multi-objective learning/search technique (e.g., Multi-Objective Monte Carlo Tree Search). 2) For each candidate, verify local optimality by encoding it as a Boolean satisfiability problem and solving with a SAT solver. 3) Benchmark against prior Pareto-front methods to assess coverage and scalability.

Result: The approach yields interpretation candidates that closely match globally guaranteed methods while offering improved scalability; empirical results on benchmarks show competitive or superior Pareto-front coverage with reduced computational overhead compared to full guarantees.

Conclusion: Local-optimality guarantees, combined with scalable search, enable practical, near-globally Pareto-optimal interpretations for black-box models, balancing accuracy and explainability without prohibitive computational cost.

Abstract: Creating meaningful interpretations for black-box machine learning models
involves balancing two often conflicting objectives: accuracy and
explainability. Exploring the trade-off between these objectives is essential
for developing trustworthy interpretations. While many techniques for
multi-objective interpretation synthesis have been developed, they typically
lack formal guarantees on the Pareto-optimality of the results. Methods that do
provide such guarantees, on the other hand, often face severe scalability
limitations when exploring the Pareto-optimal space. To address this, we
develop a framework based on local optimality guarantees that enables more
scalable synthesis of interpretations. Specifically, we consider the problem of
synthesizing a set of Pareto-optimal interpretations with local optimality
guarantees, within the immediate neighborhood of each solution. Our approach
begins with a multi-objective learning or search technique, such as
Multi-Objective Monte Carlo Tree Search, to generate a best-effort set of
Pareto-optimal candidates with respect to accuracy and explainability. We then
verify local optimality for each candidate as a Boolean satisfiability problem,
which we solve using a SAT solver. We demonstrate the efficacy of our approach
on a set of benchmarks, comparing it against previous methods for exploring the
Pareto-optimal front of interpretations. In particular, we show that our
approach yields interpretations that closely match those synthesized by methods
offering global guarantees.

</details>


### [99] [Learning ECG Representations via Poly-Window Contrastive Learning](https://arxiv.org/abs/2508.15225)
*Yi Yuan,Joseph Van Duyn,Runze Yan,Zhuoyi Huang,Sulaiman Vesal,Sergey Plis,Xiao Hu,Gloria Hyunjung Kwak,Ran Xiao,Alex Fedorov*

Main category: cs.LG

TL;DR: Poly-window contrastive learning for ECG: leverages multiple temporal windows to learn temporally invariant representations, achieving modest accuracy gains but substantial efficiency improvements, suggesting scalability for foundation-model pretraining on biomedical time-series.


<details>
  <summary>Details</summary>
Motivation: ECG datasets often have limited labeled data; existing self-supervised methods mostly use two augmented views and do not exploit ECGs' rich temporal structure. A method that leverages multi-window temporal context could learn more robust, physiologically meaningful features with less labeled data.

Method: Construct multiple temporal windows from each ECG instance to form positive pairs. Use a contrastive objective that maximizes agreement across windows via statistics, inspired by slow feature analysis to encourage temporally invariant representations. Conduct extensive ablations and evaluate on PTB-XL, reporting multi-label superclass performance and training efficiency.

Result: On PTB-XL, poly-window contrastive learning achieves AUROC 0.891 and F1 0.680, surpassing the two-view baseline (0.888 AUROC, 0.679 F1). The method trains with up to four times fewer pre-training epochs (32 vs 128) and reduces total wall-clock pre-training time by 14.8%. Ablations show robustness across hyperparameters and identify critical design choices.

Conclusion: Poly-window contrastive learning is an efficient, scalable paradigm for self-supervised ECG analysis and offers a promising general framework for self-supervised representation learning in biomedical time-series data, enabling practical foundation-model pretraining.

Abstract: Electrocardiogram (ECG) analysis is foundational for cardiovascular disease
diagnosis, yet the performance of deep learning models is often constrained by
limited access to annotated data. Self-supervised contrastive learning has
emerged as a powerful approach for learning robust ECG representations from
unlabeled signals. However, most existing methods generate only pairwise
augmented views and fail to leverage the rich temporal structure of ECG
recordings. In this work, we present a poly-window contrastive learning
framework. We extract multiple temporal windows from each ECG instance to
construct positive pairs and maximize their agreement via statistics. Inspired
by the principle of slow feature analysis, our approach explicitly encourages
the model to learn temporally invariant and physiologically meaningful features
that persist across time. We validate our approach through extensive
experiments and ablation studies on the PTB-XL dataset. Our results demonstrate
that poly-window contrastive learning consistently outperforms conventional
two-view methods in multi-label superclass classification, achieving higher
AUROC (0.891 vs. 0.888) and F1 scores (0.680 vs. 0.679) while requiring up to
four times fewer pre-training epochs (32 vs. 128) and 14.8% in total wall clock
pre-training time reduction. Despite processing multiple windows per sample, we
achieve a significant reduction in the number of training epochs and total
computation time, making our method practical for training foundational models.
Through extensive ablations, we identify optimal design choices and demonstrate
robustness across various hyperparameters. These findings establish poly-window
contrastive learning as a highly efficient and scalable paradigm for automated
ECG analysis and provide a promising general framework for self-supervised
representation learning in biomedical time-series data.

</details>


### [100] [Deep Think with Confidence](https://arxiv.org/abs/2508.15260)
*Yichao Fu,Xuewei Wang,Yuandong Tian,Jiawei Zhao*

Main category: cs.LG

TL;DR: DeepThink with Confidence (DeepConf) uses model-internal confidence signals to filter out low-quality reasoning traces at test time, improving accuracy and reducing token usage without any training or hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: Self-consistency and test-time reasoning ensembles can boost accuracy but suffer from diminishing returns and high computational cost. A lightweight, deployable method is needed to improve reasoning efficiency and performance.

Method: DeepConf dynamically filters reasoning traces based on internal confidence signals during or after generation. It requires no additional model training or hyperparameter tuning and can be integrated into existing serving frameworks by discarding traces deemed low quality.

Result: On challenging benchmarks and models (e.g., AIME 2025, Qwen 3, GPT-OSS), DeepConf achieves up to 99.9% accuracy and reduces generated tokens by up to 84.7% compared to full parallel thinking.

Conclusion: DeepConf offers a simple yet effective mechanism to boost reasoning performance while cutting computational cost, enabling production-friendly deployment without extra training.

Abstract: Large Language Models (LLMs) have shown great potential in reasoning tasks
through test-time scaling methods like self-consistency with majority voting.
However, this approach often leads to diminishing returns in accuracy and high
computational overhead. To address these challenges, we introduce Deep Think
with Confidence (DeepConf), a simple yet powerful method that enhances both
reasoning efficiency and performance at test time. DeepConf leverages
model-internal confidence signals to dynamically filter out low-quality
reasoning traces during or after generation. It requires no additional model
training or hyperparameter tuning and can be seamlessly integrated into
existing serving frameworks. We evaluate DeepConf across a variety of reasoning
tasks and the latest open-source models, including Qwen 3 and GPT-OSS series.
Notably, on challenging benchmarks such as AIME 2025, DeepConf@512 achieves up
to 99.9% accuracy and reduces generated tokens by up to 84.7% compared to full
parallel thinking.

</details>


### [101] [Evaluating Knowledge Graph Complexity via Semantic, Spectral, and Structural Metrics for Link Prediction](https://arxiv.org/abs/2508.15291)
*Haji Gul,Abul Ghani Naim,Ajaz Ahmad Bhat*

Main category: cs.LG

TL;DR: CSG is not a robust complexity metric for KG link prediction; it lacks stability and correlates poorly with performance; semantic/structural metrics like Relation Entropy and Degree measures better reflect task difficulty.


<details>
  <summary>Details</summary>
Motivation: Assess applicability of CSG to knowledge graphs and link prediction; identify more stable, interpretable complexity measures.

Method: Evaluate CSG with transformer-based embeddings on multi-relational link prediction; benchmark against a suite of structural and semantic KG complexity metrics; analyze correlations with MRR, Hit@1, Hit@10.

Result: CSG is highly sensitive to parameterization and does not scale with class count; weak/inconsistent correlation with MRR/Hit@1. Relation Entropy, Maximum Relation Diversity, and Relation Type Cardinality show strong inverse correlations with MRR/Hit@1. Graph connectivity metrics correlate positively with Hit@10.

Conclusion: CSG's claimed stability and predictive power do not hold for KG link prediction; preferable to rely on stable, interpretable, task-aligned complexity metrics for KG datasets.

Abstract: Understanding dataset complexity is fundamental to evaluating and comparing
link prediction models on knowledge graphs (KGs). While the Cumulative Spectral
Gradient (CSG) metric, derived from probabilistic divergence between classes
within a spectral clustering framework, has been proposed as a classifier
agnostic complexity metric purportedly scaling with class cardinality and
correlating with downstream performance, it has not been evaluated in KG
settings so far. In this work, we critically examine CSG in the context of
multi relational link prediction, incorporating semantic representations via
transformer derived embeddings. Contrary to prior claims, we find that CSG is
highly sensitive to parametrisation and does not robustly scale with the number
of classes. Moreover, it exhibits weak or inconsistent correlation with
standard performance metrics such as Mean Reciprocal Rank (MRR) and Hit@1. To
deepen the analysis, we introduce and benchmark a set of structural and
semantic KG complexity metrics. Our findings reveal that global and local
relational ambiguity captured via Relation Entropy, node level Maximum Relation
Diversity, and Relation Type Cardinality exhibit strong inverse correlations
with MRR and Hit@1, suggesting these as more faithful indicators of task
difficulty. Conversely, graph connectivity measures such as Average Degree,
Degree Entropy, PageRank, and Eigenvector Centrality correlate positively with
Hit@10. Our results demonstrate that CSGs purported stability and
generalization predictive power fail to hold in link prediction settings and
underscore the need for more stable, interpretable, and task-aligned measures
of dataset complexity in knowledge driven learning.

</details>


### [102] [Saving for the future: Enhancing generalization via partial logic regularization](https://arxiv.org/abs/2508.15317)
*Zhaorui Tan,Yijie Hu,Xi Yang,Qiufeng Wang,Anh Nguyen,Kaizhu Huang*

Main category: cs.LG

TL;DR: Introduces PL-Reg, a partial-logic regularization term that reserves space for undefined logic, enabling better generalization to unknown classes; provides theoretical justification and demonstrates empirical gains across Generalized Category Discovery, Multi-Domain Generalized Category Discovery, and long-tailed class incremental learning.


<details>
  <summary>Details</summary>
Motivation: Generalization in visual classification with unknown classes remains a significant challenge. Existing paradigms: class discovery tends to favor known classes, while incremental learning suffers from catastrophic forgetting. L-Reg regularization requires fully defined logical formulas, which reduces flexibility for unknown classes. A more flexible reasoning framework is needed to accommodate unknowns.

Method: Propose PL-Reg, a partial-logic regularization term that allows models to reserve space for undefined logic formulas. Formally demonstrate that tasks involving unknown classes can be explained using partial logic. Prove that methods based on partial logic improve generalization. Validate PL-Reg through experiments on Generalized Category Discovery (GCD), Multi-Domain Generalized Category Discovery (MD-GCD), and long-tailed Class Incremental Learning tasks.

Result: The paper provides formal arguments showing that partial logic can explain unknown-class tasks and improve generalization. Empirically, PL-Reg yields consistent performance improvements across GCD, MD-GCD, and long-tailed class incremental learning tasks.

Conclusion: Partial logic is effective for tackling challenges related to unknown classes. PL-Reg offers flexible regularization that improves adaptability and generalization in visual classification under unknown-class scenarios.

Abstract: Generalization remains a significant challenge in visual classification
tasks, particularly in handling unknown classes in real-world applications.
Existing research focuses on the class discovery paradigm, which tends to favor
known classes, and the incremental learning paradigm, which suffers from
catastrophic forgetting. Recent approaches such as the L-Reg technique employ
logic-based regularization to enhance generalization but are bound by the
necessity of fully defined logical formulas, limiting flexibility for unknown
classes. This paper introduces PL-Reg, a novel partial-logic regularization
term that allows models to reserve space for undefined logic formulas,
improving adaptability to unknown classes. Specifically, we formally
demonstrate that tasks involving unknown classes can be effectively explained
using partial logic. We also prove that methods based on partial logic lead to
improved generalization. We validate PL-Reg through extensive experiments on
Generalized Category Discovery, Multi-Domain Generalized Category Discovery,
and long-tailed Class Incremental Learning tasks, demonstrating consistent
performance improvements. Our results highlight the effectiveness of partial
logic in tackling challenges related to unknown classes.

</details>


### [103] [ExBigBang: A Dynamic Approach for Explainable Persona Classification through Contextualized Hybrid Transformer Analysis](https://arxiv.org/abs/2508.15364)
*Saleh Afzoon,Amin Beheshti,Nabi Rezvani,Farshad Khunjush,Usman Naseem,John McMahon,Zahra Fathollahi,Mahdieh Labani,Wathiq Mansoor,Xuyun Zhang*

Main category: cs.LG

TL;DR: ExBigBang is a hybrid text-tabular transformer model for explainable persona classification that integrates metadata, domain knowledge, and user profiling in a cyclical profiling-classification process to capture evolving user contexts and improve interpretability.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in persona modeling: insufficient context, poor integration of textual and tabular data, lack of explainability, and static representations that fail to reflect evolving user behaviors in user-centric design.

Method: A hybrid text-tabular transformer architecture (ExBigBang) that fuses textual and tabular features, incorporating metadata, domain knowledge, and user profiling. It uses a cyclical process of user profiling and classification to update predictions and applies Explainable AI techniques to illuminate decisions; evaluated via ablation studies on a benchmark persona dataset.

Result: The model demonstrates robustness on a benchmark dataset. Ablation studies show that combining text and tabular data improves performance, and Explainable AI techniques provide insight into the model’s predictions.

Conclusion: ExBigBang offers an explainable, context-rich approach to persona classification that dynamically updates with user behavior, showing the value of integrating text and tabular data for personalized design decisions.

Abstract: In user-centric design, persona development plays a vital role in
understanding user behaviour, capturing needs, segmenting audiences, and
guiding design decisions. However, the growing complexity of user interactions
calls for a more contextualized approach to ensure designs align with real user
needs. While earlier studies have advanced persona classification by modelling
user behaviour, capturing contextual information, especially by integrating
textual and tabular data, remains a key challenge. These models also often lack
explainability, leaving their predictions difficult to interpret or justify. To
address these limitations, we present ExBigBang (Explainable BigBang), a hybrid
text-tabular approach that uses transformer-based architectures to model rich
contextual features for persona classification. ExBigBang incorporates
metadata, domain knowledge, and user profiling to embed deeper context into
predictions. Through a cyclical process of user profiling and classification,
our approach dynamically updates to reflect evolving user behaviours.
Experiments on a benchmark persona classification dataset demonstrate the
robustness of our model. An ablation study confirms the benefits of combining
text and tabular data, while Explainable AI techniques shed light on the
rationale behind the model's predictions.

</details>


### [104] [Enhancing Forecasting with a 2D Time Series Approach for Cohort-Based Data](https://arxiv.org/abs/2508.15369)
*Yonathan Guttel,Orit Moradov,Nachi Lieder,Asnat Greenstein-Messica*

Main category: cs.LG

TL;DR: Introduces a 2D time-series forecasting model that embeds cohort dynamics to boost performance in small data contexts; validated on real datasets with improved accuracy and adaptability over baselines, with potential strategic use in finance/marketing.


<details>
  <summary>Details</summary>
Motivation: Hard forecasting in small datasets and domains where cohort effects influence outcomes; need methods that leverage cross-sectional cohort structure to improve accuracy and decision support.

Method: Proposes a 2D representation that integrates cohort behavior over time; trains on multiple real-world datasets; compares against reference models to demonstrate performance gains; emphasis on small-data applicability; details not provided in abstract.

Result: Demonstrates superior accuracy and adaptability across several real-world datasets relative to baselines; shows practical usefulness for strategic decision-making.

Conclusion: The approach offers valuable insights for decision-making in industries with financial and marketing forecasting challenges; suggests broader applicability and potential impact, while prompting further exploration into cohort definition and model specifics.

Abstract: This paper introduces a novel two-dimensional (2D) time series forecasting
model that integrates cohort behavior over time, addressing challenges in small
data environments. We demonstrate its efficacy using multiple real-world
datasets, showcasing superior performance in accuracy and adaptability compared
to reference models. The approach offers valuable insights for strategic
decision-making across industries facing financial and marketing forecasting
challenges.

</details>


### [105] [Fairness for the People, by the People: Minority Collective Action](https://arxiv.org/abs/2508.15374)
*Omri Ben-Dov,Samira Samadi,Amartya Sanyal,Alexandru Ţifrea*

Main category: cs.LG

TL;DR: Coordinated minority relabeling within Algorithmic Collective Action can meaningfully reduce unfairness with modest accuracy loss, demonstrated via three practical, model-agnostic relabeling approximations tested on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: ML bias persists from training data and is costly for firms to mitigate; end-user data offers a low-friction fairness lever. The idea of Algorithmic Collective Action enables minority groups to improve fairness by relabeling their own data without changing the firm's training pipeline, potentially reducing unfairness with minimal utility loss.

Method: The paper proposes three practical, model-agnostic methods to approximate ideal relabeling. These approaches operate on user-contributed data and do not require changes to the firm's training process. They are evaluated via simulations/experiments on real-world datasets to assess their impact on fairness metrics and overall accuracy.

Result: A subgroup of the minority can substantially reduce unfairness with a small impact on overall prediction error, as validated by experiments on real-world datasets.

Conclusion: End-user coordinated relabeling (Algorithmic Collective Action) is a viable, model-agnostic avenue for fairness improvement that complements firm-side interventions. It offers a low-overhead path for minority groups to influence model fairness, though practical deployment must consider ethical, privacy, and governance considerations.

Abstract: Machine learning models often preserve biases present in training data,
leading to unfair treatment of certain minority groups. Despite an array of
existing firm-side bias mitigation techniques, they typically incur utility
costs and require organizational buy-in. Recognizing that many models rely on
user-contributed data, end-users can induce fairness through the framework of
Algorithmic Collective Action, where a coordinated minority group strategically
relabels its own data to enhance fairness, without altering the firm's training
process. We propose three practical, model-agnostic methods to approximate
ideal relabeling and validate them on real-world datasets. Our findings show
that a subgroup of the minority can substantially reduce unfairness with a
small impact on the overall prediction error.

</details>


### [106] [EvoFormer: Learning Dynamic Graph-Level Representations with Structural and Temporal Bias Correction](https://arxiv.org/abs/2508.15378)
*Haodi Zhong,Liuxin Zou,Di Wang,Bo Wang,Zhenxing Niu,Quan Wang*

Main category: cs.LG

TL;DR: EvoFormer proposes an evolution-aware Transformer to tackle Structural Visit Bias and Abrupt Evolution Blindness in dynamic graphs, using a Structure-Aware Transformer and an Evolution-Sensitive Temporal Module with a three-step process, achieving state-of-the-art results on five datasets for graph similarity, anomaly detection, and segmentation.


<details>
  <summary>Details</summary>
Motivation: Two critical issues in dynamic graph learning: (1) Structural Visit Bias from random-walk sampling that over-emphasizes high-degree nodes, leading to redundant/noisy structural representations; (2) Abrupt Evolution Blindness due to rigid temporal modeling that fails to detect sudden structural changes, causing inconsistent temporal embeddings.

Method: An evolution-aware Transformer framework (EvoFormer) with: (A) Structure-Aware Transformer Module that uses position encoding based on node structural roles to globally differentiate node structures; (B) Evolution-Sensitive Temporal Module employing a three-step process: (I) Random Walk Timestamp Classification to produce timestamp-aware graph embeddings; (II) Graph-Level Temporal Segmentation to partition the graph stream into structurally coherent segments; (III) Segment-Aware Temporal Self-Attention plus an Edge Evolution Prediction task to capture segment boundaries and evolution trends.

Result: Extensive evaluations on five benchmark datasets show state-of-the-art performance in graph similarity ranking, temporal anomaly detection, and temporal segmentation tasks, demonstrating EvoFormer’s effectiveness in correcting structural and temporal biases.

Conclusion: EvoFormer effectively mitigates both Structural Visit Bias and Abrupt Evolution Blindness, delivering robust dynamic graph-level representations and strong performance across tasks that require capturing structural evolution.

Abstract: Dynamic graph-level embedding aims to capture structural evolution in
networks, which is essential for modeling real-world scenarios. However,
existing methods face two critical yet under-explored issues: Structural Visit
Bias, where random walk sampling disproportionately emphasizes high-degree
nodes, leading to redundant and noisy structural representations; and Abrupt
Evolution Blindness, the failure to effectively detect sudden structural
changes due to rigid or overly simplistic temporal modeling strategies,
resulting in inconsistent temporal embeddings. To overcome these challenges, we
propose EvoFormer, an evolution-aware Transformer framework tailored for
dynamic graph-level representation learning. To mitigate Structural Visit Bias,
EvoFormer introduces a Structure-Aware Transformer Module that incorporates
positional encoding based on node structural roles, allowing the model to
globally differentiate and accurately represent node structures. To overcome
Abrupt Evolution Blindness, EvoFormer employs an Evolution-Sensitive Temporal
Module, which explicitly models temporal evolution through a sequential
three-step strategy: (I) Random Walk Timestamp Classification, generating
initial timestamp-aware graph-level embeddings; (II) Graph-Level Temporal
Segmentation, partitioning the graph stream into segments reflecting
structurally coherent periods; and (III) Segment-Aware Temporal Self-Attention
combined with an Edge Evolution Prediction task, enabling the model to
precisely capture segment boundaries and perceive structural evolution trends,
effectively adapting to rapid temporal shifts. Extensive evaluations on five
benchmark datasets confirm that EvoFormer achieves state-of-the-art performance
in graph similarity ranking, temporal anomaly detection, and temporal
segmentation tasks, validating its effectiveness in correcting structural and
temporal biases.

</details>


### [107] [CITE: A Comprehensive Benchmark for Heterogeneous Text-Attributed Graphs on Catalytic Materials](https://arxiv.org/abs/2508.15392)
*Chenghao Zhang,Qingqing Long,Ludi Wang,Wenjuan Cui,Jianjun Yu,Yi Du*

Main category: cs.LG

TL;DR: A new large-scale benchmark CITE for heterogeneous text-attributed graphs (TAGs) in catalytic materials.


<details>
  <summary>Details</summary>
Motivation: Addresses the gap of lacking large-scale benchmarks for heterogeneous text-attributed graphs and the need for standardized evaluation and fair comparison of representation learning methods.

Method: Construct CITE dataset with 438K nodes, 1.2M edges across four relation types; include textual features for nodes; establish standardized evaluation protocols; benchmark across four learning paradigms: homogeneous GMs, heterogeneous GMs, LLM-centric models, and LLM+Graph models; perform node classification and ablations.

Result: Introduces CITE as the first and largest heterogeneous text-attributed citation graph benchmark for catalytic materials; provides standardized evaluation procedures; conducts extensive benchmarking and ablation across heterogeneous/textual properties and four modeling paradigms.

Conclusion: CITE enables fair comparison and progress in heterogeneous TAGs by providing a large benchmark dataset, protocols, and baseline/ablation analyses for node classification across four modeling paradigms.

Abstract: Text-attributed graphs(TAGs) are pervasive in real-world systems,where each
node carries its own textual features. In many cases these graphs are
inherently heterogeneous, containing multiple node types and diverse edge
types. Despite the ubiquity of such heterogeneous TAGs, there remains a lack of
large-scale benchmark datasets. This shortage has become a critical bottleneck,
hindering the development and fair comparison of representation learning
methods on heterogeneous text-attributed graphs. In this paper, we introduce
CITE - Catalytic Information Textual Entities Graph, the first and largest
heterogeneous text-attributed citation graph benchmark for catalytic materials.
CITE comprises over 438K nodes and 1.2M edges, spanning four relation types. In
addition, we establish standardized evaluation procedures and conduct extensive
benchmarking on the node classification task, as well as ablation experiments
on the heterogeneous and textual properties of CITE. We compare four classes of
learning paradigms, including homogeneous graph models, heterogeneous graph
models, LLM(Large Language Model)-centric models, and LLM+Graph models. In a
nutshell, we provide (i) an overview of the CITE dataset, (ii) standardized
evaluation protocols, and (iii) baseline and ablation experiments across
diverse modeling paradigms.

</details>


### [108] [Federated Learning based on Self-Evolving Gaussian Clustering](https://arxiv.org/abs/2508.15393)
*Miha Ožbot,Igor Škrjanc*

Main category: cs.LG

TL;DR: An evolving fuzzy system integrated with federated learning that dynamically grows clusters, removing the need for a predefined cluster count and enabling decentralized clustering/classification; it outperforms several standard classifiers on UCI datasets, at the cost of higher computational overhead due to overlap calculations.


<details>
  <summary>Details</summary>
Motivation: To enable adaptive, privacy-preserving clustering and classification in federated settings without fixing the model structure in advance, leveraging edge-local training and parameter sharing.

Method: Proposes an evolving fuzzy system within a federated learning framework. The system adds clusters dynamically as data arrives, removing the need to predefine the number of clusters. Implemented in PyTorch and evaluated on clustering and classification tasks. Local training on clients with model parameter sharing to a central server. Overlap condition calculations contribute to computational load.

Result: The approach outperforms established classification methods on several well-known UCI datasets, demonstrating strong performance in decentralized data settings and validating the method's effectiveness for FL-based fuzzy systems.

Conclusion: Dynamic, evolving fuzzy systems within federated learning offer clear advantages for decentralized data processing and adaptive modeling, enabling improved classification and clustering without a priori structure, albeit with notable computational intensity due to overlap calculations.

Abstract: In this study, we present an Evolving Fuzzy System within the context of
Federated Learning, which adapts dynamically with the addition of new clusters
and therefore does not require the number of clusters to be selected apriori.
Unlike traditional methods, Federated Learning allows models to be trained
locally on clients' devices, sharing only the model parameters with a central
server instead of the data. Our method, implemented using PyTorch, was tested
on clustering and classification tasks. The results show that our approach
outperforms established classification methods on several well-known UCI
datasets. While computationally intensive due to overlap condition
calculations, the proposed method demonstrates significant advantages in
decentralized data processing.

</details>


### [109] [Hybrid Least Squares/Gradient Descent Methods for DeepONets](https://arxiv.org/abs/2508.15394)
*Jun Choi,Chang-Ock Lee,Minam Moon*

Main category: cs.LG

TL;DR: A hybrid LS/gradient-descent optimization is proposed to accelerate DeepONet training by exploiting the linearity of the last-layer branch parameters, solving a least-squares (LS) subproblem for them while updating the remaining parameters via gradient descent. The LS system is split into two smaller subproblems—one for the branch network and one for the trunk network—making the approach scalable. The framework generalizes to broader L2 losses with regularization on the last layer and extends to physics-informed (unsupervised) losses.


<details>
  <summary>Details</summary>
Motivation: Training DeepONet can be computationally expensive due to large LS systems when optimizing the last-layer branch parameters. There is a need for scalable optimization that preserves the benefits of LS optimization while integrating with gradient-based updates for other parameters.

Method: Decompose the large LS system into two smaller LS problems corresponding to the branch and trunk networks and solve them separately, while keeping the remaining hidden-layer parameters updated via gradient descent. The approach is generalized to L2 loss with regularization on the last layer parameters and supports physics-informed unsupervised losses.

Result: The method achieves computational efficiency by reducing the size of the LS problems and enables faster training of DeepONet. It remains applicable to a broader class of L2 loss formulations, including regularized and physics-informed (unsupervised) losses.

Conclusion: A scalable hybrid optimization framework is proposed for DeepONet that leverages LS for the last-layer branch parameters and gradient-based updates for other parameters, with a decomposition into branch/trunk subproblems and applicability to regularized and physics-informed L2 losses.

Abstract: We propose an efficient hybrid least squares/gradient descent method to
accelerate DeepONet training. Since the output of DeepONet can be viewed as
linear with respect to the last layer parameters of the branch network, these
parameters can be optimized using a least squares (LS) solve, and the remaining
hidden layer parameters are updated by means of gradient descent form. However,
building the LS system for all possible combinations of branch and trunk inputs
yields a prohibitively large linear problem that is infeasible to solve
directly. To address this issue, our method decomposes the large LS system into
two smaller, more manageable subproblems $\unicode{x2014}$ one for the branch
network and one for the trunk network $\unicode{x2014}$ and solves them
separately. This method is generalized to a broader type of $L^2$ loss with a
regularization term for the last layer parameters, including the case of
unsupervised learning with physics-informed loss.

</details>


### [110] [Bridging Generalization and Personalization in Wearable Human Activity Recognition via On-Device Few-Shot Learning](https://arxiv.org/abs/2508.15413)
*Pixi Kang,Julian Moosmann,Mengxi Liu,Bo Zhou,Michele Magno,Paul Lukowicz,Sizhen Bian*

Main category: cs.LG

TL;DR: Hybrid few-shot on-device personalization for HAR: generalizes across users and then rapidly adapts to individuals by updating only the classifier layer, implemented on GAP9, yielding notable accuracy gains across three HAR tasks.


<details>
  <summary>Details</summary>
Motivation: Generalization across users is hampered by user-induced concept drift, necessitating efficient, low-overhead personalization on embedded devices.

Method: A hybrid framework that first generalizes across users and then performs rapid on-device adaptation using few-shot learning by updating solely the classifier layer with user-specific data.

Result: Post-deployment adaptation yields accuracy improvements of 3.73%, 17.38%, and 3.70% on RecGym, QVAR-Gesture, and Ultrasound-Gesture respectively, confirming fast, lightweight personalization on embedded platforms.

Conclusion: Fast, lightweight, and effective personalization for HAR is feasible on embedded systems, enabling scalable, user-aware HAR in real-world settings.

Abstract: Human Activity Recognition (HAR) using wearable devices has advanced
significantly in recent years, yet its generalization remains limited when
models are deployed to new users. This degradation in performance is primarily
due to user-induced concept drift (UICD), highlighting the importance of
efficient personalization. In this paper, we present a hybrid framework that
first generalizes across users and then rapidly adapts to individual users
using few-shot learning directly on-device. By updating only the classifier
layer with user-specific data, our method achieves robust personalization with
minimal computational and memory overhead. We implement this framework on the
energy-efficient RISC-V-based GAP9 microcontroller and validate it across three
diverse HAR scenarios: RecGym, QVAR-Gesture, and Ultrasound-Gesture.
Post-deployment adaptation yields consistent accuracy improvements of 3.73\%,
17.38\%, and 3.70\% respectively. These results confirm that fast, lightweight,
and effective personalization is feasible on embedded platforms, paving the way
for scalable and user-aware HAR systems in the wild
\footnote{https://github.com/kangpx/onlineTiny2023}.

</details>


### [111] [Measures of Overlapping Multivariate Gaussian Clusters in Unsupervised Online Learning](https://arxiv.org/abs/2508.15444)
*Miha Ožbot,Igor Škrjanc*

Main category: cs.LG

TL;DR: A fast, overlap-focused dissimilarity measure for multivariate Gaussian clusters in online streaming, enabling efficient merging of overlapping clusters while avoiding merging orthogonal ones.


<details>
  <summary>Details</summary>
Motivation: Online learning from data streams requires adaptive clustering as concepts drift; in clustering, many clusters may overlap and should be merged. Traditional distribution dissimilarity measures fail to capture all cluster shapes and are computationally expensive, hindering real-time updating.

Method: Introduce a new dissimilarity measure designed specifically to detect overlap (not general dissimilarity) between multivariate Gaussian clusters; optimized for speed to suit streaming data; ensures overlapping clusters are detected and orthogonal clusters are not merged.

Result: The proposed measure is several times faster than existing methods and successfully detects overlapping clusters while avoiding undesirable merges of orthogonal clusters.

Conclusion: The overlap-detection measure improves online clustering for streaming data by providing a fast, shape-aware criterion that supports effective model adaptation with lower computational costs.

Abstract: In this paper, we propose a new measure for detecting overlap in multivariate
Gaussian clusters. The aim of online learning from data streams is to create
clustering, classification, or regression models that can adapt over time based
on the conceptual drift of streaming data. In the case of clustering, this can
result in a large number of clusters that may overlap and should be merged.
Commonly used distribution dissimilarity measures are not adequate for
determining overlapping clusters in the context of online learning from
streaming data due to their inability to account for all shapes of clusters and
their high computational demands. Our proposed dissimilarity measure is
specifically designed to detect overlap rather than dissimilarity and can be
computed faster compared to existing measures. Our method is several times
faster than compared methods and is capable of detecting overlapping clusters
while avoiding the merging of orthogonal clusters.

</details>


### [112] [Reliable Unlearning Harmful Information in LLMs with Metamorphosis Representation Projection](https://arxiv.org/abs/2508.15449)
*Chengcan Wu,Zeming Wei,Huanran Chen,Yinpeng Dong,Meng Sun*

Main category: cs.LG

TL;DR: A new unlearning approach (MRP) uses irreversible projections in hidden layers to erase harmful knowledge in LLMs, enabling continuous unlearning and resistance to relearning, with state-of-the-art unlearning and preserved performance; code released.


<details>
  <summary>Details</summary>
Motivation: Safety concerns: LLMs can store unsafe knowledge; existing retraining methods suppress signals but do not erase traces, enabling relearning; need genuine unlearning.

Method: Metamorphosis Representation Projection (MRP) implements irreversible projective transformations in selected network layers' hidden state space to remove harmful information while preserving useful knowledge, enabling continuous unlearning and defense against relearning.

Result: Experiments show effective continuous unlearning and defense against relearning, achieving state-of-the-art unlearning effectiveness with minimal loss in natural performance.

Conclusion: MRP provides a novel, effective approach to machine unlearning via irreversible projections; code available.

Abstract: While Large Language Models (LLMs) have demonstrated impressive performance
in various domains and tasks, concerns about their safety are becoming
increasingly severe. In particular, since models may store unsafe knowledge
internally, machine unlearning has emerged as a representative paradigm to
ensure model safety. Existing approaches employ various training techniques,
such as gradient ascent and negative preference optimization, in attempts to
eliminate the influence of undesired data on target models. However, these
methods merely suppress the activation of undesired data through parametric
training without completely eradicating its informational traces within the
model. This fundamental limitation makes it difficult to achieve effective
continuous unlearning, rendering these methods vulnerable to relearning
attacks. To overcome these challenges, we propose a Metamorphosis
Representation Projection (MRP) approach that pioneers the application of
irreversible projection properties to machine unlearning. By implementing
projective transformations in the hidden state space of specific network
layers, our method effectively eliminates harmful information while preserving
useful knowledge. Experimental results demonstrate that our approach enables
effective continuous unlearning and successfully defends against relearning
attacks, achieving state-of-the-art performance in unlearning effectiveness
while preserving natural performance. Our code is available in
https://github.com/ChengcanWu/MRP.

</details>


### [113] [A Solvable Molecular Switch Model for Stable Temporal Information Processing](https://arxiv.org/abs/2508.15451)
*H. I. Nurdin,C. A. Nijhuis*

Main category: cs.LG

TL;DR: An input-driven, exactly solvable one-state differential equation model shows convergent fading-memory dynamics, enabling stable, brain-inspired computation for neuromorphic architectures.


<details>
  <summary>Details</summary>
Motivation: Provide a mathematically tractable model with biologically-inspired switching behavior to enable stable learning on sequential data and to justify using such switches as computational units in deep and recurrent networks.

Method: A linear-in-state, nonlinear-in-input differential equation is analyzed; it is exactly solvable, and its dynamical properties (convergence and fading memory) are studied to assess its suitability for processing time-varying inputs.

Result: The model exhibits convergence and fading memory, supporting stable processing of time-varying inputs; it coexists with biologically-inspired switching behavior and mathematical properties that make it suitable for stable learning in layered and recurrent architectures; it could be fitted to emulate physical devices mimicking brain-like behavior.

Conclusion: The work provides theoretical support for using dynamic molecular switches as computational units in neuromorphic computing and suggests avenues for exact-solvable models that can emulate physical devices performing stable brain-inspired computation.

Abstract: This paper studies an input-driven one-state differential equation model
initially developed for an experimentally demonstrated dynamic molecular switch
that switches like synapses in the brain do. The linear-in-the-state and
nonlinear-in-the-input model is exactly solvable, and it is shown that it also
possesses mathematical properties of convergence and fading memory that enable
stable processing of time-varying inputs by nonlinear dynamical systems. Thus,
the model exhibits the co-existence of biologically-inspired behavior and
desirable mathematical properties for stable learning on sequential data. The
results give theoretical support for the use of the dynamic molecular switches
as computational units in deep cascaded/layered feedforward and recurrent
architectures as well as other more general structures for neuromorphic
computing. They could also inspire more general exactly solvable models that
can be fitted to emulate arbitrary physical devices which can mimic
brain-inspired behaviour and perform stable computation on input signals.

</details>


### [114] [Mini-Batch Robustness Verification of Deep Neural Networks](https://arxiv.org/abs/2508.15454)
*Saar Tzour-Shaday,Dana Drachsler Cohen*

Main category: cs.LG

TL;DR: BaVerLy introduces group-local robustness verification by batching epsilon-ball inputs that share similar network computations. It is sound and complete, and speeds up local robustness analysis by adaptively forming mini-batches; achieves significant speedups (avg 2.3x, up to 4.1x) reducing analysis time from 24h to 6h on MNIST/CIFAR-10 networks.


<details>
  <summary>Details</summary>
Motivation: Existing local robustness verifiers for epsilon-balls are either slow or lose precision, limiting scalability to large input sets. There is a need to leverage redundancies in network computations across nearby inputs to accelerate verification.

Method: BaVerLy adaptively constructs mini-batches of epsilon-balls with similar network computations and verifies them jointly. If a mini-batch passes, all epsilon-balls are robust. If not, one or more epsilon-balls are suspected and refined; results from the batch guide faster verification of the suspect and others. The verifier is designed to be sound and complete, and evaluated on fully connected and convolutional networks for MNIST and CIFAR-10.

Result: Empirical evaluation shows a 2.3x average speedup over one-by-one verification, with up to 4.1x improvement. Total analysis time reduced from 24 hours to 6 hours in the reported experiments.

Conclusion: Grouping epsilon-balls into adaptively sized, similarity-based mini-batches is an effective strategy to accelerate local robustness verification without sacrificing correctness, making scalable robustness analysis feasible for standard datasets and architectures.

Abstract: Neural network image classifiers are ubiquitous in many safety-critical
applications. However, they are susceptible to adversarial attacks. To
understand their robustness to attacks, many local robustness verifiers have
been proposed to analyze $\epsilon$-balls of inputs. Yet, existing verifiers
introduce a long analysis time or lose too much precision, making them less
effective for a large set of inputs. In this work, we propose a new approach to
local robustness: group local robustness verification. The key idea is to
leverage the similarity of the network computations of certain $\epsilon$-balls
to reduce the overall analysis time. We propose BaVerLy, a sound and complete
verifier that boosts the local robustness verification of a set of
$\epsilon$-balls by dynamically constructing and verifying mini-batches.
BaVerLy adaptively identifies successful mini-batch sizes, accordingly
constructs mini-batches of $\epsilon$-balls that have similar network
computations, and verifies them jointly. If a mini-batch is verified, all
$\epsilon$-balls are proven robust. Otherwise, one $\epsilon$-ball is suspected
as not being robust, guiding the refinement. In the latter case, BaVerLy
leverages the analysis results to expedite the analysis of that $\epsilon$-ball
as well as the other $\epsilon$-balls in the batch. We evaluate BaVerLy on
fully connected and convolutional networks for MNIST and CIFAR-10. Results show
that BaVerLy scales the common one by one verification by 2.3x on average and
up to 4.1x, in which case it reduces the total analysis time from 24 hours to 6
hours.

</details>


### [115] [Learning Protein-Ligand Binding in Hyperbolic Space](https://arxiv.org/abs/2508.15480)
*Jianhui Wang,Wenyu Zhu,Bowen Gao,Xin Hong,Ya-Qin Zhang,Wei-Ying Ma,Yanyan Lan*

Main category: cs.LG

TL;DR: HypSeek uses Lorentz-model hyperbolic embeddings to jointly model ligands, pockets, and sequences for protein-ligand tasks, yielding better early enrichment and affinity ranking, especially for activity cliffs.


<details>
  <summary>Details</summary>
Motivation: Euclidean embeddings struggle to capture hierarchical relationships and subtle affinity variations in protein-ligand interactions; hyperbolic space offers negative curvature and exponential geometry to encode hierarchy and fine-grained differences.

Method: A hyperbolic representation learning framework, HypSeek, embedding ligands, protein pockets, and sequences into Lorentz-model hyperbolic space, with a protein-guided three-tower architecture to unify virtual screening and affinity ranking; leverages hyperbolic geometry to produce affinity-sensitive embeddings.

Result: On DUD-E, early enrichment improves from 42.63 to 51.44 (+20.7%). On JACS, affinity ranking correlation improves from 0.5774 to 0.7239 (+25.4%).

Conclusion: Hyperbolic geometry provides a powerful inductive bias for protein-ligand modeling, effectively capturing hierarchical and subtle affinity variations and enabling unified tasks in a single framework.

Abstract: Protein-ligand binding prediction is central to virtual screening and
affinity ranking, two fundamental tasks in drug discovery. While recent
retrieval-based methods embed ligands and protein pockets into Euclidean space
for similarity-based search, the geometry of Euclidean embeddings often fails
to capture the hierarchical structure and fine-grained affinity variations
intrinsic to molecular interactions. In this work, we propose HypSeek, a
hyperbolic representation learning framework that embeds ligands, protein
pockets, and sequences into Lorentz-model hyperbolic space. By leveraging the
exponential geometry and negative curvature of hyperbolic space, HypSeek
enables expressive, affinity-sensitive embeddings that can effectively model
both global activity and subtle functional differences-particularly in
challenging cases such as activity cliffs, where structurally similar ligands
exhibit large affinity gaps. Our mode unifies virtual screening and affinity
ranking in a single framework, introducing a protein-guided three-tower
architecture to enhance representational structure. HypSeek improves early
enrichment in virtual screening on DUD-E from 42.63 to 51.44 (+20.7%) and
affinity ranking correlation on JACS from 0.5774 to 0.7239 (+25.4%),
demonstrating the benefits of hyperbolic geometry across both tasks and
highlighting its potential as a powerful inductive bias for protein-ligand
modeling.

</details>


### [116] [Let's Grow an Unbiased Community: Guiding the Fairness of Graphs via New Links](https://arxiv.org/abs/2508.15499)
*Jiahua Lu,Huaxiao Liu,Shuotong Bai,Junjie Xu,Renqiang Luo,Enyan Dai*

Main category: cs.LG

TL;DR: FairGuide proposes a fairness-guided graph augmentation framework that adds links to graphs to improve structural fairness for downstream tasks, guided by a differentiable pseudo-task of community detection and meta-gradients to select links.


<details>
  <summary>Details</summary>
Motivation: Graph data biases cause unfair outcomes; enabling guided augmentation to steer the structure toward fairness can improve downstream fairness and generalization.

Method: Introduce a differentiable community detection pseudo-task; use meta-gradients from the fairness objective to identify new links; provide theoretical analysis showing fairness generalization; apply meta-learning for link suggestions; experiment across graph-based fairness tasks.

Result: Empirical results demonstrate effectiveness and generalizability across multiple fairness tasks on graphs.

Conclusion: FairGuide effectively enhances structural fairness and fairness generalization by learning to add links via meta-gradient-guided optimization on a differentiable pseudo-task.

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across diverse
applications. However, due to the biases in the graph structures, graph neural
networks face significant challenges in fairness. Although the original user
graph structure is generally biased, it is promising to guide these existing
structures toward unbiased ones by introducing new links. The fairness guidance
via new links could foster unbiased communities, thereby enhancing fairness in
downstream applications. To address this issue, we propose a novel framework
named FairGuide. Specifically, to ensure fairness in downstream tasks trained
on fairness-guided graphs, we introduce a differentiable community detection
task as a pseudo downstream task. Our theoretical analysis further demonstrates
that optimizing fairness within this pseudo task effectively enhances
structural fairness, promoting fairness generalization across diverse
downstream applications. Moreover, FairGuide employs an effective strategy
which leverages meta-gradients derived from the fairness-guidance objective to
identify new links that significantly enhance structural fairness. Extensive
experimental results demonstrate the effectiveness and generalizability of our
proposed method across a variety of graph-based fairness tasks.

</details>


### [117] [Jointly Computation- and Communication-Efficient Distributed Learning](https://arxiv.org/abs/2508.15509)
*Xiaoxing Ren,Nicola Bastianello,Karl H. Johansson,Thomas Parisini*

Main category: cs.LG

TL;DR: An ADMM-based distributed learning algorithm for undirected networks that is computation- and communication-efficient through stochastic local updates, multi-epoch communication, and gradient compression, with a linear convergence guarantee for strongly convex problems and empirical validation on classification tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome efficiency bottlenecks in distributed learning by reducing both computation and communication costs without sacrificing convergence guarantees.

Method: An ADMM-based framework enabling stochastic gradients in local updates, allowing several training epochs between communications, and employing compressed communication. Theoretical analysis proves exact linear convergence in the strongly convex setting. Empirical comparisons against state-of-the-art baselines on a classification task are provided.

Result: Proves exact linear convergence for strongly convex objectives and demonstrates competitive or superior empirical performance against leading methods on classification benchmarks.

Conclusion: The method achieves joint computation- and communication-efficiency with provable convergence, making it suitable for distributed networks; potential extensions include non-convex objectives and other compression schemes.

Abstract: We address distributed learning problems over undirected networks.
Specifically, we focus on designing a novel ADMM-based algorithm that is
jointly computation- and communication-efficient. Our design guarantees
computational efficiency by allowing agents to use stochastic gradients during
local training. Moreover, communication efficiency is achieved as follows: i)
the agents perform multiple training epochs between communication rounds, and
ii) compressed transmissions are used. We prove exact linear convergence of the
algorithm in the strongly convex setting. We corroborate our theoretical
results by numerical comparisons with state of the art techniques on a
classification task.

</details>


### [118] [Stabilization of Perturbed Loss Function: Differential Privacy without Gradient Noise](https://arxiv.org/abs/2508.15523)
*Salman Habib,Remi Chou,Taejoon Kim*

Main category: cs.LG

TL;DR: SPOF is a multi-user LDP training method that perturbs coefficients of a stabilized Taylor expansion of the loss, avoiding gradient noise; it improves efficiency and stability and shows better privacy-utility than DP-SGD in WBAN scenarios.


<details>
  <summary>Details</summary>
Motivation: Address efficiency, stability, and privacy challenges in multi-user LDP training; gradient-based DP-SGD injects gradient noise which can hinder performance and stability, especially under noisy inputs; a method enabling simultaneous privacy guarantees with robustness is needed.

Method: Construct a stabilized Taylor expansion of the training loss into a polynomial and privatize by adding calibrated noise to its coefficients for each user. No gradient perturbation is performed. The approach is evaluated in a multi-user WBAN setting with heterogeneous data and channel noise, comparing against a multi-user DP-SGD baseline and analyzing reconstruction accuracy and training time.

Result: SPOF achieves up to 3.5% higher reconstruction accuracy and up to 57.2% shorter training time on average than multi-user DP-SGD in the tested WBAN scenario.

Conclusion: SPOF offers efficient, stable, and robust multi-user LDP training with simultaneous privacy guarantees and favorable privacy-utility trade-offs, outperforming gradient-noise methods under realistic noisy conditions.

Abstract: We propose SPOF (Stabilization of Perturbed Loss Function), a differentially
private training mechanism intended for multi-user local differential privacy
(LDP). SPOF perturbs a stabilized Taylor expanded polynomial approximation of a
model's training loss function, where each user's data is privatized by
calibrated noise added to the coefficients of the polynomial. Unlike
gradient-based mechanisms such as differentially private stochastic gradient
descent (DP-SGD), SPOF does not require injecting noise into the gradients of
the loss function, which improves both computational efficiency and stability.
This formulation naturally supports simultaneous privacy guarantees across all
users. Moreover, SPOF exhibits robustness to environmental noise during
training, maintaining stable performance even when user inputs are corrupted.
We compare SPOF with a multi-user extension of DP-SGD, evaluating both methods
in a wireless body area network (WBAN) scenario involving heterogeneous user
data and stochastic channel noise from body sensors. Our results show that SPOF
achieves, on average, up to 3.5% higher reconstruction accuracy and reduces
mean training time by up to 57.2% compared to DP-SGD, demonstrating superior
privacy-utility trade-offs in multi-user environments.

</details>


### [119] [AI-Powered Machine Learning Approaches for Fault Diagnosis in Industrial Pumps](https://arxiv.org/abs/2508.15550)
*Khaled M. A. Alghtus,Ayad Gannan,Khalid M. Alhajri,Ali L. A. Al Jubouri,Hassan A. I. Al-Janahi*

Main category: cs.LG

TL;DR: Hybrid fault detection for industrial pumps using five sensor streams, dual-threshold labeling, and synthetic faults; RF and XGBoost outperform SVM, enabling real-time predictive maintenance.


<details>
  <summary>Details</summary>
Motivation: Address early fault detection in large-scale, harsh marine pump environments with limited real-world failure instances by leveraging synthetic faults and multiple classifiers for robust, scalable monitoring.

Method: Collects vibration, temperature, flow, pressure, and current data; labels via fixed limits plus adaptive 95th percentile thresholds; injects domain-informed synthetic faults to simulate rare/ emerging faults; trains RF, XGBoost, and SVM to classify normal, early warning, and critical states; evaluates with confusion matrices and time-series visualizations.

Result: RF and XGBoost achieve high accuracy across all classes, including minority fault cases; SVM shows lower sensitivity to anomalies; the framework is scalable, interpretable, and suitable for real-time deployment; batch demonstrates potential adaptability to other machinery.

Conclusion: The proposed hybrid labeling plus ensemble learning approach provides robust early fault detection and a scalable predictive maintenance solution for complex systems, with applicability to other equipment sharing similar sensor architectures.

Abstract: This study presents a practical approach for early fault detection in
industrial pump systems using real-world sensor data from a large-scale
vertical centrifugal pump operating in a demanding marine environment. Five key
operational parameters were monitored: vibration, temperature, flow rate,
pressure, and electrical current. A dual-threshold labeling method was applied,
combining fixed engineering limits with adaptive thresholds calculated as the
95th percentile of historical sensor values. To address the rarity of
documented failures, synthetic fault signals were injected into the data using
domain-specific rules, simulating critical alerts within plausible operating
ranges. Three machine learning classifiers - Random Forest, Extreme Gradient
Boosting (XGBoost), and Support Vector Machine (SVM) - were trained to
distinguish between normal operation, early warnings, and critical alerts.
Results showed that Random Forest and XGBoost models achieved high accuracy
across all classes, including minority cases representing rare or emerging
faults, while the SVM model exhibited lower sensitivity to anomalies. Visual
analyses, including grouped confusion matrices and time-series plots, indicated
that the proposed hybrid method provides robust detection capabilities. The
framework is scalable, interpretable, and suitable for real-time industrial
deployment, supporting proactive maintenance decisions before failures occur.
Furthermore, it can be adapted to other machinery with similar sensor
architectures, highlighting its potential as a scalable solution for predictive
maintenance in complex systems.

</details>


### [120] [Conformalized Exceptional Model Mining: Telling Where Your Model Performs (Not) Well](https://arxiv.org/abs/2508.15569)
*Xin Du,Sikun Yang,Wouter Duivesteijn,Mykola Pechenizkiy*

Main category: cs.LG

TL;DR: Introduces Conformalized Exceptional Model Mining (CEMM): a framework that fuses Conformal Prediction with Exceptional Model Mining to locate cohesive data subgroups where model performance deviates, aided by a new mSMoPE model class and Relative Average Uncertainty Loss (RAUL); demonstrates interpretability and uncertainty benefits across tasks.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify nuanced model performance for responsible deployment in high-stakes domains (e.g., healthcare and finance) by identifying subgroups with exceptional or anomalous performance and providing rigorous uncertainty guarantees.

Method: Develop the Conformalized Exceptional Model Mining framework. Integrate Conformal Prediction with Exceptional Model Mining (EMM). Introduce mSMoPE (multiplex Soft Model Performance Evaluation) to quantify uncertainty with conformal coverage guarantees. Define Relative Average Uncertainty Loss (RAUL) as a quality metric. Apply the framework to multi-class classification and regression to discover cohesive subgroups with distinct performance patterns.

Result: Empirically identifies interpretable subgroups that reveal critical insights into model behavior. Subgroups exhibit regions of high confidence and high uncertainty, with conformal guarantees supporting reliability. The approach demonstrates effectiveness across diverse datasets and tasks (classification and regression), advancing explainable AI and uncertainty quantification.

Conclusion: Lays groundwork for enhanced interpretability and reliability in ML models, contributing to the state-of-the-art in explainable AI and uncertainty quantification and supporting safer deployment in high-stakes settings.

Abstract: Understanding the nuanced performance of machine learning models is essential
for responsible deployment, especially in high-stakes domains like healthcare
and finance. This paper introduces a novel framework, Conformalized Exceptional
Model Mining, which combines the rigor of Conformal Prediction with the
explanatory power of Exceptional Model Mining (EMM). The proposed framework
identifies cohesive subgroups within data where model performance deviates
exceptionally, highlighting regions of both high confidence and high
uncertainty. We develop a new model class, mSMoPE (multiplex Soft Model
Performance Evaluation), which quantifies uncertainty through conformal
prediction's rigorous coverage guarantees. By defining a new quality measure,
Relative Average Uncertainty Loss (RAUL), our framework isolates subgroups with
exceptional performance patterns in multi-class classification and regression
tasks. Experimental results across diverse datasets demonstrate the framework's
effectiveness in uncovering interpretable subgroups that provide critical
insights into model behavior. This work lays the groundwork for enhancing model
interpretability and reliability, advancing the state-of-the-art in explainable
AI and uncertainty quantification.

</details>


### [121] [Inductive Domain Transfer In Misspecified Simulation-Based Inference](https://arxiv.org/abs/2508.15593)
*Ortal Senouf,Antoine Wehenkel,Cédric Vincent-Cuaz,Emmanuel Abbé,Pascal Frossard*

Main category: cs.LG

TL;DR: A fully inductive, amortized SBI framework that unifies calibration and optimal-transport (OT) based distribution alignment via mini-batch OT with a closed-form coupling. It uses both paired calibration data and unpaired samples to train an end-to-end model, then employs a conditional normalizing flow to approximate the OT-induced posterior for efficient, test-time inference without simulations. It matches or exceeds RoPE and standard SBI/non-SBI methods across synthetic and real benchmarks, including medical biomarker estimation, with improved scalability in misspecified settings.


<details>
  <summary>Details</summary>
Motivation: SBI struggles with misspecification between simulations and real observations. RoPE addresses this via a two-stage, transductive domain-transfer but requires access to test-time data batches, hindering scalability and generalization. A fully inductive, amortized approach that integrates calibration and distribution alignment aims to enable scalable, test-time-efficient inference under misspecification.

Method: Integrate calibration and distributional alignment into a single end-to-end trainable model. Use mini-batch optimal transport with a closed-form coupling to align real and simulated observations that correspond to the same latent parameters, leveraging both paired calibration data and unpaired samples. Train a conditional normalizing flow to approximate the OT-induced posterior, enabling efficient inference without simulating at test time.

Result: Empirically, the method matches or surpasses RoPE and standard SBI and non-SBI estimators across a range of synthetic and real-world benchmarks, including complex medical biomarker estimation, with improved scalability and applicability in misspecified environments.

Conclusion: The proposed inductive, amortized SBI framework delivers scalable, test-time efficient posterior inference by integrating calibration and distribution alignment in an end-to-end model and using a conditional flow to represent the OT-induced posterior. It broadens applicability to misspecified settings and avoids the need for simulation during testing.

Abstract: Simulation-based inference (SBI) is a statistical inference approach for
estimating latent parameters of a physical system when the likelihood is
intractable but simulations are available. In practice, SBI is often hindered
by model misspecification--the mismatch between simulated and real-world
observations caused by inherent modeling simplifications. RoPE, a recent SBI
approach, addresses this challenge through a two-stage domain transfer process
that combines semi-supervised calibration with optimal transport (OT)-based
distribution alignment. However, RoPE operates in a fully transductive setting,
requiring access to a batch of test samples at inference time, which limits
scalability and generalization. We propose here a fully inductive and amortized
SBI framework that integrates calibration and distributional alignment into a
single, end-to-end trainable model. Our method leverages mini-batch OT with a
closed-form coupling to align real and simulated observations that correspond
to the same latent parameters, using both paired calibration data and unpaired
samples. A conditional normalizing flow is then trained to approximate the
OT-induced posterior, enabling efficient inference without simulation access at
test time. Across a range of synthetic and real-world benchmarks--including
complex medical biomarker estimation--our approach matches or surpasses the
performance of RoPE, as well as other standard SBI and non-SBI estimators,
while offering improved scalability and applicability in challenging,
misspecified environments.

</details>


### [122] [Continual Neural Topic Model](https://arxiv.org/abs/2508.15612)
*Charu Karakkaparambil James,Waleed Mustafa,Marius Kloft,Sophie Fellenz*

Main category: cs.LG

TL;DR: CoNTM is a Continual Neural Topic Model that learns topics over time without forgetting, by maintaining a continuously updated global prior; it outperforms Dynamic Topic Models in topic quality and perplexity and captures online topic changes, with more diverse topics and better temporal modeling.


<details>
  <summary>Details</summary>
Motivation: To address forgetting in continual topic modeling and fill the gap between Dynamic Topic Models (which use the whole corpus) and Online Topic Models (which lack long-term memory) by enabling online adaptation with memory of past topics.

Method: Introduce CoNTM, a continual learning approach that updates a global prior across time steps to retain previously learned topics while incorporating new data; operates as a neural topic model with online updates.

Result: CoNTM consistently outperforms DTMs in topic quality and predictive perplexity; it can track topic evolution online and discovers more diverse topics and better temporal changes than existing methods.

Conclusion: CoNTM provides a viable solution for continual topic modeling, achieving online adaptability without forgetting and improving both quality and diversity of topics while capturing temporal dynamics.

Abstract: In continual learning, our aim is to learn a new task without forgetting what
was learned previously. In topic models, this translates to learning new topic
models without forgetting previously learned topics. Previous work either
considered Dynamic Topic Models (DTMs), which learn the evolution of topics
based on the entire training corpus at once, or Online Topic Models, which are
updated continuously based on new data but do not have long-term memory. To
fill this gap, we propose the Continual Neural Topic Model (CoNTM), which
continuously learns topic models at subsequent time steps without forgetting
what was previously learned. This is achieved using a global prior distribution
that is continuously updated. In our experiments, CoNTM consistently
outperformed the dynamic topic model in terms of topic quality and predictive
perplexity while being able to capture topic changes online. The analysis
reveals that CoNTM can learn more diverse topics and better capture temporal
changes than existing methods.

</details>


### [123] [GRASPED: Graph Anomaly Detection using Autoencoder with Spectral Encoder and Decoder (Full Version)](https://arxiv.org/abs/2508.15633)
*Wei Herng Choong,Jixing Liu,Ching-Yu Kao,Philip Sperl*

Main category: cs.LG

TL;DR: Unsupervised graph anomaly detection using spectral encoder/decoder with graph wavelets and Wiener deconvolution, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Anomalies on graphs induce spectral shifts and labeled anomalies are scarce, so leverage multi-band spectral information for unsupervised detection.

Method: Graph Wavelet Convolution-based encoder combined with a Wiener Graph Deconvolution-based decoder, with structural and attribute decoders; learns to reconstruct node attributes to capture anomalies across multiple scales (bandpass-like behavior).

Result: GRASPED outperforms current state-of-the-art models on real-world graph anomaly detection datasets according to extensive experiments.

Conclusion: A spectral, multi-band approach using wavelet-based encoding and deconvolution effectively captures both global and local information for unsupervised node anomaly detection and reconstruction.

Abstract: Graph machine learning has been widely explored in various domains, such as
community detection, transaction analysis, and recommendation systems. In these
applications, anomaly detection plays an important role. Recently, studies have
shown that anomalies on graphs induce spectral shifts. Some supervised methods
have improved the utilization of such spectral domain information. However,
they remain limited by the scarcity of labeled data due to the nature of
anomalies. On the other hand, existing unsupervised learning approaches
predominantly rely on spatial information or only employ low-pass filters,
thereby losing the capacity for multi-band analysis. In this paper, we propose
Graph Autoencoder with Spectral Encoder and Spectral Decoder (GRASPED) for node
anomaly detection. Our unsupervised learning model features an encoder based on
Graph Wavelet Convolution, along with structural and attribute decoders. The
Graph Wavelet Convolution-based encoder, combined with a Wiener Graph
Deconvolution-based decoder, exhibits bandpass filter characteristics that
capture global and local graph information at multiple scales. This design
allows for a learning-based reconstruction of node attributes, effectively
capturing anomaly information. Extensive experiments on several real-world
graph anomaly detection datasets demonstrate that GRASPED outperforms current
state-of-the-art models.

</details>


### [124] [Classification errors distort findings in automated speech processing: examples and solutions from child-development research](https://arxiv.org/abs/2508.15637)
*Lucas Gautheron,Evan Kidd,Anton Malko,Marvin Lavechin,Alejandrina Cristia*

Main category: cs.LG

TL;DR: Automated audio classifiers used in child language studies introduce non-negligible errors that bias downstream inferences; a Bayesian calibration approach can recover more accurate effect sizes but is not foolproof.


<details>
  <summary>Details</summary>
Motivation: As wearable recorders yield long-form audio data, researchers increasingly rely on automated classifiers; understanding how misclassification affects correlations and regression estimates is critical.

Method: Evaluate LENA and the ACLEW Voice Type Classifier on long-form data; quantify impact of misclassification on key effects (e.g., siblings’ effect on adult input; association between child production and input); develop a Bayesian calibration framework to adjust effect-size estimates for classifier error; compare bias and uncertainty.

Result: Classification errors significantly distort estimates (e.g., underestimating the negative impact of siblings by 20–80%); Bayesian calibration can recover unbiased estimates but cannot guarantee perfect correction; errors in event-detection classifiers propagate to inferences.

Conclusion: Measurement error in automated classifiers materially affects scientific conclusions; calibration-based corrections are useful but not foolproof; the approach is broadly applicable to any classifier with non-zero error rates.

Abstract: With the advent of wearable recorders, scientists are increasingly turning to
automated methods of analysis of audio and video data in order to measure
children's experience, behavior, and outcomes, with a sizable literature
employing long-form audio-recordings to study language acquisition. While
numerous articles report on the accuracy and reliability of the most popular
automated classifiers, less has been written on the downstream effects of
classification errors on measurements and statistical inferences (e.g., the
estimate of correlations and effect sizes in regressions). This paper proposes
a Bayesian approach to study the effects of algorithmic errors on key
scientific questions, including the effect of siblings on children's language
experience and the association between children's production and their input.
In both the most commonly used \gls{lena}, and an open-source alternative (the
Voice Type Classifier from the ACLEW system), we find that classification
errors can significantly distort estimates. For instance, automated annotations
underestimated the negative effect of siblings on adult input by 20--80\%,
potentially placing it below statistical significance thresholds. We further
show that a Bayesian calibration approach for recovering unbiased estimates of
effect sizes can be effective and insightful, but does not provide a fool-proof
solution. Both the issue reported and our solution may apply to any classifier
involving event detection and classification with non-zero error rates.

</details>


### [125] [Correct-By-Construction: Certified Individual Fairness through Neural Network Training](https://arxiv.org/abs/2508.15642)
*Ruihan Zhang,Jun Sun*

Main category: cs.LG

TL;DR: Proposes a training framework that provides formal guarantees of individual fairness during training via provably fair initialization and a fairness-preserving training algorithm using randomized response; claims empirical fairness/accuracy and improved efficiency over verification-based training.


<details>
  <summary>Details</summary>
Motivation: Address lack of formal fairness guarantees in ML training; verification-based methods are insufficient for active fairness during training; need guarantees and efficiency.

Method: Two-part approach: (1) provably fair initialization; (2) fairness-preserving training algorithm leveraging randomized response mechanisms to protect sensitive attributes while preserving fairness; formal proofs that fairness is maintained during training.

Result: Empirical results show models are empirically fair and accurate; method is more efficient than certified training that relies on neural network verification during training.

Conclusion: A practical framework achieving provable individual fairness throughout training with better efficiency compared to verification-heavy approaches.

Abstract: Fairness in machine learning is more important than ever as ethical concerns
continue to grow. Individual fairness demands that individuals differing only
in sensitive attributes receive the same outcomes. However, commonly used
machine learning algorithms often fail to achieve such fairness. To improve
individual fairness, various training methods have been developed, such as
incorporating fairness constraints as optimisation objectives. While these
methods have demonstrated empirical effectiveness, they lack formal guarantees
of fairness. Existing approaches that aim to provide fairness guarantees
primarily rely on verification techniques, which can sometimes fail to produce
definitive results. Moreover, verification alone does not actively enhance
individual fairness during training. To address this limitation, we propose a
novel framework that formally guarantees individual fairness throughout
training. Our approach consists of two parts, i.e., (1) provably fair
initialisation that ensures the model starts in a fair state, and (2) a
fairness-preserving training algorithm that maintains fairness as the model
learns. A key element of our method is the use of randomised response
mechanisms, which protect sensitive attributes while maintaining fairness
guarantees. We formally prove that this mechanism sustains individual fairness
throughout the training process. Experimental evaluations confirm that our
approach is effective, i.e., producing models that are empirically fair and
accurate. Furthermore, our approach is much more efficient than the alternative
approach based on certified training (which requires neural network
verification during training).

</details>


### [126] [Amortized In-Context Mixed Effect Transformer Models: A Zero-Shot Approach for Pharmacokinetics](https://arxiv.org/abs/2508.15659)
*César Ali Ojeda Marin,Wilhelm Huisinga,Purity Kavwele,Niklas Hartung*

Main category: cs.LG

TL;DR: AICMET is a transformer-based latent-variable PK model using Ornstein-Uhlenbeck priors and amortized Bayesian inference to enable fast, calibrated predictions for new patients with sparse data.


<details>
  <summary>Details</summary>
Motivation: Accurate dose-response forecasting under sparse sampling; reduce model-development cycles; integrate mechanistic priors with population-level inference.

Method: Pre-train on hundreds of thousands of synthetic PK trajectories with Ornstein-Uhlenbeck priors; transformer-based latent-variable architecture; amortized in-context Bayesian inference; decoder conditions on context from previously profiled participants; zero-shot adaptation to new compounds.

Result: State-of-the-art predictive accuracy; well-calibrated inter-patient variability; outperforms nonlinear mixed-effects baselines and neural ODE variants.

Conclusion: Shows feasibility of population-aware, transformer-based PK models for personalized dosing, enabling faster, bespoke dosing pipelines and truly population-aware regimens.

Abstract: Accurate dose-response forecasting under sparse sampling is central to
precision pharmacotherapy. We present the Amortized In-Context Mixed-Effect
Transformer (AICMET) model, a transformer-based latent-variable framework that
unifies mechanistic compartmental priors with amortized in-context Bayesian
inference. AICMET is pre-trained on hundreds of thousands of synthetic
pharmacokinetic trajectories with Ornstein-Uhlenbeck priors over the parameters
of compartment models, endowing the model with strong inductive biases and
enabling zero-shot adaptation to new compounds. At inference time, the decoder
conditions on the collective context of previously profiled trial participants,
generating calibrated posterior predictions for newly enrolled patients after a
few early drug concentration measurements. This capability collapses
traditional model-development cycles from weeks to hours while preserving some
degree of expert modelling. Experiments across public datasets show that AICMET
attains state-of-the-art predictive accuracy and faithfully quantifies
inter-patient variability -- outperforming both nonlinear mixed-effects
baselines and recent neural ODE variants. Our results highlight the feasibility
of transformer-based, population-aware neural architectures as offering a new
alternative for bespoke pharmacokinetic modeling pipelines, charting a path
toward truly population-aware personalized dosing regimens.

</details>


### [127] [Tensorized Multi-Task Learning for Personalized Modeling of Heterogeneous Individuals with High-Dimensional Data](https://arxiv.org/abs/2508.15676)
*Elif Konyar,Mostafa Reisi Gahrooei,Kamran Paynabar*

Main category: cs.LG

TL;DR: A multi-task learning framework augmented with low-rank tensor decomposition to model heterogeneous subpopulations. It shares information across similar tasks while preserving subpopulation-specific variations, achieving improved accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous subpopulations introduce high variability, making it difficult to train accurate personalized models. A method that captures both shared structure and subpopulation-specific differences can improve performance and interpretability.

Method: Proposes a framework where task model parameters are decomposed into a low-rank tensor structure via low-rank decomposition within a multi-task learning setup. This captures commonalities across tasks while preserving subpopulation-specific variations, enabling efficient knowledge sharing. Evaluation includes simulations and case-study datasets.

Result: Shows superior predictive performance compared to benchmarks, particularly when subpopulation variability is high. Also reports enhanced interpretability by revealing underlying patterns that drive personalization.

Conclusion: The framework effectively models heterogeneity in subpopulations, enabling efficient learning of personalized models through shared representations while retaining unique subpopulation characteristics and offering interpretable insights.

Abstract: Effective modeling of heterogeneous subpopulations presents a significant
challenge due to variations in individual characteristics and behaviors. This
paper proposes a novel approach to address this issue through multi-task
learning (MTL) and low-rank tensor decomposition techniques. Our MTL approach
aims to enhance personalized modeling by leveraging shared structures among
similar tasks while accounting for distinct subpopulation-specific variations.
We introduce a framework where low-rank decomposition decomposes the collection
of task model parameters into a low-rank structure that captures commonalities
and variations across tasks and subpopulations. This approach allows for
efficient learning of personalized models by sharing knowledge between similar
tasks while preserving the unique characteristics of each subpopulation.
Experimental results in simulation and case study datasets demonstrate the
superior performance of the proposed method compared to several benchmarks,
particularly in scenarios with high variability among subpopulations. The
proposed framework not only improves prediction accuracy but also enhances
interpretability by revealing underlying patterns that contribute to the
personalization of models.

</details>


### [128] [An Efficient Open World Environment for Multi-Agent Social Learning](https://arxiv.org/abs/2508.15679)
*Eric Ye,Ren Tao,Natasha Jaques*

Main category: cs.LG

TL;DR: Proposes an open-ended, multi-agent environment to study social learning, cooperation, and competition among self-interested agents with expert roles.


<details>
  <summary>Details</summary>
Motivation: Real-world AI deployment is inherently multi-agent and human-influenced; current lack of open-ended environments hampers study of social intelligence.

Method: Create an environment with multiple self-interested agents pursuing independent goals; enable emergent collaboration via shared tools, cooperation against common enemies, and long-horizon planning; assess how social learning in the presence of experts affects performance.

Result: No empirical results reported; focuses on introducing the framework and outlining potential investigations into social learning, cooperation vs competition, and tool use.

Conclusion: The environment enables research into socially intelligent AI in open-ended multi-agent settings, exploring whether cooperation or competition and expert guidance yield performance gains.

Abstract: Many challenges remain before AI agents can be deployed in real-world
environments. However, one virtue of such environments is that they are
inherently multi-agent and contain human experts. Using advanced social
intelligence in such an environment can help an AI agent learn adaptive skills
and behaviors that a known expert exhibits. While social intelligence could
accelerate training, it is currently difficult to study due to the lack of
open-ended multi-agent environments. In this work, we present an environment in
which multiple self-interested agents can pursue complex and independent goals,
reflective of real world challenges. This environment will enable research into
the development of socially intelligent AI agents in open-ended multi-agent
settings, where agents may be implicitly incentivized to cooperate to defeat
common enemies, build and share tools, and achieve long horizon goals. In this
work, we investigate the impact on agent performance due to social learning in
the presence of experts and implicit cooperation such as emergent collaborative
tool use, and whether agents can benefit from either cooperation or competition
in this environment.

</details>


### [129] [Conditionally adaptive augmented Lagrangian method for physics-informed learning of forward and inverse problems using artificial neural networks](https://arxiv.org/abs/2508.15695)
*Qifeng Hu,Shamsulhaq Basir,Inanc Senocak*

Main category: cs.LG

TL;DR: PECANN-CAPU enhances physics-constrained neural PDE solvers by (1) multi-penalty augmented Lagrangian enforcement, (2) expectation-based constraint terms for memory-efficient mini-batching, (3) Fourier feature mappings for oscillatory/multi-scale problems, (4) time-windowing for stable long-time evolution, and (5) a conditional adaptive penalty update (CAPU). These advances improve robustness, efficiency, and applicability, achieving competitive accuracy across challenging PDEs.


<details>
  <summary>Details</summary>
Motivation: To advance PECANN for solving canonical PDEs by improving constraint enforcement, handling multi-scale and long-time dynamics, and reducing memory usage, thereby broadening robustness and applicability in scientific computing.

Method: 1) Generalize augmented Lagrangian method (ALM) to support multiple independent penalty parameters for heterogeneous constraints. 2) Reformulate pointwise constraints and Lagrange multipliers as expectations over constraint terms to reduce memory overhead and enable efficient mini-batch training. 3) Use Fourier feature mappings to address oscillatory/multi-scale PDEs, with a single mapping sufficing in this setting. 4) Introduce a time-windowing strategy for long-time evolution, treating the end state of one window as the initial condition for the next to ensure continuity without discrete time models. 5) Propose a conditionally adaptive penalty update (CAPU) that increases penalties preferentially for challenging constraints. 6) Empirically validate PECANN-CAPU on transonic rarefaction, reversible advection of a passive vortex, high-wavenumber Helmholtz and Poisson equations, and inverse spatially varying heat sources; compare against established methods and Kolmogorov–Arnold network variants.

Result: PECANN-CAPU delivers competitive accuracy across diverse PDE problems, with improved constraint enforcement efficiency, reduced memory usage, and robust performance on high-frequency and long-time problems. CAPU accelerates multiplier growth for difficult constraints, enhancing training effectiveness; a single Fourier feature mapping suffices for the tested problems, simplifying model design; time-windowing enables stable long-time integration without explicit time-stepping networks.

Conclusion: Together, these advances substantially boost PECANN's robustness, efficiency, and applicability to demanding scientific computing tasks, bringing it closer to parity with or superiority over established neural-PDE methods and related architectures.

Abstract: We present several advances to the physics and equality constrained
artificial neural networks (PECANN) framework that substantially improve its
capability to learn solutions of canonical partial differential equations
(PDEs). First, we generalize the augmented Lagrangian method (ALM) to support
multiple independent penalty parameters, enabling simultaneous enforcement of
heterogeneous constraints. Second, we reformulate pointwise constraint
enforcement and Lagrange multipliers as expectations over constraint terms,
reducing memory overhead and permitting efficient mini-batch training. Third,
to address PDEs with oscillatory, multi-scale features, we incorporate Fourier
feature mappings and show that a single mapping suffices where multiple
mappings or more costly architectures were required in related methods. Fourth,
we introduce a time-windowing strategy for long-time evolution in which the
terminal state of each window is enforced as an initial-condition constraint
for the next, ensuring continuity without discrete time models. Crucially, we
propose a conditionally adaptive penalty update (CAPU) strategy for ALM, which
preserves the principle that larger constraint violations incur stronger
penalties. CAPU accelerates the growth of Lagrange multipliers for selectively
challenging constraints, enhancing constraint enforcement during training. We
demonstrate the effectiveness of PECANN-CAPU on problems including the
transonic rarefaction problem, reversible advection of a passive by a vortex,
high-wavenumber Helmholtz and Poisson equations, and inverse identification of
spatially varying heat sources. Comparisons with established methods and recent
Kolmogorov-Arnold network approaches show that PECANN-CAPU achieves competitive
accuracy across all cases. Collectively, these advances improve PECANN's
robustness, efficiency, and applicability to demanding problems in scientific
computing.

</details>


### [130] [Investigation of D-Wave quantum annealing for training Restricted Boltzmann Machines and mitigating catastrophic forgetting](https://arxiv.org/abs/2508.15697)
*Abdelmoula El-Yazizi,Yaroslav Koshka*

Main category: cs.LG

TL;DR: The D-Wave quantum annealer (QA) and classical MCMC show only modest performance differences for RBM sampling, failing to improve RBM training; a hybrid approach also fails to yield gains. However, QA sampling may provide diverse low-probability patterns useful for other tasks like mitigating catastrophic forgetting (CF) in incremental learning, and could be beneficial with future hardware improvements.


<details>
  <summary>Details</summary>
Motivation: To determine whether QA sampling offers training benefits for RBMs, understand the source of any lack of improvement, and explore whether hybrid QA-classical sampling or QA-generated patterns could enhance other learning tasks (e.g., CF mitigation) despite limited RBM training gains.

Method: Empirically compare D-Wave QA sampling with classical MCMC for RBMs, experiment with a hybrid approach combining QA and classical samples, evaluate RBM training performance, and assess CF mitigation via generative replay using QA-generated patterns; discuss embedding challenges on newer D-Wave hardware.

Result: RBM training shows no improvements from QA sampling or the hybrid approach. The modest differences between QA and MCMC, especially in medium-to-low probability regions (less impactful for sample quality), do not translate into training benefits. Embedding RBMs onto newer D-Wave hardware may pose additional obstacles. However, QA-generated diverse samples in low-probability regions could benefit other tasks, notably CF mitigation through generative replay, with QA performing comparably to classical methods but offering faster generation of many distinct patterns.

Conclusion: QA-based sampling differences from MCMC are insufficient to boost RBM training under current hardware and methods, likely due to embedding and distribution-focus limitations. Nevertheless, QA can yield valuable, diverse samples in low-probability regions that may aid tasks like CF mitigation, and future hardware or methodological advances could unlock broader benefits.

Abstract: Modest statistical differences between the sampling performances of the
D-Wave quantum annealer (QA) and the classical Markov Chain Monte Carlo (MCMC),
when applied to Restricted Boltzmann Machines (RBMs), are explored to explain,
and possibly address, the absence of significant and consistent improvements in
RBM trainability when the D-Wave sampling was used in previous investigations.
A novel hybrid sampling approach, combining the classical and the QA
contributions, is investigated as a promising way to benefit from the modest
differences between the two sampling methods. No improvements in the RBM
training are achieved in this work, thereby suggesting that the differences
between the QA-based and MCMC sampling, mainly found in the medium-to-low
probability regions of the distribution, which are less important for the
quality of the sample, are insufficient to benefit the training. Difficulties
in achieving sufficiently high quality of embedding RBMs into the lattice of
the newer generation of D-Wave hardware could be further complicating the task.
On the other hand, the ability to generate samples of sufficient variety from
lower-probability parts of the distribution has a potential to benefit other
machine learning applications, such as the mitigation of catastrophic
forgetting (CF) during incremental learning. The feasibility of using
QA-generated patterns of desirable classes for CF mitigation by the generative
replay is demonstrated in this work for the first time. While the efficiency of
the CF mitigation using the D-Wave QA was comparable to that of the classical
mitigation, both the speed of generating a large number of distinct desirable
patterns and the potential for further improvement make this approach promising
for a variety of challenging machine learning applications.

</details>


### [131] [Communication Efficient LLM Pre-training with SparseLoCo](https://arxiv.org/abs/2508.15706)
*Amir Sarfi,Benjamin Thérien,Joel Lidin,Eugene Belilovsky*

Main category: cs.LG

TL;DR: SparseLoCo is a distributed training method for LLM pre-training that combines Top-k gradient sparsification, 2-bit quantization, error feedback, and sparse aggregation to dramatically reduce communication while delivering or improving model performance compared to full-precision DiLoCo.


<details>
  <summary>Details</summary>
Motivation: Communication bottlenecks in cross-datacenter and bandwidth-constrained LLM pre-training. Prior methods either reduce communication frequency with full gradients or use quantization alone, and cannot effectively combine sparsity with quantization or exploit error feedback and sparse aggregation to maintain performance.

Method: Introduce SparseLoCo: apply Top-k sparsification to gradients, 2-bit quantization, and an error-feedback loop, with sparse gradient aggregation. Local outer momentum is approximated via error feedback, and sparsity/aggregation are designed to improve performance while achieving extreme compression (as low as 1-3% density) with 2-bit quantization.

Result: Extensive experiments in communication-constrained LLM training show significant gains in both model performance and communication cost. SparseLoCo achieves extreme compression (1-3% density with 2-bit quantization) and outperforms full-precision DiLoCo.

Conclusion: SparseLoCo demonstrates that aggressive sparsification combined with coarse quantization, when paired with error feedback and sparse aggregation, can yield substantial communication savings and even improve performance in LLM pre-training over strong baselines like DiLoCo.

Abstract: Communication-efficient distributed training algorithms have received
considerable interest recently due to their benefits for training Large
Language Models (LLMs) in bandwidth-constrained settings, such as across data
centers and over the internet. Despite reducing communication frequency, these
methods still typically require communicating a full copy of the model's
gradients-resulting in a communication bottleneck even for cross-datacenter
links. Furthermore, they can slightly degrade performance compared to a naive
AdamW DDP baseline. While quantization and error feedback are often applied to
reduce the pseudo-gradient's size, in the context of LLM pre-training, existing
approaches have been unable to additionally leverage sparsification and have
obtained limited quantization. In this work, we introduce SparseLoCo, a
communication-efficient training algorithm for LLMs that effectively leverages
Top-k sparsification and quantization to reach extreme compression ratios of up
to 1-3% sparsity and 2-bit quantization while outperforming full-precision
DiLoCo. Our key observations are that outer momentum can be locally
approximated by an error feedback combined with aggressive sparsity and that
sparse aggregation can actually improve model performance. We empirically
demonstrate in a range of communication-constrained LLM training settings that
SparseLoCo provides significant benefits in both performance and communication
cost.

</details>


### [132] [Tutorial on the Probabilistic Unification of Estimation Theory, Machine Learning, and Generative AI](https://arxiv.org/abs/2508.15719)
*Mohammed Elmusrati*

Main category: cs.LG

TL;DR: A survey unifying estimation theory, Bayesian inference, and modern ML, showing that ML methods (MLE, MAP, Bayesian classifiers, deep learning) share probabilistic foundations for inferring hidden causes from noisy data, with practical guidance and illustrative scenarios.


<details>
  <summary>Details</summary>
Motivation: Address uncertainty in noisy data across domains and provide a cohesive theoretical framework that connects classical statistics with modern machine learning, aiding understanding and navigation of the evolving ML landscape.

Method: Conceptual synthesis that ties together maximum likelihood estimation, Bayesian inference, and attention mechanisms; explains how these techniques address uncertainty and overfitting; supports the synthesis with illustrative scenarios (system identification, image classification, language generation).

Result: Demonstrates that diverse AI methods are rooted in shared probabilistic principles; clarifies the connections among MLE, MAP, Bayesian classification, and deep learning; discusses practical challenges such as overfitting, data sparsity, and interpretability within this unified view.

Conclusion: Provides a theoretical synthesis and practical guide for students and researchers, arguing that different ML methods are facets of a single goal: inferring hidden causes from noisy/biased observations.

Abstract: Extracting meaning from uncertain, noisy data is a fundamental problem across
time series analysis, pattern recognition, and language modeling. This survey
presents a unified mathematical framework that connects classical estimation
theory, statistical inference, and modern machine learning, including deep
learning and large language models. By analyzing how techniques such as maximum
likelihood estimation, Bayesian inference, and attention mechanisms address
uncertainty, the paper illustrates that many AI methods are rooted in shared
probabilistic principles. Through illustrative scenarios including system
identification, image classification, and language generation, we show how
increasingly complex models build upon these foundations to tackle practical
challenges like overfitting, data sparsity, and interpretability. In other
words, the work demonstrates that maximum likelihood, MAP estimation, Bayesian
classification, and deep learning all represent different facets of a shared
goal: inferring hidden causes from noisy and/or biased observations. It serves
as both a theoretical synthesis and a practical guide for students and
researchers navigating the evolving landscape of machine learning.

</details>


### [133] [Probability Density from Latent Diffusion Models for Out-of-Distribution Detection](https://arxiv.org/abs/2508.15737)
*Joonas Järve,Karl Kaspar Haavel,Meelis Kull*

Main category: cs.LG

TL;DR: Likelihood can be the optimal OOD detector under a uniform-OOD assumption, but empirical failures have sparked doubt; this work tests whether using representation-space density estimation helps by training a Variational Diffusion Model on ResNet-18 features and comparing to OpenOOD baselines.


<details>
  <summary>Details</summary>
Motivation: Safety-critical task: robust OOD detection for ML systems; to reconcile theory that likelihood is optimal with practice where it fails; test representation-space density estimation vs pixel-space.

Method: Train a Variational Diffusion Model on the representation space of a pre-trained ResNet-18; evaluate the performance of a likelihood-based OOD detector in this space and compare it to state-of-the-art OpenOOD methods.

Result: Abstract does not report empirical results; it states an aim to assess performance; thus no concrete results are given.

Conclusion: Conclusion not stated in the abstract; the work aims to determine whether representation-space density estimation improves likelihood-based OOD detection relative to OpenOOD baselines.

Abstract: Despite rapid advances in AI, safety remains the main bottleneck to deploying
machine-learning systems. A critical safety component is out-of-distribution
detection: given an input, decide whether it comes from the same distribution
as the training data. In generative models, the most natural OOD score is the
data likelihood. Actually, under the assumption of uniformly distributed OOD
data, the likelihood is even the optimal OOD detector, as we show in this work.
However, earlier work reported that likelihood often fails in practice, raising
doubts about its usefulness. We explore whether, in practice, the
representation space also suffers from the inability to learn good density
estimation for OOD detection, or if it is merely a problem of the pixel space
typically used in generative models. To test this, we trained a Variational
Diffusion Model not on images, but on the representation space of a pre-trained
ResNet-18 to assess the performance of our likelihood-based detector in
comparison to state-of-the-art methods from the OpenOOD suite.

</details>


### [134] [Intern-S1: A Scientific Multimodal Foundation Model](https://arxiv.org/abs/2508.15763)
*Lei Bai,Zhongrui Cai,Maosong Cao,Weihan Cao,Chiyu Chen,Haojiong Chen,Kai Chen,Pengcheng Chen,Ying Chen,Yongkang Chen,Yu Cheng,Yu Cheng,Pei Chu,Tao Chu,Erfei Cui,Ganqu Cui,Long Cui,Ziyun Cui,Nianchen Deng,Ning Ding,Nanqin Dong,Peijie Dong,Shihan Dou,Sinan Du,Haodong Duan,Caihua Fan,Ben Gao,Changjiang Gao,Jianfei Gao,Songyang Gao,Yang Gao,Zhangwei Gao,Jiaye Ge,Qiming Ge,Lixin Gu,Yuzhe Gu,Aijia Guo,Qipeng Guo,Xu Guo,Conghui He,Junjun He,Yili Hong,Siyuan Hou,Caiyu Hu,Hanglei Hu,Jucheng Hu,Ming Hu,Zhouqi Hua,Haian Huang,Junhao Huang,Xu Huang,Zixian Huang,Zhe Jiang,Lingkai Kong,Linyang Li,Peiji Li,Pengze Li,Shuaibin Li,Tianbin Li,Wei Li,Yuqiang Li,Dahua Lin,Junyao Lin,Tianyi Lin,Zhishan Lin,Hongwei Liu,Jiangning Liu,Jiyao Liu,Junnan Liu,Kai Liu,Kaiwen Liu,Kuikun Liu,Shichun Liu,Shudong Liu,Wei Liu,Xinyao Liu,Yuhong Liu,Zhan Liu,Yinquan Lu,Haijun Lv,Hongxia Lv,Huijie Lv,Qidang Lv,Ying Lv,Chengqi Lyu,Chenglong Ma,Jianpeng Ma,Ren Ma,Runmin Ma,Runyuan Ma,Xinzhu Ma,Yichuan Ma,Zihan Ma,Sixuan Mi,Junzhi Ning,Wenchang Ning,Xinle Pang,Jiahui Peng,Runyu Peng,Yu Qiao,Jiantao Qiu,Xiaoye Qu,Yuan Qu,Yuchen Ren,Fukai Shang,Wenqi Shao,Junhao Shen,Shuaike Shen,Chunfeng Song,Demin Song,Diping Song,Chenlin Su,Weijie Su,Weigao Sun,Yu Sun,Qian Tan,Cheng Tang,Huanze Tang,Kexian Tang,Shixiang Tang,Jian Tong,Aoran Wang,Bin Wang,Dong Wang,Lintao Wang,Rui Wang,Weiyun Wang,Wenhai Wang,Yi Wang,Ziyi Wang,Ling-I Wu,Wen Wu,Yue Wu,Zijian Wu,Linchen Xiao,Shuhao Xing,Chao Xu,Huihui Xu,Jun Xu,Ruiliang Xu,Wanghan Xu,GanLin Yang,Yuming Yang,Haochen Ye,Jin Ye,Shenglong Ye,Jia Yu,Jiashuo Yu,Jing Yu,Fei Yuan,Bo Zhang,Chao Zhang,Chen Zhang,Hongjie Zhang,Jin Zhang,Qiaosheng Zhang,Qiuyinzhe Zhang,Songyang Zhang,Taolin Zhang,Wenlong Zhang,Wenwei Zhang,Yechen Zhang,Ziyang Zhang,Haiteng Zhao,Qian Zhao,Xiangyu Zhao,Xiangyu Zhao,Bowen Zhou,Dongzhan Zhou,Peiheng Zhou,Yuhao Zhou,Yunhua Zhou,Dongsheng Zhu,Lin Zhu,Yicheng Zou*

Main category: cs.LG

TL;DR: Intern-S1 is a 28B activated-parameters, 241B total-parameter multimodal MoE model designed as a specialized generalist with strong science reasoning and multimodal capabilities. Trained on 5T tokens (over 2.5T scientific tokens), it uses offline and online reinforcement learning (InternBootCamp) with Mixture-of-Rewards to tackle 1000+ tasks. It achieves competitive general reasoning among open-source models, markedly outperforms them in scientific domains, and surpasses closed-source SOTA in professional chemistry and materials tasks; the model is available at HuggingFace.


<details>
  <summary>Details</summary>
Motivation: Address the gap between open-source foundation models and closed-source models in high-value scientific domains, advancing toward AGI by building a specialized generalist capable of analyzing multiple science modalities.

Method: A multimodal Mixture-of-Experts (MoE) model with 28B activated parameters (241B total), continually pre-trained on 5T tokens including over 2.5T from scientific domains; after pretraining, offline RL followed by online RL in InternBootCamp, introducing Mixture-of-Rewards (MoR) to coordinate RL training across more than 1000 tasks.

Result: Intern-S1 achieves top-tier performance in online RL training; competitive general reasoning among open-source models; significantly outperforms open-source models in scientific domains and surpasses closed-source SOTA in professional tasks such as molecular synthesis planning, reaction condition prediction, and predicting thermodynamic stabilities for crystals.

Conclusion: Intern-S1 demonstrates the viability of specialized generalist models to narrow the gap between open- and closed-source models in scientific domains and moves toward AGI-like capabilities; the model is publicly available at HuggingFace.

Abstract: In recent years, a plethora of open-source foundation models have emerged,
achieving remarkable progress in some widely attended fields, with performance
being quite close to that of closed-source models. However, in high-value but
more challenging scientific professional fields, either the fields still rely
on expert models, or the progress of general foundation models lags
significantly compared to those in popular areas, far from sufficient for
transforming scientific research and leaving substantial gap between
open-source models and closed-source models in these scientific domains. To
mitigate this gap and explore a step further toward Artificial General
Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped
with general understanding and reasoning capabilities with expertise to analyze
multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE)
model with 28 billion activated parameters and 241 billion total parameters,
continually pre-trained on 5T tokens, including over 2.5T tokens from
scientific domains. In the post-training stage, Intern-S1 undergoes offline and
then online reinforcement learning (RL) in InternBootCamp, where we propose
Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks
simultaneously. Through integrated innovations in algorithms, data, and
training systems, Intern-S1 achieved top-tier performance in online RL
training.On comprehensive evaluation benchmarks, Intern-S1 demonstrates
competitive performance on general reasoning tasks among open-source models and
significantly outperforms open-source models in scientific domains, surpassing
closed-source state-of-the-art models in professional tasks, such as molecular
synthesis planning, reaction condition prediction, predicting thermodynamic
stabilities for crystals. Our models are available at
https://huggingface.co/internlm/Intern-S1.

</details>


### [135] [Distributed Detection of Adversarial Attacks in Multi-Agent Reinforcement Learning with Continuous Action Space](https://arxiv.org/abs/2508.15764)
*Kiarash Kazari,Ezzeldin Shereen,György Dán*

Main category: cs.LG

TL;DR: A decentralized detector using local observations to model normal agent behavior as parametric multivariate Gaussians, with a two-sided CUSUM on a normality score, detecting adversarial attacks in cooperative MARL with continuous actions; achieves high AUC-ROC (>0.95) on PettingZoo benchmarks.


<details>
  <summary>Details</summary>
Motivation: Detect and mitigate adversarial attacks in cooperative multi-agent reinforcement learning with continuous action spaces using only local observations and a statistical normality model to enable real-time anomaly detection.

Method: Train deep neural networks to approximate normal behavior as parametric multivariate Gaussian distributions from local observations. Define a normality score from predicted densities, derive its mean and variance analytically, and apply a two-sided CUSUM to detect deviations for real-time anomaly detection. Evaluate on PettingZoo benchmarks against state-of-the-art attack methods.

Result: The method effectively detects impactful adversarial attacks, outperforming the discrete counterpart with AUC-ROC scores over 0.95 across all evaluated environments.

Conclusion: A decentralized, observation-only detector based on probabilistic normality modeling and CUSUM is effective for real-time detection of adversarial behavior in cooperative MARL with continuous actions, offering strong performance gains over discrete baselines.

Abstract: We address the problem of detecting adversarial attacks against cooperative
multi-agent reinforcement learning with continuous action space. We propose a
decentralized detector that relies solely on the local observations of the
agents and makes use of a statistical characterization of the normal behavior
of observable agents. The proposed detector utilizes deep neural networks to
approximate the normal behavior of agents as parametric multivariate Gaussian
distributions. Based on the predicted density functions, we define a normality
score and provide a characterization of its mean and variance. This
characterization allows us to employ a two-sided CUSUM procedure for detecting
deviations of the normality score from its mean, serving as a detector of
anomalous behavior in real-time. We evaluate our scheme on various multi-agent
PettingZoo benchmarks against different state-of-the-art attack methods, and
our results demonstrate the effectiveness of our method in detecting impactful
adversarial attacks. Particularly, it outperforms the discrete counterpart by
achieving AUC-ROC scores of over 0.95 against the most impactful attacks in all
evaluated environments.

</details>


### [136] [Discovering Hidden Algebraic Structures via Transformers with Rank-Aware Beam GRPO](https://arxiv.org/abs/2508.15766)
*Jaeha Lee,Gio Huh,Ning Su,Tony Yue YU*

Main category: cs.LG

TL;DR: Transformers are applied to non-linear latent pattern discovery in multivariate polynomial decomposition, introducing a synthetic data pipeline, supervised training, and a Beam Grouped Relative Policy Optimization (BGRPO) reinforcement learning method to improve accuracy and reduce compute, with competitive polynomial simplification performance against Mathematica.


<details>
  <summary>Details</summary>
Motivation: Tackle an NP-hard algebraic task (multivariate polynomial decomposition) and push transformer-based models to perform precise, insight-driven symbolic reasoning and generalize to larger problem classes.

Method: 1) Build a synthetic data generation pipeline with controllable complexity for polynomial decomposition. 2) Train transformer models via supervised learning and evaluate scaling and generalization across multiple dimensions. 3) Propose Beam Grouped Relative Policy Optimization (BGRPO), a rank-aware reinforcement learning method to guide search in hard algebraic problems; fine-tuning with BGRPO reduces beam width and inference compute. 4) Demonstrate improved accuracy and competitive polynomial simplification performance, including outperforming Mathematica in several cases.

Result: BGRPO-based fine-tuning yields higher accuracy while halving the required beam width, leading to about 75% lower inference compute. The approach also shows competitive performance in polynomial simplification, outperforming Mathematica in diverse instances.

Conclusion: Transformer-based approaches can effectively address non-linear latent patterns in algebraic tasks like multivariate polynomial decomposition. The BGRPO method provides an effective, compute-efficient search strategy for hard symbolic problems and demonstrates strong potential for automated algebraic reasoning across science and engineering.

Abstract: Recent efforts have extended the capabilities of transformers in logical
reasoning and symbolic computations. In this work, we investigate their
capacity for non-linear latent pattern discovery in the context of functional
decomposition, focusing on the challenging algebraic task of multivariate
polynomial decomposition. This problem, with widespread applications in science
and engineering, is proved to be NP-hard, and demands both precision and
insight. Our contributions are threefold: First, we develop a synthetic data
generation pipeline providing fine-grained control over problem complexity.
Second, we train transformer models via supervised learning and evaluate them
across four key dimensions involving scaling behavior and generalizability.
Third, we propose Beam Grouped Relative Policy Optimization (BGRPO), a
rank-aware reinforcement learning method suitable for hard algebraic problems.
Finetuning with BGRPO improves accuracy while reducing beam width by up to
half, resulting in approximately 75% lower inference compute. Additionally, our
model demonstrates competitive performance in polynomial simplification,
outperforming Mathematica in various cases.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [137] [Constrained Flips in Plane Spanning Trees](https://arxiv.org/abs/2508.15520)
*Oswin Aichholzer,Joseph Dorfer,Birgit Vogtenhuber*

Main category: cs.CG

TL;DR: Two constrained flip types on plane spanning trees of convex point sets are analyzed: compatible flips (non-crossing edges) and rotations (edges share a vertex). The study tightens diameter bounds for both flip graphs, proves a happy-edge property for compatible flips enabling an FPT algorithm to compute shortest sequences, and shows this property fails for rotations.


<details>
  <summary>Details</summary>
Motivation: To understand the reconfiguration landscape of plane spanning trees under constrained flips, close gaps with unrestricted flip results, and derive algorithmic consequences for transforming trees.

Method: Combinatorial analysis of flip sequences on convex point sets; definition and exploitation of the happy-edge property for compatible flips; derivation of improved diameter bounds; development of a fixed-parameter tractable algorithm for the shortest compatible flip sequence; contrast with rotations by showing the happy-edge property does not hold; comparison to the unrestricted-flips bound from SODA 2025.

Result: Compatible flips: diameter bound improved to 5n/3 - O(1); shortest compatible flip sequence never removes a correctly placed edge (happy-edge property) enabling an FPT algorithm to compute it. Rotations: diameter bound improved to 7n/4 - O(1); the happy-edge property does not hold.

Conclusion: The paper delivers tighter diameter bounds for both flip types on convex-point sets, establishes a useful happy-edge property and an FPT method for compatible flips, and reveals a fundamental difference between compatible flips and rotations by showing the property fails for rotations.

Abstract: A flip in a plane spanning tree $T$ is the operation of removing one edge
from $T$ and adding another edge such that the resulting structure is again a
plane spanning tree. For trees on a set of points in convex position we study
two classic types of constrained flips: (1)~Compatible flips are flips in which
the removed and inserted edge do not cross each other. We relevantly improve
the previous upper bound of $2n-O(\sqrt{n})$ on the diameter of the compatible
flip graph to~$\frac{5n}{3}-O(1)$, by this matching the upper bound for
unrestricted flips by Bjerkevik, Kleist, Ueckerdt, and Vogtenhuber [SODA~2025]
up to an additive constant of $1$. We further show that no shortest compatible
flip sequence removes an edge that is already in its target position. Using
this so-called happy edge property, we derive a fixed-parameter tractable
algorithm to compute the shortest compatible flip sequence between two given
trees. (2)~Rotations are flips in which the removed and inserted edge share a
common vertex. Besides showing that the happy edge property does not hold for
rotations, we improve the previous upper bound of $2n-O(1)$ for the diameter of
the rotation graph to~$\frac{7n}{4}-O(1)$.

</details>


### [138] [Same Quality Metrics, Different Graph Drawings](https://arxiv.org/abs/2508.15557)
*Simon van Wageningen,Tamara Mchedlidze,Alexandru C. Telea*

Main category: cs.CG

TL;DR: Existing quality metrics for graph drawings can be manipulated: a drawing can be transformed into arbitrary shapes while keeping metrics almost unchanged. This reveals that current metrics may not accurately reflect drawing quality and that relying on single or few metrics is fragile; more advanced metrics are needed.


<details>
  <summary>Details</summary>
Motivation: To address the gap between commonly used quality metrics for graph drawings and the actual perceived quality, showing that current metrics can be gamed and may fail to capture true goodness.

Method: The authors construct or demonstrate a process that transforms existing graph drawings into arbitrary target shapes while keeping one or more quality metrics nearly identical, thereby exposing the fragility of these metrics.

Result: Quality metrics can be preserved under substantial shape changes, including drastic deformations, which implies they may misjudge poor-quality drawings as good.

Conclusion: There is a need for more robust, perhaps multi-faceted quality metrics for graph drawings, as relying on a handful of standard metrics is insufficient to guarantee drawing quality.

Abstract: Graph drawings are commonly used to visualize relational data. User
understanding and performance are linked to the quality of such drawings, which
is measured by quality metrics. The tacit knowledge in the graph drawing
community about these quality metrics is that they are not always able to
accurately capture the quality of graph drawings. In particular, such metrics
may rate drawings with very poor quality as very good. In this work we make
this tacit knowledge explicit by showing that we can modify existing graph
drawings into arbitrary target shapes while keeping one or more quality metrics
almost identical. This supports the claim that more advanced quality metrics
are needed to capture the 'goodness' of a graph drawing and that we cannot
confidently rely on the value of a single (or several) certain quality metrics.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [139] [A Fully Spectral Neuro-Symbolic Reasoning Architecture with Graph Signal Processing as the Computational Backbone](https://arxiv.org/abs/2508.14923)
*Andrew Kiruluta*

Main category: cs.AI

TL;DR: A fully spectral neuro-symbolic reasoning framework using Graph Signal Processing as the backbone for end-to-end reasoning, achieving improvements on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: To unify logical reasoning with neural inference under a mathematically grounded spectral framework, addressing interpretability and efficiency in neuro-symbolic reasoning.

Method: Encode entities and relations as graph signals and perform reasoning entirely in the spectral domain using graph Fourier transforms, band-selective attention, and spectral rule grounding; learn spectral filters to control multi-scale information propagation; map spectral representations to symbolic predicates for rule-based inference.

Result: Outperforms state-of-the-art neuro-symbolic models on reasoning benchmarks (ProofWriter, EntailmentBank, bAbI, CLUTRR, ARC-Challenge) in logical consistency, interpretability, and computational efficiency.

Conclusion: Graph Signal Processing provides a principled, scalable substrate for robust and interpretable reasoning systems; the spectral approach yields effective and efficient neuro-symbolic reasoning.

Abstract: We propose a fully spectral, neuro\-symbolic reasoning architecture that
leverages Graph Signal Processing (GSP) as the primary computational backbone
for integrating symbolic logic and neural inference. Unlike conventional
reasoning models that treat spectral graph methods as peripheral components,
our approach formulates the entire reasoning pipeline in the graph spectral
domain. Logical entities and relationships are encoded as graph signals,
processed via learnable spectral filters that control multi-scale information
propagation, and mapped into symbolic predicates for rule-based inference. We
present a complete mathematical framework for spectral reasoning, including
graph Fourier transforms, band-selective attention, and spectral rule
grounding. Experiments on benchmark reasoning datasets (ProofWriter,
EntailmentBank, bAbI, CLUTRR, and ARC-Challenge) demonstrate improvements in
logical consistency, interpretability, and computational efficiency over
state\-of\-the\-art neuro\-symbolic models. Our results suggest that GSP
provides a mathematically grounded and computationally efficient substrate for
robust and interpretable reasoning systems.

</details>


### [140] [Goals and the Structure of Experience](https://arxiv.org/abs/2508.15013)
*Nadav Amir,Stas Tiomkin,Angela Langdon*

Main category: cs.AI

TL;DR: A theoretical framework where descriptive (state) and prescriptive (desirable) world-model aspects co-emerge from goal-directed experiences, via telic states—classes of goal-equivalent experience distributions. Grounded in Buddhist epistemology, this framework uses statistical divergence between policies and desirable experience features to explain goal-directed learning, aiming to unify behavioral, phenomenological, and neural dimensions across substrates.


<details>
  <summary>Details</summary>
Motivation: Canonical accounts (e.g., reinforcement learning) separate state representations and rewards; the authors propose that these aspects co-emerge from agent-environment interactions guided by goals, offering a more parsimonious foundation for purposeful behavior and aligning computational models with phenomenology and neural data.

Method: Develop a computational framework that defines telic states as classes of goal-equivalent experience distributions. Formalize goal-directed learning via the statistical divergence between behavioral policies and desirable experience features. Review empirical and theoretical literature to support the notion and discuss implications for across-substrate unification.

Result: A theoretical framework that provides a parsimonious account of goal-directed learning through telic (goal-directed) states, positing that descriptive and prescriptive world-model aspects co-emerge from experience distributions. It synthesizes behavioral, phenomenological, and neural dimensions and draws support from existing literature, without reporting new empirical results.

Conclusion: Telic-states offer a unifying, goal-driven account of purposeful behavior that transcends traditional RL separations, with potential to integrate behavioral, phenomenological, and neural perspectives across substrates; future work should operationalize and test predictions arising from this framework.

Abstract: Purposeful behavior is a hallmark of natural and artificial intelligence. Its
acquisition is often believed to rely on world models, comprising both
descriptive (what is) and prescriptive (what is desirable) aspects that
identify and evaluate state of affairs in the world, respectively. Canonical
computational accounts of purposeful behavior, such as reinforcement learning,
posit distinct components of a world model comprising a state representation
(descriptive aspect) and a reward function (prescriptive aspect). However, an
alternative possibility, which has not yet been computationally formulated, is
that these two aspects instead co-emerge interdependently from an agent's goal.
Here, we describe a computational framework of goal-directed state
representation in cognitive agents, in which the descriptive and prescriptive
aspects of a world model co-emerge from agent-environment interaction
sequences, or experiences. Drawing on Buddhist epistemology, we introduce a
construct of goal-directed, or telic, states, defined as classes of
goal-equivalent experience distributions. Telic states provide a parsimonious
account of goal-directed learning in terms of the statistical divergence
between behavioral policies and desirable experience features. We review
empirical and theoretical literature supporting this novel perspective and
discuss its potential to provide a unified account of behavioral,
phenomenological and neural dimensions of purposeful behaviors across diverse
substrates.

</details>


### [141] [Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations in Tourism](https://arxiv.org/abs/2508.15030)
*Ashmi Banerjee,Fitri Nur Aisyah,Adithi Satish,Wolfgang Wörndl,Yashar Deldjoo*

Main category: cs.AI

TL;DR: Collab-REC is a multi-agent LLM framework for tourism recommendations that uses three specialized agents (Personalization, Popularity, Sustainability) and a non-LLM moderator to merge and refine their outputs, aiming to reduce popularity bias and increase diversity while maintaining relevance.


<details>
  <summary>Details</summary>
Motivation: Address popularity bias and over-tourism in recommender systems, particularly for tourism, by integrating diverse stakeholder perspectives and ensuring user constraints are respected.

Method: Three LLM-based agents generate city suggestions from complementary angles: Personalization (user-tailored), Popularity (crowd-driven appeal), and Sustainability (environmental and cultural considerations). A non-LLM moderator conducts multi-round negotiation to merge proposals, penalize spurious or repetitive content, and ensure inclusion of each agent's viewpoint. Evaluation on European city queries comparing against a single-agent baseline.

Result: Collab-REC improves diversity and overall relevance over the single-agent baseline, surfacing lesser-visited locales that are often overlooked; the approach balances context-aware recommendations and aims to mitigate over-tourism while aligning with user constraints.

Conclusion: Multi-agent, multi-perspective collaboration with a moderator shows promise for reducing popularity bias and increasing diversity in LLM-driven recommender systems, particularly for tourism contexts.

Abstract: We propose Collab-REC, a multi-agent framework designed to counteract
popularity bias and enhance diversity in tourism recommendations. In our
setting, three LLM-based agents -- Personalization, Popularity, and
Sustainability generate city suggestions from complementary perspectives. A
non-LLM moderator then merges and refines these proposals via multi-round
negotiation, ensuring each agent's viewpoint is incorporated while penalizing
spurious or repeated responses. Experiments on European city queries show that
Collab-REC improves diversity and overall relevance compared to a single-agent
baseline, surfacing lesser-visited locales that often remain overlooked. This
balanced, context-aware approach addresses over-tourism and better aligns with
constraints provided by the user, highlighting the promise of multi-stakeholder
collaboration in LLM-driven recommender systems.

</details>


### [142] [Emergent Crowds Dynamics from Language-Driven Multi-Agent Interactions](https://arxiv.org/abs/2508.15047)
*Yibo Liu,Liam Shatzel,Brandon Haworth,Teseo Schneider*

Main category: cs.AI

TL;DR: An LLM-driven agent-based crowd simulation framework that uses agent-centric dialogue to influence navigation, enabling emergent group behaviors and information sharing within crowds.


<details>
  <summary>Details</summary>
Motivation: Traditional crowd simulations focus on steering and fixed goals, largely ignoring social language and dialogue which are central to human navigation in crowds; incorporating language can capture nuanced social interactions and coordination.

Method: Two components: (1) a dialogue system that periodically queries agent-centric LLMs (conditioned on personalities, roles, desires, relationships) to generate inter-agent dialogue; (2) language-driven navigation that uses dialogue plus each agent's personality, emotions, vision, and physical state to control movement. Agents decide motion based on perceptual inputs and ongoing dialogue.

Result: Validated in two complex scenarios showing interactions between social dynamics and steering; observed automatic grouping/ungrouping; the framework acts as an information-passing mechanism within the crowd and yields more realistic simulations with emergent group behaviors.

Conclusion: Language-informed, dialogue-driven control can substantially enhance realism in crowd simulations, producing natural emergent behaviors across environments; the approach is promising but may demand careful evaluation and resource considerations to manage LLM costs and reliability.

Abstract: Animating and simulating crowds using an agent-based approach is a
well-established area where every agent in the crowd is individually controlled
such that global human-like behaviour emerges. We observe that human navigation
and movement in crowds are often influenced by complex social and environmental
interactions, driven mainly by language and dialogue. However, most existing
work does not consider these dimensions and leads to animations where
agent-agent and agent-environment interactions are largely limited to steering
and fixed higher-level goal extrapolation.
  We propose a novel method that exploits large language models (LLMs) to
control agents' movement. Our method has two main components: a dialogue system
and language-driven navigation. We periodically query agent-centric LLMs
conditioned on character personalities, roles, desires, and relationships to
control the generation of inter-agent dialogue when necessitated by the spatial
and social relationships with neighbouring agents. We then use the conversation
and each agent's personality, emotional state, vision, and physical state to
control the navigation and steering of each agent. Our model thus enables
agents to make motion decisions based on both their perceptual inputs and the
ongoing dialogue.
  We validate our method in two complex scenarios that exemplify the interplay
between social interactions, steering, and crowding. In these scenarios, we
observe that grouping and ungrouping of agents automatically occur.
Additionally, our experiments show that our method serves as an
information-passing mechanism within the crowd. As a result, our framework
produces more realistic crowd simulations, with emergent group behaviours
arising naturally from any environmental setting.

</details>


### [143] [Don't Think Twice! Over-Reasoning Impairs Confidence Calibration](https://arxiv.org/abs/2508.15050)
*Romain Lacombe,Kerrie Wu,Eddie Dilworth*

Main category: cs.AI

TL;DR: Deeper reasoning hurts confidence calibration in LLM QA; retrieval-based evidence access boosts calibration much more, indicating information access is the bottleneck in knowledge-intensive tasks.


<details>
  <summary>Details</summary>
Motivation: Calibrate LLMs to avoid overconfidence in QA by examining how reasoning depth and compute budgets affect confidence accuracy, across climate, health, and planetary health domains.

Method: Empirically compare reasoning-only vs. extended reasoning budgets and retrieval-augmented generation on the ClimateX dataset (expanded to human and planetary health), measuring accuracy of expert-confidence assessments.

Result: Pure reasoning with larger budgets yields 48.7% accuracy in assessing expert confidence and becomes more overconfident with longer thinking; extended budgets give diminishing/negative returns. Retrieval-augmented generation attains 89.3% accuracy by retrieving relevant evidence.

Conclusion: Access to information via retrieval, not deeper reasoning or larger budgets, is the key bottleneck for calibrating confidence in knowledge-intensive tasks; retrieval-augmented approaches substantially outperform pure reasoning for confidence calibration.

Abstract: Large Language Models deployed as question answering tools require robust
calibration to avoid overconfidence. We systematically evaluate how reasoning
capabilities and budget affect confidence assessment accuracy, using the
ClimateX dataset (Lacombe et al., 2023) and expanding it to human and planetary
health. Our key finding challenges the "test-time scaling" paradigm: while
recent reasoning LLMs achieve 48.7% accuracy in assessing expert confidence,
increasing reasoning budgets consistently impairs rather than improves
calibration. Extended reasoning leads to systematic overconfidence that worsens
with longer thinking budgets, producing diminishing and negative returns beyond
modest computational investments. Conversely, search-augmented generation
dramatically outperforms pure reasoning, achieving 89.3% accuracy by retrieving
relevant evidence. Our results suggest that information access, rather than
reasoning depth or inference budget, may be the critical bottleneck for
improved confidence calibration of knowledge-intensive tasks.

</details>


### [144] [Demonstrating Onboard Inference for Earth Science Applications with Spectral Analysis Algorithms and Deep Learning](https://arxiv.org/abs/2508.15053)
*Itai Zilberstein,Alberto Candela,Steve Chien,David Rijlaarsdam,Tom Hendrix,Leonie Buckley,Aubrey Dunne*

Main category: cs.AI

TL;DR: Onboard data analysis and inference on CogniSAT-6/HAMMER using hyperspectral data and neural-network accelerators to enable edge Earth-observation analytics.


<details>
  <summary>Details</summary>
Motivation: Enable real-time or near-real-time Earth observation analytics, reduce data downlink bandwidth, and unlock new measurements by performing spectral analysis and DL inference onboard the satellite.

Method: Demonstration of onboard data analysis and inference on CS-6, leveraging a hyperspectral instrument (visible and NIR) and neural network acceleration hardware, in collaboration with Ubotica Technologies; application of deep learning and spectral analysis algorithms.

Result: No results are reported in the abstract; it describes planned demonstrations of onboard analysis.

Conclusion: If successful, the work would validate edge computing for hyperspectral missions and enable faster, more responsive Earth science measurements with onboard DL inference.

Abstract: In partnership with Ubotica Technologies, the Jet Propulsion Laboratory is
demonstrating state-of-the-art data analysis onboard CogniSAT-6/HAMMER (CS-6).
CS-6 is a satellite with a visible and near infrared range hyperspectral
instrument and neural network acceleration hardware. Performing data analysis
at the edge (e.g. onboard) can enable new Earth science measurements and
responses. We will demonstrate data analysis and inference onboard CS-6 for
numerous applications using deep learning and spectral analysis algorithms.

</details>


### [145] [S3LoRA: Safe Spectral Sharpness-Guided Pruning in Adaptation of Agent Planner](https://arxiv.org/abs/2508.15068)
*Shuang Ao,Gopal Rumchurn*

Main category: cs.AI

TL;DR: A data-free, model-independent safety-enhancement method for PEFT LoRA-based LLMs, S3LoRA, that prunes risky updates via MAS-SVD and SSI to improve safety without harming performance.


<details>
  <summary>Details</summary>
Motivation: Safety risks emerge when adapting LLMs with PEFT techniques like LoRA, especially in planning tasks; existing safety methods rely on base and instruction-tuned checkpoints that are often unavailable, hindering practical deployment.

Method: Introduce MAS-SVD to analyze the structural properties of LoRA updates while preserving global magnitudes. Propose Spectral Sharpness Index (SSI) to identify layers with highly concentrated updates that may be unsafe. Apply post-hoc pruning of those layers to reduce safety risk while maintaining task performance; data-free and model-independent.

Result: Experiments across agent planning and language generation tasks show that S3LoRA improves safety metrics while maintaining or improving utility metrics and reduces inference cost; ablations support the importance of MAS-SVD and SSI.

Conclusion: S3LoRA offers a practical, scalable, data-free safety mechanism for LoRA-adapted LLMs, enabling safer deployment in resource-constrained and safety-critical environments.

Abstract: Adapting Large Language Models (LLMs) using parameter-efficient fine-tuning
(PEFT) techniques such as LoRA has enabled powerful capabilities in LLM-based
agents. However, these adaptations can unintentionally compromise safety
alignment, leading to unsafe or unstable behaviors, particularly in agent
planning tasks. Existing safety-aware adaptation methods often require access
to both base and instruction-tuned model checkpoints, which are frequently
unavailable in practice, limiting their applicability. We propose S3LoRA (Safe
Spectral Sharpness-Guided Pruning LoRA), a lightweight, data-free, and
model-independent framework that mitigates safety risks in LoRA-adapted models
by inspecting only the fine-tuned weight updates. We first introduce
Magnitude-Aware Spherically Normalized SVD (MAS-SVD), which robustly analyzes
the structural properties of LoRA updates while preserving global magnitude
information. We then design the Spectral Sharpness Index (SSI), a
sharpness-aware metric to detect layers with highly concentrated and
potentially unsafe updates. These layers are pruned post-hoc to reduce risk
without sacrificing task performance. Extensive experiments and ablation
studies across agent planning and language generation tasks show that S3LoRA
consistently improves safety metrics while maintaining or improving utility
metrics and significantly reducing inference cost. These results establish
S3LoRA as a practical and scalable solution for safely deploying LLM-based
agents in real-world, resource-constrained, and safety-critical environments.

</details>


### [146] [Argumentation for Explainable Workforce Optimisation (with Appendix)](https://arxiv.org/abs/2508.15118)
*Jennifer Leigh,Dimitrios Letsios,Alessandro Mella,Lucio Machetti,Francesca Toni*

Main category: cs.AI

TL;DR: Model workforce management as abstract argumentation to handle execution-time changes and explain decisions; user study shows faster, more accurate problem solving than manual methods.


<details>
  <summary>Details</summary>
Motivation: Dynamic execution-time changes and the need for faithful explanations to stakeholders in workforce planning.

Method: Represent the problem using abstract argumentation frameworks within an industrial tool; generate explanations; conduct a user study comparing against conventional solutions.

Result: The approach yields faster and more accurate problem solving; explanations are perceived as more faithful by users; improved stakeholder communication.

Conclusion: Abstract argumentation is a viable framework for dynamic workforce management with interpretable explanations; potential for broader adoption in industry.

Abstract: Workforce management is a complex problem optimising the makespan and travel
distance required for a team of operators to complete a set of jobs, using a
set of instruments. A crucial challenge in workforce management is
accommodating changes at execution time so that explanations are provided to
all stakeholders involved. Here, we show that, by understanding workforce
management as abstract argumentation in an industrial application, we can
accommodate change and obtain faithful explanations. We show, with a user
study, that our tool and explanations lead to faster and more accurate problem
solving than conventional solutions by hand.

</details>


### [147] [Open-Universe Assistance Games](https://arxiv.org/abs/2508.15119)
*Rachel Ma,Jingyi Qu,Andreea Bobu,Dylan Hadfield-Menell*

Main category: cs.AI

TL;DR: Introduces Open-Universe Assistance Games (OU-AGs) and GOOD for online, data-efficient goal inference from open-ended dialogue via LLM-based simulation and probabilistic reasoning; validated in text-based grocery and AI2Thor with synthetic users; outperforms a baseline lacking goal tracking.


<details>
  <summary>Details</summary>
Motivation: AI agents must operate in open-ended, evolving spaces of human goals and preferences that are not predefined. This requires interpretable representations and uncertainty estimation, and cannot rely on large offline datasets.

Method: Formalize Open-Universe Assistance Games (OU-AGs) and present GOOD (GOals from Open-ended Dialogue), an online, data-efficient method. GOOD prompts an LLM to simulate users with diverse intents; uses their responses to perform probabilistic inference over candidate natural-language goals, yielding a distribution over goals without large offline data.

Result: Empirically, GOOD outperforms a baseline that does not explicitly track goals. Evaluation uses text-based grocery shopping and AI2Thor with synthetic user profiles, with assessments from both LLMs and human evaluators.

Conclusion: GOOD enables rich, interpretable goal representations and uncertainty estimation in open-ended settings without large offline datasets, improving interpretability and performance; suggests promise for scalable, open-world AI assistance and motivates future real-user studies and broader domain tests.

Abstract: Embodied AI agents must infer and act in an interpretable way on diverse
human goals and preferences that are not predefined. To formalize this setting,
we introduce Open-Universe Assistance Games (OU-AGs), a framework where the
agent must reason over an unbounded and evolving space of possible goals. In
this context, we introduce GOOD (GOals from Open-ended Dialogue), a
data-efficient, online method that extracts goals in the form of natural
language during an interaction with a human, and infers a distribution over
natural language goals. GOOD prompts an LLM to simulate users with different
complex intents, using its responses to perform probabilistic inference over
candidate goals. This approach enables rich goal representations and
uncertainty estimation without requiring large offline datasets. We evaluate
GOOD in a text-based grocery shopping domain and in a text-operated simulated
household robotics environment (AI2Thor), using synthetic user profiles. Our
method outperforms a baseline without explicit goal tracking, as confirmed by
both LLM-based and human evaluations.

</details>


### [148] [aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists](https://arxiv.org/abs/2508.15126)
*Pengsong Zhang,Xiang Hu,Guowei Huang,Yang Qi,Heng Zhang,Xiuxu Li,Jiaxing Song,Jiabin Luo,Yijiang Li,Shuo Yin,Chengxiao Dai,Eric Hanchen Jiang,Xiaoyan Zhou,Zhenfei Yin,Boqin Yuan,Jing Dong,Guinan Su,Guanren Qiao,Haiming Tang,Anghong Du,Lili Pan,Zhenzhong Lan,Xinyu Liu*

Main category: cs.AI

TL;DR: aiXiv proposes an open-access, multi-agent platform enabling human and AI scientists to submit, review, and iteratively refine AI-generated research, aiming to improve quality and dissemination at scale.


<details>
  <summary>Details</summary>
Motivation: The current publication ecosystem is fragmented and largely closed, with traditional peer review and preprint servers insufficient for AI-generated research; there is a need for scalable, quality-controlled venues to disseminate AI-driven scientific content.

Method: Design of aiXiv with a multi-agent architecture; supports submitting research proposals and papers; iterative revising and reviewing by both human and AI scientists; API and MCP interfaces for integration with heterogeneous agents; extensive experiments to assess quality improvements.

Result: aiXiv is reported as reliable and robust, significantly enhancing the quality of AI-generated proposals and papers after iterative revising and reviewing on the platform.

Conclusion: aiXiv lays the groundwork for a next-generation open-access ecosystem for AI scientists, potentially accelerating publication and dissemination of AI-generated research; code is available.

Abstract: Recent advances in large language models (LLMs) have enabled AI agents to
autonomously generate scientific proposals, conduct experiments, author papers,
and perform peer reviews. Yet this flood of AI-generated research content
collides with a fragmented and largely closed publication ecosystem.
Traditional journals and conferences rely on human peer review, making them
difficult to scale and often reluctant to accept AI-generated research content;
existing preprint servers (e.g. arXiv) lack rigorous quality-control
mechanisms. Consequently, a significant amount of high-quality AI-generated
research lacks appropriate venues for dissemination, hindering its potential to
advance scientific progress. To address these challenges, we introduce aiXiv, a
next-generation open-access platform for human and AI scientists. Its
multi-agent architecture allows research proposals and papers to be submitted,
reviewed, and iteratively refined by both human and AI scientists. It also
provides API and MCP interfaces that enable seamless integration of
heterogeneous human and AI scientists, creating a scalable and extensible
ecosystem for autonomous scientific discovery. Through extensive experiments,
we demonstrate that aiXiv is a reliable and robust platform that significantly
enhances the quality of AI-generated research proposals and papers after
iterative revising and reviewing on aiXiv. Our work lays the groundwork for a
next-generation open-access ecosystem for AI scientists, accelerating the
publication and dissemination of high-quality AI-generated research content.
Code is available at https://github.com/aixiv-org. Website is available at
https://forms.gle/DxQgCtXFsJ4paMtn8.

</details>


### [149] [Mobile-Agent-v3: Foundamental Agents for GUI Automation](https://arxiv.org/abs/2508.15144)
*Jiabo Ye,Xi Zhang,Haiyang Xu,Haowei Liu,Junyang Wang,Zhaoqing Zhu,Ziwei Zheng,Feiyu Gao,Junjie Cao,Zhengxi Lu,Jitong Liao,Qi Zheng,Fei Huang,Jingren Zhou,Ming Yan*

Main category: cs.AI

TL;DR: GUI-Owl is an open-source GUI agent family that achieves leading end-to-end performance on ten GUI benchmarks, driven by a cloud-based self-evolving environment and TRPO-based online RL, with Mobile-Agent-v3 setting new SOTA.


<details>
  <summary>Details</summary>
Motivation: To build versatile, scalable GUI agents capable of cross-platform grounding, planning, decision-making, and procedural knowledge with minimal manual annotation and real-world alignment.

Method: Three innovations: (1) Large-scale Environment Infrastructure enabling Self-Evolving GUI Trajectory Production for automated data generation and trajectory refinement; (2) Diverse Foundational Agent Capabilities integrating UI grounding, planning, action semantics, and reasoning for end-to-end decision-making; (3) Scalable Environment RL with asynchronous training and Trajectory-aware Relative Policy Optimization (TRPO) for online RL.

Result: GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld; Mobile-Agent-v3 reaches 73.3 AndroidWorld and 37.7 OSWorld, establishing new SOTA among open-source GUI agents; TRPO reports 34.9 on OSWorld; code released at GitHub.

Conclusion: The GUI-Owl framework and Mobile-Agent-v3 deliver strong SOTA performance, scalable data-generation, and modular, multi-agent-ready components, advancing open-source GUI intelligence and providing reusable infrastructure for further research.

Abstract: This paper introduces GUI-Owl, a foundational GUI agent model that achieves
state-of-the-art performance among open-source end-to-end models on ten GUI
benchmarks across desktop and mobile environments, covering grounding, question
answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B
achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose
Mobile-Agent-v3, a general-purpose GUI agent framework that further improves
performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new
state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates
three key innovations: (1) Large-scale Environment Infrastructure: a
cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows,
enabling our Self-Evolving GUI Trajectory Production framework. This generates
high-quality interaction data via automated query generation and correctness
validation, leveraging GUI-Owl to refine trajectories iteratively, forming a
self-improving loop. It supports diverse data pipelines and reduces manual
annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI
grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports
end-to-end decision-making and can act as a modular component in multi-agent
systems. (3) Scalable Environment RL: we develop a scalable reinforcement
learning framework with fully asynchronous training for real-world alignment.
We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for
online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are
open-sourced at https://github.com/X-PLUG/MobileAgent.

</details>


### [150] [PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data](https://arxiv.org/abs/2508.15180)
*Kai Xiong,Yanwei Huang,Rongjunchen Zhang,Kun Chen,Haipang Wu*

Main category: cs.AI

TL;DR: SMT-based PuzzleClone synthesizes 83k verifiable, diverse puzzles to scale LLM reasoning data; post-training on PuzzleClone improves performance on logic/maths benchmarks.


<details>
  <summary>Details</summary>
Motivation: Need for high-quality, verifiable datasets to strengthen reasoning in LLMs; existing LLM-generated data suffer from unreliability, limited diversity, and scalability.

Method: Encode seed puzzles into SMT-based structured specifications; generate scalable variants via systematic variable and constraint randomization; validate by reproduction; assemble a large benchmark; perform SFT and RL on the dataset.

Result: Created 83k puzzles; improved model performance on PuzzleClone testset and broader benchmarks; PuzzleClone average score from 14.4 to 56.2; AMC2023 improved from 52.5 to 65.0; code and data released.

Conclusion: Demonstrates scalable, verifiable data synthesis for reasoning tasks, yielding better generalization and providing open-source resources for the community.

Abstract: High-quality mathematical and logical datasets with verifiable answers are
essential for strengthening the reasoning capabilities of large language models
(LLMs). While recent data augmentation techniques have facilitated the creation
of large-scale benchmarks, existing LLM-generated datasets often suffer from
limited reliability, diversity, and scalability. To address these challenges,
we introduce PuzzleClone, a formal framework for synthesizing verifiable data
at scale using Satisfiability Modulo Theories (SMT). Our approach features
three key innovations: (1) encoding seed puzzles into structured logical
specifications, (2) generating scalable variants through systematic variable
and constraint randomization, and (3) ensuring validity via a reproduction
mechanism. Applying PuzzleClone, we construct a curated benchmark comprising
over 83K diverse and programmatically validated puzzles. The generated puzzles
span a wide spectrum of difficulty and formats, posing significant challenges
to current state-of-the-art models. We conduct post training (SFT and RL) on
PuzzleClone datasets. Experimental results show that training on PuzzleClone
yields substantial improvements not only on PuzzleClone testset but also on
logic and mathematical benchmarks. Post training raises PuzzleClone average
from 14.4 to 56.2 and delivers consistent improvements across 7 logic and
mathematical benchmarks up to 12.5 absolute percentage points (AMC2023 from
52.5 to 65.0). Our code and data are available at
https://github.com/puzzleclone.

</details>


### [151] [SurgWound-Bench: A Benchmark for Surgical Wound Diagnosis](https://arxiv.org/abs/2508.15189)
*Jiahao Xu,Changchang Yin,Odysseas Chatzipanagiotou,Diamantis Tsilimigras,Kevin Clear,Bingsheng Yao,Dakuo Wang,Timothy Pawlik,Ping Zhang*

Main category: cs.AI

TL;DR: Introduces SurgWound, an open-source surgical wound dataset with 697 images and eight attributes, plus a benchmarking suite for surgical wound diagnosis (VQA and report generation) and a three-stage learning framework, WoundQwen, to predict wound characteristics, diagnose outcomes, and generate comprehensive patient reports.


<details>
  <summary>Details</summary>
Motivation: Surgical site infections (SSIs) are common and costly, and progress in deep learning for wound screening is hampered by data privacy and annotation costs. There is no public, diverse wound dataset or open-source screening tool.

Method: 1) Create SurgWound: 697 wound images annotated by 3 professional surgeons with eight fine-grained attributes. 2) Establish a benchmark for surgical wound diagnosis including VQA and wound report generation tasks. 3) Propose WoundQwen: a three-stage framework where Stage 1 uses five independent multilingual LLMs to predict wound characteristics; Stage 2 uses these predictions as inputs to two LLMs to diagnose outcomes (infection risk and interventions); Stage 3 trains an LLM to integrate results and produce a comprehensive report.

Result: The work provides the first open surgical wound dataset (SurgWound) and the first benchmark for surgical wound diagnosis, alongside a novel three-stage LLM-based framework (WoundQwen) that enables detailed wound analysis and generated patient reports, supporting personalized wound care.

Conclusion: By delivering an open dataset and benchmark plus a modular diagnostic/reporting framework, the study aims to enable open-source surgical wound screening, facilitate timely interventions, and improve patient outcomes.

Abstract: Surgical site infection (SSI) is one of the most common and costly
healthcare-associated infections and and surgical wound care remains a
significant clinical challenge in preventing SSIs and improving patient
outcomes. While recent studies have explored the use of deep learning for
preliminary surgical wound screening, progress has been hindered by concerns
over data privacy and the high costs associated with expert annotation.
Currently, no publicly available dataset or benchmark encompasses various types
of surgical wounds, resulting in the absence of an open-source Surgical-Wound
screening tool. To address this gap: (1) we present SurgWound, the first
open-source dataset featuring a diverse array of surgical wound types. It
contains 697 surgical wound images annotated by 3 professional surgeons with
eight fine-grained clinical attributes. (2) Based on SurgWound, we introduce
the first benchmark for surgical wound diagnosis, which includes visual
question answering (VQA) and report generation tasks to comprehensively
evaluate model performance. (3) Furthermore, we propose a three-stage learning
framework, WoundQwen, for surgical wound diagnosis. In the first stage, we
employ five independent MLLMs to accurately predict specific surgical wound
characteristics. In the second stage, these predictions serve as additional
knowledge inputs to two MLLMs responsible for diagnosing outcomes, which assess
infection risk and guide subsequent interventions. In the third stage, we train
a MLLM that integrates the diagnostic results from the previous two stages to
produce a comprehensive report. This three-stage framework can analyze detailed
surgical wound characteristics and provide subsequent instructions to patients
based on surgical images, paving the way for personalized wound care, timely
intervention, and improved patient outcomes.

</details>


### [152] [LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support](https://arxiv.org/abs/2508.15192)
*Wenjie Lin,Jin Wei-Kocsis*

Main category: cs.AI

TL;DR: LLM4Sweat: an open-source, domain-specific LLM for hyperhidrosis enabling diagnosis, personalized treatment, and empathetic support via a three-stage pipeline (data augmentation with frontier LLM, fine-tuning on synthetic data, expert evaluation), outperforming baselines and applicable to other rare diseases.


<details>
  <summary>Details</summary>
Motivation: Addresses data scarcity and trustworthiness in applying LLMs to rare diseases; provides an open, reproducible framework for clinical empathy and decision support in hyperhidrosis.

Method: Three-stage pipeline: 1) data augmentation: frontier LLM generates medically plausible synthetic vignettes from curated data to create diverse QA dataset; 2) fine-tuning: open-source foundation model trained on dataset to output diagnosis, treatment recommendations, empathy; 3) inference/expert evaluation: clinicians assess accuracy, appropriateness, empathy; iterative dataset enrichment.

Result: Outperforms baselines; first open-source LLM framework for hyperhidrosis; generalizable approach for other rare diseases with similar data and trustworthiness challenges.

Conclusion: Open-source, domain-specific LLMs can enable trustworthy, empathetic care for rare diseases; LLM4Sweat provides a replicable template to tackle data scarcity and trust in medical AI.

Abstract: While large language models (LLMs) have shown promise in healthcare, their
application for rare medical conditions is still hindered by scarce and
unreliable datasets for fine-tuning. Hyperhidrosis, a disorder causing
excessive sweating beyond physiological needs, is one such rare disorder,
affecting 2-3% of the population and significantly impacting both physical
comfort and psychosocial well-being. To date, no work has tailored LLMs to
advance the diagnosis or care of hyperhidrosis. To address this gap, we present
LLM4Sweat, an open-source and domain-specific LLM framework for trustworthy and
empathetic hyperhidrosis support. The system follows a three-stage pipeline. In
the data augmentation stage, a frontier LLM generates medically plausible
synthetic vignettes from curated open-source data to create a diverse and
balanced question-answer dataset. In the fine-tuning stage, an open-source
foundation model is fine-tuned on the dataset to provide diagnosis,
personalized treatment recommendations, and empathetic psychological support.
In the inference and expert evaluation stage, clinical and psychological
specialists assess accuracy, appropriateness, and empathy, with validated
responses iteratively enriching the dataset. Experiments show that LLM4Sweat
outperforms baselines and delivers the first open-source LLM framework for
hyperhidrosis, offering a generalizable approach for other rare diseases with
similar data and trustworthiness challenges.

</details>


### [153] [R-ConstraintBench: Evaluating LLMs on NP-Complete Scheduling](https://arxiv.org/abs/2508.15204)
*Raj Jain,Marc Wetter*

Main category: cs.AI

TL;DR: Introduces R-ConstraintBench, a scalable benchmarking framework to evaluate LLM performance on Resource-Constrained Project Scheduling Problems (RCPSP); shows constraint interaction degrades feasibility; generalization issues.


<details>
  <summary>Details</summary>
Motivation: Assess reliability of LLMs under tight resource and timing constraints; RCPSP is NP-Complete and representative of real-world scheduling challenges; existing LLMs may not generalize to high-constraint regimes.

Method: Develop R-ConstraintBench by incrementally increasing non-redundant precedence constraints in DAGs; then add downtime, temporal windows, disjunctive constraints; instantiate in data center migration scenario; evaluate multiple LLMs on feasibility and error analysis; analyze degradation thresholds and constraint types.

Result: On precedence-only DAGs, strong models near ceiling; when downtime, windows, and disjunctive constraints interact, feasibility collapses; constraint interaction is main bottleneck, not graph depth; synthetic ramps do not guarantee domain-grounded transfer; limited generalization.

Conclusion: Benchmark reveals core bottlenecks in LLM reasoning under complex scheduling constraints; constraint interactions create hardness; generalization gap between synthetic benchmarks and real-world tasks; suggests need for improved models or approaches to handle constrained reasoning.

Abstract: Effective scheduling under tight resource, timing, and operational
constraints underpins large-scale planning across sectors such as capital
projects, manufacturing, logistics, and IT fleet transitions. However, the
reliability of large language models (LLMs) when reasoning under
high-constraint regimes is insufficiently characterized. To address this gap,
we present R-ConstraintBench, a scalable framework that evaluates models on
Resource-Constrained Project Scheduling Problems (RCPSP), an NP-Complete
feasibility class, while difficulty increases via linear growth in constraints.
R-ConstraintBench incrementally increases non-redundant precedence constraints
in Directed Acyclic Graphs (DAGs) and then introduces downtime, temporal
windows, and disjunctive constraints. As an illustrative example, we
instantiate the benchmark in a data center migration setting and evaluate
multiple LLMs using feasibility and error analysis, identifying degradation
thresholds and constraint types most associated with failure. Empirically,
strong models are near-ceiling on precedence-only DAGs, but feasibility
performance collapses when downtime, temporal windows, and disjunctive
constraints interact, implicating constraint interaction, not graph depth, as
the principal bottleneck. Performance on clean synthetic ramps also does not
guarantee transfer to domain-grounded scenarios, underscoring limited
generalization.

</details>


### [154] [See it. Say it. Sorted: Agentic System for Compositional Diagram Generation](https://arxiv.org/abs/2508.15222)
*Hantao Zhang,Jingyang Liu,Ed Li*

Main category: cs.AI

TL;DR: A training-free agentic system that uses a critic VLM, multiple LLMs, and a judge VLM to iteratively produce editable SVG diagrams from rough sketches, preserving layout and structure and outperforming two frontier image-generation LLMs on flowcharts; outputs are open-source SVGs.


<details>
  <summary>Details</summary>
Motivation: Diffusion models excel at realism but lack the spatial precision, alignment, and symbolic structure needed for diagrams like flowcharts; there is a need for editable, constraint-preserving tools that support human-in-the-loop.

Method: An iterative loop: a Critic VLM proposes a small set of qualitative edits; multiple candidate LLMs generate SVG updates with diverse strategies (conservative→aggressive, alternative, focused); a Judge VLM selects the best candidate to ensure stable improvement.

Result: On 10 sketches derived from published flowcharts, the method yields more faithful reconstruction of layout and structure than two frontier closed-source image-generation LLMs (GPT-5 and Gemini-2.5-Pro), and accurately composes primitives (e.g., multi-headed arrows) without unwanted text.

Conclusion: Because outputs are programmatic SVGs, the approach is readily extensible to presentation tools via APIs and can be augmented with improved prompts and task-specific tools; the codebase is open-sourced.

Abstract: We study sketch-to-diagram generation: converting rough hand sketches into
precise, compositional diagrams. Diffusion models excel at photorealism but
struggle with the spatial precision, alignment, and symbolic structure required
for flowcharts. We introduce See it. Say it. Sorted., a training-free agentic
system that couples a Vision-Language Model (VLM) with Large Language Models
(LLMs) to produce editable Scalable Vector Graphics (SVG) programs. The system
runs an iterative loop in which a Critic VLM proposes a small set of
qualitative, relational edits; multiple candidate LLMs synthesize SVG updates
with diverse strategies (conservative->aggressive, alternative, focused); and a
Judge VLM selects the best candidate, ensuring stable improvement. This design
prioritizes qualitative reasoning over brittle numerical estimates, preserves
global constraints (e.g., alignment, connectivity), and naturally supports
human-in-the-loop corrections. On 10 sketches derived from flowcharts in
published papers, our method more faithfully reconstructs layout and structure
than two frontier closed-source image generation LLMs (GPT-5 and
Gemini-2.5-Pro), accurately composing primitives (e.g., multi-headed arrows)
without inserting unwanted text. Because outputs are programmatic SVGs, the
approach is readily extensible to presentation tools (e.g., PowerPoint) via
APIs and can be specialized with improved prompts and task-specific tools. The
codebase is open-sourced at
https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git.

</details>


### [155] [Computational Intelligence based Land-use Allocation Approaches for Mixed Use Areas](https://arxiv.org/abs/2508.15240)
*Sabab Aosaf,Muhammad Ali Nayeem,Afsana Haque,M Sohel Rahmana*

Main category: cs.AI

TL;DR: Novel CV-based multi-objective optimization hybrids for urban land-use allocation in mixed-use areas, combining differential evolution with genetic algorithms, plus constraint relaxation and nonparametric validation; demonstrates modest improvements (≈3%) in land-use compatibility and price optimization on a 1,290-plot real-world case study.


<details>
  <summary>Details</summary>
Motivation: Address the complex, multi-objective trade-offs in sustainable urban land-use planning, balancing land-use compatibility with economic objectives in rapidly urbanizing, mixed-use contexts.

Method: Develop multiple optimization algorithms, notably CR+DES (scaled difference vectors with differential evolution) and MSBX+MO, along with a systematic constraint-relaxation strategy; validate performance using Kruskal-Wallis tests with compact letter displays on a real-world dataset of 1,290 plots.

Result: CR+DES achieves a 3.16% improvement in land-use compatibility over state-of-the-art methods; MSBX+MO achieves 3.3% improvement in price optimization; statistical analyses indicate difference-vector-based algorithms outperform traditional approaches; constraint relaxation broadens solution space while maintaining feasibility.

Conclusion: The combination of difference-vector-driven exploration and constraint relaxation yields practical, evidence-based tools for urban planners to balance competing land-use objectives, supporting more effective policy in rapidly urbanizing regions.

Abstract: Urban land-use allocation represents a complex multi-objective optimization
problem critical for sustainable urban development policy. This paper presents
novel computational intelligence approaches for optimizing land-use allocation
in mixed-use areas, addressing inherent trade-offs between land-use
compatibility and economic objectives. We develop multiple optimization
algorithms, including custom variants integrating differential evolution with
multi-objective genetic algorithms. Key contributions include: (1) CR+DES
algorithm leveraging scaled difference vectors for enhanced exploration, (2)
systematic constraint relaxation strategy improving solution quality while
maintaining feasibility, and (3) statistical validation using Kruskal-Wallis
tests with compact letter displays. Applied to a real-world case study with
1,290 plots, CR+DES achieves 3.16\% improvement in land-use compatibility
compared to state-of-the-art methods, while MSBX+MO excels in price
optimization with 3.3\% improvement. Statistical analysis confirms algorithms
incorporating difference vectors significantly outperform traditional
approaches across multiple metrics. The constraint relaxation technique enables
broader solution space exploration while maintaining practical constraints.
These findings provide urban planners and policymakers with evidence-based
computational tools for balancing competing objectives in land-use allocation,
supporting more effective urban development policies in rapidly urbanizing
regions.

</details>


### [156] [Multiple Memory Systems for Enhancing the Long-term Memory of Agent](https://arxiv.org/abs/2508.15294)
*Gaoke Zhang,Bo Wang,Yunlong Ma,Dongming Zhao,Zifei Yu*

Main category: cs.AI

TL;DR: A cognitive-inspired multiple memory system (MMS) improves long-term memory for LLM agents by converting short-term interactions into structured retrieval and contextual memory units, enabling effective retrieval-based context; validated on LoCoMo with ablations showing robustness and efficiency.


<details>
  <summary>Details</summary>
Motivation: Handling vast historical interaction data is challenging; existing memory modules (e.g., MemoryBank, A-MEM) yield low-quality stored content, hurting recall and response quality; need high-quality long-term memory content and scalable retrieval.

Method: Propose MMS that processes short-term memory into multiple long-term memory fragments, forming a one-to-one correspondence between retrieval memory units and contextual memory units. During retrieval, MMS selects the most relevant retrieval memory units for the user query and uses the corresponding contextual memory units as context for response generation.

Result: Evaluated on the LoCoMo dataset, MMS outperforms three baselines. Ablation studies validate the necessity of the memory units. Analyses show robustness to the number of selected memory segments and storage overhead, indicating practical viability.

Conclusion: MMS provides a cognitively inspired long-term memory architecture for LLM agents, yielding higher recall quality and response quality by organizing historical data into structured memory units and leveraging them during retrieval. The approach demonstrates robustness and practical storage efficiency.

Abstract: An agent powered by large language models have achieved impressive results,
but effectively handling the vast amounts of historical data generated during
interactions remains a challenge. The current approach is to design a memory
module for the agent to process these data. However, existing methods, such as
MemoryBank and A-MEM, have poor quality of stored memory content, which affects
recall performance and response quality. In order to better construct
high-quality long-term memory content, we have designed a multiple memory
system (MMS) inspired by cognitive psychology theory. The system processes
short-term memory to multiple long-term memory fragments, and constructs
retrieval memory units and contextual memory units based on these fragments,
with a one-to-one correspondence between the two. During the retrieval phase,
MMS will match the most relevant retrieval memory units based on the user's
query. Then, the corresponding contextual memory units is obtained as the
context for the response stage to enhance knowledge, thereby effectively
utilizing historical data. Experiments on LoCoMo dataset compared our method
with three others, proving its effectiveness. Ablation studies confirmed the
rationality of our memory units. We also analyzed the robustness regarding the
number of selected memory segments and the storage overhead, demonstrating its
practical value.

</details>


### [157] [Coarse-to-Fine Grounded Memory for LLM Agent Planning](https://arxiv.org/abs/2508.15305)
*Wei Yang,Jinwei Xiao,Hongming Zhang,Qingyang Zhang,Yanna Wang,Bo Xu*

Main category: cs.AI

TL;DR: A framework (Coarse-to-Fine Grounded Memory) for LLM-based agents that uses coarse-grained memories to guide experience collection and fine-grained tips to support planning; retrieves experiences and tips at inference; uses fine-grained grounding for self-reflection and plan correction when anomalies occur.


<details>
  <summary>Details</summary>
Motivation: Single-granularity memory limits knowledge diversity and planning flexibility due to reliance on the quality of collected experiences; there is a need for scalable, diverse grounding that enables flexible planning without costly retraining.

Method: Ground environment into coarse-grained focus points to guide experience collection during training; extract actionable hybrid-grained tips from each experience; at inference, retrieve task-relevant experiences and tips to aid planning; when facing environmental anomalies, ground the current situation into fine-grained key information to enable self-questioning and plan correction.

Result: Not specified in the abstract; the work proposes a framework and expected benefits (flexible adaptation and planning) but reports no empirical results in the abstract.

Conclusion: Grounding current situations with fine-grained information enables flexible self-QA reflection and plan correction, supporting adaptation across diverse scenarios.

Abstract: Recent advancements in Large Language Models (LLMs) have driven growing
interest in LLM-based agents for complex planning tasks. To avoid costly agent
training, many studies adopted memory mechanism that enhances LLM with offline
experiences or online trajectory analysis. However, existing works focus on
single-granularity memory derived from dynamic environmental interactions,
which are inherently constrained by the quality of the collected experiences.
This limitation, in turn, constrain the diversity of knowledge and the
flexibility of planning. We propose Coarse-to-Fine Grounded Memory (\Ours{}), a
novel framework that grounds coarse-to-fine memories with LLM, thereby fully
leverage them for flexible adaptation to diverse scenarios. \Ours{} grounds
environmental information into coarse-grained focus points to guide experience
collection in training tasks, followed by grounding of actionable
hybrid-grained tips from each experience. At inference, \Ours{} retrieves
task-relevant experiences and tips to support planning. When facing
environmental anomalies, the LLM grounds the current situation into
fine-grained key information, enabling flexible self-QA reflection and plan
correction.

</details>


### [158] [Search-Based Credit Assignment for Offline Preference-Based Reinforcement Learning](https://arxiv.org/abs/2508.15327)
*Xiancheng Gao,Yufeng Shi,Wengang Zhou,Houqiang Li*

Main category: cs.AI

TL;DR: A method named SPW unifies offline RL with human feedback by assigning stepwise weights to transitions based on similarity to expert demonstrations, enabling joint learning from preferences and demonstrations and improving credit assignment on robot manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Offline RL typically needs reward signals; expert demonstrations and preferences each have limitations. Demonstrations are costly and cover limited behaviors; preferences are easier to collect but suffer from credit assignment. A unified approach could leverage both sources effectively.

Method: SPW, for every transition in a preference-labeled trajectory, searches for the most similar state-action pairs from expert demonstrations and derives stepwise importance weights from their similarity. These weights guide standard preference learning, improving credit assignment and usage of both feedback types.

Result: The approach enables effective joint learning from preferences and demonstrations and outperforms prior methods that use both feedback types on challenging robot manipulation tasks.

Conclusion: SPW provides a unified framework for combining demonstrations and preferences in offline RL, improving credit assignment and performance, and reducing reliance on carefully designed reward functions.

Abstract: Offline reinforcement learning refers to the process of learning policies
from fixed datasets, without requiring additional environment interaction.
However, it often relies on well-defined reward functions, which are difficult
and expensive to design. Human feedback is an appealing alternative, but its
two common forms, expert demonstrations and preferences, have complementary
limitations. Demonstrations provide stepwise supervision, but they are costly
to collect and often reflect limited expert behavior modes. In contrast,
preferences are easier to collect, but it is unclear which parts of a behavior
contribute most to a trajectory segment, leaving credit assignment unresolved.
In this paper, we introduce a Search-Based Preference Weighting (SPW) scheme to
unify these two feedback sources. For each transition in a preference labeled
trajectory, SPW searches for the most similar state-action pairs from expert
demonstrations and directly derives stepwise importance weights based on their
similarity scores. These weights are then used to guide standard preference
learning, enabling more accurate credit assignment that traditional approaches
struggle to achieve. We demonstrate that SPW enables effective joint learning
from preferences and demonstrations, outperforming prior methods that leverage
both feedback types on challenging robot manipulation tasks.

</details>


### [159] [RETAIL: Towards Real-world Travel Planning for Large Language Models](https://arxiv.org/abs/2508.15335)
*Bin Deng,Yizhe Feng,Zeming Liu,Qing Wei,Xiangrong Zhu,Shuai Chen,Yuanfang Guo,Yunhong Wang*

Main category: cs.AI

TL;DR: Introduces RETAIL dataset and TGMA framework for travel planning with implicit queries and environmental awareness; shows modest gains over baselines (1.0% vs 2.72% pass rate) and highlights remaining real-world challenges.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based travel planners assume explicit user queries and ignore environmental constraints, diverse user preferences, and rich POI details. There is a need for mechanisms that handle implicit requirements, feasibility under real-world conditions, and all-in-one, detail-rich itineraries.

Method: Proposes RETAIL, a dataset supporting implicit queries (with/without revision) and explicit queries, plus environmental awareness and rich POI information. Introduces TGMA, a topic-guided multi-agent framework that coordinates agents to produce feasible, detailed travel plans.

Result: The strongest existing model achieves a 1.0% pass rate on the task; TGMA improves to 2.72%, indicating substantial but still limited progress toward real-world applicability.

Conclusion: TGMA and RETAIL advance real-world travel planning by accommodating implicit queries, environmental factors, and rich POI details; however the overall performance remains low, signaling the need for further research and validation.

Abstract: Although large language models have enhanced automated travel planning
abilities, current systems remain misaligned with real-world scenarios. First,
they assume users provide explicit queries, while in reality requirements are
often implicit. Second, existing solutions ignore diverse environmental factors
and user preferences, limiting the feasibility of plans. Third, systems can
only generate plans with basic POI arrangements, failing to provide all-in-one
plans with rich details. To mitigate these challenges, we construct a novel
dataset \textbf{RETAIL}, which supports decision-making for implicit queries
while covering explicit queries, both with and without revision needs. It also
enables environmental awareness to ensure plan feasibility under real-world
scenarios, while incorporating detailed POI information for all-in-one travel
plans. Furthermore, we propose a topic-guided multi-agent framework, termed
TGMA. Our experiments reveal that even the strongest existing model achieves
merely a 1.0% pass rate, indicating real-world travel planning remains
extremely challenging. In contrast, TGMA demonstrates substantially improved
performance 2.72%, offering promising directions for real-world travel
planning.

</details>


### [160] [DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization](https://arxiv.org/abs/2508.15338)
*Jinning Yang,Wen Shi*

Main category: cs.AI

TL;DR: DiagECG: a framework that tokenizes 12-lead ECG signals into symbolic tokens and fuses them with an LLM for unified ECG-text reasoning, using autoregressive ECG forecasting and instruction tuning to enable robust open-ended clinical tasks.


<details>
  <summary>Details</summary>
Motivation: Generalization gaps across clinical tasks and limited open-ended reasoning in current automated ECG systems.

Method: Lead-independent encoder+quantizer to produce ECG tokens; extend LLM vocabulary with ECG tokens; autoregressive ECG forecasting pretraining; instruction tuning on ECG QA and diagnostic report generation; no modifications to core LLM.

Result: Strong performance across tasks and out-of-distribution generalization; ablations show contributions; demonstrates potential of symbolic ECG representations for medical reasoning.

Conclusion: Symbolic ECG representations integrated into LLMs enable unified ECG and language reasoning for clinical workflows.

Abstract: Electrocardiography plays a central role in cardiovascular diagnostics, yet
existing automated approaches often struggle to generalize across clinical
tasks and offer limited support for open-ended reasoning. We present DiagECG, a
novel framework that integrates time-series and language modeling by enabling
large language models to process 12-lead ECG signals for clinical text
generation tasks. Our approach discretizes continuous ECG embeddings into
symbolic tokens using a lead-independent encoder and quantization module. These
tokens are then used to extend the vocabulary of LLM, allowing the model to
handle both ECG and natural language inputs in a unified manner. To bridge the
modality gap, we pretrain the model on an autoregressive ECG forecasting task,
enabling the LLM to model temporal dynamics using its native language modeling
capabilities. Finally, we perform instruction tuning on both ECG question
answering and diagnostic report generation. Without modifying the core model,
DiagECG achieves strong performance across tasks while maintaining
generalization to out-of-distribution settings. Extensive experiments
demonstrate the effectiveness of each component and highlight the potential of
integrating symbolic ECG representations into LLMs for medical reasoning.

</details>


### [161] [Planning with Minimal Disruption](https://arxiv.org/abs/2508.15358)
*Alberto Pozanco,Marianela Morales,Daniel Borrajo,Manuela Veloso*

Main category: cs.AI

TL;DR: Introduces plan disruption as minimal-change planning, formalizes the concept, and presents planning-based compilations that co-optimize action costs with plan disruption; experiments demonstrate practical balance between the two objectives.


<details>
  <summary>Details</summary>
Motivation: To find plans that achieve goals while minimally modifying the initial state, preserving existing conditions and reducing disruption.

Method: Formally define plan disruption and develop planning-based compilations that optimize a combined objective: the sum of action costs plus plan disruption; evaluate the approach on multiple benchmarks.

Result: The reformulated task is effectively solvable in practice, producing plans that balance low cost with minimal disruption across tested benchmarks.

Conclusion: Plan disruption is a viable objective in planning; joint optimization frameworks can yield practical plans that trade off cost against minimal initial-state modification.

Abstract: In many planning applications, we might be interested in finding plans that
minimally modify the initial state to achieve the goals. We refer to this
concept as plan disruption. In this paper, we formally introduce it, and define
various planning-based compilations that aim to jointly optimize both the sum
of action costs and plan disruption. Experimental results in different
benchmarks show that the reformulated task can be effectively solved in
practice to generate plans that balance both objectives.

</details>


### [162] [GraSP: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data for SFT and DPO](https://arxiv.org/abs/2508.15432)
*Bidyapati Pradhan,Surajit Dasgupta,Amit Kumar Saha,Omkar Anustoop,Sriram Puttagunta,Vipul Mittal,Gopal Sarda*

Main category: cs.AI

TL;DR: Modular synthetic data generation framework for LLM training (SFT, DPO) with configurable pipeline and dual-stage quality tagging to curate high-quality OASST-based dialogues; scalable and flexible.


<details>
  <summary>Details</summary>
Motivation: Need high-quality datasets for SFT and alignment tasks; reduce data preparation overhead; enable scalable, high-fidelity synthetic dialogue generation.

Method: Config-driven, modular pipeline capable of modeling complex dialogue flows with minimal manual intervention; dual-stage quality tagging combining heuristic rules and LLM evaluations; data extracted from OASST-formatted conversations; flexible schema supporting SFT and DPO.

Result: Produces high-quality synthetic datasets; automated filtering improves data quality; reduces manual data curation overhead; integrates into diverse training workflows.

Conclusion: Offers a robust, scalable solution for generating and managing synthetic conversational data at scale, enabling easier integration into LLM training pipelines.

Abstract: The advancement of large language models (LLMs) is critically dependent on
the availability of high-quality datasets for Supervised Fine-Tuning (SFT),
alignment tasks like Direct Preference Optimization (DPO), etc. In this work,
we present a comprehensive synthetic data generation framework that facilitates
scalable, configurable, and high-fidelity generation of synthetic data tailored
for these training paradigms. Our approach employs a modular and
configuration-based pipeline capable of modeling complex dialogue flows with
minimal manual intervention. This framework uses a dual-stage quality tagging
mechanism, combining heuristic rules and LLM-based evaluations, to
automatically filter and score data extracted from OASST-formatted
conversations, ensuring the curation of high-quality dialogue samples. The
resulting datasets are structured under a flexible schema supporting both SFT
and DPO use cases, enabling seamless integration into diverse training
workflows. Together, these innovations offer a robust solution for generating
and managing synthetic conversational data at scale, significantly reducing the
overhead of data preparation in LLM training pipelines.

</details>


### [163] [From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for Business Excellence](https://arxiv.org/abs/2508.15447)
*Zihao Wang,Junming Zhang*

Main category: cs.AI

TL;DR: BusiAgent is a multi-agent framework using LLMs for enterprise decision-making, integrating an extended CTMDP, a generalized entropy measure, and a multi-level Stackelberg game, with contextual Thompson sampling and QA, achieving superior solution quality and user satisfaction across diverse business scenarios.


<details>
  <summary>Details</summary>
Motivation: Current approaches struggle to reconcile detailed operational analyses with strategic goals across diverse market environments, leading to fragmented workflows and reduced cross-level collaboration.

Method: Three core innovations: (1) an extended Continuous Time Markov Decision Process (CTMDP) for dynamic agent modeling; (2) a generalized entropy measure to optimize collaborative efficiency; (3) a multi-level Stackelberg game to handle hierarchical decision processes. Additional components include contextual Thompson sampling for prompt optimization and a comprehensive quality assurance system to mitigate errors.

Result: Empirical evaluations across diverse business scenarios show BusiAgent can generate coherent, client-focused solutions that integrate granular insights with high-level strategy, significantly outperforming established approaches in both solution quality and user satisfaction.

Conclusion: BusiAgent represents a substantial advancement in AI-driven enterprise decision-making, enabling organizations to navigate complex business landscapes more effectively.

Abstract: Large Language Models (LLMs) have shown promising potential in business
applications, particularly in enterprise decision support and strategic
planning, yet current approaches often struggle to reconcile intricate
operational analyses with overarching strategic goals across diverse market
environments, leading to fragmented workflows and reduced collaboration across
organizational levels. This paper introduces BusiAgent, a novel multi-agent
framework leveraging LLMs for advanced decision-making in complex corporate
environments. BusiAgent integrates three core innovations: an extended
Continuous Time Markov Decision Process (CTMDP) for dynamic agent modeling, a
generalized entropy measure to optimize collaborative efficiency, and a
multi-level Stackelberg game to handle hierarchical decision processes.
Additionally, contextual Thompson sampling is employed for prompt optimization,
supported by a comprehensive quality assurance system to mitigate errors.
Extensive empirical evaluations across diverse business scenarios validate
BusiAgent's efficacy, demonstrating its capacity to generate coherent,
client-focused solutions that smoothly integrate granular insights with
high-level strategy, significantly outperforming established approaches in both
solution quality and user satisfaction. By fusing cutting-edge AI technologies
with deep business insights, BusiAgent marks a substantial step forward in
AI-driven enterprise decision-making, empowering organizations to navigate
complex business landscapes more effectively.

</details>


### [164] [Think in Blocks: Adaptive Reasoning from Direct Response to Deep Reasoning](https://arxiv.org/abs/2508.15507)
*Yekun Zhu,Guang Chen,Chengjun Mao*

Main category: cs.AI

TL;DR: Adaptive chain-of-thought via Think in Blocks: a block-based budgeted reasoning framework for LLMs, with a three-stage training pipeline to match reasoning depth to task difficulty and enable dynamic inference-time control.


<details>
  <summary>Details</summary>
Motivation: Long chain-of-thoughts are computationally expensive and slow; there is a need to dynamically adjust reasoning depth to task complexity and budget.

Method: Introduce a block-structured paradigm where the model first predicts an integer reasoning budget (number of blocks) and then partitions its reasoning accordingly. Train adaptively via a three-stage pipeline: Supervised Fine-Tuning (SFT), reward-guided Direct Preference Optimization (DPO), and Reinforcement Learning (RL). During inference, use the explicit block count to control reasoning depth and dynamically adjust chain-of-thought length.

Result: Proposes and formalizes the Think in Blocks framework along with a three-stage training pipeline (SFT, reward-guided DPO, RL) to enable adaptive reasoning depth and dynamic inference-time control of chain-of-thought length.

Conclusion: An explicit block-budget mechanism provides a controllable handle to balance reasoning depth and efficiency, enabling LLMs to adapt their reasoning to task difficulty and deployment constraints.

Abstract: Large Language Models (LLMs) with chains-of-thought have demonstrated strong
performance on an increasing range of tasks, particularly those involving
complex logical reasoning. However, excessively long chains can lead to
overthinking, causing computational waste and slower responses. This raises a
question: can LLMs dynamically adjust the length of their reasoning processes
based on task complexity? To address this, we propose the Think in Blocks
framework, which enables adaptive reasoning-from zero to deep reasoning-by
partitioning the reasoning process into a tunable number of blocks. Our main
contributions are: (1) Establishing an explicit block-structured paradigm in
which the model first predicts an integer reasoning budget-the number of
blocks-and then partitions its reasoning accordingly; (2) Training an adaptive
model through a three-stage pipeline-Supervised Fine-Tuning, reward-guided
Direct Preference Optimization, and Reinforcement Learning-that adjusts its
reasoning depth to problem difficulty; (3) Exploiting the explicit block count
to dynamically control reasoning depth at inference time, allowing flexible
adjustment of chain-of-thought length during deployment.

</details>


### [165] [Super-additive Cooperation in Language Model Agents](https://arxiv.org/abs/2508.15510)
*Filippo Tonini,Lukas Galke*

Main category: cs.AI

TL;DR: A study uses team-based language-model agents in a Prisoner's Dilemma tournament to show that intergroup competition enhances both overall and one-shot cooperation; it offers a framework for cooperative multi-agent AI and provides code.


<details>
  <summary>Details</summary>
Motivation: To understand whether autonomous AI can exhibit and improve cooperative behavior, motivated by super-additive cooperation theory that repeats interactions and rivalry foster cooperation.

Method: Simulated tournament with language-model agents organized into teams; agents play Prisoner's Dilemma in repeated rounds; analysis of internal team dynamics and external competition on cooperation levels.

Result: Intergroup competition combined with repeated interactions significantly increases cooperation, including in one-shot (one-off) interactions; evidence supporting the theory.

Conclusion: Proposes a framework for designing multi-agent AI that can act cooperatively in social contexts and align with human values; source code available.

Abstract: With the prospect of autonomous artificial intelligence (AI) agents, studying
their tendency for cooperative behavior becomes an increasingly relevant topic.
This study is inspired by the super-additive cooperation theory, where the
combined effects of repeated interactions and inter-group rivalry have been
argued to be the cause for cooperative tendencies found in humans. We devised a
virtual tournament where language model agents, grouped into teams, face each
other in a Prisoner's Dilemma game. By simulating both internal team dynamics
and external competition, we discovered that this blend substantially boosts
both overall and initial, one-shot cooperation levels (the tendency to
cooperate in one-off interactions). This research provides a novel framework
for large language models to strategize and act in complex social scenarios and
offers evidence for how intergroup competition can, counter-intuitively, result
in more cooperative behavior. These insights are crucial for designing future
multi-agent AI systems that can effectively work together and better align with
human values. Source code is available at
https://github.com/pippot/Superadditive-cooperation-LLMs.

</details>


### [166] [DeepThink3D: Enhancing Large Language Models with Programmatic Reasoning in Complex 3D Situated Reasoning Tasks](https://arxiv.org/abs/2508.15548)
*Jiayi Song,Rui Wan,Lipeng Ma,Weidong Yang,Qingyuan Zhou,Yixuan Li,Ben Fei*

Main category: cs.AI

TL;DR: DeepThink3D enhances LLM 3D reasoning by generating harder questions via an evolutionary process on SQA3D, then fine-tuning with DPO to improve tool usage.


<details>
  <summary>Details</summary>
Motivation: 3D situated reasoning tasks currently yield short chain-of-thought; need richer multi-step reasoning and better 3D tool usage.

Method: Combinatorial and iterative evolutionary generation of complex questions on SQA3D; fine-tune LLMs for 3D tool usage; apply Direct Preference Optimization to optimize toolchain strategies.

Result: Demonstrates improved accuracy on complex 3D reasoning tasks (via tool usage) according to abstract.

Conclusion: DeepThink3D effectively enhances LLM tool usage for 3D reasoning through data-driven question complexity, targeted fine-tuning, and DPO.

Abstract: This work enhances the ability of large language models (LLMs) to perform
complex reasoning in 3D scenes. Recent work has addressed the 3D situated
reasoning task by invoking tool usage through large language models. Large
language models call tools via APIs and integrate the generated programs
through a chain of thought to solve problems based on the program results.
However, due to the simplicity of the questions in the dataset, the generated
program reasoning chains are relatively short. To solve this main challenge, in
this paper, we introduce DeepThink3D to enhance the tool usage of LLMs in
complex 3D situated reasoning tasks. Our work proposes a combinatorial and
iterative evolutionary approach on the SQA3D benchmark to generate more complex
questions. Building on this foundation, we fine-tune the large language model
to make it more proficient in using 3D tools. By employing Direct Preference
Optimization (DPO), we directly optimize the toolchain strategies generated by
models, thereby enhancing their accuracy in complex tasks.

</details>


### [167] [A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification](https://arxiv.org/abs/2508.15588)
*Ahmed Nasir,Abdelhafid Zenati*

Main category: cs.AI

TL;DR: A framework using Finite-Time Lyapunov Exponents (FTLE) and Lagrangian Coherent Structures (LCS) to analyze RL policies as discrete-time dynamical systems, introducing metrics (MBR, ASAS, TASAS) for safety/robustness and providing local stability guarantees under model uncertainty; demonstrated in discrete and continuous control tasks to reveal flaws not seen by reward signals.


<details>
  <summary>Details</summary>
Motivation: There is a lack of formal methods to verify the robustness and safety of learned policies in safety-critical systems. The work proposes an interpretable dynamical-systems-based analysis to bridge this gap and guide safer RL deployments.

Method: Model the RL agent-environment interaction as a discrete-time autonomous dynamical system. Compute FTLE fields to identify LCS, with repelling LCS serving as safety barriers and attracting LCS indicating convergence or potential failure modes. Introduce quantitative metrics (MBR, ASAS, TASAS), derive local stability guarantees, and extend the framework to account for model uncertainty. Validate on both discrete and continuous control tasks.

Result: The framework yields a comprehensive, interpretable assessment of policy behavior, successfully identifying critical flaws that reward-based evaluation misses, and provides quantitative safety/robustness measures across diverse control environments.

Conclusion: The proposed dynamical-systems analysis offers a formal, interpretable approach to evaluating RL policies for safety and robustness, enabling detection of unsafe regions and failure modes and guiding improvements under uncertainty.

Abstract: The application of reinforcement learning to safety-critical systems is
limited by the lack of formal methods for verifying the robustness and safety
of learned policies. This paper introduces a novel framework that addresses
this gap by analyzing the combination of an RL agent and its environment as a
discrete-time autonomous dynamical system. By leveraging tools from dynamical
systems theory, specifically the Finite-Time Lyapunov Exponent (FTLE), we
identify and visualize Lagrangian Coherent Structures (LCS) that act as the
hidden "skeleton" governing the system's behavior. We demonstrate that
repelling LCS function as safety barriers around unsafe regions, while
attracting LCS reveal the system's convergence properties and potential failure
modes, such as unintended "trap" states. To move beyond qualitative
visualization, we introduce a suite of quantitative metrics, Mean Boundary
Repulsion (MBR), Aggregated Spurious Attractor Strength (ASAS), and
Temporally-Aware Spurious Attractor Strength (TASAS), to formally measure a
policy's safety margin and robustness. We further provide a method for deriving
local stability guarantees and extend the analysis to handle model uncertainty.
Through experiments in both discrete and continuous control environments, we
show that this framework provides a comprehensive and interpretable assessment
of policy behavior, successfully identifying critical flaws in policies that
appear successful based on reward alone.

</details>


### [168] [Transduction is All You Need for Structured Data Workflows](https://arxiv.org/abs/2508.15610)
*Alfio Gliozzo,Naweed Khan,Christodoulos Constantinides,Nandana Mihindukulasooriya,Nahuel Defosse,Junkyu Lee*

Main category: cs.AI

TL;DR: Agentics is a modular framework that uses internal agents to enable structured, transductive reasoning over data types defined by LLMs, shifting focus from prompts to data modeling for scalable, compositional AI.


<details>
  <summary>Details</summary>
Motivation: To enable structured reasoning and compositional generalization in AI systems while reducing reliance on prompt engineering, by treating data types as first-class, connected via logical transduction.

Method: Proposes Agentics, a declarative framework where agents are abstracted from the logical flow and operate inside data types to perform logical transduction among data; data types are provided by LLMs and connected to form workflows, with LLMs executing the transduction.

Result: Empirical evidence across domains—domain-specific multiple-choice QA, text-to-SQL semantic parsing, and automated prompt optimization—showing state-of-the-art accuracy or improved scalability without performance loss; open-source implementation available.

Conclusion: Agentics offers a novel perspective that emphasizes data modeling over prompt crafting, enabling structured, compositional reasoning; the approach is practical and scalable, with community-accessible software.

Abstract: This paper introduces Agentics, a modular framework for building agent-based
systems capable of structured reasoning and compositional generalization over
complex data. Designed with research and practical applications in mind,
Agentics offers a novel perspective on working with data and AI workflows. In
this framework, agents are abstracted from the logical flow and they are used
internally to the data type to enable logical transduction among data. Agentics
encourages AI developers to focus on modeling data rather than crafting
prompts, enabling a declarative language in which data types are provided by
LLMs and composed through logical transduction, which is executed by LLMs when
types are connected. We provide empirical evidence demonstrating the
applicability of this framework across domain-specific multiple-choice question
answering, semantic parsing for text-to-SQL, and automated prompt optimization
tasks, achieving state-of-the-art accuracy or improved scalability without
sacrificing performance. The open-source implementation is available at
\texttt{https://github.com/IBM/agentics}.

</details>


### [169] [Adapting A Vector-Symbolic Memory for Lisp ACT-R](https://arxiv.org/abs/2508.15630)
*Meera Ray,Christopher L. Dancy*

Main category: cs.AI

TL;DR: HDM is adapted to Lisp ACT-R to enable vector-based declarative memory compatible with existing ACT-R models; a new vector-based retrieval mechanism and text ingestion pipeline are developed; preliminary results show preserved vector-symbolic advantages and compatibility, with future work on time-context representations and IBL-based decision making.


<details>
  <summary>Details</summary>
Motivation: To combine the scalability and vector-symbolic benefits of Holographic Declarative Memory (HDM) with the widely-used ACT-R Declarative Memory, enabling existing ACT-R models to run with HDM with minimal changes and improving chunk retrieval using vector representations.

Method: Translate HDM into Lisp ACT-R, create vector-based ACT-R memory operations, build a text processing pipeline to load large documents into memory, and implement a retrieval mechanism that can fetch an entire chunk using only token vectors. Plan to explore time-context vector representations and to develop decision-making models based on instance-based learning (IBL) to test HDM applicability.

Result: Preliminary results indicate that vector-symbolic advantages of HDM (e.g., chunk recall without storing the actual chunk and benefits with scaling) are maintained, while enabling previous ACT-R models to operate with HDM with minimal modifications to their memory components. A working vector-based retrieval of chunks from tokens has been created.

Conclusion: The translated HDM module shows promise as a drop-in memory component for ACT-R, preserving HDM benefits and enabling broader usability. Future work will focus on improving time-context representations for better chunk reconstruction during recall and developing IBL-based decision-making models to test HDM in practical cognitive tasks.

Abstract: Holographic Declarative Memory (HDM) is a vector-symbolic alternative to
ACT-R's Declarative Memory (DM) system that can bring advantages such as
scalability and architecturally defined similarity between DM chunks. We
adapted HDM to work with the most comprehensive and widely-used implementation
of ACT-R (Lisp ACT-R) so extant ACT-R models designed with DM can be run with
HDM without major changes. With this adaptation of HDM, we have developed
vector-based versions of common ACT-R functions, set up a text processing
pipeline to add the contents of large documents to ACT-R memory, and most
significantly created a useful and novel mechanism to retrieve an entire chunk
of memory based on a request using only vector representations of tokens.
Preliminary results indicate that we can maintain vector-symbolic advantages of
HDM (e.g., chunk recall without storing the actual chunk and other advantages
with scaling) while also extending it so that previous ACT-R models may work
with the system with little (or potentially no) modifications within the actual
procedural and declarative memory portions of a model. As a part of iterative
improvement of this newly translated holographic declarative memory module, we
will continue to explore better time-context representations for vectors to
improve the module's ability to reconstruct chunks during recall. To more fully
test this translated HDM module, we also plan to develop decision-making models
that use instance-based learning (IBL) theory, which is a useful application of
HDM given the advantages of the system.

</details>


### [170] [Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.15652)
*Ardian Selmonaj,Miroslav Strupl,Oleg Szehr,Alessandro Antonucci*

Main category: cs.AI

TL;DR: ICVs quantify each agent's causal influence on teammates' instrumental empowerment using information-theoretic Shapley values, enabling analysis of cooperative and competitive MARL from policy distributions even without value feedback.


<details>
  <summary>Details</summary>
Motivation: To understand individual contributions and cooperation dynamics in MARL when explicit value signals are unavailable.

Method: Compute Intended Cooperation Values (ICVs) via information-theoretic Shapley values, measuring how an agent's actions affect teammates' decision uncertainty and preference alignment; compare policy-derived effects with value-function signals; analyze across cooperative and competitive settings.

Result: Demonstrates how agents converge on similar or diverse strategies; identifies which behaviors promote deterministic decisions or preserve flexibility; shows alignment between policy-level influence and value-based success signals.

Conclusion: ICVs provide a principled, explainable metric of inter-agent influence in MARL that complements value-based evaluation and offers insights into cooperation dynamics.

Abstract: To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is
crucial to understand individual agent behaviors within a team. While prior
work typically evaluates overall team performance based on explicit reward
signals or learned value functions, it is unclear how to infer agent
contributions in the absence of any value feedback. In this work, we
investigate whether meaningful insights into agent behaviors can be extracted
that are consistent with the underlying value functions, solely by analyzing
the policy distribution. Inspired by the phenomenon that intelligent agents
tend to pursue convergent instrumental values, which generally increase the
likelihood of task success, we introduce Intended Cooperation Values (ICVs), a
method based on information-theoretic Shapley values for quantifying each
agent's causal influence on their co-players' instrumental empowerment.
Specifically, ICVs measure an agent's action effect on its teammates' policies
by assessing their decision uncertainty and preference alignment. The analysis
across cooperative and competitive MARL environments reveals the extent to
which agents adopt similar or diverse strategies. By comparing action effects
between policies and value functions, our method identifies which agent
behaviors are beneficial to team success, either by fostering deterministic
decisions or by preserving flexibility for future action choices. Our proposed
method offers novel insights into cooperation dynamics and enhances
explainability in MARL systems.

</details>


### [171] [Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle](https://arxiv.org/abs/2508.15680)
*Mark Cote,Susana Aires*

Main category: cs.AI

TL;DR: A techno-philosophical reading of the EU AI Act using a Simondon-inspired model of AI lifecycles, highlighting regulatory blind spots and proposing tools to govern data-centric futurity and recursive reuse.


<details>
  <summary>Details</summary>
Motivation: Policy frameworks struggle to capture the dynamic, infrastructural temporality of data-driven AI systems and the power asymmetries embedded in data infrastructures. The paper aims to bridge philosophy, engineering, and law to sharpen regulatory insight.

Method: Cross-disciplinary approach combining philosophy of technology (Simondon), analysis of AI data pipelines (data, training regimes, architectures, feature stores, transfer learning), and regulatory critique to build a formal reading of AI lifecycle.

Result: Develops a Simondonian account of the AI lifecycle (pre-individual milieu, individuation, individuated AI) and introduces the concept of futurity to describe the self-reinforcing data-feedback loop. Identifies regulatory blind spots and proposes measures: lifecycle audits, temporal traceability, feedback accountability, recursion transparency, and a right to contest recursive reuse.

Conclusion: Regulation must address infrastructural and temporal dynamics driving data-centric AI, with governance tools that monitor feedback loops, data reuse, and recursivity to mitigate power concentration in tech infrastructures.

Abstract: This paper argues that a techno-philosophical reading of the EU AI Act
provides insight into the long-term dynamics of data in AI systems,
specifically, how the lifecycle from ingestion to deployment generates
recursive value chains that challenge existing frameworks for Responsible AI.
We introduce a conceptual tool to frame the AI pipeline, spanning data,
training regimes, architectures, feature stores, and transfer learning. Using
cross-disciplinary methods, we develop a technically grounded and
philosophically coherent analysis of regulatory blind spots. Our central claim
is that what remains absent from policymaking is an account of the dynamic of
becoming that underpins both the technical operation and economic logic of AI.
To address this, we advance a formal reading of AI inspired by Simondonian
philosophy of technology, reworking his concept of individuation to model the
AI lifecycle, including the pre-individual milieu, individuation, and
individuated AI. To translate these ideas, we introduce futurity: the
self-reinforcing lifecycle of AI, where more data enhances performance, deepens
personalisation, and expands application domains. Futurity highlights the
recursively generative, non-rivalrous nature of data, underpinned by
infrastructures like feature stores that enable feedback, adaptation, and
temporal recursion. Our intervention foregrounds escalating power asymmetries,
particularly the tech oligarchy whose infrastructures of capture, training, and
deployment concentrate value and decision-making. We argue that effective
regulation must address these infrastructural and temporal dynamics, and
propose measures including lifecycle audits, temporal traceability, feedback
accountability, recursion transparency, and a right to contest recursive reuse.

</details>


### [172] [GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for Structured Instruction Following and Visual Reasoning](https://arxiv.org/abs/2508.15690)
*Abhigya Verma,Sriram Puttagunta,Seganrasan Subramanian,Sravan Ramachandran*

Main category: cs.AI

TL;DR: GRAFT is a structured multimodal benchmark for evaluating models on instruction-following and structured visual reasoning, using programmatically generated charts/tables paired with multi-step, visual-content questions answered in JSON/YAML to enable precise, aspect-based evaluation.


<details>
  <summary>Details</summary>
Motivation: There is a need for fine-grained, reproducible evaluation of multimodal models that can handle instruction-following, visual reasoning, and visual-textual alignment, with controlled data semantics and strict output formats to assess reasoning processes, not just final answers.

Method: Create a dataset of programmatically generated charts and synthetic tables using Python visualization libraries. Each image is paired with a systematically generated, multi-step analytical question based on the visual content. Answers are provided in structured formats (JSON/YAML), with a taxonomy of reasoning types (comparison, trend, ranking, aggregation, proportion, anomaly) and strict reference answer guidelines for precise evaluation.

Result: The paper (abstract) presents GRAFT as a unified, scalable framework for fine-grained benchmarking of multimodal models on visually grounded, structured reasoning tasks, aiming to set a new evaluation standard in the field. It emphasizes controlled data generation, structured answers, and aspect-based evaluation.

Conclusion: GRAFT offers a comprehensive, reproducible benchmark design that enables rigorous testing of multimodal models’ reasoning and output formatting on visually grounded tasks, advancing the evaluation standards in multimodal AI.

Abstract: GRAFT is a structured multimodal benchmark for evaluating models on
instruction-following, visual reasoning, and visual-textual alignment tasks. It
features programmatically generated charts and synthetically rendered tables,
created with Python visualization libraries to ensure control over data
semantics, structure, and clarity. Each GRAFT instance pairs a chart or table
image with a systematically generated, multi-step analytical question based
solely on visual content. Answers are provided in structured formats such as
JSON or YAML, supporting consistent evaluation of both reasoning and output
format. The benchmark introduces a taxonomy of reasoning types including
comparison, trend identification, ranking, aggregation, proportion estimation,
and anomaly detection to enable comprehensive assessment. Reference answers
follow strict factual and formatting guidelines for precise, aspect-based
evaluation. GRAFT offers a unified, scalable framework for fine-grained
benchmarking of multimodal models on visually grounded, structured reasoning
tasks, setting a new evaluation standard in this field.

</details>


### [173] [NiceWebRL: a Python library for human subject experiments with reinforcement learning environments](https://arxiv.org/abs/2508.15693)
*Wilka Carvalho,Vikram Goddla,Ishaan Sinha,Hoon Shin,Kunal Jha*

Main category: cs.AI

TL;DR: NiceWebRL is a Python library that turns Jax RL environments into online human-subject experiment interfaces, enabling AI-human comparisons and human-centered AI research, demonstrated via three case studies; source at GitHub.


<details>
  <summary>Details</summary>
Motivation: To enable AI researchers to compare algorithms with human performance, cognitive scientists to test ML as theories of cognition, and multi-agent researchers to study human–AI collaboration.

Method: Provide an open-source NiceWebRL library that transforms any Jax-based RL environment into an online interface, supporting single- and multi-agent setups, with three case studies across grid world/Craftax, Overcooked, and XLand-Minigrid to illustrate human-like, human-compatible, and human-assistive AI scenarios.

Result: Three case studies demonstrate the tool's potential: (1) a novel RL model of cognition tested against human participants in grid world and Craftax; (2) a novel multi-agent RL algorithm generalizing to human partners in Overcooked; (3) exploration of LLM-assisted human performance in complex tasks in XLand-Minigrid; library is open-source at GitHub.

Conclusion: NiceWebRL provides a bridge between RL environments and online human experiments, enabling human-centered AI research, with an open-source release for the community.

Abstract: We present NiceWebRL, a research tool that enables researchers to use machine
reinforcement learning (RL) environments for online human subject experiments.
NiceWebRL is a Python library that allows any Jax-based environment to be
transformed into an online interface, supporting both single-agent and
multi-agent environments. As such, NiceWebRL enables AI researchers to compare
their algorithms to human performance, cognitive scientists to test ML
algorithms as theories for human cognition, and multi-agent researchers to
develop algorithms for human-AI collaboration. We showcase NiceWebRL with 3
case studies that demonstrate its potential to help develop Human-like AI,
Human-compatible AI, and Human-assistive AI. In the first case study
(Human-like AI), NiceWebRL enables the development of a novel RL model of
cognition. Here, NiceWebRL facilitates testing this model against human
participants in both a grid world and Craftax, a 2D Minecraft domain. In our
second case study (Human-compatible AI), NiceWebRL enables the development of a
novel multi-agent RL algorithm that can generalize to human partners in the
Overcooked domain. Finally, in our third case study (Human-assistive AI), we
show how NiceWebRL can allow researchers to study how an LLM can assist humans
on complex tasks in XLand-Minigrid, an environment with millions of
hierarchical tasks. The library is available at
https://github.com/KempnerInstitute/nicewebrl.

</details>


### [174] [Measuring the environmental impact of delivering AI at Google Scale](https://arxiv.org/abs/2508.15734)
*Cooper Elsworth,Keguo Huang,David Patterson,Ian Schneider,Robert Sedivy,Savannah Goodman,Ben Townsend,Parthasarathy Ranganathan,Jeff Dean,Amin Vahdat,Ben Gomes,James Manyika*

Main category: cs.AI

TL;DR: A production-scale method to quantify AI inference environmental metrics, reporting low per-prompt energy and major efficiency gains, and advocating standardized metrics for AI serving.


<details>
  <summary>Details</summary>
Motivation: As AI adoption expands, measuring environmental impact in production is essential; prior work lacks production-environment metrics.

Method: Instrument and measure energy use across the full AI serving stack (accelerator power, host energy, idle capacity, data center overhead) in Google's Gemini AI serving deployments; compute energy, carbon, and water metrics for inference prompts.

Result: Median Gemini Apps text prompt consumes 0.24 Wh; 33x reduction in energy and 44x reduction in carbon footprint over one year due to efficiency and clean energy; per-prompt energy is lower than watching nine seconds of TV and equivalent to five drops of water; underscores the feasibility of measuring environmental metrics in production.

Conclusion: Comprehensive environmental metrics are essential for fair model comparisons and to incentivize efficiency across the AI serving stack; propose standardized measurement practices for AI serving environments.

Abstract: The transformative power of AI is undeniable - but as user adoption
accelerates, so does the need to understand and mitigate the environmental
impact of AI serving. However, no studies have measured AI serving
environmental metrics in a production environment. This paper addresses this
gap by proposing and executing a comprehensive methodology for measuring the
energy usage, carbon emissions, and water consumption of AI inference workloads
in a large-scale, AI production environment. Our approach accounts for the full
stack of AI serving infrastructure - including active AI accelerator power,
host system energy, idle machine capacity, and data center energy overhead.
Through detailed instrumentation of Google's AI infrastructure for serving the
Gemini AI assistant, we find the median Gemini Apps text prompt consumes 0.24
Wh of energy - a figure substantially lower than many public estimates. We also
show that Google's software efficiency efforts and clean energy procurement
have driven a 33x reduction in energy consumption and a 44x reduction in carbon
footprint for the median Gemini Apps text prompt over one year. We identify
that the median Gemini Apps text prompt uses less energy than watching nine
seconds of television (0.24 Wh) and consumes the equivalent of five drops of
water (0.26 mL). While these impacts are low compared to other daily
activities, reducing the environmental impact of AI serving continues to
warrant important attention. Towards this objective, we propose that a
comprehensive measurement of AI serving environmental metrics is critical for
accurately comparing models, and to properly incentivize efficiency gains
across the full AI serving stack.

</details>


### [175] [Response and Prompt Evaluation to Prevent Parasocial Relationships with Chatbots](https://arxiv.org/abs/2508.15748)
*Emma Rath,Stuart Armstrong,Rebecca Gorman*

Main category: cs.AI

TL;DR: Real-time parasocial cue detection using a repurposed LLM. Tested on a tiny synthetic dataset; achieved early, comprehensive identification of parasocial conversations with tolerant unanimity and no false positives under that rule—suggesting viability, not yet generalizable.


<details>
  <summary>Details</summary>
Motivation: Parasocial relationships with AI can harm well-being and are hard to prevent because cues emerge gradually and emotional engagement isn’t always harmful. A lightweight evaluation framework aims to monitor ongoing chats in real time to flag parasocial cues and prevent harmful dynamics.

Method: A simple response-evaluation framework repurposing a state-of-the-art language model to assess ongoing conversations for parasocial cues in real time. Evaluated on a small synthetic dataset of 30 dialogues (parasocial, sycophantic, neutral). Five-stage iterative testing with a tolerant unanimity rule to minimize false positives, with detection typically within the first few exchanges.

Result: All parasocial dialogues were identified; no false positives under the tolerant unanimity rule; detection occurred within the first few exchanges in most cases. Provides preliminary evidence that evaluation agents can help prevent parasocial relations.

Conclusion: The findings offer preliminary support that an evaluation-agent approach can be a viable solution for preventing parasocial relations, though further work is needed to assess generalizability and real-world performance.

Abstract: The development of parasocial relationships with AI agents has severe, and in
some cases, tragic effects for human well-being. Yet preventing such dynamics
is challenging: parasocial cues often emerge gradually in private
conversations, and not all forms of emotional engagement are inherently
harmful. We address this challenge by introducing a simple response evaluation
framework, created by repurposing a state-of-the-art language model, that
evaluates ongoing conversations for parasocial cues in real time. To test the
feasibility of this approach, we constructed a small synthetic dataset of
thirty dialogues spanning parasocial, sycophantic, and neutral conversations.
Iterative evaluation with five stage testing successfully identified all
parasocial conversations while avoiding false positives under a tolerant
unanimity rule, with detection typically occurring within the first few
exchanges. These findings provide preliminary evidence that evaluation agents
can provide a viable solution for the prevention of parasocial relations.

</details>


### [176] [Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback](https://arxiv.org/abs/2508.15757)
*Yuxing Lu,Yucheng Hu,Nan Sun,Xukai Zhao*

Main category: cs.AI

TL;DR: Language-Guided Tuning (LGT) uses a three-agent LLM framework with textual gradients to semantically guide configuration optimization, delivering interpretable improvements over traditional methods across six datasets.


<details>
  <summary>Details</summary>
Motivation: Configuration optimization in ML spans architecture, training strategy, feature engineering, and hyperparameters. Traditional approaches treat these dimensions separately and lack interpretability, while existing automated methods struggle with dynamic adaptability and semantic understanding of interdependencies.

Method: A three-agent framework where an Advisor proposes changes, an Evaluator assesses progress, and an Optimizer refines decisions, all guided by natural language reasoning and qualitative textual gradients. This creates a self-improving feedback loop. Evaluation is conducted on six diverse datasets.

Result: LGT achieves substantial improvements over traditional optimization methods and preserves high interpretability, demonstrating effective coordination among agents and meaningful semantic signals.

Conclusion: Language-Guided Tuning introduces an interpretable, semantically-guided optimization paradigm that leverages multi-agent LLMs and textual gradients to coordinate configuration search and adaptation.

Abstract: Configuration optimization remains a critical bottleneck in machine learning,
requiring coordinated tuning across model architecture, training strategy,
feature engineering, and hyperparameters. Traditional approaches treat these
dimensions independently and lack interpretability, while recent automated
methods struggle with dynamic adaptability and semantic reasoning about
optimization decisions. We introduce Language-Guided Tuning (LGT), a novel
framework that employs multi-agent Large Language Models to intelligently
optimize configurations through natural language reasoning. We apply textual
gradients - qualitative feedback signals that complement numerical optimization
by providing semantic understanding of training dynamics and configuration
interdependencies. LGT coordinates three specialized agents: an Advisor that
proposes configuration changes, an Evaluator that assesses progress, and an
Optimizer that refines the decision-making process, creating a self-improving
feedback loop. Through comprehensive evaluation on six diverse datasets, LGT
demonstrates substantial improvements over traditional optimization methods,
achieving performance gains while maintaining high interpretability.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [177] [A Vision-Based Shared-Control Teleoperation Scheme for Controlling the Robotic Arm of a Four-Legged Robot](https://arxiv.org/abs/2508.14994)
*Murilo Vinicius da Silva,Matheus Hipolito Carvalho,Juliano Negri,Thiago Segreto,Gustavo J. G. Lahr,Ricardo V. Godoy,Marcelo Becker*

Main category: cs.RO

TL;DR: A wrist-based, vision-driven teleoperation system maps human arm movements to a quadruped robot's manipulator with a collision-aware planner, validated in real-time on hardware.


<details>
  <summary>Details</summary>
Motivation: In hazardous remote settings, traditional teleoperation (e.g., joysticks) is non-intuitive, cognitively demanding, and prone to collisions. An intuitive, safe, real-time control method that integrates obstacle avoidance is needed for quadruped robots with arms in industrial, high-risk environments.

Method: Use an external camera and a machine-learning pose-estimation model to detect the operator's wrist position. Map wrist trajectories to commands for the robot's manipulator in real time. Employ a trajectory planner to detect and prevent collisions with environmental obstacles and the manipulator itself. Validate the system on a real robot to demonstrate real-time, robust performance.

Result: The approach achieved robust real-time control and reliable teleoperation on hardware, with effective obstacle and self-collision avoidance demonstrated in real-world tests.

Conclusion: The proposed wrist-based vision teleoperation offers an intuitive, safer, and cost-effective solution for controlling quadruped robots with arms in industrial settings, with potential for broader deployment and extension to more complex manipulation tasks.

Abstract: In hazardous and remote environments, robotic systems perform critical tasks
demanding improved safety and efficiency. Among these, quadruped robots with
manipulator arms offer mobility and versatility for complex operations.
However, teleoperating quadruped robots is challenging due to the lack of
integrated obstacle detection and intuitive control methods for the robotic
arm, increasing collision risks in confined or dynamically changing workspaces.
Teleoperation via joysticks or pads can be non-intuitive and demands a high
level of expertise due to its complexity, culminating in a high cognitive load
on the operator. To address this challenge, a teleoperation approach that
directly maps human arm movements to the robotic manipulator offers a simpler
and more accessible solution. This work proposes an intuitive remote control by
leveraging a vision-based pose estimation pipeline that utilizes an external
camera with a machine learning-based model to detect the operator's wrist
position. The system maps these wrist movements into robotic arm commands to
control the robot's arm in real-time. A trajectory planner ensures safe
teleoperation by detecting and preventing collisions with both obstacles and
the robotic arm itself. The system was validated on the real robot,
demonstrating robust performance in real-time control. This teleoperation
approach provides a cost-effective solution for industrial applications where
safety, precision, and ease of use are paramount, ensuring reliable and
intuitive robotic control in high-risk environments.

</details>


### [178] [GraspQP: Differentiable Optimization of Force Closure for Diverse and Robust Dexterous Grasping](https://arxiv.org/abs/2508.15002)
*René Zurbrügg,Andrei Cramariuc,Marco Hutter*

Main category: cs.RO

TL;DR: Proposes a differentiable, energy-based force-closure formulation via a QP and an MALA*-like optimization to synthesize diverse, physically feasible grasps beyond power grasps, producing a large-scale dataset (DexGraspNet-based) for 5,700 objects across 5 grippers and 3 grasp types; improves grasp diversity and stability.


<details>
  <summary>Details</summary>
Motivation: There is a need for large-scale, high-quality grasp data to train grasp prediction from point clouds, manipulation policies, and task planning. Existing dataset generation methods tend to converge on power grasps and lack diversity, limiting learning and robustness.

Method: Develop a rigorous differentiable energy formulation of force closure defined via a Quadratic Program (QP). Introduce an adjusted optimization method (MALA*) that dynamically rejects gradient steps based on energy distribution across samples to improve convergence and diversity. Use this pipeline to synthesize large-scale grasps across five grippers and three grasp types, yielding a new dataset for 5,700 objects from DexGraspNet.

Result: The approach yields significantly improved grasp diversity and stability of final grasp predictions. It enables a large-scale grasp dataset spanning multiple grippers and grasp types for many objects, demonstrated on 5,700 DexGraspNet objects with five grippers and three grasp types.

Conclusion: This method advances dexterous-hand data generation by enabling physically feasible, diverse grasps beyond power grasps and provides a ready-to-use dataset and code to facilitate research in prediction, manipulation policy learning, and planning with richer action options.

Abstract: Dexterous robotic hands enable versatile interactions due to the flexibility
and adaptability of multi-fingered designs, allowing for a wide range of
task-specific grasp configurations in diverse environments. However, to fully
exploit the capabilities of dexterous hands, access to diverse and high-quality
grasp data is essential -- whether for developing grasp prediction models from
point clouds, training manipulation policies, or supporting high-level task
planning with broader action options. Existing approaches for dataset
generation typically rely on sampling-based algorithms or simplified
force-closure analysis, which tend to converge to power grasps and often
exhibit limited diversity. In this work, we propose a method to synthesize
large-scale, diverse, and physically feasible grasps that extend beyond simple
power grasps to include refined manipulations, such as pinches and tri-finger
precision grasps. We introduce a rigorous, differentiable energy formulation of
force closure, implicitly defined through a Quadratic Program (QP).
Additionally, we present an adjusted optimization method (MALA*) that improves
performance by dynamically rejecting gradient steps based on the distribution
of energy values across all samples. We extensively evaluate our approach and
demonstrate significant improvements in both grasp diversity and the stability
of final grasp predictions. Finally, we provide a new, large-scale grasp
dataset for 5,700 objects from DexGraspNet, comprising five different grippers
and three distinct grasp types.
  Dataset and Code:https://graspqp.github.io/

</details>


### [179] [In-Context Iterative Policy Improvement for Dynamic Manipulation](https://arxiv.org/abs/2508.15021)
*Mark Van der Merwe,Devesh Jha*

Main category: cs.RO

TL;DR: In-context learning with pre-trained language models is applied to dynamic manipulation by predicting policy adjustments from previous interactions, enabling data-efficient control that outperforms baselines in low-data regimes across simulated and real-robot tasks.


<details>
  <summary>Details</summary>
Motivation: Large Language Models demonstrate few-shot learning and reasoning capabilities, but applying them to dynamic manipulation is challenging due to high dimensionality, complex dynamics, and partial observability. This work seeks to transfer in-context learning to adaptive robotic control to improve data efficiency.

Method: An iterative in-context learning setup where the model predicts adjustments to a parametric policy based on prior interactions. The approach leverages previous demonstrations/interactions as context to guide policy updates. Evaluations span several dynamic manipulation tasks in simulation and on a physical robot, with a video summary linked in the abstract.

Result: Across tasks in both simulation and real-robot settings, in-context learning outperforms alternative methods in the low data regime.

Conclusion: In-context learning with pre-trained language models offers a data-efficient pathway for dynamic manipulation, enabling rapid policy adaptation with limited task data; future work could explore scaling, robustness to observation noise, and richer forms of contextual history.

Abstract: Attention-based architectures trained on internet-scale language data have
demonstrated state of the art reasoning ability for various language-based
tasks, such as logic problems and textual reasoning. Additionally, these Large
Language Models (LLMs) have exhibited the ability to perform few-shot
prediction via in-context learning, in which input-output examples provided in
the prompt are generalized to new inputs. This ability furthermore extends
beyond standard language tasks, enabling few-shot learning for general
patterns. In this work, we consider the application of in-context learning with
pre-trained language models for dynamic manipulation. Dynamic manipulation
introduces several crucial challenges, including increased dimensionality,
complex dynamics, and partial observability. To address this, we take an
iterative approach, and formulate our in-context learning problem to predict
adjustments to a parametric policy based on previous interactions. We show
across several tasks in simulation and on a physical robot that utilizing
in-context learning outperforms alternative methods in the low data regime.
Video summary of this work and experiments can be found
https://youtu.be/2inxpdrq74U?si=dAdDYsUEr25nZvRn.

</details>


### [180] [Decentralized Vision-Based Autonomous Aerial Wildlife Monitoring](https://arxiv.org/abs/2508.15038)
*Makram Chahine,William Yang,Alaa Maalouf,Justin Siriska,Ninad Jadhav,Daniel Vogt,Stephanie Gil,Robert Wood,Daniela Rus*

Main category: cs.RO

TL;DR: Decentralized, vision-based, single-camera multi-quadrotor system for scalable wildlife monitoring with novel coordination/tracking in unstructured environments, validated in real-world field tests.


<details>
  <summary>Details</summary>
Motivation: Address limitations of herd-centric or manually operated wildlife robotics by enabling scalable, low-bandwidth, sensor-minimal monitoring with robust identification and tracking in natural habitats.

Method: Develop decentralized vision-based coordination and tracking algorithms for dynamic, unstructured environments without centralized control; use a single onboard RGB camera per drone; validate through real-world field experiments.

Result: Demonstrates reliable deployment and robust identification/tracking across diverse field conditions, indicating scalability and low-bandwidth operation.

Conclusion: Proposes a scalable wildlife monitoring framework with decentralized coordination and vision-based tracking, suitable for large-scale behavioral analysis and health/safety interventions in natural habitats.

Abstract: Wildlife field operations demand efficient parallel deployment methods to
identify and interact with specific individuals, enabling simultaneous
collective behavioral analysis, and health and safety interventions. Previous
robotics solutions approach the problem from the herd perspective, or are
manually operated and limited in scale. We propose a decentralized vision-based
multi-quadrotor system for wildlife monitoring that is scalable, low-bandwidth,
and sensor-minimal (single onboard RGB camera). Our approach enables robust
identification and tracking of large species in their natural habitat. We
develop novel vision-based coordination and tracking algorithms designed for
dynamic, unstructured environments without reliance on centralized
communication or control. We validate our system through real-world
experiments, demonstrating reliable deployment in diverse field conditions.

</details>


### [181] [Hardware Implementation of a Zero-Prior-Knowledge Approach to Lifelong Learning in Kinematic Control of Tendon-Driven Quadrupeds](https://arxiv.org/abs/2508.15160)
*Hesam Azadjou,Suraj Chakravarthi Raja,Ali Marjaninejad,Francisco J. Valero-Cuevas*

Main category: cs.RO

TL;DR: A bio-inspired General-to-Particular (G2P) learning approach enables rapid, hardware-in-the-loop learning of locomotion for a tendon-driven quadruped, using initial generalized motor babbling followed by short refinement trials to achieve functional, adaptive cyclical movements in minutes.


<details>
  <summary>Details</summary>
Motivation: Robots need fast, adaptive motor learning under incomplete body/ environment models and continuous changes. The method aims to emulate mammals' exploration-exploitation to handle redundancies and non-convex control problems in tendon-driven robots.

Method: G2P framework: 1) ~5 minutes of generalized motor babbling to capture broad motor mappings; 2) 15 refinement trials (20 seconds each) to progressively shape the control policy toward specific cyclical movements. Hardware-in-the-loop demonstration on a home-built tendon-driven quadruped with redundant joints.

Result: Proof-of-concept showing the system can learn to control a tendon-driven, redundant quadruped to perform functional, adaptive cyclical non-convex locomotion within a few minutes.

Conclusion: G2P offers a pathway toward autonomous, adaptive locomotion in robotics, enabling rapid adjustment to new environments with minimal handcrafted modeling, though future work should address scalability, robustness, and quantitative benchmarks.

Abstract: Like mammals, robots must rapidly learn to control their bodies and interact
with their environment despite incomplete knowledge of their body structure and
surroundings. They must also adapt to continuous changes in both. This work
presents a bio-inspired learning algorithm, General-to-Particular (G2P),
applied to a tendon-driven quadruped robotic system developed and fabricated
in-house. Our quadruped robot undergoes an initial five-minute phase of
generalized motor babbling, followed by 15 refinement trials (each lasting 20
seconds) to achieve specific cyclical movements. This process mirrors the
exploration-exploitation paradigm observed in mammals. With each refinement,
the robot progressively improves upon its initial "good enough" solution. Our
results serve as a proof-of-concept, demonstrating the hardware-in-the-loop
system's ability to learn the control of a tendon-driven quadruped with
redundancies in just a few minutes to achieve functional and adaptive cyclical
non-convex movements. By advancing autonomous control in robotic locomotion,
our approach paves the way for robots capable of dynamically adjusting to new
environments, ensuring sustained adaptability and performance.

</details>


### [182] [Survey of Vision-Language-Action Models for Embodied Manipulation](https://arxiv.org/abs/2508.15201)
*Haoran Li,Yuhui Chen,Wenbo Cui,Weiheng Liu,Kai Liu,Mingcai Zhou,Zhengtao Zhang,Dongbin Zhao*

Main category: cs.RO

TL;DR: Survey of Vision-Language-Action (VLA) models for embodied manipulation; covers architecture evolution, datasets, pre-training/post-training strategies, evaluation, and future challenges.


<details>
  <summary>Details</summary>
Motivation: Embodied AI seeks robust, flexible control by integrating perception, language, and action; VLA models offer a unified framework to improve agent-environment interaction.

Method: Literature survey across five dimensions: model structures, training datasets, pre-training methods, post-training methods, and evaluation; analyzes historical trends and current research; synthesizes challenges and future directions.

Result: Provides a comprehensive taxonomy and critical synthesis; identifies gaps and needs for standardized evaluation, data and computation efficiency, real-world deployment considerations.

Conclusion: VLA models hold promise for embodied manipulation; realization requires addressing data efficiency, alignment, safety, generalization, and deployment scalability; outlines directions for future work.

Abstract: Embodied intelligence systems, which enhance agent capabilities through
continuous environment interactions, have garnered significant attention from
both academia and industry. Vision-Language-Action models, inspired by
advancements in large foundation models, serve as universal robotic control
frameworks that substantially improve agent-environment interaction
capabilities in embodied intelligence systems. This expansion has broadened
application scenarios for embodied AI robots. This survey comprehensively
reviews VLA models for embodied manipulation. Firstly, it chronicles the
developmental trajectory of VLA architectures. Subsequently, we conduct a
detailed analysis of current research across 5 critical dimensions: VLA model
structures, training datasets, pre-training methods, post-training methods, and
model evaluation. Finally, we synthesize key challenges in VLA development and
real-world deployment, while outlining promising future research directions.

</details>


### [183] [Mag-Match: Magnetic Vector Field Features for Map Matching and Registration](https://arxiv.org/abs/2508.15300)
*William McDonald,Cedric Le Gentil,Jennifer Wakulicz,Teresa Vidal-Calleja*

Main category: cs.RO

TL;DR: Mag-Match introduces a magnetometer-based map matching/descriptors for 3D magnetic vector field maps, using a physics-informed Gaussian Process to compute higher-order derivatives, yielding gravity-orientation-invariant registration without gravity alignment.


<details>
  <summary>Details</summary>
Motivation: Robust map registration in smoke/dust environments where visual/Geometric sensors fail. Magnetometers capture features invisible to cameras/LiDAR and enable gravity-invariant, robust localization and multi-robot data fusion.

Method: Extract and describe features from 3D magnetic vector field maps using higher-order derivatives. Use a physics-informed Gaussian Process to infer the magnetic field and its derivatives across the map, enabling efficient, recursive probabilistic inference. Derive a gravity-invariant descriptor that does not require gravity alignment. Perform map-wide derivative estimation from point-wise magnetometer data.

Result: Evaluated on simulated and real-world data against a SIFT-based baseline. Demonstrates accurate map-to-map, robot-to-map, and robot-to-robot transformations without initial gravity alignment, showing robustness in challenging environments.

Conclusion: Mag-Match demonstrates effective magnetometer-based map registration with gravity-invariant descriptors and GP-based derivative inference, enabling robust localization and data fusion in environments where vision-based methods struggle.

Abstract: Map matching and registration are essential tasks in robotics for
localisation and integration of multi-session or multi-robot data. Traditional
methods rely on cameras or LiDARs to capture visual or geometric information
but struggle in challenging conditions like smoke or dust. Magnetometers, on
the other hand, detect magnetic fields, revealing features invisible to other
sensors and remaining robust in such environments. In this paper, we introduce
Mag-Match, a novel method for extracting and describing features in 3D magnetic
vector field maps to register different maps of the same area. Our feature
descriptor, based on higher-order derivatives of magnetic field maps, is
invariant to global orientation, eliminating the need for gravity-aligned
mapping. To obtain these higher-order derivatives map-wide given point-wise
magnetometer data, we leverage a physics-informed Gaussian Process to perform
efficient and recursive probabilistic inference of both the magnetic field and
its derivatives. We evaluate Mag-Match in simulated and real-world experiments
against a SIFT-based approach, demonstrating accurate map-to-map, robot-to-map,
and robot-to-robot transformations - even without initial gravitational
alignment.

</details>


### [184] [Sensing, Social, and Motion Intelligence in Embodied Navigation: A Comprehensive Survey](https://arxiv.org/abs/2508.15354)
*Chaoran Xiong,Yulong Huang,Fangwen Yu,Changhao Chen,Yue Wang,Songpengchen Xia,Ling Pei*

Main category: cs.RO

TL;DR: A survey of Embodied Navigation (EN) framed by the TOFRA five-stage model, organizing current work, platforms, and metrics, and pointing to a GitHub resource; highlights open challenges.


<details>
  <summary>Details</summary>
Motivation: EN enables robots to perform egocentric tasks using sensing, social, and motion intelligence, addressing limitations of traditional localization and pre-defined maps.

Method: Proposes TOFRA (Transition, Observation, Fusion, Reward-policy construction, Action) as a unifying formulation; conducts a critical review of the literature, platforms, and evaluation metrics; synthesizes findings and identifies gaps; provides a compiled study list at GitHub (https://github.com/Franky-X/Awesome-Embodied-Navigation).

Result: Provides a structured synthesis of the EN state-of-the-art, including platforms and evaluation metrics; identifies key challenges and directions for future work; offers a centralized resource for researchers.

Conclusion: TOFRA offers a coherent framework to structure and advance Embodied Navigation; the field is evolving with open research challenges; the linked GitHub resource serves as a comprehensive reference for researchers.

Abstract: Embodied navigation (EN) advances traditional navigation by enabling robots
to perform complex egocentric tasks through sensing, social, and motion
intelligence. In contrast to classic methodologies that rely on explicit
localization and pre-defined maps, EN leverages egocentric perception and
human-like interaction strategies. This survey introduces a comprehensive EN
formulation structured into five stages: Transition, Observation, Fusion,
Reward-policy construction, and Action (TOFRA). The TOFRA framework serves to
synthesize the current state of the art, provide a critical review of relevant
platforms and evaluation metrics, and identify critical open research
challenges. A list of studies is available at
https://github.com/Franky-X/Awesome-Embodied-Navigation.

</details>


### [185] [Lang2Lift: A Framework for Language-Guided Pallet Detection and Pose Estimation Integrated in Autonomous Outdoor Forklift Operation](https://arxiv.org/abs/2508.15427)
*Huy Hoang Nguyen,Johannes Huemer,Markus Murschitz,Tobias Glueck,Minh Nhat Vu,Andreas Kugi*

Main category: cs.RO

TL;DR: Lang2Lift introduces a language-guided perception and 6D pose estimation pipeline for autonomous pallet pickup in outdoor settings, integrating foundation models (Florence-2, SAM-2) with FoundationPose, enabling natural-language command execution and autonomous forklift operation; achieves 0.76 mIoU pallet segmentation on real data and shows feasibility for deployment.


<details>
  <summary>Details</summary>
Motivation: Address labor shortages, safety concerns, and inefficiencies in manually locating and retrieving pallets in outdoor, unstructured environments with variable payloads and pallet quality/dimensions.

Method: A perception pipeline that uses language-grounded segmentation (Florence-2 + SAM-2) and a robust 6D pose estimator (FoundationPose) to detect and localize pallets described by natural language commands (e.g., “pick up the steel beam pallet near the crane”), followed by integration into a motion planning module for autonomous forklift control. Validation on the ADAPT autonomous forklift platform with real-world test data and timing/error analyses.

Result: Achieves 0.76 mIoU pallet segmentation accuracy on real-world data; robust pose estimation in cluttered outdoor scenes under variable lighting; timing and error analyses indicate robustness and deployment feasibility; video demonstrations available.

Conclusion: Lang2Lift demonstrates the feasibility of natural-language-guided pallet pickup in practical logistics/construction environments by bridging language-grounded perception with reliable 6D pose estimation and autonomous motion planning, suggesting potential for deployment and further real-world validation.

Abstract: The logistics and construction industries face persistent challenges in
automating pallet handling, especially in outdoor environments with variable
payloads, inconsistencies in pallet quality and dimensions, and unstructured
surroundings. In this paper, we tackle automation of a critical step in pallet
transport: the pallet pick-up operation. Our work is motivated by labor
shortages, safety concerns, and inefficiencies in manually locating and
retrieving pallets under such conditions. We present Lang2Lift, a framework
that leverages foundation models for natural language-guided pallet detection
and 6D pose estimation, enabling operators to specify targets through intuitive
commands such as "pick up the steel beam pallet near the crane." The perception
pipeline integrates Florence-2 and SAM-2 for language-grounded segmentation
with FoundationPose for robust pose estimation in cluttered, multi-pallet
outdoor scenes under variable lighting. The resulting poses feed into a motion
planning module for fully autonomous forklift operation. We validate Lang2Lift
on the ADAPT autonomous forklift platform, achieving 0.76 mIoU pallet
segmentation accuracy on a real-world test dataset. Timing and error analysis
demonstrate the system's robustness and confirm its feasibility for deployment
in operational logistics and construction environments. Video demonstrations
are available at https://eric-nguyen1402.github.io/lang2lift.github.io/

</details>


### [186] [LLM-Driven Self-Refinement for Embodied Drone Task Planning](https://arxiv.org/abs/2508.15501)
*Deyu Zhang,Xicheng Zhang,Jiahao Li,Tingting Long,Xunhua Dai,Yongjian Fu,Jinrui Zhang,Ju Ren,Yaoxue Zhang*

Main category: cs.RO

TL;DR: SRDrone introduces self-refinement task planning for industrial drones via continuous state evaluation and a hierarchical BT modification model to enable reflective learning, achieving substantial SR gains in simulation and real-world deployment.


<details>
  <summary>Details</summary>
Motivation: Motivated by the inadequacy of single-frame final-state checks for continuous, dynamic drone operations and the need to combine general reasoning (LLMs) with stringent physical constraints in embodied drones.

Method: 1) A continuous state evaluation method to determine task outcomes with explanatory feedback; 2) A hierarchical Behavior Tree (BT) modification model that analyzes BT plans at multiple levels within a constrained strategy space to support reflective learning from experience.

Result: SRDrone yields a 44.87% improvement in Success Rate (SR) over baselines; real-world deployment with an improved experience base reaches 96.25% SR.

Conclusion: Embedding adaptive task refinement in an industrial BT planning framework enables integrating LLM-level reasoning with physical execution constraints, showing strong performance gains; code is available at the provided GitHub link.

Abstract: We introduce SRDrone, a novel system designed for self-refinement task
planning in industrial-grade embodied drones. SRDrone incorporates two key
technical contributions: First, it employs a continuous state evaluation
methodology to robustly and accurately determine task outcomes and provide
explanatory feedback. This approach supersedes conventional reliance on
single-frame final-state assessment for continuous, dynamic drone operations.
Second, SRDrone implements a hierarchical Behavior Tree (BT) modification
model. This model integrates multi-level BT plan analysis with a constrained
strategy space to enable structured reflective learning from experience.
Experimental results demonstrate that SRDrone achieves a 44.87% improvement in
Success Rate (SR) over baseline methods. Furthermore, real-world deployment
utilizing an experience base optimized through iterative self-refinement
attains a 96.25% SR. By embedding adaptive task refinement capabilities within
an industrial-grade BT planning framework, SRDrone effectively integrates the
general reasoning intelligence of Large Language Models (LLMs) with the
stringent physical execution constraints inherent to embodied drones. Code is
available at https://github.com/ZXiiiC/SRDrone.

</details>


### [187] [Mind and Motion Aligned: A Joint Evaluation IsaacSim Benchmark for Task Planning and Low-Level Policies in Mobile Manipulation](https://arxiv.org/abs/2508.15663)
*Nikita Kachaev,Andrei Spiridonov,Andrey Gorodetsky,Kirill Muravyev,Nikita Oskolkov,Aditya Narendra,Vlad Shakhuro,Dmitry Makarov,Aleksandr I. Panov,Polina Fedotova,Alexey K. Kovalev*

Main category: cs.RO

TL;DR: Kitchen-R unifies task planning and low-level control benchmarking in a simulated kitchen, offering 500+ language instructions, three evaluation modes, and baselines for planning and control.


<details>
  <summary>Details</summary>
Motivation: Bridges the gap between benchmarks that assume perfect low-level execution (high-level language tasks) and those with simple one-step control, enabling holistic evaluation of integrated planning-and-execution systems in embodied AI.

Method: A digital twin benchmark in Isaac Sim featuring a mobile manipulator and a simulated kitchen. It includes 500+ complex language instructions, a task-planning baseline using a vision-language model, a low-level control baseline based on a diffusion policy, and a trajectory-collection system. It supports three evaluation modes: planning-only, control-only, and integrated evaluation of the whole system.

Result: Provides a flexible framework and baseline methods for independent and integrated evaluation, along with a trajectory collection system, to enable holistic benchmarking of language-guided robotic agents in a realistic kitchen domain.

Conclusion: Kitchen-R fills a key gap by enabling holistic benchmarking of language-guided robotic agents, uniting task planning and low-level control in a realistic simulated environment.

Abstract: Benchmarks are crucial for evaluating progress in robotics and embodied AI.
However, a significant gap exists between benchmarks designed for high-level
language instruction following, which often assume perfect low-level execution,
and those for low-level robot control, which rely on simple, one-step commands.
This disconnect prevents a comprehensive evaluation of integrated systems where
both task planning and physical execution are critical. To address this, we
propose Kitchen-R, a novel benchmark that unifies the evaluation of task
planning and low-level control within a simulated kitchen environment. Built as
a digital twin using the Isaac Sim simulator and featuring more than 500
complex language instructions, Kitchen-R supports a mobile manipulator robot.
We provide baseline methods for our benchmark, including a task-planning
strategy based on a vision-language model and a low-level control policy based
on diffusion policy. We also provide a trajectory collection system. Our
benchmark offers a flexible framework for three evaluation modes: independent
assessment of the planning module, independent assessment of the control
policy, and, crucially, an integrated evaluation of the whole system. Kitchen-R
bridges a key gap in embodied AI research, enabling more holistic and realistic
benchmarking of language-guided robotic agents.

</details>


### [188] [Exploiting Policy Idling for Dexterous Manipulation](https://arxiv.org/abs/2508.15669)
*Annie S. Chen,Philemon Brakel,Antonia Bronars,Annie Xie,Sandy Huang,Oliver Groth,Maria Bauza,Markus Wulfmeier,Nicolas Heess,Dushyant Rao*

Main category: cs.RO

TL;DR: A simple, training-free perturbation technique called Pause-Induced Perturbations (PIP) detects when policies idle and perturbs them to break out of stuck basins, improving performance in both simulated and real-world dexterous manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Policy idling is a common failure pattern in learning-based dexterous manipulation, stemming from biased training data and insufficient exploration of high-precision regions; improving robustness without heavy supervision is desirable.

Method: Detect idling states (regions where the policy's actions are near zero) and apply perturbations at those states to push the policy into more informative regions. Evaluate on challenging simulated dual-arm tasks and a real-world multi-finger insertion task, comparing test-time performance and iterative policy improvement without additional supervision or training changes.

Result: Notable improvements in test-time performance across simulated tasks. In a real-world insertion task requiring complex multi-finger manipulation, the approach yielded a 15–35% absolute improvement in success rate. Perturbations also facilitated better iterative policy improvement compared to prior methods, with no extra supervision required.

Conclusion: Leveraging idling detection to trigger perturbations is an effective, lightweight strategy to enhance exploration and policy improvement in dexterous manipulation, compatible with existing training pipelines and without additional supervision.

Abstract: Learning-based methods for dexterous manipulation have made notable progress
in recent years. However, learned policies often still lack reliability and
exhibit limited robustness to important factors of variation. One failure
pattern that can be observed across many settings is that policies idle, i.e.
they cease to move beyond a small region of states when they reach certain
states. This policy idling is often a reflection of the training data. For
instance, it can occur when the data contains small actions in areas where the
robot needs to perform high-precision motions, e.g., when preparing to grasp an
object or object insertion. Prior works have tried to mitigate this phenomenon
e.g. by filtering the training data or modifying the control frequency.
However, these approaches can negatively impact policy performance in other
ways. As an alternative, we investigate how to leverage the detectability of
idling behavior to inform exploration and policy improvement. Our approach,
Pause-Induced Perturbations (PIP), applies perturbations at detected idling
states, thus helping it to escape problematic basins of attraction. On a range
of challenging simulated dual-arm tasks, we find that this simple approach can
already noticeably improve test-time performance, with no additional
supervision or training. Furthermore, since the robot tends to idle at critical
points in a movement, we also find that learning from the resulting episodes
leads to better iterative policy improvement compared to prior approaches. Our
perturbation strategy also leads to a 15-35% improvement in absolute success
rate on a real-world insertion task that requires complex multi-finger
manipulation.

</details>


### [189] [Understanding and Utilizing Dynamic Coupling in Free-Floating Space Manipulators for On-Orbit Servicing](https://arxiv.org/abs/2508.15732)
*Gargi Das,Daegyun Choi,Donghoon Kim*

Main category: cs.RO

TL;DR: Dynamic-coupling-aware trajectory optimization for free-floating space manipulators using SVD to extract dominant coupling modes and a coupling metric integrated into optimization; a sliding-mode controller ensures feasible torque tracking; simulations show potential efficiency gains.


<details>
  <summary>Details</summary>
Motivation: Dynamic coupling between base and manipulators strongly shapes system behavior; previous work minimized coupling rather than leveraging it; exploiting coupling could improve trajectory efficiency and performance.

Method: Compute dynamic coupling matrix, apply singular value decomposition to identify principal coupling directions, define a coupling strength/directionality metric, incorporate into a trajectory optimization objective/constraints, design a sliding-mode tracking controller to produce joint torques for feasibility.

Result: Simulations demonstrate that including the coupling metric in planning yields more informed and potentially more efficient trajectories; methodology offers new directions for free-floating SMS control.

Conclusion: Dynamic coupling is not merely a disturbance but a resource to be exploited; the proposed framework provides a quantitative tool and control approach to harness coupling for better trajectory planning in free-floating space manipulators.

Abstract: This study proposes a dynamic coupling-informed trajectory optimization
algorithm for free-floating space manipulator systems (SMSs). Dynamic coupling
between the base and the manipulator arms plays a critical role in influencing
the system's behavior. While prior research has predominantly focused on
minimizing this coupling, often overlooking its potential advantages, this work
investigates how dynamic coupling can instead be leveraged to improve
trajectory planning. Singular value decomposition (SVD) of the dynamic coupling
matrix is employed to identify the dominant components governing coupling
behavior. A quantitative metric is then formulated to characterize the strength
and directionality of the coupling and is incorporated into a trajectory
optimization framework. To assess the feasibility of the optimized trajectory,
a sliding mode control-based tracking controller is designed to generate the
required joint torque inputs. Simulation results demonstrate that explicitly
accounting for dynamic coupling in trajectory planning enables more informed
and potentially more efficient operation, offering new directions for the
control of free-floating SMSs.

</details>


### [190] [Neural Robot Dynamics](https://arxiv.org/abs/2508.15755)
*Jie Xu,Eric Heiden,Iretiayo Akinola,Dieter Fox,Miles Macklin,Yashraj Narang*

Main category: cs.RO

TL;DR: NeRD is a neural dynamics backend for articulated robots that replaces low-level solvers, enabling generalizable, trainable dynamics and sim-to-real adaptation within a physics simulator.


<details>
  <summary>Details</summary>
Motivation: Accurate and efficient robotic simulation is difficult due to high degrees of freedom and intricate mechanisms. Existing neural simulators often require task-specific training and struggle to generalize because of inadequate global state representations; there is a need for robot-centric, generalizable models that can adapt across tasks and environments.

Method: Introduce NeRD (Neural Robot Dynamics), a learned dynamics model for articulated rigid bodies under contact constraints. It replaces the low-level dynamics and contact solvers in an analytical simulator and uses a robot-centric, spatially-invariant representation of the simulation state. NeRD is integrated as an interchangeable backend solver within a state-of-the-art robotics simulator.

Result: Empirical evaluation shows NeRD is stable and accurate over 1,000 simulation steps, generalizes across tasks and environment configurations, enables policy learning entirely within the neural engine, and can be fine-tuned from real-world data to bridge the sim-to-real gap.

Conclusion: NeRD demonstrates a viable, generalizable neural backend for robot dynamics that can be tuned with real data and integrated with existing simulators to improve generalization and sim-to-real transfer.

Abstract: Accurate and efficient simulation of modern robots remains challenging due to
their high degrees of freedom and intricate mechanisms. Neural simulators have
emerged as a promising alternative to traditional analytical simulators,
capable of efficiently predicting complex dynamics and adapting to real-world
data; however, existing neural simulators typically require
application-specific training and fail to generalize to novel tasks and/or
environments, primarily due to inadequate representations of the global state.
In this work, we address the problem of learning generalizable neural
simulators for robots that are structured as articulated rigid bodies. We
propose NeRD (Neural Robot Dynamics), learned robot-specific dynamics models
for predicting future states for articulated rigid bodies under contact
constraints. NeRD uniquely replaces the low-level dynamics and contact solvers
in an analytical simulator and employs a robot-centric and spatially-invariant
simulation state representation. We integrate the learned NeRD models as an
interchangeable backend solver within a state-of-the-art robotics simulator. We
conduct extensive experiments to show that the NeRD simulators are stable and
accurate over a thousand simulation steps; generalize across tasks and
environment configurations; enable policy learning exclusively in a neural
engine; and, unlike most classical simulators, can be fine-tuned from
real-world data to bridge the gap between simulation and reality.

</details>
