<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 67]
- [cs.AI](#cs.AI) [Total: 8]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.RO](#cs.RO) [Total: 26]
- [cs.LG](#cs.LG) [Total: 73]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs](https://arxiv.org/abs/2511.04727)
*Ali Faraz,Akash,Shaharukh Khan,Raja Kolla,Akshat Patidar,Suranjan Goswami,Abhinav Ravi,Chandra Khatri,Shubham Agarwal*

Main category: cs.CV

TL;DR: Intro of IndicVisionBench, a large-scale, multilingual, culturally grounded VLM benchmark for India, spanning OCR, MMT, and VQA across English + 10 Indic languages; ~5K images and 37K+ QA pairs over 13 topics; assesses 8 models and uncovers substantial multilingual/cultural gaps, with a parallel Indic-language corpus for bias analysis.


<details>
  <summary>Details</summary>
Motivation: Western-centric benchmarks inadequately assess VLMs in multilingual and culturally diverse settings; need a benchmark covering Indian languages, scripts, and culturally grounded content to evaluate and mitigate biases.

Method: Construct a large-scale benchmark: 3 multimodal tasks (OCR, MMT, VQA) across English + 10 Indic languages, 13 culturally grounded topics, ~5K images and 37K+ QA pairs; provide a parallel corpus of annotations in 10 Indic languages; evaluate 8 models (closed-source and open-weight).

Result: The study reveals substantial performance gaps for current VLMs in multilingual and culturally diverse contexts; models lag on Indic languages and culturally grounded content, indicating biases and limitations; provides a reproducible evaluation framework.

Conclusion: IndicVisionBench enables inclusive, reproducible multimodal evaluation in multilingual/culturally diverse settings and should drive more inclusive research and bias analysis in VLMs.

Abstract: Vision-language models (VLMs) have demonstrated impressive generalization
across multimodal tasks, yet most evaluation benchmarks remain Western-centric,
leaving open questions about their performance in culturally diverse and
multilingual settings. To address this gap, we introduce IndicVisionBench, the
first large-scale benchmark centered on the Indian subcontinent. Covering
English and 10 Indian languages, our benchmark spans 3 multimodal tasks,
including Optical Character Recognition (OCR), Multimodal Machine Translation
(MMT), and Visual Question Answering (VQA), covering 6 kinds of question types.
Our final benchmark consists of a total of ~5K images and 37K+ QA pairs across
13 culturally grounded topics. In addition, we release a paired parallel corpus
of annotations across 10 Indic languages, creating a unique resource for
analyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum
of 8 models, from proprietary closed-source systems to open-weights medium and
large-scale models. Our experiments reveal substantial performance gaps,
underscoring the limitations of current VLMs in culturally diverse contexts. By
centering cultural diversity and multilinguality, IndicVisionBench establishes
a reproducible evaluation framework that paves the way for more inclusive
multimodal research.

</details>


### [2] [Knowledge-based anomaly detection for identifying network-induced shape artifacts](https://arxiv.org/abs/2511.04729)
*Rucha Deshpande,Tahsin Rahman,Miguel Lago,Adarsh Subbaswamy,Jana G. Delfino,Ghada Zamzmi,Elim Thompson,Aldo Badano,Seyed Kahaki*

Main category: cs.CV

TL;DR: Two-stage knowledge-based anomaly detector for synthetic mammography detects network-induced shape artifacts; uses a gradient-angle feature space and an isolation forest; validated on CSAW-M-syn and VMLO-syn with high AUC and reasonable human agreement.


<details>
  <summary>Details</summary>
Motivation: Ensure quality and clinical utility of synthetic data by identifying artifacts that can distort model performance.

Method: Stage 1: develop a feature extractor that builds a specialized feature space by analyzing per-image distributions of angle gradients along anatomical boundaries. Stage 2: apply an isolation forest anomaly detector. Evaluated on two synthetic mammography datasets derived from CSAW-M and VinDr-Mammo; includes a reader study with imaging scientists.

Result: Artifacts concentrated in the most anomalous 1st percentile; AUCs 0.97 (CSAW-syn) and 0.91 (VMLO-syn). Reader study: mean agreement 66% and 68% with Kendall-Tau 0.45 and 0.43, indicating reasonable agreement.

Conclusion: The approach supports responsible use of synthetic data by identifying and pinpointing shape artifact issues tied to anatomical constraints, guiding improvements to synthetic datasets.

Abstract: Synthetic data provides a promising approach to address data scarcity for
training machine learning models; however, adoption without proper quality
assessments may introduce artifacts, distortions, and unrealistic features that
compromise model performance and clinical utility. This work introduces a novel
knowledge-based anomaly detection method for detecting network-induced shape
artifacts in synthetic images. The introduced method utilizes a two-stage
framework comprising (i) a novel feature extractor that constructs a
specialized feature space by analyzing the per-image distribution of angle
gradients along anatomical boundaries, and (ii) an isolation forest-based
anomaly detector. We demonstrate the effectiveness of the method for
identifying network-induced shape artifacts in two synthetic mammography
datasets from models trained on CSAW-M and VinDr-Mammo patient datasets
respectively. Quantitative evaluation shows that the method successfully
concentrates artifacts in the most anomalous partition (1st percentile), with
AUC values of 0.97 (CSAW-syn) and 0.91 (VMLO-syn). In addition, a reader study
involving three imaging scientists confirmed that images identified by the
method as containing network-induced shape artifacts were also flagged by human
readers with mean agreement rates of 66% (CSAW-syn) and 68% (VMLO-syn) for the
most anomalous partition, approximately 1.5-2 times higher than the least
anomalous partition. Kendall-Tau correlations between algorithmic and human
rankings were 0.45 and 0.43 for the two datasets, indicating reasonable
agreement despite the challenging nature of subtle artifact detection. This
method is a step forward in the responsible use of synthetic data, as it allows
developers to evaluate synthetic images for known anatomic constraints and
pinpoint and address specific issues to improve the overall quality of a
synthetic dataset.

</details>


### [3] [CPO: Condition Preference Optimization for Controllable Image Generation](https://arxiv.org/abs/2511.04753)
*Zonglin Lyu,Ming Li,Xinxin Liu,Chen Chen*

Main category: cs.CV

TL;DR: Introduces Condition Preference Optimization (CPO) to boost controllability in text-to-image generation by learning preferences over control signals (c^w vs c^l) rather than generated images, yielding lower-variance training and better results than DPO, with reduced compute and data needs.


<details>
  <summary>Details</summary>
Motivation: Improve controllability in diffusion-based text-to-image models (e.g., ControlNet++) while avoiding high computational cost and confounding factors that tie controllability to image quality. Prior approaches either rely on low-noise timestep approximations or optimize over images, which can degrade training signal and add variance.

Method: Propose Condition Preference Optimization (CPO): construct winning and losing control signals (c^w, c^l) and train the model to prefer c^w. This eliminates confounding factors, yields a lower-variance contrastive loss than Direct Preference Optimization (DPO), and reduces computation/storage for dataset curation.

Result: Empirically, CPO significantly improves controllability over ControlNet++ across multiple control types, achieving over 10% error rate reduction in segmentation, 70–80% in human pose, and 2–5% reductions in edge and depth maps.

Conclusion: CPO provides a more reliable and efficient means to tune controllability in diffusion-based T2I models, outperforming DPO and ControlNet++ in both variance and practical performance while reducing data preparation costs.

Abstract: To enhance controllability in text-to-image generation, ControlNet introduces
image-based control signals, while ControlNet++ improves pixel-level cycle
consistency between generated images and the input control signal. To avoid the
prohibitive cost of back-propagating through the sampling process, ControlNet++
optimizes only low-noise timesteps (e.g., $t < 200$) using a single-step
approximation, which not only ignores the contribution of high-noise timesteps
but also introduces additional approximation errors. A straightforward
alternative for optimizing controllability across all timesteps is Direct
Preference Optimization (DPO), a fine-tuning method that increases model
preference for more controllable images ($I^{w}$) over less controllable ones
($I^{l}$). However, due to uncertainty in generative models, it is difficult to
ensure that win--lose image pairs differ only in controllability while keeping
other factors, such as image quality, fixed. To address this, we propose
performing preference learning over control conditions rather than generated
images. Specifically, we construct winning and losing control signals,
$\mathbf{c}^{w}$ and $\mathbf{c}^{l}$, and train the model to prefer
$\mathbf{c}^{w}$. This method, which we term \textit{Condition Preference
Optimization} (CPO), eliminates confounding factors and yields a low-variance
training objective. Our approach theoretically exhibits lower contrastive loss
variance than DPO and empirically achieves superior results. Moreover, CPO
requires less computation and storage for dataset curation. Extensive
experiments show that CPO significantly improves controllability over the
state-of-the-art ControlNet++ across multiple control types: over $10\%$ error
rate reduction in segmentation, $70$--$80\%$ in human pose, and consistent
$2$--$5\%$ reductions in edge and depth maps.

</details>


### [4] [DARN: Dynamic Adaptive Regularization Networks for Efficient and Robust Foundation Model Adaptation](https://arxiv.org/abs/2511.04766)
*Dhenenjay Yadav,Rohan Sawai*

Main category: cs.CV

TL;DR: Dynamic Adaptive Regularization Networks (DARN) tailor decoders to per-sample complexity in geospatial FM adaptation, yielding state-of-the-art performance and improved robustness across both full fine-tuning and frozen-backbone settings.


<details>
  <summary>Details</summary>
Motivation: Standard adaptation methods with fixed regularization fail to account for the heavy heterogeneity in satellite imagery; there is a need for per-sample adaptive regularization to improve generalization and efficiency in foundation-model-based geospatial analysis.

Method: DARN integrates three components: (1) Task Complexity Predictor (TCP) that estimates per-sample difficulty, (2) Adaptive Dropout Modulation (ADM) that adjusts dropout rates (0.1 to 0.5) based on predicted complexity, and (3) Dynamic Capacity Gating (DCG) that modulates channel activation. The paper provides theoretical justifications linking DARN to stationary-point convergence and adaptive information bottlenecks.

Result: Empirically, DARN achieves state-of-the-art performance across adaptation paradigms: in full fine-tuning (unfrozen backbone) it sets new SOTA on GeoBench with 86.66% mIoU (+5.56 percentage points). In efficient adaptation (frozen backbone), it reaches SOTA-competitive 90.5% mIoU on Sen1Floods11, with substantial advantages: +9.5 pp mIoU in out-of-distribution generalization on AI4SmallFarms, 17% relative reduction in corruption error, and improved performance on minority classes.

Conclusion: DARN offers a more intelligent, robust, and deployment-friendly approach to leveraging foundation models for critical geospatial applications by integrating adaptive regularization mechanisms that respond to per-sample complexity.

Abstract: Foundation models (FMs) offer powerful representations for geospatial
analysis, but adapting them effectively remains challenging. Standard
adaptation methods, whether full fine-tuning or efficient frozen-backbone
approaches, typically employ decoders with fixed regularization strategies,
failing to account for the significant heterogeneity in satellite imagery. We
introduce Dynamic Adaptive Regularization Networks (DARN), a novel decoder
architecture designed to address this limitation. DARN integrates three key
innovations: (1) a lightweight Task Complexity Predictor (TCP) that estimates
per-sample difficulty, (2) Adaptive Dropout Modulation (ADM), dynamically
adjusting dropout rates (from 0.1 to 0.5) based on predicted complexity, and
(3) Dynamic Capacity Gating (DCG) that modulates channel activation. We provide
theoretical justifications linking DARN's optimization to stationary point
convergence and its mechanism to adaptive information bottlenecks. Empirically,
DARN demonstrates exceptional performance across both major adaptation
paradigms. In full fine-tuning (unfrozen backbone), DARN achieves a new
state-of-the-art on the multi-task GeoBench benchmark (86.66% mIoU, +5.56 pp
over prior SOTA). In efficient adaptation (frozen backbone), DARN achieves
SOTA-competitive accuracy (90.5% mIoU on Sen1Floods11) while delivering
substantial advantages crucial for real-world deployment: superior
out-of-distribution (OOD) generalization (+9.5 pp mIoU on AI4SmallFarms),
enhanced robustness (17% relative reduction in corruption error), and improved
performance on minority classes. DARN offers a more intelligent, robust, and
efficient approach to leveraging FMs in critical geospatial applications.

</details>


### [5] [Global 3D Reconstruction of Clouds & Tropical Cyclones](https://arxiv.org/abs/2511.04773)
*Shirin Ermis,Cesar Aybar,Lilli Freischem,Stella Girtsou,Kyriaki-Margarita Bintsi,Emiliano Diaz Salas-Porras,Michael Eisinger,William Jones,Anna Jungbluth,Benoit Tremblay*

Main category: cs.CV

TL;DR: A cross-satellite 2D-to-3D cloud reconstruction framework using a pre-training–fine-tuning pipeline to produce global instantaneous 3D cloud maps of tropical cyclones, enabling accurate reconstruction of intense storms and recovery of data when observations are missing.


<details>
  <summary>Details</summary>
Motivation: Accurate TC forecasts are hindered by sparse, incomplete satellite information about TC 3D structure and cloud processes; existing 3D reconstruction methods are limited to rare regions and poorly validated for strong hurricanes.

Method: Train a model on multi-satellite 2D imagery to infer 3D cloud properties; pre-train on globally sourced data, then fine-tune on a custom-built tropical cyclone dataset; produce global instantaneous 3D cloud maps and perform reconstructions even when observations are missing.

Result: First demonstration of global instantaneous 3D cloud maps for TCs; accurate reconstruction of intense storm structures; extended observational coverage and robustness to missing observations.

Conclusion: This framework advances TC science by enabling better understanding of intensification mechanisms and improving forecasts through 3D cloud reconstructions across the globe, including data-sparse situations.

Abstract: Accurate forecasting of tropical cyclones (TCs) remains challenging due to
limited satellite observations probing TC structure and difficulties in
resolving cloud properties involved in TC intensification. Recent research has
demonstrated the capabilities of machine learning methods for 3D cloud
reconstruction from satellite observations. However, existing approaches have
been restricted to regions where TCs are uncommon, and are poorly validated for
intense storms. We introduce a new framework, based on a
pre-training--fine-tuning pipeline, that learns from multiple satellites with
global coverage to translate 2D satellite imagery into 3D cloud maps of
relevant cloud properties. We apply our model to a custom-built TC dataset to
evaluate performance in the most challenging and relevant conditions. We show
that we can - for the first time - create global instantaneous 3D cloud maps
and accurately reconstruct the 3D structure of intense storms. Our model not
only extends available satellite observations but also provides estimates when
observations are missing entirely. This is crucial for advancing our
understanding of TC intensification and improving forecasts.

</details>


### [6] [EETnet: a CNN for Gaze Detection and Tracking for Smart-Eyewear](https://arxiv.org/abs/2511.04779)
*Andrea Aspesi,Andrea Simpsi,Aaron Tognoli,Simone Mentasti,Luca Merigo,Matteo Matteucci*

Main category: cs.CV

TL;DR: Proposes EETnet, a CNN for eye tracking from event-based data that runs on microcontrollers with limited resources, featuring two architectures: a grid-based pupil classification model and a pixel-level regression model; includes training/evaluation/quantization workflow on a public dataset.


<details>
  <summary>Details</summary>
Motivation: Enable embedded deployment of event-based eye tracking, addressing the gap where prior work is validated mainly on GPUs and not on resource-constrained devices.

Method: Design of a convolutional neural network (EETnet) tailored for sparse, asynchronous event data; development of two variants—(1) grid-based classification to locate the pupil on a superimposed grid, and (2) pixel-level regression for precise localization; outline of a training, evaluation, and quantization pipeline using a public dataset.

Result: Demonstrates feasibility of running on microcontrollers with limited resources; provides a practical workflow for training, evaluating, and quantizing the network on public data; presents two architectural options for pupil localization from event data.

Conclusion: EETnet enables embedded, low-power eye tracking from event data and offers both a grid-based classifier and a pixel-level regression model, along with a complete methodology for training and deployment on public datasets.

Abstract: Event-based cameras are becoming a popular solution for efficient, low-power
eye tracking. Due to the sparse and asynchronous nature of event data, they
require less processing power and offer latencies in the microsecond range.
However, many existing solutions are limited to validation on powerful GPUs,
with no deployment on real embedded devices. In this paper, we present EETnet,
a convolutional neural network designed for eye tracking using purely
event-based data, capable of running on microcontrollers with limited
resources. Additionally, we outline a methodology to train, evaluate, and
quantize the network using a public dataset. Finally, we propose two versions
of the architecture: a classification model that detects the pupil on a grid
superimposed on the original image, and a regression model that operates at the
pixel level.

</details>


### [7] [3D Gaussian Point Encoders](https://arxiv.org/abs/2511.04797)
*Jim James,Ben Wilson,Simon Lucey,James Hays*

Main category: cs.CV

TL;DR: Introduces 3D Gaussian Point Encoders that use mixtures of learned 3D Gaussians as explicit per-point embeddings; achieves faster, more memory-efficient 3D recognition via end-to-end optimization with natural gradients and distillation from PointNet, plus acceleration for CPU deployment and integration into Mamba3D.


<details>
  <summary>Details</summary>
Motivation: Explicit geometric representations can offer speed and memory advantages over implicit networks (e.g., PointNet) for 3D recognition; training Gaussian-based encoders end-to-end is challenging and requires novel optimization and distillation techniques.

Method: Constructs per-point embeddings as mixtures of learned 3D Gaussians; optimizes them with natural gradients and distillation from PointNet activations to align Gaussian basis with PointNet representations; applies 3D Gaussian Splatting-inspired filtering/acceleration to speed up encoding; demonstrates scalability and CPU-friendly performance and integration into Mamba3D.

Result: 3D Gaussian Point Encoders are faster and more parameter-efficient than PointNets (2.7x faster at comparable accuracy, 46% less memory, 88% fewer FLOPs); in Mamba3D, 1.27x faster with 42% memory and 54% FLOPs reductions; capable of high framerates on CPU-only devices.

Conclusion: Explicit Gaussian-based encoders are a viable, efficient alternative to implicit 3D networks for recognition tasks, enabling substantial speedups and broad CPU deployment, with effective integration into existing systems like Mamba3D.

Abstract: In this work, we introduce the 3D Gaussian Point Encoder, an explicit
per-point embedding built on mixtures of learned 3D Gaussians. This explicit
geometric representation for 3D recognition tasks is a departure from widely
used implicit representations such as PointNet. However, it is difficult to
learn 3D Gaussian encoders in end-to-end fashion with standard optimizers. We
develop optimization techniques based on natural gradients and distillation
from PointNets to find a Gaussian Basis that can reconstruct PointNet
activations. The resulting 3D Gaussian Point Encoders are faster and more
parameter efficient than traditional PointNets. As in the 3D reconstruction
literature where there has been considerable interest in the move from implicit
(e.g., NeRF) to explicit (e.g., Gaussian Splatting) representations, we can
take advantage of computational geometry heuristics to accelerate 3D Gaussian
Point Encoders further. We extend filtering techniques from 3D Gaussian
Splatting to construct encoders that run 2.7 times faster as a comparable
accuracy PointNet while using 46% less memory and 88% fewer FLOPs. Furthermore,
we demonstrate the effectiveness of 3D Gaussian Point Encoders as a component
in Mamba3D, running 1.27 times faster and achieving a reduction in memory and
FLOPs by 42% and 54% respectively. 3D Gaussian Point Encoders are lightweight
enough to achieve high framerates on CPU-only devices.

</details>


### [8] [Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose](https://arxiv.org/abs/2511.04803)
*Shuo Zhao,Jianxu Chen*

Main category: cs.CV

TL;DR: Data-centric compression enables training with small yet diverse subsets; selective replay mitigates forgetting; domain sequencing aids cross-domain generalization in Cellpose.


<details>
  <summary>Details</summary>
Motivation: Investigate data redundancy and cross-domain forgetting in generalist biomedical segmentation to improve data efficiency and retention-aware transfer.

Method: Introduce dataset quantization (DQ) to select compact diverse subsets; validate via MAE embeddings and t-SNE; conduct cross-domain finetuning with and without replay; evaluate multi-stage domain sequencing; apply across Cyto dataset using Cellpose.

Result: Segmentation saturates at ~10% data; DQ subsets capture greater feature diversity; cross-domain transfer causes significant forgetting; selective 5-10% source replay restores performance and can aid adaptation; full replay can hamper; domain sequencing improves generalization and reduces forgetting; code released.

Conclusion: Data-centric strategies and retention-aware learning are crucial for efficient and robust cross-domain biomedical segmentation; proper domain ordering and replay policies can reduce forgetting while enabling adaptation.

Abstract: Generalist biomedical image segmentation models such as Cellpose are
increasingly applied across diverse imaging modalities and cell types. However,
two critical challenges remain underexplored: (1) the extent of training data
redundancy and (2) the impact of cross domain transfer on model retention. In
this study, we conduct a systematic empirical analysis of these challenges
using Cellpose as a case study. First, to assess data redundancy, we propose a
simple dataset quantization (DQ) strategy for constructing compact yet diverse
training subsets. Experiments on the Cyto dataset show that image segmentation
performance saturates with only 10% of the data, revealing substantial
redundancy and potential for training with minimal annotations. Latent space
analysis using MAE embeddings and t-SNE confirms that DQ selected patches
capture greater feature diversity than random sampling. Second, to examine
catastrophic forgetting, we perform cross domain finetuning experiments and
observe significant degradation in source domain performance, particularly when
adapting from generalist to specialist domains. We demonstrate that selective
DQ based replay reintroducing just 5-10% of the source data effectively
restores source performance, while full replay can hinder target adaptation.
Additionally, we find that training domain sequencing improves generalization
and reduces forgetting in multi stage transfer. Our findings highlight the
importance of data centric design in biomedical image segmentation and suggest
that efficient training requires not only compact subsets but also retention
aware learning strategies and informed domain ordering. The code is available
at https://github.com/MMV-Lab/biomedseg-efficiency.

</details>


### [9] [An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention](https://arxiv.org/abs/2511.04811)
*Shuo Zhao,Yu Zhou,Jianxu Chen*

Main category: cs.CV

TL;DR: A data-centric AI pipeline combines foundation-model pseudo-labels with nnU-Net, using core-set active learning to minimize manual labeling while preserving segmentation performance.


<details>
  <summary>Details</summary>
Motivation: nnU-Net automates model configuration but needs large annotated datasets; foundation models offer zero-shot generalization but may underperform on data with unique characteristics; there is a need to reduce human labeling while leveraging both traditional and foundation-model strengths.

Method: Generate pseudo-labels with a foundation model, use them to drive nnU-Net self-configuration, then select a representative core-set for minimal manual annotation to fine-tune nnU-Net; employs an active-learning loop to balance labeling effort and model performance.

Result: The approach reduces manual annotation requirements while maintaining competitive segmentation performance and makes state-of-the-art AI techniques more accessible to biomedical researchers; code is available at the stated GitHub repository.

Conclusion: A data-centric workflow that fuses foundation-model guidance with traditional segmentation networks via active learning can mitigate labeling needs and broaden applicability of advanced AI in biomedical image segmentation.

Abstract: Biomedical image segmentation is critical for precise structure delineation
and downstream analysis. Traditional methods often struggle with noisy data,
while deep learning models such as U-Net have set new benchmarks in
segmentation performance. nnU-Net further automates model configuration, making
it adaptable across datasets without extensive tuning. However, it requires a
substantial amount of annotated data for cross-validation, posing a challenge
when only raw images but no labels are available. Large foundation models offer
zero-shot generalizability, but may underperform on specific datasets with
unique characteristics, limiting their direct use for analysis. This work
addresses these bottlenecks by proposing a data-centric AI workflow that
leverages active learning and pseudo-labeling to combine the strengths of
traditional neural networks and large foundation models while minimizing human
intervention. The pipeline starts by generating pseudo-labels from a foundation
model, which are then used for nnU-Net's self-configuration. Subsequently, a
representative core-set is selected for minimal manual annotation, enabling
effective fine-tuning of the nnU-Net model. This approach significantly reduces
the need for manual annotations while maintaining competitive performance,
providing an accessible solution for biomedical researchers to apply
state-of-the-art AI techniques in their segmentation tasks. The code is
available at https://github.com/MMV-Lab/AL_BioMed_img_seg.

</details>


### [10] [Geometry Denoising with Preferred Normal Vectors](https://arxiv.org/abs/2511.04848)
*Manuel Weiß,Lukas Baumgärtner,Roland Herzog,Stephan Schmidt*

Main category: cs.CV

TL;DR: A geometry denoising framework that uses prior surface normals (label vectors) to drive joint denoising and segmentation, regularized by total variation, solved via split Bregman (ADMM), with vertex updates via second-order shape calculus.


<details>
  <summary>Details</summary>
Motivation: To leverage prior knowledge of surface normals to improve denoising quality and simultaneously perform segmentation by classifying normals relative to a predefined set of label vectors.

Method: Introduce label vectors as prior normal directions; embed segmentation by normal-vector similarity to these labels; apply total variation regularization; solve the resulting optimization with a split Bregman / ADMM approach; update vertex positions using second-order shape calculus.

Result: The abstract presents the formulation and algorithm, but does not report empirical results; it demonstrates a principled framework for joint denoising and segmentation with a novel optimization approach.

Conclusion: Proposes a new paradigm for geometry denoising using normal-label priors, integrated segmentation, and a tractable solver; future work likely includes empirical validation and comparison with existing methods.

Abstract: We introduce a new paradigm for geometry denoising using prior knowledge
about the surface normal vector. This prior knowledge comes in the form of a
set of preferred normal vectors, which we refer to as label vectors. A
segmentation problem is naturally embedded in the denoising process. The
segmentation is based on the similarity of the normal vector to the elements of
the set of label vectors. Regularization is achieved by a total variation term.
We formulate a split Bregman (ADMM) approach to solve the resulting
optimization problem. The vertex update step is based on second-order shape
calculus.

</details>


### [11] [Self-Supervised Implicit Attention Priors for Point Cloud Reconstruction](https://arxiv.org/abs/2511.04864)
*Kyle Fogarty,Chenyue Cai,Jing Yang,Zhilin Guo,Cengiz Öztireli*

Main category: cs.CV

TL;DR: Unsupervised implicit self-prior for surface reconstruction from irregular point clouds using a dictionary-augmented implicit field and RIMLS.


<details>
  <summary>Details</summary>
Motivation: Reconstructing high-quality surfaces from sparse/noisy/irregular point clouds is ill-posed without priors; propose self-derived priors to regularize reconstruction.

Method: Train a small dictionary of learnable embeddings jointly with an implicit distance field. At each query location, cross-attention to dictionary; use self-supervised losses to learn the prior from the input data alone; sample the trained field to obtain dense points and analytic normals via automatic differentiation; integrate into a robust implicit moving least squares (RIMLS) framework for final surface extraction.

Result: Outperforms classical and learning-based methods in fidelity and detail preservation; robust to common data degradations; captures repeating structures and long-range correlations; requires no external training data.

Conclusion: A self-derived, dictionary-augmented implicit prior paired with RIMLS yields high-fidelity surfaces from irregular point clouds, preserving fine geometry while regularizing sparse regions without relying on external datasets.

Abstract: Recovering high-quality surfaces from irregular point cloud is ill-posed
unless strong geometric priors are available. We introduce an implicit
self-prior approach that distills a shape-specific prior directly from the
input point cloud itself and embeds it within an implicit neural
representation. This is achieved by jointly training a small dictionary of
learnable embeddings with an implicit distance field; at every query location,
the field attends to the dictionary via cross-attention, enabling the network
to capture and reuse repeating structures and long-range correlations inherent
to the shape. Optimized solely with self-supervised point cloud reconstruction
losses, our approach requires no external training data. To effectively
integrate this learned prior while preserving input fidelity, the trained field
is then sampled to extract densely distributed points and analytic normals via
automatic differentiation. We integrate the resulting dense point cloud and
corresponding normals into a robust implicit moving least squares (RIMLS)
formulation. We show this hybrid strategy preserves fine geometric details in
the input data, while leveraging the learned prior to regularize sparse
regions. Experiments show that our method outperforms both classical and
learning-based approaches in generating high-fidelity surfaces with superior
detail preservation and robustness to common data degradations.

</details>


### [12] [Clinical-ComBAT: a diffusion-weighted MRI harmonization method for clinical applications](https://arxiv.org/abs/2511.04871)
*Gabriel Girard,Manon Edde,Félix Dumais,Yoan David,Matthieu Dumont,Guillaume Theaud,Jean-Christophe Houde,Arnaud Boré,Maxime Descoteaux,Pierre-Marc Jodoin*

Main category: cs.CV

TL;DR: Clinical-ComBAT is a flexible, site-specific, non-linear harmonization method for diffusion-weighted MRI that enables incremental integration of new clinics, improving cross-site alignment and enabling normative modeling beyond traditional ComBAT.


<details>
  <summary>Details</summary>
Motivation: To overcome ComBAT's limitations in real-world clinical multi-site DW-MRI studies, including linear covariate assumptions, homogeneous populations, fixed site counts, and poorly populated sites, by providing a scalable, non-linear, site-referenced harmonization approach suitable for growing clinical data.

Method: Harmonize each site independently using a non-linear polynomial data model with site-specific harmonization referenced to a normative site; incorporate variance priors that adapt to small cohorts; include hyperparameter tuning and a goodness-of-fit metric; applicable to simulated and real data to support normative modeling.

Result: Showed improved alignment of diffusion metrics across sites and enhanced applicability for normative modeling in both simulated and real datasets.

Conclusion: Clinical-ComBAT offers a practical, flexible solution for multi-site DW-MRI harmonization in clinical settings, enabling incremental data integration and better cross-site comparability, with potential to support normative analyses.

Abstract: Diffusion-weighted magnetic resonance imaging (DW-MRI) derived scalar maps
are effective for assessing neurodegenerative diseases and microstructural
properties of white matter in large number of brain conditions. However, DW-MRI
inherently limits the combination of data from multiple acquisition sites
without harmonization to mitigate scanner-specific biases. While the widely
used ComBAT method reduces site effects in research, its reliance on linear
covariate relationships, homogeneous populations, fixed site numbers, and well
populated sites constrains its clinical use. To overcome these limitations, we
propose Clinical-ComBAT, a method designed for real-world clinical scenarios.
Clinical-ComBAT harmonizes each site independently, enabling flexibility as new
data and clinics are introduced. It incorporates a non-linear polynomial data
model, site-specific harmonization referenced to a normative site, and variance
priors adaptable to small cohorts. It further includes hyperparameter tuning
and a goodness-of-fit metric for harmonization assessment. We demonstrate its
effectiveness on simulated and real data, showing improved alignment of
diffusion metrics and enhanced applicability for normative modeling.

</details>


### [13] [Validating Vision Transformers for Otoscopy: Performance and Data-Leakage Effects](https://arxiv.org/abs/2511.04872)
*James Ndubuisi,Fernando Auat,Marta Vallejo*

Main category: cs.CV

TL;DR: Swin transformer models were tested against ResNet on ear-disease diagnosis using otoscopic videos; initial near-perfect accuracies were inflated due to data leakage in preprocessing; after correcting leakage, accuracies dropped to ~82–83% for all models, underscoring the critical role of data handling and preprocessing in medical ML evaluation.


<details>
  <summary>Details</summary>
Motivation: Reduce misdiagnosis of ear diseases (27% misdiagnosis among specialists) by improving automated diagnostic accuracy with advanced architectures, exploring whether vision transformers outperform CNNs on real-world otoscopic video data.

Method: Real-world dataset from a Chilean hospital (otoscopic videos). Frames were selected using Laplacian variance and Shannon entropy; blank frames removed. Models compared: Swin v1, Swin v2, and ResNet. Initial evaluation suggested very high accuracy, but data leakage in preprocessing was later identified and mitigated, leading to lower, more realistic performance.

Result: Initial accuracies: Swin v1 100%, Swin v2 99.1%, ResNet 99.5%. After addressing data leakage, corrected accuracies were ~83% for Swin v1/v2 and 82% for ResNet.

Conclusion: Vision transformers show potential for ear-disease diagnosis, but robust data handling is essential. Data leakage can dramatically inflate results; future work should prioritize rigorous validation, transparent preprocessing, and per-patient splits to ensure reliable performance estimates.

Abstract: This study evaluates the efficacy of vision transformer models, specifically
Swin transformers, in enhancing the diagnostic accuracy of ear diseases
compared to traditional convolutional neural networks. With a reported 27%
misdiagnosis rate among specialist otolaryngologists, improving diagnostic
accuracy is crucial. The research utilised a real-world dataset from the
Department of Otolaryngology at the Clinical Hospital of the Universidad de
Chile, comprising otoscopic videos of ear examinations depicting various middle
and external ear conditions. Frames were selected based on the Laplacian and
Shannon entropy thresholds, with blank frames removed. Initially, Swin v1 and
Swin v2 transformer models achieved accuracies of 100% and 99.1%, respectively,
marginally outperforming the ResNet model (99.5%). These results surpassed
metrics reported in related studies. However, the evaluation uncovered a
critical data leakage issue in the preprocessing step, affecting both this
study and related research using the same raw dataset. After mitigating the
data leakage, model performance decreased significantly. Corrected accuracies
were 83% for both Swin v1 and Swin v2, and 82% for the ResNet model. This
finding highlights the importance of rigorous data handling in machine learning
studies, especially in medical applications. The findings indicate that while
vision transformers show promise, it is essential to find an optimal balance
between the benefits of advanced model architectures and those derived from
effective data preprocessing. This balance is key to developing a reliable
machine learning model for diagnosing ear diseases.

</details>


### [14] [Beta Distribution Learning for Reliable Roadway Crash Risk Assessment](https://arxiv.org/abs/2511.04886)
*Ahmad Elallaf,Nathan Jacobs,Xinyue Ye,Mei Chen,Gongbo Liang*

Main category: cs.CV

TL;DR: Geospatial DL using satellite imagery to predict fatal crash risk with Beta-distributed uncertainty, improving recall and calibration.


<details>
  <summary>Details</summary>
Motivation: Traditional traffic safety studies analyze risk factors in isolation and overlook spatial/contextual interactions in the built environment; conventional NN risk estimators lack quantified uncertainty, hindering decisions in safety-critical domains.

Method: A geospatial deep learning framework that uses satellite imagery as the sole spatial input to model fatal crash risk and outputs a Beta distribution, providing uncertainty-aware predictions rather than a point estimate.

Result: The approach achieves a 17-23% improvement in recall over baselines and shows superior calibration, enabling reliable risk assessments from imagery alone.

Conclusion: The method offers scalable, uncertainty-aware risk assessments from satellite imagery that can enhance autonomous navigation, urban planning, and policy decisions for roadway safety.

Abstract: Roadway traffic accidents represent a global health crisis, responsible for
over a million deaths annually and costing many countries up to 3% of their
GDP. Traditional traffic safety studies often examine risk factors in
isolation, overlooking the spatial complexity and contextual interactions
inherent in the built environment. Furthermore, conventional Neural
Network-based risk estimators typically generate point estimates without
conveying model uncertainty, limiting their utility in critical
decision-making. To address these shortcomings, we introduce a novel geospatial
deep learning framework that leverages satellite imagery as a comprehensive
spatial input. This approach enables the model to capture the nuanced spatial
patterns and embedded environmental risk factors that contribute to fatal crash
risks. Rather than producing a single deterministic output, our model estimates
a full Beta probability distribution over fatal crash risk, yielding accurate
and uncertainty-aware predictions--a critical feature for trustworthy AI in
safety-critical applications. Our model outperforms baselines by achieving a
17-23% improvement in recall, a key metric for flagging potential dangers,
while delivering superior calibration. By providing reliable and interpretable
risk assessments from satellite imagery alone, our method enables safer
autonomous navigation and offers a highly scalable tool for urban planners and
policymakers to enhance roadway safety equitably and cost-effectively.

</details>


### [15] [Learning to Restore Multi-Degraded Images via Ingredient Decoupling and Task-Aware Path Adaptation](https://arxiv.org/abs/2511.04920)
*Hu Gao,Xiaoning Lei,Ying Zhang,Xichen Xu,Guannan Jiang,Lizhuang Ma*

Main category: cs.CV

TL;DR: An adaptive multi-degradation image restoration network (IMDNet) decouples degradation ingredients to guide restoration paths, achieving strong multi-degradation restoration and competitive single-degradation performance.


<details>
  <summary>Details</summary>
Motivation: Real-world images suffer from multiple coexisting degradations; existing methods targeting single degradations fail to generalize to mixtures; decoupling degradation factors can enable flexible, adaptive restoration.

Method: Introduce DIDBlock to separate degradation ingredients by combining spatial and frequency-domain cues; introduce FBlock to fuse degradation information across levels with learnable matrices; introduce TABLock (TABlock) to dynamically activate or fuse branches based on multi-degradation representation; encoder-decoder architecture IMDNet guided by multi-degradation representation to dynamically activate/fuse branches and select optimal restoration paths.

Result: Experiments show superior performance for multi-degradation restoration and strong competitiveness for single-degradation tasks, demonstrating effectiveness of decoupled degradation representations and adaptive path selection.

Conclusion: Decoupled degradation representations with adaptive path selection enable robust multi-degradation restoration and maintain performance on single-degradation tasks; the approach offers a flexible framework for handling complex real-world degradations.

Abstract: Image restoration (IR) aims to recover clean images from degraded
observations. Despite remarkable progress, most existing methods focus on a
single degradation type, whereas real-world images often suffer from multiple
coexisting degradations, such as rain, noise, and haze coexisting in a single
image, which limits their practical effectiveness. In this paper, we propose an
adaptive multi-degradation image restoration network that reconstructs images
by leveraging decoupled representations of degradation ingredients to guide
path selection. Specifically, we design a degradation ingredient decoupling
block (DIDBlock) in the encoder to separate degradation ingredients
statistically by integrating spatial and frequency domain information,
enhancing the recognition of multiple degradation types and making their
feature representations independent. In addition, we present fusion block
(FBlock) to integrate degradation information across all levels using learnable
matrices. In the decoder, we further introduce a task adaptation block
(TABlock) that dynamically activates or fuses functional branches based on the
multi-degradation representation, flexibly selecting optimal restoration paths
under diverse degradation conditions. The resulting tightly integrated
architecture, termed IMDNet, is extensively validated through experiments,
showing superior performance on multi-degradation restoration while maintaining
strong competitiveness on single-degradation tasks.

</details>


### [16] [A benchmark multimodal oro-dental dataset for large vision-language models](https://arxiv.org/abs/2511.04948)
*Haoxin Lv,Ijazul Haq,Jin Du,Jiaxin Ma,Binnian Zhu,Xiaobing Dang,Chaoan Liang,Ruxu Du,Yingjie Zhang,Muhammad Saqib*

Main category: cs.CV

TL;DR: A large, publicly available multimodal dental dataset (8775 checkups, 50k images, 8k radiographs, annotations) used to fine-tune Qwen-VL 3B/7B for six oro-dental anomaly classification and diagnostic report generation; shows improvements over baselines and GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To overcome scarcity of integrated multimodal data in dentistry and to advance AI-driven diagnostic and reporting capabilities.

Method: Assembled eight-year dataset (2018-2025) from ages 10–90; includes images, radiographs, and textual records; annotated for benchmarking; fine-tuned Qwen-VL 3B and 7B on two tasks; evaluated against base models and GPT-4o.

Result: Fine-tuned models achieved substantial gains on both tasks compared with baselines; validates dataset usefulness; dataset is publicly available.

Conclusion: Dataset is a valuable resource to drive AI dentistry research and real-world applications; encourages further multimodal AI development in oral healthcare.

Abstract: The advancement of artificial intelligence in oral healthcare relies on the
availability of large-scale multimodal datasets that capture the complexity of
clinical practice. In this paper, we present a comprehensive multimodal
dataset, comprising 8775 dental checkups from 4800 patients collected over
eight years (2018-2025), with patients ranging from 10 to 90 years of age. The
dataset includes 50000 intraoral images, 8056 radiographs, and detailed textual
records, including diagnoses, treatment plans, and follow-up notes. The data
were collected under standard ethical guidelines and annotated for
benchmarking. To demonstrate its utility, we fine-tuned state-of-the-art large
vision-language models, Qwen-VL 3B and 7B, and evaluated them on two tasks:
classification of six oro-dental anomalies and generation of complete
diagnostic reports from multimodal inputs. We compared the fine-tuned models
with their base counterparts and GPT-4o. The fine-tuned models achieved
substantial gains over these baselines, validating the dataset and underscoring
its effectiveness in advancing AI-driven oro-dental healthcare solutions. The
dataset is publicly available, providing an essential resource for future
research in AI dentistry.

</details>


### [17] [DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning](https://arxiv.org/abs/2511.04949)
*Tharindu Fernando,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: Proposes a latent-space watermarking framework with MAARL for proactive deepfake detection, outperforming state-of-the-art under manipulations on CelebA/CelebA-HQ.


<details>
  <summary>Details</summary>
Motivation: Passive deepfake detectors struggle to generalize to new forgery types; existing watermarking methods balance robustness and tampering sensitivity; need robust, adaptive proactive detection.

Method: A learnable watermark embedder operating in latent space capturing high-level semantics; uses Multi-Agent Adversarial Reinforcement Learning with an attacker agent and a benign manipulations curriculum to balance robustness and fragility; evaluated on CelebA/CelebA-HQ.

Result: Outperforms state-of-the-art by >4.5% on CelebA and >5.3% on CelebA-HQ under challenging manipulations.

Conclusion: The MAARL-based latent watermarking framework provides robust, adaptive proactive deepfake detection with strong generalization to manipulated media.

Abstract: Rapid advances in generative AI have led to increasingly realistic deepfakes,
posing growing challenges for law enforcement and public trust. Existing
passive deepfake detectors struggle to keep pace, largely due to their
dependence on specific forgery artifacts, which limits their ability to
generalize to new deepfake types. Proactive deepfake detection using watermarks
has emerged to address the challenge of identifying high-quality synthetic
media. However, these methods often struggle to balance robustness against
benign distortions with sensitivity to malicious tampering. This paper
introduces a novel deep learning framework that harnesses high-dimensional
latent space representations and the Multi-Agent Adversarial Reinforcement
Learning (MAARL) paradigm to develop a robust and adaptive watermarking
approach. Specifically, we develop a learnable watermark embedder that operates
in the latent space, capturing high-level image semantics, while offering
precise control over message encoding and extraction. The MAARL paradigm
empowers the learnable watermarking agent to pursue an optimal balance between
robustness and fragility by interacting with a dynamic curriculum of benign and
malicious image manipulations simulated by an adversarial attacker agent.
Comprehensive evaluations on the CelebA and CelebA-HQ benchmarks reveal that
our method consistently outperforms state-of-the-art approaches, achieving
improvements of over 4.5% on CelebA and more than 5.3% on CelebA-HQ under
challenging manipulation scenarios.

</details>


### [18] [CLM: Removing the GPU Memory Barrier for 3D Gaussian Splatting](https://arxiv.org/abs/2511.04951)
*Hexu Zhao,Xiwen Min,Xiaoteng Liu,Moonjun Gong,Yiming Li,Ang Li,Saining Xie,Jinyang Li,Aurojit Panda*

Main category: cs.CV

TL;DR: A system (CLM) enables 3D Gaussian Splatting to render large scenes on a single consumer GPU by offloading Gaussians to CPU memory and streaming them on demand, using a pipelined, access-pattern-aware strategy to overlap CPU/GPU computation and reduce data transfer, achieving 100M Gaussians on RTX4090 with state-of-the-art quality.


<details>
  <summary>Details</summary>
Motivation: 3DGS is fast and high-quality but memory-hungry; scaling to large/intricate scenes exceeds GPU memory; need a cost-effective solution on consumer hardware.

Method: Offload Gaussians to CPU memory; stream/load them into GPU on demand; use a pipelined overlap of GPU-CPU communication, GPU compute, CPU compute; exploit memory access patterns to reduce communication volume.

Result: Demonstrates rendering a large scene with 100 million Gaussians on RTX4090; achieves state-of-the-art reconstruction quality.

Conclusion: CLM enables scalable 3DGS on a single consumer GPU; memory offloading with access-pattern-aware scheduling is effective; broadens practicality for large-scale novel view synthesis.

Abstract: 3D Gaussian Splatting (3DGS) is an increasingly popular novel view synthesis
approach due to its fast rendering time, and high-quality output. However,
scaling 3DGS to large (or intricate) scenes is challenging due to its large
memory requirement, which exceed most GPU's memory capacity. In this paper, we
describe CLM, a system that allows 3DGS to render large scenes using a single
consumer-grade GPU, e.g., RTX4090. It does so by offloading Gaussians to CPU
memory, and loading them into GPU memory only when necessary. To reduce
performance and communication overheads, CLM uses a novel offloading strategy
that exploits observations about 3DGS's memory access pattern for pipelining,
and thus overlap GPU-to-CPU communication, GPU computation and CPU computation.
Furthermore, we also exploit observation about the access pattern to reduce
communication volume. Our evaluation shows that the resulting implementation
can render a large scene that requires 100 million Gaussians on a single
RTX4090 and achieve state-of-the-art reconstruction quality.

</details>


### [19] [Pattern-Aware Diffusion Synthesis of fMRI/dMRI with Tissue and Microstructural Refinement](https://arxiv.org/abs/2511.04963)
*Xiongri Shen,Jiaqi Wang,Yi Zhong,Zhenxi Song,Leilei Zhao,Yichen Wei,Lingyan Liang,Shuqiang Wang,Baiying Lei,Demao Deng,Zhiguo Zhang*

Main category: cs.CV

TL;DR: A pattern-aware dual-modal 3D diffusion model (PDS) for cross-modality synthesis of fMRI and dMRI, with a tissue refinement network to preserve structure. It achieves state-of-the-art image quality and competitive clinical accuracy across multiple datasets, with code released.


<details>
  <summary>Details</summary>
Motivation: Missing imaging modalities hinder clinical use; existing GAN- and diffusion-model approaches struggle to bridge fMRI and dMRI due to (1) substantial signal differences along time/gradient axes and (2) inadequate incorporation of disease-related neuroanatomical patterns during generation.

Method: Introduces a pattern-aware dual-modal 3D diffusion framework for cross-modality learning and a tissue refinement network with efficient microstructure refinement to maintain structural fidelity and fine details.

Result: Achieves state-of-the-art metrics: PSNR/SSIM for fMRI synthesis = 29.83 dB / 90.84% (improved by +1.54 dB / +4.12% over baselines); for dMRI synthesis = 30.00 dB / 77.55% (improved by +1.02 dB / +2.2%). Clinical validation shows diagnostic accuracy in hybrid real-synthetic experiments as NC vs MCI vs AD: 67.92% / 66.02% / 64.15%. Datasets: OASIS-3, ADNI, and in-house; code available on GitHub.

Conclusion: PDS effectively addresses cross-modality fMRI-dMRI synthesis challenges by leveraging pattern-aware cross-modal learning and tissue refinement, yielding high-fidelity results and clinically useful performance; the authors provide code for reproducibility.

Abstract: Magnetic resonance imaging (MRI), especially functional MRI (fMRI) and
diffusion MRI (dMRI), is essential for studying neurodegenerative diseases.
However, missing modalities pose a major barrier to their clinical use.
Although GAN- and diffusion model-based approaches have shown some promise in
modality completion, they remain limited in fMRI-dMRI synthesis due to (1)
significant BOLD vs. diffusion-weighted signal differences between fMRI and
dMRI in time/gradient axis, and (2) inadequate integration of disease-related
neuroanatomical patterns during generation. To address these challenges, we
propose PDS, introducing two key innovations: (1) a pattern-aware dual-modal 3D
diffusion framework for cross-modality learning, and (2) a tissue refinement
network integrated with a efficient microstructure refinement to maintain
structural fidelity and fine details. Evaluated on OASIS-3, ADNI, and in-house
datasets, our method achieves state-of-the-art results, with PSNR/SSIM scores
of 29.83 dB/90.84\% for fMRI synthesis (+1.54 dB/+4.12\% over baselines) and
30.00 dB/77.55\% for dMRI synthesis (+1.02 dB/+2.2\%). In clinical validation,
the synthesized data show strong diagnostic performance, achieving
67.92\%/66.02\%/64.15\% accuracy (NC vs. MCI vs. AD) in hybrid real-synthetic
experiments. Code is available in \href{https://github.com/SXR3015/PDS}{PDS
GitHub Repository}

</details>


### [20] [Learning Fourier shapes to probe the geometric world of deep neural networks](https://arxiv.org/abs/2511.04970)
*Jian Wang,Yixing Yong,Haixia Bi,Lijun He,Fan Li*

Main category: cs.CV

TL;DR: Optimized geometric shapes, parameterized by Fourier series and rasterized via a winding-number map, can act as semantic carriers, serve as precise interpretability tools, and form a new, generalizable adversarial paradigm for DNNs within a differentiable energy-constrained framework.


<details>
  <summary>Details</summary>
Motivation: To explore the geometric dimension of deep neural networks beyond texture bias, enabling geometry-driven classification, interpretation, and adversarial testing.

Method: An end-to-end differentiable pipeline that (1) parameterizes arbitrary shapes with a powerful Fourier series, (2) uses a winding-number-based mapping to convert shapes into the DNN's pixel grid, and (3) applies signal energy constraints to improve optimization efficiency and enforce physically plausible shapes.

Result: Optimized shapes can yield high-confidence classifications using purely geometric inputs, serve as high-fidelity interpretability tools that precisely isolate salient regions, and establish a new, generalizable adversarial paradigm that can deceive downstream visual tasks.

Conclusion: The framework provides a versatile toolset for probing the geometric aspects of DNNs and opens new directions for challenging and understanding machine perception.

Abstract: While both shape and texture are fundamental to visual recognition, research
on deep neural networks (DNNs) has predominantly focused on the latter, leaving
their geometric understanding poorly probed. Here, we show: first, that
optimized shapes can act as potent semantic carriers, generating
high-confidence classifications from inputs defined purely by their geometry;
second, that they are high-fidelity interpretability tools that precisely
isolate a model's salient regions; and third, that they constitute a new,
generalizable adversarial paradigm capable of deceiving downstream visual
tasks. This is achieved through an end-to-end differentiable framework that
unifies a powerful Fourier series to parameterize arbitrary shapes, a winding
number-based mapping to translate them into the pixel grid required by DNNs,
and signal energy constraints that enhance optimization efficiency while
ensuring physically plausible shapes. Our work provides a versatile framework
for probing the geometric world of DNNs and opens new frontiers for challenging
and understanding machine perception.

</details>


### [21] [Challenges in 3D Data Synthesis for Training Neural Networks on Topological Features](https://arxiv.org/abs/2511.04972)
*Dylan Peek,Matthew P. Skerritt,Siddharth Pritam,Stephan Chalup*

Main category: cs.CV

TL;DR: Synthetic labeled 3D dataset for TDA developed via Repulsive Surface algorithm; enables training of neural genus estimators; shows topology alone isn't enough—geometric complexity affects performance.


<details>
  <summary>Details</summary>
Motivation: There is a lack of labeled 3D data with class distributions tailored for supervised learning in TDA tasks, making it hard to train and benchmark neural estimators that approximate persistent-homology computations.

Method: Use the Repulsive Surface algorithm to systematically generate 3D shapes with controllable topological invariants (e.g., hole count) and attach genus-based labels. Train a genus estimator using a 3D convolutional transformer architecture. Evaluate how geometric deformations impact performance.

Result: The estimator's accuracy decreases as deformations increase, indicating that geometric complexity, not just topological complexity, affects generalization. The dataset fills a gap in labeled 3D data for training/evaluating TDA-focused ML methods.

Conclusion: The work provides a useful synthetic labeled 3D dataset for training and benchmarking neural estimators in TDA and highlights the need to account for geometric complexity when building generalized models.

Abstract: Topological Data Analysis (TDA) involves techniques of analyzing the
underlying structure and connectivity of data. However, traditional methods
like persistent homology can be computationally demanding, motivating the
development of neural network-based estimators capable of reducing
computational overhead and inference time. A key barrier to advancing these
methods is the lack of labeled 3D data with class distributions and diversity
tailored specifically for supervised learning in TDA tasks. To address this, we
introduce a novel approach for systematically generating labeled 3D datasets
using the Repulsive Surface algorithm, allowing control over topological
invariants, such as hole count. The resulting dataset offers varied geometry
with topological labeling, making it suitable for training and benchmarking
neural network estimators. This paper uses a synthetic 3D dataset to train a
genus estimator network, created using a 3D convolutional transformer
architecture. An observed decrease in accuracy as deformations increase
highlights the role of not just topological complexity, but also geometric
complexity, when training generalized estimators. This dataset fills a gap in
labeled 3D datasets and generation for training and evaluating models and
techniques for TDA.

</details>


### [22] [GSE: Evaluating Sticker Visual Semantic Similarity via a General Sticker Encoder](https://arxiv.org/abs/2511.04977)
*Heng Er Metilda Chee,Jiayin Wang,Zhiqiang Guo,Weizhi Ma,Min Zhang*

Main category: cs.CV

TL;DR: Introduces Triple-S, the first benchmark for Sticker Semantic Similarity with 905 labeled sticker pairs, and General Sticker Encoder (GSE), a lightweight model that learns robust sticker embeddings using Triple-S and additional data; GSE excels on unseen stickers and downstream tasks; resources released for future research.


<details>
  <summary>Details</summary>
Motivation: Stickers are a popular, symbolic visual language whose semantic relationships are difficult to quantify; there is a need for standardized evaluation and robust embeddings to advance sticker understanding, retrieval, and multimodal content generation.

Method: Define the Sticker Semantic Similarity task and build Triple-S as the first benchmark with 905 human-annotated sticker pairs. Propose General Sticker Encoder (GSE), a lightweight model trained on Triple-S and additional datasets to produce robust sticker embeddings. Evaluate on unseen stickers and downstream tasks like emotion classification and sticker-to-sticker retrieval.

Result: GSE achieves superior performance on unseen stickers and shows strong results on downstream tasks. Triple-S and GSE are publicly released to enable standardized evaluation and embeddings for sticker understanding and retrieval.

Conclusion: The work provides standardized evaluation tools and robust embeddings that advance sticker understanding and multimodal content generation, facilitating future research in sticker-related tasks.

Abstract: Stickers have become a popular form of visual communication, yet
understanding their semantic relationships remains challenging due to their
highly diverse and symbolic content. In this work, we formally {define the
Sticker Semantic Similarity task} and introduce {Triple-S}, the first benchmark
for this task, consisting of 905 human-annotated positive and negative sticker
pairs. Through extensive evaluation, we show that existing pretrained vision
and multimodal models struggle to capture nuanced sticker semantics. To address
this, we propose the {General Sticker Encoder (GSE)}, a lightweight and
versatile model that learns robust sticker embeddings using both Triple-S and
additional datasets. GSE achieves superior performance on unseen stickers, and
demonstrates strong results on downstream tasks such as emotion classification
and sticker-to-sticker retrieval. By releasing both Triple-S and GSE, we
provide standardized evaluation tools and robust embeddings, enabling future
research in sticker understanding, retrieval, and multimodal content
generation. The Triple-S benchmark and GSE have been publicly released and are
available here.

</details>


### [23] [Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings](https://arxiv.org/abs/2511.05017)
*Aakriti Agrawal,Gouthaman KV,Rohith Aralikatti,Gauri Jagatap,Jiaxin Yuan,Vijay Kamarshi,Andrea Fanelli,Furong Huang*

Main category: cs.CV

TL;DR: A lightweight fusion of average-pooled visual features into textual embeddings to reduce modality bias and hallucinations in LVLMs, improving visual grounding.


<details>
  <summary>Details</summary>
Motivation: Prevailing LVLM architectures exhibit an inherent bias toward language because visual information is often appended to text input, causing modality imbalance and hallucinations. A simple method to rebalance by enriching textual embeddings with visual cues aims to improve grounding.

Method: Refine textual embeddings by integrating average-pooled visual features into the text representation. The approach is simple, robust, and efficient, avoiding complex cross-modal fusion.

Result: The method improves visual grounding and significantly reduces hallucinations on established benchmarks.

Conclusion: Average-pooling provides a simple, robust means to incorporate visual information and mitigate modality imbalance. The authors acknowledge that more sophisticated fusion strategies could further enhance cross-modal alignment and grounding and leave such exploration for future work.

Abstract: In this work, we identify an inherent bias in prevailing LVLM architectures
toward the language modality, largely resulting from the common practice of
simply appending visual embeddings to the input text sequence. To address this,
we propose a simple yet effective method that refines textual embeddings by
integrating average-pooled visual features. Our approach demonstrably improves
visual grounding and significantly reduces hallucinations on established
benchmarks. While average pooling offers a straightforward, robust, and
efficient means of incorporating visual information, we believe that more
sophisticated fusion methods could further enhance visual grounding and
cross-modal alignment. Given that the primary focus of this work is to
highlight the modality imbalance and its impact on hallucinations -- and to
show that refining textual embeddings with visual information mitigates this
issue -- we leave exploration of advanced fusion strategies for future work.

</details>


### [24] [Dynamic Residual Encoding with Slide-Level Contrastive Learning for End-to-End Whole Slide Image Representation](https://arxiv.org/abs/2511.05034)
*Jing Jin,Xu Liu,Te Gao,Zhihong Shi,Yixiong Liang,Ruiqing Zheng,Hulin Kuang,Min Zeng,Shichao Kan*

Main category: cs.CV

TL;DR: A dynamic residual encoding with slide-level contrastive learning (DRE-SLCL) approach for end-to-end WSI representation leverages a memory bank of tile features, sampling tiles per WSI per batch, retrieving additional tiles from memory, and fusing them via residual encoding to produce slide representations for slide-level contrastive learning across WSIs.


<details>
  <summary>Details</summary>
Motivation: Whole-slide images contain tens of thousands of tiles, making end-to-end training with full gradient flow infeasible on standard GPUs. There is a need for scalable, discriminative slide-level representations that can leverage local tile information without exhaustively processing all tiles in every batch.

Method: In each training batch, sample a subset of tiles per WSI and compute their features with a tile encoder. Retrieve additional tile features for the same WSI from a memory bank that stores features across all WSIs. Use a residual encoding scheme to combine sampled and retrieved features into a single WSI representation. Compute a slide-level contrastive loss using the produced representations and the corresponding histopathology reports within the batch.

Result: Experiments on cancer subtyping, recognition, and mutation prediction show that DRE-SLCL effectively learns end-to-end WSI representations, outperforming baselines on these tasks.

Conclusion: DRE-SLCL provides a scalable and effective end-to-end framework for WSI representation learning by integrating memory-based tile retrieval with residual encoding and slide-level contrastive learning to leverage both local and global slide information.

Abstract: Whole Slide Image (WSI) representation is critical for cancer subtyping,
cancer recognition and mutation prediction.Training an end-to-end WSI
representation model poses significant challenges, as a standard gigapixel
slide can contain tens of thousands of image tiles, making it difficult to
compute gradients of all tiles in a single mini-batch due to current GPU
limitations. To address this challenge, we propose a method of dynamic residual
encoding with slide-level contrastive learning (DRE-SLCL) for end-to-end WSI
representation. Our approach utilizes a memory bank to store the features of
tiles across all WSIs in the dataset. During training, a mini-batch usually
contains multiple WSIs. For each WSI in the batch, a subset of tiles is
randomly sampled and their features are computed using a tile encoder. Then,
additional tile features from the same WSI are selected from the memory bank.
The representation of each individual WSI is generated using a residual
encoding technique that incorporates both the sampled features and those
retrieved from the memory bank. Finally, the slide-level contrastive loss is
computed based on the representations and histopathology reports ofthe WSIs
within the mini-batch. Experiments conducted over cancer subtyping, cancer
recognition, and mutation prediction tasks proved the effectiveness of the
proposed DRE-SLCL method.

</details>


### [25] [Pressure2Motion: Hierarchical Motion Synthesis from Ground Pressure with Text Guidance](https://arxiv.org/abs/2511.05038)
*Zhengxuan Li,Qinhui Yang,Yiyu Zhuang,Chuan Guo,Xinxin Zuo,Xiaoxiao Long,Yao Yao,Xun Cao,Qiu Shen,Hao Zhu*

Main category: cs.CV

TL;DR: Pressure2Motion synthesizes human motion from ground pressure sequences plus text prompts, enabling privacy-preserving, camera-free motion capture; it uses a dual-level feature extractor and a hierarchical diffusion model, and introduces the MPL benchmark with claimed state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Mocap from pressure data is severely ill-posed due to the indeterminate mapping from pressure signals to full-body motion; there is a need for privacy-preserving, low-cost, camera-free mocap, and linguistic priors may help constrain the generation.

Method: A dual-level pressure feature extractor interprets pressure data; a hierarchical diffusion model generates motion by modeling broad trajectories and fine postures; text prompts serve as high-level guidance constraints; a new MPL benchmark is established for this task.

Result: Experiments report high-fidelity, physically plausible motions and state-of-the-art performance on the MPL benchmark; codes and benchmarks will be released publicly.

Conclusion: Pressure2Motion pioneers the use of both pressure data and linguistic priors for motion generation, establishing a new research direction and providing a standard benchmark (MPL) for this task; this approach promises privacy-preserving, low-cost mocap applications.

Abstract: We present Pressure2Motion, a novel motion capture algorithm that synthesizes
human motion from a ground pressure sequence and text prompt. It eliminates the
need for specialized lighting setups, cameras, or wearable devices, making it
suitable for privacy-preserving, low-light, and low-cost motion capture
scenarios. Such a task is severely ill-posed due to the indeterminate nature of
the pressure signals to full-body motion. To address this issue, we introduce
Pressure2Motion, a generative model that leverages pressure features as input
and utilizes a text prompt as a high-level guiding constraint. Specifically,
our model utilizes a dual-level feature extractor that accurately interprets
pressure data, followed by a hierarchical diffusion model that discerns
broad-scale movement trajectories and subtle posture adjustments. Both the
physical cues gained from the pressure sequence and the semantic guidance
derived from descriptive texts are leveraged to guide the motion generation
with precision. To the best of our knowledge, Pressure2Motion is a pioneering
work in leveraging both pressure data and linguistic priors for motion
generation, and the established MPL benchmark is the first benchmark for this
task. Experiments show our method generates high-fidelity, physically plausible
motions, establishing a new state-of-the-art for this task. The codes and
benchmarks will be publicly released upon publication.

</details>


### [26] [Medical Referring Image Segmentation via Next-Token Mask Prediction](https://arxiv.org/abs/2511.05044)
*Xinyu Chen,Yiran Wang,Gaoyang Pang,Jiafu Hao,Chentao Yue,Luping Zhou,Yonghui Li*

Main category: cs.CV

TL;DR: Reframe MRIS as autoregressive next-token prediction over a unified multimodal sequence, enabling a simple, end-to-end architecture that leverages pretrained tokenizers; introduces NkTP, TCL, and HET to tackle exposure bias, long-tail token distributions, and fine-grained edges, achieving state-of-the-art on QaTa-COV19 and MosMedData+.


<details>
  <summary>Details</summary>
Motivation: MRIS tasks traditionally rely on complex multimodal fusion modules and multi-stage decoders, making models bulky and harder to train end-to-end. A unified, token-based formulation could simplify design, leverage pretrained multimodal tokenizers, and improve generalization across datasets.

Method: Formulates MRIS as autoregressive next-token prediction over a single multimodal sequence containing image, text, and mask tokens. Utilizes pretrained tokenizers from large multimodal models. Introduces three strategies: (1) Next-k Token Prediction (NkTP) to reduce error accumulation, (2) Token-level Contrastive Learning (TCL) to sharpen boundaries and mitigate long-tail token frequencies, and (3) memory-based Hard Error Token (HET) optimization to emphasize difficult tokens during training.

Result: On QaTa-COV19 and MosMedData+ datasets, the approach achieves new state-of-the-art performance, while offering a streamlined, end-to-end pipeline without modality-specific fusion or external segmentation models.

Conclusion: Reframing MRIS as a unified autoregressive token prediction task with targeted training strategies yields superior performance and a simpler, more adaptable architecture, highlighting the viability of token-based, end-to-end MRIS models leveraging pretrained multimodal tokenizers.

Abstract: Medical Referring Image Segmentation (MRIS) involves segmenting target
regions in medical images based on natural language descriptions. While
achieving promising results, recent approaches usually involve complex design
of multimodal fusion or multi-stage decoders. In this work, we propose
NTP-MRISeg, a novel framework that reformulates MRIS as an autoregressive
next-token prediction task over a unified multimodal sequence of tokenized
image, text, and mask representations. This formulation streamlines model
design by eliminating the need for modality-specific fusion and external
segmentation models, supports a unified architecture for end-to-end training.
It also enables the use of pretrained tokenizers from emerging large-scale
multimodal models, enhancing generalization and adaptability. More importantly,
to address challenges under this formulation-such as exposure bias, long-tail
token distributions, and fine-grained lesion edges-we propose three novel
strategies: (1) a Next-k Token Prediction (NkTP) scheme to reduce cumulative
prediction errors, (2) Token-level Contrastive Learning (TCL) to enhance
boundary sensitivity and mitigate long-tail distribution effects, and (3) a
memory-based Hard Error Token (HET) optimization strategy that emphasizes
difficult tokens during training. Extensive experiments on the QaTa-COV19 and
MosMedData+ datasets demonstrate that NTP-MRISeg achieves new state-of-the-art
performance, offering a streamlined and effective alternative to traditional
MRIS pipelines.

</details>


### [27] [No Pose Estimation? No Problem: Pose-Agnostic and Instance-Aware Test-Time Adaptation for Monocular Depth Estimation](https://arxiv.org/abs/2511.05055)
*Mingyu Sung,Hyeonmin Choe,Il-Min Kim,Sangseok Yun,Jae Mo Kang*

Main category: cs.CV

TL;DR: A new test-time adaptation framework for monocular depth estimation, PITTA, achieves high performance under varying conditions using pose-agnostic adaptation, instance-aware masking of dynamic objects, and edge-enhanced cues.


<details>
  <summary>Details</summary>
Motivation: Monocular depth estimation suffers from domain shifts between training and real-world test environments. Test-time adaptation (TTA) is practical but existing self-supervised methods struggle in dynamic, diverse settings. There is a need for pose-agnostic TTA that does not rely on camera poses and can effectively handle moving objects.

Method: PITTA introduces two main strategies: (i) a pose-agnostic TTA paradigm for MDE that operates without camera pose information, and (ii) instance-aware masking that derives object-level masks for dynamic elements (vehicles, pedestrians) from a pretrained panoptic segmentation network, removing static background. An edge extraction component is also added to enhance input images and depth maps. The method is evaluated on DrivingStereo and Waymo datasets under varying conditions.

Result: PITTA surpasses existing state-of-the-art TTA methods for monocular depth estimation, delivering notable performance gains on DrivingStereo and Waymo across varying environmental conditions.

Conclusion: PITTA provides an effective, pose-agnostic TTA framework for MDE that leverages instance-aware masking and edge-enhanced cues to better handle dynamic scenes and domain shifts, achieving superior generalization under test-time variations.

Abstract: Monocular depth estimation (MDE), inferring pixel-level depths in single RGB
images from a monocular camera, plays a crucial and pivotal role in a variety
of AI applications demanding a three-dimensional (3D) topographical scene. In
the real-world scenarios, MDE models often need to be deployed in environments
with different conditions from those for training. Test-time (domain)
adaptation (TTA) is one of the compelling and practical approaches to address
the issue. Although there have been notable advancements in TTA for MDE,
particularly in a self-supervised manner, existing methods are still
ineffective and problematic when applied to diverse and dynamic environments.
To break through this challenge, we propose a novel and high-performing TTA
framework for MDE, named PITTA. Our approach incorporates two key innovative
strategies: (i) pose-agnostic TTA paradigm for MDE and (ii) instance-aware
image masking. Specifically, PITTA enables highly effective TTA on a pretrained
MDE network in a pose-agnostic manner without resorting to any camera pose
information. Besides, our instance-aware masking strategy extracts
instance-wise masks for dynamic objects (e.g., vehicles, pedestrians, etc.)
from a segmentation mask produced by a pretrained panoptic segmentation
network, by removing static objects including background components. To further
boost performance, we also present a simple yet effective edge extraction
methodology for the input image (i.e., a single monocular image) and depth map.
Extensive experimental evaluations on DrivingStereo and Waymo datasets with
varying environmental conditions demonstrate that our proposed framework,
PITTA, surpasses the existing state-of-the-art techniques with remarkable
performance improvements in MDE during TTA.

</details>


### [28] [Role-SynthCLIP: A Role Play Driven Diverse Synthetic Data Approach](https://arxiv.org/abs/2511.05057)
*Yuanxiang Huangfu,Chaochao Wang,Weilei Wang*

Main category: cs.CV

TL;DR: Role-SynthCLIP uses multi-perspective prompts to generate semantically diverse captions for CLIP training, achieving strong results with fewer synthetic pairs.


<details>
  <summary>Details</summary>
Motivation: To overcome limited semantic diversity and shallow captions in existing synthetic data, where simply increasing data volume yields diminishing returns for image-text alignment.

Method: Utilize Multimodal LLMs guided by role-playing prompts (e.g., compositional analyst, image-context interpreter) to generate diverse captions from distinct viewpoints, keeping the total number of image-text pairs constant and improving caption expressiveness and alignment.

Result: A CLIP-B/16 model trained on 1M Role-SynthCLIP pairs achieves Recall@1 64.1% on MS COCO val, beating the best synthetic baseline (trained on 5M pairs) by 2.8 percentage points.

Conclusion: Role-SynthCLIP demonstrates effective and efficient synthetic data generation that enhances semantic diversity and image-text alignment; code and models are released.

Abstract: The effectiveness of Contrastive Language-Image Pre-training (CLIP) models
critically depends on the semantic diversity and quality of their training
data. However, while existing synthetic data generation methods primarily focus
on increasing data volume, such emphasis often leads to limited semantic
diversity and redundant or shallow captions. To address this limitation, we
propose Role-SynthCLIP, a novel data synthesis framework that leverages
multi-perspective role-playing prompts (e.g., a compositional analyst, an
interpreter of image context) to guide Multimodal Large Language Models (MLLMs)
in generating semantically diverse captions from distinct viewpoints. This
mechanism enhances the semantic diversity and fine-grained image-text alignment
of synthetic pairs, thereby improving caption expressiveness and accuracy while
keeping the total number of image-text pairs unchanged. Experimental results
demonstrate the effectiveness and efficiency of our method. A CLIP-B/16 model
trained on only 1 million Role-SynthCLIP pairs achieves a Recall@1 of 64.1% on
the MS COCO validation set, surpassing the best existing synthetic data
baseline (trained on 5M pairs) by 2.8 percentage points. The code and trained
models are released at https://github.com/huangfu170/Role-SynthCLIP.

</details>


### [29] [SurgiATM: A Physics-Guided Plug-and-Play Model for Deep Learning-Based Smoke Removal in Laparoscopic Surgery](https://arxiv.org/abs/2511.05059)
*Mingyu Sheng,Jianan Fan,Dongnan Liu,Guoyan Zheng,Ron Kikinis,Weidong Cai*

Main category: cs.CV

TL;DR: SurgiATM is a lightweight, plug-and-play module that improves surgical smoke removal by bridging physics-based atmospheric modeling with data-driven DL, adding two hyperparameters and no extra trainable weights, yielding better accuracy and generalization across datasets and methods.


<details>
  <summary>Details</summary>
Motivation: Surgical smoke degrades endoscopic visualization, increasing error risk and hindering both clinical decision-making and computer-assisted analysis. A robust, generalizable desmoking approach is needed that can generalize across procedures with minimal overhead.

Method: Introduce SurgiATM as a statistical bridge between a physics-based atmospheric model and data-driven networks. It requires only two hyperparameters and adds no trainable weights, functioning as a plug-and-play module that can be integrated into existing desmoking architectures. Evaluated on three public surgical datasets across multiple procedures (cholecystectomy, partial nephrectomy, diaphragm dissection) and ten desmoking methods with various network architectures.

Result: Incorporation of SurgiATM consistently reduces restoration errors and enhances generalizability of existing desmoking models without adding trainable layers or weights, with minimal computational/architectural overhead. The method demonstrates broad compatibility across methods and procedures; code is released.

Conclusion: SurgiATM offers a convenient, low-cost, and effective enhancement for surgical desmoking, enabling easier integration into diverse pipelines and improving reliability for clinical decision-making and computer-assisted analysis.

Abstract: During laparoscopic surgery, smoke generated by tissue cauterization can
significantly degrade the visual quality of endoscopic frames, increasing the
risk of surgical errors and hindering both clinical decision-making and
computer-assisted visual analysis. Consequently, removing surgical smoke is
critical to ensuring patient safety and maintaining operative efficiency. In
this study, we propose the Surgical Atmospheric Model (SurgiATM) for surgical
smoke removal. SurgiATM statistically bridges a physics-based atmospheric model
and data-driven deep learning models, combining the superior generalizability
of the former with the high accuracy of the latter. Furthermore, SurgiATM is
designed as a lightweight, plug-and-play module that can be seamlessly
integrated into diverse surgical desmoking architectures to enhance their
accuracy and stability, better meeting clinical requirements. It introduces
only two hyperparameters and no additional trainable weights, preserving the
original network architecture with minimal computational and modification
overhead. We conduct extensive experiments on three public surgical datasets
with ten desmoking methods, involving multiple network architectures and
covering diverse procedures, including cholecystectomy, partial nephrectomy,
and diaphragm dissection. The results demonstrate that incorporating SurgiATM
commonly reduces the restoration errors of existing models and relatively
enhances their generalizability, without adding any trainable layers or
weights. This highlights the convenience, low cost, effectiveness, and
generalizability of the proposed method. The code for SurgiATM is released at
https://github.com/MingyuShengSMY/SurgiATM.

</details>


### [30] [Deep learning models are vulnerable, but adversarial examples are even more vulnerable](https://arxiv.org/abs/2511.05073)
*Jun Li,Yanwei Xu,Keran Li,Xiaoli Zhang*

Main category: cs.CV

TL;DR: Adversarial examples exhibit higher confidence volatility under occlusion than clean samples; the authors introduce SMCE to quantify this, and propose SWM-AED for detecting adversarial examples, achieving robust detection on CIFAR-10 across multiple attacks without overfitting, with accuracies typically above 62% and up to 96.5%.


<details>
  <summary>Details</summary>
Motivation: To understand intrinsic differences between adversarial and clean samples, focusing on occlusion sensitivity; to improve robustness and detection without costly adversarial training.

Method: Empirical CIFAR-10 study with nine attacks (e.g., FGSM, PGD); define Sliding Mask Confidence Entropy (SMCE) to measure confidence fluctuation under occlusion; use Mask Entropy Field Maps and distributions; propose Sliding Window Mask-based Adversarial Example Detection (SWM-AED) to mitigate overfitting; evaluate across classifiers and attacks.

Result: SMCE analyses show adversarial examples have significantly higher confidence volatility under occlusion; SWM-AED yields robust detection performance, with CIFAR-10 accuracy results typically >62% and up to 96.5% in some settings.

Conclusion: Occlusion-induced confidence volatility is a distinguishing feature of adversarial inputs; the SWM-AED method provides effective, training-light detection across attacks and classifiers, offering a practical defense improvement.

Abstract: Understanding intrinsic differences between adversarial examples and clean
samples is key to enhancing DNN robustness and detection against adversarial
attacks. This study first empirically finds that image-based adversarial
examples are notably sensitive to occlusion. Controlled experiments on CIFAR-10
used nine canonical attacks (e.g., FGSM, PGD) to generate adversarial examples,
paired with original samples for evaluation. We introduce Sliding Mask
Confidence Entropy (SMCE) to quantify model confidence fluctuation under
occlusion. Using 1800+ test images, SMCE calculations supported by Mask Entropy
Field Maps and statistical distributions show adversarial examples have
significantly higher confidence volatility under occlusion than originals.
Based on this, we propose Sliding Window Mask-based Adversarial Example
Detection (SWM-AED), which avoids catastrophic overfitting of conventional
adversarial training. Evaluations across classifiers and attacks on CIFAR-10
demonstrate robust performance, with accuracy over 62% in most cases and up to
96.5%.

</details>


### [31] [A Dual-stage Prompt-driven Privacy-preserving Paradigm for Person Re-Identification](https://arxiv.org/abs/2511.05092)
*Ruolin Li,Min Liu,Yuan Bian,Zhaoyang Li,Yuzhen Li,Xueping Wang,Yaonan Wang*

Main category: cs.CV

TL;DR: A two-stage prompt-driven framework (DPPP) creates a large synthetic Re-ID dataset (GenePerson) via diffusion prompts and learns domain-invariant features with a prompt-driven disentanglement mechanism (PDM), achieving state-of-the-art generalization.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns and limited applicability of real data push the use of virtual data for Re-ID. Existing synthetic datasets from game engines are hard to construct and suffer from poor domain generalization, hindering real-world transfer.

Method: Stage 1: Use multi-dimensional prompts (appearance, illumination, viewpoint) to drive diffusion models to synthesize diverse person images, building GenePerson with 130,519 images across 6,641 identities. Stage 2: Apply a Prompt-driven Disentanglement Mechanism (PDM) with contrastive learning; employ two textual inversion networks to map images to pseudo-words for style and content, creating style-disentangled content prompts to learn domain-invariant content features at the image level.

Result: Models trained on GenePerson with PDM achieve state-of-the-art generalization in Re-ID, outperforming popular real and virtual datasets.

Conclusion: The DPPP framework effectively leverages privacy-preserving synthetic data and style-content disentanglement to improve cross-domain generalization for person Re-ID, indicating strong potential for scalable, domain-generalizable synthetic datasets.

Abstract: With growing concerns over data privacy, researchers have started using
virtual data as an alternative to sensitive real-world images for training
person re-identification (Re-ID) models. However, existing virtual datasets
produced by game engines still face challenges such as complex construction and
poor domain generalization, making them difficult to apply in real scenarios.
To address these challenges, we propose a Dual-stage Prompt-driven
Privacy-preserving Paradigm (DPPP). In the first stage, we generate rich
prompts incorporating multi-dimensional attributes such as pedestrian
appearance, illumination, and viewpoint that drive the diffusion model to
synthesize diverse data end-to-end, building a large-scale virtual dataset
named GenePerson with 130,519 images of 6,641 identities. In the second stage,
we propose a Prompt-driven Disentanglement Mechanism (PDM) to learn
domain-invariant generalization features. With the aid of contrastive learning,
we employ two textual inversion networks to map images into pseudo-words
representing style and content, respectively, thereby constructing
style-disentangled content prompts to guide the model in learning
domain-invariant content features at the image level. Experiments demonstrate
that models trained on GenePerson with PDM achieve state-of-the-art
generalization performance, surpassing those on popular real and virtual Re-ID
datasets.

</details>


### [32] [Real-World Adverse Weather Image Restoration via Dual-Level Reinforcement Learning with High-Quality Cold Start](https://arxiv.org/abs/2511.05095)
*Fuyang Liu,Jiaqi Xu,Xiaowei Hu*

Main category: cs.CV

TL;DR: Proposes HFLS-Weather and a dual-level RL framework for robust perception under adverse weather, combining physics-driven data with local restoration learned via perturbation rewards and a global meta-controller for dynamic model orchestration; achieves state-of-the-art results and enables continuous adaptation.


<details>
  <summary>Details</summary>
Motivation: Real-world vision systems struggle to generalize to complex weather degradations. Synthetic datasets with fixed parameters fail to capture real-world variability. High-fidelity, physics-based data plus adaptive training can bridge the sim-to-real gap.

Method: 1) Construct HFLS-Weather, a physics-driven, high-fidelity weather dataset. 2) Initialize a dual-level reinforcement learning framework with HFLS-Weather for cold-start training. Local level: refine weather-specific restoration models via perturbation-driven image quality optimization with reward-based learning, without paired supervision. Global level: a meta-controller dynamically selects models and their execution order based on scene degradation.

Result: Achieves state-of-the-art performance across a wide range of adverse weather scenarios. Enables continuous adaptation to real-world conditions. Code is released at the provided GitHub link.

Conclusion: The framework effectively addresses generalization to adverse weather by combining high-fidelity data, perturbation-driven local learning, and a global orchestration strategy, reducing reliance on paired supervision and supporting ongoing adaptation to real-world conditions.

Abstract: Adverse weather severely impairs real-world visual perception, while existing
vision models trained on synthetic data with fixed parameters struggle to
generalize to complex degradations. To address this, we first construct
HFLS-Weather, a physics-driven, high-fidelity dataset that simulates diverse
weather phenomena, and then design a dual-level reinforcement learning
framework initialized with HFLS-Weather for cold-start training. Within this
framework, at the local level, weather-specific restoration models are refined
through perturbation-driven image quality optimization, enabling reward-based
learning without paired supervision; at the global level, a meta-controller
dynamically orchestrates model selection and execution order according to scene
degradation. This framework enables continuous adaptation to real-world
conditions and achieves state-of-the-art performance across a wide range of
adverse weather scenarios. Code is available at
https://github.com/xxclfy/AgentRL-Real-Weather

</details>


### [33] [Early Alzheimer's Disease Detection from Retinal OCT Images: A UK Biobank Study](https://arxiv.org/abs/2511.05106)
*Yasemin Turkan,F. Boray Tek,M. Serdar Nazlı,Öykü Eren*

Main category: cs.CV

TL;DR: DL on raw OCT B-scans for early AD detection shows baseline feasibility; AUC ~0.62 in 4-year window; not yet clinically actionable; larger datasets and multimodal approaches needed.


<details>
  <summary>Details</summary>
Motivation: Move beyond segmented retinal thickness measures to end-to-end DL on raw OCT data for preclinical AD detection; address the challenge that early-stage signals precede clinical diagnosis by years.

Method: Fine-tuned multiple pretrained models (ImageNet-based and OCT-specific RETFound transformer) on raw OCT B-scans. Subject-level cross-validation on UK Biobank data matched for age, sex, and imaging instances. Applied standard and OCT-specific augmentation to reduce overfitting and used a year-weighted loss to emphasize cases diagnosed within four years.

Result: ResNet-34 yielded the most stable performance with AUC 0.62 in the 4-year cohort. Explainability analyses revealed localized differences in the central macular subfield between AD and controls. Findings establish a baseline for OCT-based AD prediction but indicate current limitations for clinical deployment.

Conclusion: Demonstrates feasibility of predicting AD risk from raw OCT B-scans, but performance is insufficient for clinical use. Highlights the need for larger datasets and multimodal approaches to capture subtle retinal biomarkers years before AD diagnosis.

Abstract: Alterations in retinal layer thickness, measurable using Optical Coherence
Tomography (OCT), have been associated with neurodegenerative diseases such as
Alzheimer's disease (AD). While previous studies have mainly focused on
segmented layer thickness measurements, this study explored the direct
classification of OCT B-scan images for the early detection of AD. To our
knowledge, this is the first application of deep learning to raw OCT B-scans
for AD prediction in the literature. Unlike conventional medical image
classification tasks, early detection is more challenging than diagnosis
because imaging precedes clinical diagnosis by several years. We fine-tuned and
evaluated multiple pretrained models, including ImageNet-based networks and the
OCT-specific RETFound transformer, using subject-level cross-validation
datasets matched for age, sex, and imaging instances from the UK Biobank
cohort. To reduce overfitting in this small, high-dimensional dataset, both
standard and OCT-specific augmentation techniques were applied, along with a
year-weighted loss function that prioritized cases diagnosed within four years
of imaging. ResNet-34 produced the most stable results, achieving an AUC of
0.62 in the 4-year cohort. Although below the threshold for clinical
application, our explainability analyses confirmed localized structural
differences in the central macular subfield between the AD and control groups.
These findings provide a baseline for OCT-based AD prediction, highlight the
challenges of detecting subtle retinal biomarkers years before AD diagnosis,
and point to the need for larger datasets and multimodal approaches.

</details>


### [34] [SnowyLane: Robust Lane Detection on Snow-covered Rural Roads Using Infrastructural Elements](https://arxiv.org/abs/2511.05108)
*Jörg Gamerdinger,Benedict Wetzel,Patrick Schulz,Sven Teufel,Oliver Bringmann*

Main category: cs.CV

TL;DR: A robust, real-time lane-detection method for winter driving that does not rely on traditional lane markings but on roadside delineators; uses a Bezier-curve model to fit the lane trajectory from detected posts; introduces SnowyLane, a synthetic dataset with 80,000 annotated frames under varying snow and lighting conditions; reports improved robustness under heavy snow occlusion and provides a resource for all-weather autonomous driving research.


<details>
  <summary>Details</summary>
Motivation: Winter driving often occludes or eliminates lane markings, undermining traditional lane-detection systems. By leveraging vertical roadside posts (delineators) as indirect lane cues and fitting a smooth Bezier lane model, the approach aims to maintain reliable lane estimation in snow and adverse weather. The SnowyLane dataset supports training and evaluation in these conditions.

Method: Detect vertical roadside delineators from sensor data; fit a smooth lane trajectory using a parameterized Bezier curve, guided by spatial consistency and road geometry; operate in real time and evaluated on the SnowyLane dataset.

Result: Compared to state-of-the-art lane-detection systems, the method shows significantly improved robustness in adverse weather, particularly under heavy snow occlusion, and achieves real-time performance. The SnowyLane dataset provides a valuable resource for future research.

Conclusion: The approach establishes a strong foundation for reliable lane detection in winter scenarios by exploiting roadside cues and Bezier-based lane modeling, and contributes a large synthetic dataset to support ongoing research in all-weather autonomous driving.

Abstract: Lane detection for autonomous driving in snow-covered environments remains a
major challenge due to the frequent absence or occlusion of lane markings. In
this paper, we present a novel, robust and realtime capable approach that
bypasses the reliance on traditional lane markings by detecting roadside
features,specifically vertical roadside posts called delineators, as indirect
lane indicators. Our method first perceives these posts, then fits a smooth
lane trajectory using a parameterized Bezier curve model, leveraging spatial
consistency and road geometry. To support training and evaluation in these
challenging scenarios, we introduce SnowyLane, a new synthetic dataset
containing 80,000 annotated frames capture winter driving conditions, with
varying snow coverage, and lighting conditions. Compared to state-of-the-art
lane detection systems, our approach demonstrates significantly improved
robustness in adverse weather, particularly in cases with heavy snow occlusion.
This work establishes a strong foundation for reliable lane detection in winter
scenarios and contributes a valuable resource for future research in
all-weather autonomous driving. The dataset is available at
https://ekut-es.github.io/snowy-lane

</details>


### [35] [From Linear Probing to Joint-Weighted Token Hierarchy: A Foundation Model Bridging Global and Cellular Representations in Biomarker Detection](https://arxiv.org/abs/2511.05150)
*Jingsong Liu,Han Li,Nassir Navab,Peter J. Schüffler*

Main category: cs.CV

TL;DR: JWTH fuses local cell-level tokens with global patch tokens via a joint-weighted token hierarchy, enabling improved AI-based biomarker detection from H&E slides.


<details>
  <summary>Details</summary>
Motivation: Pathology foundation models (PFMs) often rely on global patch-level embeddings and overlook cell-level morphology, limiting accuracy and interpretability across biomarkers and cohorts; there is a need to integrate cell-level cues into scalable PFMs.

Method: JWTH combines large-scale self-supervised pretraining with cell-centric post-tuning and attention pooling to fuse local (cell-level) and global (patch-level) tokens into a joint-weighted token hierarchy.

Result: Across four biomarker tasks and eight cohorts, JWTH achieves up to 8.3% higher balanced accuracy and a 1.2% average improvement over prior PFMs, demonstrating improved accuracy and robustness.

Conclusion: JWTH advances interpretable and robust AI-based biomarker detection in digital pathology by integrating local cell morphology with global context through a joint-weighted token hierarchy.

Abstract: AI-based biomarkers can infer molecular features directly from hematoxylin &
eosin (H&E) slides, yet most pathology foundation models (PFMs) rely on global
patch-level embeddings and overlook cell-level morphology. We present a PFM
model, JWTH (Joint-Weighted Token Hierarchy), which integrates large-scale
self-supervised pretraining with cell-centric post-tuning and attention pooling
to fuse local and global tokens. Across four tasks involving four biomarkers
and eight cohorts, JWTH achieves up to 8.3% higher balanced accuracy and 1.2%
average improvement over prior PFMs, advancing interpretable and robust
AI-based biomarker detection in digital pathology.

</details>


### [36] [Splatography: Sparse multi-view dynamic Gaussian Splatting for filmmaking challenges](https://arxiv.org/abs/2511.05152)
*Adrian Azzarelli,Nantheera Anantrasirichai,David R Bull*

Main category: cs.CV

TL;DR: Foreground/background split of Deformable Gaussian Splatting with separate pre-training and deformation parameters enables high-quality, segmented dynamic reconstructions under sparse camera setups, achieving SotA results with reduced model size and without dense mask supervision.


<details>
  <summary>Details</summary>
Motivation: In filmmaking, budgets often yield sparse multi-view video, which challenges state-of-the-art dynamic reconstruction methods and their ability to accurately capture complex dynamic features. There is a need for robust methods that can operate under sparse camera configurations and provide interpretable segmentation of dynamic content, including transparent textures.

Method: Split the canonical Gaussian Splatting (GS) representation into foreground and background using sparse masks available at t=0. Pre-train each representation with separate losses for canonical stages. During dynamic training, learn distinct deformation parameters for foreground and background: foreground models dynamic color, position, and rotation changes; background (e.g., film crew and equipment) is typically dim and less dynamic, so only position changes are learned. This approach reduces reliance on dense masks and adapts to sparse camera configurations common in filmmaking.

Result: Achieves state-of-the-art qualitative and quantitative results on 3-D and 2.5-D entertainment datasets, including up to 3 PSNR improvements with about 50% model size on 3-D scenes. The method also yields segmented dynamic reconstructions, including transparent and dynamic textures, without requiring dense mask supervision.

Conclusion: Extends Deformable Gaussian Splatting to sparse-camera filmmaking scenarios by explicitly modeling foreground and background dynamics with separate training and deformation parameters. This enables high-quality dynamic reconstructions and interpretable segmentation under budget-constrained capture setups; code and video comparisons are available online.

Abstract: Deformable Gaussian Splatting (GS) accomplishes photorealistic dynamic 3-D
reconstruction from dense multi-view video (MVV) by learning to deform a
canonical GS representation. However, in filmmaking, tight budgets can result
in sparse camera configurations, which limits state-of-the-art (SotA) methods
when capturing complex dynamic features. To address this issue, we introduce an
approach that splits the canonical Gaussians and deformation field into
foreground and background components using a sparse set of masks for frames at
t=0. Each representation is separately trained on different loss functions
during canonical pre-training. Then, during dynamic training, different
parameters are modeled for each deformation field following common filmmaking
practices. The foreground stage contains diverse dynamic features so changes in
color, position and rotation are learned. While, the background containing
film-crew and equipment, is typically dimmer and less dynamic so only changes
in point position are learned. Experiments on 3-D and 2.5-D entertainment
datasets show that our method produces SotA qualitative and quantitative
results; up to 3 PSNR higher with half the model size on 3-D scenes. Unlike the
SotA and without the need for dense mask supervision, our method also produces
segmented dynamic reconstructions including transparent and dynamic textures.
Code and video comparisons are available online:
https://interims-git.github.io/

</details>


### [37] [Another BRIXEL in the Wall: Towards Cheaper Dense Features](https://arxiv.org/abs/2511.05168)
*Alexander Lappe,Martin A. Giese*

Main category: cs.CV

TL;DR: BRIXEL is a distillation method letting a student reproduce high-resolution dense feature maps with far less compute, outperforming DINOv3 baselines at fixed resolution.


<details>
  <summary>Details</summary>
Motivation: Reduce compute and high-resolution input requirements for dense, transformer-based vision models while preserving fine-grained feature maps.

Method: Knowledge distillation where the student learns to reproduce its own feature maps at higher resolution, effectively upscaling and aligning dense representations with coaching from high-resolution targets (teacher or self-generated). The approach is simple and architecture-agnostic; yields high-res maps with lower cost.

Result: At fixed resolution, BRIXEL significantly outperforms baseline DINOv3 on downstream tasks and produces feature maps close to the teacher’s with a fraction of computation.

Conclusion: BRIXEL provides an efficient route to dense, high-resolution visual representations, achieving strong performance with reduced compute; code and weights released.

Abstract: Vision foundation models achieve strong performance on both global and
locally dense downstream tasks. Pretrained on large images, the recent DINOv3
model family is able to produce very fine-grained dense feature maps, enabling
state-of-the-art performance. However, computing these feature maps requires
the input image to be available at very high resolution, as well as large
amounts of compute due to the squared complexity of the transformer
architecture. To address these issues, we propose BRIXEL, a simple knowledge
distillation approach that has the student learn to reproduce its own feature
maps at higher resolution. Despite its simplicity, BRIXEL outperforms the
baseline DINOv3 models by large margins on downstream tasks when the resolution
is kept fixed. Moreover, it is able to produce feature maps that are very
similar to those of the teacher at a fraction of the computational cost. Code
and model weights are available at https://github.com/alexanderlappe/BRIXEL.

</details>


### [38] [MUSE: Multi-Scale Dense Self-Distillation for Nucleus Detection and Classification](https://arxiv.org/abs/2511.05170)
*Zijiang Yang,Hanqing Chao,Bokai Zhao,Yelin Yang,Yunshuo Zhang,Dongmei Fu,Junping Zhang,Le Lu,Ke Yan,Dakai Jin,Minfeng Xu,Yun Bian,Hui Jiang*

Main category: cs.CV

TL;DR: MUSE is a self-supervised framework for nucleus detection and classification that uses NuLo, a nucleus-guided local self-distillation, enabling cross-scale local representations and achieving state-of-the-art results without heavy annotations.


<details>
  <summary>Details</summary>
Motivation: Histopathology nucleus-level annotations are labor-intensive, and existing methods fail to fully exploit large unlabeled data for discriminative nucleus representations.

Method: MUSE introduces NuLo (Nucleus-based Local self-distillation): a coordinate-guided mechanism for flexible local distillation based on predicted nucleus positions, relaxing strict spatial alignment between augmented views to enable cross-scale alignment. It employs an encoder–decoder backbone and a large field-of-view semi-supervised fine-tuning strategy to maximize unlabeled data value.

Result: Extensive experiments on three benchmarks show MUSE surpasses state-of-the-art supervised baselines and generic pathology foundation models.

Conclusion: MUSE addresses core challenges in nucleus detection and classification by leveraging unlabeled data to learn fine-grained nucleus representations, achieving strong, transferable performance and outperforming existing methods.

Abstract: Nucleus detection and classification (NDC) in histopathology analysis is a
fundamental task that underpins a wide range of high-level pathology
applications. However, existing methods heavily rely on labor-intensive
nucleus-level annotations and struggle to fully exploit large-scale unlabeled
data for learning discriminative nucleus representations. In this work, we
propose MUSE (MUlti-scale denSE self-distillation), a novel self-supervised
learning method tailored for NDC. At its core is NuLo (Nucleus-based Local
self-distillation), a coordinate-guided mechanism that enables flexible local
self-distillation based on predicted nucleus positions. By removing the need
for strict spatial alignment between augmented views, NuLo allows critical
cross-scale alignment, thus unlocking the capacity of models for fine-grained
nucleus-level representation. To support MUSE, we design a simple yet effective
encoder-decoder architecture and a large field-of-view semi-supervised
fine-tuning strategy that together maximize the value of unlabeled pathology
images. Extensive experiments on three widely used benchmarks demonstrate that
MUSE effectively addresses the core challenges of histopathological NDC. The
resulting models not only surpass state-of-the-art supervised baselines but
also outperform generic pathology foundation models.

</details>


### [39] [Walk the Lines 2: Contour Tracking for Detailed Segmentation](https://arxiv.org/abs/2511.05210)
*André Peter Kelm,Max Braeschke,Emre Gülsoylu,Simone Frintrop*

Main category: cs.CV

TL;DR: WtL2 extends Walk the Lines to infrared and RGB object segmentation by replacing NMS with contour tracking, refining contours to 1-pixel-wide closed shapes that binarize for segmentation. It claims high detail and IoU, broadening WtL's applicability to IR ships and diverse RGB objects.


<details>
  <summary>Details</summary>
Motivation: To achieve detailed, high-quality segmentation for infrared ships and a broader set of RGB objects, addressing limitations of NMS-based contour methods and enabling precise, closed contours with fine details.

Method: Adapt the input to IR ships and broaden to diverse RGB objects. Replace standard NMS with contour-tracking that iteratively refines object contours until a 1-pixel-wide closed contour forms and can be binarized, yielding a segmentable foreground region.

Result: The approach reportedly outperforms the latest contour-based methods when achieving a closed object contour, with high peak IoU and detailed boundaries, indicating strong performance on IR ships and varied RGB objects.

Conclusion: WtL2 broadens the Origianl WtL's scope, offering a specialized, high-detail contour-based segmentation method suitable for niche applications and potentially accelerating progress in segmentation research and practice.

Abstract: This paper presents Walk the Lines 2 (WtL2), a unique contour tracking
algorithm specifically adapted for detailed segmentation of infrared (IR) ships
and various objects in RGB.1 This extends the original Walk the Lines (WtL)
[12], which focused solely on detailed ship segmentation in color. These
innovative WtLs can replace the standard non-maximum suppression (NMS) by using
contour tracking to refine the object contour until a 1-pixel-wide closed shape
can be binarized, forming a segmentable area in foreground-background
scenarios. WtL2 broadens the application range of WtL beyond its original
scope, adapting to IR and expanding to diverse objects within the RGB context.
To achieve IR segmentation, we adapt its input, the object contour detector, to
IR ships. In addition, the algorithm is enhanced to process a wide range of RGB
objects, outperforming the latest generation of contour-based methods when
achieving a closed object contour, offering high peak Intersection over Union
(IoU) with impressive details. This positions WtL2 as a compelling method for
specialized applications that require detailed segmentation or high-quality
samples, potentially accelerating progress in several niche areas of image
segmentation.

</details>


### [40] [FreeControl: Efficient, Training-Free Structural Control via One-Step Attention Extraction](https://arxiv.org/abs/2511.05219)
*Jiang Lin,Xinyu Chen,Song Wu,Zhiqiu Zhang,Jizhi Zhang,Ye Wang,Qiang Tang,Qian Wang,Jian Yang,Zili Yi*

Main category: cs.CV

TL;DR: Training-free, test-time control for diffusion models via one-shot attention extraction and Latent-Condition Decoupling, enabling compositional, structurally aligned generation with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Diffusion models offer high-quality images but struggle with precise spatial and semantic control. Existing methods rely on retraining (e.g., ControlNet) or expensive inversion-based approaches, limiting flexibility, speed, and generalizability.

Method: FreeControl uses one-step attention extraction from a single optimally chosen key timestep and reuses it across denoising. Latent-Condition Decoupling (LCD) separates the key timestep from the noised latent used in attention extraction. It supports test-time compositional control via reference images and provides approximately 5% extra computational cost, compatible with modern diffusion models.

Result: Achieves better structural and semantic alignment without inversion or retraining, with improved stability and image quality. Enables compositional scene design from multiple references and stronger prompt alignment, at low additional cost.

Conclusion: Introduces a new test-time control paradigm for diffusion models, delivering structurally and semantically guided generation directly from raw images with minimal overhead and broad compatibility.

Abstract: Controlling the spatial and semantic structure of diffusion-generated images
remains a challenge. Existing methods like ControlNet rely on handcrafted
condition maps and retraining, limiting flexibility and generalization.
Inversion-based approaches offer stronger alignment but incur high inference
cost due to dual-path denoising. We present FreeControl, a training-free
framework for semantic structural control in diffusion models. Unlike prior
methods that extract attention across multiple timesteps, FreeControl performs
one-step attention extraction from a single, optimally chosen key timestep and
reuses it throughout denoising. This enables efficient structural guidance
without inversion or retraining. To further improve quality and stability, we
introduce Latent-Condition Decoupling (LCD): a principled separation of the key
timestep and the noised latent used in attention extraction. LCD provides finer
control over attention quality and eliminates structural artifacts. FreeControl
also supports compositional control via reference images assembled from
multiple sources - enabling intuitive scene layout design and stronger prompt
alignment. FreeControl introduces a new paradigm for test-time control,
enabling structurally and semantically aligned, visually coherent generation
directly from raw images, with the flexibility for intuitive compositional
design and compatibility with modern diffusion models at approximately 5
percent additional cost.

</details>


### [41] [4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos](https://arxiv.org/abs/2511.05229)
*Mengqi Guo,Bo Xu,Yanyan Li,Gim Hee Lee*

Main category: cs.CV

TL;DR: Pose-free 4D dynamic neural rendering for monocular videos, using MA-BA and MA-GS to decouple static and dynamic content, achieving higher quality with fewer computations.


<details>
  <summary>Details</summary>
Motivation: Dynamic scenes with unknown camera poses are hard for NeRF/3DGS; dynamic content and pose estimation are major bottlenecks, limiting realism in monocular video synthesis.

Method: Two-stage pipeline: (1) initialize pose/geometry with 3D foundational models; (2) refine motion. MA-BA blends transformer-based priors with SAM2-based dynamic segmentation for robust pose refinement; MA-GS employs control points, a deformation-field MLP, and linear blend skinning to model dynamic motion efficiently.

Result: On real-world dynamic datasets, up to 1.8 dB PSNR improvement over state-of-the-art; especially effective with large dynamic objects. Computational cost reduced by about 5x compared to previous dynamic scene representations.

Conclusion: 4D3R enables pose-free dynamic neural rendering by decoupling static and dynamic components in a two-stage framework, delivering improved quality and efficiency through MA-BA and MA-GS.

Abstract: Novel view synthesis from monocular videos of dynamic scenes with unknown
camera poses remains a fundamental challenge in computer vision and graphics.
While recent advances in 3D representations such as Neural Radiance Fields
(NeRF) and 3D Gaussian Splatting (3DGS) have shown promising results for static
scenes, they struggle with dynamic content and typically rely on pre-computed
camera poses. We present 4D3R, a pose-free dynamic neural rendering framework
that decouples static and dynamic components through a two-stage approach. Our
method first leverages 3D foundational models for initial pose and geometry
estimation, followed by motion-aware refinement. 4D3R introduces two key
technical innovations: (1) a motion-aware bundle adjustment (MA-BA) module that
combines transformer-based learned priors with SAM2 for robust dynamic object
segmentation, enabling more accurate camera pose refinement; and (2) an
efficient Motion-Aware Gaussian Splatting (MA-GS) representation that uses
control points with a deformation field MLP and linear blend skinning to model
dynamic motion, significantly reducing computational cost while maintaining
high-quality reconstruction. Extensive experiments on real-world dynamic
datasets demonstrate that our approach achieves up to 1.8dB PSNR improvement
over state-of-the-art methods, particularly in challenging scenarios with large
dynamic objects, while reducing computational requirements by 5x compared to
previous dynamic scene representations.

</details>


### [42] [Accurate online action and gesture recognition system using detectors and Deep SPD Siamese Networks](https://arxiv.org/abs/2511.05250)
*Mohamed Sanim Akremi,Rim Slama,Hedi Tabia*

Main category: cs.CV

TL;DR: An online, end-to-end skeleton-based motion recognition system with a detector and classifier that operates on streaming data using SPD representations and a Siamese network to locate intervals and classify actions, achieving state-of-the-art performance on gesture and action benchmarks.


<details>
  <summary>Details</summary>
Motivation: Online continuous motion recognition is more practical than segment-based methods; skeleton-based approaches are powerful but often assume pre-segmented sequences, limiting real-time applicability.

Method: A two-component system: detector + classifier. Uses Semi-Positive Definite (SPD) matrix representations to capture statistics of skeletal data and a Siamese network to learn semantic similarity, enabling interval detection in unsegmented streams and per-interval action classification.

Result: Extensive experiments on hand gesture and body action benchmarks show high accuracy, with performances surpassing state-of-the-art in most cases.

Conclusion: The proposed SPD-Siamese online framework effectively enables continuous online recognition from skeletal streams, providing accurate interval detection and action classification in online settings.

Abstract: Online continuous motion recognition is a hot topic of research since it is
more practical in real life application cases. Recently, Skeleton-based
approaches have become increasingly popular, demonstrating the power of using
such 3D temporal data. However, most of these works have focused on
segment-based recognition and are not suitable for the online scenarios. In
this paper, we propose an online recognition system for skeleton sequence
streaming composed from two main components: a detector and a classifier, which
use a Semi-Positive Definite (SPD) matrix representation and a Siamese network.
The powerful statistical representations for the skeletal data given by the SPD
matrices and the learning of their semantic similarity by the Siamese network
enable the detector to predict time intervals of the motions throughout an
unsegmented sequence. In addition, they ensure the classifier capability to
recognize the motion in each predicted interval. The proposed detector is
flexible and able to identify the kinetic state continuously. We conduct
extensive experiments on both hand gesture and body action recognition
benchmarks to prove the accuracy of our online recognition system which in most
cases outperforms state-of-the-art performances.

</details>


### [43] [ADPretrain: Advancing Industrial Anomaly Detection via Anomaly Representation Pretraining](https://arxiv.org/abs/2511.05245)
*Xincheng Yao,Yan Luo,Zefeng Qian,Chongyang Zhang*

Main category: cs.CV

TL;DR: Proposes a pretrained anomaly-detection-specific representation learned via angle- and norm-oriented contrastive losses on RealIAD, evaluated by replacing features in five AD methods across five datasets/backbones, showing consistent gains over ImageNet-pretrained features; code released.


<details>
  <summary>Details</summary>
Motivation: ImageNet pretraining is not tailored for anomaly detection and may not generalize well due to distribution shift between natural images and industrial AD data; there is a need for representations specifically designed for AD tasks.

Method: Pretrain on RealIAD using angle-oriented contrastive loss (maximize angle between normal and abnormal features) and norm-oriented contrastive loss (maximize norm difference). Use residual, class-generalizable representations. Then evaluate by substituting pretrained features into five embedding-based AD methods across multiple datasets/backbones.

Result: Extensive experiments across five AD datasets and five backbones show that the pretrained AD representations consistently outperform ImageNet-pretrained features when embedded into five AD methods.

Conclusion: AD-specific pretrained representations mitigate distribution shift and improve performance over ImageNet-based features; RealIAD is effective for pretraining; the approach is generalizable to various embedding-based AD methods and datasets; code is available.

Abstract: The current mainstream and state-of-the-art anomaly detection (AD) methods
are substantially established on pretrained feature networks yielded by
ImageNet pretraining. However, regardless of supervised or self-supervised
pretraining, the pretraining process on ImageNet does not match the goal of
anomaly detection (i.e., pretraining in natural images doesn't aim to
distinguish between normal and abnormal). Moreover, natural images and
industrial image data in AD scenarios typically have the distribution shift.
The two issues can cause ImageNet-pretrained features to be suboptimal for AD
tasks. To further promote the development of the AD field, pretrained
representations specially for AD tasks are eager and very valuable. To this
end, we propose a novel AD representation learning framework specially designed
for learning robust and discriminative pretrained representations for
industrial anomaly detection. Specifically, closely surrounding the goal of
anomaly detection (i.e., focus on discrepancies between normals and anomalies),
we propose angle- and norm-oriented contrastive losses to maximize the angle
size and norm difference between normal and abnormal features simultaneously.
To avoid the distribution shift from natural images to AD images, our
pretraining is performed on a large-scale AD dataset, RealIAD. To further
alleviate the potential shift between pretraining data and downstream AD
datasets, we learn the pretrained AD representations based on the
class-generalizable representation, residual features. For evaluation, based on
five embedding-based AD methods, we simply replace their original features with
our pretrained representations. Extensive experiments on five AD datasets and
five backbones consistently show the superiority of our pretrained features.
The code is available at https://github.com/xcyao00/ADPretrain.

</details>


### [44] [OregairuChar: A Benchmark Dataset for Character Appearance Frequency Analysis in My Teen Romantic Comedy SNAFU](https://arxiv.org/abs/2511.05263)
*Qi Sun,Dingju Zhou,Lina Zhang*

Main category: cs.CV

TL;DR: Dataset and benchmarking for appearance-frequency analysis of characters in the anime Oregairu (My Teen Romantic Comedy SNAFU), enabling episode-level narrative insights.


<details>
  <summary>Details</summary>
Motivation: Understanding narrative structure, character prominence, and story progression in stylized media; current lack of annotated datasets for appearance frequency.

Method: Construct OregairuChar with 1600 manually selected frames from season 3, annotated with 2860 bounding boxes for 11 main characters; evaluate multiple object detectors on the dataset and perform episode-level analysis of character presence over time.

Result: Object detectors yield predictions enabling fine-grained analysis of character presence; patterns of prominence and their evolution within episodes can be observed.

Conclusion: OregairuChar is a valuable resource for computational narrative dynamics and character-centric storytelling in stylized media, enabling appearance-frequency studies.

Abstract: The analysis of character appearance frequency is essential for understanding
narrative structure, character prominence, and story progression in anime. In
this work, we introduce OregairuChar, a benchmark dataset designed for
appearance frequency analysis in the anime series My Teen Romantic Comedy
SNAFU. The dataset comprises 1600 manually selected frames from the third
season, annotated with 2860 bounding boxes across 11 main characters.
OregairuChar captures diverse visual challenges, including occlusion, pose
variation, and inter-character similarity, providing a realistic basis for
appearance-based studies. To enable quantitative research, we benchmark several
object detection models on the dataset and leverage their predictions for
fine-grained, episode-level analysis of character presence over time. This
approach reveals patterns of character prominence and their evolution within
the narrative. By emphasizing appearance frequency, OregairuChar serves as a
valuable resource for exploring computational narrative dynamics and
character-centric storytelling in stylized media.

</details>


### [45] [Automatic segmentation of colorectal liver metastases for ultrasound-based navigated resection](https://arxiv.org/abs/2511.05253)
*Tiziano Natali,Karin A. Olthof,Niels F. M. Kok,Koert F. D. Kuhlmann,Theo J. M. Ruers,Matteo Fusaglia*

Main category: cs.CV

TL;DR: Cropped-region 3D U-Net on intraoperative 3D ultrasound enables near real-time automated CRLM delineation with improved accuracy over full-volume approaches.


<details>
  <summary>Details</summary>
Motivation: To improve intraoperative delineation of colorectal liver metastases (CRLM) by reducing noise and operator dependency in iUS, enabling automated, efficient navigation during hepatic surgery.

Method: Train/evaluate a 3D U-Net (nnU-Net framework) on 85 tracked 3D iUS volumes from CRLM patients; compare full-volume vs cropped-tumor regions; evaluate with DSC, HDist, and RVD on retrospective and prospective data; integrate into 3D Slicer for real-time use.

Result: Cropped-volume model outperforms full-volume (AUC-ROC 0.898 vs 0.718); median DSC 0.74, recall 0.79, HDist 17.1 mm; ~1 min per inference; robust in prospective intraoperative testing; clinically acceptable accuracy for real-time guidance.

Conclusion: Automatic cropping-based 3D U-Net segmentation provides reliable, near real-time CRLM delineation in iUS, enabling registration-free ultrasound-guided navigation with reduced manual workload and procedure time.

Abstract: Introduction: Accurate intraoperative delineation of colorectal liver
metastases (CRLM) is crucial for achieving negative resection margins but
remains challenging using intraoperative ultrasound (iUS) due to low contrast,
noise, and operator dependency. Automated segmentation could enhance precision
and efficiency in ultrasound-based navigation workflows.
  Methods: Eighty-five tracked 3D iUS volumes from 85 CRLM patients were used
to train and evaluate a 3D U-Net implemented via the nnU-Net framework. Two
variants were compared: one trained on full iUS volumes and another on cropped
regions around tumors. Segmentation accuracy was assessed using Dice Similarity
Coefficient (DSC), Hausdorff Distance (HDist.), and Relative Volume Difference
(RVD) on retrospective and prospective datasets. The workflow was integrated
into 3D Slicer for real-time intraoperative use.
  Results: The cropped-volume model significantly outperformed the full-volume
model across all metrics (AUC-ROC = 0.898 vs 0.718). It achieved median DSC =
0.74, recall = 0.79, and HDist. = 17.1 mm comparable to semi-automatic
segmentation but with ~4x faster execution (~ 1 min). Prospective
intraoperative testing confirmed robust and consistent performance, with
clinically acceptable accuracy for real-time surgical guidance.
  Conclusion: Automatic 3D segmentation of CRLM in iUS using a cropped 3D U-Net
provides reliable, near real-time results with minimal operator input. The
method enables efficient, registration-free ultrasound-based navigation for
hepatic surgery, approaching expert-level accuracy while substantially reducing
manual workload and procedure time.

</details>


### [46] [DeepEyesV2: Toward Agentic Multimodal Model](https://arxiv.org/abs/2511.05271)
*Jack Hong,Chenxiao Zhao,ChengLin Zhu,Weiheng Lu,Guohai Xu,Xing Yu*

Main category: cs.CV

TL;DR: DeepEyesV2 proposes an agentic multimodal model that learns to actively use external tools via a two-stage training pipeline and a new RealX-Bench benchmark, achieving robust, context-aware tool use across perception and reasoning.


<details>
  <summary>Details</summary>
Motivation: To enable multimodal models to go beyond passive understanding and become capable of orchestrating tool use (code, search) to handle real-world, tool-requiring tasks.

Method: Two-stage training: cold-start to bootstrap tool-use patterns, then reinforcement learning to refine tool invocation; curated dataset emphasizing beneficial tool use; RealX-Bench benchmark for evaluation; analysis of task-adaptive tool invocation; RL enables complex tool combinations and context-based selection.

Result: DeepEyesV2 demonstrates effectiveness across real-world understanding, mathematical reasoning, and search-intensive tasks; shows task-adaptive tool usage (image operations for perception, numerical computations for reasoning); RL improves tool repertoire and selective invocation.

Conclusion: The study provides practical guidance for building agentic multimodal models and highlights how structured data and staged training can cultivate robust tool use in multimodal reasoning.

Abstract: Agentic multimodal models should not only comprehend text and images, but
also actively invoke external tools, such as code execution environments and
web search, and integrate these operations into reasoning. In this work, we
introduce DeepEyesV2 and explore how to build an agentic multimodal model from
the perspectives of data construction, training methods, and model evaluation.
We observe that direct reinforcement learning alone fails to induce robust
tool-use behavior. This phenomenon motivates a two-stage training pipeline: a
cold-start stage to establish tool-use patterns, and reinforcement learning
stage to further refine tool invocation. We curate a diverse, moderately
challenging training dataset, specifically including examples where tool use is
beneficial. We further introduce RealX-Bench, a comprehensive benchmark
designed to evaluate real-world multimodal reasoning, which inherently requires
the integration of multiple capabilities, including perception, search, and
reasoning. We evaluate DeepEyesV2 on RealX-Bench and other representative
benchmarks, demonstrating its effectiveness across real-world understanding,
mathematical reasoning, and search-intensive tasks. Moreover, DeepEyesV2
exhibits task-adaptive tool invocation, tending to use image operations for
perception tasks and numerical computations for reasoning tasks. Reinforcement
learning further enables complex tool combinations and allows model to
selectively invoke tools based on context. We hope our study can provide
guidance for community in developing agentic multimodal models.

</details>


### [47] [LiveStar: Live Streaming Assistant for Real-World Online Video Understanding](https://arxiv.org/abs/2511.05299)
*Zhenyu Yang,Kairui Zhang,Yuhang Hu,Bing Wang,Shengsheng Qian,Bin Wen,Fan Yang,Tingting Gao,Weiming Dong,Changsheng Xu*

Main category: cs.CV

TL;DR: LiveStar introduces an always-on proactive streaming assistant for online Video-LLMs, combining adaptive streaming decoding, incremental video-language alignment, and memory-efficient online inference to boost real-time performance and narrative coherence on long videos, validated on the OmniStar dataset.


<details>
  <summary>Details</summary>
Motivation: Online Video-LLMs struggle to process continuous frame-by-frame inputs while determining optimal response timing, leading to latency and coherence issues for long videos.

Method: Threefold approach: (1) incremental video-language alignment for variable-length streams to preserve temporal consistency; (2) response-silence decoding to select proactive timing via a single forward-pass verification; (3) memory-aware acceleration using peak-end memory compression and a streaming key-value cache to enable online inference on 10+ minute videos, achieving 1.53x faster inference.

Result: State-of-the-art performance: 19.5% improvement in semantic correctness and 18.1% reduction in timing difference versus existing online Video-LLMs; 12.0% FPS improvement across five OmniStar tasks; OmniStar dataset includes 15 scenarios and 5 evaluation tasks; code released at GitHub.

Conclusion: LiveStar enables always-on proactive responses for online video understanding, advancing real-time capabilities of Video-LLMs and providing a benchmark/dataset (OmniStar) for training and evaluation.

Abstract: Despite significant progress in Video Large Language Models (Video-LLMs) for
offline video understanding, existing online Video-LLMs typically struggle to
simultaneously process continuous frame-by-frame inputs and determine optimal
response timing, often compromising real-time responsiveness and narrative
coherence. To address these limitations, we introduce LiveStar, a pioneering
live streaming assistant that achieves always-on proactive responses through
adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a
training strategy enabling incremental video-language alignment for
variable-length video streams, preserving temporal consistency across
dynamically evolving frame sequences; (2) a response-silence decoding framework
that determines optimal proactive response timing via a single forward pass
verification; (3) memory-aware acceleration via peak-end memory compression for
online inference on 10+ minute videos, combined with streaming key-value cache
to achieve 1.53x faster inference. We also construct an OmniStar dataset, a
comprehensive dataset for training and benchmarking that encompasses 15 diverse
real-world scenarios and 5 evaluation tasks for online video understanding.
Extensive experiments across three benchmarks demonstrate LiveStar's
state-of-the-art performance, achieving an average 19.5% improvement in
semantic correctness with 18.1% reduced timing difference compared to existing
online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks.
Our model and dataset can be accessed at https://github.com/yzy-bupt/LiveStar.

</details>


### [48] [What's on Your Plate? Inferring Chinese Cuisine Intake from Wearable IMUs](https://arxiv.org/abs/2511.05292)
*Jiaxi Yin,Pengcheng Wang,Han Ding,Fei Wang*

Main category: cs.CV

TL;DR: CuisineSense combines smartwatch hand-motion cues with head-dynamics from smart glasses in a two-stage pipeline to detect eating and classify Chinese dishes across 11 categories, validated on 27.5 hours of IMU data from 10 participants, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Addresses (1) recall bias in self-report dietary data, (2) privacy concerns with camera-based monitoring, and (3) limited food-type coverage of existing wearables, by focusing on the diversity of Chinese cuisine with unobtrusive devices.

Method: Two-stage approach: Stage 1 detects eating states by extracting characteristic temporal patterns from multimodal sensor data (hand motion from a smartwatch and head dynamics from smart glasses) to filter irrelevant activities. Stage 2 performs fine-grained food-type recognition from the intake-motion cues. Uses IMU data; dataset consists of 27.5 hours across 11 food categories with 10 participants; code released publicly.

Result: Experimental results show high accuracy for both eating-state detection and food-type classification, demonstrating the practicality of unobtrusive, wearable-based dietary monitoring.

Conclusion: CuisineSense demonstrates feasibility of privacy-preserving, multimodal wearable systems for broad Chinese cuisine recognition, offering a practical solution for continuous dietary monitoring and enabling real-world deployment; code is publicly available.

Abstract: Accurate food intake detection is vital for dietary monitoring and chronic
disease prevention. Traditional self-report methods are prone to recall bias,
while camera-based approaches raise concerns about privacy. Furthermore,
existing wearable-based methods primarily focus on a limited number of food
types, such as hamburgers and pizza, failing to address the vast diversity of
Chinese cuisine. To bridge this gap, we propose CuisineSense, a system that
classifies Chinese food types by integrating hand motion cues from a smartwatch
with head dynamics from smart glasses. To filter out irrelevant daily
activities, we design a two-stage detection pipeline. The first stage
identifies eating states by distinguishing characteristic temporal patterns
from non-eating behaviors. The second stage then conducts fine-grained food
type recognition based on the motions captured during food intake. To evaluate
CuisineSense, we construct a dataset comprising 27.5 hours of IMU recordings
across 11 food categories and 10 participants. Experiments demonstrate that
CuisineSense achieves high accuracy in both eating state detection and food
classification, offering a practical solution for unobtrusive, wearable-based
dietary monitoring.The system code is publicly available at
https://github.com/joeeeeyin/CuisineSense.git.

</details>


### [49] [Rethinking Metrics and Diffusion Architecture for 3D Point Cloud Generation](https://arxiv.org/abs/2511.05308)
*Matteo Bastico,David Ryckelynck,Laurent Corté,Yannick Tillier,Etienne Decencière*

Main category: cs.CV

TL;DR: Chamfer Distance-based metrics for 3D point clouds are brittle to defects and misalignment; we introduce Density-Aware Chamfer Distance (DCD) with an alignment prior, and a Surface Normal Concordance (SNC) metric for local surface fidelity, alongside a Diffusion Point Transformer that achieves state-of-the-art results on ShapeNet; code is released.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation metrics (CD-based) fail to robustly assess generated point clouds, missing geometric fidelity and local shape consistency. There is a need for alignment-aware, density-sensitive metrics and complementary normal-based similarity, coupled with architectures that deliver high-fidelity 3D structures.

Method: 1) Diagnose robustness issues of Chamfer Distance for point clouds. 2) Propose an alignment-prior pre-processing before distance computation. 3) Replace CD with Density-Aware Chamfer Distance (DCD). 4) Introduce Surface Normal Concordance (SNC) by comparing estimated normals to capture surface similarity. 5) Build Diffusion Point Transformer leveraging serialized patch attention for high-fidelity 3D generation. 6) Evaluate on ShapeNet with proposed metrics and baseline methods; release code.

Result: DCD improves robustness of evaluation metrics, SNC captures surface similarity and complements traditional metrics, and the Diffusion Point Transformer achieves state-of-the-art quality in generated point clouds on ShapeNet, outperforming previous methods. Code available at the provided GitHub link.

Conclusion: Combining a density-aware, alignment-considerate evaluation suite (DCD + SNC) with a powerful diffusion-based transformer model yields robust assessment and high-quality generation for 3D point clouds, advancing the state-of-the-art; public code is released.

Abstract: As 3D point clouds become a cornerstone of modern technology, the need for
sophisticated generative models and reliable evaluation metrics has grown
exponentially. In this work, we first expose that some commonly used metrics
for evaluating generated point clouds, particularly those based on Chamfer
Distance (CD), lack robustness against defects and fail to capture geometric
fidelity and local shape consistency when used as quality indicators. We
further show that introducing samples alignment prior to distance calculation
and replacing CD with Density-Aware Chamfer Distance (DCD) are simple yet
essential steps to ensure the consistency and robustness of point cloud
generative model evaluation metrics. While existing metrics primarily focus on
directly comparing 3D Euclidean coordinates, we present a novel metric, named
Surface Normal Concordance (SNC), which approximates surface similarity by
comparing estimated point normals. This new metric, when combined with
traditional ones, provides a more comprehensive evaluation of the quality of
generated samples. Finally, leveraging recent advancements in transformer-based
models for point cloud analysis, such as serialized patch attention , we
propose a new architecture for generating high-fidelity 3D structures, the
Diffusion Point Transformer. We perform extensive experiments and comparisons
on the ShapeNet dataset, showing that our model outperforms previous solutions,
particularly in terms of quality of generated point clouds, achieving new
state-of-the-art. Code available at
https://github.com/matteo-bastico/DiffusionPointTransformer.

</details>


### [50] [Cross-domain EEG-based Emotion Recognition with Contrastive Learning](https://arxiv.org/abs/2511.05293)
*Rui Yan,Yibo Li,Han Ding,Fei Wang*

Main category: cs.CV

TL;DR: Introduces EmotionCLIP, a CLIP-based EEG–text matching framework with an SST-LegoViT backbone, achieving strong cross-subject and cross-time generalization on SEED/SEED-IV.


<details>
  <summary>Details</summary>
Motivation: EEG-based emotion recognition suffers from limited feature utilization and poor cross-domain generalization; robust and generalizable models are needed for affective computing.

Method: Reformulates emotion recognition as EEG–text matching within the CLIP framework. Proposes SST-LegoViT to capture spatial, spectral, and temporal features via multi-scale convolution and Transformer modules. Employs multimodal contrastive learning to align EEG representations with text.

Result: On SEED and SEED-IV, achieves cross-subject accuracies of 88.69% and 73.50%, and cross-time accuracies of 88.46% and 77.54%, outperforming existing models.

Conclusion: Multimodal contrastive learning with EEG–text alignment is effective for robust EEG emotion recognition and improves cross-domain/generalization performance.

Abstract: Electroencephalogram (EEG)-based emotion recognition is vital for affective
computing but faces challenges in feature utilization and cross-domain
generalization. This work introduces EmotionCLIP, which reformulates
recognition as an EEG-text matching task within the CLIP framework. A tailored
backbone, SST-LegoViT, captures spatial, spectral, and temporal features using
multi-scale convolution and Transformer modules. Experiments on SEED and
SEED-IV datasets show superior cross-subject accuracies of 88.69% and 73.50%,
and cross-time accuracies of 88.46% and 77.54%, outperforming existing models.
Results demonstrate the effectiveness of multimodal contrastive learning for
robust EEG emotion recognition.

</details>


### [51] [AI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly](https://arxiv.org/abs/2511.05394)
*Alexander Htet Kyaw,Haotian Ma,Sasa Zivkovic,Jenny Sabin*

Main category: cs.CV

TL;DR: An AI-assisted AR assembly system uses deep learning-based object recognition to identify components and overlay step-by-step instructions with bounding boxes and placement cues, demonstrated on LEGO sculpture assembly.


<details>
  <summary>Details</summary>
Motivation: To reduce manual searching, sorting, and labeling of components in assembly tasks by leveraging AI and AR to provide real-time, component-aware guidance.

Method: Integrates deep learning-based object recognition with an AR workflow. For each assembly step, the system displays bounding boxes around the relevant components and indicates where they should be placed in the physical space, dynamically linking instructions with real-time component locations to guide assembly. A LEGO sculpture case study demonstrates feasibility.

Result: Feasibility is demonstrated via the LEGO case study, showing that object recognition can support AR-assisted assembly by guiding users to the correct components and placements.

Conclusion: The approach shows promise for AI-assisted AR-guided assembly, potentially reducing manual search/sort/label tasks and streamlining assembly workflows, with scope for broader validation and extension to other domains.

Abstract: We present an AI-assisted Augmented Reality assembly workflow that uses deep
learning-based object recognition to identify different assembly components and
display step-by-step instructions. For each assembly step, the system displays
a bounding box around the corresponding components in the physical space, and
where the component should be placed. By connecting assembly instructions with
the real-time location of relevant components, the system eliminates the need
for manual searching, sorting, or labeling of different components before each
assembly. To demonstrate the feasibility of using object recognition for
AR-assisted assembly, we highlight a case study involving the assembly of LEGO
sculptures.

</details>


### [52] [Multi-modal Loop Closure Detection with Foundation Models in Severely Unstructured Environments](https://arxiv.org/abs/2511.05404)
*Laura Alejandra Encinar Gonzalez,John Folkesson,Rudolph Triebel,Riccardo Giubilato*

Main category: cs.CV

TL;DR: A multimodal SLAM loop-closure system (MPRF) uses foundation-model features and geometric verification to robustly detect loops in GNSS-denied, low-texture environments by unifying place recognition with 6-DoF pose estimation.


<details>
  <summary>Details</summary>
Motivation: In GNSS-denied, unstructured environments (e.g., planetary exploration), visual place recognition struggles due to aliasing and weak textures, and LiDAR can be sparse or ambiguous. There is a need for robust, efficient loop-closure that also provides pose estimates and interpretable correspondences.

Method: MPRF integrates vision and LiDAR through transformer-based foundation models. It employs a two-stage visual retrieval using DINOv2 features with SALAD aggregation for candidate screening and explicit 6-DoF pose estimation, and uses SONATA-based LiDAR descriptors for geometric verification. The system provides interpretable correspondences suitable for SLAM back-ends.

Result: On the S3LI and S3LI Vulcano datasets, MPRF outperforms state-of-the-art retrieval methods in precision and improves pose-estimation robustness in low-texture regions, while offering reliable, interpretable correspondences and a favorable accuracy/efficiency/reliability trade-off.

Conclusion: Foundation-model-based multimodal SLAM can unify place recognition and pose estimation, enabling robust loop closure in challenging environments. Code and models are to be released, enabling reproducibility and broader adoption.

Abstract: Robust loop closure detection is a critical component of Simultaneous
Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as
in the context of planetary exploration. In these settings, visual place
recognition often fails due to aliasing and weak textures, while LiDAR-based
methods suffer from sparsity and ambiguity. This paper presents MPRF, a
multimodal pipeline that leverages transformer-based foundation models for both
vision and LiDAR modalities to achieve robust loop closure in severely
unstructured environments. Unlike prior work limited to retrieval, MPRF
integrates a two-stage visual retrieval strategy with explicit 6-DoF pose
estimation, combining DINOv2 features with SALAD aggregation for efficient
candidate screening and SONATA-based LiDAR descriptors for geometric
verification. Experiments on the S3LI dataset and S3LI Vulcano dataset show
that MPRF outperforms state-of-the-art retrieval methods in precision while
enhancing pose estimation robustness in low-texture regions. By providing
interpretable correspondences suitable for SLAM back-ends, MPRF achieves a
favorable trade-off between accuracy, efficiency, and reliability,
demonstrating the potential of foundation models to unify place recognition and
pose estimation. Code and models will be released at github.com/DLR-RM/MPRF.

</details>


### [53] [$\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language Models](https://arxiv.org/abs/2511.05319)
*Huanqi Wu,Huangbiao Xu,Runfeng Xie,Jiaxin Cai,Kaixin Zhang,Xiao Ke*

Main category: cs.CV

TL;DR: A semantic steganography framework to hide sentence-level messages in images, via S^2LM and a new IVT benchmark, enabling high-level textual content embedding with LLMs.


<details>
  <summary>Details</summary>
Motivation: Traditional steganography struggles to embed semantically rich, sentence-level content; in the era of AI-generated content, there is demand to conceal high-level information in images.

Method: Propose Sentence-to-Image Steganography as a semantic steganography task; establish the Invisible Text (IVT) benchmark with diverse sentence-level messages; introduce S^2LM (Semantic Steganographic Language Model) that deploys large language models throughout an end-to-end pipeline to embed sentences/paragraphs into images.

Result: Quantitative and qualitative experiments demonstrate effective embedding of semantically rich content and expanded semantic steganography capabilities for LLMs; source code will be released.

Conclusion: Extends steganography from bit-level to semantic-level messages, enabling new applications in the AI-assisted generation era; provides a benchmark (IVT) and a generative pipeline (S^2LM) for embedding sentences in images.

Abstract: Although steganography has made significant advancements in recent years, it
still struggles to embed semantically rich, sentence-level information into
carriers. However, in the era of AIGC, the capacity of steganography is more
critical than ever. In this work, we present Sentence-to-Image Steganography,
an instance of Semantic Steganography, a novel task that enables the hiding of
arbitrary sentence-level messages within a cover image. Furthermore, we
establish a benchmark named Invisible Text (IVT), comprising a diverse set of
sentence-level texts as secret messages for evaluation. Finally, we present
$\mathbf{S^2LM}$: Semantic Steganographic Language Model, which utilizes large
language models (LLMs) to embed high-level textual information, such as
sentences or even paragraphs, into images. Unlike traditional bit-level
counterparts, $\mathrm{S^2LM}$ enables the integration of semantically rich
content through a newly designed pipeline in which the LLM is involved
throughout the entire process. Both quantitative and qualitative experiments
demonstrate that our method effectively unlocks new semantic steganographic
capabilities for LLMs. The source code will be released soon.

</details>


### [54] [Canonical Space Representation for 4D Panoptic Segmentation of Articulated Objects](https://arxiv.org/abs/2511.05356)
*Manuel Gomes,Bogdan Raducanu,Miguel Oliveira*

Main category: cs.CV

TL;DR: Artic4D provides a 4D articulated object dataset with 4D panoptic annotations and articulation parameters, and CanonSeg4D is a 4D panoptic segmentation framework that aligns parts in a canonical space over time to improve segmentation accuracy, outperforming prior methods in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: Articulated object perception faces challenges due to neglected temporal dynamics; 4D data and panoptic segmentation for articulated objects are underexplored; lack of benchmarks hampers progress.

Method: Introduce Artic4D dataset (derived from PartNet Mobility with synthetic sensor data) with 4D panoptic annotations and articulation parameters; develop CanonSeg4D that estimates per-frame offsets to map observed parts to a learned canonical space, enabling consistent cross-frame alignment and improved part-level segmentation.

Result: Extensive experiments on Artic4D show CanonSeg4D achieving higher panoptic segmentation accuracy than state-of-the-art methods in more complex scenarios.

Conclusion: Temporal modeling and canonical alignment are effective for 4D articulated object understanding; the Artic4D dataset and CanonSeg4D framework pave the way for future advances in 4D articulated perception.

Abstract: Articulated object perception presents significant challenges in computer
vision, particularly because most existing methods ignore temporal dynamics
despite the inherently dynamic nature of such objects. The use of 4D temporal
data has not been thoroughly explored in articulated object perception and
remains unexamined for panoptic segmentation. The lack of a benchmark dataset
further hurt this field. To this end, we introduce Artic4D as a new dataset
derived from PartNet Mobility and augmented with synthetic sensor data,
featuring 4D panoptic annotations and articulation parameters. Building on this
dataset, we propose CanonSeg4D, a novel 4D panoptic segmentation framework.
This approach explicitly estimates per-frame offsets mapping observed object
parts to a learned canonical space, thereby enhancing part-level segmentation.
The framework employs this canonical representation to achieve consistent
alignment of object parts across sequential frames. Comprehensive experiments
on Artic4D demonstrate that the proposed CanonSeg4D outperforms state of the
art approaches in panoptic segmentation accuracy in more complex scenarios.
These findings highlight the effectiveness of temporal modeling and canonical
alignment in dynamic object understanding, and pave the way for future advances
in 4D articulated object perception.

</details>


### [55] [Dense Motion Captioning](https://arxiv.org/abs/2511.05369)
*Shiyao Xu,Benedetta Liberatori,Gül Varol,Paolo Rota*

Main category: cs.CV

TL;DR: Proposes Dense Motion Captioning to temporally localize and caption actions in 3D human motion; introduces CompMo, a large-scale dataset with 60,000 complex sequences and precise temporal boundaries; presents DEMO, a model that combines a large language model with a motion adapter to produce dense, temporally grounded captions; DEMO outperforms baselines on CompMo and adapted benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addresses a gap in 3D motion understanding beyond text-to-motion translation by enabling detailed temporal annotations and captioning of complex, multi-action sequences; existing datasets lack long sequences with precise action boundaries.

Method: CompMo is built via a data generation pipeline to create long, complex motion sequences annotated with multiple actions and tight temporal extents; DEMO integrates a large language model with a simple motion adapter to generate dense, temporally grounded captions.

Result: DEMO substantially outperforms existing methods on CompMo and adapted benchmarks, establishing a robust baseline and demonstrating the framework's effectiveness for 3D motion understanding and captioning.

Conclusion: Introduces a new task (dense motion captioning) and a strong benchmark (CompMo) along with a capable model (DEMO) that advances 3D motion understanding and captioning, paving the way for future research.

Abstract: Recent advances in 3D human motion and language integration have primarily
focused on text-to-motion generation, leaving the task of motion understanding
relatively unexplored. We introduce Dense Motion Captioning, a novel task that
aims to temporally localize and caption actions within 3D human motion
sequences. Current datasets fall short in providing detailed temporal
annotations and predominantly consist of short sequences featuring few actions.
To overcome these limitations, we present the Complex Motion Dataset (CompMo),
the first large-scale dataset featuring richly annotated, complex motion
sequences with precise temporal boundaries. Built through a carefully designed
data generation pipeline, CompMo includes 60,000 motion sequences, each
composed of multiple actions ranging from at least two to ten, accurately
annotated with their temporal extents. We further present DEMO, a model that
integrates a large language model with a simple motion adapter, trained to
generate dense, temporally grounded captions. Our experiments show that DEMO
substantially outperforms existing methods on CompMo as well as on adapted
benchmarks, establishing a robust baseline for future research in 3D motion
understanding and captioning.

</details>


### [56] [PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning for Visual Quality Assessment via Preference-Response Disentangled Policy Optimization](https://arxiv.org/abs/2511.05393)
*Zehui Feng,Tian Qiu,Tong Wu,Junxuan Li,Huayuan Xu,Ting Han*

Main category: cs.CV

TL;DR: Introduces PreResQ-R1, a dual-branch RL framework for visual QA that unifies absolute scoring and relative preferences, extended to video, achieving state-of-the-art results with interpretable reasoning traces.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLM approaches to visual quality assessment rely on supervised fine-tuning or ranking objectives, leading to shallow reasoning, miscalibration, and limited cross-domain generalization. A unified, reasoning-driven optimization with disentangled rewards aims to improve calibration, generalization, and interpretability.

Method: Proposes a dual-branch reward formulation: intra-sample response coherence and inter-sample preference alignment, optimized via Group Relative Policy Optimization (GRPO). Extends to videos with global-temporal and local-spatial data flow. Reinforcement fine-tuning on 6k images and 28k videos.

Result: Achieves state-of-the-art across 10 IQA and 5 VQA benchmarks under SRCC and PLCC, with IQA gains of 5.30% (SRCC) and 2.15% (PLCC). Produces human-aligned reasoning traces.

Conclusion: The method delivers fine-grained, stable, and interpretable perceptual-quality reasoning, with strong cross-domain performance and reduced reliance on large-scale supervision; code and model are available.

Abstract: Visual Quality Assessment (QA) seeks to predict human perceptual judgments of
visual fidelity. While recent multimodal large language models (MLLMs) show
promise in reasoning about image and video quality, existing approaches mainly
rely on supervised fine-tuning or rank-only objectives, resulting in shallow
reasoning, poor score calibration, and limited cross-domain generalization. We
propose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning
framework that unifies absolute score regression and relative ranking
consistency within a single reasoning-driven optimization scheme. Unlike prior
QA methods, PreResQ-R1 introduces a dual-branch reward formulation that
separately models intra-sample response coherence and inter-sample preference
alignment, optimized via Group Relative Policy Optimization (GRPO). This design
encourages fine-grained, stable, and interpretable chain-of-thought reasoning
about perceptual quality. To extend beyond static imagery, we further design a
global-temporal and local-spatial data flow strategy for Video Quality
Assessment. Remarkably, with reinforcement fine-tuning on only 6K images and
28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5
VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30%
and textbf2.15% in IQA task, respectively. Beyond quantitative gains, it
produces human-aligned reasoning traces that reveal the perceptual cues
underlying quality judgments. Code and model are available.

</details>


### [57] [TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning](https://arxiv.org/abs/2511.05489)
*Junwen Pan,Qizhe Zhang,Rui Zhang,Ming Lu,Xin Wan,Yuan Zhang,Chang Liu,Qi She*

Main category: cs.CV

TL;DR: TimeSearch-R uses interleaved text-video reasoning with reinforcement learning to perform end-to-end temporal search; introduces GRPO-CSV to ensure complete reasoning and prepares datasets for SFT cold-start and RL training; achieves state-of-the-art results on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To overcome hand-crafted, non-end-to-end temporal search strategies and improve exploration and completeness in identifying minimal relevant frames for long-form video understanding.

Method: Proposes TimeSearch-R, an RL-based framework that interleaves search with reasoning, enhances GRPO with Completeness Self-Verification (GRPO-CSV), and builds targeted datasets by filtering for temporal dependencies.

Result: Demonstrates significant improvements on Haystack-LVBench, Haystack-Ego4D, VideoMME, and MLVU; achieves new state-of-the-art on LongVideoBench with 4.1% improvement over Qwen2.5-VL and 2.0% over Video-R1.

Conclusion: End-to-end RL-based temporal search with self-verification improves temporal search completeness and long-form video understanding; code is released at the provided GitHub URL.

Abstract: Temporal search aims to identify a minimal set of relevant frames from tens
of thousands based on a given query, serving as a foundation for accurate
long-form video understanding. Existing works attempt to progressively narrow
the search space. However, these approaches typically rely on a hand-crafted
search process, lacking end-to-end optimization for learning optimal search
strategies. In this paper, we propose TimeSearch-R, which reformulates temporal
search as interleaved text-video thinking, seamlessly integrating searching
video clips into the reasoning process through reinforcement learning (RL).
However, applying RL training methods, such as Group Relative Policy
Optimization (GRPO), to video reasoning can result in unsupervised intermediate
search decisions. This leads to insufficient exploration of the video content
and inconsistent logical reasoning. To address these issues, we introduce GRPO
with Completeness Self-Verification (GRPO-CSV), which gathers searched video
frames from the interleaved reasoning process and utilizes the same policy
model to verify the adequacy of searched frames, thereby improving the
completeness of video reasoning. Additionally, we construct datasets
specifically designed for the SFT cold-start and RL training of GRPO-CSV,
filtering out samples with weak temporal dependencies to enhance task
difficulty and improve temporal search capabilities. Extensive experiments
demonstrate that TimeSearch-R achieves significant improvements on temporal
search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as
long-form video understanding benchmarks like VideoMME and MLVU. Notably,
TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1%
improvement over the base model Qwen2.5-VL and 2.0% over the advanced video
reasoning model Video-R1. Our code is available at
https://github.com/Time-Search/TimeSearch-R.

</details>


### [58] [PALM: A Dataset and Baseline for Learning Multi-subject Hand Prior](https://arxiv.org/abs/2511.05403)
*Zicong Fan,Edoardo Remelli,David Dimond,Fadime Sener,Liuhao Ge,Bugra Tekin,Cem Keskin,Shreyas Hampali*

Main category: cs.CV

TL;DR: PALM introduces a large-scale hand dataset and a baseline avatar model, enabling realistic, relightable single-image hand personalization by leveraging 13k scans, 263 subjects, and 90k multi-view images.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in high-quality hand avatar creation due to complex geometry, appearance, articulation, and lighting; highlights lack of datasets with accurate 3D geometry, high-res multiview imagery, and diverse subjects.

Method: Dataset collection (PALM): 13k high-quality hand scans from 263 subjects and 90k multiview images; baseline PALM-Net: learning a multi-subject prior over hand geometry and material properties via physically based inverse rendering to enable relightable hand avatars from a single image.

Result: PALM provides scale and diversity as a real-world resource; PALM-Net demonstrates realistic, relightable hand avatars from single images, validating the utility of the dataset for hand modeling and related research.

Conclusion: PALM stands as a valuable resource for advancing hand modeling, enabling improved avatars and broader research contributions; suggests paths for future work in personalization and relighting.

Abstract: The ability to grasp objects, signal with gestures, and share emotion through
touch all stem from the unique capabilities of human hands. Yet creating
high-quality personalized hand avatars from images remains challenging due to
complex geometry, appearance, and articulation, particularly under
unconstrained lighting and limited views. Progress has also been limited by the
lack of datasets that jointly provide accurate 3D geometry, high-resolution
multiview imagery, and a diverse population of subjects. To address this, we
present PALM, a large-scale dataset comprising 13k high-quality hand scans from
263 subjects and 90k multi-view images, capturing rich variation in skin tone,
age, and geometry. To show its utility, we present a baseline PALM-Net, a
multi-subject prior over hand geometry and material properties learned via
physically based inverse rendering, enabling realistic, relightable
single-image hand avatar personalization. PALM's scale and diversity make it a
valuable real-world resource for hand modeling and related research.

</details>


### [59] [Sharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration](https://arxiv.org/abs/2511.05421)
*Aupendu Kar,Krishnendu Ghosh,Prabir Kumar Biswas*

Main category: cs.CV

TL;DR: A lightweight continual-learning approach for image restoration using a simple convolution modification that preserves the backbone, enabling scalable addition of restoration tasks with modest overhead.


<details>
  <summary>Details</summary>
Motivation: Continual learning in image restoration is underexplored due to large image sizes, diverse degradations, and the need to adapt without heavy architecture changes or regu​larization-based methods.

Method: Introduce a minimal modification to convolution layers to transfer knowledge from previous restoration tasks, effectively creating a task-specific but backbone-agnostic adapter; can be applied to any deep architecture; increases trainable parameters without significant compute overhead; relies on building a knowledge base from prior tasks.

Result: Experiments show new restoration tasks can be added without degrading existing task performance; new-task performance improves by leveraging prior knowledge; modest computational overhead; code released.

Conclusion: The approach enables scalable continual restoration with minimal architectural changes and can be integrated into existing models; potential extension to other domains.

Abstract: Continual learning is an emerging topic in the field of deep learning, where
a model is expected to learn continuously for new upcoming tasks without
forgetting previous experiences. This field has witnessed numerous
advancements, but few works have been attempted in the direction of image
restoration. Handling large image sizes and the divergent nature of various
degradation poses a unique challenge in the restoration domain. However,
existing works require heavily engineered architectural modifications for new
task adaptation, resulting in significant computational overhead.
Regularization-based methods are unsuitable for restoration, as different
restoration challenges require different kinds of feature processing. In this
direction, we propose a simple modification of the convolution layer to adapt
the knowledge from previous restoration tasks without touching the main
backbone architecture. Therefore, it can be seamlessly applied to any deep
architecture without any structural modifications. Unlike other approaches, we
demonstrate that our model can increase the number of trainable parameters
without significantly increasing computational overhead or inference time.
Experimental validation demonstrates that new restoration tasks can be
introduced without compromising the performance of existing tasks. We also show
that performance on new restoration tasks improves by adapting the knowledge
from the knowledge base created by previous restoration tasks. The code is
available at https://github.com/aupendu/continual-restore.

</details>


### [60] [Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis](https://arxiv.org/abs/2511.05432)
*Dogucan Yaman,Seymanur Akti,Fevziye Irem Eyiokur,Alexander Waibel*

Main category: cs.CV

TL;DR: Two-stage text-to-talking-face synthesis using Wav2Vec2 latent embeddings generated from text, with pretraining on Wav2Vec2 features and finetuning on TTS outputs to align audio-visuals; achieves natural speech, synchronized facial motion, and preserved speaker identity without needing ground-truth audio at inference.


<details>
  <summary>Details</summary>
Motivation: To achieve tight audio-visual alignment in talking-face synthesis under distribution shifts between clean embeddings and TTS-predicted features, while preserving speaker identity and avoiding the need for ground-truth audio during inference, and to surpass cascaded pipelines.

Method: A Text-to-Vec module converts text into Wav2Vec2 embeddings that condition both speech and face generation. The approach leverages latent representations from HierSpeech++ and employs a two-stage training regime: (1) pretraining on Wav2Vec2 embeddings, (2) finetuning on TTS outputs to align predicted features with realistic audio-visual behavior. This avoids reliance on ground-truth audio at inference and aims for tight audio-visual alignment and natural, expressive speech.

Result: Empirical results show that conditioning on TTS-predicted latent features outperforms cascaded pipelines, improving lip-sync accuracy and visual realism while preserving speaker identity and producing synchronized facial motion without external audio.

Conclusion: The proposed framework demonstrates robust audio-visual alignment and high-quality talking-face synthesis by training to map text to TTS-predicted latent features, outperforming cascaded baselines and eliminating the need for ground-truth audio at inference.

Abstract: We propose a text-to-talking-face synthesis framework leveraging latent
speech representations from HierSpeech++. A Text-to-Vec module generates
Wav2Vec2 embeddings from text, which jointly condition speech and face
generation. To handle distribution shifts between clean and TTS-predicted
features, we adopt a two-stage training: pretraining on Wav2Vec2 embeddings and
finetuning on TTS outputs. This enables tight audio-visual alignment, preserves
speaker identity, and produces natural, expressive speech and synchronized
facial motion without ground-truth audio at inference. Experiments show that
conditioning on TTS-predicted latent features outperforms cascaded pipelines,
improving both lip-sync and visual realism.

</details>


### [61] [How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?](https://arxiv.org/abs/2511.05449)
*Tuan Anh Tran,Duy M. H. Nguyen,Hoai-Chau Tran,Michael Barz,Khoa D. Doan,Roger Wattenhofer,Ngo Anh Vien,Mathias Niepert,Daniel Sonntag,Paul Swoboda*

Main category: cs.CV

TL;DR: Gitmerge3D: a graph-based token merging method that cuts 3D transformer tokens by 90-95% while preserving performance, revealing redundancy and boosting efficiency.


<details>
  <summary>Details</summary>
Motivation: 3D point cloud transformers are highly tokenized, leading to large compute and memory costs. There is likely significant token redundancy; reducing tokens could yield more scalable, efficient models without sacrificing accuracy.

Method: Globally informed graph token merging (gitmerge3D) that merges tokens across the entire point cloud using a graph-based strategy, dramatically reducing token counts while preserving task performance.

Result: Token counts can be reduced by up to 90-95% with competitive performance across 3D vision tasks (e.g., semantic segmentation, reconstruction). The work is among the first to quantify and exploit redundancy in large-scale 3D transformers and demonstrates practical efficiency gains.

Conclusion: Token redundancy exists in 3D transformers; many models are over-tokenized. gitmerge3D offers a path toward more efficient 3D foundation architectures, with code and checkpoints released for reproducibility.

Abstract: Recent advances in 3D point cloud transformers have led to state-of-the-art
results in tasks such as semantic segmentation and reconstruction. However,
these models typically rely on dense token representations, incurring high
computational and memory costs during training and inference. In this work, we
present the finding that tokens are remarkably redundant, leading to
substantial inefficiency. We introduce gitmerge3D, a globally informed graph
token merging method that can reduce the token count by up to 90-95% while
maintaining competitive performance. This finding challenges the prevailing
assumption that more tokens inherently yield better performance and highlights
that many current models are over-tokenized and under-optimized for
scalability. We validate our method across multiple 3D vision tasks and show
consistent improvements in computational efficiency. This work is the first to
assess redundancy in large-scale 3D transformer models, providing insights into
the development of more efficient 3D foundation architectures. Our code and
checkpoints are publicly available at https://gitmerge3d.github.io

</details>


### [62] [The Potential of Copernicus Satellites for Disaster Response: Retrieving Building Damage from Sentinel-1 and Sentinel-2](https://arxiv.org/abs/2511.05461)
*Olivier Dietrich,Merlin Alfredsson,Emilia Arens,Nando Metzger,Torben Peters,Linus Scheibenreif,Jan Dirk Wegner,Konrad Schindler*

Main category: cs.CV

TL;DR: Medium-resolution Copernicus Sentinel-1/2 images (10 m GSD) can support rapid, wide-area building damage mapping, complementing VHR imagery; introduces xBD-S12 dataset, showing damage can be detected well in many disasters; complex models and geospatial foundation models provide limited extra benefit; dataset and models released.


<details>
  <summary>Details</summary>
Motivation: Need for fast, large-area post-disaster damage assessment when very-high-resolution data are limited or unavailable, to complement existing benchmarks and enable rapid humanitarian response.

Method: Create xBD-S12 by assembling 10,315 pre- and post-disaster image pairs from Sentinel-1 and Sentinel-2, aligned with the xBD benchmark; conduct experiments across multiple disaster scenarios to assess damage detection/mapping at ~10 m GSD; compare architectural complexity and the utility of geospatial foundation models; release dataset, code, and trained models.

Result: Building damage can be detected and mapped at 10 m resolution in many disaster scenarios; more complex model architectures do not consistently improve generalization to unseen disasters; geospatial foundation models yield little practical benefit; Copernicus data are viable for rapid, wide-area damage assessment and can complement VHR images.

Conclusion: Copernicus images offer a viable data source for rapid, wide-area damage assessment and should play an important role alongside very-high-resolution imagery; public release of xBD-S12 dataset, code, and trained models facilitates further research.

Abstract: Natural disasters demand rapid damage assessment to guide humanitarian
response. Here, we investigate whether medium-resolution Earth observation
images from the Copernicus program can support building damage assessment,
complementing very-high resolution imagery with often limited availability. We
introduce xBD-S12, a dataset of 10,315 pre- and post-disaster image pairs from
both Sentinel-1 and Sentinel-2, spatially and temporally aligned with the
established xBD benchmark. In a series of experiments, we demonstrate that
building damage can be detected and mapped rather well in many disaster
scenarios, despite the moderate 10$\,$m ground sampling distance. We also find
that, for damage mapping at that resolution, architectural sophistication does
not seem to bring much advantage: more complex model architectures tend to
struggle with generalization to unseen disasters, and geospatial foundation
models bring little practical benefit. Our results suggest that Copernicus
images are a viable data source for rapid, wide-area damage assessment and
could play an important role alongside VHR imagery. We release the xBD-S12
dataset, code, and trained models to support further research.

</details>


### [63] [Photo Dating by Facial Age Aggregation](https://arxiv.org/abs/2511.05464)
*Jakub Paplham,Vojtech Franc*

Main category: cs.CV

TL;DR: A probabilistic multi-face framework for dating photos using face recognition/age estimates and career priors, leveraging CSFD-1.6M; outperforms scene-only baselines, especially with many identifiable faces.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap in photo dating by exploiting temporal signals embedded in faces and public-career priors, enabling robust use of multi-face cues beyond scene context.

Method: Probabilistic integration of face recognition scores, age estimates, and career-based temporal priors across multiple detected faces in an image.

Result: Evidence aggregation across multiple faces yields consistent performance gains and superior results relative to strong scene-based baselines.

Conclusion: Demonstrates the usefulness of multi-face aggregation for photo dating and provides a large annotated dataset for future research (CSFD-1.6M).

Abstract: We introduce a novel method for Photo Dating which estimates the year a
photograph was taken by leveraging information from the faces of people present
in the image. To facilitate this research, we publicly release CSFD-1.6M, a new
dataset containing over 1.6 million annotated faces, primarily from movie
stills, with identity and birth year annotations. Uniquely, our dataset
provides annotations for multiple individuals within a single image, enabling
the study of multi-face information aggregation. We propose a probabilistic
framework that formally combines visual evidence from modern face recognition
and age estimation models, and career-based temporal priors to infer the photo
capture year. Our experiments demonstrate that aggregating evidence from
multiple faces consistently improves the performance and the approach
significantly outperforms strong, scene-based baselines, particularly for
images containing several identifiable individuals.

</details>


### [64] [EventFlow: Real-Time Neuromorphic Event-Driven Classification of Two-Phase Boiling Flow Regimes](https://arxiv.org/abs/2511.05467)
*Sanghyeon Chang,Srikar Arani,Nishant Sai Nuthalapati,Youngjoon Suh,Nicholas Choi,Siavash Khodakarami,Md Rakibul Hasan Roni,Nenad Miljkovic,Aparna Chandramowlishwaran,Yoonjin Won*

Main category: cs.CV

TL;DR: Neuromorphic-event-based sensing enables real-time flow regime classification in flow boiling, outperforming frame-based methods with 97.6% accuracy at 0.28 ms; five models tested, with event-based LSTM offering best efficiency; asynchronous pipeline with majority voting for reliable control.


<details>
  <summary>Details</summary>
Motivation: Need for low-latency, accurate real-time monitoring of flow regime transitions in flow boiling to preserve thermal performance and reliability; traditional imaging suffers from high computational load and limited temporal resolution.

Method: Develop and compare five classification models using traditional frame data and event-based data from neuromorphic sensors; evaluate performance and latency; implement asynchronous processing with majority voting to provide stable real-time predictions.

Result: Event-based models outperform frame-based approaches; best performance is an event-based long short-term memory model achieving 97.6% accuracy with 0.28 ms processing; system supports continuous, low-latency predictions with stability through majority voting.

Conclusion: Neuromorphic, event-based sensing enables reliable, low-latency real-time feedback for experimental control and intelligent thermal management in flow boiling; asynchronous architecture reduces latency and improves robustness.

Abstract: Flow boiling is an efficient heat transfer mechanism capable of dissipating
high heat loads with minimal temperature variation, making it an ideal thermal
management method. However, sudden shifts between flow regimes can disrupt
thermal performance and system reliability, highlighting the need for accurate
and low-latency real-time monitoring. Conventional optical imaging methods are
limited by high computational demands and insufficient temporal resolution,
making them inadequate for capturing transient flow behavior. To address this,
we propose a real-time framework based on signals from neuromorphic sensors for
flow regime classification. Neuromorphic sensors detect changes in brightness
at individual pixels, which typically correspond to motion at edges, enabling
fast and efficient detection without full-frame reconstruction, providing
event-based information. We develop five classification models using both
traditional image data and event-based data, demonstrating that models
leveraging event data outperform frame-based approaches due to their
sensitivity to dynamic flow features. Among these models, the event-based long
short-term memory model provides the best balance between accuracy and speed,
achieving 97.6% classification accuracy with a processing time of 0.28 ms. Our
asynchronous processing pipeline supports continuous, low-latency predictions
and delivers stable output through a majority voting mechanisms, enabling
reliable real-time feedback for experimental control and intelligent thermal
management.

</details>


### [65] [Semantic-Guided Natural Language and Visual Fusion for Cross-Modal Interaction Based on Tiny Object Detection](https://arxiv.org/abs/2511.05474)
*Xian-Hong Huang,Hui-Kai Su,Chi-Chia Sun,Jun-Wei Hsieh*

Main category: cs.CV

TL;DR: A cross-modal tiny object detector that fuses BERT-based language cues with a CNN-based PRB-FPN backbone (featuring ELAN, MSP, and CSP variants) to improve small-object detection, achieving 52.6 AP on COCO2017 val and competitive efficiency compared to Transformer-based models.


<details>
  <summary>Details</summary>
Motivation: To enhance tiny object detection by aligning semantic cues from natural language with robust visual features, enabling accurate, scalable detection in resource-constrained settings.

Method: Integrates BERT with a CNN-based Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN-Net), leveraging backbones such as ELAN, MSP, and CSP; employs lemmatization and fine-tuning to align textual semantics with visual representations.

Result: On COCO2017 validation, attains 52.6 AP, outperforming YOLO-World while using about half the parameters of Transformer-based models like GLIP; validated on COCO and Objects365, with multi-scale robustness across backbones.

Conclusion: Demonstrates the viability of integrating natural language understanding with advanced backbone architectures to boost accuracy, efficiency, and adaptability of object detectors in real-world, resource-constrained scenarios.

Abstract: This paper introduces a cutting-edge approach to cross-modal interaction for
tiny object detection by combining semantic-guided natural language processing
with advanced visual recognition backbones. The proposed method integrates the
BERT language model with the CNN-based Parallel Residual Bi-Fusion Feature
Pyramid Network (PRB-FPN-Net), incorporating innovative backbone architectures
such as ELAN, MSP, and CSP to optimize feature extraction and fusion. By
employing lemmatization and fine-tuning techniques, the system aligns semantic
cues from textual inputs with visual features, enhancing detection precision
for small and complex objects. Experimental validation using the COCO and
Objects365 datasets demonstrates that the model achieves superior performance.
On the COCO2017 validation set, it attains a 52.6% average precision (AP),
outperforming YOLO-World significantly while maintaining half the parameter
consumption of Transformer-based models like GLIP. Several test on different of
backbones such ELAN, MSP, and CSP further enable efficient handling of
multi-scale objects, ensuring scalability and robustness in
resource-constrained environments. This study underscores the potential of
integrating natural language understanding with advanced backbone
architectures, setting new benchmarks in object detection accuracy, efficiency,
and adaptability to real-world challenges.

</details>


### [66] [GroupKAN: Rethinking Nonlinearity with Grouped Spline-based KAN Modeling for Efficient Medical Image Segmentation](https://arxiv.org/abs/2511.05477)
*Guojie Li,Anwar P. P. Abdul Majeed,Muhammad Ateeq,Anh Nguyen,Fan Zhang*

Main category: cs.CV

TL;DR: GroupKAN is a scalable, lightweight, and interpretable medical image segmentation network that improves upon U-KAN by using grouped KAN modules to reduce complexity and parameters, achieving higher IoU.


<details>
  <summary>Details</summary>
Motivation: Convolutional nets lack adaptive nonlinearity and clear decision processes, while Transformers have quadratic complexity and opaque attention. U-KAN improves accuracy and interpretability but suffers from O(C^2) full-channel complexity, limiting scalability. GroupKAN tackles these issues.

Method: Introduce two structured modules: (1) Grouped KAN Transform, which partitions channels into G groups for multivariate spline mappings, reducing complexity to O(C^2/G); (2) Grouped KAN Activation, which applies shared spline-based mappings within each channel group for efficient, token-wise nonlinearity. Evaluated on BUSI, GlaS, and CVC benchmarks.

Result: GroupKAN achieves an average IoU of 79.80% across the three medical benchmarks, surpassing U-KAN by +1.11%. It uses 3.02M parameters versus 6.35M for U-KAN, i.e., about 47.6% of the parameters, indicating substantial efficiency gains.

Conclusion: GroupKAN delivers a scalable, efficient, and more interpretable segmentation solution that outperforms the prior U-KAN while dramatically reducing parameter count.

Abstract: Medical image segmentation requires models that are accurate, lightweight,
and interpretable. Convolutional architectures lack adaptive nonlinearity and
transparent decision-making, whereas Transformer architectures are hindered by
quadratic complexity and opaque attention mechanisms. U-KAN addresses these
challenges using Kolmogorov-Arnold Networks, achieving higher accuracy than
both convolutional and attention-based methods, fewer parameters than
Transformer variants, and improved interpretability compared to conventional
approaches. However, its O(C^2) complexity due to full-channel transformations
limits its scalability as the number of channels increases. To overcome this,
we introduce GroupKAN, a lightweight segmentation network that incorporates two
novel, structured functional modules: (1) Grouped KAN Transform, which
partitions channels into G groups for multivariate spline mappings, reducing
complexity to O(C^2/G), and (2) Grouped KAN Activation, which applies shared
spline-based mappings within each channel group for efficient, token-wise
nonlinearity. Evaluated on three medical benchmarks (BUSI, GlaS, and CVC),
GroupKAN achieves an average IoU of 79.80 percent, surpassing U-KAN by +1.11
percent while requiring only 47.6 percent of the parameters (3.02M vs 6.35M),
and shows improved interpretability.

</details>


### [67] [Visual Spatial Tuning](https://arxiv.org/abs/2511.05491)
*Rui Yang,Ziyu Zhu,Yanwei Li,Jingjia Huang,Shen Yan,Siyuan Zhou,Zhe Liu,Xiangtai Li,Shuangye Li,Wenqian Wang,Yi Lin,Hengshuang Zhao*

Main category: cs.CV

TL;DR: Visual Spatial Tuning (VST) enables VLMs to gain visuospatial abilities without extra encoders by using large-scale perception and reasoning datasets and a progressive training pipeline, achieving state-of-the-art on MMSI-Bench and VSIBench and improving Vision-Language-Action models.


<details>
  <summary>Details</summary>
Motivation: To overcome the inefficiency and potential harm to general capabilities caused by adding specialized spatial encoders for VLMs, and to equip general architectures with human-like visuospatial perception and reasoning.

Method: Construct VST-P (4.1M samples, 19 spatial skills across single views, multi-image, and video) for perception and VST-R (135K samples) for spatial reasoning; employ a progressive training pipeline of supervised fine-tuning to build foundational spatial knowledge, followed by reinforcement learning to enhance spatial reasoning, without degrading general capabilities.

Result: Achieves state-of-the-art on MMSI-Bench (34.8%) and VSIBench (61.2%); demonstrates that spatial tuning improves Vision-Language-Action models while preserving general abilities.

Conclusion: VST offers a comprehensive, general-purpose framework to imbue VLMs with visuospatial perception and reasoning, enabling more physically grounded AI without the burden of extra encoders.

Abstract: Capturing spatial relationships from visual inputs is a cornerstone of
human-like general intelligence. Several previous studies have tried to enhance
the spatial awareness of Vision-Language Models (VLMs) by adding extra expert
encoders, which brings extra overhead and usually harms general capabilities.
To enhance the spatial ability in general architectures, we introduce Visual
Spatial Tuning (VST), a comprehensive framework to cultivate VLMs with
human-like visuospatial abilities, from spatial perception to reasoning. We
first attempt to enhance spatial perception in VLMs by constructing a
large-scale dataset termed VST-P, which comprises 4.1 million samples spanning
19 skills across single views, multiple images, and videos. Then, we present
VST-R, a curated dataset with 135K samples that instruct models to reason in
space. In particular, we adopt a progressive training pipeline: supervised
fine-tuning to build foundational spatial knowledge, followed by reinforcement
learning to further improve spatial reasoning abilities. Without the
side-effect to general capabilities, the proposed VST consistently achieves
state-of-the-art results on several spatial benchmarks, including $34.8\%$ on
MMSI-Bench and $61.2\%$ on VSIBench. It turns out that the
Vision-Language-Action models can be significantly enhanced with the proposed
spatial tuning paradigm, paving the way for more physically grounded AI.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [68] [A hybrid solution approach for the Integrated Healthcare Timetabling Competition 2024](https://arxiv.org/abs/2511.04685)
*Daniela Guericke,Rolf van der Hulst,Asal Karimpour,Ieke Schrader,Matthias Walter*

Main category: cs.AI

TL;DR: Team Twente's entry to the Integrated Healthcare Timetabling Competition 2024 uses a 3-phase, decomposition-based hybrid of mixed-integer programming, constraint programming, and simulated annealing to solve healthcare timetabling; achieved 3rd place and provides the first lower bounds on optimal values for benchmark instances; discusses design choices and outlines open problems for improvement.


<details>
  <summary>Details</summary>
Motivation: Healthcare timetabling is a hard combinatorial optimization problem; providing effective algorithms and understanding (including lower bounds) helps gauge solution quality and pushes progress in benchmark contexts.

Method: A 3-phase solution approach based on decomposition into subproblems that integrates mixed-integer programming (MIP), constraint programming (CP), and simulated annealing (SA). The paper also discusses design decisions and implementation details.

Result: The approach secured third place in the competition; it also presents for the first time lower bounds on the optimal solution values for the benchmark instances and shares insights gleaned from the process.

Conclusion: Open problems are highlighted with suggested directions that could further improve the approach; the work introduces lower bounds and points to future research opportunities to enhance performance.

Abstract: We report about the algorithm, implementation and results submitted to the
Integrated Healthcare Timetabling Competition 2024 by Team Twente, which scored
third in the competition. Our approach combines mixed-integer programming,
constraint programming and simulated annealing in a 3-phase solution approach
based on decomposition into subproblems. Next to describing our approach and
describing our design decisions, we share our insights and, for the first time,
lower bounds on the optimal solution values for the benchmark instances. We
finally highlight open problems for which we think that addressing them could
improve our approach even further.

</details>


### [69] [Epistemic Reject Option Prediction](https://arxiv.org/abs/2511.04855)
*Vojtech Franc,Jakub Paplham*

Main category: cs.AI

TL;DR: A Bayesian, regret-based abstention framework (epistemic reject-option) that abstains in inputs with high epistemic uncertainty due to limited data; defines abstention via regret relative to Bayes-optimal predictor and a rejection cost.


<details>
  <summary>Details</summary>
Motivation: High-stakes settings require not just accuracy but reliable uncertainty estimates; traditional reject-option focuses on aleatoric uncertainty and fails when data are scarce, making epistemic uncertainty important.

Method: Define the optimal predictor as one that minimizes expected regret to the Bayes-optimal predictor with full distribution knowledge; abstain when the input's regret exceeds a specified rejection cost; builds on Bayesian learning to quantify epistemic uncertainty and guide abstention.

Result: Proposes a principled framework for identifying inputs where training data are insufficient for reliable decisions; formalizes regret-based abstention and shows how to implement an epistemic reject-option predictor (novelty claim: first principled framework of this kind).

Conclusion: This work introduces the first principled framework for epistemic reject-option prediction, enabling learning predictors that abstain in regions with insufficient data to make reliable decisions.

Abstract: In high-stakes applications, predictive models must not only produce accurate
predictions but also quantify and communicate their uncertainty. Reject-option
prediction addresses this by allowing the model to abstain when prediction
uncertainty is high. Traditional reject-option approaches focus solely on
aleatoric uncertainty, an assumption valid only when large training data makes
the epistemic uncertainty negligible. However, in many practical scenarios,
limited data makes this assumption unrealistic. This paper introduces the
epistemic reject-option predictor, which abstains in regions of high epistemic
uncertainty caused by insufficient data. Building on Bayesian learning, we
redefine the optimal predictor as the one that minimizes expected regret -- the
performance gap between the learned model and the Bayes-optimal predictor with
full knowledge of the data distribution. The model abstains when the regret for
a given input exceeds a specified rejection cost. To our knowledge, this is the
first principled framework that enables learning predictors capable of
identifying inputs for which the training data is insufficient to make reliable
decisions.

</details>


### [70] [DMA: Online RAG Alignment with Human Feedback](https://arxiv.org/abs/2511.04880)
*Yu Bai,Yukai Miao,Dawei Wang,Li Chen,Fei Long,Rundi Zhai,Dan Li,Yanyu Ren,Tianfeng Liu,Hongtao Xie,Ce Yang,Xuhui Cai*

Main category: cs.AI

TL;DR: DMA is an online learning framework that uses multi-granularity human feedback to dynamically align RAG ranking, improving engagement and knowledge-intensive QA while preserving baseline retrieval capabilities.


<details>
  <summary>Details</summary>
Motivation: RAG systems often rely on static retrieval, leading to adaptation lag when user intents and content drift occur. There is a need for online, feedback-driven adaptation that can update ranking and behavior in real time.

Method: DMA collects signals at document-, list-, and response-level and trains pointwise and listwise rankers in supervised fashion; uses policy optimization guided by response-level preferences; distills knowledge into a lightweight scorer for low-latency serving; memory here refers to the model's working memory/context for in-context learning.

Result: Online A/B ablations and a multi-month industrial deployment show substantial improvements in human engagement; offline, DMA preserves competitive retrieval while yielding gains on conversational QA benchmarks (TriviaQA, HotpotQA).

Conclusion: DMA provides a principled framework for feedback-driven, real-time adaptation in retrieval-augmented generation without sacrificing baseline capabilities.

Abstract: Retrieval-augmented generation (RAG) systems often rely on static retrieval,
limiting adaptation to evolving intent and content drift. We introduce Dynamic
Memory Alignment (DMA), an online learning framework that systematically
incorporates multi-granularity human feedback to align ranking in interactive
settings. DMA organizes document-, list-, and response-level signals into a
coherent learning pipeline: supervised training for pointwise and listwise
rankers, policy optimization driven by response-level preferences, and
knowledge distillation into a lightweight scorer for low-latency serving.
Throughout this paper, memory refers to the model's working memory, which is
the entire context visible to the LLM for In-Context Learning.
  We adopt a dual-track evaluation protocol mirroring deployment: (i)
large-scale online A/B ablations to isolate the utility of each feedback
source, and (ii) few-shot offline tests on knowledge-intensive benchmarks.
Online, a multi-month industrial deployment further shows substantial
improvements in human engagement. Offline, DMA preserves competitive
foundational retrieval while yielding notable gains on conversational QA
(TriviaQA, HotpotQA). Taken together, these results position DMA as a
principled approach to feedback-driven, real-time adaptation in RAG without
sacrificing baseline capability.

</details>


### [71] [Real-Time Reasoning Agents in Evolving Environments](https://arxiv.org/abs/2511.04898)
*Yule Wen,Yixin Ye,Yanzhe Zhang,Diyi Yang,Hao Zhu*

Main category: cs.AI

TL;DR: A framework for real-time reasoning in dynamic environments that blends reactive and planning language-model agents using AgileThinker, showing improvements over single-paradigm approaches under time pressure.


<details>
  <summary>Details</summary>
Motivation: Agents operating in the real world must make timely judgments amid changing environments; current LLM reasoning approaches are too slow or not temporally constrained, necessitating a real-time reasoning testbed and methods.

Method: Introduce Real-Time Reasoning Gym as a testbed; evaluate two paradigms—reactive agents with bounded reasoning for fast responses, and planning agents with extended reasoning for complex tasks; propose AgileThinker that combines both paradigms; conduct experiments to compare performance under varying task difficulty and time pressure.

Result: State-of-the-art models struggle to consistently make logical and timely judgments; AgileThinker outperforms agents relying on a single paradigm, especially as task difficulty or time pressure rises, by balancing reasoning depth and latency.

Conclusion: Real-time reasoning is a critical testbed for practical agents and lays a foundation for temporally constrained AI research, pointing toward the development of real-time capable agents.

Abstract: Agents in the real world must make not only logical but also timely
judgments. This requires continuous awareness of the dynamic environment:
hazards emerge, opportunities arise, and other agents act, while the agent's
reasoning is still unfolding. Despite advances in language model reasoning,
existing approaches fail to account for this dynamic nature. We introduce
real-time reasoning as a new problem formulation for agents in evolving
environments and build Real-Time Reasoning Gym to demonstrate it. We study two
paradigms for deploying language models in agents: (1) reactive agents, which
employ language models with bounded reasoning computation for rapid responses,
and (2) planning agents, which allow extended reasoning computation for complex
problems. Our experiments show that even state-of-the-art models struggle with
making logical and timely judgments in either paradigm. To address this
limitation, we propose AgileThinker, which simultaneously engages both
reasoning paradigms. AgileThinker consistently outperforms agents engaging only
one reasoning paradigm as the task difficulty and time pressure rise,
effectively balancing reasoning depth and response latency. Our work
establishes real-time reasoning as a critical testbed for developing practical
agents and provides a foundation for research in temporally constrained AI
systems, highlighting a path toward real-time capable agents.

</details>


### [72] [ORCHID: Orchestrated Retrieval-Augmented Classification with Human-in-the-Loop Intelligent Decision-Making for High-Risk Property](https://arxiv.org/abs/2511.04956)
*Maria Mahbub,Vanessa Lama,Sanjay Das,Brian Starks,Christopher Polchek,Saffell Silvers,Lauren Deck,Prasanna Balaprakash,Tirthankar Ghosal*

Main category: cs.AI

TL;DR: ORCHID is a modular, agent-based system that uses retrieval-augmented generation (RAG) and human oversight to classify high-risk property (HRP) at DOE sites, delivering auditable, policy-based outputs with on-premise operation.


<details>
  <summary>Details</summary>
Motivation: To replace slow, backlog-prone expert-only HRP classification with a framework that maintains policy alignment, transparency, and auditability amid evolving export-control rules, while keeping data on-site.

Method: A cooperative multi-agent architecture (retrieval, description refiner, classifier, validator, feedback logger) that communicates via agent-to-agent messaging and the Model Context Protocol (MCP). It implements an Item–Evidence–Decision loop with step-by-step reasoning, grounded citations, and append-only audit bundles (run-cards, prompts, evidence) for auditable outputs, all operable on-premise.

Result: Preliminary tests on real HRP cases show improved accuracy and traceability versus a non-agentic baseline, with uncertain items deferred to SMEs; demonstration includes single-item submissions, grounded citations, SME feedback capture, and exportable audit artifacts.

Conclusion: ORCHID demonstrates a practical, auditable path for trustworthy LLM-assisted DOE compliance workflows, balancing automation with SME oversight and on-premise governance.

Abstract: High-Risk Property (HRP) classification is critical at U.S. Department of
Energy (DOE) sites, where inventories include sensitive and often dual-use
equipment. Compliance must track evolving rules designated by various export
control policies to make transparent and auditable decisions. Traditional
expert-only workflows are time-consuming, backlog-prone, and struggle to keep
pace with shifting regulatory boundaries. We demo ORCHID, a modular agentic
system for HRP classification that pairs retrieval-augmented generation (RAG)
with human oversight to produce policy-based outputs that can be audited. Small
cooperating agents, retrieval, description refiner, classifier, validator, and
feedback logger, coordinate via agent-to-agent messaging and invoke tools
through the Model Context Protocol (MCP) for model-agnostic on-premise
operation. The interface follows an Item to Evidence to Decision loop with
step-by-step reasoning, on-policy citations, and append-only audit bundles
(run-cards, prompts, evidence). In preliminary tests on real HRP cases, ORCHID
improves accuracy and traceability over a non-agentic baseline while deferring
uncertain items to Subject Matter Experts (SMEs). The demonstration shows
single item submission, grounded citations, SME feedback capture, and
exportable audit artifacts, illustrating a practical path to trustworthy LLM
assistance in sensitive DOE compliance workflows.

</details>


### [73] [Autonomous generation of different courses of action in mechanized combat operations](https://arxiv.org/abs/2511.05182)
*Johan Schubert,Patrik Hansen,Pontus Hörling,Ronnie Johansson*

Main category: cs.AI

TL;DR: A decision-support method for real-time military execution that generates and evaluates thousands of action alternatives for a mechanized battalion, updating recommendations as conditions evolve to aid sequential commander decisions.


<details>
  <summary>Details</summary>
Motivation: To improve decision quality and speed in dynamic ground combat by systematically exploring a large space of courses of action (COAs) and continuously updating them as the engagement unfolds.

Method: Generate thousands of action alternatives for a mechanized battalion, evaluate them against the opponent's status/actions, considering unit composition, force ratios, offense/defense types, and anticipated advance rates. Use field manuals to assess battle outcomes and progress rates. Generation and evaluation occur concurrently, with prior evaluated actions guiding new COA generation. As conditions evolve, formulate revised COAs within a sequential decision-making framework for the commander.

Result: A large pool of viable COAs with superior expected outcomes and a mechanism to update and refine options in real time as the battle unfolds, enabling the decision-maker to consider multiple routes through a dynamic environment.

Conclusion: The approach supports dynamic, sequential decision-making in combat by enabling rapid generation, evaluation, and refinement of action options to improve engagement outcomes.

Abstract: In this paper, we propose a methodology designed to support decision-making
during the execution phase of military ground combat operations, with a focus
on one's actions. This methodology generates and evaluates recommendations for
various courses of action for a mechanized battalion, commencing with an
initial set assessed by their anticipated outcomes. It systematically produces
thousands of individual action alternatives, followed by evaluations aimed at
identifying alternative courses of action with superior outcomes. These
alternatives are appraised in light of the opponent's status and actions,
considering unit composition, force ratios, types of offense and defense, and
anticipated advance rates. Field manuals evaluate battle outcomes and
advancement rates. The processes of generation and evaluation work
concurrently, yielding a variety of alternative courses of action. This
approach facilitates the management of new course generation based on
previously evaluated actions. As the combat unfolds and conditions evolve,
revised courses of action are formulated for the decision-maker within a
sequential decision-making framework.

</details>


### [74] [Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance](https://arxiv.org/abs/2511.05311)
*Valeriu Dimidov,Faisal Hawlader,Sasan Jafarnejad,Raphaël Frank*

Main category: cs.AI

TL;DR: LLM-based agents can effectively clean maintenance logs with common data quality issues, enabling PdM deployment; domain-specific errors still need targeted training.


<details>
  <summary>Details</summary>
Motivation: Overcome economic constraints, data scarcity, and lack of specialized expertise hindering predictive maintenance (PdM) adoption in the automotive sector by leveraging LLMs to automate data cleaning in maintenance logs.

Method: Evaluate LLM-based agents on cleaning tasks across six noise types in maintenance logs and assess performance and generalizability.

Result: LLMs handle generic cleaning tasks effectively and show promise for industrial deployment; domain-specific errors remain challenging; improvements likely with domain-adapted training and enhanced agent capabilities.

Conclusion: LLMs provide a promising foundation for PdM data cleaning, but practical deployment requires targeted, domain-adapted training to address domain-specific noise.

Abstract: Economic constraints, limited availability of datasets for reproducibility
and shortages of specialized expertise have long been recognized as key
challenges to the adoption and advancement of predictive maintenance (PdM) in
the automotive sector. Recent progress in large language models (LLMs) presents
an opportunity to overcome these barriers and speed up the transition of PdM
from research to industrial practice. Under these conditions, we explore the
potential of LLM-based agents to support PdM cleaning pipelines. Specifically,
we focus on maintenance logs, a critical data source for training
well-performing machine learning (ML) models, but one often affected by errors
such as typos, missing fields, near-duplicate entries, and incorrect dates. We
evaluate LLM agents on cleaning tasks involving six distinct types of noise.
Our findings show that LLMs are effective at handling generic cleaning tasks
and offer a promising foundation for future industrial applications. While
domain-specific errors remain challenging, these results highlight the
potential for further improvements through specialized training and enhanced
agentic capabilities.

</details>


### [75] [Reasoning Is All You Need for Urban Planning AI](https://arxiv.org/abs/2511.05375)
*Sijie Yang,Jiatong Li,Filip Biljecki*

Main category: cs.AI

TL;DR: Reasoning-capable urban planning agents: a three-layer (Perception, Foundation, Reasoning) and six-component (Analysis, Generation, Verification, Evaluation, Collaboration, Decision) architecture for AI-assisted planning, enhanced by multi-agent collaboration; aims for value-based, rule-grounded, explainable decisions that augment—not replace—human planners; outlines architecture, metrics, and challenges.


<details>
  <summary>Details</summary>
Motivation: Current AI in urban planning is strong at pattern learning but lacks explicit, normative, and explainable reasoning aligned with constraints and stakeholder values. This framework seeks to move toward prescriptive, transparent AI agents that can justify decisions and collaborate with humans.

Method: Proposes the Agentic Urban Planning AI Framework combining three cognitive layers and six logic components within a multi-agent collaboration setup. Builds on CoT prompting, ReAct, and multi-agent reasoning to enable explicit reasoning, constraint satisfaction, and trade-off deliberation. Includes architectural design, proposed benchmark metrics, and discussion of research challenges.

Result: A conceptual, architecture-level framework (not an empirical study) with defined components, evaluation metrics, and a roadmap for implementing reasoning-enabled planning agents. Demonstrates how such agents can explore solution spaces, verify regulatory compliance, and transparently justify decisions.

Conclusion: Reasoning-capable planning agents can amplify human planning judgment by providing normative, rule-grounded, and explainable deliberations. They aim to augment, not replace, human planners by enhancing exploration, verification, and transparent justification of trade-offs.

Abstract: AI has proven highly successful at urban planning analysis -- learning
patterns from data to predict future conditions. The next frontier is
AI-assisted decision-making: agents that recommend sites, allocate resources,
and evaluate trade-offs while reasoning transparently about constraints and
stakeholder values. Recent breakthroughs in reasoning AI -- CoT prompting,
ReAct, and multi-agent collaboration frameworks -- now make this vision
achievable.
  This position paper presents the Agentic Urban Planning AI Framework for
reasoning-capable planning agents that integrates three cognitive layers
(Perception, Foundation, Reasoning) with six logic components (Analysis,
Generation, Verification, Evaluation, Collaboration, Decision) through a
multi-agents collaboration framework. We demonstrate why planning decisions
require explicit reasoning capabilities that are value-based (applying
normative principles), rule-grounded (guaranteeing constraint satisfaction),
and explainable (generating transparent justifications) -- requirements that
statistical learning alone cannot fulfill. We compare reasoning agents with
statistical learning, present a comprehensive architecture with benchmark
evaluation metrics, and outline critical research challenges. This framework
shows how AI agents can augment human planners by systematically exploring
solution spaces, verifying regulatory compliance, and deliberating over
trade-offs transparently -- not replacing human judgment but amplifying it with
computational reasoning capabilities.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [76] [Star-Based Separators for Intersection Graphs of $c$-Colored Pseudo-Segments](https://arxiv.org/abs/2511.05371)
*M. de Berg,B. M. P. Jansen,J. S. K. Lamme*

Main category: cs.CG

TL;DR: Introduces star-based (biclique-based) separators for intersection graphs of c-oriented segments and polygons, achieving O(sqrt(n)) separators, and presents a distance oracle with subquadratic storage and sublinear query time with additive-2 error.


<details>
  <summary>Details</summary>
Motivation: Planar separator theorems do not directly yield efficient separators for non-planar geometric intersection graphs like segment and polygon intersection graphs; a separator structure based on bicliques/stars could enable faster algorithms.

Method: Define star-based separators and biclique-based separators; prove existence for c-oriented segments (and pseudo-segments partitioned into c disjoint subsets) and extend to c-oriented polygons; derive separator size O(sqrt(n)); design a distance oracle leveraging these separators.

Result: Proves that any c-oriented set of n segments can be separated by O(sqrt(n)) stars; extends to c-oriented pseudo-segments and polygons; yields an almost-exact distance oracle with O(n sqrt(n)) storage, O(sqrt(n)) query time, and additive error at most 2; this is the first such oracle with subquadratic storage and sublinear query time with additive error.

Conclusion: Star-based/biclique-based separators provide efficient structural tools for geometric intersection graphs where clique-based separators fail; they enable fast, space-efficient distance queries and broaden applicability of separator techniques to a wider class of geometric graphs.

Abstract: The Planar Separator Theorem, which states that any planar graph
$\mathcal{G}$ has a separator consisting of $O(\sqrt{n})$ nodes whose removal
partitions $\mathcal{G}$ into components of size at most $\tfrac{2n}{3}$, is a
widely used tool to obtain fast algorithms on planar graphs. Intersection
graphs of disks, which generalize planar graphs, do not admit such separators.
It has recently been shown that disk graphs do admit so-called clique-based
separators that consist of $O(\sqrt{n})$ cliques. This result has been
generalized to intersection graphs of various other types of disk-like objects.
Unfortunately, segment intersection graphs do not admit small clique-based
separators, because they can contain arbitrarily large bicliques. This is true
even in the simple case of axis-aligned segments.
  In this paper we therefore introduce biclique-based separators (and, in
particular, star-based separators), which are separators consisting of a small
number of bicliques (or stars). We prove that any $c$-oriented set of $n$
segments in the plane, where $c$ is a constant, admits a star-based separator
consisting of $O(\sqrt{n})$ stars. In fact, our result is more general, as it
applies to any set of $n$ pseudo-segments that is partitioned into $c$ subsets
such that the pseudo-segments in the same subset are pairwise disjoint. We
extend our result to intersection graphs of $c$-oriented polygons. These
results immediately lead to an almost-exact distance oracle for such
intersection graphs, which has $O(n\sqrt{n})$ storage and $O(\sqrt{n})$ query
time, and that can report the hop-distance between any two query nodes in the
intersection graph with an additive error of at most 2. This is the first
distance oracle for such types of intersection graphs that has subquadratic
storage and sublinear query time and that only has an additive error.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [77] [ScheduleStream: Temporal Planning with Samplers for GPU-Accelerated Multi-Arm Task and Motion Planning & Scheduling](https://arxiv.org/abs/2511.04758)
*Caelan Garrett,Fabio Ramos*

Main category: cs.RO

TL;DR: ScheduleStream is a general-purpose planning and scheduling framework with sampling-based methods that enables parallel, asynchronous, and durative actions for multi-arm robots, improving planning efficiency over traditional single-arm TAMP approaches.


<details>
  <summary>Details</summary>
Motivation: Controlling multiple arms leads to a hybrid discrete-continuous action space and traditional TAMP often yields plans with only one arm moving at a time, failing to exploit parallelism and causing inefficiency.

Method: Introduce ScheduleStream with hybrid durative actions that can start asynchronously and have duration dependent on parameters; develop domain-independent algorithms; integrate with TAMPAS by using GPU-accelerated samplers to speed planning.

Result: In simulations, ScheduleStream and its variants produce more efficient solutions than ablations; real-world bimanual tasks demonstrate feasibility; the work provides empirical support for scheduling capabilities and parallel arm motion.

Conclusion: ScheduleStream is the first general-purpose framework for planning and scheduling with sampling operations and can be effectively applied to TAMPAS to enable parallel arm motion and faster planning, with GPU acceleration contributing to practical performance gains.

Abstract: Bimanual and humanoid robots are appealing because of their human-like
ability to leverage multiple arms to efficiently complete tasks. However,
controlling multiple arms at once is computationally challenging due to the
growth in the hybrid discrete-continuous action space. Task and Motion Planning
(TAMP) algorithms can efficiently plan in hybrid spaces but generally produce
plans, where only one arm is moving at a time, rather than schedules that allow
for parallel arm motion. In order to extend TAMP to produce schedules, we
present ScheduleStream, the first general-purpose framework for planning &
scheduling with sampling operations. ScheduleStream models temporal dynamics
using hybrid durative actions, which can be started asynchronously and persist
for a duration that's a function of their parameters. We propose
domain-independent algorithms that solve ScheduleStream problems without any
application-specific mechanisms. We apply ScheduleStream to Task and Motion
Planning & Scheduling (TAMPAS), where we use GPU acceleration within samplers
to expedite planning. We compare ScheduleStream algorithms to several ablations
in simulation and find that they produce more efficient solutions. We
demonstrate ScheduleStream on several real-world bimanual robot tasks at
https://schedulestream.github.io.

</details>


### [78] [ReGen: Generative Robot Simulation via Inverse Design](https://arxiv.org/abs/2511.04769)
*Phat Nguyen,Tsun-Hsuan Wang,Zhang-Wei Hong,Erfan Aasi,Andrew Silva,Guy Rosman,Sertac Karaman,Daniela Rus*

Main category: cs.RO

TL;DR: ReGen is a generative simulation framework that uses large language models to inverse-design plausible driving/manipulation scenarios from a robot's observed behavior, converting a cause–effect graph into executable simulations to yield diverse, controllable environments for robust policy learning.


<details>
  <summary>Details</summary>
Motivation: Constructing realistic simulations is labor-intensive and often lacks diversity; scalable, controllable simulation generation is needed to validate and generalize robot policies.

Method: Use LLMs to expand a directed causal graph encoding entities, properties, and relationships from a robot's behavior and description; convert the graph into a symbolic program that configures a simulator; support ego-agent augmentation, counterfactuals, cognition state reasoning, and multi-sensing modalities.

Result: Produces more diverse and complex simulated environments than existing simulators, with high success rates and controllable generation of corner cases in autonomous driving and manipulation tasks; includes code and example videos.

Conclusion: ReGen enables scalable validation and data/simulation augmentation for robust, generalizable robot learning.

Abstract: Simulation plays a key role in scaling robot learning and validating
policies, but constructing simulations remains a labor-intensive process. This
paper introduces ReGen, a generative simulation framework that automates
simulation design via inverse design. Given a robot's behavior -- such as a
motion trajectory or an objective function -- and its textual description,
ReGen infers plausible scenarios and environments that could have caused the
behavior. ReGen leverages large language models to synthesize scenarios by
expanding a directed graph that encodes cause-and-effect relationships,
relevant entities, and their properties. This structured graph is then
translated into a symbolic program, which configures and executes a robot
simulation environment. Our framework supports (i) augmenting simulations based
on ego-agent behaviors, (ii) controllable, counterfactual scenario generation,
(iii) reasoning about agent cognition and mental states, and (iv) reasoning
with distinct sensing modalities, such as braking due to faulty GPS signals. We
demonstrate ReGen in autonomous driving and robot manipulation tasks,
generating more diverse, complex simulated environments compared to existing
simulations with high success rates, and enabling controllable generation for
corner cases. This approach enhances the validation of robot policies and
supports data or simulation augmentation, advancing scalable robot learning for
improved generalization and robustness. We provide code and example videos at:
https://regen-sim.github.io/

</details>


### [79] [Unified Multimodal Diffusion Forcing for Forceful Manipulation](https://arxiv.org/abs/2511.04812)
*Zixuan Huang,Huaidian Hou,Dmitry Berenson*

Main category: cs.RO

TL;DR: Multimodal Diffusion Forcing (MDF) introduces a diffusion-based, partially masked reconstruction objective to learn from multimodal robot trajectories (observations, actions, rewards, etc.), enabling cross-modal and temporal understanding beyond direct action prediction.


<details>
  <summary>Details</summary>
Motivation: Standard imitation learning often maps observations to actions and overlooks the rich interplay among modalities (sensory inputs, actions, rewards). Capturing these cross-modal and temporal dependencies is crucial for robust, task-relevant robot behavior and understanding outcomes.

Method: MDF applies random partial masking to multimodal trajectory data and trains a diffusion model to reconstruct the full trajectory, encouraging learning of temporal dynamics and cross-modal relationships such as how actions affect force signals or how states can be inferred from partial observations.

Result: Empirical evaluations on contact-rich, forceful manipulation tasks in simulation and on real hardware show MDF provides versatile capabilities, strong performance, and robustness to noisy observations.

Conclusion: A unified, diffusion-based framework for multimodal imitation learning that goes beyond action generation, enabling cross-modal reasoning and more robust robot behavior across modalities.

Abstract: Given a dataset of expert trajectories, standard imitation learning
approaches typically learn a direct mapping from observations (e.g., RGB
images) to actions. However, such methods often overlook the rich interplay
between different modalities, i.e., sensory inputs, actions, and rewards, which
is crucial for modeling robot behavior and understanding task outcomes. In this
work, we propose Multimodal Diffusion Forcing, a unified framework for learning
from multimodal robot trajectories that extends beyond action generation.
Rather than modeling a fixed distribution, MDF applies random partial masking
and trains a diffusion model to reconstruct the trajectory. This training
objective encourages the model to learn temporal and cross-modal dependencies,
such as predicting the effects of actions on force signals or inferring states
from partial observations. We evaluate MDF on contact-rich, forceful
manipulation tasks in simulated and real-world environments. Our results show
that MDF not only delivers versatile functionalities, but also achieves strong
performance, and robustness under noisy observations. More visualizations can
be found on our website https://unified-df.github.io

</details>


### [80] [Pixi: Unified Software Development and Distribution for Robotics and AI](https://arxiv.org/abs/2511.04827)
*Tobias Fischer,Wolf Vollprecht,Bas Zalmstra,Ruben Arts,Tim de Jager,Alejandro Fontan,Adam D Hines,Michael Milford,Silvio Traversaro,Daniel Claes,Scarlett Raine*

Main category: cs.RO

TL;DR: Pixi is a unified, lockfile-based package-management framework for robotics/AI that ensures bit-for-bit reproducibility across platforms by capturing exact dependency states and fast SAT-based resolution, integrating conda-forge and PyPI to simplify workflows.


<details>
  <summary>Details</summary>
Motivation: The reproducibility crisis in scientific computing and robotics arises from fragmented, multi-language, hardware-specific toolchains and complex environments, leading to unrepeatable results and deployment barriers.

Method: Develop Pixi to (1) capture exact dependency states in project-level lockfiles, (2) employ a high-performance SAT solver for fast dependency resolution, and (3) integrate conda-forge and PyPI ecosystems to unify package management across environments.

Result: Pixi achieves up to 10x faster dependency resolution, has been adopted in over 5,300 projects since 2023, and cuts setup times from hours to minutes, lowering barriers for reproducible research.

Conclusion: Pixi enables scalable, reproducible, collaborative research infrastructure for robotics and AI by unifying package ecosystems and mitigating dependency fragmentation.

Abstract: The reproducibility crisis in scientific computing constrains robotics
research. Existing studies reveal that up to 70% of robotics algorithms cannot
be reproduced by independent teams, while many others fail to reach deployment
because creating shareable software environments remains prohibitively complex.
These challenges stem from fragmented, multi-language, and hardware-software
toolchains that lead to dependency hell. We present Pixi, a unified
package-management framework that addresses these issues by capturing exact
dependency states in project-level lockfiles, ensuring bit-for-bit
reproducibility across platforms. Its high-performance SAT solver achieves up
to 10x faster dependency resolution than comparable tools, while integration of
the conda-forge and PyPI ecosystems removes the need for multiple managers.
Adopted in over 5,300 projects since 2023, Pixi reduces setup times from hours
to minutes and lowers technical barriers for researchers worldwide. By enabling
scalable, reproducible, collaborative research infrastructure, Pixi accelerates
progress in robotics and AI.

</details>


### [81] [Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot Learning](https://arxiv.org/abs/2511.04831)
*NVIDIA,:,Mayank Mittal,Pascal Roth,James Tigue,Antoine Richard,Octi Zhang,Peter Du,Antonio Serrano-Muñoz,Xinjie Yao,René Zurbrügg,Nikita Rudin,Lukasz Wawrzyniak,Milad Rakhsha,Alain Denzler,Eric Heiden,Ales Borovicka,Ossama Ahmed,Iretiayo Akinola,Abrar Anwar,Mark T. Carlson,Ji Yuan Feng,Animesh Garg,Renato Gasoto,Lionel Gulich,Yijie Guo,M. Gussert,Alex Hansen,Mihir Kulkarni,Chenran Li,Wei Liu,Viktor Makoviychuk,Grzegorz Malczyk,Hammad Mazhar,Masoud Moghani,Adithyavairavan Murali,Michael Noseworthy,Alexander Poddubny,Nathan Ratliff,Welf Rehberg,Clemens Schwarke,Ritvik Singh,James Latham Smith,Bingjie Tang,Ruchik Thaker,Matthew Trepte,Karl Van Wyk,Fangzhou Yu,Alex Millane,Vikram Ramasamy,Remo Steiner,Sangeeta Subramanian,Clemens Volk,CY Chen,Neel Jawale,Ashwin Varghese Kuruttukulam,Michael A. Lin,Ajay Mandlekar,Karsten Patzwaldt,John Welsh,Huihua Zhao,Fatima Anes,Jean-Francois Lafleche,Nicolas Moënne-Loccoz,Soowan Park,Rob Stepinski,Dirk Van Gelder,Chris Amevor,Jan Carius,Jumyung Chang,Anka He Chen,Pablo de Heras Ciechomski,Gilles Daviet,Mohammad Mohajerani,Julia von Muralt,Viktor Reutskyy,Michael Sauter,Simon Schirm,Eric L. Shi,Pierre Terdiman,Kenny Vilella,Tobias Widmer,Gordon Yeoman,Tiffany Chen,Sergey Grizan,Cathy Li,Lotus Li,Connor Smith,Rafael Wiltz,Kostas Alexis,Yan Chang,David Chu,Linxi "Jim" Fan,Farbod Farshidian,Ankur Handa,Spencer Huang,Marco Hutter,Yashraj Narang,Soha Pouya,Shiwei Sheng,Yuke Zhu,Miles Macklin,Adam Moravanszky,Philipp Reist,Yunrong Guo,David Hoeller,Gavriel State*

Main category: cs.RO

TL;DR: Isaac Lab is a GPU-native robotics simulation platform that scales RL and imitation learning with high-fidelity physics, photorealistic rendering, rich sensing, and data pipelines, aiming to integrate a differentiable Newton engine for data-efficient, gradient-based learning.


<details>
  <summary>Details</summary>
Motivation: To unify and scale simulation, sensing, and data collection for robotics research at data-center scale, enabling broad experimentation across whole-body control, dexterous manipulation, cross-embodiment mobility, and leveraging human demonstrations.

Method: Extend Isaac Gym with a modular, GPU-accelerated framework that combines high-fidelity parallel physics, photorealistic rendering, actuator models, multi-frequency sensor simulation, data collection pipelines, and domain randomization; supports reinforcement and imitation learning in a unified platform; outlines future integration with a differentiable, GPU-accelerated Newton physics engine.

Result: Presents a comprehensive, scalable framework and set of capabilities for robotics simulation and learning, demonstrated through applications to whole-body control, cross-embodiment mobility, contact-rich manipulation, and human demonstrations; emphasizes a unified platform for RL/IL at scale and potential data-efficient gradient-based methods via Newton integration.

Conclusion: Isaac Lab, with its advanced simulation capabilities, rich sensing, and large-scale execution, is positioned to accelerate next-generation robotics research by unifying best practices and enabling scalable, data-efficient learning and experimentation.

Abstract: We present Isaac Lab, the natural successor to Isaac Gym, which extends the
paradigm of GPU-native robotics simulation into the era of large-scale
multi-modal learning. Isaac Lab combines high-fidelity GPU parallel physics,
photorealistic rendering, and a modular, composable architecture for designing
environments and training robot policies. Beyond physics and rendering, the
framework integrates actuator models, multi-frequency sensor simulation, data
collection pipelines, and domain randomization tools, unifying best practices
for reinforcement and imitation learning at scale within a single extensible
platform. We highlight its application to a diverse set of challenges,
including whole-body control, cross-embodiment mobility, contact-rich and
dexterous manipulation, and the integration of human demonstrations for skill
acquisition. Finally, we discuss upcoming integration with the differentiable,
GPU-accelerated Newton physics engine, which promises new opportunities for
scalable, data-efficient, and gradient-based approaches to robot learning. We
believe Isaac Lab's combination of advanced simulation capabilities, rich
sensing, and data-center scale execution will help unlock the next generation
of breakthroughs in robotics research.

</details>


### [82] [Conformalized Non-uniform Sampling Strategies for Accelerated Sampling-based Motion Planning](https://arxiv.org/abs/2511.04835)
*Shubham Natraj,Bruno Sinopoli,Yiannis Kantaros*

Main category: cs.RO

TL;DR: A non-uniform sampling method for SBMPs uses conformal-prediction-based certified regions around an initial heuristic path; it provides probabilistic guarantees that the optimal solution lies within sampled regions, improving planning speed and generalization.


<details>
  <summary>Details</summary>
Motivation: Uniform sampling in sampling-based motion planners is often inefficient in complex environments; there is a need to bias sampling toward promising regions while still providing probabilistic guarantees on finding the optimal path.

Method: 1) Generate an initial path with a heuristic predictor (e.g., A*, vision-language model). 2) Apply conformal prediction to quantify the predictor's uncertainty and form prediction sets around the initial path. 3) Use these certified regions to bias SBMP sampling toward promising areas. 4) Integrate the approach into existing SBMP frameworks, maintaining probabilistic guarantees on the sampling region.

Result: Empirical evaluations show faster discovery of feasible paths and better generalization to unseen environments compared with baselines; the authors claim this is the first method to provide probabilistic guarantees on SBMP sampling regions.

Conclusion: Non-uniform, certifiably-guided sampling can significantly improve SBMP efficiency while offering probabilistic guarantees, enabling effective use of heuristic predictors within motion planning.

Abstract: Sampling-based motion planners (SBMPs) are widely used to compute dynamically
feasible robot paths. However, their reliance on uniform sampling often leads
to poor efficiency and slow planning in complex environments. We introduce a
novel non-uniform sampling strategy that integrates into existing SBMPs by
biasing sampling toward `certified' regions. These regions are constructed by
(i) generating an initial, possibly infeasible, path using any heuristic path
predictor (e.g., A* or vision-language models) and (ii) applying conformal
prediction to quantify the predictor's uncertainty. This process yields
prediction sets around the initial-guess path that are guaranteed, with
user-specified probability, to contain the optimal solution. To our knowledge,
this is the first non-uniform sampling approach for SBMPs that provides such
probabilistically correct guarantees on the sampling regions. Extensive
evaluations demonstrate that our method consistently finds feasible paths
faster and generalizes better to unseen environments than existing baselines.

</details>


### [83] [Design Exploration for Protection and Cleaning of Solar Panels with Case Studies for Space Missions](https://arxiv.org/abs/2511.04837)
*Cameron Robinson,Ganghee Jang*

Main category: cs.RO

TL;DR: Design and evaluation of panel cleaning mechanisms and protective materials to keep solar panels operational under dust and debris; wiper cleaning outperforms rail cleaning; soft interlayer with polycarbonate is promising as protective strategy.


<details>
  <summary>Details</summary>
Motivation: Solar panels in space and harsh terrestrial environments are at risk from dust, debris, and collision; maintaining performance is essential for mission-critical applications.

Method: Compared two cleaning mechanisms (wiper vs rail) and tested protective materials via collision tests; assessed cost, cleaning speed, and total power consumption; found polycarbonate promising with a soft interlayer; emphasized interfacial layering as key.

Result: Wiper-based cleaning system is more efficient than rail-based in cost, cleaning speed, and total power consumption. Polycarbonate emerges as a promising protective material, with the most important factor being a soft material layer between the panel surface and the hard material.

Conclusion: Recommend wiper-based cleaning for efficiency and durability; adopt a protective strategy that includes a soft interlayer between hard materials and the panel surface to maximize protection against dust and debris.

Abstract: Solar energy is used for many mission-critical applications including space
exploration, sensor systems to monitor wildfires, etc. Their operation can be
limited or even terminated if solar panels are covered with dust or hit by
space debris. To address this issue, we designed panel cleaning mechanisms and
tested protective materials. For cleaning mechanisms, we designed and compared
a wiper system and a rail system. For protective materials, we found through
collision tests that polycarbonate was very promising, though the most
important factor was layering a soft material between the panel's surface and a
hard material. In the cleaning system comparisons, the wiper-based system was
more efficient than the rail-based system in terms of cost, cleaning speed, and
total power consumption.

</details>


### [84] [iFlyBot-VLM Technical Report](https://arxiv.org/abs/2511.04976)
*Xin Nie,Zhiyuan Cheng,Yuan Zhang,Chao Ji,Jiajia Wu,Yuhan Zhang,Jia Pan*

Main category: cs.RO

TL;DR: Introduces iFlyBot-VLM, a general-purpose Vision-Language Model for Embodied Intelligence that bridges perception and robotic control via a transferable Operational Language, enabling four core capabilities, evaluated on 10 mainstream VLM benchmarks with claimed optimal performance, and plans public release of data and weights.


<details>
  <summary>Details</summary>
Motivation: Bridge the semantic gap between high-dimensional environmental perception and low-level robotic motion control, enabling seamless perception-action loops across diverse robotic platforms and moving toward a generalist embodied AI foundation.

Method: Proposes four capabilities: (1) Spatial Understanding and Metric Reasoning; (2) Interactive Target Grounding; (3) Action Abstraction and Control Parameter Generation; (4) Task Planning and Skill Sequencing. Develops a body-agnostic, transferable Operational Language to unify perception-action. Evaluates on 10 mainstream embodied-intelligence VLM benchmarks (e.g., Blink, Where2Place) and intends to publicly release training data and model weights.

Result: Reported optimal performance on the evaluated benchmarks while preserving the model’s general capabilities, suggesting strong cross-domain generalization and robust perception-action coordination.

Conclusion: iFlyBot-VLM is positioned as a scalable, generalizable foundation model for embodied AI, aiming to replace task-specific systems with cognitively capable, generalist agents. Public release of data and weights is planned to advance research in the field.

Abstract: We introduce iFlyBot-VLM, a general-purpose Vision-Language Model (VLM) used
to improve the domain of Embodied Intelligence. The central objective of
iFlyBot-VLM is to bridge the cross-modal semantic gap between high-dimensional
environmental perception and low-level robotic motion control. To this end, the
model abstracts complex visual and spatial information into a body-agnostic and
transferable Operational Language, thereby enabling seamless perception-action
closed-loop coordination across diverse robotic platforms. The architecture of
iFlyBot-VLM is systematically designed to realize four key functional
capabilities essential for embodied intelligence: 1) Spatial Understanding and
Metric Reasoning; 2) Interactive Target Grounding; 3) Action Abstraction and
Control Parameter Generation; 4) Task Planning and Skill Sequencing. We
envision iFlyBot-VLM as a scalable and generalizable foundation model for
embodied AI, facilitating the progression from specialized task-oriented
systems toward generalist, cognitively capable agents. We conducted evaluations
on 10 current mainstream embodied intelligence-related VLM benchmark datasets,
such as Blink and Where2Place, and achieved optimal performance while
preserving the model's general capabilities. We will publicly release both the
training data and model weights to foster further research and development in
the field of Embodied Intelligence.

</details>


### [85] [A semi-analytical approach for computing the largest singularity-free spheres of a class of 6-6 Stewart-Gough platforms for specified orientation workspaces](https://arxiv.org/abs/2511.04992)
*Bibekananda Patra,Sandipan Bandyopadhyay*

Main category: cs.RO

TL;DR: Analytical-per-orientation method to compute the largest singularity-free sphere (SFS) for a 6-6 Stewart-Gough platform under a fixed orientation, with sampling over the orientation workspace and comparison across four SGPM architectures.


<details>
  <summary>Details</summary>
Motivation: Singularities constrain motion in parallel manipulators; quantifying safe operating regions via the SFS helps in analysis and design; comparing architectures informs design choices for larger singularity-free volumes.

Method: For a fixed moving-platform orientation, compute the SFS analytically. Repeat across a set of orientation samples in the workspace and designate the smallest SFS as the SFS for that orientation workspace. Perform numerical experiments on four SGPM architectures to compare their SFS volumes within the same orientation workspace.

Result: The method yields comparative SFS volumes across four SGPM architectures, illustrating the utility of the approach for analysis and design, though specific numerical results are not provided in the abstract.

Conclusion: The proposed computational method offers a practical tool to evaluate and guide SGPM design by characterizing singularity-free regions across orientations.

Abstract: This article presents a method for computing the largest singularity-free
sphere (SFS) of a 6-6 Stewart-Gough platform manipulator (SGPM) over a
specified orientation workspace. For a fixed orientation of the moving
platform, the SFS is computed analytically. This process is repeated over a set
of samples generated within the orientation workspace, and the smallest among
them is designated as the desired SFS for the given orientation workspace.
Numerical experiments are performed on four distinct architectures of the SGPM
to understand their relative performances w.r.t. SFS volumes over the same
orientation workspace. This study demonstrates the potential utility of the
proposed computational method both in analysis and design of SGPMs.

</details>


### [86] [Encoding Biomechanical Energy Margin into Passivity-based Synchronization for Networked Telerobotic Systems](https://arxiv.org/abs/2511.04994)
*Xingyuan Zhou,Peter Paik,S. Farokh Atashzar*

Main category: cs.RO

TL;DR: TBPS2 offers a biomechanics-aware, two-port passivity-based stabilizer to improve position synchronization in networked haptic teleoperation, reducing conservatism while ensuring stability; validated via simulations and experiments under various delays and conditions.


<details>
  <summary>Details</summary>
Motivation: Maintain system stability and accurate position tracking in networked robotic/haptic systems, addressing desynchronization caused by imperfect communication and non-passive behaviors, by integrating human biomechanics into stabilizers.

Method: Propose TBPS2: a two-port biomechanics-aware passivity-based synchronizer and stabilizer. Provide mathematical design synthesis and a stability proof. Validate through grid simulations and systematic experiments, comparing against state-of-the-art under varying time delays and environmental conditions.

Result: TBPS2 optimizes position synchronization and reduces conservatism in stabilization activation. The design is proven stable, and simulations/experiments show favorable performance compared with state-of-the-art under delayed and diverse conditions.

Conclusion: TBPS2 advances biomechanics-aware stability in teleoperation by combining a two-port passivity-based framework with reduced activation conservatism, with theoretical guarantees and empirical validation across delays and environments.

Abstract: Maintaining system stability and accurate position tracking is imperative in
networked robotic systems, particularly for haptics-enabled human-robot
interaction. Recent literature has integrated human biomechanics into the
stabilizers implemented for teleoperation, enhancing force preservation while
guaranteeing convergence and safety. However, position desynchronization due to
imperfect communication and non-passive behaviors remains a challenge. This
paper proposes a two-port biomechanics-aware passivity-based synchronizer and
stabilizer, referred to as TBPS2. This stabilizer optimizes position
synchronization by leveraging human biomechanics while reducing the
stabilizer's conservatism in its activation. We provide the mathematical design
synthesis of the stabilizer and the proof of stability. We also conducted a
series of grid simulations and systematic experiments, comparing their
performance with that of state-of-the-art solutions under varying time delays
and environmental conditions.

</details>


### [87] [MoE-DP: An MoE-Enhanced Diffusion Policy for Robust Long-Horizon Robotic Manipulation with Skill Decomposition and Failure Recovery](https://arxiv.org/abs/2511.05007)
*Baiye Cheng,Tianhai Liang,Suning Huang,Maanping Shao,Feihong Zhang,Botian Xu,Zhengrong Xue,Huazhe Xu*

Main category: cs.RO

TL;DR: Mixture of Experts-Enhanced Diffusion Policy (MoE-DP) inserts a Mixture of Experts layer between the visual encoder and the diffusion policy to obtain robust, interpretable visuomotor control. Dynamic experts handle different task phases; achieves robustness gains and interpretable skill decomposition; 6 long-horizon tasks show 36% relative success improvement under disturbances; validated in real world; enables inference-time subtask rearrangement without retraining; code/video available.


<details>
  <summary>Details</summary>
Motivation: Long-horizon visuomotor tasks suffer from subtask failures and brittle representations; need robust recovery and interpretable, modular policy components.

Method: Introduce a MoE layer between visual encoder and diffusion model, with multiple specialized experts activated dynamically to cover different task phases; train to decompose policy knowledge into experts; evaluate robustness and interpretability; compare with baselines.

Result: 36% average relative improvement in success rate under disturbed conditions across 6 simulation tasks; real-world validation; interpretable skill decomposition where experts align with semantic primitives (e.g., approaching, grasping); inference-time control by rearranging subtasks without retraining.

Conclusion: MoE-DP improves robustness and interpretability in diffusion-based visuomotor policies; modular expert decomposition enables flexible, disturbance-resilient control and reordering of subtasks without re-training; results supported by sim and real-world experiments; code/video available.

Abstract: Diffusion policies have emerged as a powerful framework for robotic
visuomotor control, yet they often lack the robustness to recover from subtask
failures in long-horizon, multi-stage tasks and their learned representations
of observations are often difficult to interpret. In this work, we propose the
Mixture of Experts-Enhanced Diffusion Policy (MoE-DP), where the core idea is
to insert a Mixture of Experts (MoE) layer between the visual encoder and the
diffusion model. This layer decomposes the policy's knowledge into a set of
specialized experts, which are dynamically activated to handle different phases
of a task. We demonstrate through extensive experiments that MoE-DP exhibits a
strong capability to recover from disturbances, significantly outperforming
standard baselines in robustness. On a suite of 6 long-horizon simulation
tasks, this leads to a 36% average relative improvement in success rate under
disturbed conditions. This enhanced robustness is further validated in the real
world, where MoE-DP also shows significant performance gains. We further show
that MoE-DP learns an interpretable skill decomposition, where distinct experts
correspond to semantic task primitives (e.g., approaching, grasping). This
learned structure can be leveraged for inference-time control, allowing for the
rearrangement of subtasks without any re-training.Our video and code are
available at the https://moe-dp-website.github.io/MoE-DP-Website/.

</details>


### [88] [Tunable Passivity Control for Centralized Multiport Networked Systems](https://arxiv.org/abs/2511.05026)
*Xingyuan Zhou,Peter Paik,S. Farokh Atashzar*

Main category: cs.RO

TL;DR: A centralized, data-driven passivity-based stabilization framework for CMND systems, introducing Tunable Centralized Optimal Passivity Control (TCoPC) with a centralized passivity observer to distribute dissipation and ensure L2 stability under time-varying delays, relaxing node passivity assumptions and improving scalability.


<details>
  <summary>Details</summary>
Motivation: Stability of Centralized Multiport Networked Dynamic (CMND) systems with network-induced artifacts; limitations of conventional distributed passive controllers; need for centralized, optimal, model-free stabilization to guarantee strict passivity and scalability.

Method: Combine a centralized passivity observer with a tunable centralized optimal passivity controller (TCoPC). The controller optimizes dissipation distribution across sub-nets in a data-driven, model-free manner to guarantee strict passivity and L2 stability, even with time-varying delays and relaxed node-passivity requirements.

Result: Simulation results show effective stabilization and performance under varying delays, with flexibility in dissipation allocation and improved scalability and generalizability while relaxing minimum-phase and passivity assumptions.

Conclusion: The proposed TCoPC framework provides a centralized, data-driven route to guarantee strict passivity and L2 stability for CMND systems, enabling flexible dissipation distribution, robustness to delays, and enhanced scalability across complex networked dynamics.

Abstract: Centralized Multiport Networked Dynamic (CMND) systems have emerged as a key
architecture with applications in several complex network systems, such as
multilateral telerobotics and multi-agent control. These systems consist of a
hub node/subsystem connecting with multiple remote nodes/subsystems via a
networked architecture. One challenge for this system is stability, which can
be affected by non-ideal network artifacts. Conventional passivity-based
approaches can stabilize the system under specialized applications like
small-scale networked systems. However, those conventional passive stabilizers
have several restrictions, such as distributing compensation across subsystems
in a decentralized manner, limiting flexibility, and, at the same time, relying
on the restrictive assumptions of node passivity. This paper synthesizes a
centralized optimal passivity-based stabilization framework for CMND systems.
It consists of a centralized passivity observer monitoring overall energy flow
and an optimal passivity controller that distributes the just-needed
dissipation among various nodes, guaranteeing strict passivity and, thus, L2
stability. The proposed data-driven model-free approach, i.e., Tunable
Centralized Optimal Passivity Control (TCoPC), optimizes total performance
based on the prescribed dissipation distribution strategy while ensuring
stability. The controller can put high dissipation loads on some sub-networks
while relaxing the dissipation on other nodes. Simulation results demonstrate
the proposed frameworks performance in a complex task under different
time-varying delay scenarios while relaxing the remote nodes minimum phase and
passivity assumption, enhancing the scalability and generalizability.

</details>


### [89] [Epically Powerful: An open-source software and mechatronics infrastructure for wearable robotic systems](https://arxiv.org/abs/2511.05033)
*Jennifer K. Leestma,Siddharth R. Nathella,Christoph P. O. Nuesslein,Snehil Mathur,Gregory S. Sawicki,Aaron J. Young*

Main category: cs.RO

TL;DR: Epically Powerful is an open-source software and hardware infrastructure that simplifies building wearable robotics by providing a Python-based interface, real-time visualization, and comprehensive hardware guides, targeting QDD actuators, SBCs, and sensors to accelerate development from raw hardware to modular, deployable devices.


<details>
  <summary>Details</summary>
Motivation: Wearable robotics development is often hindered by fragmented toolchains and lack of standardized software for integration of actuators, sensors, timing, data logging, and visualization. This work aims to lower barriers and speed prototyping by delivering a cohesive, extensible framework with documentation and example controllers.

Method: Develop an open-source software stack (Epically Powerful) with a Python interface that integrates quasi-direct drive actuators, single-board computers, and common sensors. Include real-time visualization, data logging, and timing support, plus a hardware compatibility guide, example controllers, and a parts list to facilitate hardware selection and assembly.

Result: A modular framework and documentation suite that enables rapid development of wearable robotic systems and beyond, providing interfaces, controllers, and visualization to move from raw hardware to robust, deployable devices with reduced engineering effort.

Conclusion: Epically Powerful lowers the barrier to designing and deploying custom wearable robotics (and other systems using QDD actuators) by offering an extensible, well-documented infrastructure that supports hardware diversity and rapid prototyping without prescribing a fixed form factor.

Abstract: Epically Powerful is an open-source robotics infrastructure that streamlines
the underlying framework of wearable robotic systems - managing communication
protocols, clocking, actuator commands, visualization, sensor data acquisition,
data logging, and more - while also providing comprehensive guides for hardware
selection, system assembly, and controller implementation. Epically Powerful
contains a code base enabling simplified user implementation via Python that
seamlessly interfaces with various commercial state-of-the-art quasi-direct
drive (QDD) actuators, single-board computers, and common sensors, provides
example controllers, and enables real-time visualization. To further support
device development, the package also includes a recommended parts list and
compatibility guide and detailed documentation on hardware and software
implementation. The goal of Epically Powerful is to lower the barrier to
developing and deploying custom wearable robotic systems without a
pre-specified form factor, enabling researchers to go from raw hardware to
modular, robust devices quickly and effectively. Though originally designed
with wearable robotics in mind, Epically Powerful is broadly applicable to
other robotic domains that utilize QDD actuators, single-board computers, and
sensors for closed-loop control.

</details>


### [90] [TAPOM: Task-Space Topology-Guided Motion Planning for Manipulating Elongated Object in Cluttered Environments](https://arxiv.org/abs/2511.05052)
*Zihao Li,Yiming Zhu,Zhe Zhong,Qinyuan Ren,Yijiang Huang*

Main category: cs.RO

TL;DR: A topology-aware planning framework TAPOM improves low-clearance robotic manipulation by combining task-space topology analysis with guiding keyframes to steer a low-level planner, yielding higher success rates and efficiency.


<details>
  <summary>Details</summary>
Motivation: In narrow passages with elongated objects, existing planners struggle due to sampling difficulties and local minima, limiting manipulation in real-world constrained environments.

Method: TAPOM performs high-level topology analysis to identify critical pathways and generate guiding keyframes, which are used to steer a low-level configuration-space trajectory planner to feasible solutions.

Result: Experiments show significantly higher success rates and improved efficiency compared with state-of-the-art methods on low-clearance manipulation tasks.

Conclusion: Topology-aware planning broadens robotic manipulation capabilities in complex real-world environments and has wide applicability for constrained-space manipulation.

Abstract: Robotic manipulation in complex, constrained spaces is vital for widespread
applications but challenging, particularly when navigating narrow passages with
elongated objects. Existing planning methods often fail in these low-clearance
scenarios due to the sampling difficulties or the local minima. This work
proposes Topology-Aware Planning for Object Manipulation (TAPOM), which
explicitly incorporates task-space topological analysis to enable efficient
planning. TAPOM uses a high-level analysis to identify critical pathways and
generate guiding keyframes, which are utilized in a low-level planner to find
feasible configuration space trajectories. Experimental validation demonstrates
significantly high success rates and improved efficiency over state-of-the-art
methods on low-clearance manipulation tasks. This approach offers broad
implications for enhancing manipulation capabilities of robots in complex
real-world environments.

</details>


### [91] [Decomposed Object Manipulation via Dual-Actor Policy](https://arxiv.org/abs/2511.05129)
*Bin Fan,Jianjian Jiang,Zhuohao Li,Yixiang He,Xiaoming Wu,Yihan Yang,Shengbang Liu,Weishi Zheng*

Main category: cs.RO

TL;DR: A dual-actor policy for object manipulation that separates approaching and manipulation stages with stage-aware decision making and two visual priors, achieving better performance than SOTA on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Object manipulation across similar parts requires distinct strategies for the approaching and manipulation stages. Relying on a single policy with limited priors hampers generalization and efficiency across diverse objects.

Method: Propose Dual-Actor Policy (DAP) with an affordance-based actor to locate the functional part and guide the approaching stage, a motion flow-based actor to model the component's movement for the manipulation stage, and a decision maker to switch between actors based on the current stage. Address data scarcity by constructing the Dual-Prior Object Manipulation Dataset that combines both visual priors and includes seven tasks, including long-term multi-stage tasks. Evaluate on the RoboTwin benchmark and real-world scenarios.

Result: DAP achieves consistent improvements over the state-of-the-art: on average by 5.55% on the Dual-Prior Object Manipulation Dataset, 14.7% on RoboTwin, and 10.4% in real-world scenarios.

Conclusion: Integrating stage-aware policies with complementary visual priors and a stage-detection module yields robust multi-stage object manipulation, demonstrating strong performance gains across simulated and real environments. The proposed dataset also provides a resource for training and evaluating stage-aware manipulation methods; future work could explore additional priors, more stages, and closer sim-to-real alignment.

Abstract: Object manipulation, which focuses on learning to perform tasks on similar
parts across different types of objects, can be divided into an approaching
stage and a manipulation stage. However, previous works often ignore this
characteristic of the task and rely on a single policy to directly learn the
whole process of object manipulation. To address this problem, we propose a
novel Dual-Actor Policy, termed DAP, which explicitly considers different
stages and leverages heterogeneous visual priors to enhance each stage.
Specifically, we introduce an affordance-based actor to locate the functional
part in the manipulation task, thereby improving the approaching process.
Following this, we propose a motion flow-based actor to capture the movement of
the component, facilitating the manipulation process. Finally, we introduce a
decision maker to determine the current stage of DAP and select the
corresponding actor. Moreover, existing object manipulation datasets contain
few objects and lack the visual priors needed to support training. To address
this, we construct a simulated dataset, the Dual-Prior Object Manipulation
Dataset, which combines the two visual priors and includes seven tasks,
including two challenging long-term, multi-stage tasks. Experimental results on
our dataset, the RoboTwin benchmark and real-world scenarios illustrate that
our method consistently outperforms the SOTA method by 5.55%, 14.7% and 10.4%
on average respectively.

</details>


### [92] [Follow-Me in Micro-Mobility with End-to-End Imitation Learning](https://arxiv.org/abs/2511.05158)
*Sahar Salimpour,Iacopo Catalano,Tomi Westerlund,Mohsen Falahi,Jorge Peña Queralta*

Main category: cs.RO

TL;DR: Imitation learning improves comfort in autonomous wheelchair follow-me mode, outperforming manually-tuned controllers; evaluates multiple end-to-end NN architectures for production deployments.


<details>
  <summary>Details</summary>
Motivation: Deployment environments for autonomous micro-mobility (large indoor spaces, crowded urban areas) are challenging, with user comfort and experience as critical metrics alongside traditional ones like time or distance, especially for commercial applications.

Method: Train controllers via imitation learning and compare different end-to-end neural network architectures; validate on DAAV's autonomous wheelchair in follow-me mode; assess usability in real-world production-level deployments.

Result: Imitation learning produces smoother, higher-comfort controllers and achieves state-of-the-art comfort in follow-me mode; demonstrates viability of various neural architectures for production deployments.

Conclusion: Imitation learning is effective for optimizing user comfort and overall UX in autonomous micro-mobility; end-to-end architectures are viable for real-world deployments; comfort should be a central evaluation metric in design.

Abstract: Autonomous micro-mobility platforms face challenges from the perspective of
the typical deployment environment: large indoor spaces or urban areas that are
potentially crowded and highly dynamic. While social navigation algorithms have
progressed significantly, optimizing user comfort and overall user experience
over other typical metrics in robotics (e.g., time or distance traveled) is
understudied. Specifically, these metrics are critical in commercial
applications. In this paper, we show how imitation learning delivers smoother
and overall better controllers, versus previously used manually-tuned
controllers. We demonstrate how DAAV's autonomous wheelchair achieves
state-of-the-art comfort in follow-me mode, in which it follows a human
operator assisting persons with reduced mobility (PRM). This paper analyzes
different neural network architectures for end-to-end control and demonstrates
their usability in real-world production-level deployments.

</details>


### [93] [Procedimiento de auditoría de ciberseguridad para sistemas autónomos: metodología, amenazas y mitigaciones](https://arxiv.org/abs/2511.05185)
*Adrián Campazas-Vega,Claudia Álvarez-Aparicio,David Sobrín-Hidalgo,Laura Inyesto-Alonso,Francisco Javier Rodríguez-Lera,Vicente Matellán-Olivera,Ángel Manuel Guerrero-Higueras*

Main category: cs.RO

TL;DR: A security auditing framework for autonomous systems using a layered approach, tailored robot threat taxonomy, and concrete mitigations; validated via four real-world robotic case studies.


<details>
  <summary>Details</summary>
Motivation: Autonomous systems are expanding into critical and human-interactive domains, which increases security risks and attack surfaces due to complexity and integration with diverse environments.

Method: Introduce a layer-structured security auditing procedure, adapt a threat taxonomy to robotics, and provide concrete mitigation measures; validate the approach through four case studies on Ghost Robotics Vision 60, Unitree A1, Universal Robots UR3, and Pepper by Aldebaran.

Result: Demonstrates the feasibility and relevance of the proposed framework across different robotic platforms, offering actionable mitigation guidance and a basis for systematic security audits.

Conclusion: The framework offers a practical, cross-platform approach to security auditing for autonomous systems and could inform standardization and tooling; further work could address quantitative validation, broader domain generalization, and automation of the auditing process.

Abstract: The deployment of autonomous systems has experienced remarkable growth in
recent years, driven by their integration into sectors such as industry,
medicine, logistics, and domestic environments. This expansion is accompanied
by a series of security issues that entail significant risks due to the
critical nature of autonomous systems, especially those operating in
human-interaction environments. Furthermore, technological advancement and the
high operational and architectural complexity of autonomous systems have
resulted in an increased attack surface. This article presents a specific
security auditing procedure for autonomous systems, based on a layer-structured
methodology, a threat taxonomy adapted to the robotic context, and a set of
concrete mitigation measures. The validity of the proposed approach is
demonstrated through four practical case studies applied to representative
robotic platforms: the Vision 60 military quadruped from Ghost Robotics, the A1
robot from Unitree Robotics, the UR3 collaborative arm from Universal Robots,
and the Pepper social robot from Aldebaran Robotics.

</details>


### [94] [Let Me Show You: Learning by Retrieving from Egocentric Video for Robotic Manipulation](https://arxiv.org/abs/2511.05199)
*Yichen Zhu,Feifei Feng*

Main category: cs.RO

TL;DR: A retrieval-from-video framework uses human demonstrations and mid-level cues to train robots via a video retriever and policy generator, improving manipulation performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Robots struggle with manipulation in complex and uncertain environments with limited data; humans learn by watching videos; leveraging abundant human demonstrations and mid-level signals can boost learning and generalization.

Method: Construct a video bank of human task demonstrations; extract mid-level information such as object affordance masks and hand motion trajectories; implement a dual-component system: a video retriever that fetches task-relevant videos from an external bank based on task specification, and a policy generator that integrates retrieved knowledge into the learning loop.

Result: Demonstrates a marked improvement in performance over conventional robotic systems across multiple simulated and real-world tests, with better generalization to unseen tasks.

Conclusion: RfV is a promising direction for robot learning, showing that external video retrieval and mid-level representations can enhance manipulation policy learning and generalization; future work could broaden the video bank and refine the mid-level cues.

Abstract: Robots operating in complex and uncertain environments face considerable
challenges. Advanced robotic systems often rely on extensive datasets to learn
manipulation tasks. In contrast, when humans are faced with unfamiliar tasks,
such as assembling a chair, a common approach is to learn by watching video
demonstrations. In this paper, we propose a novel method for learning robot
policies by Retrieving-from-Video (RfV), using analogies from human
demonstrations to address manipulation tasks. Our system constructs a video
bank comprising recordings of humans performing diverse daily tasks. To enrich
the knowledge from these videos, we extract mid-level information, such as
object affordance masks and hand motion trajectories, which serve as additional
inputs to enhance the robot model's learning and generalization capabilities.
We further feature a dual-component system: a video retriever that taps into an
external video bank to fetch task-relevant video based on task specification,
and a policy generator that integrates this retrieved knowledge into the
learning cycle. This approach enables robots to craft adaptive responses to
various scenarios and generalize to tasks beyond those in the training data.
Through rigorous testing in multiple simulated and real-world settings, our
system demonstrates a marked improvement in performance over conventional
robotic systems, showcasing a significant breakthrough in the field of
robotics.

</details>


### [95] [Beyond Master and Apprentice: Grounding Foundation Models for Symbiotic Interactive Learning in a Shared Latent Space](https://arxiv.org/abs/2511.05203)
*Linus Nwankwo,Björn Ellensohn,Christian Rauch,Elmar Rueckert*

Main category: cs.RO

TL;DR: SIL enables mutual, bidirectional learning between human and embodied agent, moving from reactive instruction-following to proactive clarification and shared plan refinement through a shared latent task space and memory.


<details>
  <summary>Details</summary>
Motivation: Current HRI grounding relies on a master–apprentice model where the embodied agent passively executes commands, lacking bidirectional learning and co-adaptation. This limits handling of long-horizon tasks and smooth human–robot collaboration; there's a need to emulate the bidirectional dynamics of human–human interaction.

Method: Formalize Symbiotic Interactive Learning (SIL) as a co-adaptation process in a shared latent task space with joint belief states that evolve via interaction history. Enable proactive clarification, adaptive suggestions, and shared plan refinement. Leverage pre-trained foundation models for spatial perception and reasoning, use a lightweight latent encoder to ground outputs into task-specific representations, and employ a memory architecture to prevent forgetting. Validate on simulated and real-world embodied tasks (instruction following, information retrieval, query-oriented reasoning, and interactive dialogues). Public demos/resources at the provided URL.

Result: Demonstrated across both simulated and real-world embodied tasks, showing that agents can engage in proactive, bidirectional interactions and plan refinement with humans; memory mechanisms help stabilize evolving task representations. Demos and resources are publicly available at the stated URL.

Conclusion: SIL enables symbiotic, bidirectional learning in HRI, moving beyond reactive execution toward proactive collaboration, plan refinement, and sustained adaptation. The work provides public resources to facilitate replication and further study.

Abstract: Today's autonomous agents can understand free-form natural language
instructions and execute long-horizon tasks in a manner akin to human-level
reasoning. These capabilities are mostly driven by large-scale pre-trained
foundation models (FMs). However, the approaches with which these models are
grounded for human-robot interaction (HRI) perpetuate a master-apprentice
model, where the apprentice (embodied agent) passively receives and executes
the master's (human's) commands without reciprocal learning. This reactive
interaction approach does not capture the co-adaptive dynamics inherent in
everyday multi-turn human-human interactions. To address this, we propose a
Symbiotic Interactive Learning (SIL) approach that enables both the master and
the apprentice to co-adapt through mutual, bidirectional interactions. We
formalised SIL as a co-adaptation process within a shared latent task space,
where the agent and human maintain joint belief states that evolve based on
interaction history. This enables the agent to move beyond reactive execution
to proactive clarification, adaptive suggestions, and shared plan refinement.
To realise these novel behaviours, we leveraged pre-trained FMs for spatial
perception and reasoning, alongside a lightweight latent encoder that grounds
the models' outputs into task-specific representations. Furthermore, to ensure
stability as the tasks evolve, we augment SIL with a memory architecture that
prevents the forgetting of learned task-space representations. We validate SIL
on both simulated and real-world embodied tasks, including instruction
following, information retrieval, query-oriented reasoning, and interactive
dialogues. Demos and resources are public
at:~\href{https://linusnep.github.io/SIL/}{https://linusnep.github.io/SIL/}.

</details>


### [96] [Context-aware Learned Mesh-based Simulation via Trajectory-Level Meta-Learning](https://arxiv.org/abs/2511.05234)
*Philipp Dahlinger,Niklas Freymuth,Tai Hoang,Tobias Würth,Michael Volpp,Luise Kärger,Gerhard Neumann*

Main category: cs.RO

TL;DR: A trajectory-level meta-learning framework for learned graph network simulators (M3GN) that uses Conditional Neural Processes and movement primitives to rapidly adapt to new deformation scenarios, delivering faster and more accurate mesh-based simulations than existing GNSs.


<details>
  <summary>Details</summary>
Motivation: Existing learned simulators rely on single-step observations and autoregressive rollouts, which limit learning of material properties and cause error accumulation over long trajectories. There is a need for temporally aware, fast-adapting simulators in applications like robotics, manufacturing, and structural mechanics.

Method: Frame mesh-based simulation as a trajectory-level meta-learning problem using Conditional Neural Processes to enable rapid adaptation to new scenarios from limited initial data. Employ movement primitives to predict fast, stable, and accurate simulations from a single model call. The proposed model is Movement-primitive Meta-MeshGraphNet (M3GN).

Result: M3GN achieves higher simulation accuracy at a fraction of the runtime cost compared to state-of-the-art GNSs across multiple tasks.

Conclusion: The trajectory-level meta-learning approach with Conditional Neural Processes and movement primitives yields fast, accurate, and adaptable mesh-based simulations (M3GN), enabling more effective and scalable simulations in engineering and robotics.

Abstract: Simulating object deformations is a critical challenge across many scientific
domains, including robotics, manufacturing, and structural mechanics. Learned
Graph Network Simulators (GNSs) offer a promising alternative to traditional
mesh-based physics simulators. Their speed and inherent differentiability make
them particularly well suited for applications that require fast and accurate
simulations, such as robotic manipulation or manufacturing optimization.
However, existing learned simulators typically rely on single-step
observations, which limits their ability to exploit temporal context. Without
this information, these models fail to infer, e.g., material properties.
Further, they rely on auto-regressive rollouts, which quickly accumulate error
for long trajectories. We instead frame mesh-based simulation as a
trajectory-level meta-learning problem. Using Conditional Neural Processes, our
method enables rapid adaptation to new simulation scenarios from limited
initial data while capturing their latent simulation properties. We utilize
movement primitives to directly predict fast, stable and accurate simulations
from a single model call. The resulting approach, Movement-primitive
Meta-MeshGraphNet (M3GN), provides higher simulation accuracy at a fraction of
the runtime cost compared to state-of-the-art GNSs across several tasks.

</details>


### [97] [TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models](https://arxiv.org/abs/2511.05275)
*Hokyun Im,Euijin Jeong,Jianlong Fu,Andrey Kolobov,Youngwoon Lee*

Main category: cs.RO

TL;DR: TwinVLA introduces a modular approach that pairs two pretrained single-arm VLAs to form a coordinated bimanual policy, achieving data-efficient high performance without bimanual pretraining.


<details>
  <summary>Details</summary>
Motivation: Public datasets are dominated by single-arm demonstrations, making end-to-end bimanual VLA training data-intensive. A modular method that reuses existing single-arm data could enable effective bimanual manipulation with less data and compute.

Method: Create TwinVLA by composing two copies of a pretrained single-arm VLA into a coordinated bimanual system. The framework is modular and does not require bimanual pretraining. The approach is evaluated on diverse bimanual tasks in real-world and simulation, and compared against a monolithic cross-embodiment model and strong baselines like RDT-1B and π0.

Result: TwinVLA achieves superior data efficiency and performance relative to a comparably-sized monolithic model and outperforms baselines without bimanual pretraining. It narrows the gap to the state-of-the-art π0, which relies on extensive proprietary bimanual data and compute. The results support modular composition as a scalable, data-efficient path for high-performance bimanual manipulation.

Conclusion: Modular composition of pretrained single-arm VLAs is a viable and scalable strategy for bimanual manipulation, enabling competitive performance using public single-arm data and avoiding large-scale bimanual pretraining.

Abstract: Vision-language-action models (VLAs) trained on large-scale robotic datasets
have demonstrated strong performance on manipulation tasks, including bimanual
tasks. However, because most public datasets focus on single-arm
demonstrations, adapting VLAs for bimanual tasks typically requires substantial
additional bimanual data and fine-tuning. To address this challenge, we
introduce TwinVLA, a modular framework that composes two copies of a pretrained
single-arm VLA into a coordinated bimanual VLA. Unlike monolithic
cross-embodiment models trained on mixtures of single-arm and bimanual data,
TwinVLA improves both data efficiency and performance by composing pretrained
single-arm policies. Across diverse bimanual tasks in real-world and simulation
settings, TwinVLA outperforms a comparably-sized monolithic RDT-1B model
without requiring any bimanual pretraining. Furthermore, it narrows the gap to
state-of-the-art model, $\pi_0$ which rely on extensive proprietary bimanual
data and compute cost. These results establish our modular composition approach
as a data-efficient and scalable path toward high-performance bimanual
manipulation, leveraging public single-arm data.

</details>


### [98] [Force-Safe Environment Maps and Real-Time Detection for Soft Robot Manipulators](https://arxiv.org/abs/2511.05307)
*Akua K. Dickson,Juan C. Pacheco Garcia,Andrew P. Sabelhaus*

Main category: cs.RO

TL;DR: A force-safety aware framework for soft robot manipulators that maps contact-force limits from task space to configuration space to enable real-time force-safe manipulation in delicate environments, validated on a two-segment pneumatic soft arm.


<details>
  <summary>Details</summary>
Motivation: Delicate obstacles require strict force limits; current obstacle avoidance often ignores interaction forces, risking damage. There is a need for real-time force-safety guarantees that respect environmental contact-force limits.

Method: Map force safety criteria from task space to configuration space via forward kinematics, incorporating allowable contact-force limits for obstacles. Classify configurations as safe if the predicted forces remain below thresholds, enabling real-time force-safe planning. Validation performed in simulation and hardware on a two-segment pneumatic soft robot.

Result: The approach accurately detects force safety during interactions with deformable obstacles in both simulation and hardware experiments, enabling real-time force-safe planning foundations for soft manipulators.

Conclusion: Provides a provable notion of force safety for soft robot manipulators, enabling real-time safe planning in delicate, cluttered environments and paving the way for deployment in contact-rich tasks.

Abstract: Soft robot manipulators have the potential for deployment in delicate
environments to perform complex manipulation tasks. However, existing obstacle
detection and avoidance methods do not consider limits on the forces that
manipulators may exert upon contact with delicate obstacles. This work
introduces a framework that maps force safety criteria from task space (i.e.
positions along the robot's body) to configuration space (i.e. the robot's
joint angles) and enables real-time force safety detection. We incorporate
limits on allowable environmental contact forces for given task-space
obstacles, and map them into configuration space (C-space) through the
manipulator's forward kinematics. This formulation ensures that configurations
classified as safe are provably below the maximum force thresholds, thereby
allowing us to determine force-safe configurations of the soft robot
manipulator in real-time. We validate our approach in simulation and hardware
experiments on a two-segment pneumatic soft robot manipulator. Results
demonstrate that the proposed method accurately detects force safety during
interactions with deformable obstacles, thereby laying the foundation for
real-time safe planning of soft manipulators in delicate, cluttered
environments.

</details>


### [99] [ETHOS: A Robotic Encountered-Type Haptic Display for Social Interaction in Virtual Reality](https://arxiv.org/abs/2511.05379)
*Eric Godden,Jacquie Groenewegen,Matthew K. X. J. Pan*

Main category: cs.RO

TL;DR: ETHOS is a dynamic encountered-type haptic display for natural VR social touch, using a torque-controlled manipulator with interchangeable props, marker-based registration, and safety gating; it offers static and dynamic control modes with measurable alignment (≈5 mm) and contact latency (~29 ms).


<details>
  <summary>Details</summary>
Motivation: To enable natural, contact-rich social interactions in VR (e.g., handovers, fist bumps, high-fives) by addressing the limitations of static haptic displays and enhancing realism through dynamic prop positioning and safety controls.

Method: Hardware: torque-controlled robotic manipulator; interchangeable passive props (silicone hand replicas, baton); marker-based physical-virtual registration using a ChArUco board; safety monitor gating motion by head/hand pose. Control strategies: (i) static mode with stationary prop aligned to virtual counterpart, (ii) dynamic mode that updates prop position via exponential blending of an initial mid-point trajectory with real-time hand tracking to create a unique contact point per interaction. Evaluation included bench tests for colocated accuracy and user experiments measuring contact latency.

Result: Static colocated accuracy: 5.09 ± 0.94 mm. User interactions achieved average contact latency of 28.53 ± 31.21 ms across all conditions. The system demonstrates feasibility of recreating socially meaningful haptics in VR, with safety and control mechanisms enabling high-fidelity dynamic interpersonal interactions.

Conclusion: ETHOS provides a practical foundation for high-fidelity, dynamic interpersonal haptics in VR, balancing realistic contact with safety through well-integrated hardware, tracking, and control strategies.

Abstract: We present ETHOS (Encountered-Type Haptics for On-demand Social Interaction),
a dynamic encountered-type haptic display (ETHD) that enables natural physical
contact in virtual reality (VR) during social interactions such as handovers,
fist bumps, and high-fives. The system integrates a torque-controlled robotic
manipulator with interchangeable passive props (silicone hand replicas and a
baton), marker-based physical-virtual registration via a ChArUco board, and a
safety monitor that gates motion based on the user's head and hand pose. We
introduce two control strategies: (i) a static mode that presents a stationary
prop aligned with its virtual counterpart, consistent with prior ETHD
baselines, and (ii) a dynamic mode that continuously updates prop position by
exponentially blending an initial mid-point trajectory with real-time hand
tracking, generating a unique contact point for each interaction. Bench tests
show static colocation accuracy of 5.09 +/- 0.94 mm, while user interactions
achieved temporal alignment with an average contact latency of 28.53 +/- 31.21
ms across all interaction and control conditions. These results demonstrate the
feasibility of recreating socially meaningful haptics in VR. By incorporating
essential safety and control mechanisms, ETHOS establishes a practical
foundation for high-fidelity, dynamic interpersonal interactions in virtual
environments.

</details>


### [100] [EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation](https://arxiv.org/abs/2511.05397)
*Samarth Chopra,Alex McMoil,Ben Carnovale,Evan Sokolson,Rajkumar Kubendran,Samuel Dickerson*

Main category: cs.RO

TL;DR: Low-cost, assemble-it-yourself 6-DOF robot uses a unified vision-language-action model with adaptive re-planning, achieving strong performance and enabling broader access to robotic foundation models.


<details>
  <summary>Details</summary>
Motivation: VLA models for robotics often rely on expensive hardware and underperform in novel or cluttered scenes. There is a need for affordable, reliable, and broadly accessible robotic systems that can operate in real-world environments.

Method: Introduces EverydayVLA, a ~$300 6-DOF manipulator; a single unified model outputs discrete and continuous actions; an adaptive-horizon ensemble monitors motion uncertainty to trigger on-the-fly re-planning for safe operation; evaluated on LIBERO and in real-world tests.

Result: On LIBERO, EverydayVLA matches state-of-the-art success rates; in real-world tests it outperforms prior methods by 49% in-distribution and 34.9% out-of-distribution.

Conclusion: Coupling a state-of-the-art VLA framework with cost-effective hardware democratizes access to a robotic foundation model, enabling economical use in homes and research labs; experiment videos and details are provided.

Abstract: While Vision-Language-Action (VLA) models map visual inputs and language
instructions directly to robot actions, they often rely on costly hardware and
struggle in novel or cluttered scenes. We introduce EverydayVLA, a 6-DOF
manipulator that can be assembled for under $300, capable of modest payloads
and workspace. A single unified model jointly outputs discrete and continuous
actions, and our adaptive-horizon ensemble monitors motion uncertainty to
trigger on-the-fly re-planning for safe, reliable operation. On LIBERO,
EverydayVLA matches state-of-the-art success rates, and in real-world tests it
outperforms prior methods by 49% in-distribution and 34.9% out-of-distribution.
By combining a state-of-the-art VLA with cost-effective hardware, EverydayVLA
democratizes access to a robotic foundation model and paves the way for
economical use in homes and research labs alike. Experiment videos and details:
https://everydayvla.github.io/

</details>


### [101] [Stable and Robust SLIP Model Control via Energy Conservation-Based Feedback Cancellation for Quadrupedal Applications](https://arxiv.org/abs/2511.05402)
*Muhammad Saud Ul Hassan,Derek Vasquez,Hamza Asif,Christian Hubicki*

Main category: cs.RO

TL;DR: An energy-conservation-based control architecture for stable dynamic quadruped running using a SLIP model, enabling leg orientation in flight and leg-length control during stance to track a stable parabolic spline; simulations on Ghost Robotics Minitaur show robust bouncing, even with up to 10% sensor error.


<details>
  <summary>Details</summary>
Motivation: To achieve robust, dynamic running gaits in quadrupeds by leveraging a reduced-order SLIP model and energy-conserving control to replicate stable bouncing behavior observed in biological and bio-inspired robots.

Method: Model the quadruped with a spring-loaded inverted pendulum (SLIP). Use leg orientation control in flight and leg-length control in stance. Compute a target stable parabolic spline during stance via energy conservation, and validate via simulations based on Ghost Robotics Minitaur.

Result: The control law enables stable bouncing gaits in simulation and demonstrates robustness to sensor measurement errors up to 10%.

Conclusion: An energy-conservation-based SLIP controller can achieve stable, robust dynamic locomotion in quadrupeds, suggesting viability for bio-inspired quadruped robots and potential real-world deployment.

Abstract: In this paper, we present an energy-conservation based control architecture
for stable dynamic motion in quadruped robots. We model the robot as a
Spring-loaded Inverted Pendulum (SLIP), a model well-suited to represent the
bouncing motion characteristic of running gaits observed in various biological
quadrupeds and bio-inspired robotic systems. The model permits leg-orientation
control during flight and leg-length control during stance, a design choice
inspired by natural quadruped behaviors and prevalent in robotic quadruped
systems. Our control algorithm uses the reduced-order SLIP dynamics of the
quadruped to track a stable parabolic spline during stance, which is calculated
using the principle of energy conservation. Through simulations based on the
design specifications of an actual quadruped robot, Ghost Robotics Minitaur, we
demonstrate that our control algorithm generates stable bouncing gaits.
Additionally, we illustrate the robustness of our controller by showcasing its
ability to maintain stable bouncing even when faced with up to a 10% error in
sensor measurements.

</details>


### [102] [Bioinspired Soft Quadrotors Jointly Unlock Agility, Squeezability, and Collision Resilience](https://arxiv.org/abs/2511.05426)
*Luca Girardi,Gabriel Maquignaz,Stefano Mintchev*

Main category: cs.RO

TL;DR: Soft-frame quadrotor FlexiQuad combines high deformability with agile flight, enabling squeezability and collision resilience, with a 405 g prototype achieving speeds over 80 km/h, accelerations up to 3 g linear and 300 rad/s^2, thrust-to-weight up to 8, and improved collision resilience; optimal softness 0.006–0.77 N/mm, scalable from 20–3000 g.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of rigid quadrotors in cluttered environments by introducing a soft-frame design inspired by natural flyers' anisotropic stiffness and distributed mass-energy, enabling safety-critical interactions without sacrificing performance.

Method: Design and test a soft-frame quadrotor (FlexiQuad) ~405 g; characterize flight performance (speed, accelerations), collision resilience (frontal/glancing), and compressibility through gaps; perform parametric analysis of frame softness to identify an optimal range; assess scalability to other masses (20–3000 g).

Result: Prototype demonstrates high-speed acrobatics and extreme accelerations while being far more compliant; thrust-to-weight up to 8 comparable to rigid quads; fourfold higher collision resilience; survivable frontal impacts at 5 m/s; reduced forces in glancing collisions by 39x; enables passage through gaps equal to 70% of nominal width; optimal softness 0.006–0.77 N/mm; scalable 20–3000 g.

Conclusion: FlexiQuad expands drone capabilities in cluttered environments by enabling robust physical interactions without sacrificing flight performance; identifies a practical softness window and suggests broad applicability across sizes.

Abstract: Natural flyers use soft wings to seamlessly enable a wide range of flight
behaviours, including agile manoeuvres, squeezing through narrow passageways,
and withstanding collisions. In contrast, conventional quadrotor designs rely
on rigid frames that support agile flight but inherently limit collision
resilience and squeezability, thereby constraining flight capabilities in
cluttered environments. Inspired by the anisotropic stiffness and distributed
mass-energy structures observed in biological organisms, we introduce
FlexiQuad, a soft-frame quadrotor design approach that limits this trade-off.
We demonstrate a 405-gram FlexiQuad prototype, three orders of magnitude more
compliant than conventional quadrotors, yet capable of acrobatic manoeuvres
with peak speeds above 80 km/h and linear and angular accelerations exceeding 3
g and 300 rad/s$^2$, respectively. Analysis demonstrates it can replicate
accelerations of rigid counterparts up to a thrust-to-weight ratio of 8.
Simultaneously, FlexiQuad exhibits fourfold higher collision resilience,
surviving frontal impacts at 5 m/s without damage and reducing destabilising
forces in glancing collisions by a factor of 39. Its frame can fully compress,
enabling flight through gaps as narrow as 70% of its nominal width. Our
analysis identifies an optimal structural softness range, from 0.006 to 0.77
N/mm, comparable to that of natural flyers' wings, whereby agility,
squeezability, and collision resilience are jointly achieved for FlexiQuad
models from 20 to 3000 grams. FlexiQuad expands hovering drone capabilities in
complex environments, enabling robust physical interactions without
compromising flight performance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [103] [Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity](https://arxiv.org/abs/2511.04686)
*Pratik Poudel*

Main category: cs.LG

TL;DR: KV cache growth in LLMs is not benign; eviction must respect positional encodings and the model's context window; simple strategies that preserve contiguous context blocks outperform aggressive or non-contiguous eviction.


<details>
  <summary>Details</summary>
Motivation: To understand how unbounded KV cache growth in stateful, multi-turn LLM inference interacts with architectural context limits and positional encodings, and how eviction strategies affect generation quality.

Method: Empirical analysis using a stateful benchmarking framework across models (e.g., Meta-Llama-3-8b-instruct). Systematically varied KV cache size and eviction policies (including high-retention ones like 99% via AttentionTop) to assess impact on generation quality and positional coherence.

Result: Generation quality degrades sharply as the accumulated KV cache approaches or exceeds the model’s trained context window (e.g., 8192 tokens). High-retention eviction can worsen performance if it disrupts positional coherence. Compacting the cache by removing non-contiguous tokens can scramble positional signals (e.g., RoPE) and degrade outputs. Simpler strategies that preserve contiguous context blocks (e.g., keeping an initial gist) yield more coherent generations than complex or position-disruptive ones.

Conclusion: Eviction strategies should respect architectural context limits, preserve positional structure, and treat cache health holistically rather than size alone.

Abstract: The Key-Value (KV) cache is integral to efficient autoregressive inference in
large language models (LLMs), yet its unbounded growth in stateful multi-turn
scenarios presents major challenges. This paper examines the interplay between
KV cache management strategies, the architectural context limits of models like
meta-llama/Meta-Llama-3-8b-instruct, and the often-overlooked integrity of
positional encodings. Through empirical analysis using a stateful benchmarking
framework, we show that LLM generation quality degrades sharply when the
accumulated KV cache approaches or exceeds the model's trained context window
(e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory
exhaustion. Common eviction strategies, even high-retention ones (e.g., 99% via
AttentionTop), can worsen performance if they disrupt positional coherence.
Because LLMs rely on consistent positional signals (e.g., RoPE), compacting a
cache by removing non-contiguous tokens can scramble these signals and lead to
degenerative outputs. We further show that simple strategies preserving
contiguous context blocks (e.g., keeping an initial "gist") can yield more
coherent generations than complex or positionally disruptive ones. We advocate
for eviction techniques that respect architectural limits, preserve positional
structure, and view "cache health" holistically beyond mere size.

</details>


### [104] [Ada-FCN: Adaptive Frequency-Coupled Network for fMRI-Based Brain Disorder Classification](https://arxiv.org/abs/2511.04718)
*Yue Xun,Jiaxing Xu,Wenbo Gao,Chen Yang,Shujun Wang*

Main category: cs.LG

TL;DR: Adaptive Cascade Decomposition and Frequency-Coupled Connectivity Learning within a Unified-GCN for rs-fMRI-based diagnosis, learning individualized frequency sub-bands and cross-band interactions.


<details>
  <summary>Details</summary>
Motivation: Current rs-fMRI models treat BOLD signals as a monolithic time series, neglecting multi-frequency information. Neurological disorders often manifest in disruptions within specific frequency bands, and predefined bands fail to capture individual variability.

Method: Adaptive Cascade Decomposition to learn task-relevant frequency sub-bands per brain region; Frequency-Coupled Connectivity Learning to capture intra- and cross-band interactions in a unified functional network; integrated into a Unified-GCN with a novel message-passing mechanism for diagnostic prediction.

Result: Demonstrates superior diagnostic performance on ADNI and ABIDE datasets compared to existing methods; code available at GitHub.

Conclusion: Introduces a unified framework that jointly learns frequency sub-bands and cross-band connectivity to enhance rs-fMRI-based diagnosis, with potential for personalized brain-network representations.

Abstract: Resting-state fMRI has become a valuable tool for classifying brain disorders
and constructing brain functional connectivity networks
  by tracking BOLD signals across brain regions. However, existing mod els
largely neglect the multi-frequency nature of neuronal oscillations,
  treating BOLD signals as monolithic time series. This overlooks the cru cial
fact that neurological disorders often manifest as disruptions within
  specific frequency bands, limiting diagnostic sensitivity and specificity.
  While some methods have attempted to incorporate frequency informa tion, they
often rely on predefined frequency bands, which may not be
  optimal for capturing individual variability or disease-specific alterations.
  To address this, we propose a novel framework featuring Adaptive Cas cade
Decomposition to learn task-relevant frequency sub-bands for each
  brain region and Frequency-Coupled Connectivity Learning to capture
  both intra- and nuanced cross-band interactions in a unified functional
  network. This unified network informs a novel message-passing mecha nism
within our Unified-GCN, generating refined node representations
  for diagnostic prediction. Experimental results on the ADNI and ABIDE
  datasets demonstrate superior performance over existing methods. The
  code is available at https://github.com/XXYY20221234/Ada-FCN.

</details>


### [105] [AWEMixer: Adaptive Wavelet-Enhanced Mixer Network for Long-Term Time Series Forecasting](https://arxiv.org/abs/2511.04722)
*Qianyang Li,Xingjun Zhang,Peng Tao,Shaoxun Wang,Yancheng Pan,Jia Wei*

Main category: cs.LG

TL;DR: AWEMixer introduces an Adaptive Wavelet-Enhanced Mixer Network for long-term IoT time-series forecasting, combining a Frequency Router and a Coherent Gated Fusion Block to achieve robust time-frequency localization and improved long-horizon accuracy over Transformer/MLP baselines.


<details>
  <summary>Details</summary>
Motivation: Long-term forecasting in IoT is challenging due to non-stationarity and multi-scale sensor signals; error accumulation worsens far-future forecasts; Fourier-based global views blur transient events in time-domain representations.

Method: Proposes AWEMixer: Frequency Router uses FFT-based global periodicity to adaptively weight localized wavelet subbands; Coherent Gated Fusion Block uses cross-attention and gating to selectively fuse prominent frequency features across multi-scale temporal representations, enabling accurate time-frequency localization while resisting noise.

Result: Evaluated on seven public benchmarks; outperformed recent state-of-the-art transformer- and MLP-based models in long-sequence forecasting; publicly available code.

Conclusion: Demonstrates that adaptively combining wavelet-localized features with global frequency cues yields robust, accurate long-horizon forecasts for non-stationary, multi-scale IoT data.

Abstract: Forecasting long-term time series in IoT environments remains a significant
challenge due to the non-stationary and multi-scale characteristics of sensor
signals. Furthermore, error accumulation causes a decrease in forecast quality
when predicting further into the future. Traditional methods are restricted to
operate in time-domain, while the global frequency information achieved by
Fourier transform would be regarded as stationary signals leading to blur the
temporal patterns of transient events. We propose AWEMixer, an Adaptive
Wavelet-Enhanced Mixer Network including two innovative components: 1) a
Frequency Router designs to utilize the global periodicity pattern achieved by
Fast Fourier Transform to adaptively weight localized wavelet subband, and 2) a
Coherent Gated Fusion Block to achieve selective integration of prominent
frequency features with multi-scale temporal representation through
cross-attention and gating mechanism, which realizes accurate time-frequency
localization while remaining robust to noise. Seven public benchmarks validate
that our model is more effective than recent state-of-the-art models.
Specifically, our model consistently achieves performance improvement compared
with transformer-based and MLP-based state-of-the-art models in long-sequence
time series forecasting. Code is available at
https://github.com/hit636/AWEMixer

</details>


### [106] [Temporal convolutional and fusional transformer model with Bi-LSTM encoder-decoder for multi-time-window remaining useful life prediction](https://arxiv.org/abs/2511.04723)
*Mohamadreza Akbari Pour,Mohamad Sadeq Karimi,Amir Hossein Mazloumi*

Main category: cs.LG

TL;DR: A novel hybrid model for RUL prediction that combines Temporal Convolutional Networks with a Bi-LSTM-augmented Temporal Fusion Transformer and a multi-time-window strategy, achieving up to 5.5% RMSE reduction over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Accurate Remaining Useful Life prediction requires capturing fine-grained temporal dependencies and dynamically prioritizing features across time across varying operating conditions in industrial systems.

Method: The approach integrates Temporal Convolutional Networks (TCNs) for localized temporal feature extraction with a modified Temporal Fusion Transformer (TFT) enhanced by a Bi-LSTM encoder–decoder. A multi-time-window methodology improves adaptability across diverse operating conditions, bridging short- and long-term dependencies and emphasizing salient temporal patterns.

Result: Extensive evaluations on benchmark datasets show the proposed model reduces average RMSE by up to 5.5% compared to state-of-the-art methods, indicating improved predictive accuracy for RUL.

Conclusion: The framework advances industrial prognostics by addressing gaps in temporal feature modeling and highlights the potential of advanced time-series transformers for robust RUL prediction across varying conditions.

Abstract: Health prediction is crucial for ensuring reliability, minimizing downtime,
and optimizing maintenance in industrial systems. Remaining Useful Life (RUL)
prediction is a key component of this process; however, many existing models
struggle to capture fine-grained temporal dependencies while dynamically
prioritizing critical features across time for robust prognostics. To address
these challenges, we propose a novel framework that integrates Temporal
Convolutional Networks (TCNs) for localized temporal feature extraction with a
modified Temporal Fusion Transformer (TFT) enhanced by Bi-LSTM encoder-decoder.
This architecture effectively bridges short- and long-term dependencies while
emphasizing salient temporal patterns. Furthermore, the incorporation of a
multi-time-window methodology improves adaptability across diverse operating
conditions. Extensive evaluations on benchmark datasets demonstrate that the
proposed model reduces the average RMSE by up to 5.5%, underscoring its
improved predictive accuracy compared to state-of-the-art methods. By closing
critical gaps in current approaches, this framework advances the effectiveness
of industrial prognostic systems and highlights the potential of advanced
time-series transformers for RUL prediction.

</details>


### [107] [Regularized GLISp for sensor-guided human-in-the-loop optimization](https://arxiv.org/abs/2511.04751)
*Matteo Cercola,Michele Lomuscio,Dario Piga,Simone Formentin*

Main category: cs.LG

TL;DR: Sensor-guided regularized extension of GLISp that adds physics-informed descriptors and regularization to preference-based optimization, creating a grey-box approach that improves convergence and final performance over standard GLISp.


<details>
  <summary>Details</summary>
Motivation: Standard preference-based optimization methods (e.g., Preferential Bayesian Optimization, GLISp) treat the system as a black box and miss informative sensor measurements. Incorporating quantitative sensor descriptors can inject domain knowledge and guide the search more efficiently.

Method: Introduce a sensor-guided regularized extension of GLISp. Integrate measurable descriptors into the preference-learning loop via a physics-informed hypothesis function and a least-squares regularization term, creating a grey-box structure that combines subjective feedback with sensor data while keeping the flexibility of preference-based search.

Result: Numerical evaluations on an analytical benchmark and a human-in-the-loop vehicle suspension tuning task show faster convergence and superior final solutions compared to baseline GLISp.

Conclusion: Grey-box, sensor-informed preference learning with regularization improves both convergence speed and solution quality over standard black-box preference-based methods, while retaining flexibility.

Abstract: Human-in-the-loop calibration is often addressed via preference-based
optimization, where algorithms learn from pairwise comparisons rather than
explicit cost evaluations. While effective, methods such as Preferential
Bayesian Optimization or Global optimization based on active preference
learning with radial basis functions (GLISp) treat the system as a black box
and ignore informative sensor measurements. In this work, we introduce a
sensor-guided regularized extension of GLISp that integrates measurable
descriptors into the preference-learning loop through a physics-informed
hypothesis function and a least-squares regularization term. This injects
grey-box structure, combining subjective feedback with quantitative sensor
information while preserving the flexibility of preference-based search.
Numerical evaluations on an analytical benchmark and on a human-in-the-loop
vehicle suspension tuning task show faster convergence and superior final
solutions compared to baseline GLISp.

</details>


### [108] [When Data Falls Short: Grokking Below the Critical Threshold](https://arxiv.org/abs/2511.04760)
*Vaibhav Singh,Eugene Belilovsky,Rahaf Aljundi*

Main category: cs.LG

TL;DR: KD from a grokked model on one distribution can induce, accelerate, and stabilize grokking on a second distribution under limited data, and KD aids generalization in joint and continual training with distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Grokking represents delayed generalization after overfitting; in data-scarce regimes grokking may be unobservable, yet real-world systems must adapt to shifting distributions with limited data. Understanding how knowledge transfer (KD) can enable or speed up generalization under such constraints is practically important for deployed models.

Method: 1) Demonstrate KD from a grokked model on distribution p1 to induce/accelerate grokking on distribution p2 with data below the critical threshold. 2) Train on the joint distribution (p1, p2) and assess whether standard supervised training fails with insufficient data and whether KD from grokked models enables generalization. 3) Explore continual pretraining where a grokked model moves from p1 to p2 and evaluate whether KD mitigates catastrophic forgetting and improves performance with limited data (~10%).

Result: KD can induce and accelerate grokking on a new distribution under low-data conditions. Distilling from grokked models enables generalization when training on the joint distribution would fail due to data gaps. In continual pretraining, KD both speeds up generalization and reduces forgetting, yielding strong performance with only 10% data.

Conclusion: Knowledge Distillation is central to enabling generalization under low data and evolving distributions, offering practical strategies for adapting models via transfer learning when grokking is otherwise hard to observe or maintain.

Abstract: In this paper, we investigate the phenomenon of grokking, where models
exhibit delayed generalization following overfitting on training data. We focus
on data-scarce regimes where the number of training samples falls below the
critical threshold, making grokking unobservable, and on practical scenarios
involving distribution shift. We first show that Knowledge Distillation (KD)
from a model that has already grokked on a distribution (p1) can induce and
accelerate grokking on a different distribution (p2), even when the available
data lies below the critical threshold. This highlights the value of KD for
deployed models that must adapt to new distributions under limited data. We
then study training on the joint distribution (p1, p2) and demonstrate that
while standard supervised training fails when either distribution has
insufficient data, distilling from models grokked on the individual
distributions enables generalization. Finally, we examine a continual
pretraining setup, where a grokked model transitions from p1 to p2, and find
that KD both accelerates generalization and mitigates catastrophic forgetting,
achieving strong performance even with only 10% of the data. Together, our
results provide new insights into the mechanics of grokking under knowledge
transfer and underscore the central role of KD in enabling generalization in
low-data and evolving distribution settings.

</details>


### [109] [FuseFlow: A Fusion-Centric Compilation Framework for Sparse Deep Learning on Streaming Dataflow](https://arxiv.org/abs/2511.04768)
*Rubens Lacouture,Nathan Zhang,Ritvik Sharma,Marco Siracusa,Fredrik Kjolstad,Kunle Olukotun,Olivia Hsu*

Main category: cs.LG

TL;DR: FuseFlow is a PyTorch-to-fused sparse dataflow compiler for reconfigurable dataflow architectures (RDAs). It enables cross-expression fusion of sparse operations, along with optimizations like parallelization, dataflow ordering, and sparsity blocking, and uses a cycle-accurate simulator for design-space exploration. Full cross-expression fusion is not always optimal; a heuristic helps prune suboptimal configurations. It reports up to ~2.7x speedup over unfused baselines on GPT-3 with BigBird block-sparse attention.


<details>
  <summary>Details</summary>
Motivation: As sparse computation and specialized dataflow hardware rise in importance, there is a need for compilers that can fuse sparse operations across expressions and map them effectively to RDAs. The paper investigates how fusion granularity affects performance and provides tooling for exploration.

Method: Introduce FuseFlow, a compiler that translates PyTorch models into fused sparse dataflow graphs for RDAs, supporting cross-expression fusion and optimizations (parallelization, dataflow ordering, sparsity blocking). It targets a cycle-accurate dataflow simulator to analyze fusion strategies and performs design-space exploration across four real-world sparsity-enabled ML apps, along with a heuristic to prune suboptimal configurations.

Result: Demonstrates that full fusion across all computation is not universally optimal; fusion granularity should be chosen per model. Shows performance improvements, including about 2.7x speedup over an unfused baseline for GPT-3 with BigBird block-sparse attention.

Conclusion: Fusion granularity is model-dependent; FuseFlow enables systematic exploration and optimization of fused sparse dataflow mappings on RDAs, achieving substantial speedups and guiding fusion strategy decisions.

Abstract: As deep learning models scale, sparse computation and specialized dataflow
hardware have emerged as powerful solutions to address efficiency. We propose
FuseFlow, a compiler that converts sparse machine learning models written in
PyTorch to fused sparse dataflow graphs for reconfigurable dataflow
architectures (RDAs). FuseFlow is the first compiler to support general
cross-expression fusion of sparse operations. In addition to fusion across
kernels (expressions), FuseFlow also supports optimizations like
parallelization, dataflow ordering, and sparsity blocking. It targets a
cycle-accurate dataflow simulator for microarchitectural analysis of fusion
strategies. We use FuseFlow for design-space exploration across four real-world
machine learning applications with sparsity, showing that full fusion (entire
cross-expression fusion across all computation in an end-to-end model) is not
always optimal for sparse models-fusion granularity depends on the model
itself. FuseFlow also provides a heuristic to identify and prune suboptimal
configurations. Using Fuseflow, we achieve performance improvements, including
a ~2.7x speedup over an unfused baseline for GPT-3 with BigBird block-sparse
attention.

</details>


### [110] [SLOFetch: Compressed-Hierarchical Instruction Prefetching for Cloud Microservices](https://arxiv.org/abs/2511.04774)
*Liu Jiang,Zerui Bao,Shiqi Sheng,Di Zhu*

Main category: cs.LG

TL;DR: A scalable, ML-guided instruction prefetching extension to EIP that compresses metadata, uses hierarchical on-chip/off-chip storage, and adapts prefetch decisions to SLO-driven cloud workloads, preserving speedups with reduced on-chip state.


<details>
  <summary>Details</summary>
Motivation: Cloud-scale services with deep software stacks and microservice orchestration increase instruction footprints, frontend stalls, tail latency, and energy; there is a need for adaptive, low-overhead prefetching that aligns with SLOs and ML-era workloads.

Method: Extend Entangling Instruction Prefetcher (EIP) with a Compressed Entry (up to eight destinations around a base using 36 bits) exploiting spatial clustering; implement Hierarchical Metadata Storage to keep L1-resident, frequently queried entries on-chip while virtualizing bulk metadata to lower levels; add a lightweight Online ML Controller to score prefetch profitability using context features and a bandit-adjusted threshold.

Result: Data center applications maintain EIP-like speedups while reducing on-chip state; improved efficiency for networked services in ML-era workloads.

Conclusion: The proposed combination enables fast, adaptive prefetching with lower on-chip state and improved efficiency under SLO-driven, self-optimizing cloud environments, aligning prefetching with ML-era workloads.

Abstract: Large-scale networked services rely on deep soft-ware stacks and microservice
orchestration, which increase instruction footprints and create frontend stalls
that inflate tail latency and energy. We revisit instruction prefetching for
these cloud workloads and present a design that aligns with SLO driven and self
optimizing systems. Building on the Entangling Instruction Prefetcher (EIP), we
introduce a Compressed Entry that captures up to eight destinations around a
base using 36 bits by exploiting spatial clustering, and a Hierarchical
Metadata Storage scheme that keeps only L1 resident and frequently queried
entries on chip while virtualizing bulk metadata into lower levels. We further
add a lightweight Online ML Controller that scores prefetch profitability using
context features and a bandit adjusted threshold. On data center applications,
our approach preserves EIP like speedups with smaller on chip state and
improves efficiency for networked services in the ML era.

</details>


### [111] [Conditional Neural ODE for Longitudinal Parkinson's Disease Progression Forecasting](https://arxiv.org/abs/2511.04789)
*Xiaoda Wang,Yuji Zhao,Kaiqiao Han,Xiao Luo,Sanne van Rooij,Jennifer Stevens,Lifang He,Liang Zhan,Yizhou Sun,Wei Wang,Carl Yang*

Main category: cs.LG

TL;DR: CNODE uses conditional neural ODEs to model continuous PD brain morphometry trajectories, aligning patient-specific timelines to a shared progression path, and outperforms baselines on the PPMI dataset.


<details>
  <summary>Details</summary>
Motivation: PD exhibits heterogeneous, evolving brain-morphometry with irregular and sparse MRI data; traditional RNN/Transformer models struggle to handle irregular sampling and individual heterogeneity, limiting mechanistic understanding and forecasting.

Method: A continuous-time neural ODE framework that models morphological brain changes as a continuous process; jointly learns patient-specific initial time and progression speed to align individual trajectories into a shared progression trajectory; validated on the PPMI dataset.

Result: CNODE outperforms state-of-the-art baselines in forecasting longitudinal PD progression on the PPMI dataset.

Conclusion: CNODE enables more accurate, continuous-time forecasting of PD progression, addressing irregular sampling and heterogeneity, with potential for mechanistic insight and digital-twin applications.

Abstract: Parkinson's disease (PD) shows heterogeneous, evolving brain-morphometry
patterns. Modeling these longitudinal trajectories enables mechanistic insight,
treatment development, and individualized 'digital-twin' forecasting. However,
existing methods usually adopt recurrent neural networks and transformer
architectures, which rely on discrete, regularly sampled data while struggling
to handle irregular and sparse magnetic resonance imaging (MRI) in PD cohorts.
Moreover, these methods have difficulty capturing individual heterogeneity
including variations in disease onset, progression rate, and symptom severity,
which is a hallmark of PD. To address these challenges, we propose CNODE
(Conditional Neural ODE), a novel framework for continuous, individualized PD
progression forecasting. The core of CNODE is to model morphological brain
changes as continuous temporal processes using a neural ODE model. In addition,
we jointly learn patient-specific initial time and progress speed to align
individual trajectories into a shared progression trajectory. We validate CNODE
on the Parkinson's Progression Markers Initiative (PPMI) dataset. Experimental
results show that our method outperforms state-of-the-art baselines in
forecasting longitudinal PD progression.

</details>


### [112] [Causal Structure and Representation Learning with Biomedical Applications](https://arxiv.org/abs/2511.04790)
*Caroline Uhler,Jiaqi Zhang*

Main category: cs.LG

TL;DR: A framework integrating representation learning with causal inference to leverage multi-modal observational and perturbational data for causal discovery, learning causal representations, and designing perturbations in biomedical contexts.


<details>
  <summary>Details</summary>
Motivation: Prediction-focused representation learning often fails for causal tasks; there is a need for causal-aware representations and strategies to combine observational and perturbational data across modalities to uncover true causal structure.

Method: A statistical and computational framework that couples representation learning with causal inference to (i) perform causal discovery on observed causal variables using observational and perturbational data, (ii) learn causal variables from multi-modal views, and (iii) design optimal perturbations.

Result: Conceptual framework; outlines methodological pathways and potential applications without reporting empirical results in the abstract.

Conclusion: The proposed framework aims to integrate multi-modal data, representation learning, and causal inference to enable causal discovery and perturbation design in biomedical contexts.

Abstract: Massive data collection holds the promise of a better understanding of
complex phenomena and, ultimately, better decisions. Representation learning
has become a key driver of deep learning applications, as it allows learning
latent spaces that capture important properties of the data without requiring
any supervised annotations. Although representation learning has been hugely
successful in predictive tasks, it can fail miserably in causal tasks including
predicting the effect of a perturbation/intervention. This calls for a marriage
between representation learning and causal inference. An exciting opportunity
in this regard stems from the growing availability of multi-modal data
(observational and perturbational, imaging-based and sequencing-based, at the
single-cell level, tissue-level, and organism-level). We outline a statistical
and computational framework for causal structure and representation learning
motivated by fundamental biomedical questions: how to effectively use
observational and perturbational data to perform causal discovery on observed
causal variables; how to use multi-modal views of the system to learn causal
variables; and how to design optimal perturbations.

</details>


### [113] [DuetServe: Harmonizing Prefill and Decode for LLM Serving via Adaptive GPU Multiplexing](https://arxiv.org/abs/2511.04791)
*Lei Gao,Chaoyi Jiang,Hossein Entezari Zarch,Daniel Wong,Murali Annavaram*

Main category: cs.LG

TL;DR: DuetServe is a unified LLM serving framework that achieves disaggregation-like isolation inside a single GPU by adaptively partitioning SMs to separate prefill and decode when contention threatens latency, yielding up to 1.3x throughput with low latency.


<details>
  <summary>Details</summary>
Motivation: Existing approaches either run prefill and decode on shared GPUs (causing interference and degraded TBT) or fully disaggregate across GPUs (wasting resources). There is a need for high-throughput LLM serving that maintains strict latency SLOs without incurring cross-GPU costs.

Method: Operate in aggregated mode by default and dynamically activate SM-level spatial multiplexing when TBT degradation is predicted. Decouple prefill and decode only when needed via fine-grained adaptive SM partitioning that provides phase isolation under contention. Components include (1) an attention-aware roofline model to forecast iteration latency, (2) a partitioning optimizer to select the optimal SM split to maximize throughput under TBT constraints, and (3) an interruption-free execution engine that eliminates CPU-GPU synchronization overhead.

Result: Evaluations show DuetServe improves total throughput by up to 1.3x while maintaining low generation latency compared to state-of-the-art frameworks.

Conclusion: DuetServe demonstrates that adaptive, fine-grained GPU partitioning can achieve near-disaggregated isolation within a single GPU, balancing throughput and latency without the overhead of full cross-GPU disaggregation.

Abstract: Modern LLM serving systems must sustain high throughput while meeting strict
latency SLOs across two distinct inference phases: compute-intensive prefill
and memory-bound decode phases. Existing approaches either (1) aggregate both
phases on shared GPUs, leading to interference between prefill and decode
phases, which degrades time-between-tokens (TBT); or (2) disaggregate the two
phases across GPUs, improving latency but wasting resources through duplicated
models and KV cache transfers. We present DuetServe, a unified LLM serving
framework that achieves disaggregation-level isolation within a single GPU.
DuetServe operates in aggregated mode by default and dynamically activates
SM-level GPU spatial multiplexing when TBT degradation is predicted. Its key
idea is to decouple prefill and decode execution only when needed through
fine-grained, adaptive SM partitioning that provides phase isolation only when
contention threatens latency service level objectives (SLOs). DuetServe
integrates (1) an attention-aware roofline model to forecast iteration latency,
(2) a partitioning optimizer that selects the optimal SM split to maximize
throughput under TBT constraints, and (3) an interruption-free execution engine
that eliminates CPU-GPU synchronization overhead. Evaluations show that
DuetServe improves total throughput by up to 1.3x while maintaining low
generation latency compared to state-of-the-art frameworks.

</details>


### [114] [Simplex-FEM Networks (SiFEN): Learning A Triangulated Function Approximator](https://arxiv.org/abs/2511.04804)
*Chaymae Yahyati,Ismail Lamaakal,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: SiFEN is a learned finite-element style network that uses a learned simplicial mesh and piecewise Bernstein polynomials to achieve locality, smoothness control, and FEM-like approximation guarantees, with improved calibration and lower latency compared to MLPs and edge-spline nets.


<details>
  <summary>Details</summary>
Motivation: To combine finite-element theory with learning for locality, interpretability, and theoretical guarantees, addressing limitations of dense MLPs and existing geometric nets in tabular regression/classification and as CNN heads.

Method: Learned globally C^r finite-element field on a learned simplicial mesh in a warped input space. Each query activates one simplex and at most d+1 Bernstein-Bézier basis functions via barycentric coordinates. Uses a light invertible warp; trained end-to-end with shape regularization, semi-discrete OT coverage, and differentiable edge flips. Assumes shape-regularity and bi-Lipschitz warp, yielding FEM-like approximation rates.

Result: Under standard assumptions, SiFEN achieves the FEM approximation rate M^(-m/d) with M mesh vertices. Empirically, it matches or surpasses MLPs and KANs at equal parameter budgets on synthetic tasks, tabular regression/classification, and as a drop-in head for compact CNNs; it improves calibration (lower ECE/Brier) and reduces inference latency due to geometric locality.

Conclusion: SiFEN provides a compact, interpretable, and theoretically grounded alternative to dense MLPs and edge-spline networks, combining FEM theory with learnable meshes for strong performance, calibration, and efficiency.

Abstract: We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial
predictor that represents f: R^d -> R^k as a globally C^r finite-element field
on a learned simplicial mesh in an optionally warped input space. Each query
activates exactly one simplex and at most d+1 basis functions via barycentric
coordinates, yielding explicit locality, controllable smoothness, and
cache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with
a light invertible warp and trains end-to-end with shape regularization,
semi-discrete OT coverage, and differentiable edge flips. Under standard
shape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic
FEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic
approximation tasks, tabular regression/classification, and as a drop-in head
on compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter
budgets, improves calibration (lower ECE/Brier), and reduces inference latency
due to geometric locality. These properties make SiFEN a compact,
interpretable, and theoretically grounded alternative to dense MLPs and
edge-spline networks.

</details>


### [115] [PuzzleMoE: Efficient Compression of Large Mixture-of-Experts Models via Sparse Expert Merging and Bit-packed inference](https://arxiv.org/abs/2511.04805)
*Yushu Zhao,Zheng Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: PuzzleMoE is a training-free compression method for mixture-of-experts (MoE) models that merges experts by exploiting element-wise weight redundancy and specialization, using a dual-mask to capture shared and expert-specific parameters and a bit-packed encoding scheme to avoid storing masks/signs, yielding up to 50% compression with maintained accuracy and up to 1.28x speedup.


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts (MoE) offer scalable language modeling but incur large memory overhead due to many expert parameters. High compression is needed for deployment, yet existing dropping/merging methods often degrade performance at high compression ratios.

Method: 1) Sparse expert merging via dual-mask that identifies element-wise redundancy and specialization, capturing both shared and expert-specific parameters. 2) Bit-packed encoding scheme that reuses underutilized exponent bits to avoid storing binary masks and signs, enabling efficient GPU inference without extra mask storage.

Result: Experiments show up to 50% compression with maintained accuracy across various tasks. Outperforms prior MoE compression methods by up to 16.7% on MMLU at 50% compression and yields up to 1.28x inference speedup.

Conclusion: PuzzleMoE enables training-free compression of MoE models with strong accuracy retention and efficiency, facilitating practical deployment of large MoE models while reducing memory and compute overhead.

Abstract: Mixture-of-Experts (MoE) models have shown strong potential in scaling
language models efficiently by activating only a small subset of experts per
input. However, their widespread deployment remains limited due to the high
memory overhead associated with storing all expert parameters, particularly as
the number of experts increases. To address this challenge, prior works have
explored expert dropping and merging strategies, yet they often suffer from
performance drop at high compression ratios. In this paper, we introduce
PuzzleMoE, a training-free MoE compression method that achieves both high
accuracy and efficient inference through two key innovations: First, PuzzleMoE
performs sparse expert merging by identifying element-wise weight redundancy
and specialization. It uses a dual-mask to capture both shared and
expert-specific parameters. Second, to avoid the overhead of storing binary
masks and signs, PuzzleMoE introduces a bit-packed encoding scheme that reuses
underutilized exponent bits, enabling efficient MoE inference on GPUs.
Extensive experiments demonstrate that PuzzleMoE can compress MoE models by up
to 50% while maintaining accuracy across various tasks. Specifically, it
outperforms prior MoE compression methods by up to 16.7% on MMLU at 50%
compression ratio, and achieves up to 1.28\times inference speedup.

</details>


### [116] [Autoencoding Dynamics: Topological Limitations and Capabilities](https://arxiv.org/abs/2511.04807)
*Matthew D. Kvalheim,Eduardo D. Sontag*

Main category: cs.LG

TL;DR: A theoretical study of autoencoders for data on a manifold, detailing the topological limits and possibilities of reconstruction, and outlining when such autoencoders can preserve or encode the dynamics of systems with the manifold as an invariant.


<details>
  <summary>Details</summary>
Motivation: Understand fundamental topological constraints on representing manifold-valued data with latent spaces, and assess the feasibility of learning dynamical systems constrained to an invariant manifold via autoencoders.

Method: The authors analyze the existence and quality of encoders E and decoders D that make D∘E approximate the identity on the data manifold M, exploring topological limitations; they also characterize capabilities for autoencoding dynamical systems where M is invariant, potentially deriving conditions and constructions.

Result: Identification of intrinsic topological limitations on autoencoder expressivity for manifolds and demonstration of certain capabilities to encode dynamical systems with M invariant, including possible constructive or theoretical results linking manifold topology to achievable reconstruction/latents.

Conclusion: Autoencoders are constrained by the topology of the data manifold; however, under suitable conditions related to the manifold and dynamics, one can achieve faithful autoencoding and meaningful dynamic representations on the latent space.

Abstract: Given a "data manifold" $M\subset \mathbb{R}^n$ and "latent space"
$\mathbb{R}^\ell$, an autoencoder is a pair of continuous maps consisting of an
"encoder" $E\colon \mathbb{R}^n\to \mathbb{R}^\ell$ and "decoder" $D\colon
\mathbb{R}^\ell\to \mathbb{R}^n$ such that the "round trip" map $D\circ E$ is
as close as possible to the identity map $\mbox{id}_M$ on $M$. We present
various topological limitations and capabilites inherent to the search for an
autoencoder, and describe capabilities for autoencoding dynamical systems
having $M$ as an invariant manifold.

</details>


### [117] [Sharp Minima Can Generalize: A Loss Landscape Perspective On Data](https://arxiv.org/abs/2511.04808)
*Raymond Fan,Bryce Sandlund,Lin Myat Ko*

Main category: cs.LG

TL;DR: The abstract challenges the volume-based view of generalization by showing that dataset size reshapes the loss landscape: although sharp, generalizing minima exist, they are rare when data is scarce due to small volumes; as data increases, previously small generalizing minima inflate in volume and become more accessible.


<details>
  <summary>Details</summary>
Motivation: To explain generalization in deep learning beyond the volume hypothesis by examining how varying amounts of training data affect the geometry of minima and their generalization properties.

Method: Empirically measure minima volumes under different training data sizes and analyze how the loss landscape changes as data increases.

Result: Sharp minima that generalize can exist but are unlikely to be found when their volumes are small; increasing data alters the landscape so that those previously small generalizing minima become relatively large and more likely to be found.

Conclusion: Volume-based explanations are incomplete; dataset size actively reshapes the loss landscape, enabling generalizing minima to emerge with larger data, suggesting a more dynamic relationship between data, volume, and generalization.

Abstract: The volume hypothesis suggests deep learning is effective because it is
likely to find flat minima due to their large volumes, and flat minima
generalize well. This picture does not explain the role of large datasets in
generalization. Measuring minima volumes under varying amounts of training data
reveals sharp minima which generalize well exist, but are unlikely to be found
due to their small volumes. Increasing data changes the loss landscape, such
that previously small generalizing minima become (relatively) large.

</details>


### [118] [A Standardized Benchmark for Multilabel Antimicrobial Peptide Classification](https://arxiv.org/abs/2511.04814)
*Sebastian Ojeda,Rafael Velasquez,Nicolás Aparicio,Juanita Puentes,Paula Cárdenas,Nicolás Andrade,Gabriel González,Sergio Rincón,Carolina Muñoz-Camargo,Pablo Arbeláez*

Main category: cs.LG

TL;DR: ESCAPE is a large, standardized dataset for antimicrobial peptides with multilabel activity annotations, combined with a transformer-based classifier that achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Fragmented datasets, inconsistent annotations, and lack of standardized benchmarks hinder AI-driven antimicrobial peptide discovery; a coherent, reproducible framework is needed.

Method: Assembles over 80,000 peptides from 27 repositories into ESCAPE, clearly separating active peptides from negatives and encoding functional annotations in a multilabel hierarchy; develops a transformer-based model that uses sequence and structural features to predict multiple activities (antibacterial, antifungal, antiviral, antiparasitic).

Result: The model achieves up to 2.56% relative improvement in mean Average Precision over the second-best method and sets a new state-of-the-art for multilabel peptide classification; ESCAPE provides a comprehensive, reproducible evaluation framework.

Conclusion: ESCAPE offers a robust, scalable resource and methodology that can accelerate AI-driven antimicrobial peptide discovery by enabling consistent benchmarking and improved predictive performance.

Abstract: Antimicrobial peptides have emerged as promising molecules to combat
antimicrobial resistance. However, fragmented datasets, inconsistent
annotations, and the lack of standardized benchmarks hinder computational
approaches and slow down the discovery of new candidates. To address these
challenges, we present the Expanded Standardized Collection for Antimicrobial
Peptide Evaluation (ESCAPE), an experimental framework integrating over 80.000
peptides from 27 validated repositories. Our dataset separates antimicrobial
peptides from negative sequences and incorporates their functional annotations
into a biologically coherent multilabel hierarchy, capturing activities across
antibacterial, antifungal, antiviral, and antiparasitic classes. Building on
ESCAPE, we propose a transformer-based model that leverages sequence and
structural information to predict multiple functional activities of peptides.
Our method achieves up to a 2.56% relative average improvement in mean Average
Precision over the second-best method adapted for this task, establishing a new
state-of-the-art multilabel peptide classification. ESCAPE provides a
comprehensive and reproducible evaluation framework to advance AI-driven
antimicrobial peptide research.

</details>


### [119] [Persistent reachability homology in machine learning applications](https://arxiv.org/abs/2511.04825)
*Luigi Caputi,Nicholas Meadows,Henri Riihimäki*

Main category: cs.LG

TL;DR: PRH on directed graphs improves epilepsy-detection classification vs DPH, using Betti-curve features with an SVM.


<details>
  <summary>Details</summary>
Motivation: To improve network-based epilepsy detection by applying persistent reachability homology to digraphs and to assess its advantage over directed flag complex-based topology.

Method: Compute PRH on digraphs (which condenses graphs in the persistent filtration for smaller subgraphs) and compare with DPH. Extract Betti curves and their integrals as topological features and feed them into a support vector machine for classification.

Result: PRH outperforms DPH in the epilepsy-detection classification task.

Conclusion: PRH’s condensation-based reduction yields more discriminative topological features for this problem, making PRH a promising tool for network neuroscience classification tasks.

Abstract: We explore the recently introduced persistent reachability homology (PRH) of
digraph data, i.e. data in the form of directed graphs. In particular, we study
the effectiveness of PRH in network classification task in a key neuroscience
problem: epilepsy detection. PRH is a variation of the persistent homology of
digraphs, more traditionally based on the directed flag complex (DPH). A main
advantage of PRH is that it considers the condensations of the digraphs
appearing in the persistent filtration and thus is computed from smaller
digraphs. We compare the effectiveness of PRH to that of DPH and we show that
PRH outperforms DPH in the classification task. We use the Betti curves and
their integrals as topological features and implement our pipeline on support
vector machine.

</details>


### [120] [Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.04834)
*Jiwoo Shin,Byeonghu Na,Mina Kang,Wonhyeok Choi,Il-chul Moon*

Main category: cs.LG

TL;DR: A simple method replaces explicit negative prompts with implicit negative embeddings derived from concept inversion to boost defense performance against harmful prompts, addressing incompatibility between fine-tuning harmful concepts and training-free negative prompts without modifying existing pipelines.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models can produce harmful content; but defense strategies—fine-tuning to unlearn harm and training-free negative prompts—are often incompatible, reducing defense effectiveness when combined.

Method: Replace the negative prompts used in training-free methods with implicit negative embeddings obtained via concept inversion, integrating without modifying either approach.

Result: The proposed method yields consistent improvements in defense success rate on nudity and violence benchmarks while preserving the original input prompts' core semantics.

Conclusion: Implicit negative embeddings from concept inversion can reconcile the two defense paradigms, providing a simple, deployment-friendly improvement to defense performance without changing existing pipelines.

Abstract: Recent advances in text-to-image generative models have raised concerns about
their potential to produce harmful content when provided with malicious input
text prompts. To address this issue, two main approaches have emerged: (1)
fine-tuning the model to unlearn harmful concepts and (2) training-free
guidance methods that leverage negative prompts. However, we observe that
combining these two orthogonal approaches often leads to marginal or even
degraded defense performance. This observation indicates a critical
incompatibility between two paradigms, which hinders their combined
effectiveness. In this work, we address this issue by proposing a conceptually
simple yet experimentally robust method: replacing the negative prompts used in
training-free methods with implicit negative embeddings obtained through
concept inversion. Our method requires no modification to either approach and
can be easily integrated into existing pipelines. We experimentally validate
its effectiveness on nudity and violence benchmarks, demonstrating consistent
improvements in defense success rate while preserving the core semantics of
input prompts.

</details>


### [121] [SPECTRA: Spectral Target-Aware Graph Augmentation for Imbalanced Molecular Property Regression](https://arxiv.org/abs/2511.04838)
*Brenda Nogueira,Meng Jiang,Nitesh V. Chawla,Nuno Moniz*

Main category: cs.LG

TL;DR: Spectral, geometry-aware augmentation (SPECTRA) improves targeted-range molecular property regression by generating realistic, spectrally interpolated augmented graphs that align with molecular geometry, boosting rare-case performance while maintaining overall MAE.


<details>
  <summary>Details</summary>
Motivation: In molecular property prediction, valuable compounds often lie in sparse regions of the target space. Standard GNNs optimize for average error and underperform on these rare but critical cases; existing oversampling methods can distort molecular topology.

Method: SPECTRA reconstructs multi-attribute molecular graphs from SMILES; aligns molecule pairs using (Fused) Gromov-Wasserstein couplings to obtain node correspondences; interpolates Laplacian eigenvalues, eigenvectors and node features in a shared basis; reconstructs edges to synthesize physically plausible intermediates with interpolated targets; applies a rarity-aware budget via kernel density estimation to focus augmentation where data are scarce; uses a spectral GNN with edge-aware Chebyshev convolutions.

Result: On benchmarks, SPECTRA consistently improves error in relevant target ranges while maintaining competitive overall MAE, and yields interpretable synthetic molecules whose structures reflect the underlying spectral geometry.

Conclusion: Spectral, geometry-aware augmentation is an effective and efficient approach for addressing imbalanced molecular property regression without sacrificing global accuracy.

Abstract: In molecular property prediction, the most valuable compounds (e.g., high
potency) often occupy sparse regions of the target space. Standard Graph Neural
Networks (GNNs) commonly optimize for the average error, underperforming on
these uncommon but critical cases, with existing oversampling methods often
distorting molecular topology. In this paper, we introduce SPECTRA, a Spectral
Target-Aware graph augmentation framework that generates realistic molecular
graphs in the spectral domain. SPECTRA (i) reconstructs multi-attribute
molecular graphs from SMILES; (ii) aligns molecule pairs via (Fused)
Gromov-Wasserstein couplings to obtain node correspondences; (iii) interpolates
Laplacian eigenvalues, eigenvectors and node features in a stable share-basis;
and (iv) reconstructs edges to synthesize physically plausible intermediates
with interpolated targets. A rarity-aware budgeting scheme, derived from a
kernel density estimation of labels, concentrates augmentation where data are
scarce. Coupled with a spectral GNN using edge-aware Chebyshev convolutions,
SPECTRA densifies underrepresented regions without degrading global accuracy.
On benchmarks, SPECTRA consistently improves error in relevant target ranges
while maintaining competitive overall MAE, and yields interpretable synthetic
molecules whose structure reflects the underlying spectral geometry. Our
results demonstrate that spectral, geometry-aware augmentation is an effective
and efficient strategy for imbalanced molecular property regression.

</details>


### [122] [Sublinear iterations can suffice even for DDPMs](https://arxiv.org/abs/2511.04844)
*Matthew S. Zhang,Stephen Huan,Jerry Huang,Nicholas M. Boffi,Sitan Chen,Sinho Chewi*

Main category: cs.LG

TL;DR: Introduces the denoising diffusion randomized midpoint method (DDRaM) for DDPMs, achieving sublinear sampling complexity with respect to dimension, via a randomized midpoint integrator and the shifted composition rule. Provides theoretical guarantees (O˜(√d) score evaluations) and empirical validation on pre-trained image models, contrasting with prior Euler/ODE-based samplers.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of previous DDPM discretizations (notably exponential Euler) whose guarantees scale at least linearly with dimension or Fisher information, and to obtain sublinear sampling complexity. Motivated by log-concave sampling insights (Shen & Lee, 2019) and the shifted composition rule framework.

Method: Proposes DDRaM, an integrator that uses a randomized midpoint within the diffusion process to approximate the SDE. Analyzed under smoothness assumptions using the shifted composition rule, yielding favorable discretization properties and sublinear complexity. Provides experimental validation on pre-trained image synthesis models.

Result: Proves a sublinear bound of ~O(√d) score evaluations needed to ensure convergence for pure DDPM sampling. This is the first such sublinear complexity result for DDPM sampling (previous bounds relied on ODE-based sampling or required sampler modifications). Experimental results corroborate practical performance gains.

Conclusion: DDRaM achieves favorable discretization properties with sublinear sampling complexity under reasonable smoothness assumptions, representing a first sublinear bound for pure DDPM sampling and showing practical effectiveness for image generation with pre-trained models.

Abstract: SDE-based methods such as denoising diffusion probabilistic models (DDPMs)
have shown remarkable success in real-world sample generation tasks. Prior
analyses of DDPMs have been focused on the exponential Euler discretization,
showing guarantees that generally depend at least linearly on the dimension or
initial Fisher information. Inspired by works in log-concave sampling (Shen and
Lee, 2019), we analyze an integrator -- the denoising diffusion randomized
midpoint method (DDRaM) -- that leverages an additional randomized midpoint to
better approximate the SDE. Using a recently-developed analytic framework
called the "shifted composition rule", we show that this algorithm enjoys
favorable discretization properties under appropriate smoothness assumptions,
with sublinear $\widetilde{O}(\sqrt{d})$ score evaluations needed to ensure
convergence. This is the first sublinear complexity bound for pure DDPM
sampling -- prior works which obtained such bounds worked instead with
ODE-based sampling and had to make modifications to the sampler which deviate
from how they are used in practice. We also provide experimental validation of
the advantages of our method, showing that it performs well in practice with
pre-trained image synthesis models.

</details>


### [123] [Investigating U.S. Consumer Demand for Food Products with Innovative Transportation Certificates Based on Stated Preferences and Machine Learning Approaches](https://arxiv.org/abs/2511.04845)
*Jingchen Bi,Rodrigo Mesa-Arango*

Main category: cs.LG

TL;DR: ML-based analysis of consumer demand for innovative transportation certificates in U.S. food supply; finds safety and energy certificates most valued; price/product/decision-maker factors affect choices; offers data-driven supply chain recommendations.


<details>
  <summary>Details</summary>
Motivation: To understand how transportation-related certifications influence consumer demand for food products and identify which transport attributes drive preferences, extending prior work on supply chain traceability and stated-preference methods.

Method: Two experiments using a machine learning model. First identifies the significance of transportation factors in consumer choices for food with traceability. Second isolates specific transportation attributes (five certificates: Transportation Mode, IoT, Safety measures, Energy Source, Must Arrive By Dates) and controls for product-specific and decision-maker factors. Includes price, product type, certificates, and decision-maker factors as variables.

Result: Consumers show a notable preference for safety and energy certificates in transportation. The model reveals the impact of price and product type, as well as certificates and decision-maker factors, on purchasing choices. The study yields data-driven recommendations to improve food supply chain systems.

Conclusion: Findings support incorporating safety and energy-related transportation certifications into policy and business strategies to enhance consumer welfare and supply chain efficiency; demonstrates how ML can quantify attribute-level demand for traceability certificates.

Abstract: This paper utilizes a machine learning model to estimate the consumer's
behavior for food products with innovative transportation certificates in the
U.S. Building on previous research that examined demand for food products with
supply chain traceability using stated preference analysis, transportation
factors were identified as significant in consumer food purchasing choices.
Consequently, a second experiment was conducted to pinpoint the specific
transportation attributes valued by consumers. A machine learning model was
applied, and five innovative certificates related to transportation were
proposed: Transportation Mode, Internet of Things (IoT), Safety measures,
Energy Source, and Must Arrive By Dates (MABDs). The preference experiment also
incorporated product-specific and decision-maker factors for control purposes.
The findings reveal a notable inclination toward safety and energy certificates
within the transportation domain of the U.S. food supply chain. Additionally,
the study examined the influence of price, product type, certificates, and
decision-maker factors on purchasing choices. Ultimately, the study offers
data-driven recommendations for improving food supply chain systems.

</details>


### [124] [Grounded Test-Time Adaptation for LLM Agents](https://arxiv.org/abs/2511.04847)
*Arthur Chen,Zuxin Liu,Jianguo Zhang,Akshara Prabhakar,Zhiwei Liu,Shelby Heinecke,Silvio Savarese,Victor Zhong,Caiming Xiong*

Main category: cs.LG

TL;DR: Two deployment-time adaptation strategies for LLM agents: online distributional adaptation to align outputs with environment formats, and dynamics grounding via persona-driven exploration to learn environment dynamics and a nonparametric world model; demonstrated on function calling and web navigation with notable gains (e.g., WebArena from 2% to 23%).


<details>
  <summary>Details</summary>
Motivation: LLM agents fail to generalize to novel environments due to mismatches between pretraining and test-time conditions, including syntactic misunderstandings of environment data formats and semantic misunderstandings of state dynamics, which only reveal at test time.

Method: 1) online distributional adaptation: learn a lightweight adaptation vector that biases the model's output distribution to match environment response formats; 2) deployment-time dynamics grounding: a persona-driven exploration phase to probe the environment, learn causal dynamics, and construct a nonparametric world model for planning.

Result: Both strategies improve performance across diverse benchmarks (function calling, web navigation) with minimal computational cost; dynamics grounding is especially effective in complex, unpredictable dynamics settings; WebArena multi-site split shows success rate rising from 2% to 23%.

Conclusion: The two strategies are complementary and enable more generalizable and capable LLM-based agents; dynamics grounding provides a robust path to handle unpredictable dynamics and enhances generalization to new environments with modest overhead.

Abstract: Large language model (LLM)-based agents struggle to generalize to novel and
complex environments, such as unseen websites or new sets of functions, due to
a fundamental mismatch between their pre-training and test-time conditions.
This challenge stems from two distinct failure modes: a syntactic
misunderstanding of environment-specific components like observation formats,
and a semantic misunderstanding of state-transition dynamics, which are only
revealed at test time. To address these issues, we propose two distinct and
complementary strategies for adapting LLM agents by leveraging
environment-specific information available during deployment. First, an online
distributional adaptation method parameterizes environmental nuances by
learning a lightweight adaptation vector that biases the model's output
distribution, enabling rapid alignment with an environment response format.
Second, a deployment-time dynamics grounding method employs a persona-driven
exploration phase to systematically probe and learn the environment's causal
dynamics before task execution, equipping the agent with a nonparametric world
model. We evaluate these strategies across diverse agentic benchmarks,
including function calling and web navigation. Our empirical results show the
effectiveness of both strategies across all benchmarks with minimal
computational cost. We find that dynamics grounding is particularly effective
in complex environments where unpredictable dynamics pose a major obstacle,
demonstrating a robust path toward more generalizable and capable LLM-based
agents. For example, on the WebArena multi-site split, this method increases
the agent's success rate from 2% to 23%.

</details>


### [125] [SigmaDock: Untwisting Molecular Docking With Fragment-Based SE(3) Diffusion](https://arxiv.org/abs/2511.04854)
*Alvaro Prat,Leo Zhang,Charlotte M. Deane,Yee Whye Teh,Garrett M. Morris*

Main category: cs.LG

TL;DR: A fragment-based SE(3) diffusion model (SigmaDock) for ligand docking achieves state-of-the-art pose generation, generalizes to new proteins, and even surpasses classical physics-based docking on PB train-test.


<details>
  <summary>Details</summary>
Motivation: Address weaknesses of generative docking methods (chemically implausible outputs, limited generalizability, high computational cost) by introducing chemically informed fragmentation and geometric priors.

Method: Fragment ligands into rigid-body pieces and apply SigmaDock, an SE(3) Riemannian diffusion model, to reassemble these fragments inside the binding pocket, leveraging fragment-level geometry to simplify diffusion and improve stability.

Result: Achieves Top-1 success rate (RMSD<2 & PB-valid) >79.9% on PoseBusters, substantially higher than recent DL methods (12.7–30.8%); demonstrates generalization to unseen proteins; first DL approach to surpass physics-based docking under PB train-test split.

Conclusion: Fragment-based SE(3) diffusion for docking markedly improves reliability and generalizability of deep learning docking, representing a significant advance over both prior DL methods and classical physics-based docking.

Abstract: Determining the binding pose of a ligand to a protein, known as molecular
docking, is a fundamental task in drug discovery. Generative approaches promise
faster, improved, and more diverse pose sampling than physics-based methods,
but are often hindered by chemically implausible outputs, poor
generalisability, and high computational cost. To address these challenges, we
introduce a novel fragmentation scheme, leveraging inductive biases from
structural chemistry, to decompose ligands into rigid-body fragments. Building
on this decomposition, we present SigmaDock, an SE(3) Riemannian diffusion
model that generates poses by learning to reassemble these rigid bodies within
the binding pocket. By operating at the level of fragments in SE(3), SigmaDock
exploits well-established geometric priors while avoiding overly complex
diffusion processes and unstable training dynamics. Experimentally, we show
SigmaDock achieves state-of-the-art performance, reaching Top-1 success rates
(RMSD<2 & PB-valid) above 79.9% on the PoseBusters set, compared to 12.7-30.8%
reported by recent deep learning approaches, whilst demonstrating consistent
generalisation to unseen proteins. SigmaDock is the first deep learning
approach to surpass classical physics-based docking under the PB train-test
split, marking a significant leap forward in the reliability and feasibility of
deep learning for molecular modelling.

</details>


### [126] [Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning](https://arxiv.org/abs/2511.04856)
*Thore Gerlach,Michael Schenk,Verena Kain*

Main category: cs.LG

TL;DR: Introduces Continuous Semi-Quantum Boltzmann Machines (CSQBMs) for continuous-action RL; a hybrid quantum-classical model enabling analytic gradients and sampling-based Q-learning to improve stability.


<details>
  <summary>Details</summary>
Motivation: Aim to reduce qubit requirements while preserving expressiveness in reinforcement learning, and to enable analytic gradient computation for seamless integration into Actor-Critic methods; address instability in continuous control.

Method: Form CSQBMs by combining exponential-family priors over visible units with quantum Boltzmann distributions over hidden units, creating a hybrid model. Derive analytic gradients w.r.t. continuous variables. Propose a continuous Q-learning framework that uses sampling from the CSQBM distribution instead of global maximization and integrate into Actor-Critic architectures.

Result: Establishes a theoretically grounded hybrid model with reduced quantum resource needs, tractable gradient computations for continuous actions, and a sampling-based Q-learning approach that can stabilize training in continuous control tasks.

Conclusion: CSQBMs offer a promising quantum-inspired pathway for efficient and stable continuous-action RL, combining strong expressiveness with analytic tractability and sampling-based optimization.

Abstract: We introduce theoretically grounded Continuous Semi-Quantum Boltzmann
Machines (CSQBMs) that supports continuous-action reinforcement learning. By
combining exponential-family priors over visible units with quantum Boltzmann
distributions over hidden units, CSQBMs yield a hybrid quantum-classical model
that reduces qubit requirements while retaining strong expressiveness.
Crucially, gradients with respect to continuous variables can be computed
analytically, enabling direct integration into Actor-Critic algorithms.
Building on this, we propose a continuous Q-learning framework that replaces
global maximization by efficient sampling from the CSQBM distribution, thereby
overcoming instability issues in continuous control.

</details>


### [127] [You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL in Weak Base Models](https://arxiv.org/abs/2511.04902)
*Shuvendu Roy,Hossein Hajimirsadeghi,Mengyao Zhai,Golnoosh Samei*

Main category: cs.LG

TL;DR: Label-free RL effectiveness hinges on base model reasoning; curriculum learning and data curation can robustly boost performance across 0.5B–7B models.


<details>
  <summary>Details</summary>
Motivation: Investigate generalizability of label-free RL to smaller models; identify limitations when base model has limited reasoning ability; understand role of data difficulty in success.

Method: Systematically evaluate label-free RL across model sizes (0.5B–7B). Propose curriculum learning to progressively introduce harder problems and mask non-majority rollouts. Introduce a data curation pipeline to generate samples with predefined difficulty.

Result: Observe consistent improvements across all model sizes and reasoning capabilities; reveal that weaker models struggle to generate sufficient chain-of-thought; highlight the importance of data difficulty; demonstrate robustness gains with curriculum and data curation.

Conclusion: Provides a path toward more robust unsupervised RL that can bootstrap reasoning abilities in resource-constrained models; code is released for reproducibility.

Abstract: Recent advances in large language models have demonstrated the promise of
unsupervised reinforcement learning (RL) methods for enhancing reasoning
capabilities without external supervision. However, the generalizability of
these label-free RL approaches to smaller base models with limited reasoning
capabilities remains unexplored. In this work, we systematically investigate
the performance of label-free RL methods across different model sizes and
reasoning strengths, from 0.5B to 7B parameters. Our empirical analysis reveals
critical limitations: label-free RL is highly dependent on the base model's
pre-existing reasoning capability, with performance often degrading below
baseline levels for weaker models. We find that smaller models fail to generate
sufficiently long or diverse chain-of-thought reasoning to enable effective
self-reflection, and that training data difficulty plays a crucial role in
determining success. To address these challenges, we propose a simple yet
effective method for label-free RL that utilizes curriculum learning to
progressively introduce harder problems during training and mask no-majority
rollouts during training. Additionally, we introduce a data curation pipeline
to generate samples with predefined difficulty. Our approach demonstrates
consistent improvements across all model sizes and reasoning capabilities,
providing a path toward more robust unsupervised RL that can bootstrap
reasoning abilities in resource-constrained models. We make our code available
at https://github.com/BorealisAI/CuMa

</details>


### [128] [FoodRL: A Reinforcement Learning Ensembling Framework For In-Kind Food Donation Forecasting](https://arxiv.org/abs/2511.04865)
*Esha Sharma,Lauren Davis,Julie Ivy,Min Chi*

Main category: cs.LG

TL;DR: FoodRL is a reinforcement learning–based metalearning framework that ensembles diverse forecasting models to predict volatile in-kind donations for food banks, achieving more accurate forecasts and potential social impact (up to 1.7M additional meals annually) especially during disruptions.


<details>
  <summary>Details</summary>
Motivation: Forecasting highly volatile in-kind donations is challenged by concept drift from seasonal variation and disasters; traditional models struggle to adapt. There is a need for adaptive, context-aware ensemble methods to support equitable resource distribution in humanitarian supply chains.

Method: Reinforcement-learning-based metalearning framework that clusters and dynamically weights forecasting models based on recent performance and contextual information; evaluated on two structurally distinct U.S. food banks (West Coast wildfire-impacted and East Coast hurricane-impacted).

Result: FoodRL consistently outperforms baseline methods, particularly during disruption periods; improved forecast reliability and potential redistribution of sufficient food to meet about 1.7 million additional meals annually.

Conclusion: Adaptive ensemble learning via RL-based metalearning can enhance forecasting under volatility and concept drift in humanitarian logistics, with notable social impact; broader validation across more sites would strengthen its generalizability.

Abstract: Food banks are crucial for alleviating food insecurity, but their
effectiveness hinges on accurately forecasting highly volatile in-kind
donations to ensure equitable and efficient resource distribution. Traditional
forecasting models often fail to maintain consistent accuracy due to
unpredictable fluctuations and concept drift driven by seasonal variations and
natural disasters such as hurricanes in the Southeastern U.S. and wildfires in
the West Coast. To address these challenges, we propose FoodRL, a novel
reinforcement learning (RL) based metalearning framework that clusters and
dynamically weights diverse forecasting models based on recent performance and
contextual information. Evaluated on multi-year data from two structurally
distinct U.S. food banks-one large regional West Coast food bank affected by
wildfires and another state-level East Coast food bank consistently impacted by
hurricanes, FoodRL consistently outperforms baseline methods, particularly
during periods of disruption or decline. By delivering more reliable and
adaptive forecasts, FoodRL can facilitate the redistribution of food equivalent
to 1.7 million additional meals annually, demonstrating its significant
potential for social impact as well as adaptive ensemble learning for
humanitarian supply chains.

</details>


### [129] [A Dual Perspective on Decision-Focused Learning: Scalable Training via Dual-Guided Surrogates](https://arxiv.org/abs/2511.04909)
*Paula Rodriguez-Diaz,Kirk Bansak Elisabeth Paulson*

Main category: cs.LG

TL;DR: Dual-Guided Learning (DGL) is a scalable, decision-aligned learning objective for predict-then-optimize that minimizes reliance on expensive solvers by periodically refreshing dual variables and training with dual-adjusted targets. It matches or surpasses state-of-the-art differentiable learning (DFL) on combinatorial problems while drastically reducing solver calls and training time.


<details>
  <summary>Details</summary>
Motivation: In decision-making under uncertainty, predictions feed optimization; standard training ignores the downstream optimizer, leading to suboptimal decisions. Differentiating through solvers or using surrogates is costly. DGL leverages dual information to ensure alignment between predictions and optimal decisions with fewer solver invocations.

Method: For combinatorial one-of-many constraints (matching, knapsack, shortest path), DGL periodically solves the downstream problem to obtain dual variables, then trains between refreshes using differentiable surrogates augmented with dual-adjusted targets. As refresh frequency decreases, training approaches standard supervised learning while preserving decision alignment.

Result: Proves asymptotically diminishing decision regret; analyzes runtime; empirical results on two problem classes show DGL matches or exceeds state-of-the-art DFL with far fewer solver calls and substantially less training time.

Conclusion: DGL offers a scalable, decision-aligned alternative to full differentiable optimization, enabling practical deployment for large-scale decision problems with limited solver usage; code available.

Abstract: Many real-world decisions are made under uncertainty by solving optimization
problems using predicted quantities. This predict-then-optimize paradigm has
motivated decision-focused learning, which trains models with awareness of how
the optimizer uses predictions, improving the performance of downstream
decisions. Despite its promise, scaling is challenging: state-of-the-art
methods either differentiate through a solver or rely on task-specific
surrogates, both of which require frequent and expensive calls to an optimizer,
often a combinatorial one. In this paper, we leverage dual variables from the
downstream problem to shape learning and introduce Dual-Guided Loss (DGL), a
simple, scalable objective that preserves decision alignment while reducing
solver dependence. We construct DGL specifically for combinatorial selection
problems with natural one-of-many constraints, such as matching, knapsack, and
shortest path. Our approach (a) decouples optimization from gradient updates by
solving the downstream problem only periodically; (b) between refreshes, trains
on dual-adjusted targets using simple differentiable surrogate losses; and (c)
as refreshes become less frequent, drives training cost toward standard
supervised learning while retaining strong decision alignment. We prove that
DGL has asymptotically diminishing decision regret, analyze runtime complexity,
and show on two problem classes that DGL matches or exceeds state-of-the-art
DFL methods while using far fewer solver calls and substantially less training
time. Code is available at https://github.com/paularodr/Dual-Guided-Learning.

</details>


### [130] [Self-Interest and Systemic Benefits: Emergence of Collective Rationality in Mixed Autonomy Traffic Through Deep Reinforcement Learning](https://arxiv.org/abs/2511.04883)
*Di Chen,Jia Li,Michael Zhang*

Main category: cs.LG

TL;DR: Self-interested DRL-driven autonomous vehicles in mixed traffic can naturally develop collective rationality (CR), enabling cooperation and potential system-wide benefits without explicit optimization, across diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: To determine whether CR can emerge among DRL-trained driving agents in mixed-autonomy traffic, addressing whether selfish incentives undermine system-wide gains and exploring mechanisms for cooperative behavior.

Method: Train autonomous-vehicle agents with a simple reward design using deep reinforcement learning, simulate mixed traffic with AVs and HVs across multiple scenarios, evaluate emergence and robustness of CR, propose a mechanism explaining CR emergence, verify via simulation, and discuss implications for federated learning as a route to promote cooperation.

Result: CR emerges consistently across scenarios among self-interested DRL agents, indicating robust cooperative behavior without explicit system-level objectives; a mechanism is proposed and simulation-supported; Federated learning is suggested as a potential approach to facilitate such cooperation.

Conclusion: CR among self-interested driving agents trained with DRL is attainable, implying that system-wide benefits can arise without hard-coding global objectives, and offering directions for future research and applications in mixed autonomy with learning-based agents.

Abstract: Autonomous vehicles (AVs) are expected to be commercially available in the
near future, leading to mixed autonomy traffic consisting of both AVs and
human-driven vehicles (HVs). Although numerous studies have shown that AVs can
be deployed to benefit the overall traffic system performance by incorporating
system-level goals into their decision making, it is not clear whether the
benefits still exist when agents act out of self-interest -- a trait common to
all driving agents, both human and autonomous. This study aims to understand
whether self-interested AVs can bring benefits to all driving agents in mixed
autonomy traffic systems. The research is centered on the concept of collective
rationality (CR). This concept, originating from game theory and behavioral
economics, means that driving agents may cooperate collectively even when
pursuing individual interests. Our recent research has proven the existence of
CR in an analytical game-theoretical model and empirically in mixed
human-driven traffic. In this paper, we demonstrate that CR can be attained
among driving agents trained using deep reinforcement learning (DRL) with a
simple reward design. We examine the extent to which self-interested traffic
agents can achieve CR without directly incorporating system-level objectives.
Results show that CR consistently emerges in various scenarios, which indicates
the robustness of this property. We also postulate a mechanism to explain the
emergence of CR in the microscopic and dynamic environment and verify it based
on simulation evidence. This research suggests the possibility of leveraging
advanced learning methods (such as federated learning) to achieve collective
cooperation among self-interested driving agents in mixed-autonomy systems.

</details>


### [131] [Multi-agent Coordination via Flow Matching](https://arxiv.org/abs/2511.05005)
*Dongsu Lee,Daehee Lee,Amy Zhang*

Main category: cs.LG

TL;DR: MAC-Flow learns a flow-based joint-behavior model offline and distills it into decentralized one-step policies, achieving strong coordination with a large speedup.


<details>
  <summary>Details</summary>
Motivation: There is a trade-off in multi-agent coordination between rich offline representations of joint behaviors and fast real-time execution. Diffusion-based methods offer expressive coordination but are computationally slow, while Gaussian-policy methods are fast but brittle with multi-agent interactions.

Method: A two-stage approach: (1) learn a flow-based representation of joint behaviors from offline data; (2) distill this into decentralized one-step policies that preserve coordination for fast execution.

Result: MAC-Flow achieves about 14.5x faster inference than diffusion-based MARL methods while maintaining good performance; its speed is comparable to prior Gaussian-policy offline MARL methods across extensive benchmarks (12 environments, 34 datasets).

Conclusion: MAC-Flow successfully balances rich coordination representation with real-time efficiency, offering a practical solution that combines the strengths of diffusion-based and policy-based offline MARL.

Abstract: This work presents MAC-Flow, a simple yet expressive framework for
multi-agent coordination. We argue that requirements of effective coordination
are twofold: (i) a rich representation of the diverse joint behaviors present
in offline data and (ii) the ability to act efficiently in real time. However,
prior approaches often sacrifice one for the other, i.e., denoising
diffusion-based solutions capture complex coordination but are computationally
slow, while Gaussian policy-based solutions are fast but brittle in handling
multi-agent interaction. MAC-Flow addresses this trade-off by first learning a
flow-based representation of joint behaviors, and then distilling it into
decentralized one-step policies that preserve coordination while enabling fast
execution. Across four different benchmarks, including $12$ environments and
$34$ datasets, MAC-Flow alleviates the trade-off between performance and
computational cost, specifically achieving about $\boldsymbol{\times14.5}$
faster inference compared to diffusion-based MARL methods, while maintaining
good performance. At the same time, its inference speed is similar to that of
prior Gaussian policy-based offline multi-agent reinforcement learning (MARL)
methods.

</details>


### [132] [Multi-Agent Craftax: Benchmarking Open-Ended Multi-Agent Reinforcement Learning at the Hyperscale](https://arxiv.org/abs/2511.04904)
*Bassel Al Omari,Michael Matthews,Alexander Rutherford,Jakob Nicolaus Foerster*

Main category: cs.LG

TL;DR: Presents Craftax-MA and Craftax-Coop, scalable MARL benchmarks extending Craftax; fast in JAX; reveal long-horizon credit assignment, exploration, and cooperation challenges; aims to drive long-term MARL research.


<details>
  <summary>Details</summary>
Motivation: Need challenging, generalizable benchmarks for MARL that test long-term dependencies and multi-agent cooperation beyond short-horizon tasks.

Method: Extend Craftax with multiple agents (Craftax-MA) in JAX; add heterogeneous agents, trading, and cooperative mechanics in Craftax-Coop to require coordination; evaluate with metrics on long-horizon credit assignment, exploration, and cooperation.

Result: Existing MARL algorithms struggle on these challenges; benchmark runs are fast (250M interactions under an hour); empirical justification that benchmarks stress key MARL difficulties.

Conclusion: Craftax-MA and Craftax-Coop offer a compelling, fast, general benchmark suite to push MARL research toward long-horizon, cooperative, and generalizable capabilities.

Abstract: Progress in multi-agent reinforcement learning (MARL) requires challenging
benchmarks that assess the limits of current methods. However, existing
benchmarks often target narrow short-horizon challenges that do not adequately
stress the long-term dependencies and generalization capabilities inherent in
many multi-agent systems. To address this, we first present
\textit{Craftax-MA}: an extension of the popular open-ended RL environment,
Craftax, that supports multiple agents and evaluates a wide range of general
abilities within a single environment. Written in JAX, \textit{Craftax-MA} is
exceptionally fast with a training run using 250 million environment
interactions completing in under an hour. To provide a more compelling
challenge for MARL, we also present \textit{Craftax-Coop}, an extension
introducing heterogeneous agents, trading and more mechanics that require
complex cooperation among agents for success. We provide analysis demonstrating
that existing algorithms struggle with key challenges in this benchmark,
including long-horizon credit assignment, exploration and cooperation, and
argue for its potential to drive long-term research in MARL.

</details>


### [133] [SAD-Flower: Flow Matching for Safe, Admissible, and Dynamically Consistent Planning](https://arxiv.org/abs/2511.05355)
*Tzu-Yuan Huang,Armin Lederer,Dai-Jie Wu,Xiaobing Dai,Sihua Zhang,Stefan Sosnowski,Shao-Hua Sun,Sandra Hirche*

Main category: cs.LG

TL;DR: SAD-Flower augments flow-based planning with a virtual control input to guarantee safety and dynamical feasibility at test time, providing state/action constraint satisfaction without retraining and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Flow matching (FM) lacks formal guarantees for state/action constraints and dynamic consistency, risking unsafe or infeasible trajectories in data-driven planning.

Method: Introduce a virtual control augmentation to the flow; apply nonlinear control theory to derive formal guarantees for state constraints, action constraints, and dynamic consistency; operate at test time without retraining.

Result: Empirical experiments show SAD-Flower outperforms generative-model baselines in constraint satisfaction across multiple tasks.

Conclusion: SAD-Flower provides a safe, admissible, and dynamically consistent trajectory generation framework, extending flow matching with formal guarantees and test-time adaptability for unseen constraints.

Abstract: Flow matching (FM) has shown promising results in data-driven planning.
However, it inherently lacks formal guarantees for ensuring state and action
constraints, whose satisfaction is a fundamental and crucial requirement for
the safety and admissibility of planned trajectories on various systems.
Moreover, existing FM planners do not ensure the dynamical consistency, which
potentially renders trajectories inexecutable. We address these shortcomings by
proposing SAD-Flower, a novel framework for generating Safe, Admissible, and
Dynamically consistent trajectories. Our approach relies on an augmentation of
the flow with a virtual control input. Thereby, principled guidance can be
derived using techniques from nonlinear control theory, providing formal
guarantees for state constraints, action constraints, and dynamic consistency.
Crucially, SAD-Flower operates without retraining, enabling test-time
satisfaction of unseen constraints. Through extensive experiments across
several tasks, we demonstrate that SAD-Flower outperforms various
generative-model-based baselines in ensuring constraint satisfaction.

</details>


### [134] [Efficient Swap Multicalibration of Elicitable Properties](https://arxiv.org/abs/2511.04907)
*Lunjia Hu,Haipeng Luo,Spandan Senapati,Vatsal Sharan*

Main category: cs.LG

TL;DR: Extends multicalibration to arbitrary bounded hypothesis classes via swap multicalibration and delivers an oracle-efficient algorithm achieving fast convergence rates in swap multicalibration, resolving open questions about efficient mean multicalibration.


<details>
  <summary>Details</summary>
Motivation: To generalize multicalibration from group membership functions to arbitrary bounded hypothesis classes, and to link elicitable properties with multicalibrated predictions, while achieving computationally efficient (oracle-efficient) guarantees with strong, quantitative error bounds.

Method: Introduce swap multicalibration (generalizing mean/elicitable multicalibration). Propose an oracle-efficient algorithm that uses an online agnostic learner and relies on bounded sequential Rademacher complexity. Achieves T^{1/(r+1)} ell_r-swap multicalibration error for r>=2; special case r=2 yields T^{1/3} ell_2-swap multicalibration error.

Result: The proposed approach yields provable high-probability bounds: swap multicalibration error scales as T^{1/(r+1)} for ell_r with r>=2; in the r=2 case, ell_2 error is O(T^{1/3}). This improves on previous bounds for mean multicalibration and related works (NR23, GMS25, LSS25a) and resolves an open question from GJRR24 about achieving sqrt(T) ell_2-mean multicalibration via an oracle-efficient method.

Conclusion: The paper broadens multicalibration theory to arbitrary bounded hypothesis classes through swap multicalibration and provides an oracle-efficient, provably fast-converging algorithm, achieving near-optimal rates and closing an important open problem in mean multicalibration efficiency.

Abstract: Multicalibration [HJKRR18] is an algorithmic fairness perspective that
demands that the predictions of a predictor are correct conditional on
themselves and membership in a collection of potentially overlapping subgroups
of a population. The work of [NR23] established a surprising connection between
multicalibration for an arbitrary property $\Gamma$ (e.g., mean or median) and
property elicitation: a property $\Gamma$ can be multicalibrated if and only if
it is elicitable, where elicitability is the notion that the true property
value of a distribution can be obtained by solving a regression problem over
the distribution. In the online setting, [NR23] proposed an inefficient
algorithm that achieves $\sqrt T$ $\ell_2$-multicalibration error for a
hypothesis class of group membership functions and an elicitable property
$\Gamma$, after $T$ rounds of interaction between a forecaster and adversary.
  In this paper, we generalize multicalibration for an elicitable property
$\Gamma$ from group membership functions to arbitrary bounded hypothesis
classes and introduce a stronger notion -- swap multicalibration, following
[GKR23]. Subsequently, we propose an oracle-efficient algorithm which, when
given access to an online agnostic learner, achieves $T^{1/(r+1)}$
$\ell_r$-swap multicalibration error with high probability (for $r\ge2$) for a
hypothesis class with bounded sequential Rademacher complexity and an
elicitable property $\Gamma$. For the special case of $r=2$, this implies an
oracle-efficient algorithm that achieves $T^{1/3}$ $\ell_2$-swap
multicalibration error, which significantly improves on the previously
established bounds for the problem [NR23, GMS25, LSS25a], and completely
resolves an open question raised in [GJRR24] on the possibility of an
oracle-efficient algorithm that achieves $\sqrt{T}$ $\ell_2$-mean
multicalibration error by answering it in a strongly affirmative sense.

</details>


### [135] [Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction](https://arxiv.org/abs/2511.05396)
*Yiting He,Zhishuai Liu,Weixin Wang,Pan Xu*

Main category: cs.LG

TL;DR: Introduces supremal visitation ratio to quantify exploration difficulty in online RMDPs with transition uncertainty; proves unbounded ratio makes online learning exponentially hard; provides a computationally efficient algorithm achieving sublinear regret under f-divergence uncertainty, with matching lower bounds and empirical validation.


<details>
  <summary>Details</summary>
Motivation: Addresses the gap in online reinforcement learning when training and deployment dynamics differ (off-dynamics RL). Existing work relies on generative models or good offline coverage; this work studies learning with only online interaction and robust transition uncertainty, a realistically challenging setting.

Method: Defines supremal visitation ratio to capture training-deployment dynamics mismatch. Formulates online RMDPs with f-divergence based transition uncertainty sets. Proposes a computationally efficient online algorithm achieving sublinear regret. Establishes upper regret bounds dependent on the supremal visitation ratio and episode count, and proves matching lower bounds. Validates results with numerical experiments.

Result: The proposed algorithm attains sublinear regret in online RMDPs under f-divergence transition uncertainty. The regret bounds are shown to be optimal with respect to the supremal visitation ratio and the number of interaction episodes. Empirical experiments corroborate the theoretical findings and illustrate practical performance.

Conclusion: This work advances online RL under model misspecification by introducing a quantifiable exploration difficulty measure (supremal visitation ratio) and delivering an algorithm with optimal regret guarantees under f-divergence uncertainty. It lays groundwork for robust online planning under dynamics mismatch and suggests directions for extending to broader uncertainty sets and scalability.

Abstract: Off-dynamics reinforcement learning (RL), where training and deployment
transition dynamics are different, can be formulated as learning in a robust
Markov decision process (RMDP) where uncertainties in transition dynamics are
imposed. Existing literature mostly assumes access to generative models
allowing arbitrary state-action queries or pre-collected datasets with a good
state coverage of the deployment environment, bypassing the challenge of
exploration. In this work, we study a more realistic and challenging setting
where the agent is limited to online interaction with the training environment.
To capture the intrinsic difficulty of exploration in online RMDPs, we
introduce the supremal visitation ratio, a novel quantity that measures the
mismatch between the training dynamics and the deployment dynamics. We show
that if this ratio is unbounded, online learning becomes exponentially hard. We
propose the first computationally efficient algorithm that achieves sublinear
regret in online RMDPs with $f$-divergence based transition uncertainties. We
also establish matching regret lower bounds, demonstrating that our algorithm
achieves optimal dependence on both the supremal visitation ratio and the
number of interaction episodes. Finally, we validate our theoretical results
through comprehensive numerical experiments.

</details>


### [136] [BiPETE: A Bi-Positional Embedding Transformer Encoder for Risk Assessment of Alcohol and Substance Use Disorder with Electronic Health Records](https://arxiv.org/abs/2511.04998)
*Daniel S. Lee,Mayra S. Haedo-Cruz,Chen Jiang,Oshin Miranda,LiRong Wang*

Main category: cs.LG

TL;DR: BiPETE: a Bi-Positional Embedding Transformer Encoder for single-disease risk prediction on EHRs, combining rotary relative-timing and sinusoidal order encodings to handle irregular visit intervals; trained on depressive disorder and PTSD cohorts to predict ASUD; achieves large AUPRC gains and provides interpretable insights via Integrated Gradients.


<details>
  <summary>Details</summary>
Motivation: Modeling temporal dependencies in EHR data with irregular visit intervals and nonuniform structure is challenging; there is a need for accurate and interpretable risk prediction without heavy pretraining.

Method: Propose BiPETE with dual positional encodings: rotary embeddings for relative visit timing and sinusoidal embeddings to preserve visit order; train on two mental health cohorts to predict ASUD risk; evaluate with AUPRC; conduct ablation; apply Integrated Gradients for interpretation.

Result: BiPETE improves AUPRC by 34% in depression and 50% in PTSD cohorts over baselines; ablation supports the dual encoding; interpretable features include inflammatory/hematologic/metabolic markers, medications, comorbidities linked to ASUD risk/protection.

Conclusion: BiPETE offers a practical, interpretable framework for EHR-based disease risk prediction with strong performance, leveraging dual positional encodings to handle temporal irregularities.

Abstract: Transformer-based deep learning models have shown promise for disease risk
prediction using electronic health records(EHRs), but modeling temporal
dependencies remains a key challenge due to irregular visit intervals and lack
of uniform structure. We propose a Bi-Positional Embedding Transformer Encoder
or BiPETE for single-disease prediction, which integrates rotary positional
embeddings to encode relative visit timing and sinusoidal embeddings to
preserve visit order. Without relying on large-scale pretraining, BiPETE is
trained on EHR data from two mental health cohorts-depressive disorder and
post-traumatic stress disorder (PTSD)-to predict the risk of alcohol and
substance use disorders (ASUD). BiPETE outperforms baseline models, improving
the area under the precision-recall curve (AUPRC) by 34% and 50% in the
depression and PTSD cohorts, respectively. An ablation study further confirms
the effectiveness of the dual positional encoding strategy. We apply the
Integrated Gradients method to interpret model predictions, identifying key
clinical features associated with ASUD risk and protection, such as abnormal
inflammatory, hematologic, and metabolic markers, as well as specific
medications and comorbidities. Overall, these key clinical features identified
by the attribution methods contribute to a deeper understanding of the risk
assessment process and offer valuable clues for mitigating potential risks. In
summary, our study presents a practical and interpretable framework for disease
risk prediction using EHR data, which can achieve strong performance.

</details>


### [137] [Machine Learning Algorithms in Statistical Modelling Bridging Theory and Application](https://arxiv.org/abs/2511.04918)
*A. Ganapathi Rao,Sathish Krishna Anumula,Aditya Kumar Singh,Renukhadevi M,Y. Jeevan Nagendra Kumar,Tammineni Rama Tulasi*

Main category: cs.LG

TL;DR: Hybrid ML + traditional statistical models can outperform standalone models in accuracy, robustness, and interpretability, by enriching conventional approaches with modern ML techniques.


<details>
  <summary>Details</summary>
Motivation: To understand how integrating ML algorithms with classical statistics can improve data analysis, predictive analytics, and decision-making across data-driven fields.

Method: Analytical examination of connections between ML methods and statistical models, showing how newer algorithms enrich conventional models, with demonstrations of performance, scalability, flexibility, and robustness gains in hybrid systems.

Result: Hybrid models yield notable improvements in predictive accuracy, robustness, scalability, and interpretability over traditional approaches.

Conclusion: Integrating ML with traditional statistics provides substantial benefits and should be considered a central strategy for modern predictive modeling and decision-making.

Abstract: It involves the completely novel ways of integrating ML algorithms with
traditional statistical modelling that has changed the way we analyze data, do
predictive analytics or make decisions in the fields of the data. In this
paper, we study some ML and statistical model connections to understand ways in
which some modern ML algorithms help 'enrich' conventional models; we
demonstrate how new algorithms improve performance, scale, flexibility and
robustness of the traditional models. It shows that the hybrid models are of
great improvement in predictive accuracy, robustness, and interpretability

</details>


### [138] [Leak@$k$: Unlearning Does Not Make LLMs Forget Under Probabilistic Decoding](https://arxiv.org/abs/2511.04934)
*Hadi Reisizadeh,Jiajun Ruan,Yiwei Chen,Soumyadeep Pal,Sijia Liu,Mingyi Hong*

Main category: cs.LG

TL;DR: Most unlearning methods fail to truly forget in LLMs; leakage resurfaces under probabilistic decoding; introduces leak@k metric and large-scale evaluation across TOFU, MUSE, WMDP showing persistent leakage; calls for robust unlearning approaches.


<details>
  <summary>Details</summary>
Motivation: Regulatory compliance and ethical AI require removing sensitive information (private, toxic, illegal, copyrighted) from LLMs. Greedy decoding can mask leakage; robust forgetting must hold under realistic sampling.

Method: Introduce leak@k metric to quantify forgetting under k-sample generation; perform large-scale evaluation across three benchmarks (TOFU, MUSE, WMDP) and multiple unlearning methods; compare outcomes under greedy vs probabilistic decoding; analyze persistence of leakage.

Result: Knowledge leakage persists across methods and tasks; existing unlearning techniques provide only limited forgetting; standard greedy-evaluation overestimates forgetting; leak@k reveals vulnerability to reappearance of forgotten content.

Conclusion: Current state-of-the-art unlearning techniques are insufficient; robust unlearning approaches and broader evaluation frameworks (like leak@k) are urgently needed to ensure true forgetting in LLMs.

Abstract: Unlearning in large language models (LLMs) is critical for regulatory
compliance and for building ethical generative AI systems that avoid producing
private, toxic, illegal, or copyrighted content. Despite rapid progress, in
this work we show that \textit{almost all} existing unlearning methods fail to
achieve true forgetting in practice. Specifically, while evaluations of these
`unlearned' models under deterministic (greedy) decoding often suggest
successful knowledge removal using standard benchmarks (as has been done in the
literature), we show that sensitive information reliably resurfaces when models
are sampled with standard probabilistic decoding. To rigorously capture this
vulnerability, we introduce \texttt{leak@$k$}, a new meta-evaluation metric
that quantifies the likelihood of forgotten knowledge reappearing when
generating $k$ samples from the model under realistic decoding strategies.
Using three widely adopted benchmarks, TOFU, MUSE, and WMDP, we conduct the
first large-scale, systematic study of unlearning reliability using our newly
defined \texttt{leak@$k$} metric. Our findings demonstrate that knowledge
leakage persists across methods and tasks, underscoring that current
state-of-the-art unlearning techniques provide only limited forgetting and
highlighting the urgent need for more robust approaches to LLM unlearning.

</details>


### [139] [OvA-LP: A Simple and Efficient Framework for Federated Learning on Non-IID Data](https://arxiv.org/abs/2511.05028)
*Dongjin Park,Hasung Yeo,Joon-Woo Lee*

Main category: cs.LG

TL;DR: OvA-LP is a drift-suppressing minimal framework for federated fine-tuning (FFT) of foundation models, using linear probing on a frozen encoder with a one-vs-all head in a two-stage setup to prevent drift at the source, achieving near-IID performance under non-IID client distributions with low per-round cost.


<details>
  <summary>Details</summary>
Motivation: In FFT with decentralized data, local drift from heterogeneous client updates biases the global model and amplifies variance. Existing aggregation/personalization methods mainly correct drift post hoc and struggle under extreme non-IID conditions.

Method: A minimalist two-stage approach: perform linear probing on a frozen encoder with a one-vs-all head. This design preserves pretrained feature geometry and decouples logits to avoid drift amplification. Features can be precomputed so per-round cost is nearly independent of encoder size.

Result: On CIFAR-100 with 100 clients across shard-1, shard-2, and Bernoulli-Dirichlet partitions, OvA-LP retains 95.9% of its IID accuracy, while state-of-the-art FFT baselines retain only 10.1% (PFPT) and 34.5% (FFT-MoE) under the same conditions. It also remains robust under symmetric and asymmetric label noise.

Conclusion: OvA-LP provides a principled and efficient basis for robust FFT under heterogeneity by suppressing drift at its source, with strong empirical performance and favorable computational properties due to precomputed features.

Abstract: Federated fine-tuning (FFT) adapts foundation models to decentralized data
but remains fragile under heterogeneous client distributions due to local
drift, i.e., client-level update divergences that induce systematic bias and
amplified variance in the global model. Existing aggregation and
personalization methods largely correct drift post hoc, which proves brittle
under extreme non-IID conditions. We introduce OvA-LP, a minimalist framework
that is, to our knowledge, the first explicitly designed to suppress drift at
its source within the PEFT-based FFT paradigm. OvA-LP combines linear probing
on a frozen encoder with a one-vs-all head and a simple two-stage procedure,
preserving pretrained feature geometry and decoupling logits to prevent the
mechanisms that amplify drift. On CIFAR-100 with 100 clients, averaged over
shard-1, shard-2, and Bernoulli-Dirichlet partitions, OvA-LP retains 95.9% of
its IID accuracy, whereas state-of-the-art FFT baselines retain only 10.1%
(PFPT) and 34.5% (FFT-MoE) under the same conditions. OvA-LP further maintains
resilience under both symmetric and asymmetric label noise. In addition,
precomputing encoder features makes per-round cost nearly independent of
encoder size. Together, these results demonstrate that OvA-LP provides a
principled and efficient basis for robust FFT under heterogeneity.

</details>


### [140] [Structural Properties, Cycloid Trajectories and Non-Asymptotic Guarantees of EM Algorithm for Mixed Linear Regression](https://arxiv.org/abs/2511.04937)
*Zhankun Luo,Abolfazl Hashemi*

Main category: cs.LG

TL;DR: EM for 2-component MLR with unknown weights/parameters: explicit updates across all SNR, cycloid-like trajectory in noiseless case, linear/quadratic convergence depending on angle, plus non-asymptotic finite-sample guarantees; introduces a trajectory-based analysis framework.


<details>
  <summary>Details</summary>
Motivation: To understand EM behavior in fully unknown 2MLR settings where mixing weights and regression parameters are unknown, addressing gaps left by prior work that assumes known/balanced weights and high SNR.

Method: Derive explicit EM update equations for 2MLR with unknown mixing weights and regression parameters in all SNR regimes; analyze the iterative trajectory, establish cycloid trajectory in the noiseless case via a recurrence for the sub-optimality angle, quantify deviations in high SNR, derive convergence orders (linear when near-orthogonal to ground truth; quadratic when angle is small), and provide non-asymptotic finite-sample bounds linking statistical error to sub-optimality angle; prove convergence from arbitrary initialization.

Result: Demonstrates a cycloid trajectory for parameter updates in the noiseless setting; characterizes convergence order as linear or quadratic depending on angle; provides non-asymptotic finite-sample guarantees and a trajectory-based framework for EM in 2MLR with unknown weights.

Conclusion: Introduces a novel trajectory-based framework for analyzing EM in 2MLR with fully unknown weights, delivering both structural insights (cycloid behavior) and practical guarantees (finite-sample convergence from any initialization).

Abstract: This work investigates the structural properties, cycloid trajectories, and
non-asymptotic convergence guarantees of the Expectation-Maximization (EM)
algorithm for two-component Mixed Linear Regression (2MLR) with unknown mixing
weights and regression parameters. Recent studies have established global
convergence for 2MLR with known balanced weights and super-linear convergence
in noiseless and high signal-to-noise ratio (SNR) regimes. However, the
theoretical behavior of EM in the fully unknown setting remains unclear, with
its trajectory and convergence order not yet fully characterized. We derive
explicit EM update expressions for 2MLR with unknown mixing weights and
regression parameters across all SNR regimes and analyze their structural
properties and cycloid trajectories. In the noiseless case, we prove that the
trajectory of the regression parameters in EM iterations traces a cycloid by
establishing a recurrence relation for the sub-optimality angle, while in high
SNR regimes we quantify its discrepancy from the cycloid trajectory. The
trajectory-based analysis reveals the order of convergence: linear when the EM
estimate is nearly orthogonal to the ground truth, and quadratic when the angle
between the estimate and ground truth is small at the population level. Our
analysis establishes non-asymptotic guarantees by sharpening bounds on
statistical errors between finite-sample and population EM updates, relating
EM's statistical accuracy to the sub-optimality angle, and proving convergence
with arbitrary initialization at the finite-sample level. This work provides a
novel trajectory-based framework for analyzing EM in Mixed Linear Regression.

</details>


### [141] [Risk Prediction of Cardiovascular Disease for Diabetic Patients with Machine Learning and Deep Learning Techniques](https://arxiv.org/abs/2511.04971)
*Esha Chowdhury*

Main category: cs.LG

TL;DR: ML and DL models using BRFSS data can predict CVD risk in diabetics with about 0.905 accuracy; XGBoost (ML) and LSTM (DL) perform best; PCA preprocessing; high recall in some models; supports automated risk stratification.


<details>
  <summary>Details</summary>
Motivation: Diabetes prevalence drives higher CVD risk; an accurate, efficient risk prediction model is needed to inform prevention and clinical decision-making for diabetic patients.

Method: Data preprocessing included deduplication, missing-value handling, feature type identification, and PCA for feature extraction. Evaluated traditional ML models (DT, RF, KNN, SVM, AdaBoost, XGBoost) and DL models (ANN, DNN, RNN, CNN, LSTM, BiLSTM, GRU) plus hybrids (CNN+LSTM/BiLSTM/GRU) on BRFSS data.

Result: XGBoost achieved the highest ML accuracy of 0.9050. Among DL models, LSTM achieved the highest accuracy of 0.9050, with some DL models attaining perfect recall (1.00). High accuracy and F1 scores indicate strong predictive performance and potential for clinical use.

Conclusion: ML and DL approaches can effectively predict CVD risk in diabetic patients, aiding automated risk assessment and personalized prevention strategies; robust performance suggests practical utility in supporting clinical decisions.

Abstract: Accurate prediction of cardiovascular disease (CVD) risk is crucial for
healthcare institutions. This study addresses the growing prevalence of
diabetes and its strong link to heart disease by proposing an efficient CVD
risk prediction model for diabetic patients using machine learning (ML) and
hybrid deep learning (DL) approaches. The BRFSS dataset was preprocessed by
removing duplicates, handling missing values, identifying categorical and
numerical features, and applying Principal Component Analysis (PCA) for feature
extraction. Several ML models, including Decision Trees (DT), Random Forest
(RF), k-Nearest Neighbors (KNN), Support Vector Machine (SVM), AdaBoost, and
XGBoost, were implemented, with XGBoost achieving the highest accuracy of
0.9050. Various DL models, such as Artificial Neural Networks (ANN), Deep
Neural Networks (DNN), Recurrent Neural Networks (RNN), Convolutional Neural
Networks (CNN), Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), and
Gated Recurrent Unit (GRU), as well as hybrid models combining CNN with LSTM,
BiLSTM, and GRU, were also explored. Some of these models achieved perfect
recall (1.00), with the LSTM model achieving the highest accuracy of 0.9050.
Our research highlights the effectiveness of ML and DL models in predicting CVD
risk among diabetic patients, automating and enhancing clinical
decision-making. High accuracy and F1 scores demonstrate these models'
potential to improve personalized risk management and preventive strategies.

</details>


### [142] [Less Is More: Generating Time Series with LLaMA-Style Autoregression in Simple Factorized Latent Spaces](https://arxiv.org/abs/2511.04973)
*Siyuan Li,Yifan Sun,Lei Cheng,Lewen Wang,Yang Liu,Weiqing Liu,Jianlong Li,Jiang Bian,Shikai Fang*

Main category: cs.LG

TL;DR: FAR-TS is a fast autoregressive transformer-based framework for multivariate time-series generation that disentangles data into a data-adaptive basis and a discrete latent space, enabling length-flexible synthesis with preserved cross-channel correlations and much faster generation than diffusion methods.


<details>
  <summary>Details</summary>
Motivation: Generative models for multivariate time series are valuable for data augmentation, simulation, and privacy, but diffusion-based methods are slow and restricted to fixed-length windows. There is a need for fast, controllable, length-flexible generation with interpretable latent representations that preserve cross-channel correlations.

Method: Decompose each time series into a data-adaptive basis capturing static cross-channel correlations and temporal coefficients vector-quantized into discrete tokens. Train a LLaMA-style autoregressive Transformer over the token sequences to model and generate time-series of arbitrary length, enabling fast, controllable synthesis in a quantized latent space.

Result: FAR-TS achieves orders-of-magnitude faster generation than Diffusion-TS while preserving cross-channel correlations and offering an interpretable latent space, enabling high-quality and flexible time series synthesis.

Conclusion: The FAR-TS framework provides a simple, effective approach that combines disentangled factorization with autoregressive Transformers to enable fast, flexible, and controllable multivariate time-series generation with an interpretable latent space.

Abstract: Generative models for multivariate time series are essential for data
augmentation, simulation, and privacy preservation, yet current
state-of-the-art diffusion-based approaches are slow and limited to
fixed-length windows. We propose FAR-TS, a simple yet effective framework that
combines disentangled factorization with an autoregressive Transformer over a
discrete, quantized latent space to generate time series. Each time series is
decomposed into a data-adaptive basis that captures static cross-channel
correlations and temporal coefficients that are vector-quantized into discrete
tokens. A LLaMA-style autoregressive Transformer then models these token
sequences, enabling fast and controllable generation of sequences with
arbitrary length. Owing to its streamlined design, FAR-TS achieves
orders-of-magnitude faster generation than Diffusion-TS while preserving
cross-channel correlations and an interpretable latent space, enabling
high-quality and flexible time series synthesis.

</details>


### [143] [Scaling Up ROC-Optimizing Support Vector Machines](https://arxiv.org/abs/2511.04979)
*Gimun Bae,Seung Jun Shin*

Main category: cs.LG

TL;DR: A scalable ROC-SVM via incomplete U-statistics and low-rank kernel approximation, achieving similar AUC with much faster training.


<details>
  <summary>Details</summary>
Motivation: Directly optimizing AUC for imbalanced data is attractive but computationally expensive due to O(n^2) pairwise comparisons; a scalable alternative is needed.

Method: Use incomplete U-statistics to approximate ROC-AUC optimization, and apply a low-rank kernel approximation to enable nonlinear RKHS training. Provides theoretical error bounds for the approximation.

Result: Empirical results on synthetic and real datasets show comparable AUC to the original ROC-SVM with substantially reduced training time.

Conclusion: The proposed approach delivers scalable, effective ROC optimization with provable guarantees and practical speedups, enabling efficient nonlinear ROC-SVM training.

Abstract: The ROC-SVM, originally proposed by Rakotomamonjy, directly maximizes the
area under the ROC curve (AUC) and has become an attractive alternative of the
conventional binary classification under the presence of class imbalance.
However, its practical use is limited by high computational cost, as training
involves evaluating all $O(n^2)$. To overcome this limitation, we develop a
scalable variant of the ROC-SVM that leverages incomplete U-statistics, thereby
substantially reducing computational complexity. We further extend the
framework to nonlinear classification through a low-rank kernel approximation,
enabling efficient training in reproducing kernel Hilbert spaces. Theoretical
analysis establishes an error bound that justifies the proposed approximation,
and empirical results on both synthetic and real datasets demonstrate that the
proposed method achieves comparable AUC performance to the original ROC-SVM
with drastically reduced training time.

</details>


### [144] [DL101 Neural Network Outputs and Loss Functions](https://arxiv.org/abs/2511.05131)
*Fernando Berzal*

Main category: cs.LG

TL;DR: Analyzes activation functions for neural network outputs (linear, sigmoid, ReLU, softmax) and links loss functions (MSE, MAE, cross-entropy) to statistical principles, notably MLE and GLMs, highlighting practical considerations like alternative output encodings and heavy-tailed distributions.


<details>
  <summary>Details</summary>
Motivation: To provide a statistically grounded justification for choosing activation functions and loss functions in neural networks by relating them to probability distributions and generalized linear models, and to address practical encoding and tail behavior.

Method: Survey/analysis of common output activations and loss functions; establish connections to MLE, GLMs, and probability distributions; discuss scenarios with alternative encodings, constrained outputs, and heavy tails.

Result: Clarifies that selecting a loss function corresponds to assuming a particular output distribution; maps popular losses to distributional assumptions and GLMs; provides guidelines for when to use each activation/loss pair; highlights practical considerations.

Conclusion: A principled framework links activation choices and loss functions to statistical modeling, enabling informed design of neural networks; emphasizes the impact of output distribution assumptions and practical constraints on model behavior.

Abstract: The loss function used to train a neural network is strongly connected to its
output layer from a statistical point of view. This technical report analyzes
common activation functions for a neural network output layer, like linear,
sigmoid, ReLU, and softmax, detailing their mathematical properties and their
appropriate use cases. A strong statistical justification exists for the
selection of the suitable loss function for training a deep learning model.
This report connects common loss functions such as Mean Squared Error (MSE),
Mean Absolute Error (MAE), and various Cross-Entropy losses to the statistical
principle of Maximum Likelihood Estimation (MLE). Choosing a specific loss
function is equivalent to assuming a specific probability distribution for the
model output, highlighting the link between these functions and the Generalized
Linear Models (GLMs) that underlie network output layers. Additional scenarios
of practical interest are also considered, such as alternative output
encodings, constrained outputs, and distributions with heavy tails.

</details>


### [145] [Unlocking the Black Box: A Five-Dimensional Framework for Evaluating Explainable AI in Credit Risk](https://arxiv.org/abs/2511.04980)
*Rongbin Ye,Jiaqi Chen*

Main category: cs.LG

TL;DR: Introduces a five-dimensional explainability framework and demonstrates that advanced ML models in finance can be explained using SHAP/LIME, aligning performance with regulatory needs.


<details>
  <summary>Details</summary>
Motivation: Regulators require interpretability in finance; 'black box' models risk non-compliance. While SHAP/LIME exist, there is a need for a comprehensive framework to assess explainability beyond simple accuracy and to evaluate trade-offs between performance and interpretability.

Method: Apply SHAP and LIME to multiple models in a regulated financial setting; compare predictive performance with explainability; propose a five-dimensional framework—Inherent Interpretability, Global Explanations, Local Explanations, Consistency, and Complexity—and provide evaluation guidelines and a feasibility demonstration for deploying high-performing models with explainability.

Result: Findings suggest that high-performing, more complex models can achieve explainability levels comparable to simpler models when evaluated with SHAP/LIME. The five-dimensional framework enables a nuanced assessment of interpretability beyond accuracy and demonstrates the feasibility of using sophisticated ML in regulated finance under explainability constraints.

Conclusion: The proposed framework offers a structured method to balance model performance and interpretability, supporting the deployment of advanced ML in finance within regulatory limits. It also points to future work on refining explainability metrics and practical deployment strategies.

Abstract: The financial industry faces a significant challenge modeling and risk
portfolios: balancing the predictability of advanced machine learning models,
neural network models, and explainability required by regulatory entities (such
as Office of the Comptroller of the Currency, Consumer Financial Protection
Bureau). This paper intends to fill the gap in the application between these
"black box" models and explainability frameworks, such as LIME and SHAP.
Authors elaborate on the application of these frameworks on different models
and demonstrates the more complex models with better prediction powers could be
applied and reach the same level of the explainability, using SHAP and LIME.
Beyond the comparison and discussion of performances, this paper proposes a
novel five dimensional framework evaluating Inherent Interpretability, Global
Explanations, Local Explanations, Consistency, and Complexity to offer a
nuanced method for assessing and comparing model explainability beyond simple
accuracy metrics. This research demonstrates the feasibility of employing
sophisticated, high performing ML models in regulated financial environments by
utilizing modern explainability techniques and provides a structured approach
to evaluate the crucial trade offs between model performance and
interpretability.

</details>


### [146] [Deep Progressive Training: scaling up depth capacity of zero/one-layer models](https://arxiv.org/abs/2511.04981)
*Zhiqi Bu*

Main category: cs.LG

TL;DR: Progressive depth expansion with zero/one-layer training dramatically reduces compute while preserving loss, enabling scalable training of deep models.


<details>
  <summary>Details</summary>
Motivation: Deep networks offer higher accuracy at the cost of increased computation. Progressive training seeks to expand model capacity during training to reduce overall training cost with minimal loss degradation.

Method: The paper analyzes depth expansion through optimization theory and feature learning, proposing zero/one-layer progressive training. It discusses effective initialization for new layers, transfer of hyperparameters, learning rate schedules, and the timing of model expansion, with theoretical and empirical justification.

Result: Zero/one-layer progressive training on GPT-2 can save about 80% of compute (roughly a 5x speedup) while achieving almost the same loss as a fully trained 60-layer, 7B-parameter model.

Conclusion: Depth expansion can be made efficient and practical: careful initialization and scheduling enable substantial compute savings with negligible loss increase, making progressive training a viable approach for large-scale models.

Abstract: Model depth is a double-edged sword in deep learning: deeper models achieve
higher accuracy but require higher computational cost. To efficiently train
models at scale, an effective strategy is the progressive training, which
scales up model capacity during training, hence significantly reducing
computation with little to none performance degradation. In this work, we study
the depth expansion of large models through the lens of optimization theory and
feature learning, offering insights on the initialization of new layers,
hyperparameter transfer, learning rate schedule, and timing of model expansion.
Specifically, we propose zero/one-layer progressive training for the optimal
tradeoff between computation and loss. For example, zero/one-layer progressive
training on GPT2 can save $\approx 80\%$ compute, or equivalently accelerate
$\approx 5\times$ while achieving almost the same loss, compared to to a fully
trained 60-layer model with 7B parameters.

</details>


### [147] [Model Merging Improves Zero-Shot Generalization in Bioacoustic Foundation Models](https://arxiv.org/abs/2511.05171)
*Davide Marincione,Donato Crisostomi,Roberto Dessi,Emanuele Rodolà,Emanuele Rossi*

Main category: cs.LG

TL;DR: Merging NatureLM with its base LM recovers instruction-following while preserving domain knowledge, leading to strong zero-shot generalization and a new SOTA for unseen species in closed-set classification.


<details>
  <summary>Details</summary>
Motivation: To overcome the trade-off between domain-specific bioacoustic accuracy and instruction-following flexibility in foundation models, enabling better zero-shot generalization across species.

Method: Interpolate NatureLM with its base language model (a simple model merging strategy) to balance domain expertise and instruction-following.

Result: Merged model restores instruction-following with minimal loss of domain expertise; achieves >200% relative improvement in zero-shot generalization; sets new SOTA for closed-set zero-shot classification of unseen species.

Conclusion: Model merging is an effective and practical approach to jointly improve instruction-following and zero-shot generalization in bioacoustic foundation models.

Abstract: Foundation models capable of generalizing across species and tasks represent
a promising new frontier in bioacoustics, with NatureLM being one of the most
prominent examples. While its domain-specific fine-tuning yields strong
performance on bioacoustic benchmarks, we observe that it also introduces
trade-offs in instruction-following flexibility. For instance, NatureLM
achieves high accuracy when prompted for either the common or scientific name
individually, but its accuracy drops significantly when both are requested in a
single prompt. We address this by applying a simple model merging strategy that
interpolates NatureLM with its base language model, recovering
instruction-following capabilities with minimal loss of domain expertise.
Finally, we show that the merged model exhibits markedly stronger zero-shot
generalization, achieving over a 200% relative improvement and setting a new
state-of-the-art in closed-set zero-shot classification of unseen species.

</details>


### [148] [Peptide2Mol: A Diffusion Model for Generating Small Molecules as Peptide Mimics for Targeted Protein Binding](https://arxiv.org/abs/2511.04984)
*Xinheng He,Yijia Zhang,Haowei Lin,Xingang Peng,Xiangzhe Kong,Mingyu Li,Jianzhu Ma*

Main category: cs.LG

TL;DR: Peptide2Mol uses an E(3)-equivariant graph neural network diffusion model to generate small molecules guided by peptide binders and pocket context, achieving state-of-the-art non-autoregressive generation and enabling peptidomimetic optimization.


<details>
  <summary>Details</summary>
Motivation: Gap: AI-driven structure-based drug design often ignores endogenous protein–peptide interactions, potentially leading to suboptimal molecules; need a method that accounts for peptide binders and pocket environments.

Method: An E(3)-equivariant GNN diffusion model, Peptide2Mol, references original peptide binders and surrounding protein pocket; non-autoregressive generation; trained on large datasets; supports partial diffusion for optimization/peptidomimetic design.

Result: State-of-the-art performance on non-autoregressive generative tasks; molecules exhibit similarity to the original peptide binder; enables molecule optimization and peptidomimetic design.

Conclusion: Peptide2Mol is an effective deep generative framework for generating and optimizing bioactive small molecules from protein binding pockets, integrating endogenous interactions to improve design quality.

Abstract: Structure-based drug design has seen significant advancements with the
integration of artificial intelligence (AI), particularly in the generation of
hit and lead compounds. However, most AI-driven approaches neglect the
importance of endogenous protein interactions with peptides, which may result
in suboptimal molecule designs. In this work, we present Peptide2Mol, an
E(3)-equivariant graph neural network diffusion model that generates small
molecules by referencing both the original peptide binders and their
surrounding protein pocket environments. Trained on large datasets and
leveraging sophisticated modeling techniques, Peptide2Mol not only achieves
state-of-the-art performance in non-autoregressive generative tasks, but also
produces molecules with similarity to the original peptide binder.
Additionally, the model allows for molecule optimization and peptidomimetic
design through a partial diffusion process. Our results highlight Peptide2Mol
as an effective deep generative model for generating and optimizing bioactive
small molecules from protein binding pockets.

</details>


### [149] [No One-Model-Fits-All: Uncovering Spatio-Temporal Forecasting Trade-offs with Graph Neural Networks and Foundation Models](https://arxiv.org/abs/2511.05179)
*Ragini Gupta,Naman Raina,Bo Chen,Li Chen,Claudiu Danilov,Josh Eckhardt,Keyshla Bernard,Klara Nahrstedt*

Main category: cs.LG

TL;DR: A systematic study compares forecasting models under varying sensor density and sampling rates, showing STGNNs excel with sparse coverage, TSFMs excel at high-frequency data but falter with limited spatial data, and the multivariate TSFM Moirai bests all models by learning cross-sensor dependencies; code is open-sourced.


<details>
  <summary>Details</summary>
Motivation: To understand how sampling frequency, spatial coverage, and model architecture interact to affect forecasting performance in IoT environmental sensing, and to identify robust, efficient pipelines.

Method: Empirical study using real-world temperature data from a wireless sensor network. The study varies: (1) spatial sensor density, (2) sampling interval. Models evaluated include classical VAR, neural networks (GRU, Transformer), spatio-temporal graph neural networks (STGNNs), and time-series foundation models (TSFMs) Chronos Moirai and TimesFM. Performance across configurations is analyzed.

Result: STGNNs perform well when deployments are sparse and sampling is moderate, leveraging spatial correlations. TSFMs are competitive at high sampling frequencies but degrade as spatial coverage from neighbors decreases. The multivariate TSFM Moirai outperforms all others by natively learning cross-sensor dependencies.

Conclusion: Provides actionable guidance for building efficient spatio-temporal forecasting pipelines in IoT systems; all code and datasets are open-sourced for reproducibility.

Abstract: Modern IoT deployments for environmental sensing produce high volume
spatiotemporal data to support downstream tasks such as forecasting, typically
powered by machine learning models. While existing filtering and strategic
deployment techniques optimize collected data volume at the edge, they overlook
how variations in sampling frequencies and spatial coverage affect downstream
model performance. In many forecasting models, incorporating data from
additional sensors denoise predictions by providing broader spatial contexts.
This interplay between sampling frequency, spatial coverage and different
forecasting model architectures remain underexplored. This work presents a
systematic study of forecasting models - classical models (VAR), neural
networks (GRU, Transformer), spatio-temporal graph neural networks (STGNNs),
and time series foundation models (TSFMs: Chronos Moirai, TimesFM) under
varying spatial sensor nodes density and sampling intervals using real-world
temperature data in a wireless sensor network. Our results show that STGNNs are
effective when sensor deployments are sparse and sampling rate is moderate,
leveraging spatial correlations via encoded graph structure to compensate for
limited coverage. In contrast, TSFMs perform competitively at high frequencies
but degrade when spatial coverage from neighboring sensors is reduced.
Crucially, the multivariate TSFM Moirai outperforms all models by natively
learning cross-sensor dependencies. These findings offer actionable insights
for building efficient forecasting pipelines in spatio-temporal systems. All
code for model configurations, training, dataset, and logs are open-sourced for
reproducibility:
https://github.com/UIUC-MONET-Projects/Benchmarking-Spatiotemporal-Forecast-Models

</details>


### [150] [Carbon Price Forecasting with Structural Breaks: A Comparative Study of Deep Learning Models](https://arxiv.org/abs/2511.04988)
*Runsheng Ren,Jing Li,Yanxiu Li,Shixun Huang,Jun Shen,Wanqing Li,John Le,Sheng Wang*

Main category: cs.LG

TL;DR: A hybrid framework for carbon price forecasting that jointly detects structural breaks, denoises signals via wavelets, and uses deep learning models (LSTM, GRU, TCN); the PELT-WT-TCN model achieves top accuracy on EU ETS EUA data (2007–2024).


<details>
  <summary>Details</summary>
Motivation: Forecasting carbon prices is crucial for energy market decisions and decarbonization but is hindered by frequent policy-induced structural breaks and high-frequency noise. A robust, generalizable approach should unify break detection, denoising, and modeling within modern DL architectures.

Method: A comprehensive pipeline integrating structural break detection (Bai-Perron, ICSS, PELT), wavelet-based denoising, and three DL models (LSTM, GRU, TCN). Builds univariate and multivariate datasets with exogenous features (energy prices, policy indicators). Evaluates against state-of-the-art baseline (Breakpoints with Wavelet and LSTM).

Result: The proposed PELT-WT-TCN achieved the highest prediction accuracy, reducing RMSE by 22.35% and MAE by 18.63% compared to the Breakpoints with Wavelet and LSTM baseline; and reducing RMSE by 70.55% and MAE by 74.42% compared to the original LSTM without decomposition. Demonstrates improved robustness and interpretability for carbon price forecasting and other nonstationary financial time series.

Conclusion: Integrating structural awareness with multiscale decomposition enhances accuracy and interpretability of carbon price forecasts and can generalize to other nonstationary financial time series; the hybrid framework is a valuable blueprint for future forecasting tasks under regime shifts.

Abstract: Accurately forecasting carbon prices is essential for informed energy market
decision-making, guiding sustainable energy planning, and supporting effective
decarbonization strategies. However, it remains challenging due to structural
breaks and high-frequency noise caused by frequent policy interventions and
market shocks. Existing studies, including the most recent baseline approaches,
have attempted to incorporate breakpoints but often treat denoising and
modeling as separate processes and lack systematic evaluation across advanced
deep learning architectures, limiting the robustness and the generalization
capability. To address these gaps, this paper proposes a comprehensive hybrid
framework that integrates structural break detection (Bai-Perron, ICSS, and
PELT algorithms), wavelet signal denoising, and three state-of-the-art deep
learning models (LSTM, GRU, and TCN). Using European Union Allowance (EUA) spot
prices from 2007 to 2024 and exogenous features such as energy prices and
policy indicators, the framework constructs univariate and multivariate
datasets for comparative evaluation. Experimental results demonstrate that our
proposed PELT-WT-TCN achieves the highest prediction accuracy, reducing
forecasting errors by 22.35% in RMSE and 18.63% in MAE compared to the
state-of-the-art baseline model (Breakpoints with Wavelet and LSTM), and by
70.55% in RMSE and 74.42% in MAE compared to the original LSTM without
decomposition from the same baseline study. These findings underscore the value
of integrating structural awareness and multiscale decomposition into deep
learning architectures to enhance accuracy and interpretability in carbon price
forecasting and other nonstationary financial time series.

</details>


### [151] [An End-to-End Deep Reinforcement Learning Approach for Solving the Traveling Salesman Problem with Drones](https://arxiv.org/abs/2511.05265)
*Taihelong Zeng,Yun Lin,Yuhe Shi,Yan Li,Zhiqing Wei,Xuanru Ji*

Main category: cs.LG

TL;DR: A hierarchical deep reinforcement learning framework for the TSP-D using a Transformer-based encoder with sparse attention and a Minimal Gated Unit decoder within an asynchronous actor-critic setup, delivering competitive solutions with faster training and inference on N=10–100 instances.


<details>
  <summary>Details</summary>
Motivation: The Traveling Salesman Problem with Drones (TSP-D) extends classical routing with truck-drone coordination, offering potential efficiency and environmental benefits but presenting NP-hard complexity. Deep reinforcement learning offers self-supervised policy learning for adaptive routing decisions, yet existing RL approaches face training efficiency and scalability challenges.

Method: A hierarchical Actor-Critic DRL architecture: a Transformer-inspired encoder with an optimized k-NN sparse attention mechanism and global node features, coupled with a Minimal Gated Unit decoder to generate solution sequences. The model is trained within an asynchronous advantage actor-critic framework (A3C-like).

Result: On benchmark TSP-D instances with sizes N ranging from 10 to 100, the proposed model achieves competitive or superior solution quality while exhibiting shorter average computation times compared with high-performance heuristics and existing RL methods. Additionally, it substantially reduces total training time relative to advanced RL baselines while attaining better final performance.

Conclusion: The framework demonstrates effective and training-efficient solving of TSP-D, with architectural choices—sparse attention, global features, and a compact decoder—driving both solution quality and training efficiency. These contributions support scalability to moderate problem sizes and suggest applicability to broader drone-enabled VRPs, albeit with future work needed to assess scalability and real-world constraints.

Abstract: The emergence of truck-drone collaborative systems in last-mile logistics has
positioned the Traveling Salesman Problem with Drones (TSP-D) as a pivotal
extension of classical routing optimization, where synchronized vehicle
coordination promises substantial operational efficiency and reduced
environmental impact, yet introduces NP-hard combinatorial complexity beyond
the reach of conventional optimization paradigms. Deep reinforcement learning
offers a theoretically grounded framework to address TSP-D's inherent
challenges through self-supervised policy learning and adaptive
decision-making. This study proposes a hierarchical Actor-Critic deep
reinforcement learning framework for solving the TSP-D problem. The
architecture consists of two primary components: a Transformer-inspired encoder
and an efficient Minimal Gated Unit decoder. The encoder incorporates a novel,
optimized k-nearest neighbors sparse attention mechanism specifically for
focusing on relevant spatial relationships, further enhanced by the integration
of global node features. The Minimal Gated Unit decoder processes these encoded
representations to efficiently generate solution sequences. The entire
framework operates within an asynchronous advantage actor-critic paradigm.
Experimental results show that, on benchmark TSP-D instances of various scales
(N=10 to 100), the proposed model can obtain competitive or even superior
solutions in shorter average computation times compared to high-performance
heuristic algorithms and existing reinforcement learning methods. Moreover,
compared to advanced reinforcement learning algorithm benchmarks, the proposed
framework significantly reduces the total training time required while
achieving superior final performance, highlighting its notable advantage in
training efficiency.

</details>


### [152] [Usando LLMs para Programar Jogos de Tabuleiro e Variações](https://arxiv.org/abs/2511.05114)
*Álvaro Guglielmin Becker,Lana Bertoldo Rossato,Anderson Rocha Tavares*

Main category: cs.LG

TL;DR: Three LLMs (Claude, DeepSeek, ChatGPT) are evaluated for generating code for board games and their variants; the abstract outlines a testing approach rather than reporting results.


<details>
  <summary>Details</summary>
Motivation: Board game coding is time-consuming; LLMs could speed up development by turning simple prompts into working code.

Method: They propose a method to test the capability of Claude, DeepSeek, and ChatGPT at producing code for board games and variants.

Result: The abstract does not report results.

Conclusion: No conclusions are stated; the work focuses on proposing a testing methodology for LLM-assisted board game coding.

Abstract: Creating programs to represent board games can be a time-consuming task.
Large Language Models (LLMs) arise as appealing tools to expedite this process,
given their capacity to efficiently generate code from simple contextual
information. In this work, we propose a method to test how capable three LLMs
(Claude, DeepSeek and ChatGPT) are at creating code for board games, as well as
new variants of existing games.

</details>


### [153] [Integrating Score-Based Diffusion Models with Machine Learning-Enhanced Localization for Advanced Data Assimilation in Geological Carbon Storage](https://arxiv.org/abs/2511.05266)
*Gabriel Serrão Seabra,Nikolaj T. Mücke,Vinicius Luiz Santos Silva,Alexandre A. Emerick,Denis Voskov,Femke Vossepoel*

Main category: cs.LG

TL;DR: Diffusion-model–driven ML localization within ESMDA improves covariance estimation and preserves ensemble spread in CO2 injection simulations, keeping data fit comparable to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Need for accurate subsurface heterogeneity characterization and reliable uncertainty quantification in geological carbon storage; traditional localization can overly shrink ensemble spread and misrepresent covariance.

Method: Generate priors with FLUVSIM channelized permeability fields; use score-based diffusion model to create permeabilities; large ensembles (Ns=5000). Apply ML-enhanced localization using ML algorithms to estimate states and permeabilities; integrate into Ensemble Smoother with Multiple Data Assimilation (ESMDA). Simulate CO2 injection in Delft Advanced Research Terra Simulator (DARTS). Compare with/without ML localization; assess variance preservation and data-misfit.

Result: ML-based localization preserves significantly more ensemble variance than non-localized or traditional setups while achieving comparable data-matching quality; demonstrates improved covariance estimation and actionable uncertainty quantification.

Conclusion: The integrated diffusion-ML localization framework enhances reliability of uncertainty quantification for risk assessment in GCS projects and offers practical benefits for robust risk analysis.

Abstract: Accurate characterization of subsurface heterogeneity is important for the
safe and effective implementation of geological carbon storage (GCS) projects.
This paper explores how machine learning methods can enhance data assimilation
for GCS with a framework that integrates score-based diffusion models with
machine learning-enhanced localization in channelized reservoirs during CO$_2$
injection. We employ a machine learning-enhanced localization framework that
uses large ensembles ($N_s = 5000$) with permeabilities generated by the
diffusion model and states computed by simple ML algorithms to improve
covariance estimation for the Ensemble Smoother with Multiple Data Assimilation
(ESMDA). We apply ML algorithms to a prior ensemble of channelized permeability
fields, generated with the geostatistical model FLUVSIM. Our approach is
applied on a CO$_2$ injection scenario simulated using the Delft Advanced
Research Terra Simulator (DARTS). Our ML-based localization maintains
significantly more ensemble variance than when localization is not applied,
while achieving comparable data-matching quality. This framework has practical
implications for GCS projects, helping improve the reliability of uncertainty
quantification for risk assessment.

</details>


### [154] [QuAnTS: Question Answering on Time Series](https://arxiv.org/abs/2511.05124)
*Felix Divo,Maurice Kraus,Anh Q. Nguyen,Hao Xue,Imran Razzak,Flora D. Salim,Kristian Kersting,Devendra Singh Dhami*

Main category: cs.LG

TL;DR: Proposes QuAnTS, a large-scale time-series QA dataset focused on human motion skeleton trajectories, plus baselines and human performance, to spur research in TSQA.


<details>
  <summary>Details</summary>
Motivation: Time-series QA is underexplored compared with QA for vision/text. Textual access to time-series data can improve accessibility and decision-making. The authors create a dataset and baselines to bridge this gap and enable research on interacting with time-series models via text.

Method: Introduce QuAnTS, a large-scale dataset with diverse questions and answers about human motion from tracked skeleton trajectories; validate dataset quality via extensive experiments; evaluate existing and new baselines; include human performance as a reference.

Result: QuAnTS is shown to be well-formed and comprehensive; baseline evaluations are conducted to establish a foundation for TSQA; human performance is provided as a reference, demonstrating practical usability.

Conclusion: The work aims to spur future research on interacting with time-series models through text, enabling better decision-making and more transparent systems.

Abstract: Text offers intuitive access to information. This can, in particular,
complement the density of numerical time series, thereby allowing improved
interactions with time series models to enhance accessibility and
decision-making. While the creation of question-answering datasets and models
has recently seen remarkable growth, most research focuses on question
answering (QA) on vision and text, with time series receiving minute attention.
To bridge this gap, we propose a challenging novel time series QA (TSQA)
dataset, QuAnTS, for Question Answering on Time Series data. Specifically, we
pose a wide variety of questions and answers about human motion in the form of
tracked skeleton trajectories. We verify that the large-scale QuAnTS dataset is
well-formed and comprehensive through extensive experiments. Thoroughly
evaluating existing and newly proposed baselines then lays the groundwork for a
deeper exploration of TSQA using QuAnTS. Additionally, we provide human
performances as a key reference for gauging the practical usability of such
models. We hope to encourage future research on interacting with time series
models through text, enabling better decision-making and more transparent
systems.

</details>


### [155] [Consecutive Preferential Bayesian Optimization](https://arxiv.org/abs/2511.05163)
*Aras Erarslan,Carlos Sevilla Salcedo,Ville Tanskanen,Anni Nisov,Eero Päiväkumpu,Heikki Aisala,Kaisu Honkapää,Arto Klami,Petrus Mikkola*

Main category: cs.LG

TL;DR: Introduces Consecutive Preferential Bayesian Optimization to account for production costs and perceptual ambiguity in preference-based optimization; uses constrained comparisons, a Just-Noticeable Difference threshold, and information-theoretic acquisition, achieving higher accuracy when production costs are high or feedback is indifferent.


<details>
  <summary>Details</summary>
Motivation: Standard preference-based optimization assumes friendly (low-cost) evaluations and unambiguous feedback, but in many real-world scenarios both evaluation production costs and perceptual ambiguity of human feedback are significant. The work aims to make Bayesian optimization more practical by explicitly modeling these costs and ambiguities.

Method: Develops Consecutive Preferential Bayesian Optimization (CPBO) that constrains comparisons to previously generated candidates to reduce production costs; incorporates a Just-Noticeable Difference (JND) threshold into a probabilistic preference model to capture indifference to small utility differences; uses an information-theoretic acquisition strategy to select configurations that maximize information about the optimum under the augmented preference model.

Result: Empirical experiments show a notable increase in accuracy in settings with high production costs or with indifference feedback.

Conclusion: CPBO reduces production costs while effectively handling perceptual ambiguity, improving optimization efficiency when evaluations are costly or feedback is non-discriminative.

Abstract: Preferential Bayesian optimization allows optimization of objectives that are
either expensive or difficult to measure directly, by relying on a minimal
number of comparative evaluations done by a human expert. Generating candidate
solutions for evaluation is also often expensive, but this cost is ignored by
existing methods. We generalize preference-based optimization to explicitly
account for production and evaluation costs with Consecutive Preferential
Bayesian Optimization, reducing production cost by constraining comparisons to
involve previously generated candidates. We also account for the perceptual
ambiguity of the oracle providing the feedback by incorporating a
Just-Noticeable Difference threshold into a probabilistic preference model to
capture indifference to small utility differences. We adapt an
information-theoretic acquisition strategy to this setting, selecting new
configurations that are most informative about the unknown optimum under a
preference model accounting for the perceptual ambiguity. We empirically
demonstrate a notable increase in accuracy in setups with high production costs
or with indifference feedback.

</details>


### [156] [Multimodal Deep Learning for Prediction of Progression-Free Survival in Patients with Neuroendocrine Tumors Undergoing 177Lu-based Peptide Receptor Radionuclide Therapy](https://arxiv.org/abs/2511.05169)
*Simon Baur,Tristan Ruhwedel,Ekin Böke,Zuzanna Kobus,Gergana Lishkova,Christoph Wetz,Holger Amthauer,Christoph Roderburg,Frank Tacke,Julian M. Rogasch,Wojciech Samek,Henning Jann,Jackie Ma,Johannes Eschrich*

Main category: cs.LG

TL;DR: Multimodal deep learning model combining laboratory values, SR-PET, and CT provides best PFS prediction after PRRT in metastatic NETs (AUROC ~0.72, AUPRC ~0.80), outperforming unimodal models; external validation needed for clinical adoption.


<details>
  <summary>Details</summary>
Motivation: Improve prediction of progression-free survival to tailor PRRT treatment and follow-up in metastatic neuroendocrine tumors.

Method: Retrospective single-center study with 116 metastatic NET patients treated with 177Lu-DOTATOC. Collected clinical data, laboratory biomarkers, and pretherapeutic SR-PET/CT. Trained seven models: unimodal (laboratory, SR-PET, or CT) and multimodal fusion approaches; included a pretrained CT branch. Evaluated with AUROC and AUPRC; explainability via feature importance and gradient maps.

Result: Best model: multimodal fusion of laboratory values, SR-PET, and CT with pretrained CT branch; AUROC 0.72 ± 0.01, AUPRC 0.80 ± 0.01. Unimodal results: laboratory-only AUROC 0.59, SR-PET CNN AUROC 0.42, CT CNN AUROC 0.54. Short PFS (<1 year) in 36% of patients; higher baseline chromogranin A, elevated gamma-GT, and fewer PRRT cycles associated with short PFS.

Conclusion: Multimodal deep learning improves PFS prediction after PRRT over unimodal models and could support risk-adapted follow-up, pending external validation.

Abstract: Peptide receptor radionuclide therapy (PRRT) is an established treatment for
metastatic neuroendocrine tumors (NETs), yet long-term disease control occurs
only in a subset of patients. Predicting progression-free survival (PFS) could
support individualized treatment planning. This study evaluates laboratory,
imaging, and multimodal deep learning models for PFS prediction in PRRT-treated
patients. In this retrospective, single-center study 116 patients with
metastatic NETs undergoing 177Lu-DOTATOC were included. Clinical
characteristics, laboratory values, and pretherapeutic somatostatin receptor
positron emission tomography/computed tomographies (SR-PET/CT) were collected.
Seven models were trained to classify low- vs. high-PFS groups, including
unimodal (laboratory, SR-PET, or CT) and multimodal fusion approaches.
Explainability was evaluated by feature importance analysis and gradient maps.
Forty-two patients (36%) had short PFS (< 1 year), 74 patients long PFS (>1
year). Groups were similar in most characteristics, except for higher baseline
chromogranin A (p = 0.003), elevated gamma-GT (p = 0.002), and fewer PRRT
cycles (p < 0.001) in short-PFS patients. The Random Forest model trained only
on laboratory biomarkers reached an AUROC of 0.59 +- 0.02. Unimodal
three-dimensional convolutional neural networks using SR-PET or CT performed
worse (AUROC 0.42 +- 0.03 and 0.54 +- 0.01, respectively). A multimodal fusion
model laboratory values, SR-PET, and CT -augmented with a pretrained CT branch
- achieved the best results (AUROC 0.72 +- 0.01, AUPRC 0.80 +- 0.01).
Multimodal deep learning combining SR-PET, CT, and laboratory biomarkers
outperformed unimodal approaches for PFS prediction after PRRT. Upon external
validation, such models may support risk-adapted follow-up strategies.

</details>


### [157] [Associative Poisoning to Generative Machine Learning](https://arxiv.org/abs/2511.05177)
*Mathias Lundteigen Mohus,Jingyue Li,Zhirong Yang*

Main category: cs.LG

TL;DR: A stealthy data-poisoning method, associative poisoning, manipulates joint feature associations in generative outputs by poisoning training data—without training-control—supported by formal theory, empirical validation, and a proposed countermeasure.


<details>
  <summary>Details</summary>
Motivation: Generative models are widespread and attractive targets; current poisoning either degrades outputs broadly or needs training access, limiting real-world risk; hence need stealthy, practical attack and defenses.

Method: Formalize the attack; perturb training data to influence statistical associations between specific feature pairs in outputs; prove feasibility and stealthiness; validate with two state-of-the-art models showing ability to induce/suppress associations while preserving marginals and output quality.

Result: The attack successfully modulates feature associations, preserves marginal distributions, maintains high-quality outputs, and remains visually undetectable; demonstrates vulnerability of image synthesis, synthetic data generation, and NLP pipelines; shows limitations of existing defenses.

Conclusion: Highlights the need for robust defenses against stealthy data-poisoning attacks; proposes a countermeasure strategy and loci for future work.

Abstract: The widespread adoption of generative models such as Stable Diffusion and
ChatGPT has made them increasingly attractive targets for malicious
exploitation, particularly through data poisoning. Existing poisoning attacks
compromising synthesised data typically either cause broad degradation of
generated data or require control over the training process, limiting their
applicability in real-world scenarios. In this paper, we introduce a novel data
poisoning technique called associative poisoning, which compromises
fine-grained features of the generated data without requiring control of the
training process. This attack perturbs only the training data to manipulate
statistical associations between specific feature pairs in the generated
outputs. We provide a formal mathematical formulation of the attack and prove
its theoretical feasibility and stealthiness. Empirical evaluations using two
state-of-the-art generative models demonstrate that associative poisoning
effectively induces or suppresses feature associations while preserving the
marginal distributions of the targeted features and maintaining high-quality
outputs, thereby evading visual detection. These results suggest that
generative systems used in image synthesis, synthetic dataset generation, and
natural language processing are susceptible to subtle, stealthy manipulations
that compromise their statistical integrity. To address this risk, we examine
the limitations of existing defensive strategies and propose a novel
countermeasure strategy.

</details>


### [158] [ProDER: A Continual Learning Approach for Fault Prediction in Evolving Smart Grids](https://arxiv.org/abs/2511.05420)
*Emad Efatinasab,Nahal Azadi,Davide Dalle Pezze,Gian Antonio Susto,Chuadhry Mujeeb Ahmed,Mirco Rampazzo*

Main category: cs.LG

TL;DR: The paper introduces a continual learning framework for smart-grid fault prediction to cope with evolving fault types and zones, presenting ProDER, a replay-based method combining prototype regularization, logit distillation, and prototype-guided replay; it achieves minimal accuracy drop and demonstrates practicality for scalable fault prediction.


<details>
  <summary>Details</summary>
Motivation: In smart grids, fixed AI fault predictors struggle to adapt to new fault types and operational zones; a continual learning approach is needed to evolve models in tandem with changing environments.

Method: Develop a continual learning framework for smart-grid fault prediction, with four evaluation scenarios (class-incremental and domain-incremental). ProDER (Prototype-based Dark Experience Replay) integrates prototype-based feature regularization, logit distillation, and a prototype-guided replay memory to mitigate forgetting and improve adaptation.

Result: ProDER achieves the best performance among tested CL techniques, with very small accuracy drops: 0.045 for fault type prediction and 0.015 for fault zone prediction.

Conclusion: CL is practical for scalable, real-world fault prediction in smart grids, enabling models to adapt to evolving fault types and zones while maintaining accuracy.

Abstract: As smart grids evolve to meet growing energy demands and modern operational
challenges, the ability to accurately predict faults becomes increasingly
critical. However, existing AI-based fault prediction models struggle to ensure
reliability in evolving environments where they are required to adapt to new
fault types and operational zones. In this paper, we propose a continual
learning (CL) framework in the smart grid context to evolve the model together
with the environment. We design four realistic evaluation scenarios grounded in
class-incremental and domain-incremental learning to emulate evolving grid
conditions. We further introduce Prototype-based Dark Experience Replay
(ProDER), a unified replay-based approach that integrates prototype-based
feature regularization, logit distillation, and a prototype-guided replay
memory. ProDER achieves the best performance among tested CL techniques, with
only a 0.045 accuracy drop for fault type prediction and 0.015 for fault zone
prediction. These results demonstrate the practicality of CL for scalable,
real-world fault prediction in smart grids.

</details>


### [159] [Linear Gradient Prediction with Control Variates](https://arxiv.org/abs/2511.05187)
*Kamil Ciosek,Nicolò Felicioni,Juan Elenter Litwin*

Main category: cs.LG

TL;DR: Unbiased, cheaper gradient-based training by combining a control variate with an NTK-inspired gradient predictor; validated on Vision Transformer classification.


<details>
  <summary>Details</summary>
Motivation: Training neural networks is costly due to full backpropagation; aim to reduce training cost while preserving convergence by using inexpensive gradient estimates.

Method: Derive a control-variate-based unbiased gradient estimator using an approximate predicted gradient; propose a gradient predictor inspired by Neural Tangent Kernel (NTK); integrate into the training loop with theoretical unbiasedness guarantees; demonstrate effectiveness empirically on a vision transformer task.

Result: Empirical results indicate reduced training cost while maintaining accuracy on a vision transformer classification task.

Conclusion: Presents a practical pathway to cheaper neural network training via unbiased gradient estimates and NTK-inspired prediction; suggests broad applicability and directions for future work such as variance analysis and scaling to larger models.

Abstract: We propose a new way of training neural networks, with the goal of reducing
training cost. Our method uses approximate predicted gradients instead of the
full gradients that require an expensive backward pass. We derive a
control-variate-based technique that ensures our updates are unbiased estimates
of the true gradient. Moreover, we propose a novel way to derive a predictor
for the gradient inspired by the theory of the Neural Tangent Kernel. We
empirically show the efficacy of the technique on a vision transformer
classification task.

</details>


### [160] [APP: Accelerated Path Patching with Task-Specific Pruning](https://arxiv.org/abs/2511.05442)
*Frauke Andersen,William Rudman,Ruochen Zhang,Carsten Eickhoff*

Main category: cs.LG

TL;DR: APP speeds up circuit discovery by pruning attention heads with Contrastive-FLAP and then applying Path Patching, achieving substantial search-space reductions and speedups with circuits comparable to prior methods.


<details>
  <summary>Details</summary>
Motivation: Circuit discovery for mechanistic interpretability is computationally expensive, and existing methods struggle with scalability and minimality requirements, especially for smaller models.

Method: Introduce Accelerated Path Patching (APP). First prune task-specific attention heads using Contrastive-FLAP, a causal mediation-inspired contrastive pruning method that prioritizes heads important to the task, reducing the search space by about 56%. Then apply traditional Path Patching to the remaining heads to extract circuits.

Result: APP reduces the search space by ~56% and achieves speedups of 59.63% to 93.27% compared to Path Patching on dense models. The circuits found with APP show substantial overlap and similar performance to circuits obtained by established Path Patching methods.

Conclusion: APP provides meaningful computational savings while preserving circuit quality, though circuits exhibit substantial overlap; future work could address further reducing redundancy and tightening the minimality constraint in circuit analysis.

Abstract: Circuit discovery is a key step in many mechanistic interpretability
pipelines. Current methods, such as Path Patching, are computationally
expensive and have limited in-depth circuit analysis for smaller models. In
this study, we propose Accelerated Path Patching (APP), a hybrid approach
leveraging our novel contrastive attention head pruning method to drastically
reduce the search space of circuit discovery methods. Our Contrastive-FLAP
pruning algorithm uses techniques from causal mediation analysis to assign
higher pruning scores to task-specific attention heads, leading to higher
performing sparse models compared to traditional pruning techniques. Although
Contrastive-FLAP is successful at preserving task-specific heads that existing
pruning algorithms remove at low sparsity ratios, the circuits found by
Contrastive-FLAP alone are too large to satisfy the minimality constraint
required in circuit analysis. APP first applies Contrastive-FLAP to reduce the
search space on required for circuit discovery algorithms by, on average, 56\%.
Next, APP, applies traditional Path Patching on the remaining attention heads,
leading to a speed up of 59.63\%-93.27\% compared to Path Patching applied to
the dense model. Despite the substantial computational saving that APP
provides, circuits obtained from APP exhibit substantial overlap and similar
performance to previously established Path Patching circuits

</details>


### [161] [ActiTect: A Generalizable Machine Learning Pipeline for REM Sleep Behavior Disorder Screening through Standardized Actigraphy](https://arxiv.org/abs/2511.05221)
*David Bertram,Anja Ophey,Sinah Röttgen,Konstantin Kuffer,Gereon R. Fink,Elke Kalbe,Clint Hansen,Walter Maetzler,Maximilian Kapsecker,Lara M. Reimer,Stephan Jonas,Andreas T. Damgaard,Natasha B. Bertelsen,Casper Skjaerbaek,Per Borghammer,Karolien Groenewald,Pietro-Luca Ratti,Michele T. Hu,Noémie Moreau,Michael Sommerauer,Katarzyna Bozek*

Main category: cs.LG

TL;DR: Open-source ActiTect detects RBD from actigraphy with robust preprocessing and cross-cohort validation, achieving AUROC up to 0.95.


<details>
  <summary>Details</summary>
Motivation: RBD is a key prodromal marker for α-synucleinopathies; wrist actigraphy has potential for large-scale screening but requires reliable, harmonized analysis across heterogeneous devices and settings.

Method: ActiTect is a fully automated ML pipeline that performs preprocessing, automated sleep–wake detection, and extraction of interpretable motion features from actigraphy. Development used 78 participants with nested cross-validation (AUROC 0.95). Generalization tested on blinded local set (n=31, AUROC 0.86) and two external cohorts (n=113, AUROC 0.84; n=57, AUROC 0.94). Leave-one-dataset-out CV (AUROC 0.84–0.89). Stability analysis shows reproducible features. The model is pooled and deployed as an open-source resource.

Result: High discriminative performance with strong internal validation and robust cross-cohort generalization (AUROCs: 0.95, 0.86, 0.84–0.94; LooD CV: 0.84–0.89). Key predictive features remain stable across datasets, supporting a generalizable RBD detection model using wearable data.

Conclusion: Open-source and user-friendly tool that can be widely adopted; enables independent validation and collaborative improvement, advancing toward a unified, generalizable RBD detection model using wearable actigraphy.

Abstract: Isolated rapid eye movement sleep behavior disorder (iRBD) is a major
prodromal marker of $\alpha$-synucleinopathies, often preceding the clinical
onset of Parkinson's disease, dementia with Lewy bodies, or multiple system
atrophy. While wrist-worn actimeters hold significant potential for detecting
RBD in large-scale screening efforts by capturing abnormal nocturnal movements,
they become inoperable without a reliable and efficient analysis pipeline. This
study presents ActiTect, a fully automated, open-source machine learning tool
to identify RBD from actigraphy recordings. To ensure generalizability across
heterogeneous acquisition settings, our pipeline includes robust preprocessing
and automated sleep-wake detection to harmonize multi-device data and extract
physiologically interpretable motion features characterizing activity patterns.
Model development was conducted on a cohort of 78 individuals, yielding strong
discrimination under nested cross-validation (AUROC = 0.95). Generalization was
confirmed on a blinded local test set (n = 31, AUROC = 0.86) and on two
independent external cohorts (n = 113, AUROC = 0.84; n = 57, AUROC = 0.94). To
assess real-world robustness, leave-one-dataset-out cross-validation across the
internal and external cohorts demonstrated consistent performance (AUROC range
= 0.84-0.89). A complementary stability analysis showed that key predictive
features remained reproducible across datasets, supporting the final pooled
multi-center model as a robust pre-trained resource for broader deployment. By
being open-source and easy to use, our tool promotes widespread adoption and
facilitates independent validation and collaborative improvements, thereby
advancing the field toward a unified and generalizable RBD detection model
using wearable devices.

</details>


### [162] [On Flow Matching KL Divergence](https://arxiv.org/abs/2511.05480)
*Maojiang Su,Jerry Yao-Chieh Hu,Sophia Pi,Han Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler
(KL) divergence of the flow-matching distribution approximation. In particular,
if the $L_2$ flow-matching loss is bounded by $\epsilon^2 > 0$, then the KL
divergence between the true data distribution and the estimated distribution is
bounded by $A_1 \epsilon + A_2 \epsilon^2$. Here, the constants $A_1$ and $A_2$
depend only on the regularities of the data and velocity fields. Consequently,
this bound implies statistical convergence rates of Flow Matching Transformers
under the Total Variation (TV) distance. We show that, flow matching achieves
nearly minimax-optimal efficiency in estimating smooth distributions. Our
results make the statistical efficiency of flow matching comparable to that of
diffusion models under the TV distance. Numerical studies on synthetic and
learned velocities corroborate our theory.

</details>


### [163] [The Causal Round Trip: Generating Authentic Counterfactuals by Eliminating Information Loss](https://arxiv.org/abs/2511.05236)
*Rui Wu,Lizheng Wang,Yongjun Li*

Main category: cs.LG

TL;DR: A diffusion-based causal framework (BELM-MDCM) eliminates information loss in diffusion-based abduction to enable accurate, individual-level counterfactuals in SCMs, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Faithful abduction in Structural Causal Models requires precise inference of latent exogenous noise. Diffusion models are powerful but their standard design causes information loss (SRE), hindering counterfactual reasoning. A framework that preserves causal information is needed.

Method: Formalize Causal Information Conservation (CIC) as a necessary condition for faithful abduction. Propose BELM-MDCM, an invertible diffusion-based architecture that eliminates SRE. Introduce Targeted Modeling for structural regularization and a Hybrid Training Objective to inject strong causal inductive bias.

Result: Zero-SRE framework delivers state-of-the-art accuracy and, crucially, enables high-fidelity, individual-level counterfactuals suitable for deep causal inquiries, as demonstrated by rigorous experiments.

Conclusion: This work provides a foundational blueprint uniting modern generative models with classical causal theory, establishing a new rigorous standard for diffusion-based causal inference and high-quality counterfactuals.

Abstract: Judea Pearl's vision of Structural Causal Models (SCMs) as engines for
counterfactual reasoning hinges on faithful abduction: the precise inference of
latent exogenous noise. For decades, operationalizing this step for complex,
non-linear mechanisms has remained a significant computational challenge. The
advent of diffusion models, powerful universal function approximators, offers a
promising solution. However, we argue that their standard design, optimized for
perceptual generation over logical inference, introduces a fundamental flaw for
this classical problem: an inherent information loss we term the Structural
Reconstruction Error (SRE). To address this challenge, we formalize the
principle of Causal Information Conservation (CIC) as the necessary condition
for faithful abduction. We then introduce BELM-MDCM, the first diffusion-based
framework engineered to be causally sound by eliminating SRE by construction
through an analytically invertible mechanism. To operationalize this framework,
a Targeted Modeling strategy provides structural regularization, while a Hybrid
Training Objective instills a strong causal inductive bias. Rigorous
experiments demonstrate that our Zero-SRE framework not only achieves
state-of-the-art accuracy but, more importantly, enables the high-fidelity,
individual-level counterfactuals required for deep causal inquiries. Our work
provides a foundational blueprint that reconciles the power of modern
generative models with the rigor of classical causal theory, establishing a new
and more rigorous standard for this emerging field.

</details>


### [164] [DGTN: Graph-Enhanced Transformer with Diffusive Attention Gating Mechanism for Enzyme DDG Prediction](https://arxiv.org/abs/2511.05483)
*Abigail Lin*

Main category: cs.LG

TL;DR: DGTN fuses graph priors and transformer attention via diffusion to predict enzyme mutations' effects on stability, achieving state-of-the-art on benchmark datasets with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing methods process sequence and structure separately and fail to capture coupling between local geometry and global sequence patterns; need a unified diffusion-based co-learning framework.

Method: Proposes bidirectional diffusion: GNN-derived structural embeddings guide transformer attention through learnable diffusion kernels; transformer representations refine GNN message passing via attention-modulated updates; provides mathematical analysis with convergence bounds.

Result: On ProTherm and SKEMPI, achieves state-of-the-art Pearson Rho ~0.87 and RMSE ~1.21 kcal/mol, ~6.2% improvement over baselines; ablation shows diffusion contributes ~4.8 points to correlation; theoretical convergence rate O(1/sqrt(T)).

Conclusion: Establishes a principled framework for integrating heterogeneous protein representations via learnable diffusion, enabling improved structure-sequence coupling and predictive accuracy.

Abstract: Predicting the effect of amino acid mutations on enzyme thermodynamic
stability (DDG) is fundamental to protein engineering and drug design. While
recent deep learning approaches have shown promise, they often process sequence
and structure information independently, failing to capture the intricate
coupling between local structural geometry and global sequential patterns. We
present DGTN (Diffused Graph-Transformer Network), a novel architecture that
co-learns graph neural network (GNN) weights for structural priors and
transformer attention through a diffusion mechanism. Our key innovation is a
bidirectional diffusion process where: (1) GNN-derived structural embeddings
guide transformer attention via learnable diffusion kernels, and (2)
transformer representations refine GNN message passing through
attention-modulated graph updates. We provide rigorous mathematical analysis
showing this co-learning scheme achieves provably better approximation bounds
than independent processing. On ProTherm and SKEMPI benchmarks, DGTN achieves
state-of-the-art performance (Pearson Rho = 0.87, RMSE = 1.21 kcal/mol), with
6.2% improvement over best baselines. Ablation studies confirm the diffusion
mechanism contributes 4.8 points to correlation. Our theoretical analysis
proves the diffused attention converges to optimal structure-sequence coupling,
with convergence rate O(1/sqrt(T) ) where T is diffusion steps. This work
establishes a principled framework for integrating heterogeneous protein
representations through learnable diffusion.

</details>


### [165] [Embedding-Space Data Augmentation to Prevent Membership Inference Attacks in Clinical Time Series Forecasting](https://arxiv.org/abs/2511.05289)
*Marius Fracarolli,Michael Staniek,Stefan Riezler*

Main category: cs.LG

TL;DR: Data augmentation via synthetic samples (ZOO, ZOO-PCA, MixUp) reduces membership inference attacks on time-series forecasting models trained on EHR data, with ZOO-PCA offering the best privacy gains without harming accuracy.


<details>
  <summary>Details</summary>
Motivation: Protect privacy in EHR-based TSF against MIAs while preserving predictive performance; investigate how synthetic data can confuse attackers and improve generalization.

Method: Train TSF models with synthetic data produced by three augmentation strategies (ZOO, ZOO-PCA, MixUp); evaluate resistance to loss-based MIA by measuring true-positive to false-positive ratio; compare test accuracy.

Result: ZOO-PCA most effectively reduces the MIA TPR/FPR ratio while maintaining test performance; ZOO and MixUp provide less pronounced privacy gains or potential trade-offs.

Conclusion: ZOO-PCA is a promising augmentation approach to bolster privacy in TSF on EHR without sacrificing accuracy; data augmentation strategy choice crucial for MIA resilience.

Abstract: Balancing strong privacy guarantees with high predictive performance is
critical for time series forecasting (TSF) tasks involving Electronic Health
Records (EHR). In this study, we explore how data augmentation can mitigate
Membership Inference Attacks (MIA) on TSF models. We show that retraining with
synthetic data can substantially reduce the effectiveness of loss-based MIAs by
reducing the attacker's true-positive to false-positive ratio. The key
challenge is generating synthetic samples that closely resemble the original
training data to confuse the attacker, while also introducing enough novelty to
enhance the model's ability to generalize to unseen data. We examine multiple
augmentation strategies - Zeroth-Order Optimization (ZOO), a variant of ZOO
constrained by Principal Component Analysis (ZOO-PCA), and MixUp - to
strengthen model resilience without sacrificing accuracy. Our experimental
results show that ZOO-PCA yields the best reductions in TPR/FPR ratio for MIA
attacks without sacrificing performance on test data.

</details>


### [166] [Attention and Compression is all you need for Controllably Efficient Language Models](https://arxiv.org/abs/2511.05313)
*Jatin Prakash,Aahlad Puli,Rajesh Ranganath*

Main category: cs.LG

TL;DR: CAT (Compress & Attend Transformer) is a simple, adaptive transformer that uses dense attention on compressed chunks to trade compute for memory. It supports multiple chunk sizes during training to allow test-time quality–compute trade-offs without retraining, and achieves strong efficiency with competitive quality on language modeling and long-context tasks.


<details>
  <summary>Details</summary>
Motivation: The quadratic attention cost in transformers forces reliance on efficiency tricks (sparse, sliding, convolutions, linear attention) that degrade quality. A flexible approach is needed that allows varying the quality–compute trade-off at test time without complex hybrids or handcrafted states.

Method: CAT uses two ingredients: dense attention and compression. Tokens are decoded by attending to compressed chunks of the past sequence. Compression reduces the effective sequence length, lowering compute/memory. A chunk size controls the quality/computation trade-off. The model can be trained with multiple chunk sizes simultaneously, enabling test-time adaptation without retraining.

Result: In exhaustive evaluations, CAT outperforms existing efficient baselines (including hybrids) across various compute-memory budgets. It matches dense transformers in language modeling across model scales while being 1.4–3x faster and using 2–9x less total memory.

Conclusion: A single adaptive CAT model provides flexible quality–compute trade-offs using compression and dense attention, offering simpler design, competitive quality, and substantial efficiency gains for long-context language tasks.

Abstract: The quadratic cost of attention in transformers motivated the development of
efficient approaches: namely sparse and sliding window attention, convolutions
and linear attention. Although these approaches result in impressive reductions
in compute and memory, they often trade-off with quality, specifically
in-context recall performance. Moreover, apriori fixing this quality-compute
tradeoff means being suboptimal from the get-go: some downstream applications
require more memory for in-context recall, while others require lower latency
and memory. Further, these approaches rely on heuristic choices that
artificially restrict attention, or require handcrafted and complex recurrent
state update rules, or they must be carefully composed with attention at
specific layers to form a hybrid architecture that complicates the design
process, especially at scale. To address above issues, we propose Compress &
Attend Transformer (CAT), a conceptually simple architecture employing two
simple ingredients only: dense attention and compression. CAT decodes chunks of
tokens by attending to compressed chunks of the sequence so far. Compression
results in decoding from a reduced sequence length that yields compute and
memory savings, while choosing a particular chunk size trades-off quality for
efficiency. Moreover, CAT can be trained with multiple chunk sizes at once,
unlocking control of quality-compute trade-offs directly at test-time without
any retraining, all in a single adaptive architecture. In exhaustive
evaluations on common language modeling tasks, in-context recall, and
long-context understanding, a single adaptive CAT model outperforms existing
efficient baselines, including hybrid architectures, across different
compute-memory budgets. Further, a single CAT matches dense transformer in
language modeling across model scales while being 1.4-3x faster and requiring
2-9x lower total memory usage.

</details>


### [167] [Turning Adversaries into Allies: Reversing Typographic Attacks for Multimodal E-Commerce Product Retrieval](https://arxiv.org/abs/2511.05325)
*Janet Jenq,Hongda Shen*

Main category: cs.LG

TL;DR: Render product metadata text onto images to boost vision-text alignment and improve zero-shot multimodal retrieval in e-commerce.


<details>
  <summary>Details</summary>
Motivation: Vision-language models are vulnerable to typographic attacks where embedded text misleads predictions; strengthening image-text alignment is needed for robust multimodal retrieval.

Method: Visually render relevant textual content (titles, descriptions) onto product images to perform vision-text compression, evaluate across three e-commerce categories on six vision foundation models.

Result: Consistent improvements in unimodal and multimodal retrieval accuracy across categories and model families on three datasets.

Conclusion: Visually rendering product metadata is a simple yet effective enhancement for zero-shot multimodal retrieval in e-commerce, mitigating typographic attacks.

Abstract: Multimodal product retrieval systems in e-commerce platforms rely on
effectively combining visual and textual signals to improve search relevance
and user experience. However, vision-language models such as CLIP are
vulnerable to typographic attacks, where misleading or irrelevant text embedded
in images skews model predictions. In this work, we propose a novel method that
reverses the logic of typographic attacks by rendering relevant textual content
(e.g., titles, descriptions) directly onto product images to perform
vision-text compression, thereby strengthening image-text alignment and
boosting multimodal product retrieval performance. We evaluate our method on
three vertical-specific e-commerce datasets (sneakers, handbags, and trading
cards) using six state-of-the-art vision foundation models. Our experiments
demonstrate consistent improvements in unimodal and multimodal retrieval
accuracy across categories and model families. Our findings suggest that
visually rendering product metadata is a simple yet effective enhancement for
zero-shot multimodal retrieval in e-commerce applications.

</details>


### [168] [Learning Dynamics from Input-Output Data with Hamiltonian Gaussian Processes](https://arxiv.org/abs/2511.05330)
*Jan-Hendrik Ewering,Robin E. Herrmann,Niklas Wahlström,Thomas B. Schön,Thomas Seel*

Main category: cs.LG

TL;DR: A Bayesian non-conservative Hamiltonian GP framework learns dynamics from input-output data (no velocity info) with a reduced-rank GP; it estimates hidden states, GP hyperparameters, and damping, enabling uncertainty-aware predictions.


<details>
  <summary>Details</summary>
Motivation: To construct physically consistent dynamic models from limited data by embedding Hamiltonian structure and damping into Gaussian Process regression, even when velocity or momentum data are unavailable.

Method: Develop a fully Bayesian scheme to infer densities over unknown hidden states, GP hyperparameters, and structural hyperparameters (e.g., damping). Use a reduced-rank GP approximation for scalable training and prediction, applied to non-conservative Hamiltonian dynamics learned from input-output data.

Result: Evaluated on a nonlinear simulation case study, comparing against a state-of-the-art method that relies on momentum measurements; the approach provides uncertainty-aware predictions and adheres to the underlying physical principles even with IO data.

Conclusion: Non-conservative Hamiltonian GPs learned from input-output data via a Bayesian reduced-rank framework are feasible and computationally efficient, enabling physically consistent dynamics modeling and uncertainty quantification when velocity data are unavailable.

Abstract: Embedding non-restrictive prior knowledge, such as energy conservation laws,
in learning-based approaches is a key motive to construct physically consistent
models from limited data, relevant for, e.g., model-based control. Recent work
incorporates Hamiltonian dynamics into Gaussian Process (GP) regression to
obtain uncertainty-quantifying models that adhere to the underlying physical
principles. However, these works rely on velocity or momentum data, which is
rarely available in practice. In this paper, we consider dynamics learning with
non-conservative Hamiltonian GPs, and address the more realistic problem
setting of learning from input-output data. We provide a fully Bayesian scheme
for estimating probability densities of unknown hidden states, of GP
hyperparameters, as well as of structural hyperparameters, such as damping
coefficients. Considering the computational complexity of GPs, we take
advantage of a reduced-rank GP approximation and leverage its properties for
computationally efficient prediction and training. The proposed method is
evaluated in a nonlinear simulation case study and compared to a
state-of-the-art approach that relies on momentum measurements.

</details>


### [169] [Diffusion-Based Electromagnetic Inverse Design of Scattering Structured Media](https://arxiv.org/abs/2511.05357)
*Mikhail Tsukerman,Konstantin Grotov,Pavel Ginzburg*

Main category: cs.LG

TL;DR: Diffusion-based conditional model for EM inverse design producing 2x2 metasurface geometries from target scattering, enabling fast, diverse solutions with competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: Inverse design of metasurfaces is ill-posed and computationally expensive; diffusion models can capture the multi-solution space and enable rapid exploration.

Method: 1D U-Net with Feature-wise Linear Modulation conditioning; conditional diffusion model that maps target differential scattering patterns to a 2x2 dielectric-sphere layout; samples to capture non-uniqueness; trained on 11k simulated metasurfaces.

Result: Median mean percentage error (MPE) <19% on unseen targets; best case 1.39%; outperforms CMA-ES evolutionary optimization; design time reduced from hours to seconds.

Conclusion: Diffusion-based approaches show promise for rapid, diverse electromagnetic inverse design, enabling faster exploration of complex metasurface architectures; potential impact on photonics and wireless systems; code available at the provided GitHub repository.

Abstract: We present a conditional diffusion model for electromagnetic inverse design
that generates structured media geometries directly from target differential
scattering cross-section profiles, bypassing expensive iterative optimization.
Our 1D U-Net architecture with Feature-wise Linear Modulation learns to map
desired angular scattering patterns to 2x2 dielectric sphere structure,
naturally handling the non-uniqueness of inverse problems by sampling diverse
valid designs. Trained on 11,000 simulated metasurfaces, the model achieves
median MPE below 19% on unseen targets (best: 1.39%), outperforming CMA-ES
evolutionary optimization while reducing design time from hours to seconds.
These results demonstrate that employing diffusion models is promising for
advancing electromagnetic inverse design research, potentially enabling rapid
exploration of complex metasurface architectures and accelerating the
development of next-generation photonic and wireless communication systems. The
code is publicly available at
https://github.com/mikzuker/inverse_design_metasurface_generation.

</details>


### [170] [Adversarially Robust Multitask Adaptive Control](https://arxiv.org/abs/2511.05444)
*Kasra Fallah,Leonardo F. Toso,James Anderson*

Main category: cs.LG

TL;DR: A clustered multitask robust adaptive LQR framework that uses resilient aggregation to mitigate corrupted updates; regret improves with more honest systems per cluster and remains robust under a bounded fraction of adversarial systems.


<details>
  <summary>Details</summary>
Motivation: Addresses learning control policies for multiple LQR tasks under model uncertainty and adversarial corruption. The goal is to enable cooperative learning that is robust to adversarial updates while maintaining efficiency across tasks.

Method: Proposes a clustered multitask approach that combines clustering, system identification, and resilient aggregation. Analyzes how clustering accuracy, intra-cluster heterogeneity, and adversarial behavior influence the expected regret of certainty-equivalent control across LQR tasks. Establishes non-asymptotic bounds showing regret decreases inversely with the number of honest systems per cluster, and remains reduced under a bounded adversarial fraction within each cluster.

Result: Derives non-asymptotic regret bounds demonstrating that increasing the number of honest systems per cluster reduces regret inversely, and that this benefit persists even when a bounded fraction of systems in each cluster are adversarial.

Conclusion: The proposed clustered robust multitask framework improves CE-LQR learning in the presence of adversaries, with performance governed by cluster quality and the allowed adversarial fraction; practical implications include leveraging cluster structure to enhance robustness and data efficiency in multi-agent LQR tasks.

Abstract: We study adversarially robust multitask adaptive linear quadratic control; a
setting where multiple systems collaboratively learn control policies under
model uncertainty and adversarial corruption. We propose a clustered multitask
approach that integrates clustering and system identification with resilient
aggregation to mitigate corrupted model updates. Our analysis characterizes how
clustering accuracy, intra-cluster heterogeneity, and adversarial behavior
affect the expected regret of certainty-equivalent (CE) control across LQR
tasks. We establish non-asymptotic bounds demonstrating that the regret
decreases inversely with the number of honest systems per cluster and that this
reduction is preserved under a bounded fraction of adversarial systems within
each cluster.

</details>


### [171] [Parameter-Efficient Conditioning for Material Generalization in Graph-Based Simulators](https://arxiv.org/abs/2511.05456)
*Naveen Raj Manoharan,Hassan Iqbal,Krishna Kumar*

Main category: cs.LG

TL;DR: Parameter-efficient conditioning via FiLM enables graph-network simulators to generalize across material behaviors with data-efficient fine-tuning focusing on early message-passing layers, yielding accurate long-horizon rollouts and enabling inverse-design tasks.


<details>
  <summary>Details</summary>
Motivation: GNS models typically train on a single material type and struggle to generalize across different constitutive behaviors; real-world engineering requires adaptive models that can handle varying material parameters.

Method: Identify that sensitivity to material properties concentrates in the early message-passing layers. Implement a FiLM-based conditioning mechanism targeted at the first 1-5 MP layers of a pretrained 10-layer MP stack. Fine-tune with as few as 12 short trajectories from new materials, achieving performance comparable to full fine-tuning. Validate on unseen/interpolated/extrapolated material parameters (e.g., friction angle and cohesion) and demonstrate inverse problems by estimating cohesion from trajectory data.

Result: Fine-tuning only the initial few MP layers matches full fine-tuning performance; FiLM conditioning yields accurate long-term rollouts for unseen/moderately extrapolated materials (up to ~2.5° friction angle and 0.25 kPa cohesion). Achieves ~5x data reduction compared to a multi-task baseline and enables successful inverse-design of cohesion from trajectories.

Conclusion: The proposed parameter-efficient FiLM conditioning enables GNS to adapt to varying material properties, making them viable for inverse design and closed-loop control where material parameters are design variables.

Abstract: Graph network-based simulators (GNS) have demonstrated strong potential for
learning particle-based physics (such as fluids, deformable solids, and
granular flows) while generalizing to unseen geometries due to their inherent
inductive biases. However, existing models are typically trained for a single
material type and fail to generalize across distinct constitutive behaviors,
limiting their applicability in real-world engineering settings. Using granular
flows as a running example, we propose a parameter-efficient conditioning
mechanism that makes the GNS model adaptive to material parameters. We identify
that sensitivity to material properties is concentrated in the early
message-passing (MP) layers, a finding we link to the local nature of
constitutive models (e.g., Mohr-Coulomb) and their effects on information
propagation. We empirically validate this by showing that fine-tuning only the
first few (1-5) of 10 MP layers of a pretrained model achieves comparable test
performance as compared to fine-tuning the entire network. Building on this
insight, we propose a parameter-efficient Feature-wise Linear Modulation (FiLM)
conditioning mechanism designed to specifically target these early layers. This
approach produces accurate long-term rollouts on unseen, interpolated, or
moderately extrapolated values (e.g., up to 2.5 degrees for friction angle and
0.25 kPa for cohesion) when trained exclusively on as few as 12 short
simulation trajectories from new materials, representing a 5-fold data
reduction compared to a baseline multi-task learning method. Finally, we
validate the model's utility by applying it to an inverse problem, successfully
identifying unknown cohesion parameters from trajectory data. This approach
enables the use of GNS in inverse design and closed-loop control tasks where
material properties are treated as design variables.

</details>


### [172] [Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models](https://arxiv.org/abs/2511.05460)
*Sarkar Snigdha Sarathi Das,Palash Goyal,Mihir Parmar,Yiwen Song,Long T. Le,Lesly Miculicich,Jinsung Yoon,Rui Zhang,Hamid Palangi,Tomas Pfister*

Main category: cs.LG

TL;DR: Synapse is an arbitration framework that dynamically weights a pool of pre-trained time series foundational models (TSFMs) based on context and forecast horizon; it also builds a robust forecast distribution by sampling from models' output quantiles, achieving superior results to standard ensembling and individual TSFMs.


<details>
  <summary>Details</summary>
Motivation: TSFMs exhibit specialized, non-uniform performance due to diverse training data/protocols; leveraging their complementary strengths can improve forecasting across tasks, domains, and horizons.

Method: Construct a pool of TSFMs; dynamically assign weights according to context-dependent performance; adaptively sample from the output quantiles of constituent models to produce a robust forecast distribution; evaluate arbitration strategies and horizon distribution effects.

Result: Synapse consistently outperforms popular ensembling techniques and individual TSFMs across forecasting tasks, demonstrating robust gains.

Conclusion: Arbitration among heterogeneous TSFMs is a promising route for time series forecasting; dynamic weighting and output quantile sampling yield robust, superior forecasts; this approach can generalize across domains and horizon settings.

Abstract: Pre-trained Time Series Foundational Models (TSFMs) represent a significant
advance, capable of forecasting diverse time series with complex
characteristics, including varied seasonalities, trends, and long-range
dependencies. Despite their primary goal of universal time series forecasting,
their efficacy is far from uniform; divergent training protocols and data
sources cause individual TSFMs to exhibit highly variable performance across
different forecasting tasks, domains, and horizons. Leveraging this
complementary expertise by arbitrating existing TSFM outputs presents a
compelling strategy, yet this remains a largely unexplored area of research. In
this paper, we conduct a thorough examination of how different TSFMs exhibit
specialized performance profiles across various forecasting settings, and how
we can effectively leverage this behavior in arbitration between different time
series models. We specifically analyze how factors such as model selection and
forecast horizon distribution can influence the efficacy of arbitration
strategies. Based on this analysis, we propose Synapse, a novel arbitration
framework for TSFMs. Synapse is designed to dynamically leverage a pool of
TSFMs, assign and adjust predictive weights based on their relative,
context-dependent performance, and construct a robust forecast distribution by
adaptively sampling from the output quantiles of constituent models.
Experimental results demonstrate that Synapse consistently outperforms other
popular ensembling techniques as well as individual TSFMs, demonstrating
Synapse's efficacy in time series forecasting.

</details>


### [173] [SiamMM: A Mixture Model Perspective on Deep Unsupervised Learning](https://arxiv.org/abs/2511.05462)
*Xiaodong Wang,Jing Huang,Kevin J Liang*

Main category: cs.LG

TL;DR: Grounds clustering-based self-supervised methods in a probabilistic framework of classical mixture models, introducing SiamMM, which achieves state-of-the-art results and yields interpretable clusters that resemble ground-truth labels.


<details>
  <summary>Details</summary>
Motivation: Clustering-based approaches for self-supervised/unsupervised learning are promising but largely heuristic. A principled, probabilistic formulation—linking clustering to classical mixture models—could improve performance, clarity, and interpretability.

Method: Establishes formal connections between unsupervised clustering methods and classical mixture models within a unified framework and uses this insight to develop a novel model named SiamMM.

Result: SiamMM attains state-of-the-art performance across various self-supervised learning benchmarks.

Conclusion: Learned clusters closely resemble unseen ground-truth labels, revealing potential instances of mislabeling and highlighting the interpretability benefits of the proposed probabilistic clustering framework.

Abstract: Recent studies have demonstrated the effectiveness of clustering-based
approaches for self-supervised and unsupervised learning. However, the
application of clustering is often heuristic, and the optimal methodology
remains unclear. In this work, we establish connections between these
unsupervised clustering methods and classical mixture models from statistics.
Through this framework, we demonstrate significant enhancements to these
clustering methods, leading to the development of a novel model named SiamMM.
Our method attains state-of-the-art performance across various self-supervised
learning benchmarks. Inspection of the learned clusters reveals a strong
resemblance to unseen ground truth labels, uncovering potential instances of
mislabeling.

</details>


### [174] [Precipitation nowcasting of satellite data using physically conditioned neural networks](https://arxiv.org/abs/2511.05471)
*Antônio Catão,Melvin Poveda,Leonardo Voltarelli,Paulo Orenstein*

Main category: cs.LG

TL;DR: TUPANN is a satellite-only nowcasting model that decouples forecast into physically meaningful components (motion, intensity) via a variational encoder–decoder with optical-flow supervision, a lead-time-conditioned MaxViT for latent evolution, and a differentiable advection operator, achieving strong, transferable short-term rainfall nowcasts from GOES-16/IMERG data across multiple climates with real-time efficiency.


<details>
  <summary>Details</summary>
Motivation: Dense weather-radar networks limit rainfall nowcasting in regions most exposed to extremes; a global, transferable, satellite-only approach can provide timely forecasts where radars are sparse, supporting operational needs across diverse climates.

Method: A physics-aligned architecture: (1) a variational encoder–decoder infers motion and intensity fields under optical-flow supervision; (2) a lead-time-conditioned MaxViT evolves the latent state; (3) a differentiable advection operator reconstructs future frames. Trained on GOES-16 RRQPE and evaluated on GOES-16 and IMERG data across four climates (Rio de Janeiro, Manaus, Miami, La Paz) for 10–180 min lead times, using CSI and HSS over 4–64 mm/h thresholds; compared against optical-flow, DL, and hybrid baselines.

Result: TUPANN achieves best or second-best skill in most settings, with pronounced gains at higher rainfall thresholds. Training on multiple cities improves performance; cross-city tests show modest degradation but occasional gains for rare heavy-rain regimes. Outputs smooth, interpretable motion fields aligned with optical flow and runs near real time due to GOES-16 latency.

Conclusion: Physically aligned learning enables nowcasts that are skillful, transferable, and global, combining satellite data with interpretable dynamics for effective short-term rainfall forecasts.

Abstract: Accurate short-term precipitation forecasts predominantly rely on dense
weather-radar networks, limiting operational value in places most exposed to
climate extremes. We present TUPANN (Transferable and Universal Physics-Aligned
Nowcasting Network), a satellite-only model trained on GOES-16 RRQPE. Unlike
most deep learning models for nowcasting, TUPANN decomposes the forecast into
physically meaningful components: a variational encoder-decoder infers motion
and intensity fields from recent imagery under optical-flow supervision, a
lead-time-conditioned MaxViT evolves the latent state, and a differentiable
advection operator reconstructs future frames. We evaluate TUPANN on both
GOES-16 and IMERG data, in up to four distinct climates (Rio de Janeiro,
Manaus, Miami, La Paz) at 10-180min lead times using the CSI and HSS metrics
over 4-64 mm/h thresholds. Comparisons against optical-flow, deep learning and
hybrid baselines show that TUPANN achieves the best or second-best skill in
most settings, with pronounced gains at higher thresholds. Training on multiple
cities further improves performance, while cross-city experiments show modest
degradation and occasional gains for rare heavy-rain regimes. The model
produces smooth, interpretable motion fields aligned with numerical optical
flow and runs in near real time due to the low latency of GOES-16. These
results indicate that physically aligned learning can provide nowcasts that are
skillful, transferable and global.

</details>


### [175] [SoilX: Calibration-Free Comprehensive Soil Sensing Through Contrastive Cross-Component Learning](https://arxiv.org/abs/2511.05482)
*Kang Yang,Yuanlin Yang,Yuning Chen,Sikai Yang,Xinyu Zhang,Wan Du*

Main category: cs.LG

TL;DR: SoilX offers calibration-free, multi-component soil sensing (M, N, P, K, C, Al) using Contrastive Cross-Component Learning and a tetrahedral antenna array to reduce calibration and improve generalization.


<details>
  <summary>Details</summary>
Motivation: Current wireless soil sensing requires recalibration to cope with soil texture variations (aluminosilicates and organic carbon), limiting practicality. A calibration-free, texture-robust approach is needed.

Method: Joint measurement of six soil components (M, N, P, K, C, Al) with explicit modeling of C and Al. Introduces Contrastive Cross-Component Learning (3CL) featuring an Orthogonality Regularizer and a Separation Loss to disentangle cross-component interference. Develops a tetrahedral antenna array with switching to measure soil dielectric permittivity independently of device placement.

Result: Experiments show SoilX reduces estimation errors by 23.8% to 31.5% compared with baselines and generalizes well to unseen fields.

Conclusion: SoilX delivers calibration-free, texture- and carbon-independent soil sensing by jointly modeling six components and mitigating cross-component interference, aided by a robust tetrahedral antenna design.

Abstract: Precision agriculture demands continuous and accurate monitoring of soil
moisture (M) and key macronutrients, including nitrogen (N), phosphorus (P),
and potassium (K), to optimize yields and conserve resources. Wireless soil
sensing has been explored to measure these four components; however, current
solutions require recalibration (i.e., retraining the data processing model) to
handle variations in soil texture, characterized by aluminosilicates (Al) and
organic carbon (C), limiting their practicality. To address this, we introduce
SoilX, a calibration-free soil sensing system that jointly measures six key
components: {M, N, P, K, C, Al}. By explicitly modeling C and Al, SoilX
eliminates texture- and carbon-dependent recalibration. SoilX incorporates
Contrastive Cross-Component Learning (3CL), with two customized terms: the
Orthogonality Regularizer and the Separation Loss, to effectively disentangle
cross-component interference. Additionally, we design a novel tetrahedral
antenna array with an antenna-switching mechanism, which can robustly measure
soil dielectric permittivity independent of device placement. Extensive
experiments demonstrate that SoilX reduces estimation errors by 23.8% to 31.5%
over baselines and generalizes well to unseen fields.

</details>
