<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 171]
- [cs.LG](#cs.LG) [Total: 285]
- [cs.RO](#cs.RO) [Total: 55]
- [cs.AI](#cs.AI) [Total: 89]
- [cs.CG](#cs.CG) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics](https://arxiv.org/abs/2510.03287)
*Moinak Bhattacharya,Gagandeep Singh,Prateek Prasanna*

Main category: cs.CV

TL;DR: SoC-DT introduces a differentiable, patient-specific digital twin that fuses reaction-diffusion tumor growth with discrete standard-of-care interventions and personal metadata, solved with a stable IMEX solver, yielding improved predictions over baselines.


<details>
  <summary>Details</summary>
Motivation: Accurate, interpretable prediction of tumor evolution under heterogeneous SoC therapies is needed due to inter-patient variability and limitations of purely mechanistic or purely data-driven models.

Method: A unified framework SoC-DT that combines reaction-diffusion PDEs with discrete interventions (surgery, chemo, radiotherapy) and genomic/demographic personalization; uses IMEX-SoC implicit-explicit solver for stability; differentiable to enable learning and personalization; tested on synthetic and real glioma data.

Result: SoC-DT outperforms classical PDE baselines and purely data-driven neural models in predicting post-treatment tumor structure on imaging.

Conclusion: Bridges mechanistic interpretability with differentiable solvers to enable patient-specific digital twins in oncology; lays principled foundation; code to be released.

Abstract: Accurate prediction of tumor trajectories under standard-of-care (SoC)
therapies remains a major unmet need in oncology. This capability is essential
for optimizing treatment planning and anticipating disease progression.
Conventional reaction-diffusion models are limited in scope, as they fail to
capture tumor dynamics under heterogeneous therapeutic paradigms. There is
hence a critical need for computational frameworks that can realistically
simulate SoC interventions while accounting for inter-patient variability in
genomics, demographics, and treatment regimens. We introduce Standard-of-Care
Digital Twin (SoC-DT), a differentiable framework that unifies
reaction-diffusion tumor growth models, discrete SoC interventions (surgery,
chemotherapy, radiotherapy) along with genomic and demographic personalization
to predict post-treatment tumor structure on imaging. An implicit-explicit
exponential time-differencing solver, IMEX-SoC, is also proposed, which ensures
stability, positivity, and scalability in SoC treatment situations. Evaluated
on both synthetic data and real world glioma data, SoC-DT consistently
outperforms classical PDE baselines and purely data-driven neural models in
predicting tumor dynamics. By bridging mechanistic interpretability with modern
differentiable solvers, SoC-DT establishes a principled foundation for
patient-specific digital twins in oncology, enabling biologically consistent
tumor dynamics estimation. Code will be made available upon acceptance.

</details>


### [2] [Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data](https://arxiv.org/abs/2510.03292)
*Doğanay Demir,İlknur Durgar Elkahlout*

Main category: cs.CV

TL;DR: A distributed multi-GPU inference pipeline paired with an interactive visualization platform analyzes celebrity dynamics in video episodes, generating timestamped appearance data and rich visuals to reveal prominence, timing, and co-appearance patterns.


<details>
  <summary>Details</summary>
Motivation: As video content dominates media, there is a need to quantify who appears, when, and with whom across episodes/season to inform analytics, content strategy, and audience insights; scalable, high-throughput processing is required for large video corpora.

Method: Hybrid architecture using optimized ONNX models, heterogeneous batch inference, and high-throughput parallelism across GPUs; production of timestamped appearance records; transformation into diverse visuals (frequency, duration, co-appearance matrices, network graphs, heatmaps) within an interactive platform.

Result: Provides multi-dimensional insights into celebrity prominence and screen-time distribution with temporal/co-appearance analysis across episodes/seasons; enables dynamic exploration and discovery of evolving relationships.

Conclusion: Integrates distributed recognition with structured visual analytics, enabling new entertainment analytics capabilities and informing content creation and audience engagement research.

Abstract: In an era dominated by video content, understanding its structure and
dynamics has become increasingly important. This paper presents a hybrid
framework that combines a distributed multi-GPU inference system with an
interactive visualization platform for analyzing celebrity dynamics in video
episodes. The inference framework efficiently processes large volumes of video
data by leveraging optimized ONNX models, heterogeneous batch inference, and
high-throughput parallelism, ensuring scalable generation of timestamped
appearance records. These records are then transformed into a comprehensive
suite of visualizations, including appearance frequency charts, duration
analyses, pie charts, co-appearance matrices, network graphs, stacked area
charts, seasonal comparisons, and heatmaps. Together, these visualizations
provide multi-dimensional insights into video content, revealing patterns in
celebrity prominence, screen-time distribution, temporal dynamics,
co-appearance relationships, and intensity across episodes and seasons. The
interactive nature of the system allows users to dynamically explore data,
identify key moments, and uncover evolving relationships between individuals.
By bridging distributed recognition with structured, visually-driven analytics,
this work enables new possibilities for entertainment analytics, content
creation strategies, and audience engagement studies.

</details>


### [3] [Domain-Robust Marine Plastic Detection Using Vision Models](https://arxiv.org/abs/2510.03294)
*Saanvi Kataria*

Main category: cs.CV

TL;DR: Compact CNNs trained with supervision generalize well to cross-domain underwater debris detection; MobileNetV2 achieves top cross-domain F1 (~0.97); fine-tuned models reach high precision (~99%) but variable recall; zero-shot CLIP shows higher recall but lower precision, while Gemini shows the opposite profile; common errors include coral textures, particulates, and glare; conclusion: lightweight models suffice for cross-domain detection, with large vision-language models offering complementary strengths.


<details>
  <summary>Details</summary>
Motivation: Address cross-domain robustness in underwater plastic detection to mitigate dataset biases caused by domain shift.

Method: Train CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision transformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset; evaluate on a balanced cross-domain test set composed of plastic-positive images from a different source and negatives from the training domain; assess zero-shot models CLIP ViT-L14 and Gemini 2.0 Flash.

Result: MobileNetV2 achieves the strongest cross-domain performance with F1 ≈ 0.97; all fine-tuned models reach high precision (~99%) but differ in recall; zero-shot CLIP has recall ≈ 80% and precision ≈ 56%, whereas Gemini shows precision ≈ 99% and recall ≈ 81%; error analysis highlights confusion with coral textures, suspended particulates, and glare; compact CNNs with supervised training generalize well for cross-domain underwater detection, while large pretrained vision-language models provide complementary strengths.

Conclusion: Compact, supervised CNNs can generalize effectively across cross-domain underwater debris detection; large pretrained vision-language models offer complementary strengths.

Abstract: Marine plastic pollution is a pressing environmental threat, making reliable
automation for underwater debris detection essential. However, vision systems
trained on one dataset often degrade on new imagery due to domain shift. This
study benchmarks models for cross-domain robustness, training convolutional
neural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision
transformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then
evaluates them on a balanced cross-domain test set built from plastic-positive
images drawn from a different source and negatives from the training domain.
Two zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,
that leverage pretraining to classify images without fine-tuning. Results show
the lightweight MobileNetV2 delivers the strongest cross-domain performance (F1
0.97), surpassing larger models. All fine-tuned models achieved high Precision
(around 99%), but differ in Recall, indicating varying sensitivity to plastic
instances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet
prone to false positives (Precision around 56%), whereas Gemini exhibits the
inverse profile (Precision around 99%, Recall around 81%). Error analysis
highlights recurring confusions with coral textures, suspended particulates,
and specular glare. Overall, compact CNNs with supervised training can
generalize effectively for cross-domain underwater detection, while large
pretrained vision-language models provide complementary strengths.

</details>


### [4] [Multimodal Arabic Captioning with Interpretable Visual Concept Integration](https://arxiv.org/abs/2510.03295)
*Passant Elchafei,Amany Fashwan*

Main category: cs.CV

TL;DR: A CLIP-guided Arabic image captioning framework (VLCAP) grounds generation in interpretable Arabic visual concepts retrieved by multilingual encoders and uses two captioning models to generate captions, yielding competitive BLEU and LLM-based metrics across configurations.


<details>
  <summary>Details</summary>
Motivation: Address the need for interpretable, culturally coherent Arabic image captions by grounding generation in visual concepts rather than relying solely on end-to-end captioning; leverage multilingual label retrieval and expanded vocabularies to improve accuracy and relevance.

Method: Stage 1: Use three multilingual encoders (mCLIP, AraCLIP, Jina V4) to retrieve Arabic labels. Build a hybrid vocabulary from training captions plus ~21k general-domain labels translated from Visual Genome. Top-k labels are converted into fluent Arabic prompts and combined with the image as input to vision-language models. Stage 2: Evaluate two captioning models (Qwen-VL and Gemini Pro Vision) across six encoder-decoder configurations.

Result: Best results: mCLIP + Gemini Pro Vision achieved BLEU-1 of 5.34% and cosine similarity of 60.01%. AraCLIP + Qwen-VL achieved the highest LLM-judge score of 36.33%. Overall, the framework yields interpretable, culturally coherent Arabic captions with improved grounding.

Conclusion: Grounding Arabic image captioning in interpretable visual concepts via multilingual encoders and staged generation can improve contextual and cultural relevance, offering a versatile approach that balances interpretability with caption quality across encoder-decoder configurations.

Abstract: We present VLCAP, an Arabic image captioning framework that integrates
CLIP-based visual label retrieval with multimodal text generation. Rather than
relying solely on end-to-end captioning, VLCAP grounds generation in
interpretable Arabic visual concepts extracted with three multilingual
encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label
retrieval. A hybrid vocabulary is built from training captions and enriched
with about 21K general domain labels translated from the Visual Genome dataset,
covering objects, attributes, and scenes. The top-k retrieved labels are
transformed into fluent Arabic prompts and passed along with the original image
to vision-language models. In the second stage, we tested Qwen-VL and Gemini
Pro Vision for caption generation, resulting in six encoder-decoder
configurations. The results show that mCLIP + Gemini Pro Vision achieved the
best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL
obtained the highest LLM-judge score (36.33%). This interpretable pipeline
enables culturally coherent and contextually accurate Arabic captions.

</details>


### [5] [Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes](https://arxiv.org/abs/2510.03297)
*Akshar Gothi*

Main category: cs.CV

TL;DR: CNN vs ViT on SpaceNet show EfficientNet-B0 is more efficient and robust under label imbalance, achieving 93% test accuracy with strong macro-F1, while ViT-Base is competitive but heavier. Under balanced data, both perform very well (EfficientNet-B0 99%), narrowing architecture gaps; CNNs retain efficiency edge.


<details>
  <summary>Details</summary>
Motivation: Assess the relative performance and deployment characteristics of a CNN (EfficientNet-B0) and a ViT (ViT-Base) on SpaceNet under two label-distribution regimes to understand trade-offs in accuracy, robustness to class imbalance, and resource efficiency.

Method: Two architectures (EfficientNet-B0 and ViT-Base) are evaluated with matched preprocessing (224x224, ImageNet normalization) and lightweight augmentations. A fixed 40-epoch budget is used on a single NVIDIA P100. Two label regimes are tested: (1) naturally imbalanced five-class split, and (2) balanced split with 700 images per class using a 70:20:10 train/val/test split. Metrics reported include accuracy, macro-F1, balanced accuracy, per-class recall, model size, and latency. Reproducibility artifacts (manifests, logs, per-image predictions) are released.

Result: Imbalanced split: EfficientNet-B0 achieves 93% test accuracy with strong macro-F1 and lower latency; ViT-Base is competitive at 93% but has higher parameter count and longer runtime. Balanced split: EfficientNet-B0 achieves 99% accuracy; ViT-Base remains competitive. Overall, balancing reduces architecture gap but CNN retains efficiency advantage.

Conclusion: EfficientNet-B0 provides a favorable balance of accuracy and efficiency, particularly under label imbalance, while ViT-Base can match performance at the cost of greater resources. When data are balanced, both architectures perform strongly, though CNNs maintain an efficiency edge. The authors supply data and logs to support reproducibility.

Abstract: We present a controlled comparison of a convolutional neural network
(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two
label-distribution regimes: a naturally imbalanced five-class split and a
balanced-resampled split with 700 images per class (70:20:10 train/val/test).
With matched preprocessing (224x224, ImageNet normalization), lightweight
augmentations, and a 40-epoch budget on a single NVIDIA P100, we report
accuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics
(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%
test accuracy with strong macro-F1 and lower latency; ViT-Base is competitive
at 93% with a larger parameter count and runtime. On the balanced split, both
models are strong; EfficientNet-B0 reaches 99% while ViT-Base remains
competitive, indicating that balancing narrows architecture gaps while CNNs
retain an efficiency edge. We release manifests, logs, and per-image
predictions to support reproducibility.

</details>


### [6] [A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety](https://arxiv.org/abs/2510.03314)
*Shucheng Zhang,Yan Shi,Bingzhang Wang,Yuang Zhang,Muhammad Monjurul Karim,Kehua Chen,Chenxi Liu,Mehrdad Nasri,Yinhai Wang*

Main category: cs.CV

TL;DR: A comprehensive five-year review of camera-based AI sensing for VRU safety, covering detection/classification, tracking/reidentification, trajectory prediction, and intent recognition, with open data/model/deployment challenges and guidance for real-world systems.


<details>
  <summary>Details</summary>
Motivation: VRUs (pedestrians, cyclists) face high risk in dynamic urban settings; infrastructure measures are often insufficient, and existing AI surveys focus mainly on detection. Camera-based vision offers proactive, context-aware protection but requires broader coverage of tasks beyond detection.

Method: Systematic literature review of recent works (past ~5 years) on camera-based AI sensing for VRU safety, organizing the literature around four core tasks and synthesizing trends, methods, datasets, and deployment considerations; followed by a discussion of open challenges.

Result: The survey presents a cohesive framework linking perception, tracking, prediction, and intent understanding for VRU safety, synthesizes progress across the four tasks, and identifies ongoing data-, model-, and deployment-related challenges with guidance for future research.

Conclusion: Camera-based AI sensing has strong potential to enhance VRU safety when perception is effectively integrated with proactive decision-making; realizing this potential requires addressing data diversity, robust modeling, and deployment constraints, and the paper offers a roadmap for next-generation sensing systems.

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, remains a critical global challenge, as conventional
infrastructure-based measures often prove inadequate in dynamic urban
environments. Recent advances in artificial intelligence (AI), particularly in
visual perception and reasoning, open new opportunities for proactive and
context-aware VRU protection. However, existing surveys on AI applications for
VRUs predominantly focus on detection, offering limited coverage of other
vision-based tasks that are essential for comprehensive VRU understanding and
protection. This paper presents a state-of-the-art review of recent progress in
camera-based AI sensing systems for VRU safety, with an emphasis on
developments from the past five years and emerging research trends. We
systematically examine four core tasks, namely detection and classification,
tracking and reidentification, trajectory prediction, and intent recognition
and prediction, which together form the backbone of AI-empowered proactive
solutions for VRU protection in intelligent transportation systems. To guide
future research, we highlight four major open challenges from the perspectives
of data, model, and deployment. By linking advances in visual AI with practical
considerations for real-world implementation, this survey aims to provide a
foundational reference for the development of next-generation sensing systems
to enhance VRU safety.

</details>


### [7] [The View From Space: Navigating Instrumentation Differences with EOFMs](https://arxiv.org/abs/2510.03316)
*Ryan P. Demilt,Nicholas LaHaye,Karis Tenneson*

Main category: cs.CV

TL;DR: EOFMs' representations are highly dependent on sensor architecture, complicating cross-modality use and highlighting design and application pitfalls; future work should be sensor-aware.


<details>
  <summary>Details</summary>
Motivation: There is a gap in understanding how diverse sensing modalities influence the internal representations of EOFMs, which impacts robustness and transferability in remote-sensing tasks.

Method: Comparative or analytical examination of EOFM representation spaces across different sensor architectures to assess sensitivity and implications for embeddings used in similarity search and queries.

Result: The representation space of EOFMs is highly sensitive to sensor architecture, revealing crucial pitfalls in current EOFM design and signaling directions for how to improve models and their use.

Conclusion: Future EOFM development should incorporate sensor-aware design principles, robust evaluation across modalities, and community practices to ensure reliable performance in earth observation tasks.

Abstract: Earth Observation Foundation Models (EOFMs) have exploded in prevalence as
tools for processing the massive volumes of remotely sensed and other earth
observation data, and for delivering impact on the many essential earth
monitoring tasks. An emerging trend posits using the outputs of pre-trained
models as 'embeddings' which summarize high dimensional data to be used for
generic tasks such as similarity search and content-specific queries. However,
most EOFM models are trained only on single modalities of data and then applied
or benchmarked by matching bands across different modalities. It is not clear
from existing work what impact diverse sensor architectures have on the
internal representations of the present suite of EOFMs. We show in this work
that the representation space of EOFMs is highly sensitive to sensor
architecture and that understanding this difference gives a vital perspective
on the pitfalls of current EOFM design and signals for how to move forward as
model developers, users, and a community guided by robust remote-sensing
science.

</details>


### [8] [Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring](https://arxiv.org/abs/2510.03317)
*Günel Aghakishiyeva,Jiayi Zhou,Saagar Arya,James David Poling,Holly R. Houliston,Jamie N. Womble,David W. Johnston,Brinnae Bent*

Main category: cs.CV

TL;DR: An inpainting-guided, perturbation-based explanation method for ecological vision models that preserves scene realism by producing photorealistic, mask-localized edits. Applied to a YOLOv9 harbor seal detector in Glacier Bay, using Segment-Anything-Model refined masks to enable object removal/replacement and background replacement, with explanations evaluated by re-scoring perturbed images and expert review; results yield localized, plausible, domain-relevant explanations to improve trust.


<details>
  <summary>Details</summary>
Motivation: Black-box predictions limit trust and field adoption in ecological monitoring. Explanations must be faithful, localized, and maintain ecological plausibility. The proposed inpainting-based perturbations aim to reveal fine-grained morphological cues driving predictions without creating out-of-distribution artifacts.

Method: Use an inpainting-guided perturbation framework that creates photorealistic, mask-localized edits. Leverage Segment-Anything-Model refined masks for two interventions: (i) object removal/replacement (e.g., seals replaced by ice/water or boats); (ii) background replacement with animals composited into new scenes. Evaluate explanations by re-scoring perturbed images (flip rate, confidence drop) and expert review for ecological plausibility.

Result: Edits remain in-distribution and avoid deletion artifacts typical of conventional perturbations. Explanations localize diagnostic structures and provide domain-relevant insights that align with ecological expertise, supporting validation and more trustworthy deployment of AI in ecology.

Conclusion: Photorealistic, in-distribution perturbations offer faithful, interpretable explanations that reveal fine-grained cues driving ecological predictions, enabling expert validation and facilitating broader adoption of AI-assisted ecology.

Abstract: Ecological monitoring is increasingly automated by vision models, yet opaque
predictions limit trust and field adoption. We present an inpainting-guided,
perturbation-based explanation technique that produces photorealistic,
mask-localized edits that preserve scene context. Unlike masking or blurring,
these edits stay in-distribution and reveal which fine-grained morphological
cues drive predictions in tasks such as species recognition and trait
attribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for
harbor seal detection in Glacier Bay drone imagery, using
Segment-Anything-Model-refined masks to support two interventions: (i) object
removal/replacement (e.g., replacing seals with plausible ice/water or boats)
and (ii) background replacement with original animals composited onto new
scenes. Explanations are assessed by re-scoring perturbed images (flip rate,
confidence drop) and by expert review for ecological plausibility and
interpretability. The resulting explanations localize diagnostic structures,
avoid deletion artifacts common to traditional perturbations, and yield
domain-relevant insights that support expert validation and more trustworthy
deployment of AI in ecology.

</details>


### [9] [Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications](https://arxiv.org/abs/2510.03318)
*Ahmed Kabil,Ghada Khoriba,Mina Yousef,Essam A. Rashed*

Main category: cs.CV

TL;DR: A broad, methodical survey of Medical Image Segmentation (MIS) spanning traditional image processing to modern DL, including CNNs/FCNs/U-Net, attention, semi-supervised and GANs, with Transformer-based trends, cross-modality and federated learning, plus a lumbar spine case study; it also highlights key challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: MIS is crucial for accurate diagnosis, treatment planning, and monitoring; there is a need to bridge classical techniques with modern DL, address data scarcity, and ensure generalizability and clinical translation.

Method: Systematic survey of MIS techniques across categories: thresholding, edge detection, region-based, clustering, model-based traditional methods; DL architectures (CNNs, FCNs, U-Net and variants); advanced topics (attention, semi-supervised learning, GANs, Transformers); discussion of emerging trends (hybrid models, cross-modality learning, federated/distributed learning, active learning); case study on lumbar spine; analysis of challenges and real-world deployment.

Result: Provides a comprehensive synthesis, taxonomy, and gap analysis; identifies trends and potential best practices; clarifies limitations and the state of the art; offers guidance for researchers and practitioners.

Conclusion: MIS has advanced significantly but faces persistent issues (dataset bias, domain adaptation, interpretability, deployment in clinics). Future work should emphasize robustness, data efficiency, explainability, and integration with workflows, leveraging hybrid models, privacy-preserving learning, and cross-modality generalization.

Abstract: Medical Image Segmentation (MIS) stands as a cornerstone in medical image
analysis, playing a pivotal role in precise diagnostics, treatment planning,
and monitoring of various medical conditions. This paper presents a
comprehensive and systematic survey of MIS methodologies, bridging the gap
between traditional image processing techniques and modern deep learning
approaches. The survey encompasses thresholding, edge detection, region-based
segmentation, clustering algorithms, and model-based techniques while also
delving into state-of-the-art deep learning architectures such as Convolutional
Neural Networks (CNNs), Fully Convolutional Networks (FCNs), and the widely
adopted U-Net and its variants. Moreover, integrating attention mechanisms,
semi-supervised learning, generative adversarial networks (GANs), and
Transformer-based models is thoroughly explored. In addition to covering
established methods, this survey highlights emerging trends, including hybrid
architectures, cross-modality learning, federated and distributed learning
frameworks, and active learning strategies, which aim to address challenges
such as limited labeled datasets, computational complexity, and model
generalizability across diverse imaging modalities. Furthermore, a specialized
case study on lumbar spine segmentation is presented, offering insights into
the challenges and advancements in this relatively underexplored anatomical
region. Despite significant progress in the field, critical challenges persist,
including dataset bias, domain adaptation, interpretability of deep learning
models, and integration into real-world clinical workflows.

</details>


### [10] [DECOR: Deep Embedding Clustering with Orientation Robustness](https://arxiv.org/abs/2510.03328)
*Fiona Victoria Stanley Jothiraj,Arunaggiri Pandian Karunanidhi,Seth A. Eichmeyer*

Main category: cs.CV

TL;DR: DECOR is a deep clustering method that achieves orientation-robust clustering of wafer defect maps under unlabeled, imbalanced data, outperforming baselines on MixedWM38.


<details>
  <summary>Details</summary>
Motivation: Early defect detection is crucial for yield optimization. Wafer data are often unlabeled, imbalanced, and may contain multiple defects per wafer. Orientation variations further complicate clustering, necessitating a robust, tuning-free clustering approach.

Method: Introduces DECOR, a deep clustering framework with explicit orientation robustness to cluster complex wafer-map defect patterns. It accounts for rotation/alignment invariance to group spatially similar defects consistently and requires no manual tuning on the MixedWM38 dataset.

Result: DECOR outperforms existing clustering baselines and can discover meaningful clusters without manual tuning, demonstrating reliability and scalability for automated visual inspection systems.

Conclusion: Orientation-robust deep clustering is effective for wafer defect analysis, enabling more reliable, scalable automated visual inspection and contributing to improved yield through better defect pattern understanding.

Abstract: In semiconductor manufacturing, early detection of wafer defects is critical
for product yield optimization. However, raw wafer data from wafer quality
tests are often complex, unlabeled, imbalanced and can contain multiple defects
on a single wafer, making it crucial to design clustering methods that remain
reliable under such imperfect data conditions. We introduce DECOR, a deep
clustering with orientation robustness framework that groups complex defect
patterns from wafer maps into consistent clusters. We evaluate our method on
the open source MixedWM38 dataset, demonstrating its ability to discover
clusters without manual tuning. DECOR explicitly accounts for orientation
variations in wafer maps, ensuring that spatially similar defects are
consistently clustered regardless of its rotation or alignment. Experiments
indicate that our method outperforms existing clustering baseline methods, thus
providing a reliable and scalable solution in automated visual inspection
systems.

</details>


### [11] [Error correction in multiclass image classification of facial emotion on unbalanced samples](https://arxiv.org/abs/2510.03337)
*Andrey A. Lebedev,Victor B. Kazantsev,Sergey V. Stasenko*

Main category: cs.CV

TL;DR: LSTM with attention can mitigate class-imbalance in facial emotion recognition by training on six-class subsets and performing error correction for the held-out seventh class, achieving feasible restoration across classes with varying success and potential boosts for rare classes.


<details>
  <summary>Details</summary>
Motivation: Address robust emotion recognition under skewed class distributions and enable recovery of rare/emerging emotions through error correction, with potential applications in anti-fraud and other rare-event detection tasks.

Method: A neural network model based on LSTM with an attention mechanism focusing on informative facial regions. The dataset contains images labeled with seven emotions across different ages. The experiment trains on all six-class configurations and tests by correcting the excluded seventh class, assessing per-class performance and overall metrics.

Result: Error correction is possible for all seven emotions, though with varying effectiveness. Some rare classes show improved metrics on the test set when corrected, indicating promise for handling imbalanced data and rare-event detection.

Conclusion: The proposed six-vs-seven-class correction approach is effective for emotion recognition under skewed distributions and can be applied to facial expression analysis systems and similar tasks requiring stable performance on imbalanced datasets; further refinements could enhance minority-class restoration.

Abstract: This paper considers the problem of error correction in multi-class
classification of face images on unbalanced samples. The study is based on the
analysis of a data frame containing images labeled by seven different emotional
states of people of different ages. Particular attention is paid to the problem
of class imbalance, in which some emotions significantly prevail over others.
To solve the classification problem, a neural network model based on LSTM with
an attention mechanism focusing on key areas of the face that are informative
for emotion recognition is used. As part of the experiments, the model is
trained on all possible configurations of subsets of six classes with
subsequent error correction for the seventh class, excluded at the training
stage. The results show that correction is possible for all classes, although
the degree of success varies: some classes are better restored, others are
worse. In addition, on the test sample, when correcting some classes, an
increase in key quality metrics for small classes was recorded, which indicates
the promise of the proposed approach in solving applied problems related to the
search for rare events, for example, in anti-fraud systems. Thus, the proposed
method can be effectively applied in facial expression analysis systems and in
tasks requiring stable classification under skewed class distribution.

</details>


### [12] [OpusAnimation: Code-Based Dynamic Chart Generation](https://arxiv.org/abs/2510.03341)
*Bozheng Li,Miao Yang,Zhenhan Chen,Jiawang Cao,Mushui Liu,Yi Lu,Yongliang Wu,Bin Zhang,Yangguang Ji,Licheng Tang,Jay Wu,Wenbo Zhu*

Main category: cs.CV

TL;DR: DCG-Bench is a new benchmark and dataset for dynamic chart generation with a two-stage training method; it shows improvements over open-source MLLMs and competitive with larger proprietary models using a 3B expert model.


<details>
  <summary>Details</summary>
Motivation: Dynamic chart generation is underexplored; need benchmarks and datasets to evaluate MLLMs' capabilities in converting text/video to animated charts.

Method: Construct DCG-8K with instruction-code-video triplets and QA; three tasks; two-stage training with Joint-Code-Visual Reward for group-relative policy optimization to create Qwen2.5-VL-DCG-3B.

Result: 8.31% average gain over best open-source MLLM across three tasks; parity with proprietary models with only 3B params; highlights shortcomings in visual-to-chart tasks; demonstrates efficacy of training recipe.

Conclusion: DCG-8K and DCG-Bench will be public; represents progress in dynamic chart generation; potential foundation for future research.

Abstract: Dynamic Chart Generation (DCG) involves producing code-rendered animated
visualizations as charts. While recent advances in multi-modal large language
models (MLLMs) have significantly improved their capability on static chart
generation and comprehension, MLLMs' potential for handling dynamic chart
generation and understanding remains underexplored. To bridge this research
gap, we introduce DCG-Bench (Dynamic Chart Generation Benchmark), the first
benchmark evaluating MLLM's capability on dynamic chart generation tasks from
three dimensions: Simple Text-to-Chart, Detailed Text-to-Chart, and
Video-to-Chart tasks. We construct DCG-8K, a high-quality DCG dataset with
annotations covering instruction-code-video triplets and QA pairs for both code
and video evaluation. Based on DCG-8K, we explored a two-stage training recipe,
proposing Joint-Code-Visual Reward for group relative policy optimization to
construct expert MLLM Qwen2.5-VL-DCG-3B for the DCG task. Our benchmarking
result reveals shortcomings of existing MLLMs in the visual-to-chart task, and
our model beats the best open-sourced MLLM with an average 8.31% performance
gain across three tasks, and shows on par performance against proprietary
models with only 3B parameters, proving the effectiveness of our training
recipe. Our code and dataset will be publicly available.

</details>


### [13] [Visual Odometry with Transformers](https://arxiv.org/abs/2510.03348)
*Vlardimir Yugay,Duy-Kien Nguyen,Theo Gevers,Cees G. M. Snoek,Martin R. Oswald*

Main category: cs.CV

TL;DR: VoT (Visual Odometry Transformer) offers end-to-end monocular visual odometry by directly predicting camera motion from frame sequences using temporal and spatial attention, removing hand-crafted steps (bundle adjustment, feature matching, calibration, dense 3D reconstruction) and relying on pre-trained encoders. It achieves faster, more generalizable pose estimation and scales with more data.


<details>
  <summary>Details</summary>
Motivation: Conventional monocular VO pipelines rely heavily on calibrated cameras, hand-engineered components, and hyperparameters, which hamper generalization to unseen real-world scenarios. Large multi-modal 3D models help with dense reconstruction but struggle with long videos and per-frame accuracy. An end-to-end, transformer-based VO could leverage learned priors to improve robustness and efficiency.

Method: VoT processes monocular frame sequences by extracting features and modeling global relations via temporal and spatial attention, and directly predicts camera motion without estimating dense geometry. It uses a modular design that allows plugging in various pre-trained encoders as feature extractors and relies solely on camera pose supervision.

Result: Experimental results show that VoT scales with larger datasets and benefits substantially from stronger pre-trained backbones. It generalizes across diverse camera motions and calibration settings and outperforms traditional VO methods while running more than 3× faster.

Conclusion: VoT demonstrates the viability of end-to-end transformer-based monocular VO, eliminating many traditional pipeline components, improving generalization and efficiency, and enabling seamless integration with pre-trained encoders. The approach suggests a path toward robust, real-time VO without dense geometry or calibration-dependent optimization.

Abstract: Modern monocular visual odometry methods typically combine pre-trained deep
learning components with optimization modules, resulting in complex pipelines
that rely heavily on camera calibration and hyperparameter tuning, and often
struggle in unseen real-world scenarios. Recent large-scale 3D models trained
on massive amounts of multi-modal data have partially alleviated these
challenges, providing generalizable dense reconstruction and camera pose
estimation. Still, they remain limited in handling long videos and providing
accurate per-frame estimates, which are required for visual odometry. In this
work, we demonstrate that monocular visual odometry can be addressed
effectively in an end-to-end manner, thereby eliminating the need for
handcrafted components such as bundle adjustment, feature matching, camera
calibration, or dense 3D reconstruction. We introduce VoT, short for Visual
odometry Transformer, which processes sequences of monocular frames by
extracting features and modeling global relationships through temporal and
spatial attention. Unlike prior methods, VoT directly predicts camera motion
without estimating dense geometry and relies solely on camera poses for
supervision. The framework is modular and flexible, allowing seamless
integration of various pre-trained encoders as feature extractors. Experimental
results demonstrate that VoT scales effectively with larger datasets, benefits
substantially from stronger pre-trained backbones, generalizes across diverse
camera motions and calibration settings, and outperforms traditional methods
while running more than 3 times faster. The code will be released.

</details>


### [14] [Inference-Time Search using Side Information for Diffusion-based Image Reconstruction](https://arxiv.org/abs/2510.03352)
*Mahdi Farahbakhsh,Vishnu Teja Kunde,Dileep Kalathil,Krishna Narayanan,Jean-Francois Chamberland*

Main category: cs.CV

TL;DR: A diffusion-based reconstruction method that uses an inference-time side-information search to guide sampling, improving performance on ill-posed inverse problems and outperforming reward-gradient baselines; code available.


<details>
  <summary>Details</summary>
Motivation: To enhance diffusion-model priors for inverse problems by incorporating side information during inference, addressing issues with gradient-guided approaches (reward hacking) and exploiting information-rich, ill-posed settings such as box inpainting, super-resolution, and diverse deblurring tasks.

Method: Introduce a novel inference-time search algorithm that balances exploration and exploitation to guide diffusion sampling using side information; can be integrated into existing diffusion recon pipelines; avoids relying solely on gradient-based guidance.

Result: Consistent improvements in qualitative and quantitative metrics across tasks (box inpainting, SR, motion/deblurring types, blind deblurring); outperforms baselines including reward-gradient-based guidance; demonstrates reliability and generality.

Conclusion: Side-information-guided inference is effective and generalizable for diffusion-based reconstruction, offering better reconstructions in severely ill-posed scenarios and reducing reward-hacking risk; easily integrable with existing workflows; code available.

Abstract: Diffusion models have emerged as powerful priors for solving inverse
problems. However, existing approaches typically overlook side information that
could significantly improve reconstruction quality, especially in severely
ill-posed settings. In this work, we propose a novel inference-time search
algorithm that guides the sampling process using the side information in a
manner that balances exploration and exploitation. This enables more accurate
and reliable reconstructions, providing an alternative to the gradient-based
guidance that is prone to reward-hacking artifacts. Our approach can be
seamlessly integrated into a wide range of existing diffusion-based image
reconstruction pipelines. Through extensive experiments on a number of inverse
problems, such as box inpainting, super-resolution, and various deblurring
tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that
our approach consistently improves the qualitative and quantitative performance
of diffusion-based image reconstruction algorithms. We also show the superior
performance of our approach with respect to other baselines, including reward
gradient-based guidance algorithms. The code is available at
\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this
repository}.

</details>


### [15] [Sonar Image Datasets: A Comprehensive Survey of Resources, Challenges, and Applications](https://arxiv.org/abs/2510.03353)
*Larissa S. Gomes,Gustavo P. Almeida,Bryan U. Moreira,Marco Quiroz,Breno Xavier,Lucas Soares,Stephanie L. Brião,Felipe G. Oliveira,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: A concise survey and roadmap of publicly available sonar image datasets across modalities (SSS, FLS, SAS, MBES, DIDSON), cataloging sizes, annotations, and tasks (classification, detection, segmentation, 3D reconstruction) to guide researchers and identify gaps.


<details>
  <summary>Details</summary>
Motivation: Data scarcity and fragmentation slow progress in sonar-based ML; a consolidated resource helps researchers locate datasets, compare characteristics, and plan future work.

Method: Review publicly accessible sonar datasets, categorize by modality, extract dataset characteristics (size, annotations, tasks), and present a master table and timeline to compare resources and track new releases.

Result: A synthesized overview and master table/timeline of datasets, detailing characteristics, sizes, and annotation details across sonar modalities; highlights current landscape and newly released datasets.

Conclusion: Provides a practical base guide for researchers to start or advance underwater acoustic data analysis, and to identify gaps and a roadmap for future dataset development and standardization.

Abstract: Sonar images are relevant for advancing underwater exploration, autonomous
navigation, and ecosystem monitoring. However, the progress depends on data
availability. The scarcity of publicly available, well-annotated sonar image
datasets creates a significant bottleneck for the development of robust machine
learning models. This paper presents a comprehensive and concise review of the
current landscape of sonar image datasets, seeking not only to catalog existing
resources but also to contextualize them, identify gaps, and provide a clear
roadmap, serving as a base guide for researchers of any kind who wish to start
or advance in the field of underwater acoustic data analysis. We mapped
publicly accessible datasets across various sonar modalities, including Side
Scan Sonar (SSS), Forward-Looking Sonar (FLS), Synthetic Aperture Sonar (SAS),
Multibeam Echo Sounder (MBES), and Dual-Frequency Identification Sonar
(DIDSON). An analysis was conducted on applications such as classification,
detection, segmentation, and 3D reconstruction. This work focuses on
state-of-the-art advancements, incorporating newly released datasets. The
findings are synthesized into a master table and a chronological timeline,
offering a clear and accessible comparison of characteristics, sizes, and
annotation details datasets.

</details>


### [16] [Learned Display Radiance Fields with Lensless Cameras](https://arxiv.org/abs/2510.03356)
*Ziyang Chen,Yuta Itoh,Kaan Akşit*

Main category: cs.CV

TL;DR: A lensless camera plus an implicit neural representation method enables reconstruction of display light fields from multiple viewpoints, enabling headset-free, hardware-light display calibration.


<details>
  <summary>Details</summary>
Motivation: Display calibration is essential for optimal visuals but is hindered by the need for specialized equipment and a dark room, limiting accessibility for most users.

Method: Co-design of a lensless camera with an Implicit Neural Representation (INR) based algorithm to capture and reconstruct display characteristics from various viewpoints, yielding efficient reconstruction of light fields within a viewing cone of 46.6° × 37.6°.

Result: The pipeline enables efficient reconstruction of light fields emitted from a display from the specified viewing cone, representing a practical step toward easy display calibration and characterization.

Conclusion: This work lays the groundwork for effortless display calibration by reducing hardware constraints and providing a scalable pipeline for viewpoint-aware display characterization.

Abstract: Calibrating displays is a basic and regular task that content creators must
perform to maintain optimal visual experience, yet it remains a troublesome
issue. Measuring display characteristics from different viewpoints often
requires specialized equipment and a dark room, making it inaccessible to most
users. To avoid specialized hardware requirements in display calibrations, our
work co-designs a lensless camera and an Implicit Neural Representation based
algorithm for capturing display characteristics from various viewpoints. More
specifically, our pipeline enables efficient reconstruction of light fields
emitted from a display from a viewing cone of 46.6{\deg} X 37.6{\deg}. Our
emerging pipeline paves the initial steps towards effortless display
calibration and characterization.

</details>


### [17] [Provenance Networks: End-to-End Exemplar-Based Explainability](https://arxiv.org/abs/2510.03361)
*Ali Kayyam,Anusha Madan Gopal,M. Anthony Lewis*

Main category: cs.CV

TL;DR: End-to-end trainable explainability via provenance networks that link predictions to training exemplars, functioning like a learned KNN; provides data-driven interpretability with trade-offs in compute and scaling.


<details>
  <summary>Details</summary>
Motivation: Integrate interpretability into the model architecture to address opaqueness, hallucination, and assignment of credit to data contributors; enable verification of whether inputs were in the training data; study memorization vs generalization; detect mislabeled/anomalous data and improve robustness.

Method: A neural model architecture where each output is explained by concrete training exemplars weighted by relevance in feature space, effectively a learned KNN. The model is trained jointly on the primary task and an explainability objective, embedding interpretability into the model.

Result: Claims include tracing predictions to supporting exemplars, assessing memorization vs. generalization, detecting training-data inclusion and mislabeled data, improved robustness to perturbations, and identification of similar inputs contributing to a new point; however, it adds computational cost and currently scales to moderately sized datasets.

Conclusion: Provenance networks offer a complementary, architecture-driven approach to explainability that addresses opacity and hallucination while enhancing transparency and trust, at the cost of increased computation and limited scalability.

Abstract: We introduce provenance networks, a novel class of neural models designed to
provide end-to-end, training-data-driven explainability. Unlike conventional
post-hoc methods, provenance networks learn to link each prediction directly to
its supporting training examples as part of the model's normal operation,
embedding interpretability into the architecture itself. Conceptually, the
model operates similarly to a learned KNN, where each output is justified by
concrete exemplars weighted by relevance in the feature space. This approach
facilitates systematic investigations of the trade-off between memorization and
generalization, enables verification of whether a given input was included in
the training set, aids in the detection of mislabeled or anomalous data points,
enhances resilience to input perturbations, and supports the identification of
similar inputs contributing to the generation of a new data point. By jointly
optimizing the primary task and the explainability objective, provenance
networks offer insights into model behavior that traditional deep networks
cannot provide. While the model introduces additional computational cost and
currently scales to moderately sized datasets, it provides a complementary
approach to existing explainability techniques. In particular, it addresses
critical challenges in modern deep learning, including model opaqueness,
hallucination, and the assignment of credit to data contributors, thereby
improving transparency, robustness, and trustworthiness in neural models.

</details>


### [18] [Unified Unsupervised Anomaly Detection via Matching Cost Filtering](https://arxiv.org/abs/2510.03363)
*Zhe Zhang,Mingxiu Cai,Gaochang Wu,Jing Zhang,Lingqiao Liu,Dacheng Tao,Tianyou Chai,Xiatian Zhu*

Main category: cs.CV

TL;DR: Unified Cost Filtering (UCF) refines anomaly cost volumes in unsupervised anomaly detection (UAD) by post-hoc, learnable filtering with multi-layer attention, improving both unimodal RGB and multimodal (RGB-3D, RGB-Text) anomaly localization.


<details>
  <summary>Details</summary>
Motivation: Current UAD methods rely on reconstruction or embedding-based matching, but matching noise limits detection, and multimodal UAD remains underexplored and isolated. A unified post-hoc refinement can transfer benefits across modalities.

Method: Construct an anomaly cost volume by matching a test sample against normal samples from the same or different modalities. Apply a learnable filtering module with multi-layer attention guided by the test sample to suppress matching noise and highlight subtle anomalies.

Result: Experiments on 22 benchmarks show consistent improvements, achieving state-of-the-art results in both unimodal (RGB) and multimodal (RGB-3D, RGB-Text) UAD settings.

Conclusion: UCF provides a generic, post-hoc refinement framework that can enhance a wide range of UAD models across unimodal and multimodal scenarios by mitigating matching noise and better highlighting anomalies.

Abstract: Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level
anomalies using only normal training data, with wide applications such as
industrial inspection and medical analysis, where anomalies are scarce due to
privacy concerns and cold-start constraints. Existing methods, whether
reconstruction-based (restoring normal counterparts) or embedding-based
(pretrained representations), fundamentally conduct image- or feature-level
matching to generate anomaly maps. Nonetheless, matching noise has been largely
overlooked, limiting their detection ability. Beyond earlier focus on unimodal
RGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D
and RGB--Text, enabled by point cloud sensing and vision--language models.
Despite shared challenges, these lines remain largely isolated, hindering a
comprehensive understanding and knowledge transfer. In this paper, we advocate
unified UAD for both unimodal and multimodal settings in the matching
perspective. Under this insight, we present Unified Cost Filtering (UCF), a
generic post-hoc refinement framework for refining anomaly cost volume of any
UAD model. The cost volume is constructed by matching a test sample against
normal samples from the same or different modalities, followed by a learnable
filtering module with multi-layer attention guidance from the test sample,
mitigating matching noise and highlighting subtle anomalies. Comprehensive
experiments on 22 diverse benchmarks demonstrate the efficacy of UCF in
enhancing a variety of UAD methods, consistently achieving new state-of-the-art
results in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD
scenarios. Code and models will be released at
https://github.com/ZHE-SAPI/CostFilter-AD.

</details>


### [19] [Visual Language Model as a Judge for Object Detection in Industrial Diagrams](https://arxiv.org/abs/2510.03376)
*Sanjukta Ghosh*

Main category: cs.CV

TL;DR: A Visual Language Model (VLM)-based framework evaluates and refines object detection outputs on industrial diagrams (P&IDs), enabling automated quality assessment and improved detection performance.


<details>
  <summary>Details</summary>
Motivation: There is a gap in automatic evaluation of object detection results for complex industrial diagrams, hindering reliable digital twins and automated industrial automation.

Method: Leverage the multimodal reasoning capabilities of Visual Language Models to identify missing or inconsistent detections in P&ID diagrams and guide automated refinement, effectively creating a QA loop that enhances detection accuracy.

Result: The framework demonstrates potential to automatically assess detection quality and improve overall detection performance on complex industrial diagrams by exploiting VLMs’ multimodal capabilities.

Conclusion: Integrating VLM-based evaluation into the detection pipeline can enhance the reliability and efficiency of digitalization efforts for industrial plants.

Abstract: Industrial diagrams such as piping and instrumentation diagrams (P&IDs) are
essential for the design, operation, and maintenance of industrial plants.
Converting these diagrams into digital form is an important step toward
building digital twins and enabling intelligent industrial automation. A
central challenge in this digitalization process is accurate object detection.
Although recent advances have significantly improved object detection
algorithms, there remains a lack of methods to automatically evaluate the
quality of their outputs. This paper addresses this gap by introducing a
framework that employs Visual Language Models (VLMs) to assess object detection
results and guide their refinement. The approach exploits the multimodal
capabilities of VLMs to identify missing or inconsistent detections, thereby
enabling automated quality assessment and improving overall detection
performance on complex industrial diagrams.

</details>


### [20] [Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning](https://arxiv.org/abs/2510.03441)
*Chashi Mahiul Islam,Oteo Mamo,Samuel Jacob Chacko,Xiuwen Liu,Weikuan Yu*

Main category: cs.CV

TL;DR: SpatialViLT enhances vision-language models with explicit spatial cues (depth, 3D coordinates, and edge maps) via a multi-task framework. Two variants—SpatialViLT and MaskedSpatialViLT—address full and masked object regions, and SpatialEnsemble combines both to achieve state-of-the-art spatial reasoning on the Visual Spatial Reasoning (VSR) dataset, excelling in directional, topological, and proximity relations.


<details>
  <summary>Details</summary>
Motivation: Vision-language models still struggle with spatial reasoning in 3D scenes and complex object configurations. Incorporating explicit spatial features can enrich multimodal embeddings and improve understanding of spatial relations.

Method: Introduce SpatialViLT with multi-task learning that fuses spatial features (depth maps, 3D coordinates, edge maps) into VLM embeddings. Propose two variants: SpatialViLT (full object regions) and MaskedSpatialViLT (masked regions). SpatialEnsemble combines both approaches. Evaluate on the Visual Spatial Reasoning (VSR) dataset across directional, topological, and proximity relations.

Result: The approach achieves state-of-the-art accuracy on VSR, with strong performance in spatial reasoning categories (directional, topological, proximity). Demonstrates that integrating spatial cues into VLMs yields improved spatial understanding in 3D scenes and complex configurations.

Conclusion: This work significantly advances spatial intelligence in multimodal AI, enhancing spatial reasoning capabilities for real-world applications and more robust multimodal understanding.

Abstract: Vision-language models (VLMs) have advanced multimodal reasoning but still
face challenges in spatial reasoning for 3D scenes and complex object
configurations. To address this, we introduce SpatialViLT, an enhanced VLM that
integrates spatial features like depth maps, 3D coordinates, and edge maps
through a multi-task learning framework. This approach enriches multimodal
embeddings with spatial understanding. We propose two variants: SpatialViLT and
MaskedSpatialViLT, focusing on full and masked object regions, respectively.
Additionally, SpatialEnsemble combines both approaches, achieving
state-of-the-art accuracy. Our models excel in spatial reasoning categories
such as directional, topological, and proximity relations, as demonstrated on
the challenging Visual Spatial Reasoning (VSR) dataset. This work represents a
significant step in enhancing the spatial intelligence of AI systems, crucial
for advanced multimodal understanding and real-world applications.

</details>


### [21] [Denoising of Two-Phase Optically Sectioned Structured Illumination Reconstructions Using Encoder-Decoder Networks](https://arxiv.org/abs/2510.03452)
*Allison Davis,Yezhi Shen,Xiaoyu Ji,Fengqing Zhu*

Main category: cs.CV

TL;DR: Synthetic training for two-phase OS-SI artifact denoising using encoder-decoder networks trained on artifact-augmented synthetic data; shows improved image quality and artifact-type–dependent strengths.


<details>
  <summary>Details</summary>
Motivation: Address residual artifacts in two-phase optical-sectioning SI due to reduced acquisition time, where conventional denoising fails and clean ground-truth data are unavailable.

Method: Create synthetic data by applying real artifact fields to synthetic images; train an asymmetrical denoising autoencoder (DAE) and a U-Net; evaluate on real OS-SI images.

Result: Both networks improve image clarity; each excels for different artifact types.

Conclusion: Synthetic training enables supervised denoising of OS-SI images and demonstrates the potential of encoder-decoder networks to streamline reconstruction workflows.

Abstract: Structured illumination (SI) enhances image resolution and contrast by
projecting patterned light onto a sample. In two-phase optical-sectioning SI
(OS-SI), reduced acquisition time introduces residual artifacts that
conventional denoising struggles to suppress. Deep learning offers an
alternative to traditional methods; however, supervised training is limited by
the lack of clean, optically sectioned ground-truth data. We investigate
encoder-decoder networks for artifact reduction in two-phase OS-SI, using
synthetic training pairs formed by applying real artifact fields to synthetic
images. An asymmetrical denoising autoencoder (DAE) and a U-Net are trained on
the synthetic data, then evaluated on real OS-SI images. Both networks improve
image clarity, with each excelling against different artifact types. These
results demonstrate that synthetic training enables supervised denoising of
OS-SI images and highlight the potential of encoder-decoder networks to
streamline reconstruction workflows.

</details>


### [22] [PEaRL: Pathway-Enhanced Representation Learning for Gene and Pathway Expression Prediction from Histology](https://arxiv.org/abs/2510.03455)
*Sejuti Majumder,Saarthak Kapse,Moinak Bhattacharya,Xuan Xu,Alisa Yurovsky,Prateek Prasanna*

Main category: cs.CV

TL;DR: PEaRL uses ssGSEA-derived pathway scores, a transformer encoder, and contrastive alignment with histology to integrate transcriptomics with spatial data; it outperforms state-of-the-art methods in three cancer ST datasets, improving gene- and pathway-level expression prediction.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal histology–transcriptomics approaches rely on a small set of highly variable genes, limiting predictive scope and interpretability. There is a need to incorporate coordinated biological programs (pathways) to better capture tissue phenotypes and provide interpretable models.

Method: Compute pathway activation scores from transcriptomics using ssGSEA, encode these biologically coherent signals with a transformer, and align them with histology features via contrastive learning to achieve better cross-modal correspondence and reduced dimensionality.

Result: On three cancer spatial transcriptomics datasets (breast, skin, lymph node), PEaRL outperforms state-of-the-art methods, with up to 58.9% higher Pearson correlation for gene-level expression prediction and up to 20.4% for pathway-level expression prediction.

Conclusion: Grounding transcriptomic representations in pathways yields more biologically faithful and interpretable multimodal models, strengthening cross-modal alignment and advancing computational pathology beyond gene-level embeddings.

Abstract: Integrating histopathology with spatial transcriptomics (ST) provides a
powerful opportunity to link tissue morphology with molecular function. Yet
most existing multimodal approaches rely on a small set of highly variable
genes, which limits predictive scope and overlooks the coordinated biological
programs that shape tissue phenotypes. We present PEaRL (Pathway Enhanced
Representation Learning), a multimodal framework that represents
transcriptomics through pathway activation scores computed with ssGSEA. By
encoding biologically coherent pathway signals with a transformer and aligning
them with histology features via contrastive learning, PEaRL reduces
dimensionality, improves interpretability, and strengthens cross-modal
correspondence. Across three cancer ST datasets (breast, skin, and lymph node),
PEaRL consistently outperforms SOTA methods, yielding higher accuracy for both
gene- and pathway-level expression prediction (up to 58.9 percent and 20.4
percent increase in Pearson correlation coefficient compared to SOTA). These
results demonstrate that grounding transcriptomic representation in pathways
produces more biologically faithful and interpretable multimodal models,
advancing computational pathology beyond gene-level embeddings.

</details>


### [23] [DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis](https://arxiv.org/abs/2510.03483)
*Numan Saeed,Tausifa Jan Saleem,Fadillah Maani,Muhammad Ridzuan,Hu Wang,Mohammad Yaqub*

Main category: cs.CV

TL;DR: DuPLUS is a universal, text-conditioned framework for multi-modal medical image analysis. It uses hierarchical prompts and a dual-prompt mechanism to generalize across modalities and datasets, integrate EHR for prognosis, and achieve state-of-the-art results on most tasks with efficient fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current deep-learning medical imaging models are either task-specific with limited generalization or universal models with coarse conditioning and weak medical semantic understanding. There is a need for a flexible, semantically aware, extensible framework that can handle multiple modalities, tasks, and prognostic data.

Method: DuPLUS introduces a vision-language framework with hierarchical semantic prompts for fine-grained task control and a dual-prompt mechanism enabling extensibility. It supports segmentation across three imaging modalities and 10 datasets (30+ organs/tumor types). It enables parameter-efficient fine-tuning and seamless integration of EHR data for prognosis prediction, demonstrated on a head-and-neck cancer dataset with CI=0.69.

Result: DuPLUS generalizes across 3 imaging modalities and 10 anatomical datasets for segmentation, outperforming state-of-the-art task-specific and universal models on 8 of 10 datasets. It achieves CI=0.69 on a head-and-neck cancer prognosis dataset. Code is released for reuse and extension.

Conclusion: DuPLUS offers a versatile, clinically relevant solution for medical image analysis, combining broad generalization, nuanced semantic control, and extensibility to prognosis through EHR data. Its modular design and efficient fine-tuning support rapid adaptation to new tasks and centers.

Abstract: Deep learning for medical imaging is hampered by task-specific models that
lack generalizability and prognostic capabilities, while existing 'universal'
approaches suffer from simplistic conditioning and poor medical semantic
understanding. To address these limitations, we introduce DuPLUS, a deep
learning framework for efficient multi-modal medical image analysis. DuPLUS
introduces a novel vision-language framework that leverages hierarchical
semantic prompts for fine-grained control over the analysis task, a capability
absent in prior universal models. To enable extensibility to other medical
tasks, it includes a hierarchical, text-controlled architecture driven by a
unique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize
across three imaging modalities, ten different anatomically various medical
datasets, encompassing more than 30 organs and tumor types. It outperforms the
state-of-the-art task specific and universal models on 8 out of 10 datasets. We
demonstrate extensibility of its text-controlled architecture by seamless
integration of electronic health record (EHR) data for prognosis prediction,
and on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)
of 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks
and modalities from varying centers, establishing DuPLUS as a versatile and
clinically relevant solution for medical image analysis. The code for this work
is made available at: https://anonymous.4open.science/r/DuPLUS-6C52

</details>


### [24] [Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms](https://arxiv.org/abs/2510.03501)
*Lyes Saad Saoud,Loic Lesobre,Enrico Sorato,Irfan Hussain*

Main category: cs.CV

TL;DR: A mobile-optimized two-stage framework threads YOLOv10-based detection with MobileSAM segmentation (TDM) to enable real-time wildlife detection and segmentation in resource-constrained natural environments; achieves strong accuracy and real-time latency on a cryptic species dataset; code and dataset released.


<details>
  <summary>Details</summary>
Motivation: To enable real-time, non-invasive wildlife monitoring in natural environments under limited computational resources, addressing cryptic appearances and latency in detection and segmentation.

Method: Introduce Threading Detection Model (TDM) that runs YOLOv10-based detection and MobileSAM-based segmentation in parallel (threaded two-stage pipeline), optimizing for mobile/edge devices; evaluation on a curated Houbara Bustard dataset (40k images) with real-time inference (~43.7 ms/frame).

Result: mAP50 0.9627, mAP75 0.7731, mAP95 0.7178; MobileSAM mIoU 0.7421; YOLOv10 latency 43.7 ms/frame; dataset and code publicly available on GitHub; demonstrates real-time performance with high accuracy on a conservation-priority species.

Conclusion: The threaded YOLO-SAM approach effectively enables real-time animal detection and segmentation with high accuracy on real-world wildlife data, contributing to conservation monitoring and offering reproducible resources (dataset and code).

Abstract: Real-time animal detection and segmentation in natural environments are vital
for wildlife conservation, enabling non-invasive monitoring through remote
camera streams. However, these tasks remain challenging due to limited
computational resources and the cryptic appearance of many species. We propose
a mobile-optimized two-stage deep learning framework that integrates a
Threading Detection Model (TDM) to parallelize YOLOv10-based detection and
MobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach
improves real-time performance by reducing latency through threading. YOLOv10
handles detection while MobileSAM performs lightweight segmentation, both
executed concurrently for efficient resource use. On the cryptic Houbara
Bustard, a conservation-priority species, our model achieves mAP50 of 0.9627,
mAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10
operates at 43.7 ms per frame, confirming real-time readiness. We introduce a
curated Houbara dataset of 40,000 annotated images to support model training
and evaluation across diverse conditions. The code and dataset used in this
study are publicly available on GitHub at
https://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos
and additional resources, visit
https://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.

</details>


### [25] [Platonic Transformers: A Solid Choice For Equivariance](https://arxiv.org/abs/2510.03511)
*Mohammad Mohaiminul Islam,Rishabh Anand,David R. Wessels,Friso de Kruiff,Thijs P. Kuipers,Rex Ying,Clara I. Sánchez,Sharvaree Vadgama,Georg Bökman,Erik J. Bekkers*

Main category: cs.CV

TL;DR: Introduces Platonic Transformer that injects geometric priors from Platonic symmetry groups into Transformer attention, achieving equivariance to translations and Platonic symmetries without extra cost, and equivalent to dynamic group convolution for adaptive filters; validated on vision, 3D point clouds, and molecular tasks.


<details>
  <summary>Details</summary>
Motivation: Transformers lack built-in inductive biases for geometric symmetries; existing equivariant methods incur cost and complexity; need a method with principled symmetry and efficiency.

Method: Define attention with respect to reference frames tied to Platonic symmetry groups; implement weight sharing across frames to enforce equivariance; show equivalence to dynamic group convolution enabling scalable linear-time conv variant; preserves standard Transformer architecture and cost.

Result: Competitive performance on CIFAR-10, ScanObjectNN, QM9, OMol25; leverages geometric constraints without additional cost; learns adaptive geometric filters; provides linear-time convolution variant.

Conclusion: Platonic Transformer offers a principled, efficient way to combine continuous translations with Platonic symmetries in Transformers, matching standard Transformer cost while improving geometric inductive bias; broad applicability across vision, 3D, and molecular domains.

Abstract: While widespread, Transformers lack inductive biases for geometric symmetries
common in science and computer vision. Existing equivariant methods often
sacrifice the efficiency and flexibility that make Transformers so effective
through complex, computationally intensive designs. We introduce the Platonic
Transformer to resolve this trade-off. By defining attention relative to
reference frames from the Platonic solid symmetry groups, our method induces a
principled weight-sharing scheme. This enables combined equivariance to
continuous translations and Platonic symmetries, while preserving the exact
architecture and computational cost of a standard Transformer. Furthermore, we
show that this attention is formally equivalent to a dynamic group convolution,
which reveals that the model learns adaptive geometric filters and enables a
highly scalable, linear-time convolutional variant. Across diverse benchmarks
in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular
property prediction (QM9, OMol25), the Platonic Transformer achieves
competitive performance by leveraging these geometric constraints at no
additional cost.

</details>


### [26] [Domain Generalization for Semantic Segmentation: A Survey](https://arxiv.org/abs/2510.03540)
*Manuel Schwonberg,Hanno Gottschalk*

Main category: cs.CV

TL;DR: This survey analyzes domain generalization for semantic segmentation, highlighting a shift toward foundation-model-based approaches and providing a comparative performance landscape across methods.


<details>
  <summary>Details</summary>
Motivation: Domain generalization remains challenging when unseen domains differ, and semantic segmentation is critical in applications like biomedicine and automated driving. The absence of target-domain access necessitates robust, generalizable methods; foundation models are poised to influence this field.

Method: A comprehensive literature survey that clusters and reviews existing domain generalization approaches for semantic segmentation, develops a taxonomy, and performs an extensive performance comparison to assess the impact of foundation models.

Result: Provides an organized overview and taxonomy of DG methods for semantic segmentation, identifies the paradigm shift toward foundation-model-based DG, and presents a wide-ranging performance comparison demonstrating the significant influence of foundation models on DG.

Conclusion: Aims to advance domain generalization research, highlight new directions, and encourage researchers to explore foundation-model-driven strategies for robust semantic segmentation across unseen domains.

Abstract: The generalization of deep neural networks to unknown domains is a major
challenge despite their tremendous progress in recent years. For this reason,
the dynamic area of domain generalization (DG) has emerged. In contrast to
unsupervised domain adaptation, there is no access to or knowledge about the
target domains, and DG methods aim to generalize across multiple different
unseen target domains. Domain generalization is particularly relevant for the
task semantic segmentation which is used in several areas such as biomedicine
or automated driving. This survey provides a comprehensive overview of the
rapidly evolving topic of domain generalized semantic segmentation. We cluster
and review existing approaches and identify the paradigm shift towards
foundation-model-based domain generalization. Finally, we provide an extensive
performance comparison of all approaches, which highlights the significant
influence of foundation models on domain generalization. This survey seeks to
advance domain generalization research and inspire scientists to explore new
research directions.

</details>


### [27] [From Scope to Script: An Automated Report Generation Model for Gastrointestinal Endoscopy](https://arxiv.org/abs/2510.03543)
*Evandros Kaklamanos,Kristjana Kristinsdottir,Jonathan Huang,Dustin Carlson,Rajesh Keswani,John Pandolfino,Mozziyar Etemadi*

Main category: cs.CV

TL;DR: A two-stage transformer-based model automates endoscopy report generation by pretraining on image-caption data and fine-tuning on image-report pairs, aiming to reduce documentation burden.


<details>
  <summary>Details</summary>
Motivation: Documentation burden and physician burnout in GI endoscopy (EGD and colonoscopy) hinder workflows; automating reports could streamline processes and improve patient care.

Method: A transformer-based architecture with a vision encoder and text decoder. Stage 1 pretrains on image-caption pairs to learn general vision-language features; Stage 2 fine-tunes on image-report pairs to generate clinically meaningful findings.

Result: The approach is proposed to streamline documentation and reduce physician workload, with potential to improve patient care.

Conclusion: A two-stage training framework shows promise for enhancing GI workflow and reducing clinician workload through automated report generation.

Abstract: Endoscopic procedures such as esophagogastroduodenoscopy (EGD) and
colonoscopy play a critical role in diagnosing and managing gastrointestinal
(GI) disorders. However, the documentation burden associated with these
procedures place significant strain on gastroenterologists, contributing to
inefficiencies in clinical workflows and physician burnout. To address this
challenge, we propose a novel automated report generation model that leverages
a transformer-based vision encoder and text decoder within a two-stage training
framework. In the first stage, both components are pre-trained on image/text
caption pairs to capture generalized vision-language features, followed by
fine-tuning on images/report pairs to generate clinically meaningful findings.
Our approach not only streamlines the documentation process but also holds
promise for reducing physician workload and improving patient care.

</details>


### [28] [SketchPlan: Diffusion Based Drone Planning From Human Sketches](https://arxiv.org/abs/2510.03545)
*Sixten Norelius,Aaron O. Feldman,Mac Schwager*

Main category: cs.CV

TL;DR: SketchPlan is a diffusion-based planner that translates 2D hand-drawn sketches over depth images into 3D drone trajectories, combining a SketchAdapter (sketch-to-2D-projection) with DiffPath (3D path diffusion from 2D projection and depth). It enables zero-shot sim-to-real transfer, trained on synthetic 32k paths and real sketches, and shows strong real-world performance, especially in cluttered environments.


<details>
  <summary>Details</summary>
Motivation: Provide an intuitive, human-in-the-loop method for 3D drone path planning by bridging human sketches with autonomous 3D trajectory generation, while achieving robust zero-shot transfer from simulation to real world.

Method: Two modules: SketchAdapter learns to map human sketches to projected 2D paths; DiffPath diffusion model infers 3D trajectories from 2D projections plus first-person depth image. Training data: 32k synthetic flight paths labeled by projecting onto camera plane; 872 real-human sketches used to train the adapter. Model trained on a mix of auto-labeled and human-labeled data, with a modular design.

Result: Real-world tests: 100% success in low/medium clutter; 40% in unseen high-clutter environments. In simulation and reality, ablations show the mixed data and modular design boost performance, with 20–60% gains in task completion over ablations.

Conclusion: SketchPlan demonstrates effective zero-shot sim-to-real transfer for 2D sketch-guided 3D drone planning, leveraging a modular SketchAdapter+DiffPath architecture and a hybrid dataset to interpret human intent and infer safe 3D trajectories in diverse environments.

Abstract: We propose SketchPlan, a diffusion-based planner that interprets 2D
hand-drawn sketches over depth images to generate 3D flight paths for drone
navigation. SketchPlan comprises two components: a SketchAdapter that learns to
map the human sketches to projected 2D paths, and DiffPath, a diffusion model
that infers 3D trajectories from 2D projections and a first person view depth
image. Our model achieves zero-shot sim-to-real transfer, generating accurate
and safe flight paths in previously unseen real-world environments. To train
the model, we build a synthetic dataset of 32k flight paths using a diverse set
of photorealistic 3D Gaussian Splatting scenes. We automatically label the data
by computing 2D projections of the 3D flight paths onto the camera plane, and
use this to train the DiffPath diffusion model. However, since real human 2D
sketches differ significantly from ideal 2D projections, we additionally label
872 of the 3D flight paths with real human sketches and use this to train the
SketchAdapter to infer the 2D projection from the human sketch. We demonstrate
SketchPlan's effectiveness in both simulated and real-world experiments, and
show through ablations that training on a mix of human labeled and auto-labeled
data together with a modular design significantly boosts its capabilities to
correctly interpret human intent and infer 3D paths. In real-world drone tests,
SketchPlan achieved 100\% success in low/medium clutter and 40\% in unseen
high-clutter environments, outperforming key ablations by 20-60\% in task
completion.

</details>


### [29] [Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing](https://arxiv.org/abs/2510.03548)
*Danial Samadi Vahdati,Tai Duc Nguyen,Ekta Prashnani,Koki Nagano,David Luebke,Orazio Gallo,Matthew Stamm*

Main category: cs.CV

TL;DR: A biometric leakage defense for AI-based talking-head systems that detects identity swaps by learning a pose-conditioned, large-margin contrastive embedding that isolates persistent identity cues in the transmitted latent, enabling real-time detection via a cosine similarity test without inspecting the RGB video.


<details>
  <summary>Details</summary>
Motivation: To mitigate puppeteering attacks in bandwidth-efficient talking-head videoconferencing, where the driving identity encoded in the pose-expression latent can be hijacked and where synthetic RGB frames make existing detectors ineffective.

Method: Train a pose-conditioned, large-margin contrastive encoder to disentangle persistent identity cues from the transient pose/expression in the transmitted latent. Use a simple cosine similarity test on the disentangled embedding during rendering to flag illicit identity swaps.

Result: The approach outperforms existing puppeteering defenses, operates in real time, and generalizes well to out-of-distribution scenarios across multiple talking-head generation models.

Conclusion: This work provides the first biometric leakage defense that does not rely on reconstructed RGB, showing that persistent identity information can be isolated within the latent and used to detect identity swaps in real time with strong generalization.

Abstract: AI-based talking-head videoconferencing systems reduce bandwidth by sending a
compact pose-expression latent and re-synthesizing RGB at the receiver, but
this latent can be puppeteered, letting an attacker hijack a victim's likeness
in real time. Because every frame is synthetic, deepfake and synthetic video
detectors fail outright. To address this security problem, we exploit a key
observation: the pose-expression latent inherently contains biometric
information of the driving identity. Therefore, we introduce the first
biometric leakage defense without ever looking at the reconstructed RGB video:
a pose-conditioned, large-margin contrastive encoder that isolates persistent
identity cues inside the transmitted latent while cancelling transient pose and
expression. A simple cosine test on this disentangled embedding flags illicit
identity swaps as the video is rendered. Our experiments on multiple
talking-head generation models show that our method consistently outperforms
existing puppeteering defenses, operates in real-time, and shows strong
generalization to out-of-distribution scenarios.

</details>


### [30] [Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!](https://arxiv.org/abs/2510.03550)
*Junbao Zhou,Yuan Zhou,Kesen Zhao,Qingshan Xu,Beier Zhu,Richang Hong,Hanwang Zhang*

Main category: cs.CV

TL;DR: REVEL enables streaming, fine-grained interactive video manipulation via drag-based edits; introduces DragStream to prevent latent drift and context-induced artifacts, using adaptive distribution self-rectification and spatial-frequency selective optimization to integrate with existing autoregressive video diffusion models.


<details>
  <summary>Details</summary>
Motivation: To address drift and context sensitivity that hinder real-time drag-based video editing in autoregressive video diffusion models, enabling edits 'anytime' on 'anything' with stable outputs.

Method: Propose DragStream (training-free): 1) adaptive distribution self-rectification using statistics from neighboring frames to constrain latent embedding drift; 2) spatial-frequency selective optimization to exploit context while limiting interference via selective cue propagation; compatibility with existing autoregressive video diffusion models.

Result: Extensive experiments demonstrate DragStream effectively mitigates latent drift, stabilizes streaming drag edits, and yields more natural, user-aligned video outputs under varying contexts.

Conclusion: DragStream provides a lightweight, training-free solution to unify and stabilize streaming drag-based video manipulation, improving quality and robustness for REVEL and related tasks.

Abstract: Achieving streaming, fine-grained control over the outputs of autoregressive
video diffusion models remains challenging, making it difficult to ensure that
they consistently align with user expectations. To bridge this gap, we propose
\textbf{stReaming drag-oriEnted interactiVe vidEo manipuLation (REVEL)}, a new
task that enables users to modify generated videos \emph{anytime} on
\emph{anything} via fine-grained, interactive drag. Beyond DragVideo and
SG-I2V, REVEL unifies drag-style video manipulation as editing and animating
video frames with both supporting user-specified translation, deformation, and
rotation effects, making drag operations versatile. In resolving REVEL, we
observe: \emph{i}) drag-induced perturbations accumulate in latent space,
causing severe latent distribution drift that halts the drag process;
\emph{ii}) streaming drag is easily disturbed by context frames, thereby
yielding visually unnatural outcomes. We thus propose a training-free approach,
\textbf{DragStream}, comprising: \emph{i}) an adaptive distribution
self-rectification strategy that leverages neighboring frames' statistics to
effectively constrain the drift of latent embeddings; \emph{ii}) a
spatial-frequency selective optimization mechanism, allowing the model to fully
exploit contextual information while mitigating its interference via
selectively propagating visual cues along generation. Our method can be
seamlessly integrated into existing autoregressive video diffusion models, and
extensive experiments firmly demonstrate the effectiveness of our DragStream.

</details>


### [31] [GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis](https://arxiv.org/abs/2510.03555)
*Peiran Quan,Zifan Gu,Zhuo Zhao,Qin Zhou,Donghan M. Yang,Ruichen Rong,Yang Xie,Guanghua Xiao*

Main category: cs.CV

TL;DR: GAS-MIL is a flexible ensemble MIL method that integrates features from multiple foundation models for pathology classification, achieving superior or on-par performance across PANDA, UBC-OCEAN, and TCGA-BrCa without extensive fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Adapting and benchmarking individual foundation models for specific diagnostic tasks is expensive due to their scale and diversity; a general framework to combine heterogeneous FMs can improve efficiency and performance.

Method: Proposes GAS-MIL, Group-Aggregative Selection Multi-Instance Learning, an ensemble framework that combines features from multiple FMs, preserving complementary strengths without manual feature selection or task-specific fine-tuning; evaluated on three cancer datasets.

Result: GAS-MIL achieves superior or on-par performance relative to individual FMs and established MIL methods across PANDA, UBC-OCEAN, TCGA-BrCa, showing robustness and generalizability; enables efficient integration of heterogeneous FMs and scalable deployment.

Conclusion: GAS-MIL provides a scalable, flexible foundation for multimodal and precision oncology applications and streamlines model deployment in pathology.

Abstract: Foundation models (FMs) have transformed computational pathology by providing
powerful, general-purpose feature extractors. However, adapting and
benchmarking individual FMs for specific diagnostic tasks is often
time-consuming and resource-intensive, especially given their scale and
diversity. To address this challenge, we introduce Group-Aggregative Selection
Multi-Instance Learning (GAS-MIL), a flexible ensemble framework that
seamlessly integrates features from multiple FMs, preserving their
complementary strengths without requiring manual feature selection or extensive
task-specific fine-tuning. Across classification tasks in three cancer
datasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL
consistently achieves superior or on-par performance relative to individual FMs
and established MIL methods, demonstrating its robustness and generalizability.
By enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines
model deployment for pathology and provides a scalable foundation for future
multimodal and precision oncology applications.

</details>


### [32] [Real-Time Assessment of Bystander Situation Awareness in Drone-Assisted First Aid](https://arxiv.org/abs/2510.03558)
*Shen Chang,Renran Tian,Nicole Adams,Nan Kong*

Main category: cs.CV

TL;DR: Video-based real-time bystander situational awareness in drone-assisted naloxone delivery using the DANDSD dataset; graph-embedding and transformer framework outperforms baselines and enables adaptive drone guidance to save lives.


<details>
  <summary>Details</summary>
Motivation: Improve bystander situational awareness and bystander effectiveness in opioid overdose emergencies when EMS is delayed; address the lack of real-time SA assessment in human-autonomy teams; provide a dataset and methods to advance drone-guided interventions.

Method: Propose a video-based SA assessment pipeline leveraging graph embeddings and transformers, integrating geometric, kinematic, and interaction graph features from visual data; use DANDSD (simulated OOEs with untrained bystanders) for training/evaluation; measure real-time SA and temporal segmentation against baselines such as FINCH.

Result: Achieves high SA prediction performance and strong temporal segmentation; outperforms FINCH by about 9 percentage points in MoF and 5 percentage points in IoU.

Conclusion: The framework enables adaptive drone guidance to bystanders in opioid overdose emergencies, with potential to improve response times and save lives.

Abstract: Rapid naloxone delivery via drones offers a promising solution for responding
to opioid overdose emergencies (OOEs), by extending lifesaving interventions to
medically untrained bystanders before emergency medical services (EMS) arrive.
Recognizing the critical role of bystander situational awareness (SA) in
human-autonomy teaming (HAT), we address a key research gap in real-time SA
assessment by introducing the Drone-Assisted Naloxone Delivery Simulation
Dataset (DANDSD). This pioneering dataset captures HAT during simulated OOEs,
where college students without medical training act as bystanders tasked with
administering intranasal naloxone to a mock overdose victim. Leveraging this
dataset, we propose a video-based real-time SA assessment framework that
utilizes graph embeddings and transformer models to assess bystander SA in real
time. Our approach integrates visual perception and comprehension cues--such as
geometric, kinematic, and interaction graph features--and achieves
high-performance SA prediction. It also demonstrates strong temporal
segmentation accuracy, outperforming the FINCH baseline by 9% in Mean over
Frames (MoF) and 5% in Intersection over Union (IoU). This work supports the
development of adaptive drone systems capable of guiding bystanders
effectively, ultimately improving emergency response outcomes and saving lives.

</details>


### [33] [Evaluating OCR performance on food packaging labels in South Africa](https://arxiv.org/abs/2510.03570)
*Mayimunah Nagayi,Alice Khan,Tamryn Frank,Rina Swart,Clement Nyirenda*

Main category: cs.CV

TL;DR: Four open-source OCR systems were benchmarked on real-world food packaging images. Tesseract showed the best character-level accuracy and BLEU on a ground-truth subset; EasyOCR offered strong multilingual support; PaddleOCR achieved near-complete coverage but ran CPU-bound and slower; TrOCR performed weakest even with GPU. The study establishes a packaging-specific baseline and points to layout-aware and localization improvements.


<details>
  <summary>Details</summary>
Motivation: Accurate OCR on packaging is crucial for compliance and nutrition monitoring, yet hampered by multilingual text, dense layouts, varied fonts, glare, and curved surfaces. A packaging-focused benchmark with open-source tools helps establish baselines and guide method development.

Method: Dataset of 231 products (1,628 images); ground-truth subset of 113 images (60 products) used for accuracy evaluation. Evaluated four models (Tesseract, EasyOCR, PaddleOCR, TrOCR) on speed and coverage. Metrics included CER, WER, BLEU, ROUGE-L, F1, coverage, and execution time. Note: PaddleOCR was CPU-bound due to GPU incompatibility; TrOCR used GPU acceleration.

Result: On the ground-truth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU (0.245). EasyOCR offered a strong balance between accuracy and multilingual support. PaddleOCR achieved near-complete coverage but was slower due to CPU-only operation. TrOCR produced the weakest results despite GPU acceleration.

Conclusion: This work provides a packaging-specific benchmark and baseline, highlighting directions for layout-aware methods and improved text localization for packaging OCR.

Abstract: This study evaluates four open-source Optical Character Recognition (OCR)
systems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food
packaging images. The aim is to assess their ability to extract ingredient
lists and nutrition facts panels. Accurate OCR for packaging is important for
compliance and nutrition monitoring but is challenging due to multilingual
text, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231
products (1,628 images) was processed by all four models to assess speed and
coverage, and a ground truth subset of 113 images (60 products) was created for
accuracy evaluation. Metrics include Character Error Rate (CER), Word Error
Rate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground
truth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU
(0.245). EasyOCR provided a good balance between accuracy and multilingual
support. PaddleOCR achieved near complete coverage but was slower because it
ran on CPU only due to GPU incompatibility, and TrOCR produced the weakest
results despite GPU acceleration. These results provide a packaging-specific
benchmark, establish a baseline, and highlight directions for layout-aware
methods and text localization.

</details>


### [34] [FrameOracle: Learning What to See and How Much to See in Videos](https://arxiv.org/abs/2510.03584)
*Chaoyu Li,Tianzhi Li,Fei Tao,Zhenyu Zhao,Ziqian Wu,Maozheng Zhao,Juntong Song,Cheng Niu,Pooyan Fazli*

Main category: cs.CV

TL;DR: FrameOracle is a plug-in module that learns to select the most informative frames and the required number for each query, using a four-stage curriculum and a new FrameOracle-41K dataset; it reduces the number of frames dramatically with no loss in accuracy and improves efficiency when starting from larger frame budgets, achieving strong efficiency–accuracy trade-offs across multiple VLMs and benchmarks.


<details>
  <summary>Details</summary>
Motivation: Video-language models are limited by the number of frames they can process. Fixed or uniform frame sampling is inefficient and can miss information, especially when information density and task complexity vary. There is a need for adaptive, query-aware frame selection that minimizes frames while preserving accuracy.

Method: A four-stage curriculum to train FrameOracle, beginning with weak signals (e.g., cross-modal similarity) and culminating in supervision from a new dataset FrameOracle-41K that provides minimal keyframe annotations for answering each question. The module predicts which frames are most relevant and how many frames are needed. It is evaluated as a plug-in across five VLMs and six benchmarks.

Result: FrameOracle reduces 16-frame inputs to an average of 10.4 frames with no accuracy loss. Starting from 64 frames, it reduces to 13.9 frames and gains +1.4% accuracy. It achieves state-of-the-art efficiency–accuracy trade-offs for scalable video understanding across tested models and benchmarks.

Conclusion: FrameOracle demonstrates effective and scalable adaptive frame sampling for video-language tasks, with strong empirical gains and a large-scale supervision dataset (FrameOracle-41K) to enable robust training and wider adoption.

Abstract: Vision-language models (VLMs) have advanced video understanding, but their
performance is limited by the number of input frames they can process. Existing
frame sampling strategies, such as uniform or fixed-budget selection, often
fail to adapt to variations in information density or task complexity,
resulting in inefficiency and information loss. To address this, we present
FrameOracle, a lightweight and plug-and-play module that predicts both (1)
which frames are most relevant to a given query and (2) how many frames are
needed. FrameOracle is trained using a four-stage curriculum, with the first
three stages relying on weak proxy signals such as cross-modal similarity. In
the final stage, it leverages stronger supervision from a new dataset we
introduce, FrameOracle-41K, the first large-scale VideoQA collection to provide
keyframe annotations specifying the minimal set of frames required to answer
each question. Extensive experiments across five VLMs and six benchmarks
demonstrate that FrameOracle reduces 16-frame inputs to an average of 10.4
frames without any loss in accuracy. When starting from 64-frame candidates, it
reduces the input to an average of 13.9 frames while improving accuracy by
1.4%, achieving state-of-the-art efficiency-accuracy trade-offs for scalable
video understanding.

</details>


### [35] [A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games](https://arxiv.org/abs/2510.03591)
*Faliu Yi,Sherif Abdelfattah,Wei Huang,Adrian Brown*

Main category: cs.CV

TL;DR: Hybrid Co-FineTuning (CFT) uses labeled data from the target game and diverse co-domain games plus unlabeled data to improve visual bug detection; achieves strong cross-game performance and reduces labeling needs, including competitive results with 50% labeled data.


<details>
  <summary>Details</summary>
Motivation: Manual bug detection in games is expensive and label-scarce; relying on labeled data limits scalability; a semi-supervised cross-domain approach can leverage unlabeled data and other games to improve feature learning and generalization.

Method: Integrates labeled samples from target and co-domain games with unlabeled data through Co-FineTuning (CFT); jointly learns feature representations across domains, likely using semi-supervised objectives to exploit unlabeled data and improve generalization; aims to maximize data utility and reduce target-domain labeling requirements.

Result: Empirical results show robustness and superior performance vs baselines across multiple gaming environments; CFT remains competitive even when trained with only 50% of target labels.

Conclusion: CFT enhances scalability/adaptability of visual bug detection across game titles, enabling efficient cross-domain deployment with reduced reliance on target-domain annotations.

Abstract: Manual identification of visual bugs in video games is a resource-intensive
and costly process, often demanding specialized domain knowledge. While
supervised visual bug detection models offer a promising solution, their
reliance on extensive labeled datasets presents a significant challenge due to
the infrequent occurrence of such bugs. To overcome this limitation, we propose
a hybrid Co-FineTuning (CFT) method that effectively integrates both labeled
and unlabeled data. Our approach leverages labeled samples from the target game
and diverse co-domain games, additionally incorporating unlabeled data to
enhance feature representation learning. This strategy maximizes the utility of
all available data, substantially reducing the dependency on labeled examples
from the specific target game. The developed framework demonstrates enhanced
scalability and adaptability, facilitating efficient visual bug detection
across various game titles. Our experimental results show the robustness of the
proposed method for game visual bug detection, exhibiting superior performance
compared to conventional baselines across multiple gaming environments.
Furthermore, CFT maintains competitive performance even when trained with only
50% of the labeled data from the target game.

</details>


### [36] [Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification Without Augmentation](https://arxiv.org/abs/2510.03598)
*Alexander V. Mantzaris*

Main category: cs.CV

TL;DR: HRM with two Transformer-like modules and DEQ-style training under a strict no-augmentation regime struggles on small natural images, performing well on MNIST but underperforming on CIFAR-10/100 compared with simple CNN baselines; results suggest insufficient image-specific inductive bias in its current form, though modifications might yield improvements.


<details>
  <summary>Details</summary>
Motivation: To assess whether the Hierarchical Reasoning Model (HRM) can serve as a practical image classifier without augmentation, and to understand its inductive bias and place relative to standard CNNs on small-image benchmarks.

Method: Evaluate HRM with two Transformer-style modules (f_L, f_H), one-step (DEQ-style) training, deep supervision, Rotary Position Embeddings, and RMSNorm on MNIST, CIFAR-10, and CIFAR-100 under a deliberately raw regime: no data augmentation, identical optimizer family with a one-epoch warmup then cosine-floor decay, and label smoothing.

Result: On MNIST, HRM achieves ~98% test accuracy. On CIFAR-10, HRM reaches 65.0% after 25 epochs, while a two-stage Conv–BN–ReLU baseline reaches 77.2% and trains ~30x faster per epoch. On CIFAR-100, HRM attains 29.7% test accuracy despite 91.5% train accuracy; the CNN baseline attains 45.3% test with 50.5% train accuracy. Loss traces indicate healthy optimization but insufficient image-specific inductive bias for HRM in this regime.

Conclusion: For small-resolution image classification without augmentation, HRM is not competitive with simple convolutional architectures in its current form. However, the study leaves open the possibility that architectural or training modifications could significantly improve HRM’s performance in future work.

Abstract: This paper asks whether the Hierarchical Reasoning Model (HRM) with the two
Transformer-style modules $(f_L,f_H)$, one step (DEQ-style) training, deep
supervision, Rotary Position Embeddings, and RMSNorm can serve as a practical
image classifier. It is evaluated on MNIST, CIFAR-10, and CIFAR-100 under a
deliberately raw regime: no data augmentation, identical optimizer family with
one-epoch warmup then cosine-floor decay, and label smoothing. HRM optimizes
stably and performs well on MNIST ($\approx 98\%$ test accuracy), but on small
natural images it overfits and generalizes poorly: on CIFAR-10, HRM reaches
65.0\% after 25 epochs, whereas a two-stage Conv--BN--ReLU baseline attains
77.2\% while training $\sim 30\times$ faster per epoch; on CIFAR-100, HRM
achieves only 29.7\% test accuracy despite 91.5\% train accuracy, while the
same CNN reaches 45.3\% test with 50.5\% train accuracy. Loss traces and error
analyses indicate healthy optimization but insufficient image-specific
inductive bias for HRM in this regime. It is concluded that, for
small-resolution image classification without augmentation, HRM is not
competitive with even simple convolutional architectures as the HRM currently
exist but this does not exclude possibilities that modifications to the model
may allow it to improve greatly.

</details>


### [37] [Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops](https://arxiv.org/abs/2510.03606)
*Mattia Scardecchia*

Main category: cs.CV

TL;DR: Survey of DINOv2 SSL approach highlighting multi-crop augmentation and self-distillation with a mean teacher; compares DINO/DINOv2 to SSL and weakly supervised methods; discusses emergent properties on transformer backbones and future directions.


<details>
  <summary>Details</summary>
Motivation: To understand the design choices and impact of DINOv2 in self-supervised visual representation learning, and to situate it within the SSL/WSL landscape while identifying limitations and future research directions.

Method: Historical and empirical analysis of prior work leading to DINO/DINOv2; examination of core ideas (multi-crop view augmentation, self-distillation with mean teacher); cross-method performance comparisons across downstream tasks; qualitative assessment of emergent properties with transformer backbones.

Result: DINOv2 achieves state-of-the-art performance and surpasses weakly supervised methods like OpenCLIP on many benchmarks; DINO and DINOv2 exhibit strong transferability across downstream tasks; notable emergent properties appear when using transformer backbones; the paper discusses limitations and broader impact.

Conclusion: DINOv2 has a significant impact on SSL-based visual representation learning, but there are limitations and open questions. Future work should address robustness, data efficiency, scaling, and broader task transfer to solidify and extend these gains.

Abstract: Recent advances in self-supervised learning (SSL) have made it possible to
learn general-purpose visual features that capture both the high-level
semantics and the fine-grained spatial structure of images. Most notably, the
recent DINOv2 has established a new state of the art by surpassing weakly
supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we
examine the core ideas behind its approach, multi-crop view augmentation and
self-distillation with a mean teacher, and trace their development in previous
work. We then compare the performance of DINO and DINOv2 with other SSL and WSL
methods across various downstream tasks, and highlight some remarkable emergent
properties of their learned features with transformer backbones. We conclude by
briefly discussing DINOv2's limitations, its impact, and future research
directions.

</details>


### [38] [Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL](https://arxiv.org/abs/2510.03608)
*Ruitao Wu,Yifan Zhao,Guangyao Chen,Jia Li*

Main category: cs.CV

TL;DR: A diffusion-classifier co-evolution framework (DCS) for FSCIL that uses a reward-aligned, two-level feedback loop to guide diffusion-based augmentation, achieving state-of-the-art results by improving retention and new-class learning.


<details>
  <summary>Details</summary>
Motivation: FSCIL tasks require learning new classes from few examples without forgetting old ones, but suffer from stability-plasticity tension and data scarcity. Diffusion-based augmentation can help but risks semantic misalignment with the classifier. A feedback-driven approach that aligns diffusion with the classifier’s state is needed.

Method: DCS establishes a mutual boosting loop between a diffusion model and an FSCIL classifier. It uses a dynamic reward function derived from the classifier state to steer diffusion updates at two levels: (1) feature level — semantic coherence and diversity via prototype-anchored maximum mean discrepancy and dimension-wise variance matching; (2) logits level — improved inter-class discriminability through confidence recalibration and cross-session confusion-aware mechanisms.

Result: The framework achieves state-of-the-art performance on FSCIL benchmarks, enhancing both knowledge retention and the acquisition of new classes.

Conclusion: A co-evolutionary diffusion–classifier framework with a reward-aligned learning strategy can robustly improve FSCIL by aligning generated data with classifier goals, suggesting broad applicability to data-scarce, incremental learning scenarios.

Abstract: Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially
learn new classes from minimal examples without forgetting prior knowledge, a
task complicated by the stability-plasticity dilemma and data scarcity. Current
FSCIL methods often struggle with generalization due to their reliance on
limited datasets. While diffusion models offer a path for data augmentation,
their direct application can lead to semantic misalignment or ineffective
guidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel
framework that establishes a mutual boosting loop between diffusion model and
FSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a
dynamic, multi-faceted reward function derived from the classifier's state
directs the diffusion model. This reward system operates at two levels: the
feature level ensures semantic coherence and diversity using prototype-anchored
maximum mean discrepancy and dimension-wise variance matching, while the logits
level promotes exploratory image generation and enhances inter-class
discriminability through confidence recalibration and cross-session
confusion-aware mechanisms. This co-evolutionary process, where generated
images refine the classifier and an improved classifier state yields better
reward signals, demonstrably achieves state-of-the-art performance on FSCIL
benchmarks, significantly enhancing both knowledge retention and new class
learning.

</details>


### [39] [MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations](https://arxiv.org/abs/2510.03666)
*Jiang Wu,Sichao Wu,Yinsong Ma,Guangyuan Yu,Haoyuan Xu,Lifang Zheng,Jingliang Duan*

Main category: cs.CV

TL;DR: MonitorVLM is a vision–language framework that detects mining safety violations from surveillance video using a domain-specific dataset, a clause-filter to cut latency, and a behavior magnifier to boost action recognition, achieving substantial gains over baselines and enabling automatic reporting via a web interface.


<details>
  <summary>Details</summary>
Motivation: Industrial mining suffers frequent accidents caused by unsafe worker behaviors and current manual inspections are labor-intensive, error-prone, and unable to scale to dynamic environments, necessitating intelligent automated safety monitoring.

Method: Proposes three innovations: (1) a domain-specific violation dataset with 9,000 VQA samples spanning 40 mining regulations, augmented and complemented with auxiliary detection cues; (2) a clause-filter module that dynamically selects the Top-K most relevant clauses to reduce inference latency by 13.56% while preserving accuracy; (3) a behavior magnifier module that enlarges worker regions to improve fine-grained action recognition, yielding 3.45% precision and 8.62% recall gains.

Result: Quantitative results show MonitorVLM outperforms baseline vision–language models: +22.01% precision, +34.22% recall, and +28.37% F1 score over a 72B unfine-tuned baseline. A lightweight web interface enables automatic violation reporting with video timestamping.

Conclusion: The study demonstrates the promise of multimodal large models for enhancing occupational safety monitoring in mining and beyond, driven by domain-specific data, efficient clause-based filtering, and region-focused magnification to improve recognition of unsafe behaviors.

Abstract: Industrial accidents, particularly in high-risk domains such as surface and
underground mining, are frequently caused by unsafe worker behaviors.
Traditional manual inspection remains labor-intensive, error-prone, and
insufficient for large-scale, dynamic environments, highlighting the urgent
need for intelligent and automated safety monitoring. In this paper, we present
MonitorVLM, a novel vision--language framework designed to detect safety
violations directly from surveillance video streams. MonitorVLM introduces
three key innovations: (1) a domain-specific violation dataset comprising 9,000
vision--question--answer (VQA) samples across 40 high-frequency mining
regulations, enriched with augmentation and auxiliary detection cues; (2) a
clause filter (CF) module that dynamically selects the Top-$K$ most relevant
clauses, reducing inference latency by 13.56\% while maintaining accuracy; and
(3) a behavior magnifier (BM) module that enhances worker regions to improve
fine-grained action recognition, yielding additional gains of 3.45% in
precision and 8.62% in recall. Experimental results demonstrate that MonitorVLM
significantly outperforms baseline vision--language models, achieving
improvements of 22.01% in precision, 34.22\% in recall, and 28.37% in F1 score
over the 72B unfine-tuned baseline. A lightweight web-based interface further
integrates MonitorVLM into practical workflows, enabling automatic violation
reporting with video timestamping. This study highlights the potential of
multimodal large models to enhance occupational safety monitoring in mining and
beyond.

</details>


### [40] [A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems](https://arxiv.org/abs/2510.03675)
*Siva Sai,Saksham Gupta,Vinay Chamola,Rajkumar Buyya*

Main category: cs.CV

TL;DR: Hybrid diffusion-model classifier for ITS accident detection; uses ExceptionNet outputs as conditioning; cloud-based, ablation on diffusion settings; achieves 97.32% accuracy on image-based accident detection on a public dataset.


<details>
  <summary>Details</summary>
Motivation: To improve accident detection in ITS by leveraging diffusion models to capture complex data distributions and scale via cloud computing, addressing limitations of conventional classifiers.

Method: Integrates guidance classification with diffusion; uses fine-tuned ExceptionNet outputs as input conditioning; multiple conditional modules modulate input projections via time embeddings and image covariate embeddings; responsible for dynamics across diffusion steps; cloud-based implementation; ablation study on timesteps, schedulers, count, architecture; evaluated against baselines on a public dataset.

Result: Best performance in image-based accident detection with 97.32% accuracy; diffusion model outperforms baseline models.

Conclusion: Diffusion models are effective for complex ITS data distributions; the proposed conditional diffusion framework provides robust, scalable accident detection; cloud-based deployment is feasible.

Abstract: The integration of Diffusion Models into Intelligent Transportation Systems
(ITS) is a substantial improvement in the detection of accidents. We present a
novel hybrid model integrating guidance classification with diffusion
techniques. By leveraging fine-tuned ExceptionNet architecture outputs as input
for our proposed diffusion model and processing image tensors as our
conditioning, our approach creates a robust classification framework. Our model
consists of multiple conditional modules, which aim to modulate the linear
projection of inputs using time embeddings and image covariate embeddings,
allowing the network to adapt its behavior dynamically throughout the diffusion
process. To address the computationally intensive nature of diffusion models,
our implementation is cloud-based, enabling scalable and efficient processing.
Our strategy overcomes the shortcomings of conventional classification
approaches by leveraging diffusion models inherent capacity to effectively
understand complicated data distributions. We investigate important diffusion
characteristics, such as timestep schedulers, timestep encoding techniques,
timestep count, and architectural design changes, using a thorough ablation
study, and have conducted a comprehensive evaluation of the proposed model
against the baseline models on a publicly available dataset. The proposed
diffusion model performs best in image-based accident detection with an
accuracy of 97.32%.

</details>


### [41] [SAMSOD: Rethinking SAM Optimization for RGB-T Salient Object Detection](https://arxiv.org/abs/2510.03689)
*Zhengyi Liu,Xinrui Wang,Xianyong Fang,Zhengzheng Tu,Linbo Wang*

Main category: cs.CV

TL;DR: SAMSOD improves RGB-T salient object detection by enforcing unimodal supervision for the weaker modality, applying gradient deconfliction to mitigate training gradient conflicts, and using two decoupled adapters to separately mask high- and low-activation neurons, thereby emphasizing foreground objects and strengthening background learning.


<details>
  <summary>Details</summary>
Motivation: RGB-T SOD methods suffer from modality imbalance and conflicting gradients between RGB and thermal streams, which can hinder convergence and performance.

Method: SAMSOD employs (1) unimodal supervision to bolster learning in the non-dominant modality, (2) gradient deconfliction to reduce adverse effects of gradient conflicts during training, and (3) two decoupled adapters that separately mask high- and low-activation neurons to emphasize foreground while improving background representation.

Result: Fundamental experiments on RGB-T SOD benchmarks and generalizability tests on scribble-supervised RGB-T SOD, fully supervised RGB-D SOD datasets, and RGB-D rail surface defect detection demonstrate the approach’s effectiveness.

Conclusion: The proposed SAMSOD framework enhances cross-modal learning and activation separation, achieving improved performance and generalization in RGB-T SOD and related tasks.

Abstract: RGB-T salient object detection (SOD) aims to segment attractive objects by
combining RGB and thermal infrared images. To enhance performance, the Segment
Anything Model has been fine-tuned for this task. However, the imbalance
convergence of two modalities and significant gradient difference between high-
and low- activations are ignored, thereby leaving room for further performance
enhancement. In this paper, we propose a model called \textit{SAMSOD}, which
utilizes unimodal supervision to enhance the learning of non-dominant modality
and employs gradient deconfliction to reduce the impact of conflicting
gradients on model convergence. The method also leverages two decoupled
adapters to separately mask high- and low-activation neurons, emphasizing
foreground objects by enhancing background learning. Fundamental experiments on
RGB-T SOD benchmark datasets and generalizability experiments on scribble
supervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised
RGB-D rail surface defect detection all demonstrate the effectiveness of our
proposed method.

</details>


### [42] [Referring Expression Comprehension for Small Objects](https://arxiv.org/abs/2510.03701)
*Kanoko Goto,Takumi Hirose,Mahiro Ukai,Shuhei Kurita,Nakamasa Inoue*

Main category: cs.CV

TL;DR: Proposes SOREC, a dataset for small-object referring expression comprehension in driving scenes, and PIZA, an adapter enabling progressive zooming for accurate localization with efficient fine-tuning; demonstrates gains on GroundingDINO.


<details>
  <summary>Details</summary>
Motivation: Address difficulty of localizing extremely small objects in REC, crucial for autonomous driving; existing datasets and methods underestimate performance on tiny targets; need dataset and efficient adaptation technique.

Method: Create SOREC dataset with 100k referring-expression–bbox pairs for small driving objects. Propose PIZA adapter for parameter-efficient fine-tuning that iteratively zooms to refine localization; integrate with GroundingDINO.

Result: Significant accuracy improvements on SOREC when applying PIZA to GroundingDINO; public availability of dataset, codes, and pre-trained models.

Conclusion: Dataset and PIZA offer a practical, scalable approach to small-object REC; enables better performance in safety-critical driving scenarios and supports reproducibility via released resources.

Abstract: Referring expression comprehension (REC) aims to localize the target object
described by a natural language expression. Recent advances in vision-language
learning have led to significant performance improvements in REC tasks.
However, localizing extremely small objects remains a considerable challenge
despite its importance in real-world applications such as autonomous driving.
To address this issue, we introduce a novel dataset and method for REC
targeting small objects. First, we present the small object REC (SOREC)
dataset, which consists of 100,000 pairs of referring expressions and
corresponding bounding boxes for small objects in driving scenarios. Second, we
propose the progressive-iterative zooming adapter (PIZA), an adapter module for
parameter-efficient fine-tuning that enables models to progressively zoom in
and localize small objects. In a series of experiments, we apply PIZA to
GroundingDINO and demonstrate a significant improvement in accuracy on the
SOREC dataset. Our dataset, codes and pre-trained models are publicly available
on the project page.

</details>


### [43] [Artery-Vein Segmentation from Fundus Images using Deep Learning](https://arxiv.org/abs/2510.03717)
*Sharan SK,Subin Sahayam,Umarani Jayaraman,Lakshmi Priya A*

Main category: cs.CV

TL;DR: Proposes Attention-WNet for artery-vein segmentation in retinal images by integrating an attention mechanism into the WNet architecture; achieves superior performance on HRF and DRIVE datasets compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Segmenting retinal arteries and veins enables vascular health assessment and discovery of biomarkers; improved artery-vein delineation with deep learning and attention can aid diagnosis of systemic and ocular diseases.

Method: Incorporates an attention mechanism into the WNet segmentation model to create Attention-WNet for artery-vein segmentation; evaluated on publicly available HRF and DRIVE datasets.

Result: Attention-WNet outperforms state-of-the-art models on the tested datasets.

Conclusion: Attention-WNet shows promise for accurate artery-vein segmentation in fundus images, potentially enhancing retinal vessel analysis and biomarker discovery; broader validation is needed.

Abstract: Segmenting of clinically important retinal blood vessels into arteries and
veins is a prerequisite for retinal vessel analysis. Such analysis can provide
potential insights and bio-markers for identifying and diagnosing various
retinal eye diseases. Alteration in the regularity and width of the retinal
blood vessels can act as an indicator of the health of the vasculature system
all over the body. It can help identify patients at high risk of developing
vasculature diseases like stroke and myocardial infarction. Over the years,
various Deep Learning architectures have been proposed to perform retinal
vessel segmentation. Recently, attention mechanisms have been increasingly used
in image segmentation tasks. The work proposes a new Deep Learning approach for
artery-vein segmentation. The new approach is based on the Attention mechanism
that is incorporated into the WNet Deep Learning model, and we call the model
as Attention-WNet. The proposed approach has been tested on publicly available
datasets such as HRF and DRIVE datasets. The proposed approach has outperformed
other state-of-art models available in the literature.

</details>


### [44] [Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models](https://arxiv.org/abs/2510.03721)
*Leander Girrbach,Stephan Alaniz,Genevieve Smith,Trevor Darrell,Zeynep Akata*

Main category: cs.CV

TL;DR: A large-scale annotation effort on LAION-400M creates 276M bounding boxes plus gender/race labels and captions; reveals data alone accounts for major share of demographic biases in vision-language models, with 60-70% of gender bias in CLIP/Stable Diffusion explained by direct co-occurrences.


<details>
  <summary>Details</summary>
Motivation: To understand how training data shape demographic bias in vision-language models by providing demographic annotations on web-scale datasets, enabling empirical link between dataset composition and model bias.

Method: Develop validated automatic labeling pipelines combining object detection for bounding boxes, multimodal captioning, and finetuned classifiers to assign perceived gender and race; annotate full LAION-400M; analyze biases and co-occurrence patterns; examine impact on CLIP and Stable Diffusion biases.

Result: Uncovered demographic imbalances and harmful associations (e.g., men and individuals perceived as Black/Middle Eastern linked with crime/negative content); quantified proportion of gender bias explained by co-occurrences (60-70%); created dataset resources to study bias.

Conclusion: First large-scale empirical link between dataset composition and downstream model bias; resource enables more accurate assessment and remediation of biases in vision-language models.

Abstract: Vision-language models trained on large-scale multimodal datasets show strong
demographic biases, but the role of training data in producing these biases
remains unclear. A major barrier has been the lack of demographic annotations
in web-scale datasets such as LAION-400M. We address this gap by creating
person-centric annotations for the full dataset, including over 276 million
bounding boxes, perceived gender and race/ethnicity labels, and automatically
generated captions. These annotations are produced through validated automatic
labeling pipelines combining object detection, multimodal captioning, and
finetuned classifiers. Using them, we uncover demographic imbalances and
harmful associations, such as the disproportionate linking of men and
individuals perceived as Black or Middle Eastern with crime-related and
negative content. We also show that 60-70% of gender bias in CLIP and Stable
Diffusion can be linearly explained by direct co-occurrences in the data. Our
resources establish the first large-scale empirical link between dataset
composition and downstream model bias.

</details>


### [45] [Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks](https://arxiv.org/abs/2510.03725)
*Thomas Hallopeau,Joris Guérin,Laurent Demagistri,Youssef Fouzai,Renata Gracie,Vanderlei Pascoal De Matos,Helen Gurgel,Nadine Dessay*

Main category: cs.CV

TL;DR: Compare generic pretrained networks vs. a satellite-imagery pretrained network for detecting favelas in Rio, to determine whether data volume or task specificity yields better performance.


<details>
  <summary>Details</summary>
Motivation: Understand whether broad pretraining on large, diverse image datasets or domain-specific pretraining on satellite imagery better supports detecting informal settlements in urban environments.

Method: Evaluate two pretrained models: (1) generic networks pretrained on large diverse datasets of non-specific images; (2) a specialized network pretrained on satellite imagery. Apply them to the task of detecting favelas in Rio de Janeiro and compare performance.

Result: The abstract does not report empirical results or conclusions; it outlines a comparative study to determine which pretraining strategy yields superior performance.

Conclusion: No conclusion is provided in the abstract; the study aims to reveal whether task specificity (satellite-domain pretraining) or data volume (large generic pretraining) drives better detection performance.

Abstract: While deep learning methods for detecting informal settlements have already
been developed, they have not yet fully utilized the potential offered by
recent pretrained neural networks. We compare two types of pretrained neural
networks for detecting the favelas of Rio de Janeiro: 1. Generic networks
pretrained on large diverse datasets of unspecific images, 2. A specialized
network pretrained on satellite imagery. While the latter is more specific to
the target task, the former has been pretrained on significantly more images.
Hence, this research investigates whether task specificity or data volume
yields superior performance in urban informal settlement detection.

</details>


### [46] [LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes](https://arxiv.org/abs/2510.03747)
*Zuomin Qu,Yimao Guo,Qianyue Hu,Wei Lu*

Main category: cs.CV

TL;DR: A plug-and-play LoRA patch defeats proactive Deepfake defenses with a gating mechanism and MMFA loss; reveals vulnerabilities in current defenses and proposes a defensive warning patch; claims high effectiveness with minimal data/time; code released.


<details>
  <summary>Details</summary>
Motivation: Assess the robustness of proactive defenses against adversarial preemptions and highlight vulnerabilities in defending strategies that attempt to preempt manipulation.

Method: Introduce Low-Rank Adaptation (LoRA) patching that injects a learnable patch into Deepfake generators, controlled by a gating mechanism to avoid gradient explosions; use Multi-Modal Feature Alignment (MMFA) loss to align adversarial outputs with desired outputs; also propose defensive LoRA patching that embeds visible warnings in outputs.

Result: LoRA patching defeats multiple state-of-the-art proactive defenses using only 1,000 facial examples and one epoch of fine-tuning; demonstrates a critical weakness in current defense paradigms; provides a defense-ready patch with warnings as a complementary mitigation; code released.

Conclusion: Current proactive defenses against Deepfakes may be brittle and insufficient; more robust defense strategies are required; dual-use nature of the technique highlights the need for careful defense research and safer dissemination.

Abstract: Deepfakes pose significant societal risks, motivating the development of
proactive defenses that embed adversarial perturbations in facial images to
prevent manipulation. However, in this paper, we show that these preemptive
defenses often lack robustness and reliability. We propose a novel approach,
Low-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch
into Deepfake generators to bypass state-of-the-art defenses. A learnable
gating mechanism adaptively controls the effect of the LoRA patch and prevents
gradient explosions during fine-tuning. We also introduce a Multi-Modal Feature
Alignment (MMFA) loss, encouraging the features of adversarial outputs to align
with those of the desired outputs at the semantic level. Beyond bypassing, we
present defensive LoRA patching, embedding visible warnings in the outputs as a
complementary solution to mitigate this newly identified security
vulnerability. With only 1,000 facial examples and a single epoch of
fine-tuning, LoRA patching successfully defeats multiple proactive defenses.
These results reveal a critical weakness in current paradigms and underscore
the need for more robust Deepfake defense strategies. Our code is available at
https://github.com/ZOMIN28/LoRA-Patching.

</details>


### [47] [The Overlooked Value of Test-time Reference Sets in Visual Place Recognition](https://arxiv.org/abs/2510.03751)
*Mubariz Zaffar,Liangliang Nan,Sebastian Scherer,Julian F. P. Kooij*

Main category: cs.CV

TL;DR: Reference-Set-Finetuning (RSF) tunes VPR models on the test-time reference map to bridge train-test domain gaps, yielding about +2.3% Recall@1 on several challenging benchmarks while preserving generalization.


<details>
  <summary>Details</summary>
Motivation: VPR performance on benchmarks can drop when test environments diverge from VPR training data. The map (test-time reference set) contains target-domain images and poses and is available before querying, offering a practical, unexplored source of adaptation data to reduce domain shift.

Method: Perform a lightweight finetuning of VPR models using the available map at test time (Reference-Set-Finetuning). This adapts model representations to the target domain using map images/poses before processing queries.

Result: Average Recall@1 improvements of roughly 2.3% on challenging benchmarks, with finetuned models maintaining generalization and performing well across diverse test datasets.

Conclusion: RSF provides a simple, effective, and generalizable way to boost VPR performance by leveraging the test-time reference set, reducing the train-test domain gap without requiring new data sources.

Abstract: Given a query image, Visual Place Recognition (VPR) is the task of retrieving
an image of the same place from a reference database with robustness to
viewpoint and appearance changes. Recent works show that some VPR benchmarks
are solved by methods using Vision-Foundation-Model backbones and trained on
large-scale and diverse VPR-specific datasets. Several benchmarks remain
challenging, particularly when the test environments differ significantly from
the usual VPR training datasets. We propose a complementary, unexplored source
of information to bridge the train-test domain gap, which can further improve
the performance of State-of-the-Art (SOTA) VPR methods on such challenging
benchmarks. Concretely, we identify that the test-time reference set, the
"map", contains images and poses of the target domain, and must be available
before the test-time query is received in several VPR applications. Therefore,
we propose to perform simple Reference-Set-Finetuning (RSF) of VPR models on
the map, boosting the SOTA (~2.3% increase on average for Recall@1) on these
challenging datasets. Finetuned models retain generalization, and RSF works
across diverse test datasets.

</details>


### [48] [Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization](https://arxiv.org/abs/2510.03763)
*Jiaxin Deng,Junbiao Pang*

Main category: cs.CV

TL;DR: ARSAM provides a faster alternative to SAM by reusing and adaptively updating the PSF component of the gradient, achieving comparable accuracy with ~40% speedup on CIFAR-10/100 and broad applicability to other tasks.


<details>
  <summary>Details</summary>
Motivation: SAM improves generalization but doubles computation by requiring two gradient evaluations per step. There is a need for a computationally cheaper method that preserves SAM's benefits.

Method: Decompose SAM gradient into the SGD gradient plus the Projection of the Second-order gradient onto the First-order gradient (PSF). ARSAM adaptively samples, reuses, and mixes these decomposed gradients, with the PSF becoming more influential over training, enabling a cheaper approximation of SAM steps while maintaining generalization.

Result: ARSAM attains state-of-the-art accuracies comparable to SAM across diverse architectures. On CIFAR-10/100 it matches SAM performance while delivering about a 40% speedup. It also accelerates optimization for tasks like human pose estimation and model quantization without sacrificing performance.

Conclusion: ARSAM is a practical, broadly applicable method that preserves SAM's generalization benefits at a significantly reduced computational cost, extending SAM’s advantages to a wider range of applications.

Abstract: Sharpness-Aware Minimization (SAM) improves model generalization but doubles
the computational cost of Stochastic Gradient Descent (SGD) by requiring twice
the gradient calculations per optimization step. To mitigate this, we propose
Adaptively sampling-Reusing-mixing decomposed gradients to significantly
accelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can
be decomposed into the SGD gradient and the Projection of the Second-order
gradient onto the First-order gradient (PSF). Furthermore, we observe that the
SGD gradient and PSF dynamically evolve during training, emphasizing the
growing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed
to the reused PSF and the timely updated PSF still maintain the model's
generalization ability. Extensive experiments show that ARSAM achieves
state-of-the-art accuracies comparable to SAM across diverse network
architectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a
speedup of about 40\%. Moreover, ARSAM accelerates optimization for the various
challenge tasks (\textit{e.g.}, human pose estimation, and model quantization)
without sacrificing performance, demonstrating its broad practicality.% The
code is publicly accessible at: https://github.com/ajiaaa/ARSAM.

</details>


### [49] [CoPA: Hierarchical Concept Prompting and Aggregating Network for Explainable Diagnosis](https://arxiv.org/abs/2510.03767)
*Yiheng Dong,Yi Lin,Xin Yang*

Main category: cs.CV

TL;DR: CoPA introduces multilevel concept extraction and prompt-guided tuning to improve concept-based diagnosis by aggregating layer-wise representations into aligned concept prompts.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing Concept Bottleneck Models that rely on final-layer features and lack multi-scale, guided concept encoding, hindering fine-grained concept extraction in clinical diagnostics.

Method: Proposes Concept Prompting and Aggregating (CoPA): Concept-aware Embedding Generator (CEG) extracts concept representations from each encoder layer; these representations serve as prompts for Concept Prompt Tuning (CPT) to emphasize concept-related visual cues; visual representations from all layers are aggregated to align with textual concept representations.

Result: Outperforms state-of-the-art methods on three public datasets; code available at the given GitHub link.

Conclusion: CoPA effectively captures concept-wise information across layers, improving concept and disease prediction and enhancing transparency in clinical deep learning.

Abstract: The transparency of deep learning models is essential for clinical
diagnostics. Concept Bottleneck Model provides clear decision-making processes
for diagnosis by transforming the latent space of black-box models into
human-understandable concepts. However, concept-based methods still face
challenges in concept capture capabilities. These methods often rely on encode
features solely from the final layer, neglecting shallow and multiscale
features, and lack effective guidance in concept encoding, hindering
fine-grained concept extraction. To address these issues, we introduce Concept
Prompting and Aggregating (CoPA), a novel framework designed to capture
multilayer concepts under prompt guidance. This framework utilizes the
Concept-aware Embedding Generator (CEG) to extract concept representations from
each layer of the visual encoder. Simultaneously, these representations serve
as prompts for Concept Prompt Tuning (CPT), steering the model towards
amplifying critical concept-related visual cues. Visual representations from
each layer are aggregated to align with textual concept representations. With
the proposed method, valuable concept-wise information in the images is
captured and utilized effectively, thus improving the performance of concept
and disease prediction. Extensive experimental results demonstrate that CoPA
outperforms state-of-the-art methods on three public datasets. Code is
available at https://github.com/yihengd/CoPA.

</details>


### [50] [Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation](https://arxiv.org/abs/2510.03769)
*Shimaa Elbana,Ahmad Kamal,Shahd Ahmed Ali,Ahmad Al-Kabbany*

Main category: cs.CV

TL;DR: ZFP compression preserves automated cerebrovascular segmentation quality on a large 3D vascular dataset while greatly reducing data size; Dice remains nearly unchanged (baseline ~0.8774, compressed ~0.8766) with up to ~23:1 compression in error-tolerance mode.


<details>
  <summary>Details</summary>
Motivation: Growing 3D medical imaging datasets pose barriers to collaboration and data transfer; there is a need for efficient, controlled-loss compression that preserves segmentation performance.

Method: Apply ZFP in error-tolerance and fixed-rate modes to a large-scale 3D medical dataset with ground-truth vascular segmentations; compare post-compression segmentation quality to uncompressed baseline using Dice; quantify data reduction.

Result: Substantial data reduction up to 22.89:1 in error-tolerance mode; mean Dice coefficient remains high (0.87656) versus uncompressed baseline (~0.8774).

Conclusion: ZFP is a viable and powerful tool to enable more efficient, accessible research on large-scale medical imaging datasets, facilitating broader collaboration without sacrificing segmentation performance.

Abstract: The increasing size and complexity of medical imaging datasets, particularly
in 3D formats, present significant barriers to collaborative research and
transferability. This study investigates whether the ZFP compression technique
can mitigate these challenges without compromising the performance of automated
cerebrovascular segmentation, a critical first step in intracranial aneurysm
detection. We apply ZFP in both its error tolerance and fixed-rate modes to a
large scale, and one of the most recent, datasets in the literature, 3D medical
dataset containing ground-truth vascular segmentations. The segmentation
quality on the compressed volumes is rigorously compared to the uncompressed
baseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can
achieve substantial data reduction--up to a 22.89:1 ratio in error tolerance
mode--while maintaining a high degree of fidelity, with the mean Dice
coefficient remaining high at 0.87656. These results demonstrate that ZFP is a
viable and powerful tool for enabling more efficient and accessible research on
large-scale medical datasets, fostering broader collaboration across the
community.

</details>


### [51] [MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based Fusion for Medical Image Segmentation](https://arxiv.org/abs/2510.03786)
*T-Mai Bui,Fares Bougourzi,Fadi Dornaika,Vinh Truong Hoang*

Main category: cs.CV

TL;DR: A hybrid three-branch encoder for medical image segmentation combining CNNs, Transformers, and Mamba-based Attention Fusion (MAF), with a multi-scale attention-based decoder and co-attention gates. It claims improved accuracy and generalization with comparable efficiency across benchmarks, and promises code release.


<details>
  <summary>Details</summary>
Motivation: To overcome task-specific performance and modality/region variability in medical image segmentation while balancing model complexity and efficiency for clinical use.

Method: A three-branch encoder integrates local (CNN), global (Transformer), and long-range (MAF) dependencies. A multi-scale attention-based CNN decoder reconstructs segmentation maps; a co-attention gate enhances cross-scale feature interactions during both encoding and decoding.

Result: Outperforms state-of-the-art methods in accuracy and generalization across multiple benchmark datasets while maintaining comparable computational complexity.

Conclusion: Presents a practical, scalable, and generalizable segmentation architecture suitable for diverse medical imaging tasks; code and models will be released to support reproducibility.

Abstract: In recent years, deep learning has shown near-expert performance in
segmenting complex medical tissues and tumors. However, existing models are
often task-specific, with performance varying across modalities and anatomical
regions. Balancing model complexity and performance remains challenging,
particularly in clinical settings where both accuracy and efficiency are
critical. To address these issues, we propose a hybrid segmentation
architecture featuring a three-branch encoder that integrates CNNs,
Transformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture
local, global, and long-range dependencies. A multi-scale attention-based CNN
decoder reconstructs fine-grained segmentation maps while preserving contextual
consistency. Additionally, a co-attention gate enhances feature selection by
emphasizing relevant spatial and semantic information across scales during both
encoding and decoding, improving feature interaction and cross-scale
communication. Extensive experiments on multiple benchmark datasets show that
our approach outperforms state-of-the-art methods in accuracy and
generalization, while maintaining comparable computational complexity. By
effectively balancing efficiency and effectiveness, our architecture offers a
practical and scalable solution for diverse medical imaging tasks. Source code
and trained models will be publicly released upon acceptance to support
reproducibility and further research.

</details>


### [52] [Road Damage and Manhole Detection using Deep Learning for Smart Cities: A Polygonal Annotation Approach](https://arxiv.org/abs/2510.03797)
*Rasel Hossen,Diptajoy Mistry,Mushiur Rahman,Waki As Sami Atikur Rahman Hridoy,Sajib Saha,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: A YOLOv9-based detector with polygonal annotations for road damage and manholes, trained on ~1000+ Dhaka images; achieves 78.1% image-level accuracy with high F1 for Broken (86.7) and Not Broken (89.2) but poor Manhole detection (18.2) due to class imbalance, suggesting potential for scalable urban infrastructure monitoring in developing countries.


<details>
  <summary>Details</summary>
Motivation: Manual road-asset monitoring is costly, time-consuming, and error-prone; precise localization is needed for urban safety and maintenance; polygonal annotations aim to improve localization accuracy for road defects and manholes.

Method: Train a YOLOv9-based model using polygonal annotations on a dataset of 1k+ images from Dhaka, with three classes: Broken, Not Broken, and Manhole; evaluate with image-level accuracy and per-class F1-scores.

Result: 78.1% image-level accuracy; F1: Broken 86.7%, Not Broken 89.2%, Manhole 18.2%; performance limited by class imbalance for Manhole.

Conclusion: The approach provides an efficient, scalable framework for urban infrastructure monitoring in developing countries; polygonal localization improves precision; further work needed to address Manhole class imbalance and data diversity.

Abstract: Urban safety and infrastructure maintenance are critical components of smart
city development. Manual monitoring of road damages is time-consuming, highly
costly, and error-prone. This paper presents a deep learning approach for
automated road damage and manhole detection using the YOLOv9 algorithm with
polygonal annotations. Unlike traditional bounding box annotation, we employ
polygonal annotations for more precise localization of road defects. We develop
a novel dataset comprising more than one thousand images which are mostly
collected from Dhaka, Bangladesh. This dataset is used to train a YOLO-based
model for three classes, namely Broken, Not Broken, and Manhole. We achieve
78.1% overall image-level accuracy. The YOLOv9 model demonstrates strong
performance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score)
classes, with challenges in Manhole detection (18.2% F1-score) due to class
imbalance. Our approach offers an efficient and scalable solution for
monitoring urban infrastructure in developing countries.

</details>


### [53] [Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation](https://arxiv.org/abs/2510.03821)
*Venkata Narendra Kotyada,Revanth Eranki,Nagesh Bhattu Sristy*

Main category: cs.CV

TL;DR: A diffusion-based unpaired image-to-image translation method that uses time-dependent contrastive learning to preserve domain-invariant features and guide a pre-trained SDE, achieving competitive results with faster convergence and without labels.


<details>
  <summary>Details</summary>
Motivation: Unpaired I2I lacks aligned data; diffusion models (SDEs) offer high-fidelity, diverse generation; contrastive learning can enforce semantic consistency without supervision; combining them aims to preserve semantic structure while translating across domains efficiently.

Method: Train with time-dependent contrastive learning inspired by SimCLR, treating an image and its domain-invariant feature as a positive pair to preserve domain-invariant features and discard domain-specific ones. The learned contrastive model guides inference of a pretrained SDE for I2I translation across three unpaired tasks with four evaluation metrics.

Result: The proposed method (Contrastive-SDE) achieves comparable performance to state-of-the-art baselines on several metrics, converges much faster, and requires no label supervision or classifier training, making it a more efficient approach for unpaired I2I translation.

Conclusion: Leveraging contrastive learning to steer diffusion-based unpaired I2I translation yields competitive results without supervision and with faster convergence, highlighting the benefit of aligning representations via domain-invariant contrasts to guide generative inference.

Abstract: Unpaired image-to-image translation involves learning mappings between source
domain and target domain in the absence of aligned or corresponding samples.
Score based diffusion models have demonstrated state-of-the-art performance in
generative tasks. Their ability to approximate complex data distributions
through stochastic differential equations (SDEs) enables them to generate
high-fidelity and diverse outputs, making them particularly well-suited for
unpaired I2I settings. In parallel, contrastive learning provides a powerful
framework for learning semantic similarities without the need for explicit
supervision or paired data. By pulling together representations of semantically
similar samples and pushing apart dissimilar ones, contrastive methods are
inherently aligned with the objectives of unpaired translation. Its ability to
selectively enforce semantic consistency at the feature level makes contrastive
learning particularly effective for guiding generation in unpaired scenarios.
In this work, we propose a time-dependent contrastive learning approach where a
model is trained with SimCLR by considering an image and its domain invarient
feature as a positive pair, enabling the preservation of domain-invariant
features and the discarding of domain-specific ones. The learned contrastive
model then guides the inference of a pretrained SDE for the I2I translation
task. We empirically compare Contrastive-SDE with several baselines across
three common unpaired I2I tasks, using four metrics for evaluation.
Constrastive-SDE achieves comparable results to the state-of-the-art on several
metrics. Furthermore, we observe that our model converges significantly faster
and requires no label supervision or classifier training, making it a more
efficient alternative for this task.

</details>


### [54] [LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization](https://arxiv.org/abs/2510.03827)
*Xueyang Zhou,Yangming Xu,Guiyao Tie,Yongchao Chen,Guowen Zhang,Duanfeng Chu,Pan Zhou,Lichao Sun*

Main category: cs.CV

TL;DR: LIBERO-PRO extends LIBERO to systematically test VLA models under perturbations across objects, states, instructions, and environments; standard LIBERO accuracy is high (over 90%) but generalization under perturbations collapses to near zero, revealing memorization rather than understanding; calls for robust evaluation and provides code.


<details>
  <summary>Details</summary>
Motivation: Current LIBERO evaluation settings enable memorization-based successes and unfair comparisons; there is a need for robust, generalization-focused benchmarks for Vision-Language-Action (VLA) models.

Method: Introduce LIBERO-PRO with four perturbation dimensions (manipulated objects, initial states, task instructions, environments) to evaluate model performance under generalized settings; compare results with the standard LIBERO benchmark and analyze failure modes.

Result: Models achieve >90% accuracy on standard LIBERO but drop to near 0% under LIBERO-PRO perturbations, indicating reliance on rote memorization and lack of genuine task understanding or environmental perception; examples include unchanged actions when targets are replaced with irrelevant items and unchanged outputs to corrupted instructions.

Conclusion: Current evaluation practices are flawed; robust, perturbation-based assessments are needed to measure true generalization and comprehension in VLA models; LIBERO-PRO provides a more reliable benchmark; code is available at the provided GitHub link.

Abstract: LIBERO has emerged as a widely adopted benchmark for evaluating
Vision-Language-Action (VLA) models; however, its current training and
evaluation settings are problematic, often leading to inflated performance
estimates and preventing fair model comparison. To address these issues, we
introduce LIBERO-PRO, an extended LIBERO benchmark that systematically
evaluates model performance under reasonable perturbations across four
dimensions: manipulated objects, initial states, task instructions, and
environments. Experimental results reveal that, although existing models
achieve over 90% accuracy under the standard LIBERO evaluation, their
performance collapses to 0.0% under our generalized setting. Crucially, this
discrepancy exposes the models' reliance on rote memorization of action
sequences and environment layouts from the training set, rather than genuine
task understanding or environmental perception. For instance, models persist in
executing grasping actions when the target object is replaced with irrelevant
items, and their outputs remain unchanged even when given corrupted
instructions or even messy tokens. These findings expose the severe flaws in
current evaluation practices, and we call on the community to abandon
misleading methodologies in favor of robust assessments of model generalization
and comprehension. Our code is available at:
https://github.com/Zxy-MLlab/LIBERO-PRO.

</details>


### [55] [Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models](https://arxiv.org/abs/2510.03840)
*Pranav Sharma,Shivank Garg,Durga Toshniwal*

Main category: cs.CV

TL;DR: Mirage exposes gaps in AI-generated image detection: a curated dataset of artifact-rich images that baffle detectors, while LVLMs can aid detection when artifacts are present but struggle with artifact-free cases.


<details>
  <summary>Details</summary>
Motivation: To understand why humans can spot artifacts that state-of-the-art detectors miss, and to assess whether LVLMs can provide explainable AI-based detection for AI-generated imagery.

Method: Construct Mirage, a diverse dataset of AI-generated images with visible artifacts; evaluate current detectors on Mirage and benchmarks; test Large Vision-Language Models for detection and explanation on Mirage and existing datasets.

Result: Detectors largely fail on Mirage; LVLMs achieve high detection/explanation performance for images with artifacts but their accuracy drops for images lacking visible cues; Mirage highlights limitations of current detection approaches.

Conclusion: There is a persistent gap between human and machine detection in artifact-free cases; LVLMs offer explainable cues when artifacts exist, but robust, artifact-free detectors are still needed; Mirage provides a valuable benchmark for developing and evaluating such methods.

Abstract: Recent advances in image generation models have led to models that produce
synthetic images that are increasingly difficult for standard AI detectors to
identify, even though they often remain distinguishable by humans. To identify
this discrepancy, we introduce \textbf{Mirage}, a curated dataset comprising a
diverse range of AI-generated images exhibiting visible artifacts, where
current state-of-the-art detection methods largely fail. Furthermore, we
investigate whether Large Vision-Language Models (LVLMs), which are
increasingly employed as substitutes for human judgment in various tasks, can
be leveraged for explainable AI image detection. Our experiments on both Mirage
and existing benchmark datasets demonstrate that while LVLMs are highly
effective at detecting AI-generated images with visible artifacts, their
performance declines when confronted with images lacking such cues.

</details>


### [56] [UGround: Towards Unified Visual Grounding with Unrolled Transformers](https://arxiv.org/abs/2510.03853)
*Rui Qian,Xin Yin,Chuanhang Deng,Zhiyuan Peng,Jian Xiong,Wei Zhai,Dejing Dou*

Main category: cs.CV

TL;DR: UGround proposes a unified visual grounding framework that dynamically selects intermediate layers in unrolled transformers as a 'mask as prompt', addressing errors from fixed last-layer prompts and lacking explicit spatial cues from <SEG> prompts.


<details>
  <summary>Details</summary>
Motivation: The prevailing paradigm uses a fixed last hidden layer and <SEG> as a prompt, which propagates errors and lacks explicit spatial guidance. UGround aims to unify diverse grounding tasks and provide explicit spatial prompting through a learned, layer-aware masking strategy.

Method: Policy-Prompted Masking comprises two components: (1) Stochastic Skip Connection (SSC) via reinforcement learning to dynamically route each <SEG> token through unrolled transformer layers to determine where it connects to the vision model; (2) Mask as Prompt (MasP) that, using the chosen hidden layer, constructs a soft logit mask from the similarity between the <SEG> token and image tokens to prompt SAM for mask generation, thus providing explicit spatial cues.

Result: Conceptual framework with claims of unified grounding across tasks (traditional refer-expression segmentation to reasoning segmentation; single- to multi-target; positive query to empty target). It includes a public code release. No quantitative results reported in the abstract.

Conclusion: UGround offers a unified, layer-aware prompting mechanism that (i) mitigates error accumulation from fixed-layer prompts and (ii) injects explicit spatial cues into prompting via the MasP mask, enabling a broader scope of visual grounding tasks within a single framework.

Abstract: We present UGround, a \textbf{U}nified visual \textbf{Ground}ing paradigm
that dynamically selects intermediate layers across \textbf{U}nrolled
transformers as ``mask as prompt'', diverging from the prevailing pipeline that
leverages the fixed last hidden layer as ``\texttt{<SEG>} as prompt''. UGround
addresses two primary challenges posed by the prevailing paradigm: (1) its
reliance on the fixed last hidden layer, which sequentially amplifies
cumulative errors arising from layer-by-layer propagation without intermediate
correction, and (2) its use of \texttt{<SEG>} as a prompt, which implicitly
projects textual embeddings into visual space without explicit spatial cues
(\eg, coordinates). Central to UGround is Policy-Prompted Masking, which
comprises two key components: Stochastic Skip Connection (SSC) and Mask as
Prompt (MasP). SSC is a reinforcement learning policy that, via stochastic
sampling, allows each \texttt{<SEG>} token to slide across unrolled transformer
layers, enabling dynamic layer selection at which it connects to the vision
model (\eg, SAM) in a skip-connection fashion. Given the selected hidden layer,
MasP uses the similarity map derived from the \texttt{<SEG>} token and image
tokens as a soft logit mask to prompt SAM for mask generation, offering
explicit spatial cues through its activation regions. To validate the
effectiveness of UGround, we, for the first time, have unified visual grounding
within a single framework from an attribute perspective, spanning from
traditional refer expression segmentation to newly proposed reasoning
segmentation, single-target to multi-target, positive query to false premise
(empty target). All codes and models are publicly available at
\href{https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround}.

</details>


### [57] [Optimized Minimal 4D Gaussian Splatting](https://arxiv.org/abs/2510.03857)
*Minseo Lee,Byeonghyeon Lee,Lucas Yunkyu Lee,Eunsoo Lee,Sangmin Kim,Seunghyeon Song,Joo Chan Lee,Jong Hwan Ko,Jaesik Park,Eunbyung Park*

Main category: cs.CV

TL;DR: OMG4 trims 4D Gaussian Splatting to a compact set of salient Gaussians via a 3-stage pruning pipeline plus SVQ-based compression, achieving >60% size reduction with preserved quality.


<details>
  <summary>Details</summary>
Motivation: 4D Gaussian Splatting suffers large storage overhead due to millions of Gaussians; a compact representation is needed for real-time rendering and scalability.

Method: Three-stage pruning: (1) Gaussian Sampling to identify critical primitives, (2) Gaussian Pruning to remove redundancies, (3) Gaussian Merging to fuse similar primitives; plus implicit appearance compression and extending Sub-Vector Quantization (SVQ) to 4D; evaluation on standard benchmarks.

Result: Outperforms recent state-of-the-art methods; reduces model sizes by over 60% while maintaining reconstruction quality.

Conclusion:  OMG4 advances compact 4D scene representation, enabling efficient storage with broad applications; source code available.

Abstract: 4D Gaussian Splatting has emerged as a new paradigm for dynamic scene
representation, enabling real-time rendering of scenes with complex motions.
However, it faces a major challenge of storage overhead, as millions of
Gaussians are required for high-fidelity reconstruction. While several studies
have attempted to alleviate this memory burden, they still face limitations in
compression ratio or visual quality. In this work, we present OMG4 (Optimized
Minimal 4D Gaussian Splatting), a framework that constructs a compact set of
salient Gaussians capable of faithfully representing 4D Gaussian models. Our
method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to
identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning
to remove redundancies, and (3) Gaussian Merging to fuse primitives with
similar characteristics. In addition, we integrate implicit appearance
compression and generalize Sub-Vector Quantization (SVQ) to 4D representations,
further reducing storage while preserving quality. Extensive experiments on
standard benchmark datasets demonstrate that OMG4 significantly outperforms
recent state-of-the-art methods, reducing model sizes by over 60% while
maintaining reconstruction quality. These results position OMG4 as a
significant step forward in compact 4D scene representation, opening new
possibilities for a wide range of applications. Our source code is available at
https://minshirley.github.io/OMG4/.

</details>


### [58] [Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert](https://arxiv.org/abs/2510.03896)
*Mingyu Liu,Zheng Huang,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Yating Wang,Haoyi Zhu,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: A generalizable action expert bridges VLM planning and low-level action using sparse 3D trajectories and a novel Action Pre-training, Pointcloud Fine-tuning paradigm.


<details>
  <summary>Details</summary>
Motivation: Vision-Language-Action models struggle to generalize to the physical world due to monolithic architectures and semantic ambiguity in the action module; scalable, cross-task training is hindered, necessitating fine-tuning for new environments.

Method: Plan with a Vision-Language Model to generate coarse 3D waypoints; an action expert refines these into dense, executable action sequences by sampling real-time point cloud data; introduce Action Pre-training and Pointcloud Fine-tuning to promote training efficiency and generalization.

Result: Proposes a novel framework and training paradigm that decouples planning and action, using sparse 3D trajectories as an intermediate representation and an action expert to produce dense actions, aimed at improving cross-environment generalization and data efficiency.

Conclusion: The framework offers a scalable path to generalizable visuomotor control by bridging high-level planning and low-level action through an explicit intermediate representation and specialized training, potentially reducing the need for extensive retraining in new environments.

Abstract: Although Vision-Language Models (VLM) have demonstrated impressive planning
and reasoning capabilities, translating these abilities into the physical world
introduces significant challenges. Conventional Vision-Language-Action (VLA)
models, which integrate reasoning and action into a monolithic architecture,
generalize poorly because they are constrained by scarce, narrow-domain data.
While recent dual-system approaches attempt to decouple "thinking" from
"acting", they are often constrained by semantic ambiguities within the action
module. This ambiguity makes large-scale, cross-task training infeasible.
Consequently, these systems typically necessitate fine-tuning on newly
collected data when deployed to novel environments, and the cooperation
mechanism between the two systems remains ill-defined. To address these
limitations, we introduce, for the first time, a framework centered around a
generalizable action expert. Our approach utilizes sparse 3D trajectories as an
intermediate representation, effectively bridging the high-level planning
capabilities of the VLM with the low-level physical action module. During the
planning phase, the VLM is only required to generate coarse 3D waypoints. These
waypoints are then processed by our generalizable action expert, which refines
them into dense, executable action sequences by sampling real-time point cloud
observations of the environment. To promote training efficiency and robust
generalization, we introduce a novel "Action Pre-training, Pointcloud
Fine-tuning" paradigm. Our method combines the broad generalization
capabilities of VLMs in visual understanding and planning with the
fine-grained, action-level generalization of action expert.

</details>


### [59] [Cross-View Open-Vocabulary Object Detection in Aerial Imagery](https://arxiv.org/abs/2510.03858)
*Jyoti Kini,Rohit Gupta,Mubarak Shah*

Main category: cs.CV

TL;DR: A framework that adapts open-vocabulary detectors trained on ground-view data to aerial imagery via structured domain alignment, using contrastive image-to-image alignment and multi-instance vocabulary associations, achieving notable zero-shot mAP gains across multiple aerial datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional detectors are fixed to a closed set of classes, which limits flexibility and makes incorporating new categories costly. Open-vocabulary object detection leverages pretrained ground-view image-text models, but transferring them across domains (ground-view to aerial) suffers from domain shifts, viewpoint changes, and scale differences, necessitating specialized adaptation strategies.

Method: Introduce a novel framework for adapting open-vocabulary representations from ground-view to aerial imagery through structured domain alignment. Key components include (1) contrastive image-to-image alignment to align aerial and ground-view embeddings, and (2) multi-instance vocabulary associations to align aerial images with text embeddings. Extensive evaluation uses open-vocabulary detection with ground-view pretrained models and applies domain adaptation techniques across aerial datasets.

Result: Extensive experiments on xView, DOTAv2, VisDrone, DIOR, and HRRSD show substantial zero-shot gains: +6.32 mAP on DOTAv2, +4.16 mAP on VisDrone (Images), and +3.46 mAP on HRRSD, compared to finetuned closed-vocabulary, dataset-specific baselines, demonstrating effective cross-domain transfer and improved flexibility.

Conclusion: Structured domain alignment enables open-vocabulary object detection in aerial imagery by bridging ground-view and aerial representations, paving the way for more flexible and scalable open-vocabulary detectors in aerial applications.

Abstract: Traditional object detection models are typically trained on a fixed set of
classes, limiting their flexibility and making it costly to incorporate new
categories. Open-vocabulary object detection addresses this limitation by
enabling models to identify unseen classes without explicit training.
Leveraging pretrained models contrastively trained on abundantly available
ground-view image-text classification pairs provides a strong foundation for
open-vocabulary object detection in aerial imagery. Domain shifts, viewpoint
variations, and extreme scale differences make direct knowledge transfer across
domains ineffective, requiring specialized adaptation strategies. In this
paper, we propose a novel framework for adapting open-vocabulary
representations from ground-view images to solve object detection in aerial
imagery through structured domain alignment. The method introduces contrastive
image-to-image alignment to enhance the similarity between aerial and
ground-view embeddings and employs multi-instance vocabulary associations to
align aerial images with text embeddings. Extensive experiments on the xView,
DOTAv2, VisDrone, DIOR, and HRRSD datasets are used to validate our approach.
Our open-vocabulary model achieves improvements of +6.32 mAP on DOTAv2, +4.16
mAP on VisDrone (Images), and +3.46 mAP on HRRSD in the zero-shot setting when
compared to finetuned closed-vocabulary dataset-specific model performance,
thus paving the way for more flexible and scalable object detection systems in
aerial applications.

</details>


### [60] [OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications](https://arxiv.org/abs/2510.03915)
*Sagar Bharadwaj,Harrison Williams,Luke Wang,Michael Liang,Tao Jin,Srinivasan Seshan,Anthony Rowe*

Main category: cs.CV

TL;DR: Federated OpenFLAME enables privacy-preserving, scalable VPS for AR by letting organizations 3D-scan and host their own indoor spaces, with federated image-based localization and data-merging across maps without sharing private data.


<details>
  <summary>Details</summary>
Motivation: Centralized VPSs struggle to cover private indoor spaces due to privacy, regulatory, and maintenance bottlenecks. A federated approach allows organizations to own and maintain their spaces while enabling broad AR localization.

Method: Propose OpenFLAME: a federated VPS backend with map sharding, access control, and federated image-based localization. Provides reference solutions for managing and merging data across maps without sharing private content, addressing coherency, quality control, and service selection.

Result: Conceptual framework and design principles are presented; the abstract does not report empirical results, but outlines the architecture, data-management strategies, and challenges to address.

Conclusion: A federated VPS approach can enable privacy-preserving, scalable AR localization with expanded coverage, while highlighting open challenges such as cross-map coherency, quality control, and optimal service selection.

Abstract: World-scale augmented reality (AR) applications need a ubiquitous 6DoF
localization backend to anchor content to the real world consistently across
devices. Large organizations such as Google and Niantic are 3D scanning outdoor
public spaces in order to build their own Visual Positioning Systems (VPS).
These centralized VPS solutions fail to meet the needs of many future AR
applications -- they do not cover private indoor spaces because of privacy
concerns, regulations, and the labor bottleneck of updating and maintaining 3D
scans. In this paper, we present OpenFLAME, a federated VPS backend that allows
independent organizations to 3D scan and maintain a separate VPS service for
their own spaces. This enables access control of indoor 3D scans, distributed
maintenance of the VPS backend, and encourages larger coverage. Sharding of VPS
services introduces several unique challenges -- coherency of localization
results across spaces, quality control of VPS services, selection of the right
VPS service for a location, and many others. We introduce the concept of
federated image-based localization and provide reference solutions for managing
and merging data across maps without sharing private data.

</details>


### [61] [Exploring the Challenge and Value of Deep Learning in Automated Skin Disease Diagnosis](https://arxiv.org/abs/2510.03869)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: A systematic review using PRISMA summarizes deep learning progress in skin cancer detection, highlighting promise and key challenges (noise, variation, data imbalance) and surveying strategies like data augmentation, hybrid models, and feature fusion, with emphasis on clinical integration and a call for further research.


<details>
  <summary>Details</summary>
Motivation: Skin cancer is highly prevalent and deadly; early detection greatly improves outcomes. Deep learning offers potential to improve accuracy and efficiency in dermoscopic image analysis and automated diagnosis, bridging research with clinical practice.

Method: A systematic review following the PRISMA framework that synthesizes recent deep learning studies on skin cancer detection/diagnosis, categorizing approaches (data augmentation, hybrid models, feature fusion) and discussing clinical workflow integration and validation needs.

Result: DL-based skin cancer detection shows promise in improved accuracy and efficiency, but persistent challenges remain (noise, intra-/inter-class variation, data imbalance). Innovative strategies (augmentation, hybrids, feature fusion) help mitigate issues, and integration into clinical workflows is feasible with proper validation and standards.

Conclusion: Continued advancements are needed: robust, interpretable models; diverse and larger datasets; standardized evaluation; clinical validation; regulatory considerations; and seamless integration into dermatology care pathways.

Abstract: Skin cancer is one of the most prevalent and deadly forms of cancer
worldwide, which highlights the critical importance of early detection and
diagnosis in improving patient outcomes. Deep learning (DL) has shown
significant promise in enhancing the accuracy and efficiency of automated skin
disease diagnosis, particularly in detecting and evaluating skin lesions and
classification. However, there are still several challenges for DL-based skin
cancer diagnosis, including complex features, image noise, intra-class
variation, inter-class similarity, and data imbalance. By synthesizing recent
research, this review discusses innovative approaches to cope with these
challenges, such as data augmentation, hybrid models, and feature fusion, etc.
Furthermore, the review highlights the integration of DL models into clinical
workflows, offering insights into the potential of deep learning to
revolutionize skin disease diagnosis and improve clinical decision-making. This
article follows a comprehensive methodology based on the PRISMA framework and
emphasizes the need for continued advancements to fully unlock the
transformative potential of DL in dermatological care.

</details>


### [62] [SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks](https://arxiv.org/abs/2510.03870)
*Nikolaos Kaparinos,Vasileios Mezaris*

Main category: cs.CV

TL;DR: SDAKD introduces a student discriminator to tackle capacity mismatch during GAN distillation. It uses a three-stage training procedure with adapted feature map distillation in the final two stages, and reports improvements on GCFSR and Real-ESRGAN over baselines and SOTA KD methods; code will be released.


<details>
  <summary>Details</summary>
Motivation: GANs deliver high-quality images but are heavy; distilling GANs is hard due to capacity gap between compact student generator and powerful teacher discriminator; there is a need to align the capacities of student and teacher components while preserving performance.

Method: Introduce a student discriminator to accompany the student generator during distillation to mitigate capacity mismatch. Employ a three-stage training strategy, with the last two stages incorporating an adapted feature map distillation to transfer intermediate representations from the teacher to the student.

Result: The approach yields consistent improvements over baselines and state-of-the-art GAN knowledge distillation methods on two strong SR-GANs (GCFSR and Real-ESRGAN). The code will be released upon acceptance.

Conclusion: SDAKD effectively mitigates capacity mismatch in GAN distillation and enables better performance for compact GANs, improving practicality for deployment on resource-constrained devices.

Abstract: Generative Adversarial Networks (GANs) achieve excellent performance in
generative tasks, such as image super-resolution, but their computational
requirements make difficult their deployment on resource-constrained devices.
While knowledge distillation is a promising research direction for GAN
compression, effectively training a smaller student generator is challenging
due to the capacity mismatch between the student generator and the teacher
discriminator. In this work, we propose Student Discriminator Assisted
Knowledge Distillation (SDAKD), a novel GAN distillation methodology that
introduces a student discriminator to mitigate this capacity mismatch. SDAKD
follows a three-stage training strategy, and integrates an adapted feature map
distillation approach in its last two training stages. We evaluated SDAKD on
two well-performing super-resolution GANs, GCFSR and Real-ESRGAN. Our
experiments demonstrate consistent improvements over the baselines and SOTA GAN
knowledge distillation methods. The SDAKD source code will be made openly
available upon acceptance of the paper.

</details>


### [63] [RAP: 3D Rasterization Augmented End-to-End Planning](https://arxiv.org/abs/2510.04333)
*Lan Feng,Yang Gao,Eloi Zablocki,Quanyi Li,Wuyang Li,Sichao Liu,Matthieu Cord,Alexandre Alahi*

Main category: cs.CV

TL;DR: RAP uses 3D rasterization and feature-space alignment to augment end-to-end driving training, enabling counterfactual views without photorealistic rendering, achieving SOTA robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: Imitation learning for E2E driving lacks recovery data; photorealistic simulators are costly; need semantic fidelity and scalable augmentation.

Method: 3D Rasterization of annotated primitives; generate counterfactual maneuvers and cross-agent views; Raster-to-Real feature-space alignment; integrate into planning pipeline (RAP).

Result: Achieves state-of-the-art closed-loop robustness and long-tail generalization; ranks first on NAVSIM v1/v2, Waymo Open Dataset Vision-based E2E Driving, Bench2Drive.

Conclusion: Photorealism not necessary; semantic fidelity and scalability suffice; RAP provides scalable, practical alternative to photorealistic rendering for E2E training; promising for planning.

Abstract: Imitation learning for end-to-end driving trains policies only on expert
demonstrations. Once deployed in a closed loop, such policies lack recovery
data: small mistakes cannot be corrected and quickly compound into failures. A
promising direction is to generate alternative viewpoints and trajectories
beyond the logged path. Prior work explores photorealistic digital twins via
neural rendering or game engines, but these methods are prohibitively slow and
costly, and thus mainly used for evaluation. In this work, we argue that
photorealism is unnecessary for training end-to-end planners. What matters is
semantic fidelity and scalability: driving depends on geometry and dynamics,
not textures or lighting. Motivated by this, we propose 3D Rasterization, which
replaces costly rendering with lightweight rasterization of annotated
primitives, enabling augmentations such as counterfactual recovery maneuvers
and cross-agent view synthesis. To transfer these synthetic views effectively
to real-world deployment, we introduce a Raster-to-Real feature-space alignment
that bridges the sim-to-real gap. Together, these components form Rasterization
Augmented Planning (RAP), a scalable data augmentation pipeline for planning.
RAP achieves state-of-the-art closed-loop robustness and long-tail
generalization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo
Open Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that
lightweight rasterization with feature alignment suffices to scale E2E
training, offering a practical alternative to photorealistic rendering. Project
page: https://alan-lanfeng.github.io/RAP/.

</details>


### [64] [PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis](https://arxiv.org/abs/2510.03873)
*Saja Al-Dabet,Sherzod Turaev,Nazar Zaki,Arif O. Khan,Luai Eldweik*

Main category: cs.CV

TL;DR: PoseGaze-AHP introduces a 3D, synchronized head-pose and gaze dataset for ocular-induced abnormal head posture diagnosis, generated from two head textures (n=7,920 images) using LLM-driven data extraction and the Neural Head Avatar pipeline; claims 91.92% extraction accuracy and public availability for AI-based diagnostic development.


<details>
  <summary>Details</summary>
Motivation: Existing datasets isolate head pose or gaze, hindering integrated AI approaches for ocular-induced AHP. A unified, privacy-conscious dataset is needed to enable AI-driven diagnostics and better clinical insights.

Method: Structured clinical data were extracted from medical literature using large language models (Claude 3.5 Sonnet) with iterative, hierarchical prompting. Records were imputed, then transformed into 3D representations via the Neural Head Avatar (NHA) framework, producing 7,920 images from two head textures.

Result: Extraction approach achieved 91.92% accuracy. PoseGaze-AHP is presented as the first publicly available resource tailored for AI-driven ocular-induced AHP diagnosis.

Conclusion: PoseGaze-AHP provides a publicly accessible, privacy-conscious 3D dataset integrating head pose and gaze for AI-based AHP diagnosis, enabling development of more integrated diagnostic tools.

Abstract: Diagnosing ocular-induced abnormal head posture (AHP) requires a
comprehensive analysis of both head pose and ocular movements. However,
existing datasets focus on these aspects separately, limiting the development
of integrated diagnostic approaches and restricting AI-driven advancements in
AHP analysis. To address this gap, we introduce PoseGaze-AHP, a novel 3D
dataset that synchronously captures head pose and gaze movement information for
ocular-induced AHP assessment. Structured clinical data were extracted from
medical literature using large language models (LLMs) through an iterative
process with the Claude 3.5 Sonnet model, combining stepwise, hierarchical, and
complex prompting strategies. The extracted records were systematically imputed
and transformed into 3D representations using the Neural Head Avatar (NHA)
framework. The dataset includes 7,920 images generated from two head textures,
covering a broad spectrum of ocular conditions. The extraction method achieved
an overall accuracy of 91.92%, demonstrating its reliability for clinical
dataset construction. PoseGaze-AHP is the first publicly available resource
tailored for AI-driven ocular-induced AHP diagnosis, supporting the development
of accurate and privacy-compliant diagnostic tools.

</details>


### [65] [DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human](https://arxiv.org/abs/2510.03874)
*Yunhao Li,Sijing Wu,Yucheng Zhu,Huiyu Duan,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: Introduces DHQA-4D, a large-scale dynamic 4D human mesh quality dataset with 32 real-scanned sequences and 1920 distorted variants across 11 textured distortions; presents DynaMesh-Rater, a multimodal LMM-based quality assessor (textured and non-textured) that fuses 2D visual, motion, and geometry features using LoRA-tuned instruction prompts, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Quality assessment of dynamic 4D digital humans is essential because 4D meshes degrade during capture, compression, and transmission, affecting visual quality and user experience. There is a need for a scalable dataset and an effective model that can handle both textured and non-textured dynamic meshes.

Method: Create DHQA-4D: 32 real-scanned dynamic 4D human sequences with 1920 distorted meshes across 11 texture distortions, with textured and non-textured MOS. Develop DynaMesh-Rater, a large multimodal model that extracts: (1) 2D projected video visual features, (2) motion features from cropped video clips, (3) geometry features from the 4D mesh. Fuse via a large language model (LMM) with LoRA-based instruction tuning to predict quality scores.

Result: DynaMesh-Rater demonstrates superiority over previous quality assessment methods on DHQA-4D for both textured and non-textured meshes, and enables analysis of how different distortions affect human perception of dynamic 4D meshes.

Conclusion: A comprehensive DHQA-4D dataset plus a multimodal LMM-based quality assessor provides accurate, scalable quality evaluation for dynamic 4D human meshes and supports textured/non-textured scenarios, with broad applicability to games, animation, and remote communication.

Abstract: With the rapid development of 3D scanning and reconstruction technologies,
dynamic digital human avatars based on 4D meshes have become increasingly
popular. A high-precision dynamic digital human avatar can be applied to
various fields such as game production, animation generation, and remote
immersive communication. However, these 4D human avatar meshes are prone to
being degraded by various types of noise during the processes of collection,
compression, and transmission, thereby affecting the viewing experience of
users. In light of this fact, quality assessment of dynamic 4D digital humans
becomes increasingly important. In this paper, we first propose a large-scale
dynamic digital human quality assessment dataset, DHQA-4D, which contains 32
high-quality real-scanned 4D human mesh sequences, 1920 distorted textured 4D
human meshes degraded by 11 textured distortions, as well as their
corresponding textured and non-textured mean opinion scores (MOSs). Equipped
with DHQA-4D dataset, we analyze the influence of different types of distortion
on human perception for textured dynamic 4D meshes and non-textured dynamic 4D
meshes. Additionally, we propose DynaMesh-Rater, a novel large multimodal model
(LMM) based approach that is able to assess both textured 4D meshes and
non-textured 4D meshes. Concretely, DynaMesh-Rater elaborately extracts
multi-dimensional features, including visual features from a projected 2D
video, motion features from cropped video clips, and geometry features from the
4D human mesh to provide comprehensive quality-related information. Then we
utilize a LMM model to integrate the multi-dimensional features and conduct a
LoRA-based instruction tuning technique to teach the LMM model to predict the
quality scores. Extensive experimental results on the DHQA-4D dataset
demonstrate the superiority of our DynaMesh-Rater method over previous quality
assessment methods.

</details>


### [66] [Skin Lesion Classification Based on ResNet-50 Enhanced With Adaptive Spatial Feature Fusion](https://arxiv.org/abs/2510.03876)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: An ASFF-enhanced ResNet-50 improves dermoscopic skin cancer classification by fusing multi-scale features, achieving 93.18% accuracy and AUC ~0.967-0.972 on an ISIC-2020 subset, with Grad-CAM validation.


<details>
  <summary>Details</summary>
Motivation: Address challenges in skin cancer detection due to high inter-class similarity, intra-class variability, and noise in dermoscopic images; improve multi-scale feature representation and reduce overfitting via adaptive fusion.

Method: A dual-branch ASFF integrated into ResNet-50 that fuses high-level semantic and mid-level detail features, processed through global average pooling and fully connected layers to generate adaptive fusion weights for weighted feature fusion. Evaluation on a subset of ISIC 2020 (3297 images: benign vs malignant). Grad-CAM used for explainability.

Result: Outperforms 5 classic CNN models in overall performance; accuracy 93.18%; higher precision, recall, specificity, F1; AUC values: 0.9670 (PR) and 0.9717 (ROC). Grad-CAM shows the model focuses on lesion regions and suppresses background.

Conclusion: ASFF-based ResNet-50 provides a more effective and efficient solution for computer-aided skin cancer diagnosis by enhancing feature learning, reducing noise impact, and offering interpretable attention via Grad-CAM.

Abstract: Skin cancer classification remains a challenging problem due to high
inter-class similarity, intra-class variability, and image noise in dermoscopic
images. To address these issues, we propose an improved ResNet-50 model
enhanced with Adaptive Spatial Feature Fusion (ASFF), which adaptively
integrates multi-scale semantic and surface features to improve feature
representation and reduce overfitting. The ResNet-50 model is enhanced with an
adaptive feature fusion mechanism to achieve more effective multi-scale feature
extraction and improve overall performance. Specifically, a dual-branch design
fuses high-level semantic and mid-level detail features, which are processed
through global average pooling and fully connected layers to generate adaptive
weights for weighted fusion, thereby strengthening feature learning and
reducing the impact of noise on classification. The method is evaluated on a
subset of the ISIC 2020 dataset containing 3297 benign and malignant skin
lesion images. Experimental results show that the proposed ASFF-based ResNet-50
achieves the best overall performance compared with 5 classic convolutional
neural networks (CNNs) models. The proposed model reached an accuracy of 93.18%
along with higher precision, recall, specificity, and F1 score. The improved
model achieves an AUC value of 0.9670 and 0.9717 in the P-R and ROC curve,
respectively. Then, the evaluation based on Grad-CAM further proved that the
improved model adaptively focuses on lesion-relevant regions while suppressing
irrelevant background information, thereby validating its enhanced feature
learning capability from a deep representation perspective. These findings
demonstrate that the proposed approach provides a more effective and efficient
solution for computer-aided skin cancer diagnosis.

</details>


### [67] [Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks](https://arxiv.org/abs/2510.03878)
*Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R*

Main category: cs.CV

TL;DR: Multimodal Deep Learning ensemble using DenseNet-121 across radiology, clinical, and histopathology images improves early OSCC detection with an 84.58% multimodal accuracy; radiology and histopathology modalities performed best individually.


<details>
  <summary>Details</summary>
Motivation: Late detection of OSCC contributes to high mortality; there is a need for non-invasive AI-assisted triage that can integrate multiple imaging modalities to facilitate early diagnosis.

Method: Retrospective study using publicly available multimodal datasets (radiology, clinical, histopathology). Each modality trained with DenseNet-121 via transfer learning; applied augmentation and modality-specific preprocessing; predictions fused with a validation-weighted ensemble. Evaluation used accuracy, precision, recall, F1-score on samples.

Result: Radiology: 100% validation accuracy; Histopathology: 95.12%; Clinical: 63.10%. Ensemble accuracy: 84.58% on 55-sample multimodal validation set.

Conclusion: The framework offers a non-invasive, AI-assisted triage tool to aid early identification of high-risk OSCC lesions and support clinical decision-making, potentially reducing diagnostic delays and improving outcomes in line with oncology guidelines.

Abstract: Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes
significantly to its high global mortality rate, with over 50\% of cases
detected at advanced stages and a 5-year survival rate below 50\% according to
WHO statistics. This study aims to improve early detection of OSCC by
developing a multimodal deep learning framework that integrates clinical,
radiological, and histopathological images using a weighted ensemble of
DenseNet-121 convolutional neural networks (CNNs). Material and Methods A
retrospective study was conducted using publicly available datasets
representing three distinct medical imaging modalities. Each modality-specific
dataset was used to train a DenseNet-121 CNN via transfer learning.
Augmentation and modality-specific preprocessing were applied to increase
robustness. Predictions were fused using a validation-weighted ensemble
strategy. Evaluation was performed using accuracy, precision, recall, F1-score.
Results High validation accuracy was achieved for radiological (100\%) and
histopathological (95.12\%) modalities, with clinical images performing lower
(63.10\%) due to visual heterogeneity. The ensemble model demonstrated improved
diagnostic robustness with an overall accuracy of 84.58\% on a multimodal
validation dataset of 55 samples. Conclusion The multimodal ensemble framework
bridges gaps in the current diagnostic workflow by offering a non-invasive,
AI-assisted triage tool that enhances early identification of high-risk
lesions. It supports clinicians in decision-making, aligning with global
oncology guidelines to reduce diagnostic delays and improve patient outcomes.

</details>


### [68] [Exploring Instruction Data Quality for Explainable Image Quality Assessment](https://arxiv.org/abs/2510.03880)
*Yunhao Li,Sijing Wu,Huiyu Duan,Yucheng Zhu,Qi Jia,Guangtao Zhai*

Main category: cs.CV

TL;DR: Challenges the scaling law for instruction-tuning data in explainable IQA and shows data quality matters. A clustering-based data selection method (IQA-Select) can outperform full fine-tuning using only 10% data on two benchmarks, reducing costs.


<details>
  <summary>Details</summary>
Motivation: Explainable IQA methods often rely on large instruction-tuning datasets. This study questions whether more data always improves performance and investigates the impact of data quality and selection on model effectiveness, aiming to reduce computational costs without sacrificing accuracy.

Method: 1) Fine-tune a powerful pre-trained multimodal LLM with varying sizes of instruction-tuning data. 2) Compare random subset sampling against full data. 3) Propose IQA-Select, a three-stage clustering-based data selection framework: feature extraction, cluster quota allocation, and cluster sampling. 4) Analyze choices within each stage and validate on IQA benchmarks (Q-Bench, AesBench).

Result: Randomly selected subsets with an appropriate ratio can outperform full data fine-tuning, indicating redundancy in the data. IQA-Select with only 10% of data achieves 102.1% and 103.7% of full-data fine-tuning performance on Q-Bench and AesBench, respectively, reducing computational costs while improving or matching performance.

Conclusion: The study demonstrates that current explainable IQA instruction-tuning data is partially redundant and that data quality strategies can yield equal or better results than scaling up data. Clustering-based data selection (IQA-Select) is an effective approach to maintain or exceed full-data performance with far less data.

Abstract: In recent years, with the rapid development of powerful multimodal large
language models (MLLMs), explainable image quality assessment (IQA) has
gradually become popular, aiming at providing quality-related descriptions and
answers of images. To achieve this goal, recent methods seek to construct a
large-scale instruction tuning dataset to empower the MLLM with quality
perception ability following the well-known scaling law. However, a large
amount of instruction tuning data may cause substantial computational costs and
redundant data, which in turn will cause harm to the performance of the model.
To cope with this problem, in this paper, we challenge the scaling law and
systematically investigate the role of data quality of the instruction tuning
dataset for explainable IQA. Using a powerful pre-trained MLLM, we first
investigate the changes in model performance after fine-tuning with different
sizes of instruction tuning data. We find that selecting a subset of the data
set randomly using an appropriate ratio can even lead to better results than
training with the entire instruction tuning dataset, demonstrating the
redundancy of current explainable IQA instruction tuning data. Beyond randomly
sampling a subset, we propose a clustering-based data selection framework with
three stages: clustering feature extraction, cluster quota allocation, and
cluster sampling strategy. Then we systematically analyze the choices of each
stage and propose a simple but efficient data selection method IQA-Select for
explainable IQA. The experimental results demonstrate that IQA-Select can
achieve 102.1% and 103.7% performance of full fine-tuning using only 10%
selected data in Q-Bench and AesBench respectively, significantly reducing
computational costs while achieving better performance.

</details>


### [69] [Zero-Shot Fine-Grained Image Classification Using Large Vision-Language Models](https://arxiv.org/abs/2510.03903)
*Md. Atabuzzaman,Andrew Zhang,Chris Thomas*

Main category: cs.CV

TL;DR: Transforms zero-shot fine-grained image classification into a visual QA task using LVLMs, introduces an attention intervention, and new class-description benchmarks; achieves SOTA on multiple benchmarks; code available.


<details>
  <summary>Details</summary>
Motivation: Zero-shot fine-grained classification is hard due to subtle inter-class differences; LVLMs have rich visual-language understanding that can be leveraged beyond direct class-name generation; existing datasets lack precise class descriptions.

Method: Reframe classification as visual question answering with LVLMs; introduce attention intervention technique to improve focus; build more comprehensive class-description benchmarks; evaluate on several fine-grained datasets; provide code and datasets.

Result: Consistently outperforms current SOTA methods across multiple benchmarks, confirming effectiveness of the approach and LVLM capabilities in zero-shot fine-grained tasks.

Conclusion: LVLMs hold broader potential for zero-shot fine-grained classification; the proposed attention mechanism and richer benchmarks advance the field; resources are released to enable replication.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive performance
on vision-language reasoning tasks. However, their potential for zero-shot
fine-grained image classification, a challenging task requiring precise
differentiation between visually similar categories, remains underexplored. We
present a novel method that transforms zero-shot fine-grained image
classification into a visual question-answering framework, leveraging LVLMs'
comprehensive understanding capabilities rather than relying on direct class
name generation. We enhance model performance through a novel attention
intervention technique. We also address a key limitation in existing datasets
by developing more comprehensive and precise class description benchmarks. We
validate the effectiveness of our method through extensive experimentation
across multiple fine-grained image classification benchmarks. Our proposed
method consistently outperforms the current state-of-the-art (SOTA) approach,
demonstrating both the effectiveness of our method and the broader potential of
LVLMs for zero-shot fine-grained classification tasks. Code and Datasets:
https://github.com/Atabuzzaman/Fine-grained-classification

</details>


### [70] [From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance](https://arxiv.org/abs/2510.03906)
*Ardalan Aryashad,Parsa Razmara,Amin Mahjoub,Seyedarmin Azizi,Mahdi Salmani,Arad Firouzkouhi*

Main category: cs.CV

TL;DR: A broad, task-focused benchmark of defogging pipelines for autonomous driving, comparing classical filters, modern defog networks, chained variants, and VLM-based editors on Foggy Cityscapes; it evaluates image quality and downstream detection/segmentation (mAP, PQ/RQ/SQ) and analyzes when preprocessing helps, how chaining behaves, and how VLM judgments align with task metrics.


<details>
  <summary>Details</summary>
Motivation: Improvements in image fidelity from defogging do not always translate to better downstream perception. Real-world fog transferability is unclear, and prior evaluations rely on synthetic data. This study seeks a transparent, real-world–relevant benchmark across diverse pipelines to understand when preprocessing genuinely aids autonomous perception.

Method: Benchmark four pipeline families (classical filters, modern defogging networks, chained variants, and prompt-driven visual–language model editors) on Foggy Cityscapes. Assess both image quality and downstream task performance (object detection mAP; segmentation PQ, RQ, SQ). Examine chained variants and compare VLM-based editors to dedicated approaches. Include a qualitative VLM judge and analyze its rubric-based scores against task metrics.

Result: The study shows defogging helps in certain scenarios, while chaining can yield synergy or degrade performance depending on the combination. VLM-based editors can match or surpass dedicated methods in some settings. Rubric scores from the VLM judge correlate strongly with mAP, supporting lightweight qualitative assessment. Overall, the work reveals nuanced, task-oriented conditions under which preprocessing improves autonomous perception and provides a transparent benchmark framework.

Conclusion: The work establishes a transparent, task-oriented benchmark for defogging methods in autonomous driving, clarifying when preprocessing genuinely boosts perception in adverse weather and offering guidance on pipeline selection for real-world transferability.

Abstract: Autonomous driving perception systems are particularly vulnerable in foggy
conditions, where light scattering reduces contrast and obscures fine details
critical for safe operation. While numerous defogging methods exist-from
handcrafted filters to learned restoration models-improvements in image
fidelity do not consistently translate into better downstream detection and
segmentation. Moreover, prior evaluations often rely on synthetic data, leaving
questions about real-world transferability. We present a structured empirical
study that benchmarks a comprehensive set of pipelines, including (i) classical
filters, (ii) modern defogging networks, (iii) chained variants
(filter$\rightarrow$model, model$\rightarrow$filter), and (iv) prompt-driven
visual--language image editing models (VLM) applied directly to foggy images.
Using Foggy Cityscapes, we assess both image quality and downstream performance
on object detection (mAP) and segmentation (PQ, RQ, SQ). Our analysis reveals
when defogging helps, when chaining yields synergy or degradation, and how
VLM-based editors compare to dedicated approaches. In addition, we evaluate
qualitative rubric-based scores from a VLM judge and quantify their alignment
with task metrics, showing strong correlations with mAP. Together, these
results establish a transparent, task-oriented benchmark for defogging methods
and highlight the conditions under which preprocessing genuinely improves
autonomous perception in adverse weather.

</details>


### [71] [Generating Human Motion Videos using a Cascaded Text-to-Video Framework](https://arxiv.org/abs/2510.03909)
*Hyelin Nam,Hyojun Go,Byeongjun Park,Byung-Hoon Kim,Hyungjin Chung*

Main category: cs.CV

TL;DR: CAMEO is a cascaded framework that enables general human motion video generation by bridging text-to-motion models and conditional diffusion models, with a camera-aware conditioning module for viewpoint alignment.


<details>
  <summary>Details</summary>
Motivation: General-purpose human video generation with diffusion models is underexplored, often limited to image-to-video or narrow domains. There is a risk of misalignment between motion descriptions, conditioning signals, and generated video, necessitating more robust training, conditioning, and viewpoint handling.

Method: Create a cascaded pipeline that analyzes and prepares textual prompts and visual conditions for robust VDM training, and introduce a camera-aware conditioning module that connects T2M and VDM stages by automatically selecting viewpoints aligned with the input text. Evaluate on the MovieGen benchmark and a newly introduced T2M-VDM benchmark to demonstrate versatility across use cases.

Result: Demonstrates effectiveness on MovieGen and a new T2M-VDM benchmark, highlighting versatility and robustness of the approach across diverse human-motion video generation scenarios.

Conclusion: CAMEO enables general human motion video generation by effectively bridging T2M and conditional VDMs, incorporating alignment-focused prompting/conditioning and automatic viewpoint selection to reduce manual intervention and broaden applicable use cases.

Abstract: Human video generation is becoming an increasingly important task with broad
applications in graphics, entertainment, and embodied AI. Despite the rapid
progress of video diffusion models (VDMs), their use for general-purpose human
video generation remains underexplored, with most works constrained to
image-to-video setups or narrow domains like dance videos. In this work, we
propose CAMEO, a cascaded framework for general human motion video generation.
It seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs,
mitigating suboptimal factors that may arise in this process across both
training and inference through carefully designed components. Specifically, we
analyze and prepare both textual prompts and visual conditions to effectively
train the VDM, ensuring robust alignment between motion descriptions,
conditioning signals, and the generated videos. Furthermore, we introduce a
camera-aware conditioning module that connects the two stages, automatically
selecting viewpoints aligned with the input text to enhance coherence and
reduce manual intervention. We demonstrate the effectiveness of our approach on
both the MovieGen benchmark and a newly introduced benchmark tailored to the
T2M-VDM combination, while highlighting its versatility across diverse use
cases.

</details>


### [72] [Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition](https://arxiv.org/abs/2510.03921)
*Arushi Dashore,Aryan Anumala,Emily Hui,Olivia Yang*

Main category: cs.CV

TL;DR: CNN-LSTM-based biomechanical feature extraction combined with LLM-based feedback to produce actionable, biomechanically grounded tennis-stroke feedback; evaluated for classification and interpretability on THETIS data.


<details>
  <summary>Details</summary>
Motivation: Close the gap between biomechanical analysis and accessible, coach-friendly language feedback.

Method: Use CNN-LSTM to extract biomechanical features (joint angles, limb velocities, kinetic chain patterns) from motion data; analyze relationships to stroke effectiveness and injury risk; generate user-facing feedback via large language models; leverage THETIS dataset for feature extraction; assess classification accuracy and interpretability.

Result: No numeric results reported; framework is proposed and evaluated conceptually for classification and interpretability, demonstrating potential for explainable AI in sports biomechanics.

Conclusion: The framework aims to deliver technically accurate, biomechanically grounded, and actionable feedback to players and coaches, bridging explainable AI and sports biomechanics.

Abstract: Automated tennis stroke analysis has advanced significantly with the
integration of biomechanical motion cues alongside deep learning techniques,
enhancing stroke classification accuracy and player performance evaluation.
Despite these advancements, existing systems often fail to connect
biomechanical insights with actionable language feedback that is both
accessible and meaningful to players and coaches. This research project
addresses this gap by developing a novel framework that extracts key
biomechanical features (such as joint angles, limb velocities, and kinetic
chain patterns) from motion data using Convolutional Neural Network Long
Short-Term Memory (CNN-LSTM)-based models. These features are analyzed for
relationships influencing stroke effectiveness and injury risk, forming the
basis for feedback generation using large language models (LLMs). Leveraging
the THETIS dataset and feature extraction techniques, our approach aims to
produce feedback that is technically accurate, biomechanically grounded, and
actionable for end-users. The experimental setup evaluates this framework on
classification performance and interpretability, bridging the gap between
explainable AI and sports biomechanics.

</details>


### [73] [Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs](https://arxiv.org/abs/2510.03955)
*Sameep Vani,Shreyas Jena,Maitreya Patel,Chitta Baral,Somak Aditya,Yezhou Yang*

Main category: cs.CV

TL;DR: TimeWarp uses synthetic temporal data and a preference dataset to fine-tune Video-LLMs, significantly boosting temporal understanding across seven benchmarks.


<details>
  <summary>Details</summary>
Motivation: Video-LLMs excel at general tasks but miss fine-grained temporal reasoning due to limited visual-temporal diversity in fine-tuning data; a targeted dataset is needed to ground responses in video dynamics.

Method: Propose TimeWarp to systematically create a targeted synthetic temporal dataset; construct a large-scale preference dataset using TimeWarp to capture intricate temporal dynamics; fine-tune existing Video-LLMs with these datasets; release code.

Result: Applying TimeWarp yields significant improvements on temporal understanding benchmarks with absolute improvement across seven benchmarks.

Conclusion: Synthetic temporal data and preference-based fine-tuning are effective for advancing temporal understanding in Video-LLMs; public code is available.

Abstract: While Video Large Language Models (Video-LLMs) have demonstrated remarkable
performance across general video understanding benchmarks-particularly in video
captioning and descriptive tasks-they consistently underperform on tasks that
require fine-grained temporal understanding. This limitation arises due to the
lack of visual complexity and temporal nuance in current fine-tuning datasets,
leading these models to rely heavily on language-based reasoning rather than
truly understanding video dynamics. In this work, we propose TimeWarp, a
systematic method to create a targeted synthetic temporal dataset to fine-tune
the model's responses to encourage it to focus on the given input video. We
introduce a large-scale preference dataset, created using TimeWarp, that
captures intricate temporal dynamics often overlooked, grounding the model's
responses to visual and temporal information. We demonstrate that when our
method is applied to existing models, it significantly improves performance on
temporal understanding benchmarks, highlighting the effectiveness of our
proposed datasets in advancing temporal understanding in Video-LLMs, resulting
in an absolute improvement in performance across seven benchmarks. Code is
available at https://github.com/sameepv21/timewarp.

</details>


### [74] [No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models](https://arxiv.org/abs/2510.03978)
*Min Woo Sun,Alejandro Lozano,Javier Gamazo Tejero,Vishwesh Nath,Xiao Xiao Sun,James Burgess,Yuhui Zhang,Kun Yuan,Robert Tibshirani,Sean Huver,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: This work studies the effect of long-context pretraining for biomedical vision-language models, introducing a long-context dataset and model that significantly improves long-caption retrieval and classification.


<details>
  <summary>Details</summary>
Motivation: Biomedical captions in literature are often long (>77 tokens) and truncation wastes supervision; extending text context could improve learning and performance in VLMs.

Method: Construct BIOMEDICA-LongCAP with 1M image-caption pairs containing long-context descriptions from full-text articles; train BMC-LongCLIP with a text encoder supporting up to 512 tokens; evaluate on long-caption retrieval and classification, comparing against short-context baselines and reporting token-efficiency.

Result: Long-context model achieves up to +30% absolute Recall@1 gains and ~+2% average classification gains on long-caption tasks; token waste reduces from 55% to 2.2%; faster convergence than short-context models.

Conclusion: Extending context length is a promising direction for biomedical VLMs, enabling richer supervision and better performance in long-caption scenarios.

Abstract: Embedding vision-language models (VLMs) are typically pretrained with short
text windows (<77 tokens), which forces the truncation of long-format captions.
Yet, the distribution of biomedical captions from large-scale open source
literature reveals that a huge portion of captions far exceed 77 tokens. To
this end, we investigate the impact of pretraining on long-format biomedical
captions by extending the context length of text encoders in VLMs. We find that
longer context (thus, enabling additional supervision provided in long-format
captions) correlates with better retrieval and classification performance.
Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M
image-caption pairs enriched with context-aware descriptions from full-text
articles, providing longer and additional textual supervision. Using
BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a
text encoder supporting windows of up to 512 tokens. Our model extends context
capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption
retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in
Recall@1 and +2% average improvements in classification, while also converging
faster than short-context. Our results demonstrate that long-context modeling
is a promising direction for advancing biomedical VLMs.

</details>


### [75] [Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2510.03993)
*Yaxin Hou,Bo Han,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.CV

TL;DR: CPG: a controllable pseudo-label generation framework for long-tailed semi-supervised learning with unknown unlabeled distributions. It uses a self-reinforcing cycle to filter pseudo-labels, applies a logit-adjusted Bayes-optimal classifier, and includes class-aware augmentation and an auxiliary branch. Empirically it yields significant accuracy gains (up to ~16%) and has theoretical generalization-error reductions; code released.


<details>
  <summary>Details</summary>
Motivation: In long-tailed SSL, unlabeled data distributions are often unknown or arbitrary, while many methods assume specific unlabeled distributions. This mismatch can hurt performance. A framework that can leverage unlabeled data while keeping a stable labeled-data distribution is needed.

Method: A Controllable Pseudo-label Generation (CPG) framework that (1) dynamically and controllably filters and adds reliable pseudo-labels from unlabeled data to the labeled set to enforce a known distribution; (2) constructs a Bayes-optimal classifier via logit adjustment based on the updated labeled distribution; (3) iterates this cycle to progressively improve pseudo-label quality. It also includes a class-aware adaptive augmentation module to boost minority-class representation and an auxiliary branch to maximize data utilization by leveraging all samples.

Result: The authors provide theoretical results showing the optimization cycle can substantially reduce generalization error under certain conditions. Empirically, CPG delivers consistent improvements over state-of-the-art methods across standard benchmarks, with gains up to 15.97% in accuracy. The authors also release code at the provided URL.

Conclusion: CPG effectively decouples the unlabeled data distribution from model training, is robust to arbitrary unlabeled distributions, and achieves strong empirical gains, aided by targeted augmentation and a data-utilization strategy.

Abstract: Current long-tailed semi-supervised learning methods assume that labeled data
exhibit a long-tailed distribution, and unlabeled data adhere to a typical
predefined distribution (i.e., long-tailed, uniform, or inverse long-tailed).
However, the distribution of the unlabeled data is generally unknown and may
follow an arbitrary distribution. To tackle this challenge, we propose a
Controllable Pseudo-label Generation (CPG) framework, expanding the labeled
dataset with the progressively identified reliable pseudo-labels from the
unlabeled dataset and training the model on the updated labeled dataset with a
known distribution, making it unaffected by the unlabeled data distribution.
Specifically, CPG operates through a controllable self-reinforcing optimization
cycle: (i) at each training step, our dynamic controllable filtering mechanism
selectively incorporates reliable pseudo-labels from the unlabeled dataset into
the labeled dataset, ensuring that the updated labeled dataset follows a known
distribution; (ii) we then construct a Bayes-optimal classifier using logit
adjustment based on the updated labeled data distribution; (iii) this improved
classifier subsequently helps identify more reliable pseudo-labels in the next
training step. We further theoretically prove that this optimization cycle can
significantly reduce the generalization error under some conditions.
Additionally, we propose a class-aware adaptive augmentation module to further
improve the representation of minority classes, and an auxiliary branch to
maximize data utilization by leveraging all labeled and unlabeled samples.
Comprehensive evaluations on various commonly used benchmark datasets show that
CPG achieves consistent improvements, surpassing state-of-the-art methods by up
to \textbf{15.97\%} in accuracy. The code is available at
https://github.com/yaxinhou/CPG.

</details>


### [76] [Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5](https://arxiv.org/abs/2510.04003)
*Minh Hoang Nguyen,Su Nguyen Thiet*

Main category: cs.CV

TL;DR: Fine-tuned PaddleOCRv5 on Han-Nom manuscripts improves exact recognition from 37.5% to 50%, with an interactive demo and potential downstream semantic tasks.


<details>
  <summary>Details</summary>
Motivation: Digitize Han-Nom Vietnamese historical texts and enable cross-lingual research; OCR faces degradation, non-standard glyphs, and handwriting variation.

Method: Fine-tune the text recognition module of PaddleOCRv5 using a curated subset of ancient Vietnamese-Chinese manuscripts; implement a full training pipeline (preprocessing, LMDB conversion, evaluation, visualization) and develop an interactive demo comparing pre- and post-fine-tuning results.

Result: Exact accuracy increases from 37.5% to 50.0%, notably under noisy image conditions; development of an interactive demo enhances usability for Han-Vietnamese semantic alignment, MT, and linguistics research.

Conclusion: Fine-tuning yields meaningful gains for Han-Nom OCR; supports downstream applications; demo hosted at HuggingFace Spaces.

Abstract: Recognizing and processing Classical Chinese (Han-Nom) texts play a vital
role in digitizing Vietnamese historical documents and enabling cross-lingual
semantic research. However, existing OCR systems struggle with degraded scans,
non-standard glyphs, and handwriting variations common in ancient sources. In
this work, we propose a fine-tuning approach for PaddleOCRv5 to improve
character recognition on Han-Nom texts. We retrain the text recognition module
using a curated subset of ancient Vietnamese Chinese manuscripts, supported by
a full training pipeline covering preprocessing, LMDB conversion, evaluation,
and visualization. Experimental results show a significant improvement over the
base model, with exact accuracy increasing from 37.5 percent to 50.0 percent,
particularly under noisy image conditions. Furthermore, we develop an
interactive demo that visually compares pre- and post-fine-tuning recognition
results, facilitating downstream applications such as Han-Vietnamese semantic
alignment, machine translation, and historical linguistics research. The demo
is available at https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5.

</details>


### [77] [Fit Pixels, Get Labels: Meta-learned Implicit Networks for Image Segmentation](https://arxiv.org/abs/2510.04021)
*Kushal Vyas,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: cs.CV

TL;DR: MetaSeg introduces a meta-learning framework to train implicit neural representations (INRs) for medical image segmentation, enabling rapid per-image adaptation with far fewer parameters than standard models.


<details>
  <summary>Details</summary>
Motivation: INRs are expressive but not naturally suited for predictive tasks like segmentation across image distributions. A meta-learned INR can be quickly fine-tuned to a new image while decoding semantic labels, offering a parameter-efficient alternative to heavy segmentation architectures.

Method: An INR is trained to output both per-pixel intensities and class labels. A meta-learning procedure learns optimal initial INR parameters across a training set of images and segmentation maps so that, for unseen test images, a simple fine-tuning step yields accurate pixel labels and segmentation maps.

Result: Applied to 2D and 3D brain MRI segmentation. Dice scores are comparable to U-Net models, but with roughly 90% fewer parameters, indicating strong efficiency gains.

Conclusion: MetaSeg provides a scalable, resource-efficient alternative to traditional segmentation models for medical imaging by leveraging meta-learned implicit representations, suitable for rapid adaptation to new images with minimal parameter count.

Abstract: Implicit neural representations (INRs) have achieved remarkable successes in
learning expressive yet compact signal representations. However, they are not
naturally amenable to predictive tasks such as segmentation, where they must
learn semantic structures over a distribution of signals. In this study, we
introduce MetaSeg, a meta-learning framework to train INRs for medical image
segmentation. MetaSeg uses an underlying INR that simultaneously predicts per
pixel intensity values and class labels. It then uses a meta-learning procedure
to find optimal initial parameters for this INR over a training dataset of
images and segmentation maps, such that the INR can simply be fine-tuned to fit
pixels of an unseen test image, and automatically decode its class labels. We
evaluated MetaSeg on 2D and 3D brain MRI segmentation tasks and report Dice
scores comparable to commonly used U-Net models, but with $90\%$ fewer
parameters. MetaSeg offers a fresh, scalable alternative to traditional
resource-heavy architectures such as U-Nets and vision transformers for medical
image segmentation. Our project is available at
https://kushalvyas.github.io/metaseg.html .

</details>


### [78] [Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning](https://arxiv.org/abs/2510.04022)
*Chendong Wang,Donglin Bai,Yifan Yang,Xiao Jin,Anlan Zhang,Rui Wang,Shiqi Jiang,Yuqing Yang,Hao Wu,Qi Dai,Chong Luo,Ting Cao,Lili Qiu,Suman Banerjee*

Main category: cs.CV

TL;DR: ViTL proposes a two-stage long-video QA system that preserves a fixed token budget by first skimming at low frame rate to localize question-relevant intervals, then reallocates visual tokens at higher effective frame rates to answer with span-grounded reasoning. It pairs this with a dataset (dataname) that converts description-based event graphs into span-grounded multiple-choice QA. Trained end-to-end with a group-relative objective tying temporal localization quality to answer accuracy, ViTL achieves up to 8.6% improvement using 50% fewer frames on long-video QA and temporal grounding tasks. Ablations show span-aware token reallocation outperforms uniform sampling. Overall, this yields an interpretable, compute-efficient approach for scalable long-video QA.


<details>
  <summary>Details</summary>
Motivation: Long-video question answering is computationally demanding and often lacks interpretability, especially under fixed token budgets. There is a need for efficient localization of relevant moments and grounded, verifiable reasoning, ideally accompanied by data that ties questions to exact time spans.

Method: A two-stage pipeline (ViTL): (1) low-fps skim to localize question-relevant intervals, (2) span-aware reallocation of visual tokens at higher effective frame rate to answer, producing interleaved outputs of spoken spans and a final answer. Introduces dataname: converts description-based event graphs into span-grounded MCQA by pairing each question with ground-truth time spans and reasoning. Trains end-to-end with an interleaved group-relative objective that couples temporal IoU for localization with answer correctness, enabling credit flow from answers back to spans without extra compute.

Result: Under fixed token budgets, ViTL yields up to 8.6% performance gains with 50% fewer frames on long-video QA and temporal grounding benchmarks (Charades-STA, ActivityNet-Captions). Ablations indicate span-aware token reallocation consistently surpasses uniform sampling.

Conclusion: dataname together with ViTL provides an interpretable, compute-efficient recipe for scalable long-video QA, combining span-grounded reasoning with efficient token usage.

Abstract: We present \emph{Video-in-the-Loop} (ViTL), a two-stage long-video QA
framework that preserves a fixed token budget by first \emph{localizing}
question-relevant interval(s) with a low-fps skim and then \emph{answering} via
span-aware reallocation of visual tokens at higher effective frame rate,
emitting an interleaved output with both spans and the final option for direct
attribution. We also introduce \dataname{}, which converts description based
event graphs into \emph{span-grounded} multiple-choice QA by pairing each
question with \emph{ground-truth} time span(s) and related reasoning. ViTL is
trained end-to-end with an interleaved group-relative objective that couples
temporal IoU for localization with answer correctness, allowing credit to flow
from answers back to spans without increasing compute. Under fixed token
budgets, ViTL attains up to 8.6% with 50% less frame input on long-video QA and
temporal grounding (e.g., Charades-STA, ActivityNet-Captions) and ablations
show that span-aware token reallocation consistently surpasses uniform
sampling. Together, \dataname{} and ViTL provide an interpretable,
compute-efficient recipe for scalable long-video QA.

</details>


### [79] [Enhancing Fake News Video Detection via LLM-Driven Creative Process Simulation](https://arxiv.org/abs/2510.04024)
*Yuyan Bu,Qiang Sheng,Juan Cao,Shaofei Wang,Peng Qi,Yuhui Shi,Beizhe Hu*

Main category: cs.CV

TL;DR: AgentAug is a data augmentation framework for short video fake-news detection that uses multiple LLM-driven pipelines to fabricate diverse news videos, paired with active learning to select useful samples, improving detector performance on two benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current detectors suffer from limited, biased, and non-diverse training data due to the complex many-to-many relationships between video clips and fabricated narratives. Existing datasets fail to capture these relationships, hindering generalization and robust fake-news video detection.

Method: AgentAug implements multiple LLM-driven pipelines for four fabrication categories to simulate fake-news video creation, combined with an active-learning strategy based on uncertainty sampling to select potentially useful augmented samples during training.

Result: Experiments on two benchmark datasets show that AgentAug consistently improves the performance of short video fake-news detectors.

Conclusion: Simulating typical fabrication processes with LLM-driven augmentation and coupling it with uncertainty-based sample selection can mitigate data sparsity and bias in short video fake-news detection, enhancing detector effectiveness.

Abstract: The emergence of fake news on short video platforms has become a new
significant societal concern, necessitating automatic video-news-specific
detection. Current detectors primarily rely on pattern-based features to
separate fake news videos from real ones. However, limited and less diversified
training data lead to biased patterns and hinder their performance. This
weakness stems from the complex many-to-many relationships between video
material segments and fabricated news events in real-world scenarios: a single
video clip can be utilized in multiple ways to create different fake
narratives, while a single fabricated event often combines multiple distinct
video segments. However, existing datasets do not adequately reflect such
relationships due to the difficulty of collecting and annotating large-scale
real-world data, resulting in sparse coverage and non-comprehensive learning of
the characteristics of potential fake news video creation. To address this
issue, we propose a data augmentation framework, AgentAug, that generates
diverse fake news videos by simulating typical creative processes. AgentAug
implements multiple LLM-driven pipelines of four fabrication categories for
news video creation, combined with an active learning strategy based on
uncertainty sampling to select the potentially useful augmented samples during
training. Experimental results on two benchmark datasets demonstrate that
AgentAug consistently improves the performance of short video fake news
detectors.

</details>


### [80] [Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks](https://arxiv.org/abs/2510.04034)
*Linn Bieske,Carla Lorente*

Main category: cs.CV

TL;DR: This paper analyzes how hyperparameters in prompt-to-prompt image editing with diffusion models affect consistency and quality; introduces word swap analysis, attention re-weighting, and CL P2P to improve reliability and address cycle inconsistencies.


<details>
  <summary>Details</summary>
Motivation: Despite advances, cross-attention-based editing yields variable results (e.g., hair color changes). There is a need to understand how hyperparameters and attention architecture interact to determine edit quality and reliability.

Method: Systematic study of 'word swap' method; development of 'attention re-weight' technique; proposal of 'CL P2P' framework; empirical analysis of hyperparameter-architecture interactions.

Result: Offers empirical insights into how hyperparameters influence prompt-to-prompt edits; demonstrates that the proposed methods improve adaptability and consistency (e.g., reducing cycle inconsistency) and clarifies the role of attention in image composition.

Conclusion: Hyperparameter and attention mechanism interplay is critical for reliable prompt-driven editing; the proposed methods advance precision and consistency in diffusion-based editing, guiding future design of editing frameworks.

Abstract: Recent advances in image editing have shifted from manual pixel manipulation
to employing deep learning methods like stable diffusion models, which now
leverage cross-attention mechanisms for text-driven control. This transition
has simplified the editing process but also introduced variability in results,
such as inconsistent hair color changes. Our research aims to enhance the
precision and reliability of prompt-to-prompt image editing frameworks by
exploring and optimizing hyperparameters. We present a comprehensive study of
the "word swap" method, develop an "attention re-weight method" for better
adaptability, and propose the "CL P2P" framework to address existing
limitations like cycle inconsistency. This work contributes to understanding
and improving the interaction between hyperparameter settings and the
architectural choices of neural network models, specifically their attention
mechanisms, which significantly influence the composition and quality of the
generated images.

</details>


### [81] [\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding](https://arxiv.org/abs/2510.04039)
*Bin Lei,Nuo Xu,Ali Payani,Mingyi Hong,Chunhua Liao,Yu Cao,Caiwen Ding*

Main category: cs.CV

TL;DR: GUI-Spotlight improves visual grounding for GUI LLMs by using iterative, tool-assisted reasoning to focus on exact screen elements; achieves 52.8% accuracy on ScreenSpot-Pro with 18.5k training samples, outperforming larger models trained on millions of samples.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLMs struggle with reliable visual grounding at the pointer level, limiting the ability to perform precise GUI actions like clicking or dragging in real-world environments.

Method: The model is trained for image-grounded reasoning and dynamically invokes multiple specialized tools to iteratively narrow attention to the relevant region of the screen, improving grounding accuracy.

Result: On ScreenSpot-Pro, GUI-Spotlight achieves 52.8% accuracy with only 18.5k training samples, surpassing V2P-7B (50.6% with 9.6M samples) and GTA-1-7B (50.1% with 1.56M samples).

Conclusion: Iterative tool-based grounding can substantially improve on-screen element localization for GUI tasks with less training data, enhancing the practicality of MLLMs for pointer-level actions.

Abstract: Multimodal large language models (MLLMs) have markedly expanded the
competence of graphical user-interface (GUI) systems, propelling them beyond
controlled simulations into complex, real-world environments across diverse
platforms. However, practical usefulness is still bounded by the reliability of
visual grounding, i.e., mapping textual references to exact on-screen elements.
This limitation prevents the system from accurately performing pointer-level
actions such as clicking or dragging. To address it, we introduce GUI-Spotlight
-- a model trained for image-grounded reasoning that dynamically invokes
multiple specialized tools to iteratively narrow its focus to the relevant
region of the screen, thereby substantially improving visual grounding
accuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only
18.5K training samples achieves 52.8\% accuracy, surpassing V2P-7B (50.6\% with
9.6M training samples) and GTA-1-7B (50.1\% with 1.56M training samples).

</details>


### [82] [Quantization Range Estimation for Convolutional Neural Networks](https://arxiv.org/abs/2510.04044)
*Bingtao Yang,Yujia Wang,Mengzhi Jiao,Hongwei Huo*

Main category: cs.CV

TL;DR: A locally convex optimization-based range-estimation method for post-training quantization reduces quantization error and improves accuracy at 8/6/4-bit across ResNet and Inception-v3, with released code.


<details>
  <summary>Details</summary>
Motivation: To enable effective post-training quantization by obtaining accurate quantization ranges, enabling low-bit quantization without large accuracy loss.

Method: Formulates range estimation as an optimization problem minimizing quantization errors via layer-wise local minima, proves local convexity, and develops an efficient search algorithm; additionally applies the algorithm in a transformed weight space for further gains.

Result: The method generally outperforms state-of-the-art on top-1 accuracy for ResNet and Inception-v3; shows almost no loss at 8-bit and 6-bit, and notable improvements at 4-bit; code is provided.

Conclusion: The proposed range-estimation optimization is effective for post-training quantization, enabling robust low-bit performance and practical deployment.

Abstract: Post-training quantization for reducing the storage of deep neural network
models has been demonstrated to be an effective way in various tasks. However,
low-bit quantization while maintaining model accuracy is a challenging problem.
In this paper, we present a range estimation method to improve the quantization
performance for post-training quantization. We model the range estimation into
an optimization problem of minimizing quantization errors by layer-wise local
minima. We prove this problem is locally convex and present an efficient search
algorithm to find the optimal solution. We propose the application of the above
search algorithm to the transformed weights space to do further improvement in
practice. Our experiments demonstrate that our method outperforms
state-of-the-art performance generally on top-1 accuracy for image
classification tasks on the ResNet series models and Inception-v3 model. The
experimental results show that the proposed method has almost no loss of top-1
accuracy in 8-bit and 6-bit settings for image classifications, and the
accuracy of 4-bit quantization is also significantly improved. The code is
available at https://github.com/codeiscommitting/REQuant.

</details>


### [83] [MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation](https://arxiv.org/abs/2510.04057)
*Zhenyu Pan,Yucheng Lu,Han Liu*

Main category: cs.CV

TL;DR: MetaFind introduces a scene-aware tri-modal retrieval framework for 3D assets enabling coherent, context-aware retrieval across text, image, and 3D modalities, with an equivariant layout encoder ensuring spatial and stylistic consistency.


<details>
  <summary>Details</summary>
Motivation: To address inconsistent asset retrieval that ignores spatial, semantic, and style constraints, and to fill the gap of a standardized 3D asset retrieval paradigm suited for scene generation in the metaverse.

Method: A plug-and-play retrieval system employing an equivariant layout encoder ESSGNN that models both object-level appearance and scene-level layouts, supports arbitrary combinations of text/image/3D queries, and iteratively updates retrieval results as the scene evolves, ensuring transformation-invariant coherence across coordinate frames.

Result: Empirical evaluations show improved spatial and stylistic coherence in retrieved assets across multiple tasks compared with baseline methods, demonstrating robustness to coordinate transformations and scene updates.

Conclusion: MetaFind offers a flexible, scene-aware retrieval paradigm tailored for 3D assets, enabling coherent scene construction in the metaverse by jointly modeling appearance and layout and by supporting iterative, multi-modal queries.

Abstract: We present MetaFind, a scene-aware tri-modal compositional retrieval
framework designed to enhance scene generation in the metaverse by retrieving
3D assets from large-scale repositories. MetaFind addresses two core
challenges: (i) inconsistent asset retrieval that overlooks spatial, semantic,
and stylistic constraints, and (ii) the absence of a standardized retrieval
paradigm specifically tailored for 3D asset retrieval, as existing approaches
mainly rely on general-purpose 3D shape representation models. Our key
innovation is a flexible retrieval mechanism that supports arbitrary
combinations of text, image, and 3D modalities as queries, enhancing spatial
reasoning and style consistency by jointly modeling object-level features
(including appearance) and scene-level layout structures. Methodologically,
MetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that
captures spatial relationships and object appearance features, ensuring
retrieved 3D assets are contextually and stylistically coherent with the
existing scene, regardless of coordinate frame transformations. The framework
supports iterative scene construction by continuously adapting retrieval
results to current scene updates. Empirical evaluations demonstrate the
improved spatial and stylistic consistency of MetaFind in various retrieval
tasks compared to baseline methods.

</details>


### [84] [Ordinal Encoding as a Regularizer in Binary Loss for Solar Flare Prediction](https://arxiv.org/abs/2510.04063)
*Chetraj Pandey,Jinsu Hong,Anli Ji,Rafal A. Angryk,Berkay Aydin*

Main category: cs.CV

TL;DR: Introduces an ordinality-aware regularization by weighting BCE loss with ordinal distances among sub-classes of binarized solar flare labels to penalize near-boundary errors more.


<details>
  <summary>Details</summary>
Motivation: Binary flare prediction loses useful ordinal information among sub-classes; misclassifications cluster near the threshold, motivating a loss that discourages near-boundary mistakes.

Method: Modify the binary cross-entropy loss by incorporating ordinal weights derived from the distance of sub-classes from the threshold, creating a data-driven regularization that penalizes mispredictions near the boundary more heavily.

Result: The abstract presents the method but does not report empirical results or quantitative evaluation.

Conclusion: Integrating ordinal characteristics into the loss function aims to enhance learning and improve overall solar flare prediction performance.

Abstract: The prediction of solar flares is typically formulated as a binary
classification task, distinguishing events as either Flare (FL) or No-Flare
(NF) according to a specified threshold (for example, greater than or equal to
C-class, M-class, or X-class). However, this binary framework neglects the
inherent ordinal relationships among the sub-classes contained within each
category (FL and NF). Several studies on solar flare prediction have
empirically shown that the most frequent misclassifications occur near this
prediction threshold. This suggests that the models struggle to differentiate
events that are similar in intensity but fall on opposite sides of the binary
threshold. To mitigate this limitation, we propose a modified loss function
that integrates the ordinal information among the sub-classes of the binarized
flare labels into the conventional binary cross-entropy (BCE) loss. This
approach serves as an ordinality-aware, data-driven regularization method that
penalizes the incorrect predictions of flare events in close proximity to the
prediction threshold more heavily than those away from the boundary during
model optimization. By incorporating ordinal weighting into the loss function,
we aim to enhance the model's learning process by leveraging the ordinal
characteristics of the data, thereby improving its overall performance.

</details>


### [85] [QuantDemoire: Quantization with Outlier Aware for Image Demoiréing](https://arxiv.org/abs/2510.04066)
*Zheng Chen,Kewei Zhang,Xiaoyang Liu,Weihang Zhang,Mengfan Wang,Yifan Fu,Yulun Zhang*

Main category: cs.CV

TL;DR: QuantDemoire proposes a post-training quantization framework for demoiréing that tackles distribution outliers and banding artifacts. It achieves large parameter and computation reductions while preserving quality, outperforming baselines by over 4 dB on W4A4.


<details>
  <summary>Details</summary>
Motivation: Edge devices struggle to deploy DL-based demoiréing due to high compute. Direct quantization worsens performance because demoiréing models suffer from activation outliers and weakened representations in smooth regions. A tailored post-training quantization is needed.

Method: Two components: (1) an outlier-aware quantizer with sampling-based range estimation to curb activation outliers, keeping a small set of extreme weights in FP16; (2) a frequency-aware calibration strategy that emphasizes low- and mid-frequency components during fine-tuning to mitigate banding from low-bit quantization.

Result: Extensive experiments show QuantDemoire achieves substantial parameter and computation reductions while maintaining image quality, and it outperforms existing quantization methods by over 4 dB on W4A4.

Conclusion: QuantDemoire provides an effective edge-friendly quantization solution for demoiréing, enabling efficient deployment without significant quality loss. The authors release code for reproducibility.

Abstract: Demoir\'eing aims to remove moir\'e artifacts that often occur in images.
While recent deep learning-based methods have achieved promising results, they
typically require substantial computational resources, limiting their
deployment on edge devices. Model quantization offers a compelling solution.
However, directly applying existing quantization methods to demoir\'eing models
introduces severe performance degradation. The main reasons are distribution
outliers and weakened representations in smooth regions. To address these
issues, we propose QuantDemoire, a post-training quantization framework
tailored to demoir\'eing. It contains two key components. **First}, we
introduce an outlier-aware quantizer to reduce errors from outliers. It uses
sampling-based range estimation to reduce activation outliers, and keeps a few
extreme weights in FP16 with negligible cost. **Second**, we design a
frequency-aware calibration strategy. It emphasizes low- and mid-frequency
components during fine-tuning, which mitigates banding artifacts caused by
low-bit quantization. Extensive experiments validate that our QuantDemoire
achieves large reductions in parameters and computation while maintaining
quality. Meanwhile, it outperforms existing quantization methods by over **4
dB** on W4A4. Code is released at:
https://github.com/zhengchen1999/QuantDemoire.

</details>


### [86] [Diffusion Low Rank Hybrid Reconstruction for Sparse View Medical Imaging](https://arxiv.org/abs/2510.04069)
*Zongyin Deng,Qing Zhou,Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: TV-LoRA integrates diffusion-based generative priors with anisotropic TV and low-rank LoRA regularization inside an ADMM framework to enable robust, high-fidelity low-dose sparse-view 3D CT reconstruction, achieving state-of-the-art SSIM and texture/edge quality with acceleration strategies.


<details>
  <summary>Details</summary>
Motivation: Ill-posedness and texture loss under extremely sparse views in CT reconstruction; a combination of data-driven generative priors and physics-based regularization can improve robustness, texture fidelity, and generalizability.

Method: Diffusion prior (NCSN++ with SDE modeling) + multi-regularization constraints (anisotropic TV and nuclear norm/LoRA) within an ADMM framework; 2D slice-based strategy with FFT acceleration; tensor-parallel optimization for efficient inference; applied to 3D low-dose sparse-view CT reconstruction.

Result: On AAPM-2016, CTHD, and LIDC with N_view=8,4,2, TV-LoRA consistently surpasses benchmarks in SSIM, texture recovery, edge clarity, and artifact suppression; ablation studies show complementary effects of LoRA regularization and diffusion priors; FFT-PCG module speeds up computation; demonstrates robustness and generalizability across datasets.

Conclusion: Diffusion + TV-LoRA yields high-fidelity, efficient 3D CT reconstruction in low-dose, sparse-sampling scenarios with broad clinical applicability.

Abstract: This work presents TV-LoRA, a novel method for low-dose sparse-view CT
reconstruction that combines a diffusion generative prior (NCSN++ with SDE
modeling) and multi-regularization constraints, including anisotropic TV and
nuclear norm (LoRA), within an ADMM framework. To address ill-posedness and
texture loss under extremely sparse views, TV-LoRA integrates generative and
physical constraints, and utilizes a 2D slice-based strategy with FFT
acceleration and tensor-parallel optimization for efficient inference.
Experiments on AAPM-2016, CTHD, and LIDC datasets with
$N_{\mathrm{view}}=8,4,2$ show that TV-LoRA consistently surpasses benchmarks
in SSIM, texture recovery, edge clarity, and artifact suppression,
demonstrating strong robustness and generalizability. Ablation studies confirm
the complementary effects of LoRA regularization and diffusion priors, while
the FFT-PCG module provides a speedup. Overall, Diffusion + TV-LoRA achieves
high-fidelity, efficient 3D CT reconstruction and broad clinical applicability
in low-dose, sparse-sampling scenarios.

</details>


### [87] [TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing](https://arxiv.org/abs/2510.04100)
*Jiaming Wang,Diwen Liu,Jizhuo Chen,Harold Soh*

Main category: cs.CV

TL;DR: Proposes standardized evaluation for topological mapping: formalizes topological consistency, uses localization accuracy as a surrogate metric, introduces a quantitative dataset-ambiguity measure, and releases a curated benchmark with calibrated ambiguity levels, baselines, and open-source tools to enable fair, reproducible evaluation under perceptual aliasing.


<details>
  <summary>Details</summary>
Motivation: Current topological mapping lacks consistent metrics, datasets, and protocols; perceptual aliasing strongly impacts performance but is under-quantified; need fair comparisons across environments.

Method: Define topological consistency as fundamental; propose localization accuracy as efficient surrogate; introduce a quantitative measure of dataset ambiguity; curate a diverse benchmark with calibrated ambiguity; implement and release deep-learned baselines and classical methods; evaluate and analyze under aliasing.

Result: A standardized evaluation protocol, a benchmark dataset with varying ambiguity, baselines (deep-learned and classical), and open-source evaluation tools; insights into limitations under perceptual aliasing.

Conclusion: Open-sourcing datasets, baselines, and tools to enable reproducible research in topological mapping and to advance fair comparisons across methods and environments while highlighting aliasing challenges.

Abstract: Topological mapping offers a compact and robust representation for
navigation, but progress in the field is hindered by the lack of standardized
evaluation metrics, datasets, and protocols. Existing systems are assessed
using different environments and criteria, preventing fair and reproducible
comparisons. Moreover, a key challenge - perceptual aliasing - remains
under-quantified, despite its strong influence on system performance. We
address these gaps by (1) formalizing topological consistency as the
fundamental property of topological maps and showing that localization accuracy
provides an efficient and interpretable surrogate metric, and (2) proposing the
first quantitative measure of dataset ambiguity to enable fair comparisons
across environments. To support this protocol, we curate a diverse benchmark
dataset with calibrated ambiguity levels, implement and release deep-learned
baseline systems, and evaluate them alongside classical methods. Our
experiments and analysis yield new insights into the limitations of current
approaches under perceptual aliasing. All datasets, baselines, and evaluation
tools are fully open-sourced to foster consistent and reproducible research in
topological mapping.

</details>


### [88] [Learning Efficient Meshflow and Optical Flow from Event Cameras](https://arxiv.org/abs/2510.04111)
*Xinglong Luo,Ao Luo,Kunming Luo,Zhengning Wang,Ping Tan,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: Proposes Efficient Event-based MeshFlow (EEMFlow) for fast, accurate meshflow estimation from event cameras, backed by a new High-Resolution Event Meshflow (HREM) dataset and its density-robust extension HREM+. The approach delivers substantial speedups (≈30x) over state-of-the-art flow methods and improves robustness via an Adaptive Density Module (ADM) and a Confidence-induced Detail Completion (CDC) module for dense optical flow. Code and data are released.


<details>
  <summary>Details</summary>
Motivation: There is a lack of meshflow-specific datasets and methods for event-based motion estimation, and event data density presents a significant challenge to performance and generalization.

Method: Introduce a lightweight encoder-decoder network (EEMFlow) for meshflow estimation, enhanced with a CDC module for sharp motion boundaries in dense optical flow. Create HREM, a high-resolution dataset with both optical flow and meshflow labels; extend to HREM+ featuring multi-density event data and develop ADM to adapt input density for better generalization.

Result: EEMFlow achieves ~30x faster inference compared to recent state-of-the-art flow methods, with competitive accuracy. ADM improves EEMFlow and EEMFlow+ by 8% and 10% respectively. HREM and HREM+ enable robust evaluation across resolutions and densities; code and data are released.

Conclusion: The work provides a fast, accurate meshflow estimation framework for event cameras, supported by a high-resolution, density-variant dataset and a density-adaptive module to improve generalization. The release of code and data will facilitate further research in event-based motion estimation.

Abstract: In this paper, we explore the problem of event-based meshflow estimation, a
novel task that involves predicting a spatially smooth sparse motion field from
event cameras. To start, we review the state-of-the-art in event-based flow
estimation, highlighting two key areas for further research: i) the lack of
meshflow-specific event datasets and methods, and ii) the underexplored
challenge of event data density. First, we generate a large-scale
High-Resolution Event Meshflow (HREM) dataset, which showcases its superiority
by encompassing the merits of high resolution at 1280x720, handling dynamic
objects and complex motion patterns, and offering both optical flow and
meshflow labels. These aspects have not been fully explored in previous works.
Besides, we propose Efficient Event-based MeshFlow (EEMFlow) network, a
lightweight model featuring a specially crafted encoder-decoder architecture to
facilitate swift and accurate meshflow estimation. Furthermore, we upgrade
EEMFlow network to support dense event optical flow, in which a
Confidence-induced Detail Completion (CDC) module is proposed to preserve sharp
motion boundaries. We conduct comprehensive experiments to show the exceptional
performance and runtime efficiency (30x faster) of our EEMFlow model compared
to the recent state-of-the-art flow method. As an extension, we expand HREM
into HREM+, a multi-density event dataset contributing to a thorough study of
the robustness of existing methods across data with varying densities, and
propose an Adaptive Density Module (ADM) to adjust the density of input event
data to a more optimal range, enhancing the model's generalization ability. We
empirically demonstrate that ADM helps to significantly improve the performance
of EEMFlow and EEMFlow+ by 8% and 10%, respectively. Code and dataset are
released at https://github.com/boomluo02/EEMFlowPlus.

</details>


### [89] [Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation](https://arxiv.org/abs/2510.04125)
*Seunghyun Lee,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: A diffusion-based category-level 6D object pose estimation method that accelerates training and eliminates the need for an extra pose-evaluation network. It achieves state-of-the-art accuracy with single-pose inference by pretraining the encoder with a direct pose regression head and introducing time-dependent score scaling for guided sampling.


<details>
  <summary>Details</summary>
Motivation: To address slow training convergence, end-to-end diffusion encoder learning, and reliance on a separate evaluation network in diffusion-based pose estimation, while preserving multi-modal pose conjectures for symmetric objects.

Method: 1) Pretrain the encoder with a direct regression pose head and jointly train the regression head with the diffusion denoising head to accelerate convergence and improve accuracy. 2) Use sampling guidance via time-dependent score scaling to balance exploration and exploitation, preserving multi-modality in early steps and producing high-quality poses in final steps, removing the need for an extra evaluation network.

Result: Demonstrates state-of-the-art accuracies on benchmarks REAL275, HouseCat6D, and ROPE, with effective single-pose inference and improved training/inference efficiency compared to prior methods.

Conclusion: The approach is simple yet effective, improving convergence speed and efficiency while handling multi-modal pose hypotheses without requiring an additional evaluation network.

Abstract: Latest diffusion models have shown promising results in category-level 6D
object pose estimation by modeling the conditional pose distribution with depth
image input. The existing methods, however, suffer from slow convergence during
training, learning its encoder with the diffusion denoising network in
end-to-end fashion, and require an additional network that evaluates sampled
pose hypotheses to filter out low-quality pose candidates. In this paper, we
propose a novel pipeline that tackles these limitations by two key components.
First, the proposed method pretrains the encoder with the direct pose
regression head, and jointly learns the networks via the regression head and
the denoising diffusion head, significantly accelerating training convergence
while achieving higher accuracy. Second, sampling guidance via time-dependent
score scaling is proposed s.t. the exploration-exploitation trade-off is
effectively taken, eliminating the need for the additional evaluation network.
The sampling guidance maintains multi-modal characteristics of symmetric
objects at early denoising steps while ensuring high-quality pose generation at
final steps. Extensive experiments on multiple benchmarks including REAL275,
HouseCat6D, and ROPE, demonstrate that the proposed method, simple yet
effective, achieves state-of-the-art accuracies even with single-pose
inference, while being more efficient in both training and inference.

</details>


### [90] [Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs](https://arxiv.org/abs/2510.04142)
*Xiaoyu Yang,Jie Lu,En Yu*

Main category: cs.CV

TL;DR: Addresses concept drift in distilling from multimodal LLMs by modeling multi-teacher reasoning as non-stationary multi-stream trajectories and linking concept drift to knowledge distillation; proposes Learn, Compare, Critique (LCC) and Autonomous Preference Optimization (APO); achieves improved consistency, robustness, and generalization and provides CXR-MAX dataset (170,982 trajectories).


<details>
  <summary>Details</summary>
Motivation: The key challenge is concept drift in reasoning trajectories when distilling from multiple drifting MLLM teachers, which transmits biases to the student and degrades performance. There is a gap in theoretical understanding and practical methods to stabilize distillation under non-stationary teacher dynamics.

Method: Establishes a theoretical connection between concept drift and knowledge distillation by framing non-stationary teacher dynamics as next-token prediction over multi-stream reasoning trajectories. Introduces Learn, Compare, Critique (LCC) and Autonomous Preference Optimization (APO); the student learns by self-distillation from teacher comparisons, reflects on drifting inferences, and aligns concepts via APO to achieve robust, consistent generalization.

Result: Empirical results show superior consistency, robustness, and generalization in knowledge distillation. A large-scale dataset, CXR-MAX (170,982 distilled reasoning trajectories from MIMIC-CXR across publicly available MLLMs), is released along with code and data for replication.

Conclusion: The APO-driven autonomous distillation framework effectively mitigates concept drift in multi-teacher MLLM distillation, yielding a stable, aligned, and generalizable student model. The released dataset and code support further research in robust distillation under non-stationary teacher dynamics.

Abstract: This paper identifies a critical yet underexplored challenge in distilling
from multimodal large language models (MLLMs): the reasoning trajectories
generated by multiple drifting teachers exhibit concept drift, whereby their
reasoning distributions evolve unpredictably and transmit biases to the student
model, ultimately compromising its performance. To tackle this issue, we
pioneer a theoretical connection between concept drift and knowledge
distillation, casting the non-stationary reasoning dynamics from multiple MLLM
teachers as next-token prediction of multi-stream reasoning trajectories.Guided
by concept drift, we introduce the "learn, compare, critique" paradigm,
culminating in autonomous preference optimization (APO). Under the active
guidance of the teachers, the student model first learns and self-distils
preferred thinking by comparing multiple teachers. It then engages in critical
reflection over the drifting inference from teachers, performing concept
alignment through APO, ultimately yielding a robust, consistent, and
generalizable model.Extensive experiments demonstrate our superior performance
of consistency, robustness and generalization within knowledge distillation.
Besides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers
Alignment X-rays), comprising 170,982 distilled reasoning trajectories derived
from publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public
at: https://anonymous.4open.science/r/Autonomous-Distillation/.

</details>


### [91] [Automating construction safety inspections using a multi-modal vision-language RAG framework](https://arxiv.org/abs/2510.04145)
*Chenxin Wang,Elyas Asadi Shamsabadi,Zhaohui Chen,Luming Shen,Alireza Ahmadian Fard Fini,Daniel Dias-da-Costa*

Main category: cs.CV

TL;DR: SiteShield is a multi-modal LVLM-based RAG system that automates construction safety inspection reports using visual and audio inputs, outperforming unimodal LLMs without RAG on real-world data.


<details>
  <summary>Details</summary>
Motivation: Conventional safety inspections are inefficient and information-heavy. LVLMs offer improvements but face irrelevant outputs, input limitations, and hallucinations. LLMs struggle with data availability and real-time adaptability. A multi-modal RAG approach integrating visuals, audio, and retrieval could enhance accuracy and efficiency in safety reporting.

Method: Proposes SiteShield, a retrieval-augmented generation framework that fuses visual and audio inputs with a retrieval mechanism to generate construction safety inspection reports. Evaluated on real-world data against unimodal LLMs without RAG, focusing on information retrieval and report generation quality.

Result: SiteShield achieved F1=0.82, Hamming loss=0.04, precision=0.76, recall=0.96, outperforming unimodal LLMs without RAG on real-world data, indicating improved information retrieval and efficiency in generating safety reports.

Conclusion: SiteShield demonstrates a novel pathway to enhance construction safety reporting through a multimodal RAG framework, improving accuracy and efficiency of safety inspections and reporting.

Abstract: Conventional construction safety inspection methods are often inefficient as
they require navigating through large volume of information. Recent advances in
large vision-language models (LVLMs) provide opportunities to automate safety
inspections through enhanced visual and linguistic understanding. However,
existing applications face limitations including irrelevant or unspecific
responses, restricted modal inputs and hallucinations. Utilisation of Large
Language Models (LLMs) for this purpose is constrained by availability of
training data and frequently lack real-time adaptability. This study introduces
SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)
framework for automating construction safety inspection reports by integrating
visual and audio inputs. Using real-world data, SiteShield outperformed
unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,
precision of 0.76, and recall of 0.96. The findings indicate that SiteShield
offers a novel pathway to enhance information retrieval and efficiency in
generating safety reports.

</details>


### [92] [A Modular Conditional Diffusion Framework for Image Reconstruction](https://arxiv.org/abs/2411.05993)
*Magauiya Zhussip,Iaroslav Koshelev,Stamatis Lefkimmiatis*

Main category: cs.CV

TL;DR: DP-IR introduces a modular diffusion-based IR framework that adds a small task-specific module atop pre-trained IR networks and diffusion models, enabling 4x faster sampling and maintaining or improving perceptual quality with competitive fidelity across multiple IR tasks.


<details>
  <summary>Details</summary>
Motivation: Diffusion probabilistic models offer strong perceptual results for blind image restoration but are hindered by task-specific training needs and high computational costs, limiting practical adoption across diverse IR tasks.

Method: Propose a modular DP-IR framework that merges pre-trained state-of-the-art IR networks with generative diffusion models. Only a small module (0.7M parameters) requires training for a given IR task. The architecture supports an efficient sampling strategy yielding at least fourfold reductions in neural function evaluations, compatible with acceleration techniques like DDIM.

Result: Empirical evaluation on four benchmarks covering burst JDD-SR, dynamic scene deblurring, and super-resolution shows the method achieves higher perceptual quality than competing approaches while maintaining competitive fidelity metrics.

Conclusion: DP-IR enables practical deployment of diffusion-based IR by minimizing training cost and computation, while preserving or enhancing perceptual quality and staying competitive on fidelity metrics across multiple IR tasks.

Abstract: Diffusion Probabilistic Models (DPMs) have been recently utilized to deal
with various blind image restoration (IR) tasks, where they have demonstrated
outstanding performance in terms of perceptual quality. However, the
task-specific nature of existing solutions and the excessive computational
costs related to their training, make such models impractical and challenging
to use for different IR tasks than those that were initially trained for. This
hinders their wider adoption, especially by those who lack access to powerful
computational resources and vast amount of training data. In this work we aim
to address the above issues and enable the successful adoption of DPMs in
practical IR-related applications. Towards this goal, we propose a modular
diffusion probabilistic IR framework (DP-IR), which allows us to combine the
performance benefits of existing pre-trained state-of-the-art IR networks and
generative DPMs, while it requires only the additional training of a relatively
small module (0.7M params) related to the particular IR task of interest.
Moreover, the architecture of the proposed framework allows for a sampling
strategy that leads to at least four times reduction of neural function
evaluations without suffering any performance loss, while it can also be
combined with existing acceleration techniques such as DDIM. We evaluate our
model on four benchmarks for the tasks of burst JDD-SR, dynamic scene
deblurring, and super-resolution. Our method outperforms existing approaches in
terms of perceptual quality while it retains a competitive performance with
respect to fidelity metrics.

</details>


### [93] [BLADE: Bias-Linked Adaptive DEbiasing](https://arxiv.org/abs/2510.04174)
*Piyush Arora,Navlika Singh,Vasubhya Diwan,Pratik Mazumder*

Main category: cs.CV

TL;DR: BLADE is a bias-linked adaptive debiasing framework that uses bias-domain translation to suppress spurious correlations without prior bias knowledge, achieving strong gains on benchmark datasets (notably ~18% absolute improvement on corrupted CIFAR-10) and setting a new standard for unsupervised debiasing.


<details>
  <summary>Details</summary>
Motivation: Neural networks readily pick up implicit biases and spurious correlations from training data, leading to poor generalization. Existing debiasing methods often assume knowledge of biases or access to bias-conflicting samples, which is impractical in real-world settings. A method that can debias without such priors is highly desirable.

Method: 1) Train a generative model to translate images across bias domains while preserving task-relevant features. 2) Adaptively refine each image with its synthetic (bias-translated) counterpart based on the image's susceptibility to bias. 3) Encourage robust representations by aligning an image with its bias-translated version (sharing task-relevant features but differing in bias) while misaligning it with samples that share the same bias.

Result: Experiments on multiple benchmark datasets show that BLADE significantly outperforms state-of-the-art debiasing methods, including an absolute gain of around 18 percentage points on the corrupted CIFAR-10 dataset under the worst-group setting, establishing a new benchmark for bias mitigation.

Conclusion: BLADE demonstrates that bias-aware generative translation can effectively debias models without explicit supervision, yielding robust representations. The approach is generalizable to other tasks and datasets and paves the way for further research in unsupervised debiasing.

Abstract: Neural networks have revolutionized numerous fields, yet they remain
vulnerable to a critical flaw: the tendency to learn implicit biases, spurious
correlations between certain attributes and target labels in training data.
These biases are often more prevalent and easier to learn, causing models to
rely on superficial patterns rather than task-relevant features necessary for
generalization. Existing methods typically rely on strong assumptions, such as
prior knowledge of these biases or access to bias-conflicting samples, i.e.,
samples that contradict spurious correlations and counterbalance bias-aligned
samples, samples that conform to these spurious correlations. However, such
assumptions are often impractical in real-world settings. We propose BLADE
({B}ias-{L}inked {A}daptive {DE}biasing), a generative debiasing framework that
requires no prior knowledge of bias or bias-conflicting samples. BLADE first
trains a generative model to translate images across bias domains while
preserving task-relevant features. Then, it adaptively refines each image with
its synthetic counterpart based on the image's susceptibility to bias. To
encourage robust representations, BLADE aligns an image with its
bias-translated synthetic counterpart that shares task-relevant features but
differs in bias, while misaligning it with samples sharing the same bias. We
evaluate BLADE on multiple benchmark datasets and show that it significantly
outperforms state-of-the-art methods. Notably, it exceeds the closest baseline
by an absolute margin of around 18% on the corrupted CIFAR-10 dataset under the
worst group setting, establishing a new benchmark in bias mitigation and
demonstrating its potential for developing more robust deep learning models
without explicit supervision.

</details>


### [94] [Textured Gaussians for Enhanced 3D Scene Appearance Modeling](https://arxiv.org/abs/2411.18625)
*Brian Chao,Hung-Yu Tseng,Lorenzo Porzi,Chen Gao,Tuotuo Li,Qinbo Li,Ayush Saraf,Jia-Bin Huang,Johannes Kopf,Gordon Wetzstein,Changil Kim*

Main category: cs.CV

TL;DR: Augments 3D Gaussian Splatting with per-Gaussian texture maps (alpha, RGB, RGBA) to increase expressivity, enabling spatial color/opacity variation; alpha-only improves most, RGB best; achieves image-quality gains with similar or fewer Gaussians.


<details>
  <summary>Details</summary>
Motivation: 3DGS limitations: a single color per Gaussian and ellipsoid-level geometry hinder expressivity; need richer appearance modeling to capture textures and variations; draw inspiration from texture/alpha mapping in classical graphics.

Method: Attach texture maps to each Gaussian: alpha, RGB, or RGBA; use alpha-only textures first, then RGB textures; integrate into 3DGS pipeline; evaluate on standard benchmarks and custom captures at object and scene scales; compare against baseline Gaussians with similar counts.

Result: Demonstrates image quality improvements over existing methods with similar or fewer Gaussians; alpha textures provide notable gains; RGB textures provide highest expressivity and best results; validated across object/scene datasets and custom captures.

Conclusion: Texture-augmented Gaussians substantially extend 3DGS expressivity, enabling richer appearance and geometry beyond simple ellipsoids; alpha maps offer strong benefits and RGB maps yield best performance; approach compatible with existing 3DGS pipelines and can reduce total primitives for comparable quality.

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D
reconstruction and rendering technique due to its high-quality results and fast
training and rendering time. However, pixels covered by the same Gaussian are
always shaded in the same color up to a Gaussian falloff scaling factor.
Furthermore, the finest geometric detail any individual Gaussian can represent
is a simple ellipsoid. These properties of 3DGS greatly limit the expressivity
of individual Gaussian primitives. To address these issues, we draw inspiration
from texture and alpha mapping in traditional graphics and integrate it with
3DGS. Specifically, we propose a new generalized Gaussian appearance
representation that augments each Gaussian with alpha~(A), RGB, or RGBA texture
maps to model spatially varying color and opacity across the extent of each
Gaussian. As such, each Gaussian can represent a richer set of texture patterns
and geometric structures, instead of just a single color and ellipsoid as in
naive Gaussian Splatting. Surprisingly, we found that the expressivity of
Gaussians can be greatly improved by using alpha-only texture maps, and further
augmenting Gaussians with RGB texture maps achieves the highest expressivity.
We validate our method on a wide variety of standard benchmark datasets and our
own custom captures at both the object and scene levels. We demonstrate image
quality improvements over existing methods while using a similar or lower
number of Gaussians.

</details>


### [95] [From Segments to Concepts: Interpretable Image Classification via Concept-Guided Segmentation](https://arxiv.org/abs/2510.04180)
*Ran Eisenberg,Amit Rozner,Ethan Fetaya,Ofir Lindenbaum*

Main category: cs.CV

TL;DR: SEG-MIL-CBM is a segmentation-guided MIL framework that yields spatially grounded, concept-level explanations without requiring concept annotations, improving interpretability and robustness against spurious correlations and corruptions.


<details>
  <summary>Details</summary>
Motivation: To address interpretability gaps in deep vision models, especially in safety-critical domains, by providing transparent, region-level explanations and reducing reliance on unreliable features; concept bottleneck models (CBMs) require costly concept annotations and lack spatial grounding.

Method: Integrates concept-guided image segmentation with an attention-based multiple instance learning (MIL) framework. Each segmented region is treated as an instance, and the model learns to aggregate evidence across regions. The approach eschews concept annotations, instead aligning regions with high-level concepts to produce spatially grounded, concept-level explanations.

Result: Demonstrates robust performance across settings with spurious correlations, input corruptions, and large-scale benchmarks, while offering transparent explanations.

Conclusion: SEG-MIL-CBM enables spatially grounded, concept-level explanations without concept labels, enhancing interpretability and robustness in vision models.

Abstract: Deep neural networks have achieved remarkable success in computer vision;
however, their black-box nature in decision-making limits interpretability and
trust, particularly in safety-critical applications. Interpretability is
crucial in domains where errors have severe consequences. Existing models not
only lack transparency but also risk exploiting unreliable or misleading
features, which undermines both robustness and the validity of their
explanations. Concept Bottleneck Models (CBMs) aim to improve transparency by
reasoning through human-interpretable concepts. Still, they require costly
concept annotations and lack spatial grounding, often failing to identify which
regions support each concept. We propose SEG-MIL-CBM, a novel framework that
integrates concept-guided image segmentation into an attention-based multiple
instance learning (MIL) framework, where each segmented region is treated as an
instance and the model learns to aggregate evidence across them. By reasoning
over semantically meaningful regions aligned with high-level concepts, our
model highlights task-relevant evidence, down-weights irrelevant cues, and
produces spatially grounded, concept-level explanations without requiring
annotations of concepts or groups. SEG-MIL-CBM achieves robust performance
across settings involving spurious correlations (unintended dependencies
between background and label), input corruptions (perturbations that degrade
visual quality), and large-scale benchmarks, while providing transparent,
concept-level explanations.

</details>


### [96] [Let Features Decide Their Own Solvers: Hybrid Feature Caching for Diffusion Transformers](https://arxiv.org/abs/2510.04188)
*Shikang Zheng,Guantao Chen,Qinming Zhou,Yuqi Lin,Lixuan He,Chang Zou,Peiliang Cai,Jiacheng Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: HyCa employs a dimension-wise, hybrid ODE-based caching framework to accelerate diffusion transformers without retraining, achieving near-lossless speedups by modeling per-dimension dynamics as a mixture of ODEs.


<details>
  <summary>Details</summary>
Motivation: Diffusion transformers suffer from expensive iterative sampling; uniform feature caching fails to exploit per-dimension dynamic heterogeneity, calling for a training-free acceleration that respects dimension-wise evolution.

Method: Model hidden feature evolution as a mixture of ODEs across feature dimensions; introduce HyCa, a Hybrid ODE solver that applies dimension-wise caching strategies to forecast or reuse representations during sampling, avoiding retraining.

Result: Near-lossless acceleration across diverse domains/models: 5.55x speedup on FLUX, 5.56x on HunyuanVideo, 6.24x on Qwen-Image and Qwen-Image-Edit, without retraining.

Conclusion: Dimension-wise caching with a Hybrid ODE solver markedly speeds up diffusion transformer sampling across multiple domains while preserving fidelity and without requiring retraining.

Abstract: Diffusion Transformers offer state-of-the-art fidelity in image and video
synthesis, but their iterative sampling process remains a major bottleneck due
to the high cost of transformer forward passes at each timestep. To mitigate
this, feature caching has emerged as a training-free acceleration technique
that reuses or forecasts hidden representations. However, existing methods
often apply a uniform caching strategy across all feature dimensions, ignoring
their heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by
modeling hidden feature evolution as a mixture of ODEs across dimensions, and
introduce HyCa, a Hybrid ODE solver inspired caching framework that applies
dimension-wise caching strategies. HyCa achieves near-lossless acceleration
across diverse domains and models, including 5.55 times speedup on FLUX, 5.56
times speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and
Qwen-Image-Edit without retraining.

</details>


### [97] [World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge](https://arxiv.org/abs/2510.04201)
*Moo Hyun Son,Jintaek Oh,Sun Bin Mun,Jaechul Roh,Sehyun Choi*

Main category: cs.CV

TL;DR: Introduces World-To-Image: an agent-driven web retrieval pipeline that sources images for novel concepts to augment T2I prompts, enabling better semantic alignment and aesthetics than SOTA with fewer iterations (≤3) on NICE, using LLMGrader and ImageReward for evaluation.


<details>
  <summary>Details</summary>
Motivation: T2I models suffer from knowledge cutoffs and struggle with novel or OOD entities; relying on static training data limits semantic fidelity. A dynamic, world-updated approach could bridge this gap.

Method: An autonomous agent searches the web to fetch images and related information for unknown concepts and uses this multimodal data to perform prompt optimization that steers a strong generative backbone toward accurate synthesis; evaluated with contemporary semantic/visual metrics (LLMGrader, ImageReward) and benchmarked on NICE.

Result: Significant improvements over state-of-the-art in semantic alignment and visual aesthetics; +8.1% accuracy-to-prompt on NICE; efficient operation in under three iterations.

Conclusion: World-To-Image demonstrates a practical pathway to keep T2I systems aligned with a changing world by integrating real-time, web-sourced knowledge during generation; code release aids reproducibility.

Abstract: While text-to-image (T2I) models can synthesize high-quality images, their
performance degrades significantly when prompted with novel or
out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We
introduce World-To-Image, a novel framework that bridges this gap by empowering
T2I generation with agent-driven world knowledge. We design an agent that
dynamically searches the web to retrieve images for concepts unknown to the
base model. This information is then used to perform multimodal prompt
optimization, steering powerful generative backbones toward an accurate
synthesis. Critically, our evaluation goes beyond traditional metrics,
utilizing modern assessments like LLMGrader and ImageReward to measure true
semantic fidelity. Our experiments show that World-To-Image substantially
outperforms state-of-the-art methods in both semantic alignment and visual
aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated
NICE benchmark. Our framework achieves these results with high efficiency in
less than three iterations, paving the way for T2I systems that can better
reflect the ever-changing real world. Our demo code is available
here\footnote{https://github.com/mhson-kyle/World-To-Image}.

</details>


### [98] [MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering](https://arxiv.org/abs/2510.04220)
*Lixuan He,Shikang Zheng,Linfeng Zhang*

Main category: cs.CV

TL;DR: MASC introduces a geometry-aware, density-driven hierarchical clustering of token embeddings to replace flat AR token vocabularies, yielding faster training and improved image generation quality.


<details>
  <summary>Details</summary>
Motivation: AR models for image generation are hampered by predicting over a flat, high-dimensional token vocabulary that ignores the embedding geometry, making learning inefficient and limiting quality.

Method: MASC constructs a hierarchical semantic tree directly from the codebook by using a geometry-aware distance metric and density-driven agglomerative clustering to model the token embedding manifold; it converts flat prediction into structured predictions and acts as a plug-and-play module.

Result: Training efficiency improves up to 57%; FID on LlamaGen-XL improves from 2.87 to 2.58, showing better generation quality and competitive performance with state-of-the-art methods.

Conclusion: Structuring the prediction space via manifold-aligned hierarchical clustering provides a strong inductive bias, significantly boosting AR-based generative modeling and challenging the notion that architectural complexity alone drives gains.

Abstract: Autoregressive (AR) models have shown great promise in image generation, yet
they face a fundamental inefficiency stemming from their core component: a
vast, unstructured vocabulary of visual tokens. This conventional approach
treats tokens as a flat vocabulary, disregarding the intrinsic structure of the
token embedding space where proximity often correlates with semantic
similarity. This oversight results in a highly complex prediction task, which
hinders training efficiency and limits final generation quality. To resolve
this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled
framework that constructs a hierarchical semantic tree directly from the
codebook's intrinsic structure. MASC employs a novel geometry-aware distance
metric and a density-driven agglomerative construction to model the underlying
manifold of the token embeddings. By transforming the flat, high-dimensional
prediction task into a structured, hierarchical one, MASC introduces a
beneficial inductive bias that significantly simplifies the learning problem
for the AR model. MASC is designed as a plug-and-play module, and our extensive
experiments validate its effectiveness: it accelerates training by up to 57%
and significantly improves generation quality, reducing the FID of LlamaGen-XL
from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly
competitive with state-of-the-art methods, establishing that structuring the
prediction space is as crucial as architectural innovation for scalable
generative modeling.

</details>


### [99] [Zoom-In to Sort AI-Generated Images Out](https://arxiv.org/abs/2510.04225)
*Yikun Ji,Yan Hong,Bowen Deng,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: A two-stage VLM-based forensic framework (ZoomIn) enhances detection and interpretability of AI-generated imagery, backed by a new dataset (MagniFake) with bounding boxes and explanations, achieving 96.39% accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the inability of vision-language models to detect subtle artifacts in high-quality synthetic images, and provide interpretable explanations to support digital integrity.

Method: ZoomIn operates in two stages: (1) a scan identifies suspicious regions in the image, (2) a zoomed-in analysis on those regions yields a grounded verdict. Training uses MagniFake, a dataset of 20k real and synthetic images annotated with bounding boxes and forensic explanations generated via an automated VLM-based pipeline.

Result: Achieves 96.39% accuracy with robust generalization and provides human-understandable, visually grounded explanations.

Conclusion: ZoomIn improves both forensic accuracy and interpretability for AI-generated imagery; MagniFake enables effective training and evaluation, addressing the interpretability gap in VLM-based forensics.

Abstract: The rapid growth of AI-generated imagery has blurred the boundary between
real and synthetic content, raising critical concerns for digital integrity.
Vision-language models (VLMs) offer interpretability through explanations but
often fail to detect subtle artifacts in high-quality synthetic images. We
propose ZoomIn, a two-stage forensic framework that improves both accuracy and
interpretability. Mimicking human visual inspection, ZoomIn first scans an
image to locate suspicious regions and then performs a focused analysis on
these zoomed-in areas to deliver a grounded verdict. To support training, we
introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images
annotated with bounding boxes and forensic explanations, generated through an
automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust
generalization, while providing human-understandable explanations grounded in
visual evidence.

</details>


### [100] [A Recursive Pyramidal Algorithm for Solving the Image Registration Problem](https://arxiv.org/abs/2510.04231)
*Stefan Dirnstorfer*

Main category: cs.CV

TL;DR: A compact, end-to-end trainable image registration method requiring minimal data and code, demonstrated on stereo vision; trained with ~74 images on a 19x15 window, and ~12 lines of Python.


<details>
  <summary>Details</summary>
Motivation: Image registration seeks to align two images by a transformation; current approaches often require heavy data, long training, or complex code. This work proposes a simple, trainable solution that is data-efficient and easily implementable.

Method: An end-to-end trainable algorithm that can be implemented in a few lines of Python code. It is designed to work with very little training data and training time, demonstrated on a stereo-vision task.

Result: The method achieves accurate results in some settings while requiring only modest training data/time. In the stereo vision example, it was trained from 74 images on a 19×15 input window and can be implemented with about a dozen lines of Python code.

Conclusion: The proposed approach is concise, easy to implement, and well-suited for scenarios with limited training data, time, or code complexity; it may serve as a starting point for similar registration problems.

Abstract: The problem of image registration is finding a transformation that aligns two
images, such that the corresponding points are in the same location. This paper
introduces a simple, end-to-end trainable algorithm that is implementable in a
few lines of Python code. The approach is shown to work with very little
training data and training time, while achieving accurate results in some
settings. An example application to stereo vision was trained from 74 images on
a 19x15 input window. With just a dozen lines of Python code this algorithm
excels in brevity and may serve as a good start in related scenarios with
limitations to training data, training time or code complexity.

</details>


### [101] [Detection of retinal diseases using an accelerated reused convolutional network](https://arxiv.org/abs/2510.04232)
*Amin Ahmadi Kasani,Hedieh Sajedi*

Main category: cs.CV

TL;DR: Introduces ArConv, a new convolutional layer to reduce model complexity for mobile eye-disease diagnosis, achieving higher accuracy with fewer parameters than MobileNetV2 on RfMiD.


<details>
  <summary>Details</summary>
Motivation: Increase accessibility of CNN-based eye-disease diagnosis by designing efficient convolutional layers suitable for mobile devices and resource-constrained settings.

Method: Proposes ArConv layers integrated into a general CNN model; compares performance against MobileNetV2 on the RfMiD dataset, reporting parameter counts and accuracy.

Result: ArConv-based model has 1.3M parameters and achieves 0.9328 accuracy on the RfMiD test set, outperforming MobileNetV2 which has 2.2M parameters and 0.9266 accuracy under identical conditions.

Conclusion: Efficient ArConv layers yield a mobile-friendly CNN with competitive or superior accuracy for eye-disease detection, suggesting potential for broader accessibility of diagnostic AI.

Abstract: Convolutional neural networks are continually evolving, with some efforts
aimed at improving accuracy, others at increasing speed, and some at enhancing
accessibility. Improving accessibility broadens the application of neural
networks across a wider range of tasks, including the detection of eye
diseases. Early diagnosis of eye diseases and consulting an ophthalmologist can
prevent many vision disorders. Given the importance of this issue, various
datasets have been collected from the cornea to facilitate the process of
making neural network models. However, most of the methods introduced in the
past are computationally complex. In this study, we tried to increase the
accessibility of deep neural network models. We did this at the most
fundamental level, specifically by redesigning and optimizing the convolutional
layers. By doing so, we created a new general model that incorporates our novel
convolutional layer named ArConv layers. Thanks to the efficient performance of
this new layer, the model has suitable complexity for use in mobile phones and
can perform the task of diagnosing the presence of disease with high accuracy.
The final model we present contains only 1.3 million parameters. In comparison
to the MobileNetV2 model, which has 2.2 million parameters, our model
demonstrated better accuracy when trained and evaluated on the RfMiD dataset
under identical conditions, achieving an accuracy of 0.9328 versus 0.9266 on
the RfMiD test set.

</details>


### [102] [Scaling Sequence-to-Sequence Generative Neural Rendering](https://arxiv.org/abs/2510.04236)
*Shikun Liu,Kam Woh Ng,Wonbong Jang,Jiadong Guo,Junlin Han,Haozhe Liu,Yiannis Douratsos,Juan C. Pérez,Zijian Zhou,Chi Phung,Tao Xiang,Juan-Manuel Pérez-Rúa*

Main category: cs.CV

TL;DR: Kaleido reframes 3D rendering as a video-like sequence-to-sequence task using a decoder-only rectified flow transformer, enabling 6-DoF view synthesis from arbitrary reference views and unifying 3D with video modeling; pre-trained on large-scale video data to boost spatial consistency and reduce 3D data requirements; achieves state-of-the-art, strong zero-shot and competitive many-view performance.


<details>
  <summary>Details</summary>
Motivation: To overcome scarce labeled 3D data and architectural constraints by treating 3D rendering as a sub-domain of video modelling, enabling flexible view synthesis with no explicit 3D representations.

Method: A masked autoregressive framework within a decoder-only rectified flow transformer that handles variable numbers of reference and target views, pre-trained on large-scale video data, unifying 3D and video modelling, no extra architectural modifications for 3D integration.

Result: SOTA on view synthesis benchmarks; zero-shot improvements in few-view settings; matches per-scene optimization quality in many-view settings.

Conclusion: Kaleido provides a scalable, unified approach for photorealistic neural rendering that bridges 3D and video, reducing reliance on camera-labeled 3D datasets and enabling flexible 6-DoF view synthesis without explicit 3D representations.

Abstract: We present Kaleido, a family of generative models designed for
photorealistic, unified object- and scene-level neural rendering. Kaleido
operates on the principle that 3D can be regarded as a specialised sub-domain
of video, expressed purely as a sequence-to-sequence image synthesis task.
Through a systemic study of scaling sequence-to-sequence generative neural
rendering, we introduce key architectural innovations that enable our model to:
i) perform generative view synthesis without explicit 3D representations; ii)
generate any number of 6-DoF target views conditioned on any number of
reference views via a masked autoregressive framework; and iii) seamlessly
unify 3D and video modelling within a single decoder-only rectified flow
transformer. Within this unified framework, Kaleido leverages large-scale video
data for pre-training, which significantly improves spatial consistency and
reduces reliance on scarce, camera-labelled 3D datasets -- all without any
architectural modifications. Kaleido sets a new state-of-the-art on a range of
view synthesis benchmarks. Its zero-shot performance substantially outperforms
other generative methods in few-view settings, and, for the first time, matches
the quality of per-scene optimisation methods in many-view settings.

</details>


### [103] [The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast): Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation](https://arxiv.org/abs/2510.04243)
*Jincan Lou,Jingkun Chen,Haoquan Li,Hang Li,Wenjian Huang,Weihua Chen,Fan Wang,Jianguo Zhang*

Main category: cs.CV

TL;DR: CoSSeg-TTA: a compact semi-supervised, domain-adaptive liver segmentation framework for GED4 MRI that uses nnU-Netv2, mean-teacher, a histogram-based style transfer + contrast-aware domain module, and test-time adaptation to improve Dice and Hausdorff and generalization under low annotations.


<details>
  <summary>Details</summary>
Motivation: To address limited annotations, cross-center variability, and domain shifts in contrast-enhanced MRI liver segmentation, without relying on complex cross-modality translation methods that can distort structure.

Method: nnU-Netv2-based segmentation with (1) a semi-supervised mean-teacher framework to leverage unlabeled volumes, (2) a domain adaptation module comprising a randomized histogram-based style transfer function and a trainable contrast-aware network to enrich domain diversity and mitigate center variability, and (3) continual test-time adaptation during inference.

Result: The approach consistently outperforms the nnU-Netv2 baseline in Dice and Hausdorff Distance and shows strong generalization to unseen domains under low-annotation conditions.

Conclusion: CoSSeg-TTA provides a robust, compact pipeline for cross-center liver segmentation in GED4 MRI, effectively leveraging unlabeled data and domain adaptation with test-time updates to achieve improved accuracy and generalization.

Abstract: Accurate liver segmentation from contrast-enhanced MRI is essential for
diagnosis, treatment planning, and disease monitoring. However, it remains
challenging due to limited annotated data, heterogeneous enhancement protocols,
and significant domain shifts across scanners and institutions. Traditional
image-to-image translation frameworks have made great progress in domain
generalization, but their application is not straightforward. For example,
Pix2Pix requires image registration, and cycle-GAN cannot be integrated
seamlessly into segmentation pipelines. Meanwhile, these methods are originally
used to deal with cross-modality scenarios, and often introduce structural
distortions and suffer from unstable training, which may pose drawbacks in our
single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a
compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary
phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised
mean teacher scheme to exploit large amounts of unlabeled volumes. A domain
adaptation module, incorporating a randomized histogram-based style appearance
transfer function and a trainable contrast-aware network, enriches domain
diversity and mitigates cross-center variability. Furthermore, a continual
test-time adaptation strategy is employed to improve robustness during
inference. Extensive experiments demonstrate that our framework consistently
outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff
Distance while exhibiting strong generalization to unseen domains under
low-annotation conditions.

</details>


### [104] [Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks](https://arxiv.org/abs/2510.04245)
*Ayushi Mehrotra,Derek Peng,Dipkamal Bhusal,Nidhi Rastogi*

Main category: cs.CV

TL;DR: Patch-agnostic defense against adversarial patches that uses concept-based explanations to suppress influential concept activation vectors, neutralizing patch effects without explicit detection; outperforms PatchCleanser on Imagenette with ResNet-50 across patch sizes/locations.


<details>
  <summary>Details</summary>
Motivation: Adversarial patch attacks can cause targeted misclassification in real-world settings. Existing defenses rely on prior knowledge of patch size or location, limiting their applicability. A patch-agnostic, scalable defense is needed.

Method: Leverages concept-based explanations to identify and suppress the most influential concept activation vectors, effectively neutralizing patch-induced activations without detecting patches. Implemented/evaluated on Imagenette with a ResNet-50 backbone, comparing against PatchCleanser.

Result: Outperforms the state-of-the-art PatchCleanser in robust and clean accuracy and maintains strong performance across varying patch sizes and locations.

Conclusion: Integrating interpretability (concept-based explanations) with robustness yields a scalable defense against adversarial patch attacks, suggesting concept-driven defenses as a promising broader strategy.

Abstract: Adversarial patch attacks pose a practical threat to deep learning models by
forcing targeted misclassifications through localized perturbations, often
realized in the physical world. Existing defenses typically assume prior
knowledge of patch size or location, limiting their applicability. In this
work, we propose a patch-agnostic defense that leverages concept-based
explanations to identify and suppress the most influential concept activation
vectors, thereby neutralizing patch effects without explicit detection.
Evaluated on Imagenette with a ResNet-50, our method achieves higher robust and
clean accuracy than the state-of-the-art PatchCleanser, while maintaining
strong performance across varying patch sizes and locations. Our results
highlight the promise of combining interpretability with robustness and suggest
concept-driven defenses as a scalable strategy for securing machine learning
models against adversarial patch attacks.

</details>


### [105] [Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition](https://arxiv.org/abs/2510.04282)
*Yu Kiu,Lau,Chao Chen,Ge Jin,Chen Feng*

Main category: cs.CV

TL;DR: Adapt-STformer introduces a Recurrent Deformable Transformer Encoder (Recurrent-DTE) for Seq-VPR, enabling flexible seq-lengths with fast inference and low memory usage. It claims state-of-the-art gains in recall (up to 17%), faster sequence extraction (−36%), and reduced memory (−35%) on Nordland, Oxford, and NuScenes datasets.


<details>
  <summary>Details</summary>
Motivation: Transformer-based Seq-VPR methods struggle to balance performance with flexibility and efficiency. There is a need for models that can handle varying sequence lengths in real-time with limited memory without sacrificing accuracy.

Method: Proposes Adapt-STformer built around a Recurrent Deformable Transformer Encoder (Recurrent-DTE) that uses an iterative recurrent mechanism to fuse information from multiple sequential frames, naturally supporting variable seq-lengths and enabling fast, memory-efficient inference.

Result: Experimental results on Nordland, Oxford, and NuScenes show recall improvements up to 17%, sequence extraction time reduced by 36%, and memory usage decreased by 35% compared to the second-best baseline.

Conclusion: Adapt-STformer fills a gap in transformer-based Seq-VPR by delivering a flexible, efficient model that maintains or improves recognition performance while reducing inference time and memory footprint.

Abstract: Sequential Visual Place Recognition (Seq-VPR) leverages transformers to
capture spatio-temporal features effectively; however, existing approaches
prioritize performance at the expense of flexibility and efficiency. In
practice, a transformer-based Seq-VPR model should be flexible to the number of
frames per sequence (seq-length), deliver fast inference, and have low memory
usage to meet real-time constraints. To our knowledge, no existing
transformer-based Seq-VPR method achieves both flexibility and efficiency. To
address this gap, we propose Adapt-STformer, a Seq-VPR method built around our
novel Recurrent Deformable Transformer Encoder (Recurrent-DTE), which uses an
iterative recurrent mechanism to fuse information from multiple sequential
frames. This design naturally supports variable seq-lengths, fast inference,
and low memory usage. Experiments on the Nordland, Oxford, and NuScenes
datasets show that Adapt-STformer boosts recall by up to 17% while reducing
sequence extraction time by 36% and lowering memory usage by 35% compared to
the second-best baseline.

</details>


### [106] [ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation](https://arxiv.org/abs/2510.04290)
*Jay Zhangjie Wu,Xuanchi Ren,Tianchang Shen,Tianshi Cao,Kai He,Yifan Lu,Ruiyuan Gao,Enze Xie,Shiyi Lan,Jose M. Alvarez,Jun Gao,Sanja Fidler,Zian Wang,Huan Ling*

Main category: cs.CV

TL;DR: ChronoEdit reframes image editing as a video-generation problem to enforce physical consistency, using temporal reasoning and pre-trained video priors; introduces PBench-Edit benchmark; achieves state-of-the-art results; code for 14B and 2B models to be released.


<details>
  <summary>Details</summary>
Motivation: Image editing for applications like world simulation requires edits that remain physically coherent over time. Current single-image editing with large generative models often lacks temporal consistency and physics-aware behavior, limiting reliability in dynamic contexts.

Method: Treat input and edited images as the first and last frames of a video, enabling leveraging pretrained video generative models to capture appearance and implicit physics. Introduce a temporal reasoning stage at inference time where the target frame is denoised with reasoning tokens to imagine a plausible editing trajectory, constraining the solution space to physically viable transformations. Drop reasoning tokens after a few steps to reduce computation, and validate on a new PBench-Edit benchmark designed for physical consistency in prompts.

Result: ChronoEdit outperforms state-of-the-art baselines in both visual fidelity and physical plausibility on PBench-Edit, demonstrating stronger temporal coherence and physically viable edits. Code and models for 14B and 2B variants will be released on the project page.

Conclusion: By reframing image editing as a constrained video-generation problem and incorporating explicit temporal reasoning, ChronoEdit achieves physically consistent edits suitable for world-simulation tasks, with accessible resources (code/models) for large and small scale variants.

Abstract: Recent advances in large generative models have significantly advanced image
editing and in-context image generation, yet a critical gap remains in ensuring
physical consistency, where edited objects must remain coherent. This
capability is especially vital for world simulation related tasks. In this
paper, we present ChronoEdit, a framework that reframes image editing as a
video generation problem. First, ChronoEdit treats the input and edited images
as the first and last frames of a video, allowing it to leverage large
pretrained video generative models that capture not only object appearance but
also the implicit physics of motion and interaction through learned temporal
consistency. Second, ChronoEdit introduces a temporal reasoning stage that
explicitly performs editing at inference time. Under this setting, the target
frame is jointly denoised with reasoning tokens to imagine a plausible editing
trajectory that constrains the solution space to physically viable
transformations. The reasoning tokens are then dropped after a few steps to
avoid the high computational cost of rendering a full video. To validate
ChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for
contexts that require physical consistency, and demonstrate that ChronoEdit
surpasses state-of-the-art baselines in both visual fidelity and physical
plausibility. Code and models for both the 14B and 2B variants of ChronoEdit
will be released on the project page:
https://research.nvidia.com/labs/toronto-ai/chronoedit

</details>


### [107] [CARE-PD: A Multi-Site Anonymized Clinical Dataset for Parkinson's Disease Gait Assessment](https://arxiv.org/abs/2510.04312)
*Vida Adeli,Ivan Klabucar,Javad Rajabi,Benjamin Filtjens,Soroush Mehraban,Diwei Wang,Hyewon Seo,Trung-Hieu Hoang,Minh N. Do,Candice Muller,Claudia Oliveira,Daniel Boari Coelho,Pieter Ginis,Moran Gilat,Alice Nieuwboer,Joke Spildooren,Lucas Mckay,Hyeokhyen Kwon,Gari Clifford,Christine Esper,Stewart Factor,Imari Genias,Amirhossein Dadashzadeh,Leia Shum,Alan Whone,Majid Mirmehdi,Andrea Iaboni,Babak Taati*

Main category: cs.CV

TL;DR: CARE-PD is the largest diverse PD gait dataset that converts 3D motion to SMPL meshes and enables both supervised UPDRS gait-score prediction and unsupervised pretext tasks; encoders outperform handcrafted gait features, and pretraining on CARE-PD yields major gains in 3D pose accuracy and PD severity detection; dataset and benchmarks are publicly released for non-commercial research.


<details>
  <summary>Details</summary>
Motivation: There is a lack of large, diverse, clinically annotated PD gait datasets, limiting generalizable modeling and evaluation of PD gait analysis methods.

Method: Assemble CARE-PD: 9 cohorts from 8 centers, anonymized SMPL mesh representations from RGB/MOCAP data via a harmonized preprocessing pipeline; establish two benchmarks (supervised UPDRS gait-score prediction and unsupervised pretext tasks: 2D-to-3D keypoint lifting and full-body 3D reconstruction); evaluate clinical prediction under four protocols (within-dataset, cross-dataset, leave-one-dataset-out, multi-dataset in-domain adaptation); compare state-of-the-art motion encoders against handcrafted gait features; pretrain models on CARE-PD; release code and data for non-commercial research.

Result: Encoders consistently outperform traditional handcrafted gait features for clinical prediction. Pretraining on CARE-PD dramatically improves 3D mesh estimations (MPJPE from 60.8 mm to 7.5 mm) and boosts PD severity macro-F1 by 17 percentage points, demonstrating the value of clinically curated, diverse training data.

Conclusion: CARE-PD provides a large, diverse, clinically relevant resource that enhances both 3D gait reconstruction and PD severity prediction, supports robust cross-domain evaluation, and is publicly available to accelerate PD gait analysis research.

Abstract: Objective gait assessment in Parkinson's Disease (PD) is limited by the
absence of large, diverse, and clinically annotated motion datasets. We
introduce CARE-PD, the largest publicly available archive of 3D mesh gait data
for PD, and the first multi-site collection spanning 9 cohorts from 8 clinical
centers. All recordings (RGB video or motion capture) are converted into
anonymized SMPL meshes via a harmonized preprocessing pipeline. CARE-PD
supports two key benchmarks: supervised clinical score prediction (estimating
Unified Parkinson's Disease Rating Scale, UPDRS, gait scores) and unsupervised
motion pretext tasks (2D-to-3D keypoint lifting and full-body 3D
reconstruction). Clinical prediction is evaluated under four generalization
protocols: within-dataset, cross-dataset, leave-one-dataset-out, and
multi-dataset in-domain adaptation. To assess clinical relevance, we compare
state-of-the-art motion encoders with a traditional gait-feature baseline,
finding that encoders consistently outperform handcrafted features. Pretraining
on CARE-PD reduces MPJPE (from 60.8mm to 7.5mm) and boosts PD severity macro-F1
by 17 percentage points, underscoring the value of clinically curated, diverse
training data. CARE-PD and all benchmark code are released for non-commercial
research at https://neurips2025.care-pd.ca/.

</details>


### [108] [GenAR: Next-Scale Autoregressive Generation for Spatial Gene Expression Prediction](https://arxiv.org/abs/2510.04315)
*Jiarui Ouyang,Yihui Wang,Yihang Gao,Yingxue Xu,Shu Yang,Hao Chen*

Main category: cs.CV

TL;DR: GenAR is a coarse-to-fine, autoregressive model that predicts raw, discrete gene counts from H&E images by clustering genes into hierarchical groups and generating discrete tokens, achieving state-of-the-art performance across four spatial transcriptomics datasets.


<details>
  <summary>Details</summary>
Motivation: Current spatial transcriptomics (ST) imputations often predict genes independently and treat expression as continuous regression. This ignores cross-gene co-expression structures and the discrete, count-based nature of gene expression, leading to biologically implausible outputs and challenges for downstream analyses. There is a need for a model that captures cross-gene dependencies and directly predicts counts from histology to enable cost-effective, accurate molecular profiling.

Method: GenAR clusters genes into hierarchical groups to reveal cross-gene dependencies, models expression as codebook-free discrete token generation to predict raw counts, and uses a coarse-to-fine autoregressive scheme. It conditions decoding on fused histological (H&E) and spatial embeddings, and provides an information-theoretic justification: discreteness avoids log-induced biases and the coarse-to-fine factorization aligns with principled conditional decomposition.

Result: Extensive experiments on four Spatial Transcriptomics datasets across tissue types show GenAR achieves state-of-the-art performance, demonstrating robust cross-tissue generalization and potential for precision medicine and cost-effective profiling. Code is publicly available.

Conclusion: GenAR offers a scalable, cost-effective framework for accurate gene-expression imputation from histology by leveraging cross-gene dependencies and discrete token generation in a coarse-to-fine autoregressive setting, with implications for precision medicine and molecular profiling.

Abstract: Spatial Transcriptomics (ST) offers spatially resolved gene expression but
remains costly. Predicting expression directly from widely available
Hematoxylin and Eosin (H&E) stained images presents a cost-effective
alternative. However, most computational approaches (i) predict each gene
independently, overlooking co-expression structure, and (ii) cast the task as
continuous regression despite expression being discrete counts. This mismatch
can yield biologically implausible outputs and complicate downstream analyses.
We introduce GenAR, a multi-scale autoregressive framework that refines
predictions from coarse to fine. GenAR clusters genes into hierarchical groups
to expose cross-gene dependencies, models expression as codebook-free discrete
token generation to directly predict raw counts, and conditions decoding on
fused histological and spatial embeddings. From an information-theoretic
perspective, the discrete formulation avoids log-induced biases and the
coarse-to-fine factorization aligns with a principled conditional
decomposition. Extensive experimental results on four Spatial Transcriptomics
datasets across different tissue types demonstrate that GenAR achieves
state-of-the-art performance, offering potential implications for precision
medicine and cost-effective molecular profiling. Code is publicly available at
https://github.com/oyjr/genar.

</details>


### [109] [Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction](https://arxiv.org/abs/2510.04365)
*Yuhao Luo,Yuang Zhang,Kehua Chen,Xinyu Zheng,Shucheng Zhang,Sikai Chen,Yinhai Wang*

Main category: cs.CV

TL;DR: A diffusion-based framework (Diffusion^2) for momentary pedestrian trajectory prediction using two diffusion models to reconstruct past and forecast future paths, with uncertainty estimation and adaptive noise, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: In real-world settings, observers often have only momentary data (e.g., blind spots), making accurate trajectory prediction critical for safety in autonomous driving and human-robot interaction.

Method: Diffusion^2 couples two sequential diffusion models: a backward model that reconstructs unobserved historical trajectories and a forward model that predicts future trajectories. It includes a dual-head parameterization to estimate aleatoric uncertainty in the reconstructed history and a temporally adaptive noise module to modulate forward diffusion noise.

Result: The approach achieves state-of-the-art performance on momentary trajectory prediction benchmarks, specifically ETH/UCY and Stanford Drone datasets.

Conclusion: A two-stage diffusion-based framework with uncertainty-aware components effectively handles momentary trajectory prediction gaps and improves safety-relevant prediction tasks.

Abstract: Accurate pedestrian trajectory prediction is crucial for ensuring safety and
efficiency in autonomous driving and human-robot interaction scenarios. Earlier
studies primarily utilized sufficient observational data to predict future
trajectories. However, in real-world scenarios, such as pedestrians suddenly
emerging from blind spots, sufficient observational data is often unavailable
(i.e. momentary trajectory), making accurate prediction challenging and
increasing the risk of traffic accidents. Therefore, advancing research on
pedestrian trajectory prediction under extreme scenarios is critical for
enhancing traffic safety. In this work, we propose a novel framework termed
Diffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists
of two sequentially connected diffusion models: one for backward prediction,
which generates unobserved historical trajectories, and the other for forward
prediction, which forecasts future trajectories. Given that the generated
unobserved historical trajectories may introduce additional noise, we propose a
dual-head parameterization mechanism to estimate their aleatoric uncertainty
and design a temporally adaptive noise module that dynamically modulates the
noise scale in the forward diffusion process. Empirically, Diffusion^2 sets a
new state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford
Drone datasets.

</details>


### [110] [MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator](https://arxiv.org/abs/2510.04390)
*Xuehai He,Shijie Zhou,Thivyanth Venkateswaran,Kaizhi Zheng,Ziyu Wan,Achuta Kadambi,Xin Eric Wang*

Main category: cs.CV

TL;DR: MorphoSim is a language-guided framework that generates editable, controllable 4D scenes with multi-view consistency, enabling object-level edits and viewpoint flexibility, with interactive edits via trajectory-guided generation and feature field distillation.


<details>
  <summary>Details</summary>
Motivation: World models for robotics benefit from controllable, editable spatiotemporal environments to enable scalable training data, reproducible evaluation, and flexible task design; existing text-to-video methods are limited to 2D views and offer limited interaction.

Method: MorphoSim integrates trajectory-guided generation with feature field distillation to create 4D scenes from natural language. It supports object-level controls (directing, recoloring, removing), arbitrary viewpoints, and interactive edits without full re-generation.

Result: Experiments show MorphoSim maintains high scene fidelity while enabling controllability and editability.

Conclusion: MorphoSim provides a practical framework for editable, controllable 4D world models in robotics; code is available at the provided GitHub repository.

Abstract: World models that support controllable
  and editable spatiotemporal environments are valuable
  for robotics, enabling scalable training data, repro ducible evaluation, and
flexible task design. While
  recent text-to-video models generate realistic dynam ics, they are
constrained to 2D views and offer limited
  interaction. We introduce MorphoSim, a language guided framework that
generates 4D scenes with
  multi-view consistency and object-level controls. From
  natural language instructions, MorphoSim produces
  dynamic environments where objects can be directed,
  recolored, or removed, and scenes can be observed
  from arbitrary viewpoints. The framework integrates
  trajectory-guided generation with feature field dis tillation, allowing edits
to be applied interactively
  without full re-generation. Experiments show that Mor phoSim maintains high
scene fidelity while enabling
  controllability and editability. The code is available
  at https://github.com/eric-ai-lab/Morph4D.

</details>


### [111] [Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting](https://arxiv.org/abs/2510.04401)
*Xuyang Guo,Zekai Huang,Zhenmei Shi,Zhao Song,Jiahao Zhang*

Main category: cs.CV

TL;DR: VLMs struggle with compositional counting: reliable when counting a single shape type but fail when multiple shapes are present; a minimalist benchmark (VLMCountBench) reveals a fundamental limit.


<details>
  <summary>Details</summary>
Motivation: To assess the counting and compositional reasoning abilities of Vision-Language Models under controlled, minimal conditions, isolating counting from other factors.

Method: Create VLMCountBench with basic geometric shapes and compositions; enforce strict independent variable control; perform ablations on color, size, and prompt design to study their effects on counting performance.

Result: VLMs can count reliably when only one shape type is present, but show substantial failures on compositional counting with multiple shape types, indicating a fundamental limitation.

Conclusion: Current VLMs exhibit limited compositional counting abilities in minimal settings, suggesting direction for future work in improving counting/generalization and prompting research.

Abstract: Vision-Language Models (VLMs) have become a central focus of today's AI
community, owing to their impressive abilities gained from training on
large-scale vision-language data from the Web. These models have demonstrated
strong performance across diverse tasks, including image understanding, video
understanding, complex visual reasoning, and embodied AI. Despite these
noteworthy successes, a fundamental question remains: Can VLMs count objects
correctly? In this paper, we introduce a simple yet effective benchmark,
VLMCountBench, designed under a minimalist setting with only basic geometric
shapes (e.g., triangles, circles) and their compositions, focusing exclusively
on counting tasks without interference from other factors. We adopt strict
independent variable control and systematically study the effects of simple
properties such as color, size, and prompt refinement in a controlled ablation.
Our empirical results reveal that while VLMs can count reliably when only one
shape type is present, they exhibit substantial failures when multiple shape
types are combined (i.e., compositional counting). This highlights a
fundamental empirical limitation of current VLMs and motivates important
directions for future research.

</details>


### [112] [CodeFormer++: Blind Face Restoration Using Deformable Registration and Deep Metric Learning](https://arxiv.org/abs/2510.04410)
*Venkata Bharath Reddy Reddem,Akshay P Sarashetti,Ranjith Merugu,Amit Satish Unde*

Main category: cs.CV

TL;DR: CodeFormer++ enhances blind face restoration by integrating identity preservation, high-quality generation, and dynamic fusion of features using deformable registration, texture-guided restoration, and deep metric learning, achieving better visual fidelity and identity consistency.


<details>
  <summary>Details</summary>
Motivation: Existing BFR methods trade off between visual quality and identity fidelity when injecting generative priors; there is a need to leverage generative priors without compromising identity.

Method: Three sub-tasks: (i) identity-preserving face restoration; (ii) high-quality face generation; (iii) dynamic fusion of identity features with realistic texture details. Key contributions: (1) learning-based deformable face registration aligning generated and restored faces; (2) texture-guided restoration network to extract/transfer texture to boost identity-preserving restoration; (3) deep metric learning with informative positive and hard negative samples to fuse identity-preserving and generative features.

Result: Extensive experiments on real-world and synthetic datasets show CodeFormer++ achieves superior performance in visual fidelity and identity consistency.

Conclusion: The proposed framework effectively maximizes the utility of generative priors for BFR while preserving identity, with dynamic fusion and texture guidance yielding state-of-the-art results.

Abstract: Blind face restoration (BFR) has attracted increasing attention with the rise
of generative methods. Most existing approaches integrate generative priors
into the restoration pro- cess, aiming to jointly address facial detail
generation and identity preservation. However, these methods often suffer from
a trade-off between visual quality and identity fidelity, leading to either
identity distortion or suboptimal degradation removal. In this paper, we
present CodeFormer++, a novel framework that maximizes the utility of
generative priors for high-quality face restoration while preserving identity.
We decompose BFR into three sub-tasks: (i) identity- preserving face
restoration, (ii) high-quality face generation, and (iii) dynamic fusion of
identity features with realistic texture details. Our method makes three key
contributions: (1) a learning-based deformable face registration module that
semantically aligns generated and restored faces; (2) a texture guided
restoration network to dynamically extract and transfer the texture of
generated face to boost the quality of identity-preserving restored face; and
(3) the integration of deep metric learning for BFR with the generation of
informative positive and hard negative samples to better fuse identity-
preserving and generative features. Extensive experiments on real-world and
synthetic datasets demonstrate that, the pro- posed CodeFormer++ achieves
superior performance in terms of both visual fidelity and identity consistency.

</details>


### [113] [A.I.R.: Enabling Adaptive, Iterative, and Reasoning-based Frame Selection For Video Question Answering](https://arxiv.org/abs/2510.04428)
*Yuanhao Zou,Shengji Jin,Andong Deng,Youpeng Zhao,Jun Wang,Chen Chen*

Main category: cs.CV

TL;DR: A.I.R. is a training-free, adaptive, iterative frame-selection method for VideoQA that uses a strong Vision-Language Model to perform deep semantic analysis on complex queries within a cost-efficient loop, selecting a small batch of high-potential frames to improve accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current VideoQA frame-selection methods force a trade-off: lightweight similarity models (e.g., CLIP) are fast but fail to capture complex query nuances, while VLM-based analyses are accurate but computationally expensive. There is a need for a method that combines deep understanding with efficiency.

Method: A.I.R. employs a training-free, adaptive, iterative framework. It uses a powerful Vision-Language Model to perform deep semantic reasoning on complex queries and evaluates frames in small, high-potential batches in successive iterations, balancing accuracy with computational cost.

Result: Experiments on multiple VideoQA benchmarks show that A.I.R. outperforms existing frame-selection approaches, enhances the performance of the underlying foundation VLM, and delivers substantial computational efficiency gains compared to other VLM-based techniques.

Conclusion: A.I.R. provides a practical, training-free solution that achieves accurate, semantically-informed frame selection for VideoQA with reduced computation, effectively bridging the gap between fast lightweight methods and expensive deep analyses.

Abstract: Effectively applying Vision-Language Models (VLMs) to Video Question
Answering (VideoQA) hinges on selecting a concise yet comprehensive set of
frames, as processing entire videos is computationally infeasible. However,
current frame selection methods face a critical trade-off: approaches relying
on lightweight similarity models, such as CLIP, often fail to capture the
nuances of complex queries, resulting in inaccurate similarity scores that
cannot reflect the authentic query-frame relevance, which further undermines
frame selection. Meanwhile, methods that leverage a VLM for deeper analysis
achieve higher accuracy but incur prohibitive computational costs. To address
these limitations, we propose A.I.R., a training-free approach for Adaptive,
Iterative, and Reasoning-based frame selection. We leverage a powerful VLM to
perform deep, semantic analysis on complex queries, and this analysis is
deployed within a cost-effective iterative loop that processes only a small
batch of the most high-potential frames at a time. Extensive experiments on
various VideoQA benchmarks demonstrate that our approach outperforms existing
frame selection methods, significantly boosts the performance of the foundation
VLM, and achieves substantial gains in computational efficiency over other
VLM-based techniques.

</details>


### [114] [REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization](https://arxiv.org/abs/2510.04450)
*Qiyuan He,Yicong Li,Haotian Ye,Jinghao Wang,Xinyao Liao,Pheng-Ann Heng,Stefano Ermon,James Zou,Angela Yao*

Main category: cs.CV

TL;DR: ReAR is a token-wise regularization strategy for visual autoregressive generation that aligns generator and tokenizer by training the next-token predictor to recover the current token's visual embedding and to predict the next token's embedding under noisy context, improving AR performance to rival larger diffusion models with minimal changes.


<details>
  <summary>Details</summary>
Motivation: Identify and mitigate generator-tokenizer inconsistency as a core bottleneck in visual autoregressive generation, which causes tokens to be poorly decoded by the tokenizer and harms quality.

Method: Train a causal transformer with a token-wise regularization objective: when predicting the next token, also recover the visual embedding of the current token and predict the embedding of the target token under a noisy context. No changes to the tokenizer, generation order, inference pipeline, or external models.

Result: On ImageNet, gFID improves from 3.02 to 1.86 and IS improves to 316.9 using a standard rasterization-based tokenizer. With advanced tokenizers, gFID reaches 1.42 with only 177M parameters, matching the performance of larger diffusion models (675M).

Conclusion: A simple, effective training tweak that substantially narrows the gap between visual autoregressive models and diffusion models without altering tokenizers or pipelines, achieving diffusion-like performance with fewer parameters.

Abstract: Visual autoregressive (AR) generation offers a promising path toward unifying
vision and language models, yet its performance remains suboptimal against
diffusion models. Prior work often attributes this gap to tokenizer limitations
and rasterization ordering. In this work, we identify a core bottleneck from
the perspective of generator-tokenizer inconsistency, i.e., the AR-generated
tokens may not be well-decoded by the tokenizer. To address this, we propose
reAR, a simple training strategy introducing a token-wise regularization
objective: when predicting the next token, the causal transformer is also
trained to recover the visual embedding of the current token and predict the
embedding of the target token under a noisy context. It requires no changes to
the tokenizer, generation order, inference pipeline, or external models.
Despite its simplicity, reAR substantially improves performance. On ImageNet,
it reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard
rasterization-based tokenizer. When applied to advanced tokenizers, it achieves
a gFID of 1.42 with only 177M parameters, matching the performance with larger
state-of-the-art diffusion models (675M).

</details>


### [115] [SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection](https://arxiv.org/abs/2510.04472)
*Baber Jan,Saeed Anwar,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais*

Main category: cs.CV

TL;DR: SPEGNet offers a unified, multi-scale COD network with channel calibration and spatial enhancement that derives object boundaries from context-rich representations, employing scale-aware edge modulation for progressive refinement, achieving real-time COD with strong S_alpha across CAMO, COD10K, and NC4K.


<details>
  <summary>Details</summary>
Motivation: Current camouflaged object detection methods stack multiple components (boundary modules, attention, multi-scale processors) leading to high computational burden with diminishing returns. A unified design is proposed to preserve camouflage-relevant details while maintaining efficiency.

Method: A unified architecture that integrates multi-scale features via channel calibration and spatial enhancement. Boundaries are derived directly from context-rich representations, maintaining semantic-spatial alignment. Progressive refinement uses scale-adaptive edge modulation with peak influence at intermediate resolutions to balance boundary precision and regional consistency.

Result: Quantitative gains reported: S_alpha of 0.887 on CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed. Demonstrates robustness across object sizes, occlusion, and ambiguous boundaries.

Conclusion: SPEGNet simplifies COD design by unifying feature processing and edge modulation, achieving strong performance across scales while maintaining efficiency. Code and weights are available for reproducibility.

Abstract: Camouflaged object detection segments objects with intrinsic similarity and
edge disruption. Current detection methods rely on accumulated complex
components. Each approach adds components such as boundary modules, attention
mechanisms, and multi-scale processors independently. This accumulation creates
a computational burden without proportional gains. To manage this complexity,
they process at reduced resolutions, eliminating fine details essential for
camouflage. We present SPEGNet, addressing fragmentation through a unified
design. The architecture integrates multi-scale features via channel
calibration and spatial enhancement. Boundaries emerge directly from
context-rich representations, maintaining semantic-spatial alignment.
Progressive refinement implements scale-adaptive edge modulation with peak
influence at intermediate resolutions. This design strikes a balance between
boundary precision and regional consistency. SPEGNet achieves 0.887 $S_\alpha$
on CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed.
Our approach excels across scales, from tiny, intricate objects to large,
pattern-similar ones, while handling occlusion and ambiguous boundaries. Code,
model weights, and results are available on
\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.

</details>


### [116] [MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models](https://arxiv.org/abs/2510.04477)
*Soo Yong Kim,Suin Cho,Vincent-Daniel Yun,Gyeongyeon Hwang*

Main category: cs.CV

TL;DR: MedCLM generates chain-of-thought (CoT) VQA data from detection datasets by linking lesion boxes to organ segmentation, uses a three-stage CoT curriculum, and achieves state-of-the-art results on medical VQA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Bridging clinical diagnostic reasoning with AI requires scalable data and models that can reason step-by-step. Current medical VQA datasets lack aligned CoT data and principled curricula for learning reasoning with visual grounding.

Method: Convert detection annotations into large-scale medical VQA data with CoT reasoning by linking lesion boxes to organ segmentation and structured rationales. Train with Integrated CoT-Curriculum: Easy stage with explicit lesion boxes for grounding, Medium stage for implicit localization, Hard stage for weakly supervised reasoning.

Result: Achieves state-of-the-art performance on several medical VQA benchmarks, demonstrating scalability and potential for clinically aligned medical vision-language models.

Conclusion: MedCLM provides a scalable pipeline to generate clinically relevant CoT data and to train vision-language models capable of step-by-step medical reasoning, bridging detection and reasoning in medical imaging.

Abstract: Bridging clinical diagnostic reasoning with AI remains a central challenge in
medical imaging. We introduce MedCLM, an automated pipeline that converts
detection datasets into large-scale medical visual question answering (VQA)
data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ
segmentation and structured rationales. These contextual signals enable medical
vision-language models to generate question-answer pairs with step-by-step
reasoning. To utilize this data effectively, we propose an Integrated
CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes
for visual grounding, a Medium stage that encourages implicit localization, and
a Hard stage for weakly supervised reasoning. Experimental results demonstrate
that MedCLM attains state-of-the-art performance on several medical VQA
benchmarks, providing a scalable framework for developing clinically aligned
medical vision-language models.

</details>


### [117] [VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery](https://arxiv.org/abs/2510.04479)
*Nonghai Zhang,Zeyu Zhang,Jiazi Wang,Yang Zhao,Hao Tang*

Main category: cs.CV

TL;DR: VaseVQA-3D introduces a first 3D VQA dataset for ancient Greek pottery and a domain-adaptive VaseVLM, achieving notable gains on R@1 and lexical similarity.


<details>
  <summary>Details</summary>
Motivation: Cultural heritage domains face data scarcity and limited domain knowledge; current VLMs struggle with 3D vase analysis.

Method: Construct 664 3D vase models with QA data and establish a complete data pipeline; develop VaseVLM with domain-adaptive training for vase artifacts.

Result: 12.8% improvement in R@1 and 6.6% improvement in lexical similarity over previous SOTA on VaseVQA-3D.

Conclusion: Advances digital heritage preservation by enabling better recognition/understanding of 3D vase artifacts; provides new technical pathways for domain-specific VLMs.

Abstract: Vision-Language Models (VLMs) have achieved significant progress in
multimodal understanding tasks, demonstrating strong capabilities particularly
in general tasks such as image captioning and visual reasoning. However, when
dealing with specialized cultural heritage domains like 3D vase artifacts,
existing models face severe data scarcity issues and insufficient domain
knowledge limitations. Due to the lack of targeted training data, current VLMs
struggle to effectively handle such culturally significant specialized tasks.
To address these challenges, we propose the VaseVQA-3D dataset, which serves as
the first 3D visual question answering dataset for ancient Greek pottery
analysis, collecting 664 ancient Greek vase 3D models with corresponding
question-answer data and establishing a complete data construction pipeline. We
further develop the VaseVLM model, enhancing model performance in vase artifact
analysis through domain-adaptive training. Experimental results validate the
effectiveness of our approach, where we improve by 12.8% on R@1 metrics and by
6.6% on lexical similarity compared with previous state-of-the-art on the
VaseVQA-3D dataset, significantly improving the recognition and understanding
of 3D vase artifacts, providing new technical pathways for digital heritage
preservation research.

</details>


### [118] [TBStar-Edit: From Image Editing Pattern Shifting to Consistency Enhancement](https://arxiv.org/abs/2510.04483)
*Hao Fang,Zechao Zhan,Weixin Feng,Ziwei Huang,XuBin Li,Tiezheng Ge*

Main category: cs.CV

TL;DR: TBStar-Edit is a domain-specific image-editing model for e-commerce; it uses a data pipeline, hierarchical architecture, and a two-stage training regimen to preserve product integrity and layout, achieving higher consistency than general-domain editors on an e-commerce benchmark.


<details>
  <summary>Details</summary>
Motivation: General image editing models struggle with consistency and preserving product integrity in e-commerce imagery; there is a need for domain-tailored editing models.

Method: Data engineering: collection, construction, filtering, augmentation to build high-quality, instruction-following, consistent editing data. Model architecture: hierarchical framework with a base model, pattern-shifting modules, and consistency-enhancement modules. Training: two-stage; stage 1 focuses on editing pattern shifting; stage 2 on consistency enhancement; separate datasets per stage.

Result: On a self-proposed e-commerce benchmark, TBStar-Edit outperforms general-domain editing models on objective VIE Score and subjective user preference.

Conclusion: TBStar-Edit achieves precise, high-fidelity edits while preserving product appearance/layout; demonstrates domain-specific gains and potential for improved e-commerce image editing.

Abstract: Recent advances in image generation and editing technologies have enabled
state-of-the-art models to achieve impressive results in general domains.
However, when applied to e-commerce scenarios, these general models often
encounter consistency limitations. To address this challenge, we introduce
TBStar-Edit, an new image editing model tailored for the e-commerce domain.
Through rigorous data engineering, model architecture design and training
strategy, TBStar-Edit achieves precise and high-fidelity image editing while
maintaining the integrity of product appearance and layout. Specifically, for
data engineering, we establish a comprehensive data construction pipeline,
encompassing data collection, construction, filtering, and augmentation, to
acquire high-quality, instruction-following, and strongly consistent editing
data to support model training. For model architecture design, we design a
hierarchical model framework consisting of a base model, pattern shifting
modules, and consistency enhancement modules. For model training, we adopt a
two-stage training strategy to enhance the consistency preservation: first
stage for editing pattern shifting, and second stage for consistency
enhancement. Each stage involves training different modules with separate
datasets. Finally, we conduct extensive evaluations of TBStar-Edit on a
self-proposed e-commerce benchmark, and the results demonstrate that
TBStar-Edit outperforms existing general-domain editing models in both
objective metrics (VIE Score) and subjective user preference.

</details>


### [119] [Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation](https://arxiv.org/abs/2510.04504)
*Zijing Hu,Yunze Tong,Fengda Zhang,Junkun Yuan,Jun Xiao,Kun Kuang*

Main category: cs.CV

TL;DR: Introduces asynchronous diffusion models that assign per-pixel timesteps to decouple denoising and improve prompt-driven alignment in text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: Synchronous denoising in standard diffusion models causes prompt-related regions to reference unrelated regions at the same noise level, leading to weaker text-to-image alignment.

Method: Allocate distinct timesteps per pixel and reformulate pixel-wise denoising; dynamically modulate per-pixel timestep schedules so prompt-related regions denoise more gradually, leveraging clearer inter-pixel context.

Result: Extensive experiments show significant improvement in text-to-image alignment across diverse prompts; code released at the provided GitHub URL.

Conclusion: Asynchronous per-pixel timesteps enhance text-to-image alignment by enabling better context utilization during denoising, validating the proposed framework.

Abstract: Diffusion models have achieved impressive results in generating high-quality
images. Yet, they often struggle to faithfully align the generated images with
the input prompts. This limitation arises from synchronous denoising, where all
pixels simultaneously evolve from random noise to clear images. As a result,
during generation, the prompt-related regions can only reference the unrelated
regions at the same noise level, failing to obtain clear context and ultimately
impairing text-to-image alignment. To address this issue, we propose
asynchronous diffusion models -- a novel framework that allocates distinct
timesteps to different pixels and reformulates the pixel-wise denoising
process. By dynamically modulating the timestep schedules of individual pixels,
prompt-related regions are denoised more gradually than unrelated regions,
thereby allowing them to leverage clearer inter-pixel context. Consequently,
these prompt-related regions achieve better alignment in the final images.
Extensive experiments demonstrate that our asynchronous diffusion models can
significantly improve text-to-image alignment across diverse prompts. The code
repository for this work is available at https://github.com/hu-zijing/AsynDM.

</details>


### [120] [TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling](https://arxiv.org/abs/2510.04533)
*Hyunmin Cho,Donghoon Ahn,Susung Hong,Jee Eun Kim,Seungryong Kim,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: A lightweight, plug-and-play guidance technique (TAG) for diffusion models that amplifies tangential score components relative to an intermediate projection, steering sampling toward high-probability regions without modifying the model.


<details>
  <summary>Details</summary>
Motivation: Diffusion models achieve state-of-the-art image generation but often exhibit semantic inconsistencies or hallucinations. Existing inference-time guidance methods either rely on external signals or architectural changes, adding computational overhead. There is a need for direct, efficient guidance that does not modify the diffusion model.

Method: Use an intermediate sample as a projection basis. Decompose the score into tangential and normal components relative to this basis and amplify the tangential component. Formalize with a first-order Taylor expansion to show that tangential amplification biases the trajectory toward higher-probability regions. The approach is plug-and-play and architecture-agnostic, requiring minimal additional computation.

Result: Empirically, TAG improves sampling fidelity and reduces semantic inconsistencies with only modest additional computation. It is architecture-agnostic and can be integrated without modifying the diffusion model or retraining, offering improved sample quality across different diffusion setups.

Conclusion: TAG provides a direct, efficient trajectory-based guidance framework for diffusion models. It complements existing guidance methods, enabling improved sample quality without changing the underlying model, and opens a new direction for diffusion-time guidance via trajectory manipulation.

Abstract: Recent diffusion models achieve the state-of-the-art performance in image
generation, but often suffer from semantic inconsistencies or hallucinations.
While various inference-time guidance methods can enhance generation, they
often operate indirectly by relying on external signals or architectural
modifications, which introduces additional computational overhead. In this
paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and
direct guidance method that operates solely on trajectory signals without
modifying the underlying diffusion model. TAG leverages an intermediate sample
as a projection basis and amplifies the tangential components of the estimated
scores with respect to this basis to correct the sampling trajectory. We
formalize this guidance process by leveraging a first-order Taylor expansion,
which demonstrates that amplifying the tangential component steers the state
toward higher-probability regions, thereby reducing inconsistencies and
enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module
that improves diffusion sampling fidelity with minimal computational addition,
offering a new perspective on diffusion guidance.

</details>


### [121] [Conditional Representation Learning for Customized Tasks](https://arxiv.org/abs/2510.04564)
*Honglin Liu,Chao Sun,Peng Hu,Yunfan Li,Xi Peng*

Main category: cs.CV

TL;DR: CRL builds user-specific representations by using an LLM to generate descriptive texts that define a semantic basis for a chosen criterion, and then projects image features into this conditional space via a vision-language model, enabling task-tailored representations without heavy fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Universal representations emphasize dominant semantics and may not align with downstream, user-specified tasks. There is a need for efficient, adaptable representations tailored to particular criteria without costly annotation or fine-tuning.

Method: CRL constructs a semantic basis from LLM-generated descriptive texts to capture the target criterion, then uses a vision-language model to project images into the conditional feature space defined by that basis, enabling conditional representations suitable for various tasks.

Result: Empirical evaluations on classification and retrieval demonstrate that CRL yields superior and more general performance for customized criteria compared to universal embeddings.

Conclusion: CRL provides a flexible, annotation-efficient approach to obtain task-tailored representations by operating in a criterion-conditioned semantic space without requiring extensive labeled fine-tuning.

Abstract: Conventional representation learning methods learn a universal representation
that primarily captures dominant semantics, which may not always align with
customized downstream tasks. For instance, in animal habitat analysis,
researchers prioritize scene-related features, whereas universal embeddings
emphasize categorical semantics, leading to suboptimal results. As a solution,
existing approaches resort to supervised fine-tuning, which however incurs high
computational and annotation costs. In this paper, we propose Conditional
Representation Learning (CRL), aiming to extract representations tailored to
arbitrary user-specified criteria. Specifically, we reveal that the semantics
of a space are determined by its basis, thereby enabling a set of descriptive
words to approximate the basis for a customized feature space. Building upon
this insight, given a user-specified criterion, CRL first employs a large
language model (LLM) to generate descriptive texts to construct the semantic
basis, then projects the image representation into this conditional feature
space leveraging a vision-language model (VLM). The conditional representation
better captures semantics for the specific criterion, which could be utilized
for multiple customized tasks. Extensive experiments on classification and
retrieval tasks demonstrate the superiority and generality of the proposed CRL.
The code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.

</details>


### [122] [Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior](https://arxiv.org/abs/2510.04587)
*Sheng Wang,Ruiming Wu,Charles Herndon,Yihang Liu,Shunsuke Koga,Jeanne Shen,Zhi Huang*

Main category: cs.CV

TL;DR: Behavior-grounded pathology AI: records routine WSI viewing to create scalable supervision, enabling Pathologist-o3, a two-stage agent for ROI proposing and reasoning, achieving high precision/recall on GI lymph-node metastasis and outperforming OpenAI o3.


<details>
  <summary>Details</summary>
Motivation: Pathology AI lacks scalable, expert viewing supervision because expert interpretation is tacit and not captured in textbooks; need to align AI agents with real clinician behavior to enable practical, explainable decision-making.

Method: An AI Session Recorder unobtrusively captures standard WSI viewer actions (inspect/peek, magnification, bounding boxes) and converts them into standardized behavioral commands. Human-in-the-loop reviews AI-drafted rationales to produce Pathology-CoT datasets. Train Pathologist-o3, a two-stage agent: stage 1 proposes regions of interest; stage 2 applies behavior-guided reasoning to explain decisions.

Result: On GI lymph-node metastasis detection, Pathologist-o3 achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, beating the OpenAI o3 model and generalizing across backbones; labeling time reduced roughly six-fold.

Conclusion: This work represents one of the first behavior-grounded, agentic systems in pathology, showing that everyday viewer logs can provide scalable, expert-validated supervision to build human-aligned, upgradeable clinical AI.

Abstract: Diagnosing a whole-slide image is an interactive, multi-stage process
involving changes in magnification and movement between fields. Although recent
pathology foundation models are strong, practical agentic systems that decide
what field to examine next, adjust magnification, and deliver explainable
diagnoses are still lacking. The blocker is data: scalable, clinically aligned
supervision of expert viewing behavior that is tacit and experience-based, not
written in textbooks or online, and therefore absent from large language model
training. We introduce the AI Session Recorder, which works with standard WSI
viewers to unobtrusively record routine navigation and convert the viewer logs
into standardized behavioral commands (inspect or peek at discrete
magnifications) and bounding boxes. A lightweight human-in-the-loop review
turns AI-drafted rationales into the Pathology-CoT dataset, a form of paired
"where to look" and "why it matters" supervision produced at roughly six times
lower labeling time. Using this behavioral data, we build Pathologist-o3, a
two-stage agent that first proposes regions of interest and then performs
behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,
it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the
state-of-the-art OpenAI o3 model and generalizing across backbones. To our
knowledge, this constitutes one of the first behavior-grounded agentic systems
in pathology. Turning everyday viewer logs into scalable, expert-validated
supervision, our framework makes agentic pathology practical and establishes a
path to human-aligned, upgradeable clinical AI.

</details>


### [123] [A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification](https://arxiv.org/abs/2510.04628)
*Hao Liu,Yunhao Gao,Wei Li,Mingyang Zhang,Maoguo Gong,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: Introduces S^2Fin, a spatial-spectral-frequency interaction network for multimodal remote sensing image classification, integrating spatial, spectral, and frequency-domain fusion to improve performance with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: The challenge is to extract structural and detail features from heterogeneous and redundant multimodal remote sensing images. Conventional fusion methods struggle to capture high-frequency details and phase-related information across domains, especially under limited labeled data.

Method: Proposes a high-frequency sparse enhancement transformer with sparse spatial-spectral attention; introduces a two-level spatial-frequency fusion strategy (adaptive frequency channel module combining low-frequency structure with enhanced high-frequency details; high-frequency resonance mask emphasizing edges via phase similarity); incorporates a spatial-spectral attention fusion module to boost intermediate-layer features; implements pairwise fusion modules across spatial, spectral, and frequency domains.

Result: Empirical evaluation on four benchmark multimodal remote sensing datasets with limited labeled data shows superior classification performance, outperforming state-of-the-art methods. Code is released at the provided GitHub repository.

Conclusion: Frequency-domain learning combined with spatial-spectral attention enables more effective multimodal fusion and detail preservation in remote sensing data, offering a robust architecture (S^2Fin) that achieves strong results under limited-label conditions.

Abstract: Deep learning-based methods have achieved significant success in remote
sensing Earth observation data analysis. Numerous feature fusion techniques
address multimodal remote sensing image classification by integrating global
and local features. However, these techniques often struggle to extract
structural and detail features from heterogeneous and redundant multimodal
images. With the goal of introducing frequency domain learning to model key and
sparse detail features, this paper introduces the spatial-spectral-frequency
interaction network (S$^2$Fin), which integrates pairwise fusion modules across
the spatial, spectral, and frequency domains. Specifically, we propose a
high-frequency sparse enhancement transformer that employs sparse
spatial-spectral attention to optimize the parameters of the high-frequency
filter. Subsequently, a two-level spatial-frequency fusion strategy is
introduced, comprising an adaptive frequency channel module that fuses
low-frequency structures with enhanced high-frequency details, and a
high-frequency resonance mask that emphasizes sharp edges via phase similarity.
In addition, a spatial-spectral attention fusion module further enhances
feature extraction at intermediate layers of the network. Experiments on four
benchmark multimodal datasets with limited labeled data demonstrate that
S$^2$Fin performs superior classification, outperforming state-of-the-art
methods. The code is available at https://github.com/HaoLiu-XDU/SSFin.

</details>


### [124] [SFANet: Spatial-Frequency Attention Network for Deepfake Detection](https://arxiv.org/abs/2510.04630)
*Vrushank Ahire,Aniruddh Muley,Shivam Zample,Siddharth Verma,Pranav Menon,Surbhi Madan,Abhinav Dhall*

Main category: cs.CV

TL;DR: A hybrid ensemble framework combining transformer-based architectures (Swin Transformers, ViTs) with texture-based methods to detect manipulated media, using data-splitting, sequential training, frequency splitting, patch-based attention, and face segmentation to improve robustness and generalization; achieves state-of-the-art on DFWild-Cup.


<details>
  <summary>Details</summary>
Motivation: Deepfake detection lags in generalization across diverse datasets and generation techniques. We need robust, interpretable, and transferable cues. A hybrid ensemble can leverage global structural cues from transformers and local texture cues, while targeted training strategies address dataset imbalance and high-impact facial regions.

Method: Proposed an ensemble framework that merges transformer-based models (Swin Transformers and ViTs) with texture-based methods. Introduces data-splitting, sequential training, frequency splitting, patch-based attention, and face segmentation to emphasize regions like eyes and mouth and handle imbalanced datasets. Trains and evaluates on the DFWild-Cup dataset (a diverse subset of eight deepfake datasets), leveraging the complementarity of global and texture cues.

Result: Achieves state-of-the-art performance on DFWild-Cup. The ensemble benefits from the complementary strengths of transformers (global feature extraction) and texture-based methods (interpretability), demonstrating improved generalization across diverse deepfake datasets.

Conclusion: Hybrid transformer-texture ensembles offer a robust and generalizable solution to evolving deepfake detection challenges, balancing accuracy with interpretability for real-world deployment.

Abstract: Detecting manipulated media has now become a pressing issue with the recent
rise of deepfakes. Most existing approaches fail to generalize across diverse
datasets and generation techniques. We thus propose a novel ensemble framework,
combining the strengths of transformer-based architectures, such as Swin
Transformers and ViTs, and texture-based methods, to achieve better detection
accuracy and robustness. Our method introduces innovative data-splitting,
sequential training, frequency splitting, patch-based attention, and face
segmentation techniques to handle dataset imbalances, enhance high-impact
regions (e.g., eyes and mouth), and improve generalization. Our model achieves
state-of-the-art performance when tested on the DFWild-Cup dataset, a diverse
subset of eight deepfake datasets. The ensemble benefits from the
complementarity of these approaches, with transformers excelling in global
feature extraction and texturebased methods providing interpretability. This
work demonstrates that hybrid models can effectively address the evolving
challenges of deepfake detection, offering a robust solution for real-world
applications.

</details>


### [125] [Do Superpixel Segmentation Methods Influence Deforestation Image Classification?](https://arxiv.org/abs/2510.04645)
*Hugo Resende,Fabio A. Faria,Eduardo B. Neto,Isabela Borlido,Victor Sundermann,Silvio Jamil F. Guimarães,Álvaro L. Fazenda*

Main category: cs.CV

TL;DR: Segmentation method choice plus ensemble learning affects deforestation detection in remote sensing; beyond SLIC, top superpixel methods perform similarly, but classifier fusion yields gains.


<details>
  <summary>Details</summary>
Motivation: Assess whether using superior superpixel segmentation methods (beyond SLIC) improves labeling quality and downstream classifier performance for ForestEyes deforestation detection, given prior evidence of method differences in remote sensing.

Method: Compare five segmentation methods (four top-performing plus SLIC) on label generation; train multiple classifiers via PyCaret AutoML; then apply classifier fusion to create ensemble predictions.

Result: Minimal performance differences across single segmentation methods and top AutoML classifiers; classifier ensemble leads to notable gains in balanced accuracy.

Conclusion: The combination of segmentation choice and ML ensemble enhances deforestation detection; SLIC is not universally best, and leveraging ensemble methods with appropriate segmentation improves performance.

Abstract: Image segmentation is a crucial step in various visual applications,
including environmental monitoring through remote sensing. In the context of
the ForestEyes project, which combines citizen science and machine learning to
detect deforestation in tropical forests, image segments are used for labeling
by volunteers and subsequent model training. Traditionally, the Simple Linear
Iterative Clustering (SLIC) algorithm is adopted as the segmentation method.
However, recent studies have indicated that other superpixel-based methods
outperform SLIC in remote sensing image segmentation, and might suggest that
they are more suitable for the task of detecting deforested areas. In this
sense, this study investigated the impact of the four best segmentation
methods, together with SLIC, on the training of classifiers for the target
application. Initially, the results showed little variation in performance
among segmentation methods, even when selecting the top five classifiers using
the PyCaret AutoML library. However, by applying a classifier fusion approach
(ensemble of classifiers), noticeable improvements in balanced accuracy were
observed, highlighting the importance of both the choice of segmentation method
and the combination of machine learning-based models for deforestation
detection tasks.

</details>


### [126] [EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents](https://arxiv.org/abs/2510.04648)
*Buyuan Zhu,Shiyu Hu,Yiping Ma,Yuanming Zhang,Kang Hao Cheong*

Main category: cs.CV

TL;DR: EduPersona offers the first classroom benchmark for evaluating subjective abilities of educational LLMs. It spans two languages, three subjects, ten persona types (Big Five), with 1,308 authentic dialogue rounds (12,814 teacher-student Q&A turns) and ~128k stylized turns. It defines three tasks (coherence, realism, long-term persona consistency) and shows consistent improvements after persona-fine-tuning across LLMs: Task1 +33.6%, Task2 +30.6%, Task3 +14.9%; dataset/framework to be open-sourced.


<details>
  <summary>Details</summary>
Motivation: There is a gap in evaluating classroom-oriented subjective abilities of virtual student agents, hindering understanding of model boundaries and trustworthy deployment in education. A theory-grounded benchmark is needed to assess coherence, realism, and long-term persona consistency.

Method: Construct EduPersona: a large-scale, bilingual benchmark across two languages, three subjects, ten persona types based on the Big Five. Collect 1,308 authentic classroom dialogue rounds (12,814 Q&A turns) and expand via persona stylization to ~128k turns. Define three progressive tasks: Task1 basic coherence (alignment with context), Task2 student realism, Task3 long-term persona consistency. Evaluate three representative LLMs, comparing original models to ten persona-fine-tuned variants trained on EduPersona.

Result: Fine-tuned personas yield significant gains across tasks: Task1 +33.6%, Task2 +30.6%, Task3 +14.9% on average, indicating dataset effectiveness and varying difficulty across persona modeling.

Conclusion: EduPersona delivers the first classroom-centric benchmark for subjective abilities, offers a decoupled and verifiable evaluation framework grounded in educational theory, and will open-source both dataset and framework to advance trustworthy, human-like AI in education.

Abstract: As large language models are increasingly integrated into education, virtual
student agents are becoming vital for classroom simulation and teacher
training. Yet their classroom-oriented subjective abilities remain largely
unassessed, limiting understanding of model boundaries and hindering
trustworthy deployment. We present EduPersona, a large-scale benchmark spanning
two languages, three subjects, and ten persona types based on the Big Five
theory. The dataset contains 1,308 authentic classroom dialogue rounds,
corresponding to 12,814 teacher-student Q&A turns, and is further expanded
through persona stylization into roughly 10 times larger scale (128k turns),
providing a solid foundation for evaluation. Building on this resource, we
decompose hard-to-quantify subjective performance into three progressive tasks:
TASK1 basic coherence (whether behavior, emotion, expression, and voice align
with classroom context), TASK2 student realism, and TASK3 long-term persona
consistency, thereby establishing an evaluation framework grounded in
educational theory and research value. We conduct systematic experiments on
three representative LLMs, comparing their original versions with ten
persona-fine-tuned variants trained on EduPersona. Results show consistent and
significant average improvements across all tasks: TASK1 +33.6%, TASK2 +30.6%,
and TASK3 +14.9%. These improvements highlight the dataset's effectiveness and
research value, while also revealing the heterogeneous difficulty of persona
modeling. In summary, EduPersona delivers the first classroom benchmark
centered on subjective abilities, establishes a decoupled and verifiable
research paradigm, and we will open-source both the dataset and the framework
to support the broader research community in advancing trustworthy and
human-like AI for education.

</details>


### [127] [MoME: Estimating Psychological Traits from Gait with Multi-Stage Mixture of Movement Experts](https://arxiv.org/abs/2510.04654)
*Andy Cǎtrunǎ,Adrian Cosma,Emilian Rǎdoi*

Main category: cs.CV

TL;DR: A hierarchical MoME architecture for multi-task prediction of psychological traits from gait; four-stage processing of movement complexity; achieves state-of-the-art performance on PsyMo with 17 traits, and shows auxiliary tasks boost trait estimation.


<details>
  <summary>Details</summary>
Motivation: Gait encodes rich biometric and behavioral information, but inferring psychological traits remains challenging and underexplored; need for interpretable, multi-task models that leverage gait data.

Method: Propose MoME: hierarchical, multi-stage mixture of movement experts; four stages of movement complexity; lightweight experts extract spatio-temporal features; task-specific gating modules weight experts across traits and stages; 2D pose sequences; uses auxiliary tasks (identity, gender, BMI) to boost learning.

Result: Evaluated on PsyMo benchmark with 17 traits; outperforms state-of-the-art gait models; 37.47% weighted F1 at run level; 44.6% at subject level; auxiliary tasks improve performance.

Conclusion: Demonstrates viability of multi-task gait-based learning for psychological trait estimation and lays groundwork for movement-informed psychological inference; future research directions include refining architecture and exploring other auxiliary tasks.

Abstract: Gait encodes rich biometric and behavioural information, yet leveraging the
manner of walking to infer psychological traits remains a challenging and
underexplored problem. We introduce a hierarchical Multi-Stage Mixture of
Movement Experts (MoME) architecture for multi-task prediction of psychological
attributes from gait sequences represented as 2D poses. MoME processes the
walking cycle in four stages of movement complexity, employing lightweight
expert models to extract spatio-temporal features and task-specific gating
modules to adaptively weight experts across traits and stages. Evaluated on the
PsyMo benchmark covering 17 psychological traits, our method outperforms
state-of-the-art gait analysis models, achieving a 37.47% weighted F1 score at
the run level and 44.6% at the subject level. Our experiments show that
integrating auxiliary tasks such as identity recognition, gender prediction,
and BMI estimation further improves psychological trait estimation. Our
findings demonstrate the viability of multi-task gait-based learning for
psychological trait estimation and provide a foundation for future research on
movement-informed psychological inference.

</details>


### [128] [ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion Models via Token-wise Adaptation and Attention Disentanglement](https://arxiv.org/abs/2510.04668)
*Habin Lim,Yeongseob Won,Juwon Seo,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: ConceptSplit provides a merging-free training approach (Token-wise Value Adaptation, ToVA) and an inference-time latent optimization (Latent Optimization for Disentangled Attention, LODA) to achieve robust multi-concept personalization in text-to-image diffusion, significantly reducing undesired concept mixing.


<details>
  <summary>Details</summary>
Motivation: Multi-concept personalization in T2I diffusion often suffers from concept interference (mixing) between learned subjects. Modifying key projections in existing methods can disrupt attention, worsening mixing.

Method: ToVA trains by modifying only the value projection in cross-attention, avoiding changes to the key projection to preserve the attention mechanism. LODA optimizes the input latent at inference to alleviate attention entanglement and promote disentangled attention among concepts.

Result: Qualitative and quantitative experiments show robust multi-concept personalization with mitigated interference between concepts. The authors release code at the provided GitHub repository.

Conclusion: ConceptSplit effectively tackles concept mixing through a merging-free training strategy and an inference-time latent optimization, offering a practical solution for disentangled multi-concept personalization in T2I diffusion.

Abstract: In recent years, multi-concept personalization for text-to-image (T2I)
diffusion models to represent several subjects in an image has gained much more
attention. The main challenge of this task is "concept mixing", where multiple
learned concepts interfere or blend undesirably in the output image. To address
this issue, in this paper, we present ConceptSplit, a novel framework to split
the individual concepts through training and inference. Our framework comprises
two key components. First, we introduce Token-wise Value Adaptation (ToVA), a
merging-free training method that focuses exclusively on adapting the value
projection in cross-attention. Based on our empirical analysis, we found that
modifying the key projection, a common approach in existing methods, can
disrupt the attention mechanism and lead to concept mixing. Second, we propose
Latent Optimization for Disentangled Attention (LODA), which alleviates
attention entanglement during inference by optimizing the input latent. Through
extensive qualitative and quantitative experiments, we demonstrate that
ConceptSplit achieves robust multi-concept personalization, mitigating
unintended concept interference. Code is available at
https://github.com/KU-VGI/ConceptSplit

</details>


### [129] [Label-Efficient Cross-Modality Generalization for Liver Segmentation in Multi-Phase MRI](https://arxiv.org/abs/2510.04705)
*Quang-Khai Bui-Tran,Minh-Toan Dinh,Thanh-Huy Nguyen,Ba-Thinh Lam,Mai-Anh Vu,Ulas Bagci*

Main category: cs.CV

TL;DR: Label-efficient, cross-modality liver segmentation in multi-phase MRI using a fine-tuned foundation 3D backbone and cross pseudo supervision, robust to unlabeled data and modality/vendor variability.


<details>
  <summary>Details</summary>
Motivation: Liver segmentation is critical for fibrosis assessment but labeled data are scarce and unevenly distributed across imaging modalities and vendors; real-world data have misalignment and missing phases, hindering cross-modality generalization.

Method: Adapt a foundation-scale 3D segmentation backbone via fine-tuning; use co-training with cross pseudo supervision to exploit unlabeled volumes; standardize preprocessing; avoid explicit spatial registration; promote cross-phase and cross-vendor generalization.

Result: The approach yields robust segmentation in both labeled and unlabeled domains, validating a label-efficient baseline for multi-phase, multi-vendor MRI; demonstrates potential of combining foundation-model adaptation with co-training for real-world clinical imaging.

Conclusion: Supports label-efficient, cross-modality liver segmentation in MRI; suggests that foundation model adaptation plus co-training can address real-world constraints without heavy labeling or spatial registration.

Abstract: Accurate liver segmentation in multi-phase MRI is vital for liver fibrosis
assessment, yet labeled data is often scarce and unevenly distributed across
imaging modalities and vendor systems. We propose a label-efficient
segmentation approach that promotes cross-modality generalization under
real-world conditions, where GED4 hepatobiliary-phase annotations are limited,
non-contrast sequences (T1WI, T2WI, DWI) are unlabeled, and spatial
misalignment and missing phases are common. Our method integrates a
foundation-scale 3D segmentation backbone adapted via fine-tuning, co-training
with cross pseudo supervision to leverage unlabeled volumes, and a standardized
preprocessing pipeline. Without requiring spatial registration, the model
learns to generalize across MRI phases and vendors, demonstrating robust
segmentation performance in both labeled and unlabeled domains. Our results
exhibit the effectiveness of our proposed label-efficient baseline for liver
segmentation in multi-phase, multi-vendor MRI and highlight the potential of
combining foundation model adaptation with co-training for real-world clinical
imaging tasks.

</details>


### [130] [ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion](https://arxiv.org/abs/2510.04706)
*Foivos Paraperas Papantoniou,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: Diffusion-based, identity-preserving face synthesis with explicit expression control via FLAME-guided cross-attention and adapters; supports micro-expressions and editing via a Reference Adapter.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of maintaining facial identity while enabling fine-grained, controllable expressions for AI-driven storytelling.

Method: Leverages an ID-consistent face foundation model with an expression cross-attention module guided by FLAME blendshape parameters. Trained on diverse image/video data rich in expressive variation. Includes a pluggable Reference Adapter for editing real images by transferring appearance from a reference frame during synthesis.

Result: Quantitative and qualitative evaluations show the approach outperforms existing methods in tailored and identity-consistent expression generation; code and models are released.

Conclusion: The framework enables faithful reimagining of any subject under specified expressions with broad expressiveness (including micro-expressions) and practical editing via reference frames.

Abstract: Human-centric generative models designed for AI-driven storytelling must
bring together two core capabilities: identity consistency and precise control
over human performance. While recent diffusion-based approaches have made
significant progress in maintaining facial identity, achieving fine-grained
expression control without compromising identity remains challenging. In this
work, we present a diffusion-based framework that faithfully reimagines any
subject under any particular facial expression. Building on an ID-consistent
face foundation model, we adopt a compositional design featuring an expression
cross-attention module guided by FLAME blendshape parameters for explicit
control. Trained on a diverse mixture of image and video data rich in
expressive variation, our adapter generalizes beyond basic emotions to subtle
micro-expressions and expressive transitions, overlooked by prior works. In
addition, a pluggable Reference Adapter enables expression editing in real
images by transferring the appearance from a reference frame during synthesis.
Extensive quantitative and qualitative evaluations show that our model
outperforms existing methods in tailored and identity-consistent expression
generation. Code and models can be found at
https://github.com/foivospar/Arc2Face.

</details>


### [131] [ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model](https://arxiv.org/abs/2510.04712)
*Luo Cheng,Song Siyang,Yan Siyuan,Yu Zhen,Ge Zongyuan*

Main category: cs.CV

TL;DR: Temporal diffusion with priors for realistic, diverse facial reactions in dialogue.


<details>
  <summary>Details</summary>
Motivation: To model stochasticity, dynamics, smoothness, and anatomical constraints in human facial reactions, addressing limitations of existing methods.

Method: A diffusion framework guided by two priors: (i) temporal facial behavioral kinematics and (ii) facial action unit dependencies, steering diffusion trajectories toward realistic reaction manifolds.

Result: Achieves state-of-the-art reaction quality on REACT2024, with improved diversity and reaction appropriateness.

Conclusion: ReactDiff enables realistic, diverse, and temporally coherent facial reactions in response to dialogue contexts.

Abstract: The automatic generation of diverse and human-like facial reactions in dyadic
dialogue remains a critical challenge for human-computer interaction systems.
Existing methods fail to model the stochasticity and dynamics inherent in real
human reactions. To address this, we propose ReactDiff, a novel temporal
diffusion framework for generating diverse facial reactions that are
appropriate for responding to any given dialogue context. Our key insight is
that plausible human reactions demonstrate smoothness, and coherence over time,
and conform to constraints imposed by human facial anatomy. To achieve this,
ReactDiff incorporates two vital priors (spatio-temporal facial kinematics)
into the diffusion process: i) temporal facial behavioral kinematics and ii)
facial action unit dependencies. These two constraints guide the model toward
realistic human reaction manifolds, avoiding visually unrealistic jitters,
unstable transitions, unnatural expressions, and other artifacts. Extensive
experiments on the REACT2024 dataset demonstrate that our approach not only
achieves state-of-the-art reaction quality but also excels in diversity and
reaction appropriateness.

</details>


### [132] [Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction](https://arxiv.org/abs/2510.04714)
*KunHo Heo,GiHyun Kim,SuYeon Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: An analysis of 3D semantic scene graph prediction shows object feature quality is pivotal. The paper introduces a highly discriminative object feature encoder with contrastive pretraining to decouple object representation from graph prediction, improving both object classification and relationship prediction, and achieving SOTA on 3DSSG when integrated into existing frameworks with combined geometric and semantic features.


<details>
  <summary>Details</summary>
Motivation: To address limitations in previous 3DSSG work where object/relationship features were not sufficiently discriminative and where heavy reliance on GNNs limited performance. The aim is to elevate the representational capacity of object features to boost overall scene graph accuracy.

Method: Develop a highly discriminative object feature encoder and apply contrastive pretraining that decouples object representation learning from scene graph prediction. Integrate the pretrained encoder into existing 3DSSG frameworks and fuse both geometric and semantic features for relationship prediction.

Result: Object classification accuracy improves and, when the pretrained encoder is plugged into existing frameworks, relationship prediction and other metrics also improve significantly. Experiments on the 3DSSG dataset show substantial improvements over previous state-of-the-art methods.

Conclusion: Object feature quality is crucial for 3D scene graph performance. The proposed discriminative encoder with contrastive pretraining enhances both object and relationship predictions, and the approach achieves SOTA results on 3DSSG while enabling effective integration with existing methods.

Abstract: 3D Semantic Scene Graph Prediction aims to detect objects and their semantic
relationships in 3D scenes, and has emerged as a crucial technology for
robotics and AR/VR applications. While previous research has addressed dataset
limitations and explored various approaches including Open-Vocabulary settings,
they frequently fail to optimize the representational capacity of object and
relationship features, showing excessive reliance on Graph Neural Networks
despite insufficient discriminative capability. In this work, we demonstrate
through extensive analysis that the quality of object features plays a critical
role in determining overall scene graph accuracy. To address this challenge, we
design a highly discriminative object feature encoder and employ a contrastive
pretraining strategy that decouples object representation learning from the
scene graph prediction. This design not only enhances object classification
accuracy but also yields direct improvements in relationship prediction.
Notably, when plugging in our pretrained encoder into existing frameworks, we
observe substantial performance improvements across all evaluation metrics.
Additionally, whereas existing approaches have not fully exploited the
integration of relationship information, we effectively combine both geometric
and semantic features to achieve superior relationship prediction.
Comprehensive experiments on the 3DSSG dataset demonstrate that our approach
significantly outperforms previous state-of-the-art methods. Our code is
publicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.

</details>


### [133] [Benchmark on Monocular Metric Depth Estimation in Wildlife Setting](https://arxiv.org/abs/2510.04723)
*Niccolò Niccoli,Lorenzo Seidenari,Ilaria Greco,Francesco Rovero*

Main category: cs.CV

TL;DR: A wildlife monocular depth estimation benchmark shows Depth Anything V2 as the best overall on 93 camera-trap images with ChARUCO ground truth; median depth extraction improves robustness; ZoeDepth fastest but least accurate; provides baselines and guidance for conservation monitoring.


<details>
  <summary>Details</summary>
Motivation: Fill the gap in systematic evaluation of monocular depth estimation (MDE) methods in real wildlife environments and provide benchmarks to guide practical deployment in conservation monitoring.

Method: Benchmark four state-of-the-art MDE methods (Depth Anything V2, ML Depth Pro, ZoeDepth, Metric3D) plus a geometric baseline on 93 camera-trap images with ground truth distances obtained using calibrated ChARUCO patterns; evaluate MAE and correlation; compare median vs mean depth extraction; assess computational efficiency.

Result: Depth Anything V2 achieved MAE 0.454 m and correlation 0.962; ZoeDepth exhibits substantial outdoor degradation (MAE 3.087 m); median-based depth extraction outperforms mean-based across methods; ZoeDepth fastest at 0.17 s/image, Depth Anything V2 at 0.22 s/image.

Conclusion: The benchmark establishes baselines for wildlife depth estimation, guiding practical deployments; Depth Anything V2 offers a favorable accuracy-speed trade-off for conservation monitoring, while some methods struggle in outdoor wildlife contexts, underscoring the need for environment-specific evaluation.

Abstract: Camera traps are widely used for wildlife monitoring, but extracting accurate
distance measurements from monocular images remains challenging due to the lack
of depth information. While monocular depth estimation (MDE) methods have
advanced significantly, their performance in natural wildlife environments has
not been systematically evaluated. This work introduces the first benchmark for
monocular metric depth estimation in wildlife monitoring conditions. We
evaluate four state-of-the-art MDE methods (Depth Anything V2, ML Depth Pro,
ZoeDepth, and Metric3D) alongside a geometric baseline on 93 camera trap images
with ground truth distances obtained using calibrated ChARUCO patterns. Our
results demonstrate that Depth Anything V2 achieves the best overall
performance with a mean absolute error of 0.454m and correlation of 0.962,
while methods like ZoeDepth show significant degradation in outdoor natural
environments (MAE: 3.087m). We find that median-based depth extraction
consistently outperforms mean-based approaches across all deep learning
methods. Additionally, we analyze computational efficiency, with ZoeDepth being
fastest (0.17s per image) but least accurate, while Depth Anything V2 provides
an optimal balance of accuracy and speed (0.22s per image). This benchmark
establishes performance baselines for wildlife applications and provides
practical guidance for implementing depth estimation in conservation monitoring
systems.

</details>


### [134] [ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcasts](https://arxiv.org/abs/2510.04739)
*Mehdi Houshmand Sarkhoosh,Frøy Øye,Henrik Nestor Sørlie,Nam Hoang Vu,Dag Johansen,Cise Midoglu,Tomas Kupka,Pål Halvorsen*

Main category: cs.CV

TL;DR: ExposureEngine delivers rotation-aware sponsor visibility analytics in sports broadcasts by predicting oriented bounding boxes for logos, paired with a new OBB dataset, enabling precise exposure metrics and NL-based reporting.


<details>
  <summary>Details</summary>
Motivation: Addresses the inadequacy of axis-aligned bounding boxes (HBB) in capturing sponsorship exposure under dynamic camera angles and perspective distortions, providing objective, scalable sponsor analytics in broadcasts.

Method: An end-to-end system that detects logos with oriented bounding boxes (OBB), trained on a 1,103-frame Swedish elite soccer dataset (670 unique logos). Achieves mAP@0.5 of 0.859, precision 0.96, recall 0.87. Integrates detections into a pipeline to compute exposure metrics (duration, on-screen coverage) and includes a language-driven agentic layer for natural-language reports and media content.

Result: Robust logo localization under diverse broadcast conditions; high mAP, precision, and recall; a publicly described dataset and analytics dashboard to support auditable sponsor measurement.

Conclusion: ExposureEngine offers a rotation-aware, end-to-end framework for accurate sponsor visibility analytics in sports media, enabling precise exposure metrics and user-friendly NL reporting, with potential applicability beyond soccer.

Abstract: Quantifying sponsor visibility in sports broadcasts is a critical marketing
task traditionally hindered by manual, subjective, and unscalable analysis
methods. While automated systems offer an alternative, their reliance on
axis-aligned Horizontal Bounding Box (HBB) leads to inaccurate exposuremetrics
when logos appear rotated or skewed due to dynamic camera angles and
perspective distortions. This paper introduces ExposureEngine, an end-to-end
system designed for accurate, rotation-aware sponsor visibility analytics in
sports broadcasts, demonstrated in a soccer case study. Our approach predicts
Oriented Bounding Box (OBB) to provide a geometrically precise fit to each logo
regardless of the orientation on-screen. To train and evaluate our detector, we
developed a new dataset comprising 1,103 frames from Swedish elite soccer,
featuring 670 unique sponsor logos annotated with OBBs. Our model achieves a
mean Average Precision (mAP@0.5) of 0.859, with a precision of 0.96 and recall
of 0.87, demonstrating robust performance in localizing logos under diverse
broadcast conditions. The system integrates these detections into an analytical
pipeline that calculates precise visibility metrics, such as exposure duration
and on-screen coverage. Furthermore, we incorporate a language-driven agentic
layer, enabling users to generate reports, summaries, and media content through
natural language queries. The complete system, including the dataset and the
analytics dashboard, provides a comprehensive solution for auditable and
interpretable sponsor measurement in sports media. An overview of the
ExposureEngine is available online: https://youtu.be/tRw6OBISuW4 .

</details>


### [135] [Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small Target Detection](https://arxiv.org/abs/2510.04741)
*Alina Ciocarlan,Sylvie Le Hégarat-Mascle,Sidonie Lefebvre*

Main category: cs.CV

TL;DR: Anomaly-aware integration into YOLO head for infrared small target detection (AA-YOLO) to reduce false alarms and improve robustness across backbones and data regimes.


<details>
  <summary>Details</summary>
Motivation: Infrared small target detection faces tiny targets, cluttered backgrounds, and high false alarm rates; robustness under limited data, noise, and domain shifts is needed; a generic, lightweight enhancement is desirable.

Method: Insert a statistical anomaly detection test into the YOLO detection head, treating small targets as anomalies against the background; this anomaly score modulates detections to control false positives; the approach is backbone-agnostic and compatible with lightweight models; extendable to instance segmentation YOLO.

Result: Competitive IRSTD performance on benchmarks; strong robustness to data scarcity, noise, and domain shifts; demonstrated versatility across backbones and potential for real-world deployment; code release planned.

Conclusion: AA-YOLO provides a versatile, efficient solution for IRSTD with improved false alarm control and cross-backbone applicability; suitable for resource-constrained deployments and possible extension to segmentation tasks.

Abstract: Infrared Small Target Detection (IRSTD) is a challenging task in defense
applications, where complex backgrounds and tiny target sizes often result in
numerous false alarms using conventional object detectors. To overcome this
limitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a
statistical anomaly detection test into its detection head. By treating small
targets as unexpected patterns against the background, AA-YOLO effectively
controls the false alarm rate. Our approach not only achieves competitive
performance on several IRSTD benchmarks, but also demonstrates remarkable
robustness in scenarios with limited training data, noise, and domain shifts.
Furthermore, since only the detection head is modified, our design is highly
generic and has been successfully applied across various YOLO backbones,
including lightweight models. It also provides promising results when
integrated into an instance segmentation YOLO. This versatility makes AA-YOLO
an attractive solution for real-world deployments where resources are
constrained. The code will be publicly released.

</details>


### [136] [Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics](https://arxiv.org/abs/2510.04753)
*Masoumeh Chapariniya,Teodora Vukovic,Sarah Ebling,Volker Dellwo*

Main category: cs.CV

TL;DR: Domain-specific two-stream transformers using 133 body keypoints achieve high accuracy in person identification from natural conversations; spatial cues outperform temporal dynamics, and late feature fusion yields the best performance.


<details>
  <summary>Details</summary>
Motivation: Address person identification in naturalistic, face-to-face interactions where both posture and motion cues could be informative; compare spatial vs temporal signal, and evaluate domain-specific vs transfer learning.

Method: Two-stream framework: spatial stream models spatial configurations of 133 COCO WholeBody keypoints; temporal stream models motion dynamics using a multi-scale temporal transformer. Data from a subset of the CANDOR conversational corpus. Experiments include pre-trained vs from-scratch training and exploration of velocity features. Feature-level fusion combines spatial and temporal streams.

Result: Domain-specific (from-scratch) training significantly outperforms transfer learning. Spatial configurations carry more discriminative information than temporal dynamics. Spatial transformer achieves 95.74% accuracy; multi-scale temporal transformer achieves 93.90%. Feature-level fusion pushes accuracy to 98.03%, indicating postural and dynamic information are complementary.

Conclusion: Transformers show strong potential for person identification in natural interactions. Spatial cues are more informative than motion patterns, but combining them yields the best performance. The results offer insights for future multimodal and cross-cultural studies.

Abstract: This paper investigates the performance of transformer-based architectures
for person identification in natural, face-to-face conversation scenario. We
implement and evaluate a two-stream framework that separately models spatial
configurations and temporal motion patterns of 133 COCO WholeBody keypoints,
extracted from a subset of the CANDOR conversational corpus. Our experiments
compare pre-trained and from-scratch training, investigate the use of velocity
features, and introduce a multi-scale temporal transformer for hierarchical
motion modeling. Results demonstrate that domain-specific training
significantly outperforms transfer learning, and that spatial configurations
carry more discriminative information than temporal dynamics. The spatial
transformer achieves 95.74% accuracy, while the multi-scale temporal
transformer achieves 93.90%. Feature-level fusion pushes performance to 98.03%,
confirming that postural and dynamic information are complementary. These
findings highlight the potential of transformer architectures for person
identification in natural interactions and provide insights for future
multimodal and cross-cultural studies.

</details>


### [137] [Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction](https://arxiv.org/abs/2510.04759)
*Chi Yan,Dan Xu*

Main category: cs.CV

TL;DR: PG-Occ progressively densifies a 3D Gaussian representation for open-vocabulary occupancy prediction, using anisotropy-aware sampling with spatio-temporal fusion. It achieves state-of-the-art results, with a 14.3% relative mIoU improvement over previous methods; code will be released.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of fixed semantic vocabularies in 3D occupancy prediction by enabling open-vocabulary text-aligned representations, while balancing accuracy and efficiency between sparse and dense Gaussian representations.

Method: Introduce Progressive Gaussian Transformer Framework (PG-Occ) with (1) progressive online densification to gradually refine the 3D Gaussian representation, and (2) an anisotropy-aware sampling strategy with spatio-temporal fusion that adaptively assigns receptive fields to Gaussians across scales and stages for improved feature aggregation.

Result: Achieves state-of-the-art performance with a relative 14.3% mIoU improvement over the previous best methods on open-vocabulary 3D occupancy tasks.

Conclusion: PG-Occ provides a scalable, open-vocabulary 3D occupancy solution that effectively balances detail and computation through progressive densification and adaptive sampling, with strong empirical gains and code release planned.

Abstract: The 3D occupancy prediction task has witnessed remarkable progress in recent
years, playing a crucial role in vision-based autonomous driving systems. While
traditional methods are limited to fixed semantic categories, recent approaches
have moved towards predicting text-aligned features to enable open-vocabulary
text queries in real-world scenes. However, there exists a trade-off in
text-aligned scene modeling: sparse Gaussian representation struggles to
capture small objects in the scene, while dense representation incurs
significant computational overhead. To address these limitations, we present
PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables
open-vocabulary 3D occupancy prediction. Our framework employs progressive
online densification, a feed-forward strategy that gradually enhances the 3D
Gaussian representation to capture fine-grained scene details. By iteratively
enhancing the representation, the framework achieves increasingly precise and
detailed scene understanding. Another key contribution is the introduction of
an anisotropy-aware sampling strategy with spatio-temporal fusion, which
adaptively assigns receptive fields to Gaussians at different scales and
stages, enabling more effective feature aggregation and richer scene
information capture. Through extensive evaluations, we demonstrate that PG-Occ
achieves state-of-the-art performance with a relative 14.3% mIoU improvement
over the previous best performing method. Code and pretrained models will be
released upon publication on our project page:
https://yanchi-3dv.github.io/PG-Occ

</details>


### [138] [Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning](https://arxiv.org/abs/2510.04770)
*Xiaomeng Fan,Yuchuan Mao,Zhi Gao,Yuwei Wu,Jin Chen,Yunde Jia*

Main category: cs.CV

TL;DR: Open-vocabulary learning benefits from generating unseen-class data to better estimate distributions; introduces a class-domain-wise data generation pipeline and distribution alignment; achieves up to 14% gains on 11 datasets.


<details>
  <summary>Details</summary>
Motivation: Estimating data distribution in open environments is hampered when relying only on seen-class data, leading to unidentifiable estimation error for unseen classes; thus learning beyond seen classes is essential to bound this error.

Method: Proposes a data generation pipeline that creates unseen-class samples guided by a hierarchical semantic tree and domain information inferred from seen data, followed by a distribution alignment algorithm that estimates and maximizes posterior probability to improve open-vocabulary generalization.

Result: Extensive experiments across 11 datasets show significant improvements over baselines, up to 14%.

Conclusion: Generating unseen-class data for distribution estimation enables more accurate open-world modeling and improves open-vocabulary recognition.

Abstract: Open-vocabulary learning requires modeling the data distribution in open
environments, which consists of both seen-class and unseen-class data.
  Existing methods estimate the distribution in open environments using
seen-class data, where the absence of unseen classes makes the estimation error
inherently unidentifiable.
  Intuitively, learning beyond the seen classes is crucial for distribution
estimation to bound the estimation error.
  We theoretically demonstrate that the distribution can be effectively
estimated by generating unseen-class data, through which the estimation error
is upper-bounded.
  Building on this theoretical insight, we propose a novel open-vocabulary
learning method, which generates unseen-class data for estimating the
distribution in open environments. The method consists of a class-domain-wise
data generation pipeline and a distribution alignment algorithm. The data
generation pipeline generates unseen-class data under the guidance of a
hierarchical semantic tree and domain information inferred from the seen-class
data, facilitating accurate distribution estimation. With the generated data,
the distribution alignment algorithm estimates and maximizes the posterior
probability to enhance generalization in open-vocabulary learning. Extensive
experiments on $11$ datasets demonstrate that our method outperforms baseline
approaches by up to $14\%$, highlighting its effectiveness and superiority.

</details>


### [139] [Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge](https://arxiv.org/abs/2510.04772)
*Max Kirchner,Hanna Hoffmann,Alexander C. Jenke,Oliver L. Saldanha,Kevin Pfeiffer,Weam Kanjo,Julia Alekseenko,Claas de Boer,Santhi Raj Kolamuri,Lorenzo Mazza,Nicolas Padoy,Sophia Bano,Annika Reinke,Lena Maier-Hein,Danail Stoyanov,Jakob N. Kather,Fiona R. Kolbinger,Sebastian Bodenstedt,Stefanie Speidel*

Main category: cs.CV

TL;DR: A benchmark for federated learning on surgical video classification. Generalization to unseen centers is limited; center-specific fine-tuning improves performance, but ranking stability is low. ViViT-based methods perform strongest. Highlights need for imbalance-aware, robust FL and effective spatiotemporal modeling.


<details>
  <summary>Details</summary>
Motivation: To establish a standardized benchmark for FL in surgical video classification, assessing both cross-center generalization and adaptation through local fine-tuning, and to identify practical challenges in decentralized medical video analysis.

Method: Participants developed strategies on a preliminary Appendix300 multi-center video dataset to classify inflammation stages in appendicitis. Two tasks were evaluated: (1) generalization to an unseen center, and (2) center-specific adaptation after fine-tuning. Approaches included foundation models with linear probing, metric learning with triplet loss, and FL aggregations (FedAvg, FedMedian, FedSAM). Performance was measured by F1-score and Expected Cost, with robustness assessed via bootstrapping and statistical tests.

Result: Generalization across centers was limited. All teams improved with local fine-tuning in the adaptation task, though ranking stability remained low. The ViViT-based submission achieved the strongest overall performance.

Conclusion: FedSurg provides the first benchmark for evaluating FL strategies in surgical video classification. Results highlight the trade-off between local personalization and global robustness, and underscore the importance of architecture choice, preprocessing, and loss design. The benchmark offers a reference point for developing imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.

Abstract: Purpose: The FedSurg challenge was designed to benchmark the state of the art
in federated learning for surgical video classification. Its goal was to assess
how well current methods generalize to unseen clinical centers and adapt
through local fine-tuning while enabling collaborative model development
without sharing patient data. Methods: Participants developed strategies to
classify inflammation stages in appendicitis using a preliminary version of the
multi-center Appendix300 video dataset. The challenge evaluated two tasks:
generalization to an unseen center and center-specific adaptation after
fine-tuning. Submitted approaches included foundation models with linear
probing, metric learning with triplet loss, and various FL aggregation schemes
(FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and
Expected Cost, with ranking robustness evaluated via bootstrapping and
statistical testing. Results: In the generalization task, performance across
centers was limited. In the adaptation task, all teams improved after
fine-tuning, though ranking stability was low. The ViViT-based submission
achieved the strongest overall performance. The challenge highlighted
limitations in generalization, sensitivity to class imbalance, and difficulties
in hyperparameter tuning in decentralized training, while spatiotemporal
modeling and context-aware preprocessing emerged as promising strategies.
Conclusion: The FedSurg Challenge establishes the first benchmark for
evaluating FL strategies in surgical video classification. Findings highlight
the trade-off between local personalization and global robustness, and
underscore the importance of architecture choice, preprocessing, and loss
design. This benchmarking offers a reference point for future development of
imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.

</details>


### [140] [Hands-Free Heritage: Automated 3D Scanning for Cultural Heritage Digitization](https://arxiv.org/abs/2510.04781)
*Javed Ahmad,Federico Dassiè,Selene Frascella,Gabriele Marchello,Ferdinando Cannella,Arianna Traviglia*

Main category: cs.CV

TL;DR: A two-robot automated scanning system for high-fidelity 3D digitization of cultural heritage artifacts, enabling fully automated surface scanning with improved coverage and efficiency over handheld workflows.


<details>
  <summary>Details</summary>
Motivation: To reduce dependence on expert operators and manual setup while achieving high geometric accuracy in 3D scans of artifacts for preservation and documentation.

Method: Divide scanning space into regions; coordinate a scanner-equipped robot with a tray-handling robot; optimized trajectory planning and waypoint distribution to maximize coverage, minimize occlusions, and balance accuracy with efficiency.

Result: Significantly lower Chamfer Distance and higher F-score than baselines, indicating improved geometric accuracy and reconstruction quality; better digitization efficiency and reduced operator requirements.

Conclusion: Automated dual-robot scanning achieves high-fidelity 3D digitization with reduced need for expert intervention, suggesting practical deployment for cultural heritage preservation.

Abstract: High-fidelity 3D scanning is essential for preserving cultural heritage
artefacts, supporting documentation, analysis, and long-term conservation.
However, conventional methods typically require specialized expertise and
manual intervention to maintain optimal scanning conditions and coverage. We
present an automated two-robot scanning system that eliminates the need for
handheld or semi-automatic workflows by combining coordinated robotic
manipulation with high-resolution 3D scanning. Our system parameterizes the
scanning space into distinct regions, enabling coordinated motion planning
between a scanner-equipped robot and a tray-handling robot. Optimized
trajectory planning and waypoint distribution ensure comprehensive surface
coverage, minimize occlusions, and balance reconstruction accuracy with system
efficiency. Experimental results show that our approach achieves significantly
lower Chamfer Distance and higher F-score compared to baseline methods,
offering superior geometric accuracy, improved digitization efficiency, and
reduced reliance on expert operators.

</details>


### [141] [A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation](https://arxiv.org/abs/2510.04794)
*Alon Kaya,Igal Bilik,Inna Stainvas*

Main category: cs.CV

TL;DR: ViTs vs CNNs as backbones for geometric estimation tasks under low-data constraints. CNNs excel in few-shot, ViTs excel with more data and generalize across domains; hybrid architectures balancing local and global features are promising.


<details>
  <summary>Details</summary>
Motivation: Assess how pretrained backbones (vision transformers vs CNNs) support geometric estimation tasks (2D rigid transforms and fundamental matrix) across data regimes, addressing the mismatch between pretraining objectives and geometric refinement.

Method: Systematic empirical comparison of large CNNs (ResNet, EfficientNet, CLIP-ResNet) with ViT-based models (CLIP-ViT variants, DINO) across data sizes including few-shot, evaluating on estimating 2D rigid transformations between image pairs and predicting the fundamental matrix for stereo pairs.

Result: In large downstream-data regimes, ViTs outperform CNNs during refinement. In small data regimes, CNNs’ inductive bias and smaller capacity enable them to match ViTs. ViTs also show stronger cross-domain generalization when data distributions shift.

Conclusion: Backbone choice critically affects refinement performance; results motivate hybrid architectures that balance local (CNN-like) and global (ViT-like) representations and task-specific pretraining geared toward geometric estimation.

Abstract: Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs)
have reshaped computer vision through pretrained feature representations that
enable strong transfer learning for diverse tasks. However, their efficiency as
backbone architectures for geometric estimation tasks involving image
deformations in low-data regimes remains an open question. This work considers
two such tasks: 1) estimating 2D rigid transformations between pairs of images
and 2) predicting the fundamental matrix for stereo image pairs, an important
problem in various applications, such as autonomous mobility, robotics, and 3D
scene reconstruction. Addressing this intriguing question, this work
systematically compares large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet)
with ViT-based foundation models (CLIP-ViT variants and DINO) in various data
size settings, including few-shot scenarios. These pretrained models are
optimized for classification or contrastive learning, encouraging them to focus
mostly on high-level semantics. The considered tasks require balancing local
and global features differently, challenging the straightforward adoption of
these models as the backbone. Empirical comparative analysis shows that,
similar to training from scratch, ViTs outperform CNNs during refinement in
large downstream-data scenarios. However, in small data scenarios, the
inductive bias and smaller capacity of CNNs improve their performance, allowing
them to match that of a ViT. Moreover, ViTs exhibit stronger generalization in
cross-domain evaluation where the data distribution changes. These results
emphasize the importance of carefully selecting model architectures for
refinement, motivating future research towards hybrid architectures that
balance local and global representations.

</details>


### [142] [DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing](https://arxiv.org/abs/2510.04797)
*Qi Li,Shuwen Qiu,Julien Han,Xingzi Xu,Mehmet Saygin Seyfioglu,Kee Kiat Koo,Karim Bouyarmane*

Main category: cs.CV

TL;DR: A diffusion-transformer-based Virtual Try-On (DiT-VTON) framework delivers improved detail preservation, robustness, and versatility for VTO and Virtual Try-All (VTA) tasks, outperforming state-of-the-art on VITON-HD without extra condition encoders and generalizing across thousands of product categories.


<details>
  <summary>Details</summary>
Motivation: The surge of e-commerce increases demand for realistic VTO. Current models struggle with fine-grained detail, real-world robustness, sampling efficiency, editing, and generalization across diverse products. A robust, versatile VTO/VTA framework is needed.

Method: Adopt a Diffusion Transformer (DiT) for image-conditioned VTO. Explore configurations (in-context token concatenation, channel concatenation, ControlNet integration). Train on an expanded, diverse dataset including varied backgrounds, unstructured references, and non-garment categories. Extend the task to Virtual Try-All (VTA) with pose-preserving, localized editing, texture transfer, and object-level customization. Evaluate on VITON-HD and a diverse multi-category dataset.

Result: DiT-VTON achieves superior detail preservation and robustness on VITON-HD without additional condition encoders, and outperforms VTA-equipped/editing models on a dataset spanning thousands of product categories.

Conclusion: DiT-VTON demonstrates strong generalization and robustness for VTO and VTA across diverse product categories, with data scaling improving adaptability and enabling advanced image editing and customization without extra conditioning components.

Abstract: The rapid growth of e-commerce has intensified the demand for Virtual Try-On
(VTO) technologies, enabling customers to realistically visualize products
overlaid on their own images. Despite recent advances, existing VTO models face
challenges with fine-grained detail preservation, robustness to real-world
imagery, efficient sampling, image editing capabilities, and generalization
across diverse product categories. In this paper, we present DiT-VTON, a novel
VTO framework that leverages a Diffusion Transformer (DiT), renowned for its
performance on text-conditioned image generation, adapted here for the
image-conditioned VTO task. We systematically explore multiple DiT
configurations, including in-context token concatenation, channel
concatenation, and ControlNet integration, to determine the best setup for VTO
image conditioning.
  To enhance robustness, we train the model on an expanded dataset encompassing
varied backgrounds, unstructured references, and non-garment categories,
demonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also
redefines the VTO task beyond garment try-on, offering a versatile Virtual
Try-All (VTA) solution capable of handling a wide range of product categories
and supporting advanced image editing functionalities such as pose
preservation, localized editing, texture transfer, and object-level
customization. Experimental results show that our model surpasses
state-of-the-art methods on VITON-HD, achieving superior detail preservation
and robustness without reliance on additional condition encoders. It also
outperforms models with VTA and image editing capabilities on a diverse dataset
spanning thousands of product categories.

</details>


### [143] [Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors](https://arxiv.org/abs/2510.04802)
*Han Zhang,Lalithkumar Seenivasan,Jose L. Porras,Roger D. Soberanis-Mukul,Hao Ding,Hongchao Shu,Benjamin D. Killeen,Ankita Ghosh,Lonny Yarmus,Masaru Ishii,Angela Christine Argento,Mathias Unberath*

Main category: cs.CV

TL;DR: EgoSurg reconstructs egocentric surgical viewpoints from fixed OR cameras using geometry-driven neural rendering and diffusion-based view enhancement to create a dynamic, navigable 3D record of the operating room.


<details>
  <summary>Details</summary>
Motivation: Fixed OR video captures room-level workflows but misses what individual clinicians saw. Understanding egocentric perspectives is essential for safety, training, and workflow optimization. A method to reconstruct dynamic, person-specific visual fields would unlock deeper insights.

Method: Combine geometry-driven neural rendering with diffusion-based view enhancement to synthesize arbitrary egocentric viewpoints from wall-mounted cameras. Reconstruct dynamic, person-specific visual fields and render from any perspective across time.

Result: Across multi-site cases and controlled studies, EgoSurg achieves high visual fidelity in reconstructing egocentric views and arbitrary viewpoints, enabling a navigable dynamic 3D replay.

Conclusion: Transforms existing fixed-camera OR infrastructure into a navigable dynamic 3D record, enabling immersive surgical data science to visualize, experience, and analyze practice from every angle.

Abstract: Observing surgical practice has historically relied on fixed vantage points
or recollections, leaving the egocentric visual perspectives that guide
clinical decisions undocumented. Fixed-camera video can capture surgical
workflows at the room-scale, but cannot reconstruct what each team member
actually saw. Thus, these videos only provide limited insights into how
decisions that affect surgical safety, training, and workflow optimization are
made. Here we introduce EgoSurg, the first framework to reconstruct the
dynamic, egocentric replays for any operating room (OR) staff directly from
wall-mounted fixed-camera video, and thus, without intervention to clinical
workflow. EgoSurg couples geometry-driven neural rendering with diffusion-based
view enhancement, enabling high-visual fidelity synthesis of arbitrary and
egocentric viewpoints at any moment. In evaluation across multi-site surgical
cases and controlled studies, EgoSurg reconstructs person-specific visual
fields and arbitrary viewpoints with high visual quality and fidelity. By
transforming existing OR camera infrastructure into a navigable dynamic 3D
record, EgoSurg establishes a new foundation for immersive surgical data
science, enabling surgical practice to be visualized, experienced, and analyzed
from every angle.

</details>


### [144] [Visual Representations inside the Language Model](https://arxiv.org/abs/2510.04819)
*Benlin Liu,Amita Kamath,Madeleine Grunde-McLaughlin,Winson Han,Ranjay Krishna*

Main category: cs.CV

TL;DR: MLMs possess perceptual information in visual tokens, but their finetuned multimodal setup leaks less pure visual information than dedicated encoders; prompting and controlled visual input could substantially improve perception in perception-heavy tasks.


<details>
  <summary>Details</summary>
Motivation: To understand why multimodal language models struggle on perception-heavy tasks and reveal how visual key-value tokens propagate information through the model.

Method: Analyze the flow of visual information through MLMs (LLaVA-OneVision, Qwen2.5-VL, Llama-3-LLaVA-NeXT) and compare it to a dedicated visual encoder (SigLIP). Evaluate zero-shot perception tasks (segmentation, semantic/temporal correspondence, referring expression detection). Inspect input-agnostic image key tokens in later LM layers for artifacts. Test the effect of adding a text prefix to image input. Examine BLINK Art Style results for information surfacing.

Result: Image value tokens encode sufficient information to perform several perception-heavy tasks zero-shot. The MLM augments visual information from projected inputs but contains less visual information on several tasks than SigLIP. Visual information from input-agnostic key tokens in later layers contains artifacts that reduce perception. Adding a text prefix to image input improves perception. Better control of visual information could significantly improve MLM perception; e.g., 33.3% of Art Style questions in BLINK have perception information present but not surfaced to output.

Conclusion: Highlights the role of key-value tokens in multimodal systems, enabling deeper mechanistic interpretability, and suggests training directions to improve the visual encoder and language model components to enhance perception.

Abstract: Despite interpretability work analyzing VIT encoders and transformer
activations, we don't yet understand why Multimodal Language Models (MLMs)
struggle on perception-heavy tasks. We offer an under-studied perspective by
examining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and
Llama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the
flow of visual information through the language model, finding that image value
tokens encode sufficient information to perform several perception-heavy tasks
zero-shot: segmentation, semantic correspondence, temporal correspondence, and
referring expression detection. We find that while the language model does
augment the visual information received from the projection of input visual
encodings-which we reveal correlates with overall MLM perception capability-it
contains less visual information on several tasks than the equivalent visual
encoder (SigLIP) that has not undergone MLM finetuning. Further, we find that
the visual information corresponding to input-agnostic image key tokens in
later layers of language models contains artifacts which reduce perception
capability of the overall MLM. Next, we discuss controlling visual information
in the language model, showing that adding a text prefix to the image input
improves perception capabilities of visual representations. Finally, we reveal
that if language models were able to better control their visual information,
their perception would significantly improve; e.g., in 33.3% of Art Style
questions in the BLINK benchmark, perception information present in the
language model is not surfaced to the output! Our findings reveal insights into
the role of key-value tokens in multimodal systems, paving the way for deeper
mechanistic interpretability of MLMs and suggesting new directions for training
their visual encoder and language model components.

</details>


### [145] [AvatarVTON: 4D Virtual Try-On for Animatable Avatars](https://arxiv.org/abs/2510.04822)
*Zicheng Jiang,Jixin Gao,Shengfeng He,Xinzhe Li,Yulong Zheng,Zhaotong Yang,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: AvatarVTON introduces the first 4D virtual try-on framework that generates realistic, pose-controllable try-on results from a single in-shop garment image, featuring temporal-stabilizing Reciprocal Flow Rectifier and a Non-Linear Deformer for adaptive, non-linear garment deformations, enabling free pose control, novel-view rendering, and diverse garment choices without multi-view data.


<details>
  <summary>Details</summary>
Motivation: To enable dynamic, photorealistic 4D virtual try-on under single-view supervision, addressing the lack of data and reliance on physics priors or multi-view garment captures in existing methods.

Method: Two key modules: (1) Reciprocal Flow Rectifier, a prior-free optical-flow correction strategy that stabilizes avatar fitting and ensures temporal coherence; (2) Non-Linear Deformer, which decomposes Gaussian maps into view-pose-invariant and view-pose-specific components to enable adaptive, non-linear garment deformations. The approach also extends baselines with unified modules for fair qualitative and quantitative comparisons.

Result: AvatarVTON achieves high fidelity, diversity, and dynamic garment realism in 4D virtual try-on, supporting free pose control, novel-view rendering, and diverse garment choices, and is suitable for AR/VR, gaming, and digital-human applications; experiments demonstrate strong performance under single-view supervision.

Conclusion: AvatarVTON is the first 4D virtual try-on framework under single-view supervision with dynamic garment interactions, offering a practical benchmark extension and strong potential for AR/VR, gaming, and digital-human applications.

Abstract: We propose AvatarVTON, the first 4D virtual try-on framework that generates
realistic try-on results from a single in-shop garment image, enabling free
pose control, novel-view rendering, and diverse garment choices. Unlike
existing methods, AvatarVTON supports dynamic garment interactions under
single-view supervision, without relying on multi-view garment captures or
physics priors. The framework consists of two key modules: (1) a Reciprocal
Flow Rectifier, a prior-free optical-flow correction strategy that stabilizes
avatar fitting and ensures temporal coherence; and (2) a Non-Linear Deformer,
which decomposes Gaussian maps into view-pose-invariant and view-pose-specific
components, enabling adaptive, non-linear garment deformations. To establish a
benchmark for 4D virtual try-on, we extend existing baselines with unified
modules for fair qualitative and quantitative comparisons. Extensive
experiments show that AvatarVTON achieves high fidelity, diversity, and dynamic
garment realism, making it well-suited for AR/VR, gaming, and digital-human
applications.

</details>


### [146] [Flow Matching for Conditional MRI-CT and CBCT-CT Image Synthesis](https://arxiv.org/abs/2510.04823)
*Arnela Hadzic,Simon Johannes Joham,Martin Urschler*

Main category: cs.CV

TL;DR: 3D Flow Matching-based sCT generation from MRI/CBCT; good global anatomy, limited high-frequency details due to resolution; future work: patch-based training and latent-space flow models.


<details>
  <summary>Details</summary>
Motivation: Enable MRI-only radiotherapy and CBCT-based adaptive radiotherapy by producing accurate synthetic CTs while reducing patient radiation exposure.

Method: Fully 3D Flow Matching: transform a Gaussian noise volume into an sCT by integrating a learned velocity field conditioned on features from MRI/CBCT, using a lightweight 3D encoder. Separate models for MRI→sCT and CBCT→sCT across abdomen, head & neck, and thorax; trained/evaluated on the SynthRAD2025 benchmark.

Result: Global anatomical structures are accurately reconstructed; fine details are not well preserved, limited by low training resolution due to memory/runtime constraints; patch-based training and latent-space flow models are proposed for improvement.

Conclusion: The approach demonstrates feasibility and efficiency of 3D flow-based sCT generation; improvements in resolution and local fidelity are expected with patch-based training and latent-space flow modeling.

Abstract: Generating synthetic CT (sCT) from MRI or CBCT plays a crucial role in
enabling MRI-only and CBCT-based adaptive radiotherapy, improving treatment
precision while reducing patient radiation exposure. To address this task, we
adopt a fully 3D Flow Matching (FM) framework, motivated by recent work
demonstrating FM's efficiency in producing high-quality images. In our
approach, a Gaussian noise volume is transformed into an sCT image by
integrating a learned FM velocity field, conditioned on features extracted from
the input MRI or CBCT using a lightweight 3D encoder. We evaluated the method
on the SynthRAD2025 Challenge benchmark, training separate models for MRI
$\rightarrow$ sCT and CBCT $\rightarrow$ sCT across three anatomical regions:
abdomen, head and neck, and thorax. Validation and testing were performed
through the challenge submission system. The results indicate that the method
accurately reconstructs global anatomical structures; however, preservation of
fine details was limited, primarily due to the relatively low training
resolution imposed by memory and runtime constraints. Future work will explore
patch-based training and latent-space flow models to improve resolution and
local structural fidelity.

</details>


### [147] [Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation](https://arxiv.org/abs/2510.04838)
*Muquan Li,Hang Gou,Dongyang Zhang,Shuang Liang,Xiurui Xie,Deqiang Ouyang,Ke Qin*

Main category: cs.CV

TL;DR: Automatic Truncated Backpropagation Through Time (AT-BPTT) adaptively truncates backpropagation during dataset distillation, using stage-aware timestep selection, gradient-variation-based window sizing, and low-rank Hessian approximation to reduce cost. It yields state-of-the-art accuracy and significant speedups/memory savings.


<details>
  <summary>Details</summary>
Motivation: Dataset distillation requires efficient inner-loop optimization. Random truncation is inflexible and often suboptimal because neural networks exhibit different learning dynamics across early, middle, and late training stages. Adaptive truncation aims to align backpropagation with gradient behavior to improve performance and efficiency.

Method: AT-BPTT introduces three components: (1) a probabilistic mechanism for stage-aware timestep selection, (2) an adaptive window sizing strategy based on gradient variation, and (3) a low-rank Hessian approximation to reduce computational overhead during truncated backpropagation.

Result: Experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1K show state-of-the-art performance, with an average accuracy improvement of 6.16% over baseline methods. Inner-loop optimization is accelerated by 3.9x and memory cost reduced by 63%.

Conclusion: AT-BPTT effectively captures learning dynamics through adaptive truncation, enhancing both performance and efficiency of dataset distillation, making it practical for large-scale datasets.

Abstract: The growing demand for efficient deep learning has positioned dataset
distillation as a pivotal technique for compressing training dataset while
preserving model performance. However, existing inner-loop optimization methods
for dataset distillation typically rely on random truncation strategies, which
lack flexibility and often yield suboptimal results. In this work, we observe
that neural networks exhibit distinct learning dynamics across different
training stages-early, middle, and late-making random truncation ineffective.
To address this limitation, we propose Automatic Truncated Backpropagation
Through Time (AT-BPTT), a novel framework that dynamically adapts both
truncation positions and window sizes according to intrinsic gradient behavior.
AT-BPTT introduces three key components: (1) a probabilistic mechanism for
stage-aware timestep selection, (2) an adaptive window sizing strategy based on
gradient variation, and (3) a low-rank Hessian approximation to reduce
computational overhead. Extensive experiments on CIFAR-10, CIFAR-100,
Tiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art
performance, improving accuracy by an average of 6.16% over baseline methods.
Moreover, our approach accelerates inner-loop optimization by 3.9x while saving
63% memory cost.

</details>


### [148] [Detailed Aerial Mapping of Photovoltaic Power Plants Through Semantically Significant Keypoints](https://arxiv.org/abs/2510.04840)
*Viktor Kozák,Jan Chudoba,Libor Přeučil*

Main category: cs.CV

TL;DR: Automated PV-plant mapping from aerial overview images that segments modules, infers bench/row/column structure, and fuses multiple views into a compact georeferenced 3D model for maintenance—no third-party data required.


<details>
  <summary>Details</summary>
Motivation: Accurate, up-to-date models of PV plants are crucial for operation and maintenance but are often unavailable or outdated. The proposed method aims to automate mapping from imagery, reducing reliance on external datasets and enabling detailed, module-level modeling.

Method: The approach uses visual segmentation of PV modules in overview images, infers structural information per image to assign modules to benches, rows, and columns, and identifies layout keypoints. It merges detections from multiple images while preserving structural integrity, and fuses 3D positions with semantic structures to create a compact georeferenced model.

Result: Experimental verification on two different PV power plants. The final fused dataset yields a 3D, semantically structured, georeferenced model suitable for maintenance.

Conclusion: The method enables automated, module-level PV plant mapping from aerial imagery, reducing reliance on third-party data and delivering up-to-date, detailed models for maintenance and operation.

Abstract: An accurate and up-to-date model of a photovoltaic (PV) power plant is
essential for its optimal operation and maintenance. However, such a model may
not be easily available. This work introduces a novel approach for PV power
plant mapping based on aerial overview images. It enables the automation of the
mapping process while removing the reliance on third-party data. The presented
mapping method takes advantage of the structural layout of the power plants to
achieve detailed modeling down to the level of individual PV modules. The
approach relies on visual segmentation of PV modules in overview images and the
inference of structural information in each image, assigning modules to
individual benches, rows, and columns. We identify visual keypoints related to
the layout and use these to merge detections from multiple images while
maintaining their structural integrity. The presented method was experimentally
verified and evaluated on two different power plants. The final fusion of 3D
positions and semantic structures results in a compact georeferenced model
suitable for power plant maintenance.

</details>


### [149] [From Actions to Kinesics: Extracting Human Psychological States through Bodily Movements](https://arxiv.org/abs/2510.04844)
*Cheyu Lin,Katherine A. Flanigan*

Main category: cs.CV

TL;DR: A framework that directly infers kinesics from 3D skeleton data using a hybrid ST-GCN and CNN, enabling privacy-preserving, transferable understanding of human actions in service of RL-driven human–environment modeling.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of theory-driven or questionnaire-based approaches that are static, non-generalizable, and labor-intensive for modeling human–environment interactions; there is a need for scalable, privacy-preserving representations of cognitive/emotional states derived from observable movements.

Method: A two-branch architecture combining spatial-temporal graph convolutional networks (ST-GCN) with a CNN to process 3D skeleton sequences. The model uses transfer learning to map physical actions to psychological categories without manually defined action–psychology mappings, preserving user anonymity while uncovering latent movement structures.

Result: On the Dyadic User Engagement (DUET) dataset, the method demonstrates scalable and accurate modeling of human-centered behavior, suggesting its potential to improve RL-driven simulations of human–environment interaction.

Conclusion: The proposed framework offers a privacy-preserving, scalable pathway to infer cognitive/emotional states from bodily movements directly from skeleton data, enabling more realistic, human-centered RL simulations and advancing kinesics research.

Abstract: Understanding the dynamic relationship between humans and the built
environment is a key challenge in disciplines ranging from environmental
psychology to reinforcement learning (RL). A central obstacle in modeling these
interactions is the inability to capture human psychological states in a way
that is both generalizable and privacy preserving. Traditional methods rely on
theoretical models or questionnaires, which are limited in scope, static, and
labor intensive. We present a kinesics recognition framework that infers the
communicative functions of human activity -- known as kinesics -- directly from
3D skeleton joint data. Combining a spatial-temporal graph convolutional
network (ST-GCN) with a convolutional neural network (CNN), the framework
leverages transfer learning to bypass the need for manually defined mappings
between physical actions and psychological categories. The approach preserves
user anonymity while uncovering latent structures in bodily movements that
reflect cognitive and emotional states. Our results on the Dyadic User
EngagemenT (DUET) dataset demonstrate that this method enables scalable,
accurate, and human-centered modeling of behavior, offering a new pathway for
enhancing RL-driven simulations of human-environment interaction.

</details>


### [150] [Read the Room: Inferring Social Context Through Dyadic Interaction Recognition in Cyber-physical-social Infrastructure Systems](https://arxiv.org/abs/2510.04854)
*Cheyu Lin,John Martins,Katherine A. Flanigan,Ph. D*

Main category: cs.CV

TL;DR: Depth-sensor skeleton-based interaction recognition on 12 dyadic interactions demonstrates privacy-friendly insights for Cyber-Physical-Social Infrastructure Systems (CPSIS).


<details>
  <summary>Details</summary>
Motivation: Bridge cyber-physical systems with social objectives by defining and measuring human interactions and benefits, while preserving privacy.

Method: Compare five skeleton-based interaction recognition algorithms on a dataset of 12 dyadic interactions captured with depth sensors, focusing on privacy-conscious analysis of skeletal movements; categorize interactions into communication types such as emblems and affect displays to explore cultural and emotional aspects.

Result: The study evaluates five skeleton-based recognition algorithms on 12 dyadic interactions, illustrating the feasibility of privacy-preserving depth-sensor analysis for social interactions and offering initial insights into cultural and emotional aspects of dyadic communication.

Conclusion: Skeleton-based, privacy-conscious interaction recognition is a viable component of CPSIS, enabling measurement of social benefits, but challenges remain in accuracy, cross-cultural generalization, and integrating these signals with decision-making for social outcomes.

Abstract: Cyber-physical systems (CPS) integrate sensing, computing, and control to
improve infrastructure performance, focusing on economic goals like performance
and safety. However, they often neglect potential human-centered (or
''social'') benefits. Cyber-physical-social infrastructure systems (CPSIS) aim
to address this by aligning CPS with social objectives. This involves defining
social benefits, understanding human interactions with each other and
infrastructure, developing privacy-preserving measurement methods, modeling
these interactions for prediction, linking them to social benefits, and
actuating the physical environment to foster positive social outcomes. This
paper delves into recognizing dyadic human interactions using real-world data,
which is the backbone to measuring social behavior. This lays a foundation to
address the need to enhance understanding of the deeper meanings and mutual
responses inherent in human interactions. While RGB cameras are informative for
interaction recognition, privacy concerns arise. Depth sensors offer a
privacy-conscious alternative by analyzing skeletal movements. This study
compares five skeleton-based interaction recognition algorithms on a dataset of
12 dyadic interactions. Unlike single-person datasets, these interactions,
categorized into communication types like emblems and affect displays, offer
insights into the cultural and emotional aspects of human interactions.

</details>


### [151] [ERDE: Entropy-Regularized Distillation for Early-exit](https://arxiv.org/abs/2510.04856)
*Martial Guidez,Stefan Duffner,Yannick Alpou,Oscar Röth,Christophe Garcia*

Main category: cs.CV

TL;DR: The paper proposes a dynamic CNN compression approach that combines early-exits with Knowledge Distillation (KD). It introduces an entropy-based loss for cases where the teacher misclassifies, training a lighter student early-exit model from a more complex teacher, achieving notable accuracy–efficiency gains on CIFAR-10, CIFAR-100, and SVHN.


<details>
  <summary>Details</summary>
Motivation: To enable real-time and edge deployment of CNNs by reducing computation. While early exits and KD individually help, training a student from a teacher that may err remains challenging; an entropy-based correction is proposed to improve learning in misclassified cases.

Method: Train a student early-exit model using knowledge distillation from a teacher early-exit model. Introduce a novel entropy-based loss for samples where the teacher’s classification is incorrect, complementing the conventional KD loss to optimize the accuracy–efficiency trade-off. Evaluate on CIFAR-10, CIFAR-100, and SVHN to demonstrate significant reductions in computation with maintained accuracy.

Result: Demonstrates substantial computational cost reductions with little to no loss in accuracy. Experimental results on CIFAR-10, CIFAR-100, and SVHN corroborate the effectiveness of entropy-augmented KD in dynamic, early-exit architectures and suggest broader applicability of KD techniques in related contexts.

Conclusion: Entropy-augmented knowledge distillation for student early-exit models is a viable strategy to balance accuracy and efficiency in dynamic CNNs, opening up new directions for applying KD in varied contexts beyond static architectures.

Abstract: Although deep neural networks and in particular Convolutional Neural Networks
have demonstrated state-of-the-art performance in image classification with
relatively high efficiency, they still exhibit high computational costs, often
rendering them impractical for real-time and edge applications. Therefore, a
multitude of compression techniques have been developed to reduce these costs
while maintaining accuracy. In addition, dynamic architectures have been
introduced to modulate the level of compression at execution time, which is a
desirable property in many resource-limited application scenarios. The proposed
method effectively integrates two well-established optimization techniques:
early exits and knowledge distillation, where a reduced student early-exit
model is trained from a more complex teacher early-exit model. The primary
contribution of this research lies in the approach for training the student
early-exit model. In comparison to the conventional Knowledge Distillation
loss, our approach incorporates a new entropy-based loss for images where the
teacher's classification was incorrect. The proposed method optimizes the
trade-off between accuracy and efficiency, thereby achieving significant
reductions in computational complexity without compromising classification
performance. The validity of this approach is substantiated by experimental
results on image classification datasets CIFAR10, CIFAR100 and SVHN, which
further opens new research perspectives for Knowledge Distillation in other
contexts.

</details>


### [152] [μDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy](https://arxiv.org/abs/2510.04859)
*Elena Corbetta,Thomas Bocklitz*

Main category: cs.CV

TL;DR: A deep-learning IQA model, μDeepIQA, adapts a CNN for natural images to optical microscopy to predict local and global image quality quickly and robustly, enabling patch-level quality visualization and better generalization beyond traditional metrics.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional IQA metrics in optical microscopy, which can be computationally expensive and unstable outside ideal image domains, by leveraging deep learning for fast, generalizable quality assessment.

Method: Retrain a convolutional neural network architecture originally designed for natural images to predict individual quality metrics and a global quality score for optical microscopy data, with patch-wise predictions to create spatial quality maps.

Result: Fast and stable quality predictions with improved generalization, including robustness to outliers; enables patch-level quality estimation and visualization of spatially varying quality within a single image.

Conclusion: Deep learning-based IQA approaches like μDeepIQA offer transferable, rapid, and robust quality assessment for optical microscopy, improving reliability and interpretability of image analysis through localized quality maps.

Abstract: Optical microscopy is one of the most widely used techniques in research
studies for life sciences and biomedicine. These applications require reliable
experimental pipelines to extract valuable knowledge from the measured samples
and must be supported by image quality assessment (IQA) to ensure correct
processing and analysis of the image data. IQA methods are implemented with
variable complexity. However, while most quality metrics have a straightforward
implementation, they might be time consuming and computationally expensive when
evaluating a large dataset. In addition, quality metrics are often designed for
well-defined image features and may be unstable for images out of the ideal
domain.
  To overcome these limitations, recent works have proposed deep learning-based
IQA methods, which can provide superior performance, increased generalizability
and fast prediction. Our method, named $\mathrm{\mu}$DeepIQA, is inspired by
previous studies and applies a deep convolutional neural network designed for
IQA on natural images to optical microscopy measurements. We retrained the same
architecture to predict individual quality metrics and global quality scores
for optical microscopy data. The resulting models provide fast and stable
predictions of image quality by generalizing quality estimation even outside
the ideal range of standard methods. In addition, $\mathrm{\mu}$DeepIQA
provides patch-wise prediction of image quality and can be used to visualize
spatially varying quality in a single image. Our study demonstrates that
optical microscopy-based studies can benefit from the generalizability of deep
learning models due to their stable performance in the presence of outliers,
the ability to assess small image patches, and rapid predictions.

</details>


### [153] [In-Field Mapping of Grape Yield and Quality with Illumination-Invariant Deep Learning](https://arxiv.org/abs/2510.04864)
*Ciem Cornelissen,Sander De Coninck,Axel Willekens,Sam Leroux,Pieter Simoens*

Main category: cs.CV

TL;DR: End-to-end IoT-enabled, in-field pipeline for grape yield and quality mapping using grape-bunch detection/weight estimation plus hyperspectral-based quality assessment with a domain-adversarial autoencoder (LISA) to handle illumination shifts; validated across three illumination domains with strong performance (recall 0.82; R^2 0.76); LISA improves quality prediction generalization by >20%.


<details>
  <summary>Details</summary>
Motivation: Enable non-destructive, real-time, and spatially-resolved vineyard yield and quality mapping; address domain shift due to variable illumination in in-field hyperspectral sensing; provide high-resolution, georeferenced data for precision viticulture.

Method: Two main modules: (1) a high-performance model for grape bunch detection and weight estimation; (2) a novel deep learning framework for grape quality assessment from hyperspectral data using Light-Invariant Spectral Autoencoder (LISA), a domain-adversarial architecture that learns illumination-invariant features from uncalibrated data. Validation includes a purpose-built hyperspectral dataset spanning three illumination domains (lab artificial lighting, morning sun, afternoon sun).

Result: The complete pipeline achieves  recall of 0.82 for bunch detection and R^2 of 0.76 for weight prediction. The LISA module improves quality prediction generalization by over 20% compared to baselines.

Conclusion: Robust, high-resolution, georeferenced data on grape yield and quality can be produced in field conditions, enabling data-driven, precision viticulture insights.

Abstract: This paper presents an end-to-end, IoT-enabled robotic system for the
non-destructive, real-time, and spatially-resolved mapping of grape yield and
quality (Brix, Acidity) in vineyards. The system features a comprehensive
analytical pipeline that integrates two key modules: a high-performance model
for grape bunch detection and weight estimation, and a novel deep learning
framework for quality assessment from hyperspectral (HSI) data. A critical
barrier to in-field HSI is the ``domain shift" caused by variable illumination.
To overcome this, our quality assessment is powered by the Light-Invariant
Spectral Autoencoder (LISA), a domain-adversarial framework that learns
illumination-invariant features from uncalibrated data. We validated the
system's robustness on a purpose-built HSI dataset spanning three distinct
illumination domains: controlled artificial lighting (lab), and variable
natural sunlight captured in the morning and afternoon. Results show the
complete pipeline achieves a recall (0.82) for bunch detection and a $R^2$
(0.76) for weight prediction, while the LISA module improves quality prediction
generalization by over 20% compared to the baselines. By combining these robust
modules, the system successfully generates high-resolution, georeferenced data
of both grape yield and quality, providing actionable, data-driven insights for
precision viticulture.

</details>


### [154] [BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping](https://arxiv.org/abs/2510.04876)
*Hayat Rajani,Valerio Franchi,Borja Martinez-Clavel Valles,Raimon Ramos,Rafael Garcia,Nuno Gracias*

Main category: cs.CV

TL;DR: A large, multi-modal underwater habitat dataset with ~1M side-scan sonar tiles, bathymetric maps, and co-registered optical images, including ~36k annotated segmentation masks, enabling supervised fine-tuning and self-supervised cross-modal learning, plus open-source tools.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of large annotated datasets and the lack of standardized benchmarks in underwater habitat mapping, and to enable robust multi-sensor fusion research.

Method: Collect ~1 million side-scan sonar tiles along the Catalonia coast, acquire bathymetric maps and co-registered optical images from AUV surveys, manually annotate ~36k tiles with segmentation masks, release raw data and mosaics, spatially associate optical images with SSS tiles to support self-supervised cross-modal learning, and provide preprocessing/annotation tools.

Result: Introduces a large, open-access multi-modal dataset and accompanying tools to benchmark underwater habitat mapping. Demonstrates feasibility of cross-modal learning by pairing optical and SSS data; enables supervised fine-tuning on the annotated subset and self-supervised representation learning across modalities; supports reproducible research.

Conclusion: The resource establishes a standardized benchmark for underwater habitat mapping and advances autonomous seafloor classification and multi-sensor integration in marine ecosystems.

Abstract: Benthic habitat mapping is fundamental for understanding marine ecosystems,
guiding conservation efforts, and supporting sustainable resource management.
Yet, the scarcity of large, annotated datasets limits the development and
benchmarking of machine learning models in this domain. This paper introduces a
thorough multi-modal dataset, comprising about a million side-scan sonar (SSS)
tiles collected along the coast of Catalonia (Spain), complemented by
bathymetric maps and a set of co-registered optical images from targeted
surveys using an autonomous underwater vehicle (AUV). Approximately \num{36000}
of the SSS tiles have been manually annotated with segmentation masks to enable
supervised fine-tuning of classification models. All the raw sensor data,
together with mosaics, are also released to support further exploration and
algorithm development. To address challenges in multi-sensor data fusion for
AUVs, we spatially associate optical images with corresponding SSS tiles,
facilitating self-supervised, cross-modal representation learning. Accompanying
open-source preprocessing and annotation tools are provided to enhance
accessibility and encourage research. This resource aims to establish a
standardized benchmark for underwater habitat mapping, promoting advancements
in autonomous seafloor classification and multi-sensor integration.

</details>


### [155] [Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for Motorbike Detection in Kigali Autonomous Driving Context](https://arxiv.org/abs/2510.04912)
*Ngeyen Yinkfu,Sunday Nwovu,Jonathan Kayizzi,Angelique Uwamahoro*

Main category: cs.CV

TL;DR: Comparative evaluation of four object detectors (YOLOv5, Faster R-CNN, SSD, RetinaNet) for motorbike detection in Kigali using a 198-image PyTorch transfer-learning dataset, assessing accuracy, localization, and inference speed, highlighting data/model limitations and suggesting simpler architectures for better deployment in developing countries.


<details>
  <summary>Details</summary>
Motivation: Motorcycle taxis in Kigali pose unpredictable traffic scenarios; autonomous driving requires robust, real-time motorbike detection on resource-constrained hardware, motivating a comparative study across popular detectors.

Method: Train/transfer-learn four detectors (YOLOv5, Faster R-CNN, SSD, RetinaNet) in PyTorch on a custom Kigali dataset (n=198); evaluate using accuracy, localization (IoU/precision-recall), and inference speed; discuss implementation challenges and dataset limitations.

Result: The models exhibit trade-offs between accuracy, localization, and speed; no single model dominates under constrained settings; dataset size/variety and model complexity limit performance and generalizability; real-time viability on resource-limited devices remains challenging.

Conclusion: Recommend pursuing simplified architectures and design choices to improve accessibility and deployment of autonomous systems in developing countries like Rwanda; more work on dataset quality, annotation, and efficient models.

Abstract: In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,
often navigating unpredictably and disregarding traffic rules, posing
significant challenges for autonomous driving systems. This study compares four
object detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for
motorbike detection using a custom dataset of 198 images collected in Kigali.
Implemented in PyTorch with transfer learning, the models were evaluated for
accuracy, localization, and inference speed to assess their suitability for
real-time navigation in resource-constrained settings. We identify
implementation challenges, including dataset limitations and model
complexities, and recommend simplified architectures for future work to enhance
accessibility for autonomous systems in developing countries like Rwanda.

</details>


### [156] [A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images](https://arxiv.org/abs/2510.04916)
*Giulio Weikmann,Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: SAHC introduces Semantics-Aware Hierarchical Consensus for remote sensing classification by integrating hierarchy-aware heads, trainable hierarchy matrices, and a hierarchical consensus to enforce cross-level consistency, showing robustness across datasets and backbones.


<details>
  <summary>Details</summary>
Motivation: Traditional remote sensing classification often ignores predefined class hierarchies; exploiting hierarchical structure can improve semantic consistency and generalization.

Method: Proposes SAHC with multiple hierarchy-specific classification heads at varying granularity, trainable hierarchy matrices guiding hierarchy learning in a self-supervised manner, and a hierarchical consensus mechanism that acts as a weighted ensemble to enforce consistent probability distributions across levels.

Result: Evaluated on three benchmark datasets with different hierarchical complexities and backbones; results demonstrate effectiveness in guiding network learning and robustness of the hierarchical consensus across tasks.

Conclusion: SAHC effectively learns hierarchical features and relationships, is adaptable across backbones, and demonstrates the value of leveraging hierarchical structure for remote sensing image classification.

Abstract: Deep learning has become increasingly important in remote sensing image
classification due to its ability to extract semantic information from complex
data. Classification tasks often include predefined label hierarchies that
represent the semantic relationships among classes. However, these hierarchies
are frequently overlooked, and most approaches focus only on fine-grained
classification schemes. In this paper, we present a novel Semantics-Aware
Hierarchical Consensus (SAHC) method for learning hierarchical features and
relationships by integrating hierarchy-specific classification heads within a
deep network architecture, each specialized in different degrees of class
granularity. The proposed approach employs trainable hierarchy matrices, which
guide the network through the learning of the hierarchical structure in a
self-supervised manner. Furthermore, we introduce a hierarchical consensus
mechanism to ensure consistent probability distributions across different
hierarchical levels. This mechanism acts as a weighted ensemble being able to
effectively leverage the inherent structure of the hierarchical classification
task. The proposed SAHC method is evaluated on three benchmark datasets with
different degrees of hierarchical complexity on different tasks, using distinct
backbone architectures to effectively emphasize its adaptability. Experimental
results show both the effectiveness of the proposed approach in guiding network
learning and the robustness of the hierarchical consensus for remote sensing
image classification tasks.

</details>


### [157] [REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis](https://arxiv.org/abs/2510.04923)
*Alec K. Peltekian,Halil Ertugrul Aktas,Gorkem Durak,Kevin Grudzinski,Bradford C. Bemiss,Carrie Richardson,Jane E. Dematte,G. R. Scott Budinger,Anthony J. Esposito,Alexander Misharin,Alok Choudhary,Ankit Agrawal,Ulas Bagci*

Main category: cs.CV

TL;DR: An anatomically-informed mixture-of-experts for medical imaging: seven region-specific lung experts with radiomics-DL gating improve ILD classification and show region-level performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Standard Mixture-of-Experts (MoE) in medical imaging lacks domain-specific constraints tied to anatomy; incorporating anatomical priors can better model region-specific disease heterogeneity and improve performance and interpretability.

Method: Propose Regional Expert Networks (REN): an MoE framework with seven experts dedicated to distinct lung lobes and bilateral lung combos. A multi-modal gating mechanism fuses radiomics biomarkers and deep learning features (CNN, ViT, Mamba) to weight expert contributions. Trained on ILD classification with patient-level cross-validation to assess generalization and interpretability.

Result: REN achieved average AUC 0.8646±0.0467, +12.5% vs SwinUNETR baseline (0.7685, p=0.031). Region-specific performance: lower-lobe models AUC 0.88–0.90, outperforming DL baselines (CNN 0.76–0.79). Demonstrated good generalizability and clinical interpretability, scalable to other structured medical imaging tasks.

Conclusion: An anatomically-guided MoE framework that leverages regional priors and multi-modal gating to improve ILD classification, with strong generalizability and interpretability; extendable to other structured medical imaging domains.

Abstract: Mixture-of-Experts (MoE) architectures have significantly contributed to
scalable machine learning by enabling specialized subnetworks to tackle complex
tasks efficiently. However, traditional MoE systems lack domain-specific
constraints essential for medical imaging, where anatomical structure and
regional disease heterogeneity strongly influence pathological patterns. Here,
we introduce Regional Expert Networks (REN), the first anatomically-informed
MoE framework tailored specifically for medical image classification. REN
leverages anatomical priors to train seven specialized experts, each dedicated
to distinct lung lobes and bilateral lung combinations, enabling precise
modeling of region-specific pathological variations. Multi-modal gating
mechanisms dynamically integrate radiomics biomarkers and deep learning (DL)
features (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to
interstitial lung disease (ILD) classification, REN achieves consistently
superior performance: the radiomics-guided ensemble reached an average AUC of
0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC
0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe
models achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)
and aligning with known disease progression patterns. Through rigorous
patient-level cross-validation, REN demonstrates strong generalizability and
clinical interpretability, presenting a scalable, anatomically-guided approach
readily extensible to other structured medical imaging applications.

</details>


### [158] [Unsupervised Active Learning via Natural Feature Progressive Framework](https://arxiv.org/abs/2510.04939)
*Yuxi Liu,Catherine Lalman,Yimin Yang*

Main category: cs.CV

TL;DR: NFPF is a unsupervised active learning method that uses a Specific Feature Learning Machine (SFLM) to quantify each sample's contribution to model performance and a Reconstruction Difference metric for initial sample selection, achieving state-of-the-art results among UAL methods and competitive performance with supervised AL on vision datasets, with improved robustness and distribution coverage.


<details>
  <summary>Details</summary>
Motivation: Deep learning models rely on large labeled datasets, which are costly to obtain. Active Learning reduces labeling effort but is often interactive. Unsupervised Active Learning aims to shift labeling to a single post-selection step, but current methods rely on local gradient-based scoring and shallow, one-shot selection, limiting performance, robustness, and distribution coverage. A robust, distribution-aware UAL approach is needed.

Method: Introduce the Natural Feature Progressive Framework (NFPF), which employs a Specific Feature Learning Machine (SFLM) to quantify each sample's contribution to model performance and to define a Reconstruction Difference metric for initial sample selection. The framework emphasizes progressive feature learning and non-local, distribution-aware sample selection, with extensive experiments including ablations and qualitative visualizations on vision datasets.

Result: NFPF significantly outperforms established UAL methods and reaches performance on par with supervised AL on vision datasets. Ablation studies confirm the contributions of the SFLM and Reconstruction Difference metric, and qualitative visualizations demonstrate improved robustness and more complete data distribution coverage.

Conclusion: NFPF advances Unsupervised Active Learning by providing a robust, principled mechanism to assess sample importance and execute initial selection, reducing annotation burden while achieving strong performance and better data distribution coverage. The findings suggest further exploration of SFLM-based metrics and reconstruction-style criteria across modalities.

Abstract: The effectiveness of modern deep learning models is predicated on the
availability of large-scale, human-annotated datasets, a process that is
notoriously expensive and time-consuming. While Active Learning (AL) offers a
strategic solution by labeling only the most informative and representative
data, its iterative nature still necessitates significant human involvement.
Unsupervised Active Learning (UAL) presents an alternative by shifting the
annotation burden to a single, post-selection step. Unfortunately, prevailing
UAL methods struggle to achieve state-of-the-art performance. These approaches
typically rely on local, gradient-based scoring for sample importance
estimation, which not only makes them vulnerable to ambiguous and noisy data
but also hinders their capacity to select samples that adequately represent the
full data distribution. Moreover, their use of shallow, one-shot linear
selection falls short of a true UAL paradigm. In this paper, we propose the
Natural Feature Progressive Framework (NFPF), a UAL method that revolutionizes
how sample importance is measured. At its core, NFPF employs a Specific Feature
Learning Machine (SFLM) to effectively quantify each sample's contribution to
model performance. We further utilize the SFLM to define a powerful
Reconstruction Difference metric for initial sample selection. Our
comprehensive experiments show that NFPF significantly outperforms all
established UAL methods and achieves performance on par with supervised AL
methods on vision datasets. Detailed ablation studies and qualitative
visualizations provide compelling evidence for NFPF's superior performance,
enhanced robustness, and improved data distribution coverage.

</details>


### [159] [Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion](https://arxiv.org/abs/2510.04947)
*Xin Li,Kaixiang Yang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: CA3D-Diff enables high-fidelity bidirectional translation between CC and MLO mammograms using column-aware cross-attention and implicit 3D reconstruction within a conditional diffusion framework, improving structural consistency and diagnostic performance.


<details>
  <summary>Details</summary>
Motivation: Missing or corrupted views in dual-view mammography hinder diagnosis; cross-view translation can recover missing views and improve lesion alignment, but challenges include large non-rigid deformations and tissue overlap.

Method: Propose CA3D-Diff; two core components: (1) column-aware cross-attention with Gaussian-decayed bias leveraging column-position correspondence across views to align anatomies; (2) implicit 3D structure reconstruction by back-projecting 2D latents into a coarse 3D feature volume based on projection geometry, then refine and inject into denoising UNet; use conditional diffusion model for bidirectional translation.

Result: Experiments show CA3D-Diff achieves superior performance in bidirectional tasks vs state-of-the-art; improved visual fidelity and structural consistency; synthesized views improve single-view malignancy classification in screening.

Conclusion: This approach enhances anatomical awareness and reliability of cross-view synthesis, with practical diagnostic value in real-world workflows; potential to recover missing data and improve downstream analysis.

Abstract: Dual-view mammography, including craniocaudal (CC) and mediolateral oblique
(MLO) projections, offers complementary anatomical views crucial for breast
cancer diagnosis. However, in real-world clinical workflows, one view may be
missing, corrupted, or degraded due to acquisition errors or compression
artifacts, limiting the effectiveness of downstream analysis. View-to-view
translation can help recover missing views and improve lesion alignment. Unlike
natural images, this task in mammography is highly challenging due to large
non-rigid deformations and severe tissue overlap in X-ray projections, which
obscure pixel-level correspondences. In this paper, we propose Column-Aware and
Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view
translation framework based on conditional diffusion model. To address
cross-view structural misalignment, we first design a column-aware
cross-attention mechanism that leverages the geometric property that
anatomically corresponding regions tend to lie in similar column positions
across views. A Gaussian-decayed bias is applied to emphasize local column-wise
correlations while suppressing distant mismatches. Furthermore, we introduce an
implicit 3D structure reconstruction module that back-projects noisy 2D latents
into a coarse 3D feature volume based on breast-view projection geometry. The
reconstructed 3D structure is refined and injected into the denoising UNet to
guide cross-view generation with enhanced anatomical awareness. Extensive
experiments demonstrate that CA3D-Diff achieves superior performance in
bidirectional tasks, outperforming state-of-the-art methods in visual fidelity
and structural consistency. Furthermore, the synthesized views effectively
improve single-view malignancy classification in screening settings,
demonstrating the practical value of our method in real-world diagnostics.

</details>


### [160] [SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization](https://arxiv.org/abs/2510.04961)
*Théophane Vallaeys,Jakob Verbeek,Matthieu Cord*

Main category: cs.CV

TL;DR: A GAN-free, single-step diffusion-based tokenizer (SSDD) improves over KL-VAE by using a pixel diffusion decoder enhanced with transformers, with distillation to a fast, adversarial-free single-step decoder.


<details>
  <summary>Details</summary>
Motivation: Tokenizers for generative image models must balance reconstruction quality, speed, and training stability. KL-VAE-based tokenizers rely on adversarial or perceptual losses and slower decoding due to iterative sampling. A GAN-free, single-step diffusion approach promises better scalability and efficiency while maintaining quality.

Method: Introduce SSDD: a pixel diffusion decoder architecture that leverages transformer components and GAN-free training. Train a diffusion decoder without adversarial losses. Use distillation to transfer the diffusion model’s performance into an efficient single-step decoder, enabling fast reconstruction. Evaluate as a drop-in replacement for KL-VAE and on DiTs (diffusion-based transformers) for generation quality and throughput.

Result: SSDD achieves reconstruction FID improvement from 0.87 to 0.50, with 1.4× higher throughput. It preserves DiTs’ generation quality with 3.8× faster sampling compared to existing diffusion pipelines, and serves as a drop-in replacement for KL-VAE.

Conclusion: SSDD demonstrates that a GAN-free, single-step diffusion decoder can outperform KL-VAE in reconstruction and sampling speed, enabling higher-quality, faster tokenizers for state-of-the-art image models.

Abstract: Tokenizers are a key component of state-of-the-art generative image models,
extracting the most important features from the signal while reducing data
dimension and redundancy. Most current tokenizers are based on KL-regularized
variational autoencoders (KL-VAE), trained with reconstruction, perceptual and
adversarial losses. Diffusion decoders have been proposed as a more principled
alternative to model the distribution over images conditioned on the latent.
However, matching the performance of KL-VAE still requires adversarial losses,
as well as a higher decoding time due to iterative sampling. To address these
limitations, we introduce a new pixel diffusion decoder architecture for
improved scaling and training stability, benefiting from transformer components
and GAN-free training. We use distillation to replicate the performance of the
diffusion decoder in an efficient single-step decoder. This makes SSDD the
first diffusion decoder optimized for single-step reconstruction trained
without adversarial losses, reaching higher reconstruction quality and faster
sampling than KL-VAE. In particular, SSDD improves reconstruction FID from
$0.87$ to $0.50$ with $1.4\times$ higher throughput and preserve generation
quality of DiTs with $3.8\times$ faster sampling. As such, SSDD can be used as
a drop-in replacement for KL-VAE, and for building higher-quality and faster
generative models.

</details>


### [161] [ActiveMark: on watermarking of visual foundation models via massive activations](https://arxiv.org/abs/2510.04966)
*Anna Chistyakova,Mikhail Pautov*

Main category: cs.CV

TL;DR: A watermarking approach for visual foundation models to verify ownership by embedding watermarks into internal representations via fine-tuned layers and a small encoder-decoder, robust to fine-tuning with low false positives/negatives.


<details>
  <summary>Details</summary>
Motivation: Protect IP rights of large VFMs distributed with licenses; prevent illegal redistribution; provide reliable ownership verification that distinguishes copies from independent models.

Method: Fine-tune a small set of expressive layers of the VFM together with a compact encoder-decoder network to embed digital watermarks into the internal representations of a held-out input set. Watermarks persist in functional copies even after downstream fine-tuning.

Result: Theoretically and experimentally, low false positive and low false negative rates for watermark detection; watermark remains detectable after model reuse.

Conclusion: Proposes a practical ownership verification approach for VFMs with strong robustness to downstream adaptation and minimal risk of misdetection.

Abstract: Being trained on large and vast datasets, visual foundation models (VFMs) can
be fine-tuned for diverse downstream tasks, achieving remarkable performance
and efficiency in various computer vision applications. The high computation
cost of data collection and training motivates the owners of some VFMs to
distribute them alongside the license to protect their intellectual property
rights. However, a dishonest user of the protected model's copy may illegally
redistribute it, for example, to make a profit. As a consequence, the
development of reliable ownership verification tools is of great importance
today, since such methods can be used to differentiate between a redistributed
copy of the protected model and an independent model. In this paper, we propose
an approach to ownership verification of visual foundation models by
fine-tuning a small set of expressive layers of a VFM along with a small
encoder-decoder network to embed digital watermarks into an internal
representation of a hold-out set of input images. Importantly, the watermarks
embedded remain detectable in the functional copies of the protected model,
obtained, for example, by fine-tuning the VFM for a particular downstream task.
Theoretically and experimentally, we demonstrate that the proposed method
yields a low probability of false detection of a non-watermarked model and a
low probability of false misdetection of a watermarked model.

</details>


### [162] [Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition](https://arxiv.org/abs/2510.05006)
*Koen Vellenga,H. Joe Steinhauer,Jonas Andersson,Anders Sjögren*

Main category: cs.CV

TL;DR: Proposes latent uncertainty representations (LUR) and repulsively trained LUR (RLUR) to estimate uncertainty in pre-trained DNNs for video-based driver action/intention recognition. Shows comparable in-distribution performance to last-layer probabilistic methods, with strong, efficient uncertainty-based OOD detection across datasets; introduces extensive NuScenes labeling.


<details>
  <summary>Details</summary>
Motivation: Safety-critical, resource-constrained video tasks require reliable uncertainty and OOD detection. Last-layer probabilistic deep learning (LL-PDL) methods can be inconsistent and expensive; develop latent transformation layers to yield multiple latent representations for better uncertainty estimation.

Method: Extend pre-trained DNNs with transformation layers to create multiple latent representations. Develop LUR and RLUR approaches. Evaluate against eight PDL methods on four video-based driver action/intention datasets. Use 28,000 frame-level action labels and 1,194 video-level intention labels from NuScenes. Compare in-distribution accuracy, calibration, and uncertainty-based OOD detection.

Result: LUR and RLUR achieve comparable in-distribution classification performance to other LL-PDL methods. For uncertainty-based OOD detection, LUR matches top-performing PDL methods and offers advantages in training efficiency and tuning ease compared with MCMC-based or repulsive training approaches.

Conclusion: Latent uncertainty representations are a competitive and efficient alternative to traditional LL-PDL methods for uncertainty estimation and OOD detection in safety-critical video tasks. The work also provides substantial NuScenes labels to support such research.

Abstract: Deep neural networks (DNNs) are increasingly applied to safety-critical tasks
in resource-constrained environments, such as video-based driver action and
intention recognition. While last layer probabilistic deep learning (LL-PDL)
methods can detect out-of-distribution (OOD) instances, their performance
varies. As an alternative to last layer approaches, we propose extending
pre-trained DNNs with transformation layers to produce multiple latent
representations to estimate the uncertainty. We evaluate our latent uncertainty
representation (LUR) and repulsively trained LUR (RLUR) approaches against
eight PDL methods across four video-based driver action and intention
recognition datasets, comparing classification performance, calibration, and
uncertainty-based OOD detection. We also contribute 28,000 frame-level action
labels and 1,194 video-level intention labels for the NuScenes dataset. Our
results show that LUR and RLUR achieve comparable in-distribution
classification performance to other LL-PDL approaches. For uncertainty-based
OOD detection, LUR matches top-performing PDL methods while being more
efficient to train and easier to tune than approaches that require Markov-Chain
Monte Carlo sampling or repulsive training procedures.

</details>


### [163] [Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns](https://arxiv.org/abs/2510.05015)
*Nabil Daiyan,Md Rakibul Haque*

Main category: cs.CV

TL;DR: A CNN-based, transfer learning and attention–driven framework on hand-drawn spirals and waves achieves high accuracy (spiral ~90%; wave ~96.7%; ensemble ~93.3%) for early, non-invasive PD detection using augmented data and hard voting.


<details>
  <summary>Details</summary>
Motivation: PD is a progressive neurodegenerative disease where early diagnosis is critical for treatment and prognosis. Current diagnostics are costly and cumbersome, so a non-invasive biomarker-based ML approach could enable earlier, cheaper screening.

Method: Multi-stage architecture: (1) use pre-trained CNNs for feature extraction; (2) append custom convolutional layers and attention mechanisms; (3) apply data augmentation to expand spiral and wave image sets; (4) ensemble with hard voting across models to improve robustness against overfitting.

Result: Spiral images: weighted average precision, recall, F1 = 90%. Wave images: 96.67% across these metrics. Ensemble hard voting yields overall accuracy of 93.3%.

Conclusion: The study demonstrates that ML on hand-drawn biomarkers can be a promising non-invasive approach for early PD diagnosis, with strong performance on the reported metrics and potential for cost reduction and accessibility.

Abstract: Parkinson's disease (PD) is a progressive neurodegenerative condition
characterized by the death of dopaminergic neurons, leading to various movement
disorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects,
yet traditional diagnostic methods are often cumbersome and costly. In this
study, a machine learning-based approach is proposed using hand-drawn spiral
and wave images as potential biomarkers for PD detection. Our methodology
leverages convolutional neural networks (CNNs), transfer learning, and
attention mechanisms to improve model performance and resilience against
overfitting. To enhance the diversity and richness of both spiral and wave
categories, the training dataset undergoes augmentation to increase the number
of images. The proposed architecture comprises three phases: utilizing
pre-trained CNNs, incorporating custom convolutional layers, and ensemble
voting. Employing hard voting further enhances performance by aggregating
predictions from multiple models. Experimental results show promising accuracy
rates. For spiral images, weighted average precision, recall, and F1-score are
90%, and for wave images, they are 96.67%. After combining the predictions
through ensemble hard voting, the overall accuracy is 93.3%. These findings
underscore the potential of machine learning in early PD diagnosis, offering a
non-invasive and cost-effective solution to improve patient outcomes.

</details>


### [164] [Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models](https://arxiv.org/abs/2510.05034)
*Yunlong Tang,Jing Bi,Pinxin Liu,Zhenyu Pan,Zhangyun Tan,Qianxiang Shen,Jiani Liu,Hang Hua,Junjia Guo,Yunzhong Xiao,Chao Huang,Zhiyuan Wang,Susan Liang,Xinyi Liu,Yizhi Song,Yuhe Nie,Jia-Xing Zhong,Bozheng Li,Daiqing Qi,Ziyun Zeng,Ali Vosoughi,Luchuan Song,Zeliang Zhang,Daiki Shimada,Han Liu,Jiebo Luo,Chenliang Xu*

Main category: cs.CV

TL;DR: This survey comprehensively analyzes post-training methodologies for Video-Large Multimodal Models (Video-LMMs), organizing methods into three pillars—supervised fine-tuning with chain-of-thought (SFT+CoT), reinforcement learning from verifiable objectives (RL), and test-time scaling (TTS). It provides a video-tailored taxonomy, design principles, evaluation protocols, curated benchmarks, and a roadmap for future challenges and resources.


<details>
  <summary>Details</summary>
Motivation: Post-training is the critical phase that enables Video-LMMs to move from perception to robust reasoning. The literature is fragmented across techniques and lacks a unified, video-specific framework that addresses temporal, spatiotemporal, and multimodal challenges. A comprehensive survey is needed to consolidate methods, benchmarks, and best practices to accelerate progress.

Method: Systematic literature review and synthesis that (1) categorizes post-training approaches into SFT with chain-of-thought, RL from verifiable objectives, and TTS; (2) develops a structured taxonomy of video-specific adaptations and inter-method relationships; (3) analyzes representative methods to extract design principles, evaluation protocols, and practical considerations; (4) curates benchmarks, datasets, and metrics and provides a resource repository for ongoing updates.

Result: A unified framework and taxonomy for post-training Video-LMMs, including (i) clear roles and interconnections among SFT+CoT, RL, and TTS; (ii) video-specific design principles addressing temporal localization, spatiotemporal grounding, long video efficiency, and multimodal integration; (iii) recommended evaluation protocols and benchmarks; (iv) identified open challenges in reward design, scalability, and cost-performance optimization; (v) curated essential resources and datasets with a maintained online hub for updates.

Conclusion: The survey equips researchers and practitioners with a coherent roadmap for advancing Video-LMM post-training, offering principled guidance, validated insights, and accessible resources. It highlights remaining gaps (reward design, scalability, cost-sensitive optimization) and provides a foundation for future work and community collaboration, with ongoing resources at the linked GitHub repository.

Abstract: Video understanding represents the most challenging frontier in computer
vision, requiring models to reason about complex spatiotemporal relationships,
long-term dependencies, and multimodal evidence. The recent emergence of
Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders
with powerful decoder-based language models, has demonstrated remarkable
capabilities in video understanding tasks. However, the critical phase that
transforms these models from basic perception systems into sophisticated
reasoning engines, post-training, remains fragmented across the literature.
This survey provides the first comprehensive examination of post-training
methodologies for Video-LMMs, encompassing three fundamental pillars:
supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)
from verifiable objectives, and test-time scaling (TTS) through enhanced
inference computation. We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration. Through
systematic analysis of representative methods, we synthesize key design
principles, insights, and evaluation protocols while identifying critical open
challenges in reward design, scalability, and cost-performance optimization. We
further curate essential benchmarks, datasets, and metrics to facilitate
rigorous assessment of post-training effectiveness. This survey aims to provide
researchers and practitioners with a unified framework for advancing Video-LMM
capabilities. Additional resources and updates are maintained at:
https://github.com/yunlong10/Awesome-Video-LMM-Post-Training

</details>


### [165] [SegMASt3R: Geometry Grounded Segment Matching](https://arxiv.org/abs/2510.05051)
*Rohit Jayanti,Swayam Agrawal,Vansh Garg,Siddharth Tourani,Muhammad Haris Khan,Sourav Garg,Madhava Krishna*

Main category: cs.CV

TL;DR: A segmentation-matching method that uses the inductive bias of 3D foundation models to match segments across image pairs with extreme viewpoint changes, achieving state-of-the-art gains.


<details>
  <summary>Details</summary>
Motivation: Segment-level correspondences are more robust than keypoints but are hard to establish under wide baselines; leveraging 3D foundation models' geometry-aware priors can improve cross-view segment matching.

Method: An architecture that leverages the inductive biases of 3D foundation models to match semantically/geometrically coherent segments across image pairs with up to 180° viewpoint change.

Result: Outperforms state-of-the-art methods (including SAM2 video propagator and local feature matching) by up to 30% on AUPRC on ScanNet++ and Replica datasets.

Conclusion: Demonstrates the effectiveness of 3D foundation-model priors for wide-baseline segment matching and its benefits for downstream tasks like 3D instance segmentation and image-goal navigation; project page provided.

Abstract: Segment matching is an important intermediate task in computer vision that
establishes correspondences between semantically or geometrically coherent
regions across images. Unlike keypoint matching, which focuses on localized
features, segment matching captures structured regions, offering greater
robustness to occlusions, lighting variations, and viewpoint changes. In this
paper, we leverage the spatial understanding of 3D foundation models to tackle
wide-baseline segment matching, a challenging setting involving extreme
viewpoint shifts. We propose an architecture that uses the inductive bias of
these 3D foundation models to match segments across image pairs with up to 180
degree view-point change. Extensive experiments show that our approach
outperforms state-of-the-art methods, including the SAM2 video propagator and
local feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++
and Replica datasets. We further demonstrate benefits of the proposed model on
relevant downstream tasks, including 3D instance segmentation and image-goal
navigation. Project Page: https://segmast3r.github.io/

</details>


### [166] [No-reference Quality Assessment of Contrast-distorted Images using Contrast-enhanced Pseudo Reference](https://arxiv.org/abs/2510.05053)
*Mohammad-Ali Mahmoudpour,Saeed Mahmoudpour*

Main category: cs.CV

TL;DR: A no-reference image quality assessment method for contrast distortions using pseudo-reference images generated by selecting an optimal contrast enhancement algorithm via a classifier, then performing full-reference QA between the pseudo-reference and degraded images. Validated on CCID2014, TID2013, and CSIQ with promising results.


<details>
  <summary>Details</summary>
Motivation: Contrast distortions under adverse lighting degrade visual quality and contrast NR-IQA methods specifically for contrast distortions are underexplored. The paper seeks to transform NR-IQA into FR evaluation by generating a high-quality pseudo-reference.

Method: Generate a large dataset of contrast-enhanced images using multiple enhancement algorithms. Train a classifier to select the most suitable enhancement algorithm based on image content and distortion. Create pseudo-reference images via the selected enhancement. Conduct FR quality assessment between the pseudo-reference and degraded image.

Result: Evaluation on three databases (CCID2014, TID2013, CSIQ) shows promising performance of the proposed NR-IQA method, indicating effective pseudo-reference generation and FR assessment for contrast distortions.

Conclusion: The proposed approach provides a practical NR-IQA framework for contrast distortions by leveraging pseudo-reference generation through classifier-guided enhancement selection, achieving promising cross-dataset results; future work could address refinement of enhancement choices and more robust pseudo-reference generation.

Abstract: Contrast change is an important factor that affects the quality of images.
During image capturing, unfavorable lighting conditions can cause contrast
change and visual quality loss. While various methods have been proposed to
assess the quality of images under different distortions such as blur and
noise, contrast distortion has been largely overlooked as its visual impact and
properties are different from other conventional types of distortions. In this
paper, we propose a no-reference image quality assessment (NR-IQA) metric for
contrast-distorted images. Using a set of contrast enhancement algorithms, we
aim to generate pseudo-reference images that are visually close to the actual
reference image, such that the NR problem is transformed to a Full-reference
(FR) assessment with higher accuracy. To this end, a large dataset of
contrast-enhanced images is produced to train a classification network that can
select the most suitable contrast enhancement algorithm based on image content
and distortion for pseudo-reference image generation. Finally, the evaluation
is performed in the FR manner to assess the quality difference between the
contrast-enhanced (pseudoreference) and degraded images. Performance evaluation
of the proposed method on three databases containing contrast distortions
(CCID2014, TID2013, and CSIQ), indicates the promising performance of the
proposed method.

</details>


### [167] [Neuroplastic Modular Framework: Cross-Domain Image Classification of Garbage and Industrial Surfaces](https://arxiv.org/abs/2510.05071)
*Debojyoti Ghosh,Soumya K Ghosh,Adrijit Goswami*

Main category: cs.CV

TL;DR: Hybrid classifier combining a ResNet-50 backbone, Vision Transformer (ViT), and FAISS-based memory retrieval with expandable neuroplastic modules for robust, adaptive image classification in waste management and industrial defect detection; shows superior accuracy and adaptability on KolektorSDD2 and related tasks.


<details>
  <summary>Details</summary>
Motivation: The need for robust generalization in dynamic environmental and industrial settings, leveraging local and global feature fusion plus memory-like references to improve adaptability and performance.

Method: A hybrid architecture that uses ResNet-50 for localized feature extraction, ViT for capturing global semantic context, and FAISS-based similarity retrieval as a memory reference. The neuroplastic modular design comprises expandable, learnable blocks that grow during training when performance plateaus, enabling gradual adaptation to data complexity.

Result: The model outperforms traditional static models in both accuracy and adaptability across domains, validated on waste classification datasets and KolektorSDD2, indicating strong cross-domain applicability.

Conclusion: The Neuroplastic Modular Classifier provides a scalable, high-performance solution for real-world image classification, offering improved generalization in dynamic environments across environmental and industrial domains.

Abstract: Efficient and accurate classification of waste and industrial surface defects
is essential for ensuring sustainable waste management and maintaining high
standards in quality control. This paper introduces the Neuroplastic Modular
Classifier, a novel hybrid architecture designed for robust and adaptive image
classification in dynamic environments. The model combines a ResNet-50 backbone
for localized feature extraction with a Vision Transformer (ViT) to capture
global semantic context. Additionally, FAISS-based similarity retrieval is
incorporated to provide a memory-like reference to previously encountered data,
enriching the model's feature space. A key innovation of our architecture is
the neuroplastic modular design composed of expandable, learnable blocks that
dynamically grow during training when performance plateaus. Inspired by
biological learning systems, this mechanism allows the model to adapt to data
complexity over time, improving generalization. Beyond garbage classification,
we validate the model on the Kolektor Surface Defect Dataset 2 (KolektorSDD2),
which involves industrial defect detection on metal surfaces. Experimental
results across domains show that the proposed architecture outperforms
traditional static models in both accuracy and adaptability. The Neuroplastic
Modular Classifier offers a scalable, high-performance solution for real-world
image classification, with strong applicability in both environmental and
industrial domains.

</details>


### [168] [Factuality Matters: When Image Generation and Editing Meet Structured Visuals](https://arxiv.org/abs/2510.05091)
*Le Zhuo,Songhao Han,Yuandong Pu,Boxiang Qiu,Sayak Paul,Yue Liao,Yihao Liu,Jie Shao,Xi Chen,Si Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: Introduces StructBench and StructScore for structured-visual generation; builds a 1.3M dataset, trains a unified VLM+FLUX.1 Kontext model with a 3-stage curriculum and external reasoner; reports improvements though gaps remain.


<details>
  <summary>Details</summary>
Motivation: Structured visuals (charts, diagrams, math figures) require precise composition and factual fidelity that conventional image generators struggle to deliver; a comprehensive dataset, model, and benchmark are missing.

Method: Data: 1.3M high-quality structured image pairs from executable drawing programs with chain-of-thought annotations. Model: a unified VLM integrated with FLUX.1 Kontext via a lightweight connector; training: three-stage curriculum—feature alignment, knowledge infusion, reasoning-augmented generation; inference-time external reasoner. Benchmark: StructBench (1,700+ instances) and StructScore with multi-round Q&A.

Result: 15 models evaluated; leading closed-source systems lag behind; proposed model achieves strong editing performance; inference-time reasoning provides consistent gains across architectures.

Conclusion: Release dataset, model, benchmark to propel unified multimodal foundations for structured visuals.

Abstract: While modern visual generation models excel at creating aesthetically
pleasing natural images, they struggle with producing or editing structured
visuals like charts, diagrams, and mathematical figures, which demand
composition planning, text rendering, and multimodal reasoning for factual
fidelity. To address this, we present the first comprehensive, systematic
investigation of this domain, encompassing data construction, model training,
and an evaluation benchmark. First, we construct a large-scale dataset of 1.3
million high-quality structured image pairs derived from executable drawing
programs and augmented with chain-of-thought reasoning annotations. Building on
it, we train a unified model that integrates a VLM with FLUX.1 Kontext via a
lightweight connector for enhanced multimodal understanding. A three-stage
training curriculum enables progressive feature alignment, knowledge infusion,
and reasoning-augmented generation, further boosted by an external reasoner at
inference time. Finally, we introduce StructBench, a novel benchmark for
generation and editing with over 1,700 challenging instances, and an
accompanying evaluation metric, StructScore, which employs a multi-round Q\&A
protocol to assess fine-grained factual accuracy. Evaluations of 15 models
reveal that even leading closed-source systems remain far from satisfactory.
Our model attains strong editing performance, and inference-time reasoning
yields consistent gains across diverse architectures. By releasing the dataset,
model, and benchmark, we aim to advance unified multimodal foundations for
structured visuals.

</details>


### [169] [Character Mixing for Video Generation](https://arxiv.org/abs/2510.05093)
*Tingting Liao,Chongjian Ge,Guangyi Liu,Hao Li,Yi Zhou*

Main category: cs.CV

TL;DR: We propose Cross-Character Embedding (CCE) and Cross-Character Augmentation (CCA) to enable natural cross-character interactions in text-to-video while preserving identities and styles. They tackle co-existence gaps and style delusion. Evaluated on a benchmark of cartoons and live-action with 10 characters, showing improvements in identity preservation, interaction quality, and robustness. Project page linked.


<details>
  <summary>Details</summary>
Motivation: Inter-character interaction across different worlds is challenging because characters may never have coexisted, and mixing visual styles can cause style delusion (realistic characters appearing cartoonish or vice versa). A reliable method should preserve each character's identity and behavior while enabling coherent cross-context interactions.

Method: Introduce Cross-Character Embedding (CCE) to learn identity and behavioral logic across multimodal sources; introduce Cross-Character Augmentation (CCA) to enrich training with synthetic co-existence and mixed-style data; combine these to enable natural interactions without sacrificing stylistic fidelity.

Result: Experiments on a curated benchmark of cartoons and live-action series with 10 characters show clear improvements in identity preservation, interaction quality, and robustness to style delusion; additional results and videos are available on the project page.

Conclusion: CCE+CCA enable natural interactions between previously non-coexistent characters, expanding creative storytelling possibilities while maintaining stylistic fidelity; results and demos on the project page support the claims.

Abstract: Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where
characters interact naturally across different worlds? We study inter-character
interaction in text-to-video generation, where the key challenge is to preserve
each character's identity and behaviors while enabling coherent cross-context
interaction. This is difficult because characters may never have coexisted and
because mixing styles often causes style delusion, where realistic characters
appear cartoonish or vice versa. We introduce a framework that tackles these
issues with Cross-Character Embedding (CCE), which learns identity and
behavioral logic across multimodal sources, and Cross-Character Augmentation
(CCA), which enriches training with synthetic co-existence and mixed-style
data. Together, these techniques allow natural interactions between previously
uncoexistent characters without losing stylistic fidelity. Experiments on a
curated benchmark of cartoons and live-action series with 10 characters show
clear improvements in identity preservation, interaction quality, and
robustness to style delusion, enabling new forms of generative
storytelling.Additional results and videos are available on our project page:
https://tingtingliao.github.io/mimix/.

</details>


### [170] [VChain: Chain-of-Visual-Thought for Reasoning in Video Generation](https://arxiv.org/abs/2510.05094)
*Ziqi Huang,Ning Yu,Gordon Chen,Haonan Qiu,Paul Debevec,Ziwei Liu*

Main category: cs.CV

TL;DR: A sparse, inference-time chain-of-visual-thought (VChain) uses multimodal models to produce keyframe snapshots that guide sparse fine-tuning of a video generator, boosting multi-step dynamic coherence with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Video generation struggles to model complex dynamics and long-horizon state transitions; strong visual reasoning is demonstrated by large language and multimodal models, suggesting a way to bridge this gap.

Method: An inference-time pipeline where a multimodal model generates a sparse set of keyframes as snapshots, which are then used to guide sparse inference-time tuning of a pre-trained video generator at those moments, with minimal overhead and no dense supervision.

Result: Extensive experiments on complex, multi-step scenarios show notable improvements in the quality and coherence of generated videos, while remaining tuning-efficient.

Conclusion: VChain demonstrates that injecting visual reasoning signals from multimodal models into video generation via sparse, keyframe-guided fine-tuning at inference time can significantly enhance long-horizon dynamics with low overhead.

Abstract: Recent video generation models can produce smooth and visually appealing
clips, but they often struggle to synthesize complex dynamics with a coherent
chain of consequences. Accurately modeling visual outcomes and state
transitions over time remains a core challenge. In contrast, large language and
multimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and
future prediction capabilities. To bridge these strengths, we introduce VChain,
a novel inference-time chain-of-visual-thought framework that injects visual
reasoning signals from multimodal models into video generation. Specifically,
VChain contains a dedicated pipeline that leverages large multimodal models to
generate a sparse set of critical keyframes as snapshots, which are then used
to guide the sparse inference-time tuning of a pre-trained video generator only
at these key moments. Our approach is tuning-efficient, introduces minimal
overhead and avoids dense supervision. Extensive experiments on complex,
multi-step scenarios show that VChain significantly enhances the quality of
generated videos.

</details>


### [171] [Paper2Video: Automatic Video Generation from Scientific Papers](https://arxiv.org/abs/2510.05096)
*Zeyu Zhu,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: PaperTalker provides a benchmark of 101 papers paired with author-made presentation videos and a multi-agent system to automatically generate academic presentation videos, with new evaluation metrics and a released dataset/code; it improves fidelity and informativeness over baselines.


<details>
  <summary>Details</summary>
Motivation: To reduce the labor-intensive process of producing academic presentation videos and to handle multi-modal, multi-channel alignment (text, figures, slides, speech, subtitles, talking-head) in a scalable, automated way.

Method: 1) Build PaperTalker: a dataset of 101 papers paired with slides, author videos, and metadata; 2) Propose four evaluation metrics (Meta Similarity, PresentArena, PresentQuiz, IP Memory); 3) Develop PaperTalker, a multi-agent framework that jointly generates slides, refines layout via an effective tree search visual choice, performs cursor grounding, subtitling, speech synthesis, and talking-head rendering, with slide-wise generation parallelization.

Result: Experiments on Paper2Video show the generated presentation videos are more faithful and informative than baselines; the approach demonstrates feasibility of automated academic video generation. Dataset, agent, and code are released.

Conclusion: This work establishes a practical step toward automated and ready-to-use academic presentation video generation, providing baseline data, evaluation tools, and an end-to-end generator to streamline research communication.

Abstract: Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first multi-agent framework for academic presentation
video generation. It integrates slide generation with effective layout
refinement by a novel effective tree search visual choice, cursor grounding,
subtitling, speech synthesis, and talking-head rendering, while parallelizing
slide-wise generation for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [172] [PARS: Low-Latency LLM Serving via Pairwise Learning-to-Rank](https://arxiv.org/abs/2510.03243)
*Yiheng Tao,Yihe Zhang,Matthew T. Dearing,Xin Wang,Yuping Fan,Zhiling Lan*

Main category: cs.LG

TL;DR: PARS is a prompt-aware LLM task scheduler that uses pairwise ranking to approximate shortest-job-first, reducing latency with minimal overhead and integrating into vLLM; it generalizes across models.


<details>
  <summary>Details</summary>
Motivation: Mitigate head-of-line blocking and improve latency/throughput in LLM inference by prioritizing shorter or more impactful tasks.

Method: PARS learns a pairwise ranking with a margin loss to predict response-length-based task ordering (a proxy for SJF); it is prompt-aware and integrates into the vLLM serving system to decide scheduling.

Result: Significant latency reductions across multiple LLMs and real-world inference datasets; effective for reasoning workloads; cross-model generalization when predictors are trained on different LLMs.

Conclusion: PARS offers a lightweight, generalizable prompt-aware scheduling approach that substantially improves LLM serving efficiency.

Abstract: Efficient scheduling of LLM inference tasks is essential for achieving low
latency and high throughput, particularly with the growing use of
reasoning-capable LLMs. Traditional strategies like First-Come-First-Serve
(FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks
delay shorter ones queued behind them. In this paper, we introduce PARS, a
prompt-aware LLM task scheduler that improves serving efficiency by
approximating shortest-job-first (SJF) scheduling through pairwise ranking with
margin ranking loss. PARS focuses on impactful scheduling decisions and is
seamlessly integrated into the state-of-the-art LLM serving system vLLM. It
effectively predicts response-length-based task ordering, reducing latency with
minimal overhead. Extensive experiments across multiple LLMs and real-world
inference datasets show that PARS significantly improves performance, including
for reasoning workloads. Furthermore, our cross-model evaluations demonstrate
that the design generalizes well, enabling effective scheduling even when
predictors are trained on different LLMs.

</details>


### [173] [VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion](https://arxiv.org/abs/2510.03244)
*Yanlong Wang,Hang Yu,Jian Xu,Fei Ma,Hongkang Zhang,Tongtong Feng,Zijian Zhang,Shao-Lun Huang,Danny Dongning Sun,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: VIFO converts multivariate time series into images to leverage pre-trained vision models for cross-channel pattern extraction, then fuses these visual features with time-series representations, achieving competitive performance while freezing the LVM and training only a small fraction of its parameters.


<details>
  <summary>Details</summary>
Motivation: Channel-independent time-series models miss cross-channel dependencies and large vision models have not been fully exploited for spatiotemporal data; multimodal extraction could enhance forecasting by leveraging cross-modal information.

Method: Transform multivariate series into image representations; use a frozen pre-trained large vision model to extract cross-channel patterns; align and fuse the resulting visual features with time-series features; train only 7.45% of the LVM's parameters.

Result: Competitive performance on multiple benchmarks; efficient training; demonstrates effectiveness of cross-modal cross-variable relationship capture.

Conclusion: Cross-modal forecasting with image-rendered time series and frozen LVMs can capture cross-variable relationships more effectively than channel-independent approaches, offering an efficient path to leveraging LVMs in time-series forecasting.

Abstract: Large time series foundation models often adopt channel-independent
architectures to handle varying data dimensions, but this design ignores
crucial cross-channel dependencies. Concurrently, existing multimodal
approaches have not fully exploited the power of large vision models (LVMs) to
interpret spatiotemporal data. Additionally, there remains significant
unexplored potential in leveraging the advantages of information extraction
from different modalities to enhance time series forecasting performance. To
address these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO
uniquely renders multivariate time series into image, enabling pre-trained LVM
to extract complex cross-channel patterns that are invisible to
channel-independent models. These visual features are then aligned and fused
with representations from the time series modality. By freezing the LVM and
training only 7.45% of its parameters, VIFO achieves competitive performance on
multiple benchmarks, offering an efficient and effective solution for capturing
cross-variable relationships in

</details>


### [174] [TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration](https://arxiv.org/abs/2510.05102)
*Cheng Xin,Fan Xu,Xin Ding,Jie Gao,Jiaxin Ding*

Main category: cs.LG

TL;DR: TopInG introduces a topological learning framework for interpretable GNNs that identifies persistent rationale subgraphs using persistence-based filtration, with a self-adjusted topological constraint that enforces a clear distinction between rationale and non-rationale. It comes with theoretical guarantees and strong empirical results on accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Interpretability of GNNs remains challenging when rationale subgraphs are diverse. A topological approach using persistent homology can capture persistent, robust substructures that serve as reliable explanations and resist spurious correlations.

Method: TopInG employs a rationale filtration learning process to autoregressively generate rationale subgraphs and uses a self-adjusted topological constraint called topological discrepancy to enforce persistent topological distinctions between rationale and irrelevant subgraphs. The authors provide theoretical guarantees that the loss is uniquely optimized by the ground truth under certain conditions.

Result: Empirical evaluation shows improved predictive accuracy and interpretation quality over state-of-the-art methods, especially in handling variform rationale subgraphs and mitigating spurious correlations while balancing performance and interpretability.

Conclusion: TopInG offers a principled, topologically grounded framework for intrinsically interpretable GNNs that yields robust explanations with theoretical guarantees and practical performance gains.

Abstract: Graph Neural Networks (GNNs) have shown remarkable success across various
scientific fields, yet their adoption in critical decision-making is often
hindered by a lack of interpretability. Recently, intrinsically interpretable
GNNs have been studied to provide insights into model predictions by
identifying rationale substructures in graphs. However, existing methods face
challenges when the underlying rationale subgraphs are complex and varied. In
this work, we propose TopInG: Topologically Interpretable Graph Learning, a
novel topological framework that leverages persistent homology to identify
persistent rationale subgraphs. TopInG employs a rationale filtration learning
approach to model an autoregressive generation process of rationale subgraphs,
and introduces a self-adjusted topological constraint, termed topological
discrepancy, to enforce a persistent topological distinction between rationale
subgraphs and irrelevant counterparts. We provide theoretical guarantees that
our loss function is uniquely optimized by the ground truth under specific
conditions. Extensive experiments demonstrate TopInG's effectiveness in
tackling key challenges, such as handling variform rationale subgraphs,
balancing predictive performance with interpretability, and mitigating spurious
correlations. Results show that our approach improves upon state-of-the-art
methods on both predictive accuracy and interpretation quality.

</details>


### [175] [Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability](https://arxiv.org/abs/2510.03245)
*Ali Yavari,Alireza Mohamadi,Elham Beydaghi,Rainer A. Leitgeb*

Main category: cs.LG

TL;DR: Introduces transferable frequency-aware adversarial attacks and a frequency-aware attribution method FAMPE; achieves 13.02% higher insertion score than AttEXplore and analyzes high/low-frequency roles.


<details>
  <summary>Details</summary>
Motivation: Need for reliable DNNs under real-world noise and adversarial perturbations; current attribution methods are suboptimal and require refinement.

Method: Define transferable frequency-aware attacks enabling exploration across both high- and low-frequency components; develop Frequency-Aware Model Parameter Explorer (FAMPE) for improved explainability; conduct ablation studies on-frequency components.

Result: FAMPE outperforms AttEXplore with an average 13.02% gain in Insertion Score; demonstrates improved explainability; ablation confirms roles of high- and low-frequency components.

Conclusion: FAMPE enhances explainability of DNNs and shows the value of frequency-aware analysis; supports development of attribution methods beyond current state-of-the-art.

Abstract: Ensuring the reliability of deep neural networks (DNNs) in the presence of
real world noise and intentional perturbations remains a significant challenge.
To address this, attribution methods have been proposed, though their efficacy
remains suboptimal and necessitates further refinement. In this paper, we
propose a novel category of transferable adversarial attacks, called
transferable frequency-aware attacks, enabling frequency-aware exploration via
both high-and low-frequency components. Based on this type of attacks, we also
propose a novel attribution method, named Frequency-Aware Model Parameter
Explorer (FAMPE), which improves the explainability for DNNs. Relative to the
current state-of-the-art method AttEXplore, our FAMPE attains an average gain
of 13.02% in Insertion Score, thereby outperforming existing approaches.
Through detailed ablation studies, we also investigate the role of both high-
and low-frequency components in explainability.

</details>


### [176] [StructPrune: Structured Global Pruning asymptotics with $\mathcal{O}(\sqrt{N})$ GPU Memory](https://arxiv.org/abs/2510.03246)
*Xinyuan Song,Guangji Bai,Liang Zhao*

Main category: cs.LG

TL;DR: STRUPRUNE: an ADMM-based framework that enables structured pruning with local-memory efficiency, achieving global-structured-pruning performance while reducing memory from O(N) to O(sqrt(N)).


<details>
  <summary>Details</summary>
Motivation: Global pruning offers strong performance but needs O(N) memory, impractical for billion-parameter models; local pruning is memory-efficient but misses inter-layer dependencies and hurts high-sparsity performance; structured pruning is hardware-friendly but typically relies on global optimization and can degrade when optimized locally. There is a need for a memory-efficient, hardware-friendly, structure-preserving pruning method that respects inter-layer dependencies.

Method: Divide-and-conquer across modules to decompose the global pruning problem into coordinated subproblems that fit in limited GPU memory. Use an ADMM-based framework to integrate structured sparsity, delivering a closed-form solution for structured pruning masks (layer-wise sparsity allocation) and an energy-based asymptotic framework with a softmax allocation that adapts to heterogeneous layer importance.

Result: STRUPRUNE achieves perplexity comparable to global structured pruning while reducing memory cost from O(N) to O(sqrt(N)). This enables practical deployment at the billion-parameter scale; the energy-based allocation provides a simple, adaptive layer-sparsity scheme.

Conclusion: The proposed divide-and-conquer, ADMM-based STRUPRUNE delivers hardware-friendly, structured pruning that preserves inter-layer dependencies and scales to billion-parameter models, offering strong performance with significantly reduced memory usage.

Abstract: Pruning is critical for scaling large language models (LLMs). Global pruning
achieves strong performance but requires $\mathcal{O}(N)$ memory, which is
infeasible for billion-parameter models. Local pruning reduces GPU memory usage
to that of a single layer by pruning layers independently, but it neglects
inter-layer dependencies and often leads to suboptimal performance in
high-sparsity regimes. Unlike unstructured pruning, structured pruning produces
regular sparsity patterns that align well with GPU kernels and library
optimizations, making it more hardware-efficient. However, structured pruning
typically relies on global pruning, since structured patterns are more prone to
severe performance degradation under local optimization. To jointly achieve
structured pruning and the memory efficiency of local pruning, we propose a
divide-and-conquer strategy that decomposes the global pruning problem into
coordinated subproblems across different modules, each of which fits within
limited GPU memory. Building on this idea, we design \textbf{STRUPRUNE}, an
ADMM-based framework that integrates structured sparsity into the pruning
process, combining the memory efficiency of local pruning with the hardware
compatibility of structured methods. We derive a closed-form analytical
solution for structured pruning masks that provides an explicit rule for
layer-wise sparsity allocation, and further develop an energy-based asymptotic
framework yielding a softmax-form allocation scheme that simplifies
optimization while adapting to heterogeneous layer importance. Experiments
demonstrate that STRUPRUNE matches the perplexity of global structured pruning
while reducing memory cost from $\mathcal{O}(N)$ to $\mathcal{O}(\sqrt{N})$,
enabling practical deployment at the billion-parameter scale.

</details>


### [177] [Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data](https://arxiv.org/abs/2510.03247)
*Jiancheng Zhang,Yinglun Zhu*

Main category: cs.LG

TL;DR: Multimodal active learning for unaligned data; actively acquire cross-modal alignments to reduce annotation cost by up to 40% (e.g., ColorSwap) while maintaining accuracy, using a linear-time, modality-aware uncertainty-diversity algorithm applicable to pool-based and streaming settings.


<details>
  <summary>Details</summary>
Motivation: Annotation bottleneck in multimodal learning due to cross-modal alignment cost; existing AL focuses on unimodal data and ignores alignment cost in pipelines like CLIP/SigLIP.

Method: A modality-aware algorithm that combines uncertainty and diversity to select cross-modal alignments for acquisition; supports pool-based and streaming AL; achieves linear-time acquisition; targets unaligned data rather than labeling pre-aligned pairs.

Result: Consistent reduction in multimodal annotation cost across benchmarks, with up to 40% reduction on ColorSwap, while preserving accuracy.

Conclusion: Presents the first framework for multimodal AL with unaligned data, enabling efficient cross-modal alignment acquisition in practical pipelines and adaptable to various AL settings.

Abstract: Active learning (AL) is a principled strategy to reduce annotation cost in
data-hungry deep learning. However, existing AL algorithms focus almost
exclusively on unimodal data, overlooking the substantial annotation burden in
multimodal learning. We introduce the first framework for multimodal active
learning with unaligned data, where the learner must actively acquire
cross-modal alignments rather than labels on pre-aligned pairs. This setting
captures the practical bottleneck in modern multimodal pipelines such as CLIP
and SigLIP, where unimodal features are easy to obtain but high-quality
alignment is costly. We develop a new algorithm that combines uncertainty and
diversity principles in a modality-aware design, achieves linear-time
acquisition, and applies seamlessly to both pool-based and streaming-based
settings. Extensive experiments on benchmark datasets demonstrate that our
approach consistently reduces multimodal annotation cost while preserving
performance; for instance, on the ColorSwap dataset it cuts annotation
requirements by up to $40\%$ without loss in accuracy.

</details>


### [178] [Real-Time Brain Biomechanics Prediction with Neural Operators: Toward Clinically Deployable Traumatic Brain Injury Models](https://arxiv.org/abs/2510.03248)
*Anusha Agarwal,Dibakar Roy Sarkar,Somdatta Goswami*

Main category: cs.LG

TL;DR: Neural operators enable real-time, patient-specific brain displacement predictions from MRI/MRE data, outperforming baselines; MG-FNO best accuracy; DeepONet fastest inference.


<details>
  <summary>Details</summary>
Motivation: Traumatic brain injury is highly prevalent; high-fidelity finite element models are accurate but computationally expensive, limiting clinical utility. Neural operators promise rapid, patient-specific predictions to enable real-time decision-making and digital twins.

Method: Compared four neural-operator architectures (Fourier Neural Operator, F-FNO, Multi-Grid FNO, DeepONet) trained on 249 magnetic resonance elastography datasets across 20–90 Hz. Inputs: subject-specific anatomical MRI, MRE stiffness maps, and demographic features. Output: full-field 3D brain displacement. Evaluated using mean-squared error and reported spatial fidelity; assessed computational speed (inference time or iterations per second).

Result: MG-FNO achieved highest accuracy (MSE = 0.0023) with 94.3% spatial fidelity and preserved fine-scale features. F-FNO converged ~2× faster than standard FNO. DeepONet offered the fastest inference (14.5 iterations/s) with ~7× speed-up over MG-FNO. All NOs reduced computation from hours to milliseconds while maintaining anatomical realism.

Conclusion: Neural-operator-based models enable real-time, patient-specific TBI risk assessment, clinical triage support, and optimization of protective equipment, by delivering scalable, resolution-invariant brain deformation predictions and enabling digital twins for clinical and population health applications.

Abstract: Traumatic brain injury (TBI) remains a major public health concern, with over
69 million cases annually worldwide. Finite element (FE) models offer
high-fidelity predictions of brain deformation but are computationally
expensive, requiring hours per simulation and limiting their clinical utility
for rapid decision-making. This study benchmarks state-of-the-art neural
operator (NO) architectures for rapid, patient-specific prediction of brain
displacement fields, aiming to enable real-time TBI modeling in clinical and
translational settings. We formulated TBI modeling as an operator learning
problem, mapping subject-specific anatomical MRI, magnetic resonance
elastography (MRE) stiffness maps, and demographic features to full-field 3D
brain displacement predictions. Four architectures - Fourier Neural Operator
(FNO), Factorized FNO (F-FNO), Multi-Grid FNO (MG-FNO), and Deep Operator
Network (DeepONet) were trained and evaluated on 249 MRE datasets across
physiologically relevant frequencies (20 - 90 Hz). MG-FNO achieved the highest
accuracy (MSE = 0.0023, 94.3\% spatial fidelity) and preserved fine-scale
features, while F-FNO converged 2$\times$ faster than standard FNO. DeepONet
offered the fastest inference (14.5 iterations/s) with a 7$\times$
computational speed-up over MG-FNO, suggesting utility for embedded or edge
computing applications. All NOs reduced computation time from hours to
milliseconds without sacrificing anatomical realism. NOs provide an efficient,
resolution-invariant approach for predicting brain deformation, opening the
door to real-time, patient-specific TBI risk assessment, clinical triage
support, and optimization of protective equipment. These results highlight the
potential for NO-based digital twins of the human brain, enabling scalable,
on-demand biomechanical modeling in both clinical and population health
contexts.

</details>


### [179] [Light Differentiable Logic Gate Networks](https://arxiv.org/abs/2510.03250)
*Lukas Rüttgers,Till Aczel,Andreas Plesner,Roger Wattenhofer*

Main category: cs.LG

TL;DR: A reparametrization of logic gate neurons in Differentiable Logic Gate Networks reduces parameter size logarithmically with gate input count, boosting training speed and efficiency while maintaining or improving CIFAR-100 accuracy, enabling deeper networks.


<details>
  <summary>Details</summary>
Motivation: DLGNs offer fast inference but suffer during training due to vanishing gradients, discretization errors, and high computational cost. The root cause is the underlying parametrization of logic gate neurons, which hinders scaling with depth.

Method: Introduce a reparametrization that shrinks the parameter size logarithmically in the number of inputs per gate. For binary inputs, this yields a 4x reduction in model size, up to a 1.86x speedup in the backward pass, and 8.5x fewer training steps, while maintaining stable CIFAR-100 accuracy.

Result: Empirical evidence shows substantial efficiency gains (smaller models, faster training) with stable or improved CIFAR-100 accuracy compared to the original parametrization.

Conclusion: The new parametrization addresses the root cause of training inefficiencies in DLGNs, enabling deeper networks with improved efficiency and robust accuracy, suggesting broad applicability to logic-based neural architectures.

Abstract: Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency
at inference while sustaining competitive accuracy. But vanishing gradients,
discretization errors, and high training cost impede scaling these networks.
Even with dedicated parameter initialization schemes from subsequent works,
increasing depth still harms accuracy. We show that the root cause of these
issues lies in the underlying parametrization of logic gate neurons themselves.
To overcome this issue, we propose a reparametrization that also shrinks the
parameter size logarithmically in the number of inputs per gate. For binary
inputs, this already reduces the model size by 4x, speeds up the backward pass
by up to 1.86x, and converges in 8.5x fewer training steps. On top of that, we
show that the accuracy on CIFAR-100 remains stable and sometimes superior to
the original parametrization.

</details>


### [180] [Numerion: A Multi-Hypercomplex Model for Time Series Forecasting](https://arxiv.org/abs/2510.03251)
*Hanzhong Cao,Wenbo Yan,Ying Tan*

Main category: cs.LG

TL;DR: Numerion introduces a hypercomplex multi-space MLP framework for time series forecasting, using multiple RHR-MLPs across dimensioned hypercomplex spaces to decompose and model data, guided by the observation that higher-order hypercomplex spaces emphasize lower-frequency components, achieving state-of-the-art results with a dynamic fusion mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing time series forecasting methods face computational and robustness constraints due to intricate decompositions and strong assumptions. The authors leverage theoretical insights that frequencies naturally shrink in complex and higher-order hypercomplex spaces to design a scalable, multi-space architecture.

Method: Generalize linear layers and activations to hypercomplex spaces with power-of-two dimensions. Build Real-Hypercomplex-Real Domain MLP (RHR-MLP). Use multiple RHR-MLPs to map time series into hypercomplex spaces of varying dimensions, enabling decomposition and independent modeling. Employ a dynamic fusion mechanism to adaptively combine latent patterns from different spaces.

Result: Empirical evaluation shows state-of-the-art performance on multiple public datasets. Visualizations and analyses demonstrate that multi-dimensional RHR-MLPs naturally decompose time series and that higher-dimensional hypercomplex spaces tend to capture lower-frequency features.

Conclusion: The framework provides an effective and scalable approach to time series forecasting by leveraging hypercomplex spaces for natural decomposition and frequency-aware modeling, with dynamic fusion enabling robust integration of multi-space representations.

Abstract: Many methods aim to enhance time series forecasting by decomposing the series
through intricate model structures and prior knowledge, yet they are inevitably
limited by computational complexity and the robustness of the assumptions. Our
research uncovers that in the complex domain and higher-order hypercomplex
spaces, the characteristic frequencies of time series naturally decrease.
Leveraging this insight, we propose Numerion, a time series forecasting model
based on multiple hypercomplex spaces. Specifically, grounded in theoretical
support, we generalize linear layers and activation functions to hypercomplex
spaces of arbitrary power-of-two dimensions and introduce a novel
Real-Hypercomplex-Real Domain Multi-Layer Perceptron (RHR-MLP) architecture.
Numerion utilizes multiple RHR-MLPs to map time series into hypercomplex spaces
of varying dimensions, naturally decomposing and independently modeling the
series, and adaptively fuses the latent patterns exhibited in different spaces
through a dynamic fusion mechanism. Experiments validate the model`s
performance, achieving state-of-the-art results on multiple public datasets.
Visualizations and quantitative analyses comprehensively demonstrate the
ability of multi-dimensional RHR-MLPs to naturally decompose time series and
reveal the tendency of higher dimensional hypercomplex spaces to capture lower
frequency features.

</details>


### [181] [Universal Multi-Domain Translation via Diffusion Routers](https://arxiv.org/abs/2510.03252)
*Duc Kieu,Kien Do,Tuan Hoang,Thao Minh Le,Tung Kieu,Dang Nguyen,Thin Nguyen*

Main category: cs.LG

TL;DR: A universal multi-domain translation framework (UMDT) using Diffusion Router (DR) enables translation between any pair of K domains with only K-1 central-domain datasets. DR uses a single noise predictor conditioned on source/target domain labels to model central<->non-central translations, supports indirect routing via the central domain and direct non-central mappings via a variational bound objective and Tweedie refinement. It achieves state-of-the-art results on three benchmarks, reduces sampling cost, and enables new tasks like sketch<->segmentation.


<details>
  <summary>Details</summary>
Motivation: Current MDT methods either require fully aligned data or are limited to domain pairs seen during training, which hampers scalability and practical cross-domain mapping. A universal translator that works across arbitrary domain pairs with limited paired data is highly desirable for broad applicability and efficiency.

Method: Introduce Diffusion Router (DR), a unified diffusion-based framework that handles all central↔non-central translations with one noise predictor conditioned on the source and target domain labels. Indirect non-central translations are achieved by routing through a central domain. A scalable learning strategy is proposed using a variational-bound objective and an efficient Tweedie refinement procedure to support direct non-central mappings.

Result: Empirical evaluation on three large-scale UMDT benchmarks shows state-of-the-art performance for both indirect and direct translations, with reduced sampling cost. DR also enables novel tasks such as sketch↔segmentation.

Conclusion: DR provides a scalable and versatile framework for universal translation across multiple domains, enabling flexible cross-domain mappings with limited paired data.

Abstract: Multi-domain translation (MDT) aims to learn translations between multiple
domains, yet existing approaches either require fully aligned tuples or can
only handle domain pairs seen in training, limiting their practicality and
excluding many cross-domain mappings. We introduce universal MDT (UMDT), a
generalization of MDT that seeks to translate between any pair of $K$ domains
using only $K-1$ paired datasets with a central domain. To tackle this problem,
we propose Diffusion Router (DR), a unified diffusion-based framework that
models all central$\leftrightarrow$non-central translations with a single noise
predictor conditioned on the source and target domain labels. DR enables
indirect non-central translations by routing through the central domain. We
further introduce a novel scalable learning strategy with a variational-bound
objective and an efficient Tweedie refinement procedure to support direct
non-central mappings. Through evaluation on three large-scale UMDT benchmarks,
DR achieves state-of-the-art results for both indirect and direct translations,
while lowering sampling cost and unlocking novel tasks such as
sketch$\leftrightarrow$segmentation. These results establish DR as a scalable
and versatile framework for universal translation across multiple domains.

</details>


### [182] [Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents](https://arxiv.org/abs/2510.03253)
*Heyang Gao,Zexu Sun,Erxue Min,Hengyi Cai,Shuaiqiang Wang,Dawei Yin,Xu Chen*

Main category: cs.LG

TL;DR: A hierarchical preference learning (HPL) framework for aligning LLM agents uses multi-granularity DPO signals—trajectory, step, and group level—with a dual-layer curriculum to improve credit assignment and policy stability across simple to complex tasks, outperforming state-of-the-art baselines on three benchmarks.


<details>
  <summary>Details</summary>
Motivation: Preference-based offline alignment methods (e.g., DPO) suffer from a granularity mismatch: trajectory-level signals are too coarse for fine credit assignment, while step-level signals are overly myopic for long-horizon, multi-step behaviors. A framework that integrates preferences across multiple temporal granularities is needed to reliably train agents on complex tasks.

Method: Decompose expert trajectories into semantically coherent action groups and create contrasting suboptimal groups to enable fine-grained, sub-task level preference learning. Combine trajectory- and step-level DPO losses with a group-level preference optimization driven by a dual-layer curriculum. The curriculum organizes learning from simple to complex along two axes: group length (sub-task complexity) and sample difficulty (reward gap between preferred and dispreferred groups).

Result: Empirical evaluation on three challenging agent benchmarks shows HPL outperforms existing state-of-the-art methods. Analyses indicate the hierarchical DPO loss effectively integrates signals across granularities and that the dual-layer curriculum is crucial for enabling performance across simple and multi-step tasks.

Conclusion: Hierarchical DPO with a two-axis curriculum enables robust, multi-granularity preference learning, equipping LLM agents to handle a spectrum of tasks from straightforward behaviors to long, multi-step sequences.

Abstract: Large Language Models (LLMs) as autonomous agents are increasingly tasked
with solving complex, long-horizon problems. Aligning these agents via
preference-based offline methods like Direct Preference Optimization (DPO) is a
promising direction, yet it faces a critical granularity mismatch.
Trajectory-level DPO provides a signal that is too coarse for precise credit
assignment, while step-level DPO is often too myopic to capture the value of
multi-step behaviors. To resolve this challenge, we introduce Hierarchical
Preference Learning (HPL), a hierarchical framework that optimizes LLM agents
by leveraging preference signals at multiple, synergistic granularities. While
HPL incorporates trajectory- and step-level DPO for global and local policy
stability, its core innovation lies in group-level preference optimization
guided by a dual-layer curriculum. Our approach first decomposes expert
trajectories into semantically coherent action groups and then generates
contrasting suboptimal groups to enable preference learning at a fine-grained,
sub-task level. Then, instead of treating all preference pairs equally, HPL
introduces a curriculum scheduler that organizes the learning process from
simple to complex. This curriculum is structured along two axes: the group
length, representing sub-task complexity, and the sample difficulty, defined by
the reward gap between preferred and dispreferred action groups. Experiments on
three challenging agent benchmarks show that HPL outperforms existing
state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO
loss effectively integrates preference signals across multiple granularities,
while the dual-layer curriculum is crucial for enabling the agent to solve a
wide range of tasks, from simple behaviors to complex multi-step sequences.

</details>


### [183] [Adversarial training with restricted data manipulation](https://arxiv.org/abs/2510.03254)
*David Benfield,Stefano Coniglio,Phan Tu Vuong,Alain Zemkoho*

Main category: cs.LG

TL;DR: A constrained pessimistic bilevel optimization framework for adversarial ML improves average robustness by restricting the adversary to realistic moves, outperforming unrestricted pessimistic approaches in experiments.


<details>
  <summary>Details</summary>
Motivation: Adversarial machine learning in security-sensitive domains (spam filtering, malware detection, fake image generation) requires robust models. Unrestricted pessimistic models risk unrealistic adversaries, harming real-world performance.

Method: Introduce a constrained pessimistic bilevel optimization model that limits the adversary's feasible actions, thereby producing more realistic worst-case scenarios and a robust classifier; empirical evaluation against existing pessimistic bilevel methods.

Result: The constrained approach yields better average performance than the existing unrestricted pessimistic method in experiments.

Conclusion: Constraining the adversary to realistic behavior leads to more reliable robustness and improved real-world performance; future work could refine constraint definitions and scalability.

Abstract: Adversarial machine learning concerns situations in which learners face
attacks from active adversaries. Such scenarios arise in applications such as
spam email filtering, malware detection and fake image generation, where
security methods must be actively updated to keep up with the everimproving
generation of malicious data. Pessimistic Bilevel optimisation has been shown
to be an effective method of training resilient classifiers against such
adversaries. By modelling these scenarios as a game between the learner and the
adversary, we anticipate how the adversary will modify their data and then
train a resilient classifier accordingly. However, since existing pessimistic
bilevel approaches feature an unrestricted adversary, the model is vulnerable
to becoming overly pessimistic and unrealistic. When finding the optimal
solution that defeats the classifier, it is possible that the adversary's data
becomes nonsensical and loses its intended nature. Such an adversary will not
properly reflect reality, and consequently, will lead to poor classifier
performance when implemented on real-world data. By constructing a constrained
pessimistic bilevel optimisation model, we restrict the adversary's movements
and identify a solution that better reflects reality. We demonstrate through
experiments that this model performs, on average, better than the existing
approach.

</details>


### [184] [SciTS: Scientific Time Series Understanding and Generation with LLMs](https://arxiv.org/abs/2510.03255)
*Wen Wu,Ziyang Zhang,Liwei Liu,Xuenan Xu,Junlin Liu,Ke Fan,Qitan Lv,Jimin Zhuang,Chen Zhang,Zheqi Yuan,Siyuan Hou,Tianyi Lin,Kai Chen,Bowen Zhou,Chao Zhang*

Main category: cs.LG

TL;DR: SciTS introduces a large, diverse benchmark for scientific time series (12 domains, 43 tasks, >50k instances) and TimeOmni, a framework enabling LLMs to understand and generate time series while staying compatible with general-purpose training. Key finding: general-purpose LLMs generalize better than specialized time-series models; rendering time series as text/images hurts precision.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLMs either convert time series to text or images, losing numerical precision, or rely on specialized time-series models that struggle with non-periodic, heterogeneous signals. There is a lack of comprehensive benchmarks and flexible frameworks to evaluate and enable LLMs on scientific time series across diverse domains.

Method: Constructed SciTS benchmark: 12 scientific domains, 43 tasks, >50k instances, including univariate and multivariate series with lengths 1 to 10^7 and frequencies up to 10 MHz. Benchmarked 17 models (text-only LLMs, multimodal LLMs, and unified time-series models). Introduced TimeOmni, a framework to enable LLMs to understand/generate time series while remaining compatible with standard LLM training.

Result: General-purpose LLMs demonstrated stronger generalisability than specialized time-series models. Representing time series as text or images hampers performance due to excessively long sequences and loss of numerical precision. TimeOmni provides a pathway to integrate time-series understanding/generation into LLMs without sacrificing compatibility with existing training.

Conclusion: This work fills gaps in dedicated benchmarks and modeling frameworks for scientific time series, enabling LLMs to understand and generate complex temporal scientific data across diverse domains.

Abstract: The scientific reasoning ability of large language models (LLMs) has recently
attracted significant attention. Time series, as a fundamental modality in
scientific data, presents unique challenges that are often overlooked in
current multimodal LLMs, which either encode numerical sequences as text or
convert them into images. Such approaches may be insufficient for comprehensive
scientific time series understanding and generation. Existing unified time
series models typically specialise in either forecasting or analysis, and their
effectiveness on non-periodic, heterogeneous scientific signals remains
unclear. To address these gaps, we introduce SciTS, a benchmark spanning 12
scientific domains and 43 tasks, with over 50k+ instances, both univariate and
multivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz
in frequency. We benchmark 17 models, including text-only LLMs, multimodal
LLMs, and unified time series models, and find that general-purpose LLMs
exhibit stronger generalisability than specialised time series models, while
representing time series as text or images limits their performance due to
excessively long sequences and loss of numerical precision, respectively. We
then introduce TimeOmni, a framework that equips LLMs with the ability to
understand and generate time series while remaining compatible with
general-purpose LLM training. This work fills a gap in both dedicated
benchmarks and modelling frameworks for scientific time series, paving the way
for LLMs to understand and generate complex temporal scientific data.

</details>


### [185] [Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?](https://arxiv.org/abs/2510.03257)
*Zijian Zhao,Sen Li*

Main category: cs.LG

TL;DR: Triple-BERT is a centralized single-agent RL method for large-scale ride-sharing order dispatching that uses action decomposition and a BERT-based encoder with parameter reuse to handle massive action and observation spaces; it achieves ~12% improvement over SOTA on Manhattan data, with more served orders and shorter pickup times.


<details>
  <summary>Details</summary>
Motivation: Ride-sharing platforms face real-time, complex bundling and matching under uncertainty across many drivers and orders. Independent MARL lacks global coordination, while CTDE MARL suffers from the curse of dimensionality. A scalable, centralized approach is needed to capture global information and relationships.

Method: A variant of TD3 forming Triple-BERT, addressing the large action space via action decomposition that factorizes the joint action into per-driver actions. Uses a novel BERT-based network with parameter reuse to manage growing numbers of drivers/orders, and an attention mechanism to model relationships among drivers and orders. Treats the problem as a centralized single-agent RL framework inspired by CTDE, validated on real-world Manhattan ride-hailing data. Code, parameters, and data are publicly available.

Result: About 11.95% improvement over current state-of-the-art methods, with a 4.26% increase in served orders and a 22.25% reduction in pickup times on a Manhattan dataset.

Conclusion: The combination of action decomposition and a scalable BERT-based observation encoder yields effective handling of large-scale dispatch problems, offering a practical, open-access solution for real-world ride-sharing platforms and a blueprint for scalable MARL in similar domains.

Abstract: On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate
real-time challenge of bundling and matching passengers-each with distinct
origins and destinations-to available vehicles, all while navigating
significant system uncertainties. Due to the extensive observation space
arising from the large number of drivers and orders, order dispatching, though
fundamentally a centralized task, is often addressed using Multi-Agent
Reinforcement Learning (MARL). However, independent MARL methods fail to
capture global information and exhibit poor cooperation among workers, while
Centralized Training Decentralized Execution (CTDE) MARL methods suffer from
the curse of dimensionality. To overcome these challenges, we propose
Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method
designed specifically for large-scale order dispatching on ride-sharing
platforms. Built on a variant TD3, our approach addresses the vast action space
through an action decomposition strategy that breaks down the joint action
probability into individual driver action probabilities. To handle the
extensive observation space, we introduce a novel BERT-based network, where
parameter reuse mitigates parameter growth as the number of drivers and orders
increases, and the attention mechanism effectively captures the complex
relationships among the large pool of driver and orders. We validate our method
using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves
approximately an 11.95% improvement over current state-of-the-art methods, with
a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our
code, trained model parameters, and processed data are publicly available at
the repository https://github.com/RS2002/Triple-BERT .

</details>


### [186] [POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation](https://arxiv.org/abs/2510.03258)
*Chang'an Yi,Xiaohui Deng,Shuaicheng Niu,Yan Zhou*

Main category: cs.LG

TL;DR: POEM is a general test-time adaptation (TTA) framework that enhances adaptation by exploiting previously unexplored reliable samples beyond a fixed entropy threshold and adds an Adapt Branch to balance domain-agnostic representation with target performance. It consistently improves accuracy across architectures and real-world domain shifts while remaining computationally efficient, and can augment existing TTA methods. Code is released.


<details>
  <summary>Details</summary>
Motivation: Entropy-based confidence thresholds in TTA can be brittle: samples that are actually reliable may be ignored if their entropy is just above the threshold, and samples may become reliable only as models update. There is a need to leverage time-evolving reliability signals to utilize more informative samples and stabilize adaptation.

Method: Introduce POEM, a framework that explores previously unexplored reliable samples during test-time by tracking sample reliability over adaptation steps instead of relying on a fixed entropy threshold. It reuses samples whose reliability emerges across updates and couples this with an additional Adapt Branch network that promotes domain-agnostic representations while maintaining high target performance. The approach is compatible with existing TTA methods and is validated via experiments and ablations across architectures and datasets.

Result: POEM consistently outperforms existing TTA methods across challenging scenarios and real-world domain shifts, with improved accuracy and robustness. It remains computationally efficient, and extensive ablations and analyses validate the contributions. The core idea can also augment other TTA approaches.

Conclusion: Exploiting time-evolving reliable samples in conjunction with an Adapt Branch yields robust, efficient TTA gains and can generalize as an augmentation to existing TTA methods, offering a practical and effective solution for deployment under distribution shifts.

Abstract: Test-time adaptation (TTA) aims to transfer knowledge from a source model to
unknown test data with potential distribution shifts in an online manner. Many
existing TTA methods rely on entropy as a confidence metric to optimize the
model. However, these approaches are sensitive to the predefined entropy
threshold, influencing which samples are chosen for model adaptation.
Consequently, potentially reliable target samples are often overlooked and
underutilized. For instance, a sample's entropy might slightly exceed the
threshold initially, but fall below it after the model is updated. Such samples
can provide stable supervised information and offer a normal range of gradients
to guide model adaptation. In this paper, we propose a general approach,
\underline{POEM}, to promote TTA via ex\underline{\textbf{p}}loring the
previously unexpl\underline{\textbf{o}}red reliabl\underline{\textbf{e}}
sa\underline{\textbf{m}}ples. Additionally, we introduce an extra Adapt Branch
network to strike a balance between extracting domain-agnostic representations
and achieving high performance on target data. Comprehensive experiments across
multiple architectures demonstrate that POEM consistently outperforms existing
TTA methods in both challenging scenarios and real-world domain shifts, while
remaining computationally efficient. The effectiveness of POEM is evaluated
through extensive analyses and thorough ablation studies. Moreover, the core
idea behind POEM can be employed as an augmentation strategy to boost the
performance of existing TTA approaches. The source code is publicly available
at \emph{https://github.com/ycarobot/POEM}

</details>


### [187] [Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning](https://arxiv.org/abs/2510.03259)
*Yoonjeon Kim,Doohyuk Jang,Eunho Yang*

Main category: cs.LG

TL;DR: MASA (Meta-Awareness via Self-Alignment) trains models to align their meta-predictions with actual rollouts, improving reasoning accuracy and efficiency without external data by using self-generated signals, prompt filtering, and rollout truncation. It reports notable in-domain gains and stronger out-of-domain generalization.


<details>
  <summary>Details</summary>
Motivation: Current reasoning models show weak meta-awareness, leading to misalignment between predicted meta-information and actual reasoning progress. Aligning meta-prediction with true rollouts could improve decision quality and learning efficiency. The approach aims to be self-contained (no external training data) and to enhance efficiency via filtering trivial/unsolvable prompts and early stopping.

Method: Introduce MASA: a training pipeline that uses self-generated signals to train meta-awareness by aligning meta-predictions with actual rollouts. It filters zero-variance prompts (trivial/unsolvable) and cuts off long rollouts unlikely to yield correct answers, enabling efficient training and better generalization.

Result: Significant improvements: speed up GRPO training by >1.28x; 19.3% accuracy gain on AIME25; 6.2% average gain across six math benchmarks; +3.87% GPQA-Diamond; +2.08% overall across 13 benchmarks (logical, scientific, coding).

Conclusion: Enhancing meta-awareness through self-alignment improves both accuracy and efficiency, with evidence of in-domain gains and some out-of-domain generalization. The approach avoids external data but relies on self-generated signals; effectiveness may depend on the quality of those signals and the balance of filtering/early stopping.

Abstract: Recent studies on reasoning models explore the meta-awareness of language
models, the ability to know how to think by itself. We argue that large
reasoning models lack this meta-awareness property by proving severe
misalignment between true rollouts and predicted meta information. We posit
that aligning meta-prediction with true rollouts will lead to significant
performance gains. To verify this hypothesis, we design a training pipeline
that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced
meta-awareness directly translates to improved accuracy. Unlike existing
meta-cognitive reasoning models, our method does not require external training
sources but leverages self-generated signals to train meta-awareness. Moreover,
our method enables efficient training by i) filtering out zero-variance prompts
that are either trivial or unsolvable and ii) cutting off lengthy rollouts when
they are unlikely to lead to correct answers. The results are inspiring: our
strategy yields significant improvements in both accuracy and training
efficiency on in-domain tasks and shows strong generalization to out-of-domain
benchmarks. More specifically, our method can speed up GRPO training by over
1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on
AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with
meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 %
boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks
spanning logical, scientific, and coding domains.

</details>


### [188] [Semantic-Inductive Attribute Selection for Zero-Shot Learning](https://arxiv.org/abs/2510.03260)
*Juan Jose Herrera-Aranda,Guillermo Gomez-Trenado,Francisco Herrera,Isaac Triguero*

Main category: cs.LG

TL;DR: Introduces a partitioning scheme to simulate unseen conditions in inductive ZSL and compares two feature-selection strategies (embedded ZSL-specific feature selection and a genetic algorithm) to prune semantic spaces, yielding improved unseen-class accuracy across five benchmarks; GA and RFS offer complementary trade-offs and confirm redundancy in semantic spaces.


<details>
  <summary>Details</summary>
Motivation: In ZSL, semantic spaces often include noisy or irrelevant attributes; in open-world inductive settings, unseen-class information is unavailable, making attribute relevance assessment challenging. A partitioning scheme enables evaluation of attribute usefulness without unseen semantic data.

Method: Propose partitioning scheme to mimic unseen conditions in inductive ZSL; study two complementary methods: (1) embedded feature selection adapted for ZSL to prune attributes; (2) genetic algorithm exploring attribute subsets broadly; evaluate generalization to unseen classes; benchmark on AWA2, CUB, SUN, aPY, FLO.

Result: Both methods reduce attribute redundancy and improve unseen-class accuracy; RFS is efficient but sensitive to hyperparameters; GA is more computationally intensive but explores space more broadly and reduces dependence on hyperparameters.

Conclusion: Semantic spaces are inherently redundant; the partitioning scheme is effective for refining them under inductive conditions, and the complementary methods provide practical tools for semantic pruning in ZSL.

Abstract: Zero-Shot Learning is an important paradigm within General-Purpose Artificial
Intelligence Systems, particularly in those that operate in open-world
scenarios where systems must adapt to new tasks dynamically. Semantic spaces
play a pivotal role as they bridge seen and unseen classes, but whether
human-annotated or generated by a machine learning model, they often contain
noisy, redundant, or irrelevant attributes that hinder performance. To address
this, we introduce a partitioning scheme that simulates unseen conditions in an
inductive setting (which is the most challenging), allowing attribute relevance
to be assessed without access to semantic information from unseen classes.
Within this framework, we study two complementary feature-selection strategies
and assess their generalisation. The first adapts embedded feature selection to
the particular demands of ZSL, turning model-driven rankings into meaningful
semantic pruning; the second leverages evolutionary computation to directly
explore the space of attribute subsets more broadly. Experiments on five
benchmark datasets (AWA2, CUB, SUN, aPY, FLO) show that both methods
consistently improve accuracy on unseen classes by reducing redundancy, but in
complementary ways: RFS is efficient and competitive though dependent on
critical hyperparameters, whereas GA is more costly yet explores the search
space more broadly and avoids such dependence. These results confirm that
semantic spaces are inherently redundant and highlight the proposed
partitioning scheme as an effective tool to refine them under inductive
conditions.

</details>


### [189] [Data-Driven Temperature Modelling of Machine Tools by Neural Networks: A Benchmark](https://arxiv.org/abs/2510.03261)
*C. Coelho,M. Hohmann,D. Fernández,L. Penter,S. Ihlenfeldt,O. Niggemann*

Main category: cs.LG

TL;DR: NNs predict high-fidelity temperature and heat flux fields using FE data; modular downstream components enable versatile thermal error correction with reduced hardware via informative sensor point selection; benchmarking across time-series architectures for specialized and general models; results show accurate, low-cost predictions and groundwork for generalizable corrections.


<details>
  <summary>Details</summary>
Motivation: Thermal errors harm machining precision; current corrections are limited to specific error types/locations and lack generality; need a flexible, data-driven approach that can support multiple error types through modular downstream components.

Method: Train neural networks to predict temperature and heat flux fields from varying initial conditions using finite element method data; use correlation-based sensor point selection to minimize hardware; evaluate time-series architectures (RNN, GRU, LSTM, BiLSTM, Transformer, TCN); contrast specialized models (for fixed initial conditions) with general models (extrapolate to unseen scenarios).

Result: Accurate and low-cost prediction of temperature and heat flux fields; enables flexible, generalisable thermal error correction in machine tool environments.

Conclusion: This framework enables modular, adaptable thermal error correction, reducing hardware needs and improving generalization across error types and locations in machine tools.

Abstract: Thermal errors in machine tools significantly impact machining precision and
productivity. Traditional thermal error correction/compensation methods rely on
measured temperature-deformation fields or on transfer functions. Most existing
data-driven compensation strategies employ neural networks (NNs) to directly
predict thermal errors or specific compensation values. While effective, these
approaches are tightly bound to particular error types, spatial locations, or
machine configurations, limiting their generality and adaptability. In this
work, we introduce a novel paradigm in which NNs are trained to predict
high-fidelity temperature and heat flux fields within the machine tool. The
proposed framework enables subsequent computation and correction of a wide
range of error types using modular, swappable downstream components. The NN is
trained using data obtained with the finite element method under varying
initial conditions and incorporates a correlation-based selection strategy that
identifies the most informative measurement points, minimising hardware
requirements during inference. We further benchmark state-of-the-art
time-series NN architectures, namely Recurrent NN, Gated Recurrent Unit,
Long-Short Term Memory (LSTM), Bidirectional LSTM, Transformer, and Temporal
Convolutional Network, by training both specialised models, tailored for
specific initial conditions, and general models, capable of extrapolating to
unseen scenarios. The results show accurate and low-cost prediction of
temperature and heat flux fields, laying the basis for enabling flexible and
generalisable thermal error correction in machine tool environments.

</details>


### [190] [Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout](https://arxiv.org/abs/2510.03262)
*Andi Zhang,Xuan Ding,Haofan Wang,Steven McDonagh,Samuel Kaski*

Main category: cs.LG

TL;DR: Orthogonal Monte Carlo Dropout enforces strict orthogonality when merging sparse semantic vectors for LoRA adapters, ensuring orthogonality with no extra time cost; but orthogonality alone does not guarantee semantic disentanglement or compositionality.


<details>
  <summary>Details</summary>
Motivation: To reduce interference when merging multiple LoRA adapters and to enable compositional semantics without extra runtime complexity.

Method: Introduce Orthogonal Monte Carlo Dropout to enforce strict vector orthogonality during combination of sparse semantic vectors; provide theoretical and runtime guarantees of maintaining orthogonality across merged LoRAs.

Result: Proves theoretical and runtime orthogonality; empirical analysis shows orthogonality does not yield semantic disentanglement or true compositionality; suggests re-evaluating the role of inter-adapter orthogonality.

Conclusion: Inter-LoRA orthogonality alone is insufficient for semantic compositionality; a reevaluation of adapter merging strategies is needed to achieve true compositional semantics.

Abstract: We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict
orthogonality when combining sparse semantic vectors without extra time
complexity. LoRA, a popular fine-tuning method for large models, typically
trains a module to represent a specific concept such as an object or a style.
When multiple LoRAs are merged, for example to generate an object in a
particular style, their semantic vectors may interfere with each other. Our
method guarantees, at the theoretical and runtime levels, that merged LoRAs
remain orthogonal and thus free from direct interference. However, empirical
analysis reveals that such orthogonality does not lead to the semantic
disentanglement or compositionality highlighted in prior work on compositional
adaptation. This finding suggests that inter-LoRA orthogonality alone may be
insufficient for achieving true semantic compositionality, prompting a
re-examination of its role in adapter merging.

</details>


### [191] [Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models](https://arxiv.org/abs/2510.03263)
*Agnieszka Polowczyk,Alicja Polowczyk,Joanna Waczyńska,Piotr Borycki,Przemysław Spurek*

Main category: cs.LG

TL;DR: The abstract analyzes the difficulty of unlearning in text-to-image models, introducing Memory Self-Regeneration and the MemoRa recovery strategy, and argues for robustness of knowledge retrieval as a key evaluation metric, while identifying short-term and long-term forgetting regimes.


<details>
  <summary>Details</summary>
Motivation: To prevent misuse of powerful text-to-image models by enabling selective removal of knowledge without compromising overall performance, and to address the challenge that forgetting is hard and reversal remains possible.

Method: Proposes the Memory Self-Regeneration task and the MemoRa recovery strategy as methods to support regaining previously forgotten knowledge; advocates robustness in knowledge retrieval as an evaluation criterion; conceptually distinguishes short-term vs long-term forgetting.

Result: Conceptual contributions: a framework for thinking about forgetting and recall in unlearning; identification of two forgetting regimes; introduction of tasks/strategies (Memory Self-Regeneration, MemoRa). No empirical results are reported in the abstract.

Conclusion: Robust knowledge retrieval is crucial for advancing unlearning techniques; forgetting has both short-term and long-term dimensions; the proposed Memory Self-Regeneration and MemoRa offer directions for regenerative approaches to memory in models.

Abstract: The impressive capability of modern text-to-image models to generate
realistic visuals has come with a serious drawback: they can be misused to
create harmful, deceptive or unlawful content. This has accelerated the push
for machine unlearning. This new field seeks to selectively remove specific
knowledge from a model's training data without causing a drop in its overall
performance. However, it turns out that actually forgetting a given concept is
an extremely difficult task. Models exposed to attacks using adversarial
prompts show the ability to generate so-called unlearned concepts, which can be
not only harmful but also illegal. In this paper, we present considerations
regarding the ability of models to forget and recall knowledge, introducing the
Memory Self-Regeneration task. Furthermore, we present MemoRa strategy, which
we consider to be a regenerative approach supporting the effective recovery of
previously lost knowledge. Moreover, we propose that robustness in knowledge
retrieval is a crucial yet underexplored evaluation measure for developing more
robust and effective unlearning techniques. Finally, we demonstrate that
forgetting occurs in two distinct ways: short-term, where concepts can be
quickly recalled, and long-term, where recovery is more challenging.

</details>


### [192] [Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data](https://arxiv.org/abs/2510.03264)
*Syeda Nahida Akter,Shrimai Prabhumoye,Eric Nyberg,Mostofa Patwary,Mohammad Shoeybi,Yejin Choi,Bryan Catanzaro*

Main category: cs.LG

TL;DR: Front-loading reasoning data in pretraining yields durable gains that SFT cannot fully replicate; diversity in pretraining is especially beneficial, while data quality matters more for SFT; excessive SFT scaling can erase early advantages.


<details>
  <summary>Details</summary>
Motivation: Investigate how timing, scale, diversity, and quality of reasoning data across pretraining and SFT affect LLM performance, given opaque pretraining corpora.

Method: Systematic experiments varying reasoning data across training stages (pretraining vs SFT) with controlled variations in scale, diversity, and quality; evaluate performance gains and interactions.

Result: Pretraining with reasoning data provides ~19% average gain; diversity in pretraining yields ~11% gain; SFT data quality yields ~15% gain; higher-quality pretraining data has latent effects seen after SFT; naive scaling of SFT data can be detrimental; early reasoning injection cannot be fully replicated by later training.

Conclusion: Guides principled data allocation across the entire training pipeline, advocating front-loading reasoning data in pretraining to build foundational reasoning capabilities; suggests an asymmetric allocation: diversity is crucial in pretraining, quality matters in SFT; challenge to the standard separation of LM and reasoning.

Abstract: The prevailing paradigm for enhancing the reasoning abilities of LLMs
revolves around post-training on high-quality, reasoning-intensive data. While
emerging literature suggests that reasoning data is increasingly incorporated
also during the mid-training stage-a practice that is relatively more
proprietary and less openly characterized-the role of such data in pretraining
remains unclear. In particular, due to the opaqueness of pretraining corpora in
most frontier models, the effect of reasoning data introduced at different
phases of pre- and/or post-training is relatively less reported in the
scientific literature. This raises several important questions: Is adding
reasoning data earlier during pretraining any better than introducing it during
post-training? Could earlier inclusion risk overfitting and harm
generalization, or instead establish durable foundations that later fine-tuning
cannot recover? We conduct the first systematic study of how reasoning
data-varying in scale, diversity, and quality-affects LLM performance when
introduced at different stages of training. We find that front-loading
reasoning data into pretraining is critical (19% avg gain), establishing
foundational capabilities that cannot be fully replicated by later-stage SFT,
even with more data. We uncover an asymmetric principle for optimal data
allocation: pretraining benefits most from broad diversity in reasoning
patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg
gain). We show that high-quality pretraining data has latent effects, activated
only after SFT, and that naively scaling SFT data can be detrimental, washing
away the benefits of early reasoning injection. Our results challenge the
conventional separation of language modeling and reasoning, providing a
principled guide for strategically allocating data across the entire training
pipeline to build more capable models.

</details>


### [193] [MindCraft: How Concept Trees Take Shape In Deep Models](https://arxiv.org/abs/2510.03265)
*Bowei Tian,Yexiao He,Wanghao Ye,Ziyao Wang,Meng Liu,Ang Li*

Main category: cs.LG

TL;DR: MindCraft's Concept Trees use per-layer spectral decomposition to map how concepts hierarchically emerge in large foundation models, identifying when concepts diverge from shared representations into separable subspaces, enabling cross-domain interpretability.


<details>
  <summary>Details</summary>
Motivation: Large-scale foundation models perform well but their internal organization of concepts is opaque. The work is motivated by causal inference to diagnose how concepts arise and diverge within shared representations across layers.

Method: Apply spectral decomposition at each model layer to extract principal directions, then link these directions into branching Concept Paths to form Concept Trees. These trees reconstruct the hierarchical emergence of concepts and pinpoint when they diverge into linearly separable subspaces.

Result: Empirical evaluations across domains (medical diagnosis, physics reasoning, political decision-making) show that Concept Trees recover semantic hierarchies, disentangle latent concepts, and are broadly applicable to analyze conceptual representations in deep models.

Conclusion: The Concept Tree framework provides a general, powerful tool for interpretable AI, enabling in-depth analysis of how concepts emerge and diverge in deep models and advancing interpretability across disciplines.

Abstract: Large-scale foundation models demonstrate strong performance across language,
vision, and reasoning tasks. However, how they internally structure and
stabilize concepts remains elusive. Inspired by causal inference, we introduce
the MindCraft framework built upon Concept Trees. By applying spectral
decomposition at each layer and linking principal directions into branching
Concept Paths, Concept Trees reconstruct the hierarchical emergence of
concepts, revealing exactly when they diverge from shared representations into
linearly separable subspaces. Empirical evaluations across diverse scenarios
across disciplines, including medical diagnosis, physics reasoning, and
political decision-making, show that Concept Trees recover semantic
hierarchies, disentangle latent concepts, and can be widely applied across
multiple domains. The Concept Tree establishes a widely applicable and powerful
framework that enables in-depth analysis of conceptual representations in deep
models, marking a significant step forward in the foundation of interpretable
AI.

</details>


### [194] [Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model](https://arxiv.org/abs/2510.03266)
*Bharat Sharma,Jitendra Kumar*

Main category: cs.LG

TL;DR: A variational autoencoder (VAE) is used to detect extreme gross primary productivity (GPP) events from CESM2 simulations across four US regions, benchmarked against singular spectral analysis (SSA). VAE yields similar spatial patterns to SSA but with higher anomaly thresholds; both methods indicate rising magnitude/frequency of negative extremes toward 2050-80 under SSP5-8.5, with VAE offering non-linear temporal capture and computational benefits, and not requiring predefined periodicity.


<details>
  <summary>Details</summary>
Motivation: Climate-driven anomalies disrupt the terrestrial carbon cycle; robust, flexible detection of extreme productivity events is needed. SSA has limitations (e.g., requiring predefined periodicity, linear assumptions). The study explores whether a nonlinear, data-driven approach (VAE) can improve or complement SSA in identifying extremes in GPP.

Method: Use a VAE with three dense layers and a latent space to model 12-month input sequences of normalized GPP from CESM2 under SSP5-8.5, across four AR6 regions in the Continental US. Anomalies are derived from reconstruction errors; extremes defined as the 5th percentile of anomalies. Comparisons are made to SSA-derived anomalies across three periods (1850-80, 1950-80, 2050-80). Threshold ranges reported: VAE 179–756 GgC; SSA 100–784 GgC, varying by region and period.

Result: VAE and SSA produce broadly concordant spatial patterns of extreme-event frequency. VAE tends to yield higher anomaly thresholds but shares the same qualitative trend: increasing magnitudes and frequencies of negative GPP extremes toward 2050-80, especially in Western and Central North America. The VAE performs comparably to SSA while offering computational efficiency and improved ability to capture non-linear temporal dependencies; it does not require predefined signal periodicity and discovers temporal structure from data.

Conclusion: VAE is a viable and potentially advantageous alternative to SSA for anomaly detection in GPP. It captures non-linear dynamics, avoids prespecifying periodicity, and offers computational efficiency, with results closely aligning with SSA in spatial patterns of extremes.

Abstract: Climate anomalies significantly impact terrestrial carbon cycle dynamics,
necessitating robust methods for detecting and analyzing anomalous behavior in
plant productivity. This study presents a novel application of variational
autoencoders (VAE) for identifying extreme events in gross primary productivity
(GPP) from Community Earth System Model version 2 simulations across four AR6
regions in the Continental United States. We compare VAE-based anomaly
detection with traditional singular spectral analysis (SSA) methods across
three time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario.
The VAE architecture employs three dense layers and a latent space with an
input sequence length of 12 months, trained on a normalized GPP time series to
reconstruct the GPP and identifying anomalies based on reconstruction errors.
Extreme events are defined using 5th percentile thresholds applied to both VAE
and SSA anomalies. Results demonstrate strong regional agreement between VAE
and SSA methods in spatial patterns of extreme event frequencies, despite VAE
producing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA
across regions and periods). Both methods reveal increasing magnitudes and
frequencies of negative carbon cycle extremes toward 2050-80, particularly in
Western and Central North America. The VAE approach shows comparable
performance to established SSA techniques, while offering computational
advantages and enhanced capability for capturing non-linear temporal
dependencies in carbon cycle variability. Unlike SSA, the VAE method does not
require one to define the periodicity of the signals in the data; it discovers
them from the data.

</details>


### [195] [PT$^2$-LLM: Post-Training Ternarization for Large Language Models](https://arxiv.org/abs/2510.03267)
*Xianglong Yan,Chengzhu Bao,Zhiteng Li,Tianao Zhang,Kaicheng Yang,Haotong Qin,Ruobing Xie,Xingwu Sun,Yulun Zhang*

Main category: cs.LG

TL;DR: PT2-LLM proposes post-training ternarization for LLMs using an asymmetric ternary quantizer with two-stage refinement (Iterative Ternary Fitting and Activation-aware Grid Alignment) plus a plug-and-play Structural Similarity-based Reordering. It achieves competitive results versus state-of-the-art 2-bit PTQ with lower memory and faster end-to-end speed.


<details>
  <summary>Details</summary>
Motivation: LLMs are memory- and compute-intensive; post-training quantization, especially with ternarization, is underexplored due to training-free optimization difficulties and outliers/dispersed weights. A practical PTQ framework is needed to reduce size and improve inference speed without retraining.

Method: An asymmetric ternary quantizer with two-stage refinement: (1) Iterative Ternary Fitting (ITF): alternates between constructing an optimal ternary grid and flexible rounding to minimize quantization error; (2) Activation-aware Grid Alignment (AGA): adjusts the grid to better match full-precision outputs. Additionally, a plug-and-play Structural Similarity-based Reordering (SSR) leverages inter-column structural similarity to ease quantization and mitigate outlier effects.

Result: Extensive experiments show PT2-LLM delivering competitive performance against state-of-the-art 2-bit PTQ methods with lower memory cost, while accelerating both prefill and decoding to achieve end-to-end speedup.

Conclusion: PT2-LLM provides an effective post-training ternarization framework for LLMs, enabling memory-efficient and fast inference without retraining; code and models will be released.

Abstract: Large Language Models (LLMs) have shown impressive capabilities across
diverse tasks, but their large memory and compute demands hinder deployment.
Ternarization has gained attention as a promising compression technique,
delivering substantial size reduction and high computational efficiency.
However, its potential in the post-training quantization (PTQ) setting remains
underexplored, due to the challenge of training-free parameter optimization and
the quantization difficulty posed by outliers and dispersed weights. To address
these issues, we propose PT$^2$-LLM, a post-training ternarization framework
tailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with
a two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which
alternates between optimal ternary grid construction and flexible rounding to
minimize quantization error, and (2) Activation-aware Grid Alignment (AGA),
which further refines the ternary grid to better match full-precision outputs.
In addition, we propose a plug-and-play Structural Similarity-based Reordering
(SSR) strategy that leverages inter-column structural similarity to ease
quantization and mitigate outlier effects, further enhancing overall
performance. Extensive experiments demonstrate that PT$^2$-LLM delivers
competitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with
lower memory cost, while also accelerating both prefill and decoding to achieve
end-to-end speedup. The code and models will be available at
https://github.com/XIANGLONGYAN/PT2-LLM.

</details>


### [196] [Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment](https://arxiv.org/abs/2510.03268)
*Lingjie Yi,Raphael Douady,Chao Chen*

Main category: cs.LG

TL;DR: The paper provides a theoretical framework for multimodal contrastive learning (MCL), showing that the modality gap arises from dimension collapse under certain constraints and can be eliminated or controlled by geometric constraints. It analyzes convergence to optimal representations and how alignment between modalities affects downstream tasks, offering mechanisms to achieve perfect alignment via hyperplane rotation or shared-space projection.


<details>
  <summary>Details</summary>
Motivation: To explain the mysterious modality gap observed in MCL, reconcile conflicting empirical findings on how the gap size affects downstream performance, and establish a formal theory for the convergent optimal representations and modality alignment.

Method: Theoretical analysis of MCL under different geometric constraints. The authors examine unconstrained, cone-constrained, and subspace-constrained settings. They show that the modality gap converges to zero under unconstrained or cone constraints, while under subspace constraints it equals the smallest angle between the two modality hyperplanes (dimension collapse). They prove that perfect alignment is impossible under subspace constraint and identify two pathways to achieve alignment: hyperplane rotation and shared-space projection.

Result: Key results: (1) modality gap converges to zero without constraint or under cone constraint; (2) under subspace constraint, the gap equals the minimal angle between the two modality hyperplanes, revealing dimension collapse as the origin of the gap; (3) paired samples cannot be perfectly aligned under subspace constraint; (4) perfect alignment is achievable through hyperplane rotation or shared space projection.

Conclusion: Dimension collapse is the fundamental cause of the modality gap in MCL. The paper clarifies how the gap impacts downstream tasks via sample-pair alignment and shows two strategies to attain perfect cross-modal alignment, informing the design of MCL objectives and constraints to control or eliminate the gap.

Abstract: Multimodal contrastive learning (MCL) aims to embed data from different
modalities in a shared embedding space. However, empirical evidence shows that
representations from different modalities occupy completely separate regions of
embedding space, a phenomenon referred to as the modality gap. Moreover,
experimental findings on how the size of the modality gap influences downstream
performance are inconsistent. These observations raise two key questions: (1)
What causes the modality gap? (2) How does it affect downstream tasks? To
address these questions, this paper introduces the first theoretical framework
for analyzing the convergent optimal representations of MCL and the modality
alignment when training is optimized. Specifically, we prove that without any
constraint or under the cone constraint, the modality gap converges to zero.
Under the subspace constraint (i.e., representations of two modalities fall
into two distinct hyperplanes due to dimension collapse), the modality gap
converges to the smallest angle between the two hyperplanes. This result
identifies \emph{dimension collapse} as the fundamental origin of the modality
gap. Furthermore, our theorems demonstrate that paired samples cannot be
perfectly aligned under the subspace constraint. The modality gap influences
downstream performance by affecting the alignment between sample pairs. We
prove that, in this case, perfect alignment between two modalities can still be
achieved via two ways: hyperplane rotation and shared space projection.

</details>


### [197] [General Exploratory Bonus for Optimistic Exploration in RLHF](https://arxiv.org/abs/2510.03269)
*Wendi Li,Changdae Oh,Yixuan Li*

Main category: cs.LG

TL;DR: General Exploratory Bonus (GEB) provides optimistic exploration in RLHF by counteracting divergence-induced bias, unifying prior bonuses, and showing improvements across α-divergences and LL models.


<details>
  <summary>Details</summary>
Motivation: Existing exploratory bonuses under KL/α-divergence regularization bias exploration toward high-probability regions of the reference model, causing conservatism instead of discovery.

Method: Introduce GEB, a theoretical framework with reference-dependent reward regulation that counteracts divergence-induced bias, unifies prior heuristic bonuses as special cases, and extends across the full α-divergence family.

Result: Theoretical analysis shows current KL/α-divergence-based formulations bias exploration; GEB provably satisfies the optimism principle and empirically outperforms baselines on alignment tasks across divergence settings and large language model backbones.

Conclusion: GEB offers a principled and practical solution for optimistic exploration in RLHF, with general applicability across divergences and improved exploration performance.

Abstract: Optimistic exploration is central to improving sample efficiency in
reinforcement learning with human feedback, yet existing exploratory bonus
methods to incentivize exploration often fail to realize optimism. We provide a
theoretical analysis showing that current formulations, under KL or
$\alpha$-divergence regularization, unintentionally bias exploration toward
high-probability regions of the reference model, thereby reinforcing
conservative behavior instead of promoting discovery of uncertain regions. To
address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel
theoretical framework that provably satisfies the optimism principle. GEB
counteracts divergence-induced bias via reference-dependent reward regulation
and unifies prior heuristic bonuses as special cases, while extending naturally
across the full $\alpha$-divergence family. Empirically, GEB consistently
outperforms baselines on alignment tasks across multiple divergence settings
and large language model backbones. These results demonstrate that GEB offers
both a principled and practical solution for optimistic exploration in RLHF.

</details>


### [198] [CoDA: Coding LM via Diffusion Adaptation](https://arxiv.org/abs/2510.03270)
*Haolin Chen,Shiyu Wang,Can Qin,Bo Pang,Zuxin Liu,Jielin Qiu,Jianguo Zhang,Yingbo Zhou,Zeyuan Chen,Ran Xu,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.LG

TL;DR: CoDA-1.7B diffusion coder rivals larger diffusion models on coding benchmarks with a lightweight, open-source pipeline trained on TPU and confidence-guided sampling.


<details>
  <summary>Details</summary>
Motivation: Overcome heavy computational cost and latency of diffusion-based coders by delivering a small, open-source, end-to-end diffusion coding system that still delivers strong performance.

Method: Train a 1.7B diffusion coder on TPU, combining diffusion pre-training with code-centric mid-training and instruction tuning; introduce confidence-guided sampling to trade off latency and quality; release model checkpoints, evaluation harnesses, and TPU training pipelines.

Result: On HumEval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses diffusion models up to 7B parameters; inference latency remains competitive.

Conclusion: A small, open-source diffusion coder can achieve competitive performance, and the provided training/evaluation pipelines accelerate research and development of lightweight diffusion-based coding assistants.

Abstract: Diffusion language models promise bidirectional context and infilling
capabilities that autoregressive coders lack, yet practical systems remain
heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU
with a fully open-source training pipeline. CoDA pairs large-scale diffusion
pre-training with code-centric mid-training and instruction tuning, enabling
confidence-guided sampling that keeps inference latency competitive. On
Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses
diffusion models up to 7B parameters. Our release includes model checkpoints,
evaluation harnesses, and TPU training pipelines to accelerate research on
lightweight diffusion-based coding assistants.

</details>


### [199] [Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary](https://arxiv.org/abs/2510.03271)
*Zi Liang,Zhiyao Wu,Haoyang Shang,Yulin Jin,Qingqing Ye,Huadi Zheng,Peizhao Hu,Haibo Hu*

Main category: cs.LG

TL;DR: Introduces Decision Potential Surface (DPS) for analyzing LLM decision boundaries; proves the zero-height isohypse of DPS equals the LLM decision boundary; presents K-DPS, a sampling-based approximate boundary construction with theoretical error bounds; validates empirically across models and corpora.


<details>
  <summary>Details</summary>
Motivation: Understanding decision boundaries in LLMs is important for interpretability but existing methods are computationally infeasible due to huge vocabulary-sequence spaces and autoregressive decoding; a scalable boundary analysis method is needed.

Method: Define DPS on confidences distinguishing sampling sequences for each input; prove that the zero-height isohypse corresponds to the LLM decision boundary; introduce K-DPS algorithm that uses K sequence samples to approximate the boundary; derive upper bounds for absolute error, expected error, and error concentration; discuss trade-offs between sampling budget and accuracy.

Result: Theoretical guarantees on error bounds for K-DPS; a practical algorithm for approximating LLM decision boundaries with controllable error using finite samples; extensive empirical validation across multiple LLMs and corpora demonstrating feasibility and effectiveness.

Conclusion: DPS provides a scalable framework for interpreting LLMs by enabling approximate, controllable boundary estimation; K-DPS makes boundary analysis feasible in practice and opens doors for improved interpretability and auditing of LLM behavior.

Abstract: Decision boundary, the subspace of inputs where a machine learning model
assigns equal classification probabilities to two classes, is pivotal in
revealing core model properties and interpreting behaviors. While analyzing the
decision boundary of large language models (LLMs) has raised increasing
attention recently, constructing it for mainstream LLMs remains computationally
infeasible due to the enormous vocabulary-sequence sizes and the
auto-regressive nature of LLMs. To address this issue, in this paper we propose
Decision Potential Surface (DPS), a new notion for analyzing LLM decision
boundary. DPS is defined on the confidences in distinguishing different
sampling sequences for each input, which naturally captures the potential of
decision boundary. We prove that the zero-height isohypse in DPS is equivalent
to the decision boundary of an LLM, with enclosed regions representing decision
regions. By leveraging DPS, for the first time in the literature, we propose an
approximate decision boundary construction algorithm, namely $K$-DPS, which
only requires K-finite times of sequence sampling to approximate an LLM's
decision boundary with negligible error. We theoretically derive the upper
bounds for the absolute error, expected error, and the error concentration
between K-DPS and the ideal DPS, demonstrating that such errors can be
trade-off with sampling times. Our results are empirically validated by
extensive experiments across various LLMs and corpora.

</details>


### [200] [PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling](https://arxiv.org/abs/2510.03272)
*Yukun Zhang,Xueqing Zhou*

Main category: cs.LG

TL;DR: A PDE-based continuous dynamics view of Transformer shows residual connections and layer normalization are essential stabilizers; without them, representational drift and unstable training occur.


<details>
  <summary>Details</summary>
Motivation: Provide a principled theoretical understanding of Transformer mechanisms and why certain components are necessary, by modeling Transformers as continuous dynamical systems governed by a master PDE.

Method: Map Transformer components to PDE operators: self-attention as non-local interaction, feed-forward as local reaction, with residual connections and layer normalization acting as stabilizers; compare a standard Transformer to a PDE simulator lacking stabilizers.

Result: Empirical evidence that omitting residual connections causes catastrophic drift; omitting layer normalization leads to instability; stabilizers are mathematically necessary to tame the continuous system; first-principles explanation.

Conclusion: Introduces a new paradigm for analyzing deep networks via continuous dynamics; provides theoretical justification for Transformer design choices and suggests broader analytic framework.

Abstract: The Transformer architecture has revolutionized artificial intelligence, yet
a principled theoretical understanding of its internal mechanisms remains
elusive. This paper introduces a novel analytical framework that
reconceptualizes the Transformer's discrete, layered structure as a continuous
spatiotemporal dynamical system governed by a master Partial Differential
Equation (PDE). Within this paradigm, we map core architectural components to
distinct mathematical operators: self-attention as a non-local interaction, the
feed-forward network as a local reaction, and, critically, residual connections
and layer normalization as indispensable stabilization mechanisms. We do not
propose a new model, but rather employ the PDE system as a theoretical probe to
analyze the mathematical necessity of these components. By comparing a standard
Transformer with a PDE simulator that lacks explicit stabilizers, our
experiments provide compelling empirical evidence for our central thesis. We
demonstrate that without residual connections, the system suffers from
catastrophic representational drift, while the absence of layer normalization
leads to unstable, explosive training dynamics. Our findings reveal that these
seemingly heuristic "tricks" are, in fact, fundamental mathematical stabilizers
required to tame an otherwise powerful but inherently unstable continuous
system. This work offers a first-principles explanation for the Transformer's
design and establishes a new paradigm for analyzing deep neural networks
through the lens of continuous dynamics.

</details>


### [201] [Learning without Global Backpropagation via Synergistic Information Distillation](https://arxiv.org/abs/2510.03273)
*Chenhao Ye,Ming Tang*

Main category: cs.LG

TL;DR: SID reframes deep learning as a cascade of locally trained modules with decoupled backward passes, enabling parallel training and reduced memory, while maintaining or improving BP accuracy and robustness; it serves as a drop-in BP replacement.


<details>
  <summary>Details</summary>
Motivation: BP suffers from update locking (modules idle until backward pass completes) and high memory usage for storing activations. SID aims to remove these bottlenecks while preserving feed-forward inference.

Method: Structure the network as a pipeline of modules, each with a local objective to refine a probabilistic belief about the target by balancing fidelity to the target with consistency to the preceding module's belief. By decoupling backward dependencies, training can be parallelized and memory usage reduced. The approach preserves the standard forward pass and provides a theoretical guarantee of monotonic performance improvement with depth.

Result: Empirically, SID matches or surpasses BP in classification accuracy, with improved scalability and robustness to label noise.

Conclusion: SID is a versatile drop-in replacement for BP that eliminates update locking and lowers memory requirements without sacrificing inference capability; it comes with a theoretical guarantee of monotonic improvement and code is available at the provided GitHub link.

Abstract: Backpropagation (BP), while foundational to deep learning, imposes two
critical scalability bottlenecks: update locking, where network modules remain
idle until the entire backward pass completes, and high memory consumption due
to storing activations for gradient computation. To address these limitations,
we introduce Synergistic Information Distillation (SID), a novel training
framework that reframes deep learning as a cascade of local cooperative
refinement problems. In SID, a deep network is structured as a pipeline of
modules, each imposed with a local objective to refine a probabilistic belief
about the ground-truth target. This objective balances fidelity to the target
with consistency to the belief from its preceding module. By decoupling the
backward dependencies between modules, SID enables parallel training and hence
eliminates update locking and drastically reduces memory requirements.
Meanwhile, this design preserves the standard feed-forward inference pass,
making SID a versatile drop-in replacement for BP. We provide a theoretical
foundation, proving that SID guarantees monotonic performance improvement with
network depth. Empirically, SID consistently matches or surpasses the
classification accuracy of BP, exhibiting superior scalability and pronounced
robustness to label noise.Code is available at:
https://github.com/ychAlbert/sid-bp

</details>


### [202] [Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models](https://arxiv.org/abs/2510.03274)
*Tianao Zhang,Zhiteng Li,Xianglong Yan,Haotong Qin,Yong Guo,Yulun Zhang*

Main category: cs.LG

TL;DR: Ultra-low-bit post-training quantization for diffusion LLMs (dLLMs) enabling 2-bit weight quantization that surpasses AR-based PTQ methods by tailored calibration and weight quantization.


<details>
  <summary>Details</summary>
Motivation: Reduce model size and deployment cost for diffusion LLMs (dLLMs) as model sizes grow, and address PTQ challenges at extremely low bitwidth (2-bit) which standard PTQ methods fail to handle due to masked-denoising activations.

Method: 1) Masked Calibration Simulation (MCS) aligns calibration with timestep-dependent masking in dLLMs. 2) Data-aware Any-order Quantizer (DAQ) learns ultra-low-bit weight representations via optimization guided by simulated calibration data. 3) Adaptive Blockwise Mixed Precision (ABMP) at a strict 2-bit budget to allocate bit width across channel groups based on sensitivity.

Result: Quant-dLLM at 2-bit consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer PTQ methods on dLLMs.

Conclusion: Quant-dLLM provides a practical framework for ultra-low-bit PTQ in dLLMs, leveraging calibrated masking-aware calibration, data-aware weight quantization, and adaptive precision allocation; code and models will be released.

Abstract: Diffusion large language models (dLLMs), which offer bidirectional context
and flexible masked-denoising generation, are emerging as a compelling
alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model
sizes continue to grow, motivating weight compression for deployment. Although
post-training quantization (PTQ) is effective for AR LLMs, directly
transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To
tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework
tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the
fully visible signals assumed by standard PTQ methods, we introduce Masked
Calibration Simulation (MCS) to align calibration with the timestep-dependent
masking, which yields more reliable calibrations. Moreover, we propose a
Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight
representations via an optimization algorithm. It performs iterative
approximation guided by our simulated calibration data. In addition, under a
strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a
sensitivity-based precision allocation scheme that adaptively assigns bit width
across channel groups. When restricted to 2-bit precision, Quant-dLLM
consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer
PTQ methods on dLLMs. The code and models will be available at:
https://github.com/ZTA2785/Quant-dLLM.

</details>


### [203] [SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size](https://arxiv.org/abs/2510.03275)
*Junhao Xia,Ming Zhao,Limin Xiao,Xiujun Zhang*

Main category: cs.LG

TL;DR: Introduces SDQ-LLM, a Sigma-Delta Quantization framework for 1-bit (or ~1.58-bit) LLM weights with continuously adjustable OSR, enabling dynamic memory-accuracy trade-offs and efficient inference; uses upsampling, Hadamard smoothing, and a layer-/within-layer MultiOSR strategy; validated on OPT and LLaMA with code released.


<details>
  <summary>Details</summary>
Motivation: LLMs demand enormous computational and memory resources. Extremely low-bit quantization is essential for deployment but typically harms performance. The paper aims to push the limits of quantization by allowing flexible OSR and robust quantization techniques to preserve linguistic reasoning while drastically reducing model size.

Method: Apply Sigma-Delta Quantization to upsample and binarize/ternarize weights, encoding high-precision parameters into 1-bit or ~1.58-bit representations. Replace multiplications in linear layers with additions. Apply Hadamard-based weight smoothing prior to quantization. Introduce MultiOSR: fine-grained OSR allocation across layers and within each layer based on weight variance and parameter scale, enabling fractional OSR (e.g., 2.5x).

Result: Empirical evaluation on OPT and LLaMA families shows SDQ-LLM achieves improved efficiency while maintaining high-precision performance under aggressive low-OSR settings. Code is released at the provided GitHub URL.

Conclusion: SDQ-LLM enables extremely low-bit LLM quantization with continuous OSR adaptability, balancing memory constraints and accuracy. The approach combines Sigma-Delta quantization, smoothing, and fine-grained OSR allocation to sustain linguistic capabilities in large models, with demonstrated efficacy on standard LLM benchmarks.

Abstract: Large language models (LLMs) face significant computational and memory
challenges, making extremely low-bit quantization crucial for their efficient
deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for
1-bit LLMs of any size, a novel framework that enables extremely low-bit
quantization of LLMs while preserving their linguistic reasoning capabilities.
A distinctive feature of SDQ-LLM is the continuous adjustability of the
Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM
constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal
trade-off between model size and accuracy. SDQ-LLM uses upsampling combined
with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding
high-precision parameters into 1-bit or 1.58-bit representations, replacing the
multiplication operations within linear layers with addition. This approach
significantly enhances inference efficiency under extremely low-bit
quantization. To further reduce the loss of quantization precision, we
incorporate Hadamard-based weight smoothing prior to quantization, improving
the stability and robustness of the weight representations. Furthermore, to
fully leverage the continuity of the OSR and reduce precision loss, recognizing
the correlation between quantization sensitivity and weight variance, we
propose a fine-grained, layer- and linear-wise OSR allocation strategy,
MultiOSR. This strategy distributes OSR both across layers and within each
layer, based on weight variance and parameter scale. Finally, extensive
experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a
more efficient and high-precision performance even under highly aggressive
low-OSR settings. Our code is available at
https://github.com/Dreamlittlecat/LLM-Quant-Factory.

</details>


### [204] [QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks](https://arxiv.org/abs/2510.03276)
*Qian Chen,Linxin Yang,Akang Wang,Xiaodong Luo,Yin Zhang*

Main category: cs.LG

TL;DR: A lightweight quadratic enhancer augments neural networks with quadratic feature interactions, using low-rankness, weight sharing, and sparsification to keep parameters and computation minimal; yields noticeable gains across tasks.


<details>
  <summary>Details</summary>
Motivation: To increase the nonlinearity and expressivity of neural networks beyond standard linear transforms plus activations, while keeping parameter and computational overhead low.

Method: Introduce a lightweight quadratic transformer (quadratic enhancer) that enforces quadratic interactions between features at every layer using low-rank factorization, weight sharing, and sparsification to minimize parameters and FLOPs. For a fixed architecture, this adds quadratic terms with negligible overhead.

Result: Proof-of-concept experiments on image classification, text classification, and fine-tuning large-language models showed clear and substantial improvements across tasks.

Conclusion: Quadratic interactions can meaningfully boost performance with minimal extra cost, suggesting broad applicability across architectures; future work could explore larger studies and integration strategies.

Abstract: The combination of linear transformations and non-linear activation functions
forms the foundation of most modern deep neural networks, enabling them to
approximate highly complex functions. This paper explores the introduction of
quadratic transformations to further increase nonlinearity in neural networks,
with the aim of enhancing the performance of existing architectures. To reduce
parameter complexity and computational complexity, we propose a lightweight
quadratic enhancer that uses low-rankness, weight sharing, and sparsification
techniques. For a fixed architecture, the proposed approach introduces
quadratic interactions between features at every layer, while only adding
negligible amounts of additional model parameters and forward computations. We
conduct a set of proof-of-concept experiments for the proposed method across
three tasks: image classification, text classification, and fine-tuning
large-language models. In all tasks, the proposed approach demonstrates clear
and substantial performance gains.

</details>


### [205] [Quantifying constraint hierarchies in Bayesian PINNs via per-constraint Hessian decomposition](https://arxiv.org/abs/2510.03278)
*Filip Landgren*

Main category: cs.LG

TL;DR: A matrix-free Laplace analysis decomposes the posterior Hessian by constraint in Bayesian PINNs, quantifying how each physical constraint shapes the loss landscape and uncertainty; a Van der Pol example shows that tweaking a single loss weight non-trivially redistributes curvature and constraint dominance.


<details>
  <summary>Details</summary>
Motivation: Interpretation of uncertainty in B-PINNs is obscured by the intertwined effects of data, physics, and loss terms; overconfidence may be justified by constraints, so we need a principled way to attribute influence to individual constraints.

Method: Introduce a scalable, matrix-free Laplace framework that decomposes the posterior Hessian into contributions from each constraint and provides metrics for their relative influence on the loss landscape.

Result: Applied to the Van der Pol equation, the framework reveals how constraints shape the network’s geometry; it directly shows via the Hessian that changing a single loss weight redistributes curvature and effective dominance among constraints.

Conclusion: The proposed framework clarifies how physical constraints influence uncertainty and network geometry in B-PINNs, enabling principled analysis and calibration of constraint influence on the loss landscape.

Abstract: Bayesian physics-informed neural networks (B-PINNs) merge data with governing
equations to solve differential equations under uncertainty. However,
interpreting uncertainty and overconfidence in B-PINNs requires care due to the
poorly understood effects the physical constraints have on the network;
overconfidence could reflect warranted precision, enforced by the constraints,
rather than miscalibration. Motivated by the need to further clarify how
individual physical constraints shape these networks, we introduce a scalable,
matrix-free Laplace framework that decomposes the posterior Hessian into
contributions from each constraint and provides metrics to quantify their
relative influence on the loss landscape. Applied to the Van der Pol equation,
our method tracks how constraints sculpt the network's geometry and shows,
directly through the Hessian, how changing a single loss weight non-trivially
redistributes curvature and effective dominance across the others.

</details>


### [206] [MemMamba: Rethinking Memory Patterns in State Space Model](https://arxiv.org/abs/2510.03279)
*Youjin Wang,Yangjingyi Chen,Jiahao Yan,Jiaxuan Lu,Xiao Sun*

Main category: cs.LG

TL;DR: MemMamba analyzes Mamba's memory decay and introduces a summarization-based architecture to preserve long-range information with linear complexity, achieving notable accuracy gains and a 48% inference speedup on ultra-long sequence benchmarks.


<details>
  <summary>Details</summary>
Motivation: Long-sequence modeling requires both efficiency and memory retention. RNNs struggle with gradient issues; Transformers have quadratic complexity; Mamba offers O(n) time and 1-step recurrence but suffers exponential memory decay. Understanding memory mechanics and improving retention is crucial for scalable long-sequence modeling.

Method: The authors perform mathematical derivations and information-theoretic analysis of Mamba's memory, introduce horizontal-vertical memory fidelity metrics, and propose MemMamba, which combines a state summarization mechanism with cross-layer and cross-token attention to maintain information with linear complexity.

Result: MemMamba outperforms existing Mamba variants and Transformers on long-sequence benchmarks (e.g., PG19, Passkey Retrieval) and delivers a 48% speedup in inference, demonstrating an improved complexity-memory trade-off.

Conclusion: MemMamba offers a breakthrough paradigm for ultra-long sequence modeling by mitigating long-range forgetting while preserving efficiency, suggesting a viable path forward for scalable long-sequence architectures.

Abstract: With the explosive growth of data, long-sequence modeling has become
increasingly important in tasks such as natural language processing and
bioinformatics. However, existing methods face inherent trade-offs between
efficiency and memory. Recurrent neural networks suffer from gradient vanishing
and explosion, making them hard to scale. Transformers can model global
dependencies but are constrained by quadratic complexity. Recently, selective
state-space models such as Mamba have demonstrated high efficiency with O(n)
time and O(1) recurrent inference, yet their long-range memory decays
exponentially. In this work, we conduct mathematical derivations and
information-theoretic analysis to systematically uncover the memory decay
mechanism of Mamba, answering a fundamental question: what is the nature of
Mamba's long-range memory and how does it retain information? To quantify key
information loss, we further introduce horizontal-vertical memory fidelity
metrics that capture degradation both within and across layers. Inspired by how
humans distill and retain salient information when reading long documents, we
propose MemMamba, a novel architectural framework that integrates state
summarization mechanism together with cross-layer and cross-token attention,
which alleviates long-range forgetting while preserving linear complexity.
MemMamba achieves significant improvements over existing Mamba variants and
Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,
while delivering a 48% speedup in inference efficiency. Both theoretical
analysis and empirical results demonstrate that MemMamba achieves a
breakthrough in the complexity-memory trade-off, offering a new paradigm for
ultra-long sequence modeling.

</details>


### [207] [Training Optimal Large Diffusion Language Models](https://arxiv.org/abs/2510.03280)
*Jinjie Ni,Qian Liu,Chao Du,Longxu Dou,Hang Yan,Zili Wang,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: Quokka presents the first systematic scaling law for diffusion language models (DLMs), covering both compute- and data-constrained regimes, and yields practical training guidance and broader inspiration for the AI community.


<details>
  <summary>Details</summary>
Motivation: There is a need for principled scaling laws to guide the training of diffusion language models under varying compute and data budgets, enabling informed design choices and efficient progress beyond existing models like Chinchilla.

Method: Develop and validate a comprehensive scaling framework for DLMs that accounts for model size, data throughput, compute budgets, and optimization/design choices, covering compute- and data-constrained regimes and comparing against relevant baselines.

Result: A first-of-its-kind systematic scaling law for DLMs that links model parameters, data, and compute to performance, with actionable insights for short-term training practices and potential long-term guidance for the AI community.

Conclusion: Quokka broadens the scope of diffusion models beyond prior work (e.g., Chinchilla), offering a practical tool for practitioners and an inspiration for future research in scalable AI.

Abstract: We introduce Quokka, the first systematic scaling law for diffusion language
models (DLMs), encompassing both compute-constrained and data-constrained
regimes, and studying the key modeling and optimization designs. Quokka is a
good friend of Chinchilla and provides wider scopes. We hope the results would
bring short-term practical guidance in DLMs training and long-term inspirations
for the whole AI community.

</details>


### [208] [Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework](https://arxiv.org/abs/2510.03282)
*Hao Gu,Vibhas Nair,Amrithaa Ashok Kumar,Jayvart Sharma,Ryan Lagasse*

Main category: cs.LG

TL;DR: A hybrid attribution and pruning (HAP) framework is proposed to discover sparse, faithful language-model circuits by first using attribution patching to identify a high-potential subgraph and then applying edge pruning within that subgraph to extract a faithful circuit. The method yields faster performance without losing faithfulness, demonstrated by a 46% speedup and a case study on Indirect Object Identification that preserves cooperative components like S-inhibition heads, suggesting improved scalability for mechanistic interpretability.


<details>
  <summary>Details</summary>
Motivation: To overcome the speed–faithfulness trade-off in mechanistic circuit discovery, enabling scalable analysis of larger language models without sacrificing circuit fidelity.

Method: First apply attribution patching to identify a high-potential subgraph. Then perform edge pruning within that subgraph to extract a faithful circuit. Compare against baseline methods and illustrate with a case study on Indirect Object Identification to show preservation of cooperative components.

Result: HAP achieves about a 46% reduction in running time compared with baseline circuit-discovery algorithms without sacrificing circuit faithfulness. In a case study on Indirect Object Identification, cooperative circuit components (e.g., S-inhibition heads) are preserved, which attribution-patching alone tends to prune at high sparsity.

Conclusion: HAP enhances the scalability of mechanistic interpretability for larger models by combining fast, approximate patching with precise pruning, providing faithful circuits with greater computational efficiency.

Abstract: Interpreting language models often involves circuit analysis, which aims to
identify sparse subnetworks, or circuits, that accomplish specific tasks.
Existing circuit discovery algorithms face a fundamental trade-off: attribution
patching is fast but unfaithful to the full model, while edge pruning is
faithful but computationally expensive. This research proposes a hybrid
attribution and pruning (HAP) framework that uses attribution patching to
identify a high-potential subgraph, then applies edge pruning to extract a
faithful circuit from it. We show that HAP is 46\% faster than baseline
algorithms without sacrificing circuit faithfulness. Furthermore, we present a
case study on the Indirect Object Identification task, showing that our method
preserves cooperative circuit components (e.g. S-inhibition heads) that
attribution patching methods prune at high sparsity. Our results show that HAP
could be an effective approach for improving the scalability of mechanistic
interpretability research to larger models. Our code is available at
https://anonymous.4open.science/r/HAP-circuit-discovery.

</details>


### [209] [MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment](https://arxiv.org/abs/2510.03283)
*Yufei Li,Yu Fu,Yue Dong,Cong Liu*

Main category: cs.LG

TL;DR: MACE is an edge-LMM system that co-locates inference and fine-tuning with intelligent memory management and iteration-level scheduling to balance throughput, latency, and update freshness, outperforming periodic retraining while reducing latency.


<details>
  <summary>Details</summary>
Motivation: The non-stationary nature of user data necessitates frequent retraining, creating a tension between inference latency and model accuracy under constrained GPU resources. Existing retraining strategies either delay updates, over-commit resources, or ignore iteration-level granularity, risking service-level objectives.

Method: Propose MACE, a hybrid LLM system that colocates concurrent inference (prefill, decode) and fine-tuning, with smart memory management to maximize task performance. It allocates GPU cycles by prioritizing updates that meaningfully affect output alignment, and uses trace-driven evaluation to validate performance.

Result: MACE matches or exceeds continuous retraining while reducing inference latency by up to 63% and maintaining throughput under resource constraints; achieves improved latency breakdown across prefill, decode, and finetune stages and sustains GPU utilization above 85% on NVIDIA AGX Orin.

Conclusion: Iteration-level hybrid scheduling is a promising direction for deploying LLMs with continual learning capabilities on edge platforms.

Abstract: Large language models (LLMs) deployed on edge servers are increasingly used
in latency-sensitive applications such as personalized assistants,
recommendation, and content moderation. However, the non-stationary nature of
user data necessitates frequent retraining, which introduces a fundamental
tension between inference latency and model accuracy under constrained GPU
resources. Existing retraining strategies either delay model updates,
over-commit resources to retraining, or overlook iteration-level retraining
granularity. In this paper, we identify that iteration-level scheduling is
crucial for adapting retraining frequency to model drift without violating
service-level objectives (SLOs). We propose MACE, a hybrid LLM system that
colocates concurrent inference (prefill, decode) and fine-tuning, with
intelligent memory management to maximize task performance while promising
inference throughput. MACE leverages the insight that not all model updates
equally affect output alignment and allocates GPU cycles accordingly to balance
throughput, latency, and update freshness. Our trace-driven evaluation shows
that MACE matches or exceeds continuous retraining while reducing inference
latency by up to 63% and maintaining throughput under resource constraints.
Compared to periodic retraining, MACE improves latency breakdown across
prefill, decode, and finetune stages, and sustains GPU utilization above 85% in
NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid
scheduling is a promising direction for deploying LLMs with continual learning
capabilities on edge platforms.

</details>


### [210] [Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments](https://arxiv.org/abs/2510.03284)
*Vinay Venkatesh,Vamsidhar R Kamanuru,Lav Kumar,Nikita Kothari*

Main category: cs.LG

TL;DR: Edge-FIT is a scalable federated instruction-tuning framework for LLMs that uses 4-bit QLORA to reduce communication and compute, enabling edge/home gateways to fine-tune models on IoT-domain data; demonstrates strong performance with Llama 2 7B and viable deployment on a 3.8B Phi-3-mini.


<details>
  <summary>Details</summary>
Motivation: Traditional federated learning methods struggle with the enormous parameter size of LLMs, causing high communication and computation overhead and hindering scalable, decentralized instruction tuning on edge devices.

Method: Integrates federated learning with 4-bit Quantized Low-Rank Adaptation (QLORA); filters Databricks Dolly 15k data for the IoT domain; demonstrates on Llama 2 7B and a 3.8B Phi-3-mini model to show scalability and edge deployment feasibility.

Result: Edge-FIT-tuned Llama 2(7B) achieves an F1-Score of 0.89; shows a viable trade-off with the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable framework for decentralized LLM deployment on home compute gateways.

Conclusion: Edge-FIT offers a scalable solution for decentralized LLM instruction tuning by combining federated learning with 4-bit QLORA, mitigating communication and computation bottlenecks and enabling IoT/edge-domain deployments.

Abstract: This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a
scalable framework for Federated Instruction Tuning (FIT) of Large Language
Models (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail
when confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT
framework combines federated learning with 4-bit Quantized Low-Rank Adaptation
(QLORA), mitigating the core issues of communication and computational
overhead. We demonstrate this by filtering the general-purpose Databricks Dolly
15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned
Llama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable
trade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable
framework for decentralized LLM deployment on home compute gateways.

</details>


### [211] [LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain](https://arxiv.org/abs/2510.03288)
*Chiming Duan,Minghua He,Pei Xiao,Tong Jia,Xin Zhang,Zhewei Zhong,Xiang Luo,Yan Niu,Lingzhe Zhang,Yifan Wu,Siyu Yu,Weijie Hong,Ying Li,Gang Huang*

Main category: cs.LG

TL;DR: LogAction is an active domain adaptation approach for log-based anomaly detection that reduces labeling cost by combining transfer learning from mature systems with active learning guided by free-energy and uncertainty sampling, achieving high F1 with minimal labels.


<details>
  <summary>Details</summary>
Motivation: Labeling logs is expensive and time-consuming. There is a domain shift between source (mature) and target systems, and cold-start issues hinder active learning; thus a method that leverages existing labeled data while minimizing new labels is needed.

Method: LogAction trains a base anomaly-detection model on labeled logs from a mature system (transfer learning). It then applies active learning to select logs for labeling using two strategies: free-energy-based sampling to target distribution boundaries and uncertainty-based sampling to prioritize informative instances, addressing distribution gaps with minimal labeling.

Result: On six dataset combinations, LogAction achieves an average F1 score of 93.01% with only 2% of logs manually labeled, outperforming some state-of-the-art methods by 26.28%.

Conclusion: Active domain adaptation with energy- and uncertainty-based sampling is effective for cross-system log anomaly detection with reduced labeling effort, showing strong performance gains and potential for broader applicability; further work could explore generalization across more diverse systems and scalability.

Abstract: Log-based anomaly detection is a essential task for ensuring the reliability
and performance of software systems. However, the performance of existing
anomaly detection methods heavily relies on labeling, while labeling a large
volume of logs is highly challenging. To address this issue, many approaches
based on transfer learning and active learning have been proposed.
Nevertheless, their effectiveness is hindered by issues such as the gap between
source and target system data distributions and cold-start problems. In this
paper, we propose LogAction, a novel log-based anomaly detection model based on
active domain adaptation. LogAction integrates transfer learning and active
learning techniques. On one hand, it uses labeled data from a mature system to
train a base model, mitigating the cold-start issue in active learning. On the
other hand, LogAction utilize free energy-based sampling and uncertainty-based
sampling to select logs located at the distribution boundaries for manual
labeling, thus addresses the data distribution gap in transfer learning with
minimal human labeling efforts. Experimental results on six different
combinations of datasets demonstrate that LogAction achieves an average 93.01%
F1 score with only 2% of manual labels, outperforming some state-of-the-art
methods by 26.28%. Website: https://logaction.github.io

</details>


### [212] [Why mask diffusion does not work](https://arxiv.org/abs/2510.03289)
*Haocheng Sun,Cynthia Xin Wen,Edward Hong Wang*

Main category: cs.LG

TL;DR: Mask diffusion is limited for parallel generation and bidirectional attention; this work analyzes bottlenecks and proposes training/inference strategies to improve mask diffusion.


<details>
  <summary>Details</summary>
Motivation: Diffusion language models offer parallel generation and bidirectional context, advantages over autoregressive models. Mask diffusion (absorbing diffusion) has been popular in open-source implementations but struggles with key features, hindering practicality.

Method: Diagnostic analysis of absorbing/mask diffusion, empirical evaluation across representative tasks, and proposition of a comprehensive set of training and inference strategies to enhance mask diffusion performance.

Result: Identification of core bottlenecks in parallelism and bidirectional attention for mask diffusion; introduction of effective training and inference strategies; empirical results show improved stability and generation quality compared to baselines.

Conclusion: Tailored training and inference strategies can significantly mitigate the limitations of mask diffusion, bringing it closer to the capabilities of diffusion models with full bidirectional context and parallel generation; further work should refine these strategies and explore remaining gaps.

Abstract: The main advantages of diffusion language models over autoregressive (AR)
models lie in their ability to support parallel generation and bidirectional
attention, enabling a more controllable generation process. In recent years,
open-source mask diffusion language models have emerged, most of which are
based on a variant known as absorbing diffusion. However, this paper
demonstrates why mask diffusion faces inherent difficulties in achieving
parallel generation and bidirectional attention. We also propose the most
effective training and inference strategies for mask diffusion.

</details>


### [213] [Single-Core Superscalar Optimization of Clifford Neural Layers](https://arxiv.org/abs/2510.03290)
*X. Angelo Huang,Ruben Ciranni,Giovanni Spadaccini,Carla J. López Zurita*

Main category: cs.LG

TL;DR: Clifford neural layers are optimized to speed up inference while preserving E(n) and O(n) equivariances; achieved an average speedup of 21.35x over the baseline across eleven functions, with several runtimes matching or surpassing the original PyTorch implementation.


<details>
  <summary>Details</summary>
Motivation: There is growing interest in physics-inspired networks with equivariance. Clifford neural layers provide E(n) and O(n) equivariances, but the current implementations incur inefficiencies. This work analyzes and optimizes the inner computations to reduce overhead while maintaining correctness.

Method: The authors study the theoretical structure of Clifford algebras to remove redundant matrix allocations and computations, then apply established optimization techniques to Clifford convolutional layers, implementing the changes and validating correctness.

Result: An average speedup of 21.35x over the baseline across eleven functions; runtimes are comparable to and faster than the original PyTorch implementation in six cases; in the remaining cases, performance is in the same order of magnitude as the original library.

Conclusion: The optimizations achieve substantial acceleration of Clifford convolutional layers without affecting their equivariance properties, demonstrating practical viability for efficient equivariant networks in physical-sciences contexts.

Abstract: Within the growing interest in the physical sciences in developing networks
with equivariance properties, Clifford neural layers shine as one approach that
delivers $E(n)$ and $O(n)$ equivariances given specific group actions. In this
paper, we analyze the inner structure of the computation within Clifford
convolutional layers and propose and implement several optimizations to speed
up the inference process while maintaining correctness. In particular, we begin
by analyzing the theoretical foundations of Clifford algebras to eliminate
redundant matrix allocations and computations, then systematically apply
established optimization techniques to enhance performance further. We report a
final average speedup of 21.35x over the baseline implementation of eleven
functions and runtimes comparable to and faster than the original PyTorch
implementation in six cases. In the remaining cases, we achieve performance in
the same order of magnitude as the original library.

</details>


### [214] [UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs](https://arxiv.org/abs/2510.03291)
*Yizhuo Ding,Wanying Qu,Jiawei Geng,Wenqi Shao,Yanwei Fu*

Main category: cs.LG

TL;DR: A unified, post-training pruning framework (UniPruning) uses mirror-descent optimization to combine local saliency scoring with global coordination, enabling fast, weight-free pruning of LLMs into unstructured and N:M sparsity in one shot, with hardware-awareness and strong perplexity/zero-shot results.


<details>
  <summary>Details</summary>
Motivation: Pruning large language models is essential due to prohibitive compute and memory costs. Existing pruning methods struggle to balance efficiency and robustness: local layer-wise pruning can collapse at high sparsity, while global coordination methods incur expensive weight updates or constrain semi-structured formats. A fast, stable, scalable post-training pruning approach is needed that can produce masks for arbitrary sparsity levels in one shot and adapt to hardware constraints.

Method: UniPruning combines fast layer-wise saliency scoring with a lightweight global controller that allocates a single sparsity budget, using mirror descent-based optimization and no weight updates. It supports both unstructured and N:M pruning within one framework. After a brief calibration, it can generate pruning masks for arbitrary sparsity levels in one shot and adapt to hardware-aware constraints.

Result: Extensive experiments across multiple pretrained LLM families and standard benchmarks show that UniPruning delivers competitive or superior perplexity and zero-shot accuracy. Ablation studies confirm the importance of mirror descent and local saliency anchoring.

Conclusion: UniPruning offers an efficient, principled, and scalable solution for sparsifying large-scale LLMs, with code available for reproduction.

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks
but face prohibitive computational and memory costs. Pruning offers a promising
path by inducing sparsity while preserving architectural flexibility. However,
existing methods struggle to balance efficiency and robustness: local metric
approaches prune layer by layer but often collapse under high sparsity, whereas
global feedback methods enforce consistency at the cost of expensive weight
updates or restrictive semi-structured formats. We present UniPruning, a
unified post-training pruning framework that combines the speed of local
saliency metrics with the stability of global coordination, enabled by a mirror
descent based optimization, all without updating model weights. UniPruning
leverages fast layer-wise scoring and a lightweight global controller to
allocate a single sparsity budget, supporting both unstructured and
semi-structured N :M pruning within one framework. After a brief calibration,
it can generate pruning masks for arbitrary sparsity levels in one shot, and
adapts seamlessly to hardware-aware constraints. Extensive experiments on
multiple pretrained LLM families and standard benchmarks show that UniPruning
consistently delivers competitive or superior perplexity and zero-shot
accuracy. Ablation studies further highlight the importance of mirror descent
and local saliency anchoring. Overall, UniPruning provides an efficient,
principled, and scalable solution for sparsifying large-scale LLMs. Our code is
available at: https://github.com/RainbowQTT/UniPruning.

</details>


### [215] [From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing](https://arxiv.org/abs/2510.03293)
*Rana Shahout,Colin Cai,Yilun Du,Minlan Yu,Michael Mitzenmacher*

Main category: cs.LG

TL;DR: LASER is a plug-and-play inference-time routing technique for mixture-of-experts that balances per-device load without retraining, improving latency and throughput while maintaining near-original accuracy across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: In MoE models, conditional routing reduces training costs but creates inference memory and load imbalance across experts, causing latency, throughput, and cost inefficiencies on GPUs.

Method: LASER uses gate scores from a trained model to guide routing at inference time. If gate scores are sharply peaked, it routes to the strongest experts; if scores are broad, it expands the candidate set and routes to the least-loaded among them, without retraining or finetuning.

Result: Load balancing improves latency and throughput with negligible accuracy changes on evaluated datasets (ARC-Easy, ARC-Challenge, MMLU, GSM8K) across Mixtral-8x7B and DeepSeek-MoE-16b-chat.

Conclusion: LASER offers a ready-to-use solution for adaptive, load-balanced MoE inference that preserves accuracy while reducing resource waste and system latency.

Abstract: Mixture-of-Experts (MoE) models can scale parameter capacity by routing each
token to a subset of experts through a learned gate function. While conditional
routing reduces training costs, it shifts the burden on inference memory:
expert parameters and activations consume memory, limiting the number of
experts per device. As tokens are routed, some experts become overloaded while
others are underutilized. Because experts are mapped to GPUs, this imbalance
translates directly into degraded system performance in terms of latency,
throughput, and cost. We present LASER, a plug-and-play, inference-time routing
algorithm that balances load while preserving accuracy. LASER adapts to the
shape of the gate's score distribution. When scores provide a clear preference,
it routes to the strongest experts; when scores are more uniform, it broadens
the set of viable experts and routes to the least-loaded among them. Because
LASER relies only on gate scores from a trained model, it integrates directly
into existing MoE inference pipelines without retraining or finetuning. We
evaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets
(ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing,
translating into lower latency and higher throughput, while keeping the
accuracy changes negligible.

</details>


### [216] [CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models](https://arxiv.org/abs/2510.03298)
*Dongqi Zheng,Wenjin Fu*

Main category: cs.LG

TL;DR: Constraint-aware FedAvg (CAFL-L) uses Lagrangian dual optimization to adapt training hyperparameters under device budgets, reducing memory and communication while staying competitive in accuracy.


<details>
  <summary>Details</summary>
Motivation: Federated learning on edge devices must honor energy, memory, communication, and thermal limits; standard FedAvg lacks principled constraint handling.

Method: Formulate constraints via Lagrangian dual; dynamically adjust freezing depth, local steps, batch size, and communication compression; preserve token-budget with gradient accumulation to stabilize training.

Result: On a character-level language model, CAFL-L achieves better constraint satisfaction than FedAvg (≈20% memory reduction, ≈95% communication reduction) with competitive validation accuracy.

Conclusion: A principled framework enabling resource-constrained FL deployment; dual optimization enables adaptive hyperparameters while maintaining training stability and performance.

Abstract: We introduce Constraint-Aware Federated Learning with Lagrangian Dual
Optimization (CAFL-L), a principled extension of FedAvg that explicitly
incorporates device-level resource constraints including energy, communication,
memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to
dynamically adapt training hyperparameters -- freezing depth, local steps,
batch size, and communication compression -- while preserving training
stability through token-budget preservation via gradient accumulation.
Experiments on a character-level language model demonstrate that CAFL-L
achieves superior constraint satisfaction compared to standard FedAvg (reducing
memory usage by 20% and communication by 95%) while maintaining competitive
validation performance, making it practical for deployment on
resource-constrained edge devices.

</details>


### [217] [Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles](https://arxiv.org/abs/2510.03301)
*Arthur Sedek*

Main category: cs.LG

TL;DR: An adaptive ensemble framework that combines XGBoost and neural networks via meta-learning, incorporating uncertainty quantification and feature importance to dynamically select and integrate models, yielding better performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: To exploit complementary strengths of tree-based and neural models while addressing uncertainty and interpretability in ensemble learning.

Method: An adaptive ensemble framework that uses meta-learning to orchestrate model selection/combination between XGBoost and neural networks, integrating uncertainty quantification and feature importance to guide decisions, evaluated on diverse datasets.

Result: Reports superior predictive performance and enhanced interpretability across diverse datasets.

Conclusion: This approach advances intelligent, flexible ML systems by enabling adaptive, uncertainty-aware ensembles with interpretable insights.

Abstract: This paper introduces a novel adaptive ensemble framework that
synergistically combines XGBoost and neural networks through sophisticated
meta-learning. The proposed method leverages advanced uncertainty
quantification techniques and feature importance integration to dynamically
orchestrate model selection and combination. Experimental results demonstrate
superior predictive performance and enhanced interpretability across diverse
datasets, contributing to the development of more intelligent and flexible
machine learning systems.

</details>


### [218] [Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models](https://arxiv.org/abs/2510.03302)
*Daiheng Gao,Nanxiang Jiang,Andi Zhang,Shilin Lu,Yufei Tang,Wenbo Zhou,Weiming Zhang,Zhaoxin Fan*

Main category: cs.LG

TL;DR: Existing concept erasure in diffusion models appears only as a sampling bias that emulates forgetting (an illusion of amnesia) and is reversible. The paper proposes RevAm, an RL-based trajectory optimization framework that resurrects erased concepts during diffusion without changing model weights, achieving better restoration and faster performance than prior methods.


<details>
  <summary>Details</summary>
Motivation: Current erasure methods (e.g., ESD, UCE, AC) lose effectiveness on next-generation architectures and mainly create a superficial safety gate by biasing sampling trajectories. There is a need to distinguish true forgetting from superficial safety and to develop more robust, non-reversible erasure techniques.

Method: RevAm uses reinforcement learning to steer the denoising trajectories during diffusion. It adapts Group Relative Policy Optimization (GRPO) to diffusion models to optimize trajectory-level rewards, enabling exploration of diverse recovery paths. Importantly, it does not modify model weights, relying on trajectory manipulation to resurrect concepts.

Result: RevAm achieves superior concept resurrection fidelity and reduces computational time by about 10x compared to existing erasure methods, revealing vulnerabilities in current safety mechanisms that rely on trajectory bias rather than true forgetting.

Conclusion: Current concept erasure methods provide only illusionary amnesia and are reversible. RevAm demonstrates a robust RL-based trajectory optimization approach to resurrect erased concepts without weight changes, highlighting the need for fundamentally stronger erasure techniques beyond trajectory manipulation and prompting future work on more robust, non-reversible safety mechanisms.

Abstract: Concept erasure techniques have been widely deployed in T2I diffusion models
to prevent inappropriate content generation for safety and copyright
considerations. However, as models evolve to next-generation architectures like
Flux, established erasure methods (\textit{e.g.}, ESD, UCE, AC) exhibit
degraded effectiveness, raising questions about their true mechanisms. Through
systematic analysis, we reveal that concept erasure creates only an illusion of
``amnesia": rather than genuine forgetting, these methods bias sampling
trajectories away from target concepts, making the erasure fundamentally
reversible. This insight motivates the need to distinguish superficial safety
from genuine concept removal. In this work, we propose \textbf{RevAm}
(\underline{Rev}oking \underline{Am}nesia), an RL-based trajectory optimization
framework that resurrects erased concepts by dynamically steering the denoising
process without modifying model weights. By adapting Group Relative Policy
Optimization (GRPO) to diffusion models, RevAm explores diverse recovery
trajectories through trajectory-level rewards, overcoming local optima that
limit existing methods. Extensive experiments demonstrate that RevAm achieves
superior concept resurrection fidelity while reducing computational time by
10$\times$, exposing critical vulnerabilities in current safety mechanisms and
underscoring the need for more robust erasure techniques beyond trajectory
manipulation.

</details>


### [219] [Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies](https://arxiv.org/abs/2510.03305)
*Tian Zheng,Subashree Venkatasubramanian,Shuolin Li,Amy Braverman,Xinyi Ke,Zhewen Hou,Peter Jin,Samarth Sanjay Agrawal*

Main category: cs.LG

TL;DR: Synthesis of workflow design patterns in ML-enabled climate modeling, highlighting rigorous, transparent, and reproducible practices across surrogate modeling, ML parameterization, probabilistic programming, simulation-based inference, and physics-informed transfer learning.


<details>
  <summary>Details</summary>
Motivation: Address challenges in combining ML with climate science by distilling common design patterns and promoting rigorous workflow practices to facilitate interdisciplinary collaboration and reproducibility.

Method: Analyzes a series of case studies from applied ML in climate modeling, focusing on design choices and workflow structure rather than detailed technical methods.

Result: Identifies recurring workflow patterns and proposes a framework for rigorous scientific ML in climate modeling, with guidance on grounding in physical knowledge, using simulation data, and integrating observations.

Conclusion: Offers a framework to improve rigor, transparency, evaluation, adaptation, and reproducibility in ML-enabled climate modeling and to lower barriers to cross-disciplinary collaboration.

Abstract: Machine learning has been increasingly applied in climate modeling on system
emulation acceleration, data-driven parameter inference, forecasting, and
knowledge discovery, addressing challenges such as physical consistency,
multi-scale coupling, data sparsity, robust generalization, and integration
with scientific workflows. This paper analyzes a series of case studies from
applied machine learning research in climate modeling, with a focus on design
choices and workflow structure. Rather than reviewing technical details, we aim
to synthesize workflow design patterns across diverse projects in ML-enabled
climate modeling: from surrogate modeling, ML parameterization, probabilistic
programming, to simulation-based inference, and physics-informed transfer
learning. We unpack how these workflows are grounded in physical knowledge,
informed by simulation data, and designed to integrate observations. We aim to
offer a framework for ensuring rigor in scientific machine learning through
more transparent model development, critical evaluation, informed adaptation,
and reproducibility, and to contribute to lowering the barrier for
interdisciplinary collaboration at the interface of data science and climate
modeling.

</details>


### [220] [Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval](https://arxiv.org/abs/2510.03309)
*Mallikarjuna Tupakula*

Main category: cs.LG

TL;DR: Lightweight contrastive bridges align chemical and text embeddings without full multimodal models, achieving cross-modal alignment and improved target discrimination under scaffold splits.


<details>
  <summary>Details</summary>
Motivation: Reduce reliance on heavy multimodal pretraining; explore if small projection heads over frozen encoders can align modalities in drug discovery.

Method: Use paired ChEMBL mechanisms; align ECFP4 fingerprints with biomedical sentence embeddings via dual linear projections trained with a contrastive objective; include hard negative weighting and a margin loss; evaluate on scaffold-based splits.

Result: Demonstrates non-trivial cross-modal alignment and substantial improvement in within-target discrimination compared with frozen baselines.

Conclusion: Thin bridges offer a compute-efficient alternative to large-scale multimodal pretraining, enabling scaffold-aware drug-text alignment and target-specific retrieval for precision medicine.

Abstract: Multimodal foundation models hold promise for drug discovery and biomedical
applications, but most existing approaches rely on heavy pretraining or large
scale multimodal corpora. We investigate whether thin contrastive bridges,
lightweight projection heads over frozen unimodal encoders can align chemical
and textual representations without training a full multimodal model. Using
paired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints with
biomedical sentence embeddings through dual linear projections trained with a
contrastive objective. To better handle drugs sharing the same therapeutic
target, we incorporate hard negative weighting and a margin loss. Evaluation
under scaffold based splits, which require generalization across disjoint
chemical cores, demonstrates that our approach achieves non-trivial cross modal
alignment and substantially improves within target discrimination compared to
frozen baselines. These results suggest that thin bridges offer a compute
efficient alternative to large scale multimodal pretraining, enabling scaffold
aware drug text alignment and target specific retrieval in precision medicine.

</details>


### [221] [Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management](https://arxiv.org/abs/2510.03310)
*Runze Zhang,Xiaowei Zhang,Mingyang Zhao*

Main category: cs.LG

TL;DR: LLMs can reproduce many hypothesis-level effects in behavioral operations experiments but show distributional misalignment with human data; targeted prompting and hyperparameter tuning reduce misalignment and can enable smaller/open-source models to match or beat larger models.


<details>
  <summary>Details</summary>
Motivation: To assess whether large language models can serve as cost-effective substitutes for traditional experiments in operations management by replicating human decision-making and identifying the limits of their behavioral fidelity.

Method: Analyze nine published behavioral-operations experiments using LLMs, evaluating (1) replication of hypothesis-test outcomes and (2) distributional alignment via Wasserstein distance. Examine two lightweight interventions—chain-of-thought prompting and hyperparameter tuning—for their impact on alignment across models.

Result: LLMs reproduced most hypothesis-level effects and key decision biases, but their response distributions diverged from human data, even for strong commercial models. Chain-of-thought prompting and hyperparameter tuning reduced misalignment, and in some cases allowed smaller/open-source models to match or surpass larger systems.

Conclusion: LLMs show promise as lower-cost complements to laboratory and field experiments in operations management by capturing core behavioral effects, but distributional misalignment remains, especially for large models. Prompting strategies and careful hyperparameter tuning can improve fidelity and, in some cases, enable smaller models to outperform larger ones.

Abstract: LLMs are emerging tools for simulating human behavior in business, economics,
and social science, offering a lower-cost complement to laboratory experiments,
field studies, and surveys. This paper evaluates how well LLMs replicate human
behavior in operations management. Using nine published experiments in
behavioral operations, we assess two criteria: replication of hypothesis-test
outcomes and distributional alignment via Wasserstein distance. LLMs reproduce
most hypothesis-level effects, capturing key decision biases, but their
response distributions diverge from human data, including for strong commercial
models. We also test two lightweight interventions -- chain-of-thought
prompting and hyperparameter tuning -- which reduce misalignment and can
sometimes let smaller or open-source models match or surpass larger systems.

</details>


### [222] [Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining](https://arxiv.org/abs/2510.03313)
*Anirudh Subramanyam,Yuxin Chen,Robert L. Grossman*

Main category: cs.LG

TL;DR: Introduces a dimensionless data-quality parameter Q and a quality-aware scaling law for language model pretraining that jointly scales with model size, data volume, and data quality; shows higher-quality data reduces required compute and model size; proposes two estimators for Q and validates with synthetic experiments demonstrating robustness to moderate corruption and a sublinear effective data decay with quality.


<details>
  <summary>Details</summary>
Motivation: Current scaling laws focus on model size and data volume but do not formalize data quality. The work seeks a principled, generalizable law that accounts for data quality, motivated by effective-sample-size and information-theoretic views of noisy or redundant corpora.

Method: Define a dimensionless quality parameter Q and integrate it into a Chinchilla-like loss-scaling framework to predict loss as a joint function of model size, data volume, and data quality. Propose two practical estimators for Q: a corruption-rate proxy and a deficiency measure. Validate with synthetic experiments in neural machine translation and autoregressive modeling by injecting noise and controlling coverage to study how loss scales with data quality.

Result: Loss scales predictably with data quality; higher-quality data can substantially reduce required model size and compute. Observed sublinear decay of the effective data with quality and robustness to moderate data corruption. Out-of-sample evaluations support the predictive form of the law.

Conclusion: Provides an explicit, generalizable law for data quality that complements existing scaling laws, offering practical guidance on balancing data curation effort with model scale in large-scale pretraining.

Abstract: Scaling laws for language model training traditionally characterize how
performance scales with model size and dataset volume. Prior work has explored
architecture variants and data treatments such as dataset filtering and noise
injection in language model pretraining; however, these studies have not
formalized data quality within a principled scaling law. We introduce a
dimensionless data-quality parameter Q, and propose a quality-aware scaling law
extending the Chinchilla framework to predict loss as a joint function of model
size, data volume, and data quality. The law is motivated by an
effective-sample-size and information-theoretic view of noisy or redundant
corpora, and it admits two practical estimators for Q: (i) a corruption rate
proxy and (ii) a deficiency measure. Through synthetic experiments in neural
machine translation and autoregressive modeling -- where we systematically
control data quality via multiple levels of noise injection and coverage
variation -- we show that loss scales predictably with data quality and that
higher-quality data can substantially reduce model size and hence compute
requirements. Our results demonstrate a sublinear decay of effective data with
quality and robustness to moderate data corruption; out-of-sample evaluations
further validate the predictive form of the law. Unlike prior empirical
analyses, our work establishes an explicit, generalizable law for data quality,
offering concrete guidance for balancing data curation effort and model scale
in large-scale pretraining.

</details>


### [223] [Fast frequency reconstruction using Deep Learning for event recognition in ring laser data](https://arxiv.org/abs/2510.03325)
*Giuseppe Di Somma,Giorgio Carelli,Angela D. V. Di Virgilio,Francesco Fuso,Enrico Maccioni,Paolo Marsili*

Main category: cs.LG

TL;DR: Neural network method enables ~10 ms frequency reconstruction at hundreds of Hz for Ring Laser Gyros, outperforming FFT, plus automated disturbance classification with high seismic accuracy.


<details>
  <summary>Details</summary>
Motivation: Need rapid, accurate instantaneous frequency estimation and disturbance detection in ring laser gyroscopes to enable fast triggers and robust geophysical monitoring.

Method: Train a neural network to estimate instantaneous frequency from short signal windows; develop an automated classifier to identify physical disturbances (laser instabilities, seismic events).

Result: Frequency estimation precision doubles versus standard Fourier methods in the operational range; latency ~10 ms. Seismic disturbance classifier achieves 99–100% accuracy on independent test data.

Conclusion: Demonstrates the viability of integrating AI for fast, reliable signal analysis in geophysical sensor applications.

Abstract: The reconstruction of a frequency with minimal delay from a sinusoidal signal
is a common task in several fields; for example Ring Laser Gyroscopes, since
their output signal is a beat frequency. While conventional methods require
several seconds of data, we present a neural network approach capable of
reconstructing frequencies of several hundred Hertz within approximately 10
milliseconds. This enables rapid trigger generation. The method outperforms
standard Fourier-based techniques, improving frequency estimation precision by
a factor of 2 in the operational range of GINGERINO, our Ring Laser
Gyroscope.\\ In addition to fast frequency estimation, we introduce an
automated classification framework to identify physical disturbances in the
signal, such as laser instabilities and seismic events, achieving accuracy
rates between 99\% and 100\% on independent test datasets for the seismic
class. These results mark a step forward in integrating artificial intelligence
into signal analysis for geophysical applications.

</details>


### [224] [Constant in an Ever-Changing World](https://arxiv.org/abs/2510.03330)
*Andy Wu,Chun-Cheng Lin,Yuehua Huang,Rung-Tzuo Liaw*

Main category: cs.LG

TL;DR: CIC introduces a dual-policy reinforcement learning framework to stabilize training by maintaining a representative policy and a current policy; it updates the representative policy only when the current policy is superior and uses an adaptive mechanism to coordinate critic training, yielding improved performance on MuJoCo tasks with no extra compute.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning training often exhibits severe oscillations and instability, which degrade performance; there's a need for stable training dynamics without extra computational cost.

Method: Maintain two policies: representative and current. Do not blindly update the representative policy; update it selectively when the current policy outperforms it. Introduce an adaptive adjustment mechanism that allows the two policies to jointly guide critic training.

Result: Empirical evaluation on five MuJoCo environments shows that CIC improves the performance of conventional RL algorithms without increasing computational cost.

Conclusion: CIC provides a stable training framework that enhances performance by leveraging a dual-policy setup and adaptive critic-coordination, achieving gains without extra computation.

Abstract: The training process of reinforcement learning often suffers from severe
oscillations, leading to instability and degraded performance. In this paper,
we propose a Constant in an Ever-Changing World (CIC) framework that enhances
algorithmic stability to improve performance. CIC maintains both a
representative policy and a current policy. Instead of updating the
representative policy blindly, CIC selectively updates it only when the current
policy demonstrates superiority. Furthermore, CIC employs an adaptive
adjustment mechanism, enabling the representative and current policies to
jointly facilitate critic training. We evaluate CIC on five MuJoCo
environments, and the results show that CIC improves the performance of
conventional algorithms without incurring additional computational cost.

</details>


### [225] [Semantic-Aware Scheduling for GPU Clusters with Large Language Models](https://arxiv.org/abs/2510.03334)
*Zerui Wang,Qinghao Hu,Ana Klimovic,Tianwei Zhang,Yonggang Wen,Peng Sun,Dahua Lin*

Main category: cs.LG

TL;DR: SchedMate is a semantic-aware framework that augments DL schedulers by extracting insights from unstructured data (source code, runtime logs, historical jobs) via three LLM-based components; it integrates non-intrusively and delivers faster job completion on GPU clusters.


<details>
  <summary>Details</summary>
Motivation: DL schedulers rely on limited metadata and lack semantic context, causing high overhead, inaccurate duration estimates, poor failure handling, and poor observability; capturing semantic information from unstructured sources can improve scheduling decisions.

Method: Introduce SchedMate with three LLM-based modules that extract semantic signals from unstructured data (source code, runtime logs, historic jobs) and integrate non-intrusively with existing DL schedulers; evaluate on a 128-GPU cluster and production traces via simulations.

Result: Up to 1.91x reduction in average job completion time; substantial scheduling performance improvements demonstrated in real hardware and production-trace simulations.

Conclusion: Semantic-awareness is critical for modern DL scheduling; leveraging unstructured data with LLM-based components can substantially improve scheduling, with non-intrusive integration.

Abstract: Deep learning (DL) schedulers are pivotal in optimizing resource allocation
in GPU clusters, but operate with a critical limitation: they are largely blind
to the semantic context of the jobs they manage. This forces them to rely on
limited metadata, leading to high profiling overhead, unreliable duration
estimation, inadequate failure handling, and poor observability. To this end,
we propose SchedMate, a framework that bridges this semantic gap by
systematically extracting deep insights from overlooked, unstructured data
sources: source code, runtime logs, and historical jobs. SchedMate enhances
existing schedulers non-intrusively through three LLM-based components. Our
implementation integrates seamlessly with existing deep learning schedulers.
Evaluations on a 128-GPU physical cluster and extensive simulations on
production traces show SchedMate reduces average job completion times by up to
1.91x, substantially enhancing the scheduling performance, demonstrating the
critical role of semantic-awareness in modern DL scheduling.

</details>


### [226] [Matching the Optimal Denoiser in Point Cloud Diffusion with (Improved) Rotational Alignment](https://arxiv.org/abs/2510.03335)
*Ameya Daigavane,YuQing Xie,Bodhi P. Vani,Saeed Saremi,Joseph Kleinhenz,Tess Smidt*

Main category: cs.LG

TL;DR: Alignment in 3D diffusion training for molecules/proteins can be viewed as sampling the mode of a matrix Fisher distribution on SO(3); it's a good zeroth-order approximation at small noise, explaining why Kabsch alignment is effective; the paper derives better denoisers for small noise and finds alignment is often sufficient for training-level noise.


<details>
  <summary>Details</summary>
Motivation: Handle rotational symmetry in point-cloud diffusion models where there is no canonical orientation; standard practice augments with random SO(3) rotations and then aligns predictions to ground truth before loss computation, but the effect of this alignment on learning is unclear.

Method: Provide a probabilistic formulation by showing the optimal denoiser under rotational symmetry is a matrix Fisher distribution over SO(3); demonstrate that alignment corresponds to taking the mode of this distribution; derive improved denoisers for the small-noise limit; perform experiments to compare alignment vs. more exact methods across relevant noise levels.

Result: Alignment via Kabsch-Umeyama is a good-enough zeroth-order approximation for the noise levels that matter in training diffusion models; the matrix Fisher perspective explains why the standard alignment step works well and supports improvements via better small-noise denoisers.

Conclusion: A probabilistic view on rotational alignment clarifies why current data-augmentation+alignment pipelines work in diffusion modeling for 3D point clouds; alignment is often sufficient, and more accurate denoisers can be constructed for the small-noise regime to potentially squeeze further gains.

Abstract: Diffusion models are a popular class of generative models trained to reverse
a noising process starting from a target data distribution. Training a
diffusion model consists of learning how to denoise noisy samples at different
noise levels. When training diffusion models for point clouds such as molecules
and proteins, there is often no canonical orientation that can be assigned. To
capture this symmetry, the true data samples are often augmented by
transforming them with random rotations sampled uniformly over $SO(3)$. Then,
the denoised predictions are often rotationally aligned via the Kabsch-Umeyama
algorithm to the ground truth samples before computing the loss. However, the
effect of this alignment step has not been well studied. Here, we show that the
optimal denoiser can be expressed in terms of a matrix Fisher distribution over
$SO(3)$. Alignment corresponds to sampling the mode of this distribution, and
turns out to be the zeroth order approximation for small noise levels,
explaining its effectiveness. We build on this perspective to derive better
approximators to the optimal denoiser in the limit of small noise. Our
experiments highlight that alignment is often a `good enough' approximation for
the noise levels that matter most for training diffusion models.

</details>


### [227] [Deep Reinforcement Learning for Multi-Agent Coordination](https://arxiv.org/abs/2510.03592)
*Kehinde O. Aina,Sehoon Ha*

Main category: cs.LG

TL;DR: S-MADRL uses stigmergy-inspired virtual pheromones and curriculum learning to enable decentralized, scalable coordination of robots in confined spaces, outperforming classic MADRL methods by achieving emergent workload partitioning and reduced congestion.


<details>
  <summary>Details</summary>
Motivation: Address congestion and interference in narrow environments and the need for scalable, communication-efficient multi-robot coordination. Draws on insect stigmergy to design local interaction rules that yield robust global coordination.

Method: Propose Stigmergic Multi-Agent Deep Reinforcement Learning (S-MADRL) that models local/social interactions with virtual pheromone traces, enabling decentralized coordination without explicit inter-agent communication. Employ curriculum learning to decompose complex tasks into progressively harder sub-problems and improve convergence and scalability.

Result: In simulation, the framework achieves effective coordination for up to eight agents, with robots self-organizing into asymmetric workload distributions that reduce congestion and modulate overall group performance.

Conclusion: S-MADRL offers a scalable, decentralized coordination solution for crowded environments with communication constraints, leveraging stigmergy-inspired traces and curriculum learning to overcome convergence and scalability limitations of existing MADRL approaches.

Abstract: We address the challenge of coordinating multiple robots in narrow and
confined environments, where congestion and interference often hinder
collective task performance. Drawing inspiration from insect colonies, which
achieve robust coordination through stigmergy -- modifying and interpreting
environmental traces -- we propose a Stigmergic Multi-Agent Deep Reinforcement
Learning (S-MADRL) framework that leverages virtual pheromones to model local
and social interactions, enabling decentralized emergent coordination without
explicit communication. To overcome the convergence and scalability limitations
of existing algorithms such as MADQN, MADDPG, and MAPPO, we leverage curriculum
learning, which decomposes complex tasks into progressively harder
sub-problems. Simulation results show that our framework achieves the most
effective coordination of up to eight agents, where robots self-organize into
asymmetric workload distributions that reduce congestion and modulate group
performance. This emergent behavior, analogous to strategies observed in
nature, demonstrates a scalable solution for decentralized multi-agent
coordination in crowded environments with communication constraints.

</details>


### [228] [Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models](https://arxiv.org/abs/2510.03339)
*Sofiane Ennadir,Levente Zólyomi,Oleg Smirnov,Tianze Wang,John Pertoft,Filip Cornell,Lele Cao*

Main category: cs.LG

TL;DR: Pooling in Transformers critically shapes expressivity and performance; the paper provides closed-form bounds on representational capacity for common pooling methods, showing pooling choices impact accuracy and optimization across CV, NLP, and time-series, offering principled guidance for design beyond attention.


<details>
  <summary>Details</summary>
Motivation: Pooling is underexplored yet influential in Transformer behavior. A theoretical link between pooling, expressivity, and input distinguishability is needed, across diverse attention variants, to unify theory with empirical evidence.

Method: Derive closed-form bounds on expressivity and input-distinguishability for Transformer outputs with standard pooling operations; extend analysis across different attention formulations; validate findings with empirical experiments on tasks spanning computer vision, natural language processing, and time-series analysis.

Result: Bounds on expressivity hold across attention variants; pooling strategies systematically affect accuracy, sensitivity, and optimization dynamics; empirical trends align with theoretical predictions across modalities, enabling practical guidance on pooling choices.

Conclusion: Pooling is a central architectural component in Transformer models. Theoretical insights combined with empirical evidence provide a foundation for principled pooling design and task-specific architectural choices beyond attention mechanisms.

Abstract: Transformer models have become the dominant backbone for sequence modeling,
leveraging self-attention to produce contextualized token representations.
These are typically aggregated into fixed-size vectors via pooling operations
for downstream tasks. While much of the literature has focused on attention
mechanisms, the role of pooling remains underexplored despite its critical
impact on model behavior. In this paper, we introduce a theoretical framework
that rigorously characterizes the expressivity of Transformer-based models
equipped with widely used pooling methods by deriving closed-form bounds on
their representational capacity and the ability to distinguish similar inputs.
Our analysis extends to different variations of attention formulations,
demonstrating that these bounds hold across diverse architectural variants. We
empirically evaluate pooling strategies across tasks requiring both global and
local contextual understanding, spanning three major modalities: computer
vision, natural language processing, and time-series analysis. Results reveal
consistent trends in how pooling choices affect accuracy, sensitivity, and
optimization behavior. Our findings unify theoretical and empirical
perspectives, providing practical guidance for selecting or designing pooling
mechanisms suited to specific tasks. This work positions pooling as a key
architectural component in Transformer models and lays the foundation for more
principled model design beyond attention alone.

</details>


### [229] [Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.03823)
*Adam Haroon,Tristan Schuler*

Main category: cs.LG

TL;DR: MARL with QMIX applied to HAB coordination matches optimal deterministic coverage for distributed area coverage in realistic atmospheric conditions, enabling scalable multi-HAB missions.


<details>
  <summary>Details</summary>
Motivation: Existing deterministic coordination methods (e.g., Voronoi partitioning, extremum seeking) scale poorly for small teams and localized HAB missions. While single-agent HAB control with RL has been explored, coordinated multi-agent RL (MARL) for HABs had not been systematically studied.

Method: Extend the RLHAB simulation environment to support cooperative MARL, implement QMIX with Centralized Training and Decentralized Execution, and design specialized observation spaces that include individual state, environmental context, and teammate data. Use hierarchical rewards to prioritize coverage while encouraging spatial distribution in realistic atmospheric conditions.

Result: QMIX achieves performance comparable to the theoretically optimal geometric deterministic method for distributed area coverage, validating the MARL approach and establishing a foundation for more complex multi-HAB missions where deterministic methods become intractable.

Conclusion: This work represents the first systematic application of MARL to HAB coordination for distributed area coverage, demonstrating viability and paving the way for future multi-HAB missions requiring scalable, intelligent coordination in realistic stratospheric environments.

Abstract: High Altitude Balloons (HABs) can leverage stratospheric wind layers for
limited horizontal control, enabling applications in reconnaissance,
environmental monitoring, and communications networks. Existing multi-agent HAB
coordination approaches use deterministic methods like Voronoi partitioning and
extremum seeking control for large global constellations, which perform poorly
for smaller teams and localized missions. While single-agent HAB control using
reinforcement learning has been demonstrated on HABs, coordinated multi-agent
reinforcement learning (MARL) has not yet been investigated. This work presents
the first systematic application of multi-agent reinforcement learning (MARL)
to HAB coordination for distributed area coverage. We extend our previously
developed reinforcement learning simulation environment (RLHAB) to support
cooperative multi-agent learning, enabling multiple agents to operate
simultaneously in realistic atmospheric conditions. We adapt QMIX for HAB area
coverage coordination, leveraging Centralized Training with Decentralized
Execution to address atmospheric vehicle coordination challenges. Our approach
employs specialized observation spaces providing individual state,
environmental context, and teammate data, with hierarchical rewards
prioritizing coverage while encouraging spatial distribution. We demonstrate
that QMIX achieves similar performance to the theoretically optimal geometric
deterministic method for distributed area coverage, validating the MARL
approach and providing a foundation for more complex autonomous multi-HAB
missions where deterministic methods become intractable.

</details>


### [230] [Learning Pareto-Optimal Pandemic Intervention Policies with MORL](https://arxiv.org/abs/2510.03340)
*Marian Chen,Miri Zilka*

Main category: cs.LG

TL;DR: A MORL framework with a novel stochastic-differential-equation pandemic simulator to balance epidemiological control with economic stability; demonstrates Pareto-optimal trade-offs for COVID-19 and generalizes to polio, influenza, and measles.


<details>
  <summary>Details</summary>
Motivation: There is an urgent need for intervention strategies that simultaneously contain disease spread and maintain socioeconomic stability during pandemics. Existing models often rely on simplistic or narrow simulators, limiting policy insight. A high-fidelity, adaptable framework is required to compare trade-offs and support evidence-based policymaking.

Method: Develop a multi-objective reinforcement learning framework (MORL) paired with a new SDE-based pandemic simulator calibrated to global COVID-19 data. Train a Pareto-Conditioned Network (PCN) to learn policies across the Pareto front, illustrating trade-offs between epidemiological control and economic stability. Demonstrate generality by applying to pathogens with different profiles (polio, influenza) and a measles outbreak scenario to study vaccination coverage effects.

Result: The approach reveals clear policy trade-offs between disease containment and economic impact. The high-fidelity simulator yields insights not captured by simpler models, and the PCN learns policies that vary with pathogen profile. A 5% reduction in vaccination coverage for measles necessitates significantly more stringent and costly interventions, underscoring the framework’s utility for robust policymaking and preparedness across diseases.

Conclusion: This framework provides a robust, adaptable, and transparent tool for evidence-based policymaking in public health crises, generalizable across pathogens and capable of quantifying intervention costs under varying epidemiological conditions.

Abstract: The COVID-19 pandemic underscored a critical need for intervention strategies
that balance disease containment with socioeconomic stability. We approach this
challenge by designing a framework for modeling and evaluating disease-spread
prevention strategies. Our framework leverages multi-objective reinforcement
learning (MORL) - a formulation necessitated by competing objectives - combined
with a new stochastic differential equation (SDE) pandemic simulator,
calibrated and validated against global COVID-19 data. Our simulator reproduces
national-scale pandemic dynamics with orders of magnitude higher fidelity than
other models commonly used in reinforcement learning (RL) approaches to
pandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on
this simulator, we illustrate the direct policy trade-offs between
epidemiological control and economic stability for COVID-19. Furthermore, we
demonstrate the framework's generality by extending it to pathogens with
different epidemiological profiles, such as polio and influenza, and show how
these profiles lead the agent to discover fundamentally different intervention
policies. To ground our work in contemporary policymaking challenges, we apply
the model to measles outbreaks, quantifying how a modest 5% drop in vaccination
coverage necessitates significantly more stringent and costly interventions to
curb disease spread. This work provides a robust and adaptable framework to
support transparent, evidence-based policymaking for mitigating public health
crises.

</details>


### [231] [Pilot selection in the era of Virtual reality: algorithms for accurate and interpretable machine learning models](https://arxiv.org/abs/2510.03345)
*Luoma Ke,Guangpeng Zhang,Jibo He,Yajing Li,Yan Li,Xufeng Liu,Peng Fang*

Main category: cs.LG

TL;DR: The study uses VR and machine learning to distinguish pilots from novices; SVM with MIC feature selection achieves top performance (Accuracy 0.93, AUC 0.96, F1 0.93) and outperforms alternatives, suggesting potential for eye-tracking/flight dynamics–based pilot screening and training.


<details>
  <summary>Details</summary>
Motivation: Rapidly growing aviation industry creates a need for cost-efficient, accurate pilot selection; current approaches may be limited in scalability and effectiveness.

Method: Participants: 23 trained pilots from China Eastern Airlines and 23 novices from Tsinghua University community. A VR-based simulation plus machine learning framework was used, incorporating eye-tracking and flight dynamics data. Fifteen (or more) classifiers and two feature-selection methods were evaluated; the proposed approach is SVM with MIC feature selection.

Result: SVM with MIC achieved the best performance across metrics: Accuracy 0.93, AUC 0.96, F1 0.93, outperforming four other classifiers and two other feature-selection methods. MIC can capture nonlinear relationships between features and labels.

Conclusion: The SVM+MIC combination may outperform existing pilot-selection algorithms and represents a novel implementation based on eye-tracking and flight dynamics data; VR platforms and methods could be repurposed for pilot selection and initial training.

Abstract: With the rapid growth of the aviation industry, there is a need for a large
number of flight crew. How to select the right pilots in a cost-efficient
manner has become an important research question. In the current study,
twenty-three pilots were recruited from China Eastern Airlines, and 23 novices
were from the community of Tsinghua University. A novel approach incorporating
machine learning and virtual reality technology was applied to distinguish
features between these participants with different flight skills. Results
indicate that SVM with the MIC feature selection method consistently achieved
the highest prediction performance on all metrics with an Accuracy of 0.93, an
AUC of 0.96, and an F1 of 0.93, which outperforms four other classifier
algorithms and two other feature selection methods. From the perspective of
feature selection methods, the MIC method can select features with a nonlinear
relationship to sampling labels, instead of a simple filter-out. Our new
implementation of the SVM + MIC algorithm outperforms all existing pilot
selection algorithms and perhaps provides the first implementation based on eye
tracking and flight dynamics data. This study's VR simulation platforms and
algorithms can be used for pilot selection and training.

</details>


### [232] [KVComm: Enabling Efficient LLM Communication through Selective KV Sharing](https://arxiv.org/abs/2510.03346)
*Xiangyu Shi,Marco Chiesa,Gerald Q. Maguire Jr.,Dejan Kostic*

Main category: cs.LG

TL;DR: KVComm introduces a selective KV-pair communication framework for inter-LLM interaction, achieving near upper-bound performance with only 30% of layers' KV pairs transmitted.


<details>
  <summary>Details</summary>
Motivation: Existing inter-LLM communication methods rely on natural language (high cost and information loss) or hidden states (information concentration bias and inefficiency). KVComm aims to exploit structured KV pairs to efficiently convey important information without the drawbacks of the two extremes.

Method: KVComm uses a KV layer-wise selection strategy that scores the importance of KV pairs via attention-based metrics with a Gaussian prior to identify the most informative KV pairs to communicate across LLMs.

Result: Experiments across diverse tasks and model pairs show KVComm matches the upper-bound method (directly merging inputs to a single model) while transmitting only about 30% of layers’ KV pairs.

Conclusion: KV pairs can serve as an effective and scalable medium for inter-LLM communication, enabling efficient multi-agent systems without sacrificing performance.

Abstract: Large Language Models (LLMs) are increasingly deployed in multi-agent
systems, where effective inter-model communication is crucial. Existing
communication protocols either rely on natural language, incurring high
inference costs and information loss, or on hidden states, which suffer from
information concentration bias and inefficiency. To address these limitations,
we propose KVComm, a novel communication framework that enables efficient
communication between LLMs through selective sharing of KV pairs. KVComm
leverages the rich information encoded in the KV pairs while avoiding the
pitfalls of hidden states. We introduce a KV layer-wise selection strategy
based on attention importance scores with a Gaussian prior to identify the most
informative KV pairs for communication. Extensive experiments across diverse
tasks and model pairs demonstrate that KVComm achieves comparable performance
to the upper-bound method, which directly merges inputs to one model without
any communication, while transmitting as few as 30\% of layers' KV pairs. Our
study highlights the potential of KV pairs as an effective medium for inter-LLM
communication, paving the way for scalable and efficient multi-agent systems.

</details>


### [233] [AgentCaster: Reasoning-Guided Tornado Forecasting](https://arxiv.org/abs/2510.03349)
*Michael Chen*

Main category: cs.LG

TL;DR: AgentCaster introduces a contamination-free, end-to-end multimodal LLM framework for tornado forecasting, reveals human forecasters outperform LLMs and current models tend to hallucinate; it also introduces TornadoBench and TornadoHallucination metrics.


<details>
  <summary>Details</summary>
Motivation: There is a pressing need to evaluate LLMs on complex, high‑impact, real-world reasoning tasks and to measure their ability to handle heterogeneous spatiotemporal data. Existing work shows LLMs struggle with hallucinations, geographic placement, and spatiotemporal reasoning; domain-specific benchmarks are lacking.

Method: AgentCaster ingests heterogeneous spatiotemporal data from a high-resolution forecast archive and uses multimodal LLMs end-to-end. Over 40 days with diverse historical data and ~500 tornado reports, models query from 3,625 forecast maps and 40,125 forecast soundings for a 12–36 h horizon to produce probabilistic tornado-risk polygons. Ground-truth verification uses geometric comparisons across disjoint risk bands. The study introduces domain-specific TornadoBench and TornadoHallucination metrics and compares LLMs with domain experts.

Result: Human forecasters significantly outperform state-of-the-art models. LLMs exhibit hallucination, overprediction of risk intensity, and struggle with precise geographic placement and spatiotemporal reasoning in this complex, dynamic system. TornadoBench is highly challenging for both LLMs and domain experts; models reveal notable weaknesses.

Conclusion: AgentCaster offers a contamination-free, end-to-end framework for evaluating LLMs on high-stakes, real-world reasoning tasks and establishes benchmarks to guide future improvements in LLM-based reasoning in critical domains.

Abstract: There is a growing need to evaluate Large Language Models (LLMs) on complex,
high-impact, real-world tasks to assess their true readiness as reasoning
agents. To address this gap, we introduce AgentCaster, a contamination-free
framework employing multimodal LLMs end-to-end for the challenging,
long-horizon task of tornado forecasting. Within AgentCaster, models interpret
heterogeneous spatiotemporal data from a high-resolution convection-allowing
forecast archive. We assess model performance over a 40-day period featuring
diverse historical data, spanning several major tornado outbreaks and including
over 500 tornado reports. Each day, models query interactively from a pool of
3,625 forecast maps and 40,125 forecast soundings for a forecast horizon of
12-36 hours. Probabilistic tornado-risk polygon predictions are verified
against ground truths derived from geometric comparisons across disjoint risk
bands in projected coordinate space. To quantify accuracy, we propose
domain-specific TornadoBench and TornadoHallucination metrics, with
TornadoBench highly challenging for both LLMs and domain expert human
forecasters. Notably, human experts significantly outperform state-of-the-art
models, which demonstrate a strong tendency to hallucinate and overpredict risk
intensity, struggle with precise geographic placement, and exhibit poor
spatiotemporal reasoning in complex, dynamically evolving systems. AgentCaster
aims to advance research on improving LLM agents for challenging reasoning
tasks in critical domains.

</details>


### [234] [A KL-regularization framework for learning to plan with adaptive priors](https://arxiv.org/abs/2510.04280)
*Álvaro Serra-Gomez,Daniel Jarne Ornia,Dhruva Tirumala,Thomas Moerland*

Main category: cs.LG

TL;DR: A unified KL-regularized MPPI-based RL framework (PO-MPC) that treats the planner's action distribution as a prior in policy optimization; shows previous approaches are special cases and yields improved performance on high-dimensional control tasks.


<details>
  <summary>Details</summary>
Motivation: To improve sample efficiency and long-horizon performance in MPPI-based model-based RL by aligning the learned policy with the planner's distribution, mitigating distribution shift between training and planner execution.

Method: Introduce PO-MPC, a family of KL-regularized policy optimization methods that incorporate the planner's action distribution as a prior; allow trade-off between return maximization and KL divergence; show previous approaches are special cases; analyze variations; Empirical evaluation demonstrates improvements.

Result: Extended PO-MPC configurations achieve significant performance gains and advance state of the art in MPPI-based RL on high-dimensional tasks.

Conclusion: PO-MPC provides a unifying framework for MPPI-based RL, enabling flexible policy updates via KL regularization and leading to improved sample efficiency and performance; invites exploration of additional variations.

Abstract: Effective exploration remains a central challenge in model-based
reinforcement learning (MBRL), particularly in high-dimensional continuous
control tasks where sample efficiency is crucial. A prominent line of recent
work leverages learned policies as proposal distributions for Model-Predictive
Path Integral (MPPI) planning. Initial approaches update the sampling policy
independently of the planner distribution, typically maximizing a learned value
function with deterministic policy gradient and entropy regularization.
However, because the states encountered during training depend on the MPPI
planner, aligning the sampling policy with the planner improves the accuracy of
value estimation and long-term performance. To this end, recent methods update
the sampling policy by minimizing KL divergence to the planner distribution or
by introducing planner-guided regularization into the policy update. In this
work, we unify these MPPI-based reinforcement learning methods under a single
framework by introducing Policy Optimization-Model Predictive Control (PO-MPC),
a family of KL-regularized MBRL methods that integrate the planner's action
distribution as a prior in policy optimization. By aligning the learned policy
with the planner's behavior, PO-MPC allows more flexibility in the policy
updates to trade off Return maximization and KL divergence minimization. We
clarify how prior approaches emerge as special cases of this family, and we
explore previously unstudied variations. Our experiments show that these
extended configurations yield significant performance improvements, advancing
the state of the art in MPPI-based RL.

</details>


### [235] [Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks](https://arxiv.org/abs/2510.03351)
*Song Wang,Zhenyu Lei,Zhen Tan,Jundong Li,Javier Rasero,Aiying Zhang,Chirag Agarwal*

Main category: cs.LG

TL;DR: A concept-based framework (CONCEPTNEURO) uses LLMs and neurobiological knowledge to generate interpretable functional-connectivity subgraphs for rs-fMRI, integrated with GNNs to improve psychiatric disorder diagnosis and interpretation.


<details>
  <summary>Details</summary>
Motivation: There is a need for accurate and interpretable diagnostic tools for adolescent mental health disorders. rs-fMRI captures functional connectivity biomarkers, but existing GNN-based models are often black boxes that limit clinical translation.

Method: Automatically generate, filter, and encode interpretable connectivity concepts as structured subgraphs linking brain regions using large language models and domain knowledge; feed these concepts into a concept classifier and integrate with graph neural networks for prediction and interpretability.

Result: CONCEPTNEURO-augmented GNNs consistently outperform vanilla GNNs across multiple psychiatric disorder datasets, achieving higher accuracy while providing transparent, clinically aligned explanations; concept analyses reveal disorder-specific connectivity patterns that align with expert knowledge and suggest new hypotheses.

Conclusion: The framework offers an interpretable, domain-informed approach for psychiatric disorder diagnosis, enabling reliable predictions and insights that can guide clinical translation and future research.

Abstract: Nearly one in five adolescents currently live with a diagnosed mental or
behavioral health condition, such as anxiety, depression, or conduct disorder,
underscoring the urgency of developing accurate and interpretable diagnostic
tools. Resting-state functional magnetic resonance imaging (rs-fMRI) provides a
powerful lens into large-scale functional connectivity, where brain regions are
modeled as nodes and inter-regional synchrony as edges, offering clinically
relevant biomarkers for psychiatric disorders. While prior works use graph
neural network (GNN) approaches for disorder prediction, they remain complex
black-boxes, limiting their reliability and clinical translation. In this work,
we propose CONCEPTNEURO, a concept-based diagnosis framework that leverages
large language models (LLMs) and neurobiological domain knowledge to
automatically generate, filter, and encode interpretable functional
connectivity concepts. Each concept is represented as a structured subgraph
linking specific brain regions, which are then passed through a concept
classifier. Our design ensures predictions through clinically meaningful
connectivity patterns, enabling both interpretability and strong predictive
performance. Extensive experiments across multiple psychiatric disorder
datasets demonstrate that CONCEPTNEURO-augmented GNNs consistently outperform
their vanilla counterparts, improving accuracy while providing transparent,
clinically aligned explanations. Furthermore, concept analyses highlight
disorder-specific connectivity patterns that align with expert knowledge and
suggest new hypotheses for future investigation, establishing CONCEPTNEURO as
an interpretable, domain-informed framework for psychiatric disorder diagnosis.

</details>


### [236] [High Cycle S-N curve prediction for Al 7075-T6 alloy using Recurrent Neural Networks (RNNs)](https://arxiv.org/abs/2510.03355)
*Aryan Patel*

Main category: cs.LG

TL;DR: Transfer-learning LSTM trained on axial fatigue data for Aluminum 7075-T6 predicts high-cycle torsional S-N curves, reducing data requirements and potentially lowering fatigue-test costs.


<details>
  <summary>Details</summary>
Motivation: Fatigue testing is time- and cost-intensive, especially for high-cycle regimes. A data-efficient approach using transfer learning could enable accurate torque- and cycle-based fatigue predictions without exhaustive torsional testing.

Method: Train a source LSTM model on pure axial fatigue data for Aluminum 7075-T6, then transfer the learned representation to predict high-cycle torsional S-N curves.

Result: The framework accurately predicts torsional S-N curves in the high-cycle range, suggesting good transferability from axial to torsional fatigue data for this alloy.

Conclusion: The approach could significantly reduce fatigue-characterization costs and guide test prioritization across materials, though further validation on data diversity, uncertainty, and cross-material generalization is needed.

Abstract: Aluminum is a widely used alloy, which is susceptible to fatigue failure.
Characterizing fatigue performance for materials is extremely time and cost
demanding, especially for high cycle data. To help mitigate this, a transfer
learning based framework has been developed using Long short-term memory
networks (LSTMs) in which a source LSTM model is trained based on pure axial
fatigue data for Aluminum 7075-T6 alloy which is then transferred to predict
high cycle torsional S-N curves. The framework was able to accurately predict
Al torsional S-N curves for a much higher cycle range. It is the belief that
this framework will help to drastically mitigate the cost of gathering fatigue
characteristics for different materials and help prioritize tests with better
cost and time constraints.

</details>


### [237] [Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility](https://arxiv.org/abs/2510.03358)
*Annan Yu,Danielle C. Maddix,Boran Han,Xiyuan Zhang,Abdul Fatir Ansari,Oleksandr Shchur,Christos Faloutsos,Andrew Gordon Wilson,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: Time-series Transformers exhibit strongly decaying spectra, enabling low-rank approximations of Q/K/V and compressible attention; early layers are more amenable to compression due to flow-of-ranks; applying these insights to Chronos yields large speed/memory gains with no accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To understand why text-model priors transfer imperfectly to time-series data and to reveal the rank structure of Transformers in the time-series domain. The goal is to quantify compressibility and inform architecture decisions (width, depth, heads) for time-series foundation models.

Method: The authors analyze the singular value spectrum of time-series embeddings, prove low-rank approximations for Q/K/V, and show attention compressibility linked to spectrum decay. They introduce flow-of-ranks (depth-induced rank inflation) and validate predictions empirically on Chronos, a large time-series foundation model, achieving significant hardware-efficient compression.

Result: Theoretical results establish accurate low-rank models for Q/K/V and compressible attention in time-series Transformers. Empirically, Chronos was compressed by 65% in inference time and 81% in memory without accuracy loss, with guidance for allocating width, depth, and heads in time-series models.

Conclusion: Time-series Transformers possess inherent compressibility driven by their rank structure. The work provides principled guidance for architecture design and resource allocation in time-series foundation models, leveraging rank-based compression to improve efficiency without sacrificing performance.

Abstract: Transformers are widely used across data modalities, and yet the principles
distilled from text models often transfer imperfectly to models trained to
other modalities. In this paper, we analyze Transformers through the lens of
rank structure. Our focus is on the time series setting, where the structural
properties of the data differ remarkably from those of text or vision. We show
that time-series embeddings, unlike text or vision, exhibit sharply decaying
singular value spectra: small patch sizes and smooth continuous mappings
concentrate the data into low-rank subspaces. From this, we prove that the
associated $Q/K/V$ projections admit accurate low-rank approximations, and that
attention layers become compressible in proportion to the decay of the
embedding spectrum. We introduce the concept of flow-of-ranks, a phenomenon by
which nonlinear mixing across depth inflates the rank, explaining why early
layers are most amenable to compression and why ranks grow with depth. Guided
by these theoretical and empirical results, we use these insights to compress
Chronos, a large time series foundation model, achieving a reduction of $65\%$
in inference time and $81\%$ in memory, without loss of accuracy. Our findings
provide principled guidance for allocating width, depth, and heads in time
series foundation models, and for exploiting their inherent compressibility.

</details>


### [238] [Physics-informed Neural-operator Predictive Control for Drag Reduction in Turbulent Flows](https://arxiv.org/abs/2510.03360)
*Zelin Zhao,Zongyi Li,Kimia Hassibi,Kamyar Azizzadenesheli,Junchi Yan,H. Jane Bae,Di Zhou,Anima Anandkumar*

Main category: cs.LG

TL;DR: A model-based RL framework (PINO-PC) using Physics-Informed Neural Operators for turbulent-flow control, achieving high Reynolds-number drag reduction and outperforming model-free RL.


<details>
  <summary>Details</summary>
Motivation: Efficiently model and control turbulence to reduce wall friction without expensive DNS/LES; overcome limitations of model-free RL by joint learning of policy and observer via discretization-invariant PINOs.

Method: Train a joint policy and observer in a model-based RL loop using PINO to learn turbulence dynamics and control law; discretization-invariant, captures fine scales; evaluate on unseen high-Re flows.

Result: PINO-PC outperforms prior model-free RL in challenging high-Reynolds scenarios; 39.0% drag reduction at Re=15,000, surpassing previous fluid-control methods by >32%.

Conclusion: The PINO-based model-based RL provides efficient, generalizable turbulence control with significant drag reductions, suggesting a promising direction beyond model-free RL and expensive simulations.

Abstract: Assessing turbulence control effects for wall friction numerically is a
significant challenge since it requires expensive simulations of turbulent
fluid dynamics. We instead propose an efficient deep reinforcement learning
(RL) framework for modeling and control of turbulent flows. It is model-based
RL for predictive control (PC), where both the policy and the observer models
for turbulence control are learned jointly using Physics Informed Neural
Operators (PINO), which are discretization invariant and can capture fine
scales in turbulent flows accurately. Our PINO-PC outperforms prior model-free
reinforcement learning methods in various challenging scenarios where the flows
are of high Reynolds numbers and unseen, i.e., not provided during model
training. We find that PINO-PC achieves a drag reduction of 39.0\% under a
bulk-velocity Reynolds number of 15,000, outperforming previous fluid control
methods by more than 32\%.

</details>


### [239] [Estimating link level traffic emissions: enhancing MOVES with open-source data](https://arxiv.org/abs/2510.03362)
*Lijiao Wang,Muhammad Usama,Haris N. Koutsopoulos,Zhengbing He*

Main category: cs.LG

TL;DR: Open-source data plus a neural network predicts MOVES operating mode distribution to estimate regional emissions; achieves >50% RMSE reduction vs MOVES baseline in 45 Boston-area municipalities.


<details>
  <summary>Details</summary>
Motivation: Enable scalable, transparent, low-cost emissions estimation using open data; improve accuracy at regional scale while reducing reliance on proprietary tools.

Method: Integrate MOVES with open GPS trajectories, OpenStreetMap road networks, regional traffic datasets, and satellite imagery features; train a neural network to predict the distribution of MOVES-defined operating modes from data-derived features; establish ground truth from OSM GPS trajectories; apply to 45 municipalities in the Boston metropolitan area.

Result: The model reduces RMSE by over 50% for key pollutants (CO, NOx, CO2, PM2.5) compared with the MOVES baseline in regional-scale emissions estimation.

Conclusion: Demonstrates feasibility of a low-cost, replicable, data-driven emissions estimation framework using fully open data sources at regional scales.

Abstract: Open-source data offers a scalable and transparent foundation for estimating
vehicle activity and emissions in urban regions. In this study, we propose a
data-driven framework that integrates MOVES and open-source GPS trajectory
data, OpenStreetMap (OSM) road networks, regional traffic datasets and
satellite imagery-derived feature vectors to estimate the link level operating
mode distribution and traffic emissions. A neural network model is trained to
predict the distribution of MOVES-defined operating modes using only features
derived from readily available data. The proposed methodology was applied using
open-source data related to 45 municipalities in the Boston Metropolitan area.
The "ground truth" operating mode distribution was established using OSM
open-source GPS trajectories. Compared to the MOVES baseline, the proposed
model reduces RMSE by over 50% for regional scale traffic emissions of key
pollutants including CO, NOx, CO2, and PM2.5. This study demonstrates the
feasibility of low-cost, replicable, and data-driven emissions estimation using
fully open data sources.

</details>


### [240] [Diffusion-Based, Data-Assimilation-Enabled Super-Resolution of Hub-height Winds](https://arxiv.org/abs/2510.03364)
*Xiaolong Ma,Xu Dong,Ashley Tarrant,Lei Yang,Rao Kotamarthi,Jiali Wang,Feng Yan,Rajkumar Kettimuthu*

Main category: cs.LG

TL;DR: A diffusion-based super-resolution downscaling method (WindSR) that fuses sparse observations with coarse wind simulations using dynamic-radius blending and terrain-aware conditioning to produce high-resolution hub-height wind fields, outperforming CNN/GAN baselines and reducing bias by ~20%.


<details>
  <summary>Details</summary>
Motivation: To generate high-resolution, accurate hub-height wind fields by leveraging both sparse observations and simulations, addressing data sparsity, model bias, and terrain effects for wind-farm siting and extreme-wind risk assessment.

Method: WindSR uses diffusion models with data assimilation, blending observations with simulations via a dynamic-radius method as conditioning, and incorporates terrain information during training and inference.

Result: WindSR achieves superior downscaling efficiency and accuracy compared with CNN and GAN baselines; data assimilation reduces model bias by ~20% relative to independent observations.

Conclusion: Integrating diffusion-based super-resolution with data assimilation and terrain-aware conditioning yields more accurate, higher-resolution hub-height wind fields, with tangible bias reduction and improved reliability for wind-related risk assessments.

Abstract: High-quality observations of hub-height winds are valuable but sparse in
space and time. Simulations are widely available on regular grids but are
generally biased and too coarse to inform wind-farm siting or to assess
extreme-weather-related risks (e.g., gusts) at infrastructure scales. To fully
utilize both data types for generating high-quality, high-resolution hub-height
wind speeds (tens to ~100m above ground), this study introduces WindSR, a
diffusion model with data assimilation for super-resolution downscaling of
hub-height winds. WindSR integrates sparse observational data with simulation
fields during downscaling using state-of-the-art diffusion models. A
dynamic-radius blending method is introduced to merge observations with
simulations, providing conditioning for the diffusion process. Terrain
information is incorporated during both training and inference to account for
its role as a key driver of winds. Evaluated against
convolutional-neural-network and generative-adversarial-network baselines,
WindSR outperforms them in both downscaling efficiency and accuracy. Our data
assimilation reduces WindSR's model bias by approximately 20% relative to
independent observations.

</details>


### [241] [Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis](https://arxiv.org/abs/2510.03366)
*Harshwardhan Fartale,Ashish Kattamuri,Rahul Raja,Arpita Vats,Ishita Prasad,Akshata Kishore Moharir*

Main category: cs.LG

TL;DR: The paper provides causal evidence that recall (fact retrieval) and reasoning (multistep inference) in transformer models rely on separable but interacting circuits, demonstrated by targeted interventions (activation patching and ablations) on the layers, heads, and neurons of Qwen and LLaMA models using synthetic puzzles.


<details>
  <summary>Details</summary>
Motivation: To determine whether recall and reasoning recruit distinct internal mechanisms in transformers, which has implications for generalization, evaluation, and safety interventions that target one ability without impairing the other.

Method: Use mechanistic interpretability with controlled synthetic linguistic puzzles. Apply activation patching and structured ablations across layers, heads, and neurons in two model families (Qwen and LLaMA) to causally measure contributions to recall vs. reasoning tasks.

Result: Disabling “recall circuits” impairs fact retrieval by up to ~15% while preserving reasoning, and disabling “reasoning circuits” impairs multi-step inference by a comparable margin. Neuron-level activity shows task-specific patterns but with polysemanticity; effects are less robust at the single-neuron level.

Conclusion: The results indicate separable but interacting circuits underpin recall and reasoning in transformers, advancing mechanistic interpretability and informing safer, targeted interventions for LLM deployment.

Abstract: Transformer-based language models excel at both recall (retrieving memorized
facts) and reasoning (performing multi-step inference), but whether these
abilities rely on distinct internal mechanisms remains unclear. Distinguishing
recall from reasoning is crucial for predicting model generalization, designing
targeted evaluations, and building safer interventions that affect one ability
without disrupting the other.We approach this question through mechanistic
interpretability, using controlled datasets of synthetic linguistic puzzles to
probe transformer models at the layer, head, and neuron level. Our pipeline
combines activation patching and structured ablations to causally measure
component contributions to each task type. Across two model families (Qwen and
LLaMA), we find that interventions on distinct layers and attention heads lead
to selective impairments: disabling identified "recall circuits" reduces
fact-retrieval accuracy by up to 15\% while leaving reasoning intact, whereas
disabling "reasoning circuits" reduces multi-step inference by a comparable
margin. At the neuron level, we observe task-specific firing patterns, though
these effects are less robust, consistent with neuronal polysemanticity.Our
results provide the first causal evidence that recall and reasoning rely on
separable but interacting circuits in transformer models. These findings
advance mechanistic interpretability by linking circuit-level structure to
functional specialization and demonstrate how controlled datasets and causal
interventions can yield mechanistic insights into model cognition, informing
safer deployment of large language models.

</details>


### [242] [Distributed Low-Communication Training with Decoupled Momentum Optimization](https://arxiv.org/abs/2510.03371)
*Sasho Nedelkoski,Alexander Acker,Odej Kao,Soeren Becker,Dominik Scheinert*

Main category: cs.LG

TL;DR: A momentum-based gradient compression method that uses the discrete cosine transform to split Nesterov momentum into high- and low-frequency parts; only the high-frequency component is synchronized every H steps, yielding up to 16x less communication than the DiLoCo baseline while generalizing across transformer and CNN architectures.


<details>
  <summary>Details</summary>
Motivation: Reduce the need for high-bandwidth interconnects to enable training large models on distributed, low-bandwidth resources, expanding feasible training environments beyond centralized data centers.

Method: Treat optimizer momentum as a signal and decompose Nesterov momentum into high- and low-frequency components via the discrete cosine transform (DCT). Synchronize only the high-frequency components across model replicas every H steps, integrating this with gradient momentum compression and evaluating across architectures.

Result: Empirically achieves up to a 16× reduction in communication compared to the baseline DiLoCo and generalizes across architectures including transformer-based language models and CNNs for images.

Conclusion: The approach advances the feasibility of training large models on distributed nodes with low-bandwidth interconnects by exploiting infrequent synchronization of high-frequency momentum components to reduce communication without sacrificing broad applicability.

Abstract: The training of large models demands substantial computational resources,
typically available only in data centers with high-bandwidth interconnects.
However, reducing the reliance on high-bandwidth interconnects between nodes
enables the use of distributed compute resources as an alternative to
centralized data center training. Building on recent advances in distributed
model training, we propose an approach that further reduces communication by
combining infrequent synchronizations across distributed model replicas with
gradient momentum compression. In particular, we treat the optimizer momentum
as a signal and decompose the Nesterov momentum into high- and low-frequency
components via the discrete cosine transform (DCT). Only the high-frequency
components are synchronized across model replicas every $H$ steps. Empirically,
our method achieves up to a $16\times$ reduction in communication compared to
the baseline DiLoCo, and it generalizes across architectures, including
transformer-based language models and convolutional neural networks for images.
Overall, this work advances the feasibility of training large models on
distributed nodes with low-bandwidth interconnects.

</details>


### [243] [Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation](https://arxiv.org/abs/2510.03375)
*Renrong Shao,Wei Zhang,Jun wang*

Main category: cs.LG

TL;DR: A conditional GAN-based data-free KD framework (CPSC-DFKD) that achieves category-specific, diverse sample synthesis and pseudo-supervised contrastive learning to improve student performance without access to real data.


<details>
  <summary>Details</summary>
Motivation: Data-free KD aims to compress and transfer models without exposing data, but existing methods struggle with category-wise diversity and distinguishing category distributions; leveraging pseudo-supervised learning and contrastive exposure could emulate supervised-level gains.

Method: Introduce a conditional GAN to generate category-specific, diverse images for pseudo-supervised distillation; enhance generator to distinguish category distributions; integrate pseudo-supervised contrastive learning using teacher and student views to boost diversity and learning.

Result: Comprehensive experiments on three standard datasets show that both the student model and the generator benefit from CPSC-DFKD, indicating improved distillation performance without real data.

Conclusion: CPSC-DFKD provides a novel data-free KD paradigm that combines category-aware synthesis with contrastive pseudo-supervised learning to improve knowledge transfer, with code released for reproducibility.

Abstract: Data-free knowledge distillation~(DFKD) is an effective manner to solve model
compression and transmission restrictions while retaining privacy protection,
which has attracted extensive attention in recent years. Currently, the
majority of existing methods utilize a generator to synthesize images to
support the distillation. Although the current methods have achieved great
success, there are still many issues to be explored. Firstly, the outstanding
performance of supervised learning in deep learning drives us to explore a
pseudo-supervised paradigm on DFKD. Secondly, current synthesized methods
cannot distinguish the distributions of different categories of samples, thus
producing ambiguous samples that may lead to an incorrect evaluation by the
teacher. Besides, current methods cannot optimize the category-wise diversity
samples, which will hinder the student model learning from diverse samples and
further achieving better performance. In this paper, to address the above
limitations, we propose a novel learning paradigm, i.e., conditional
pseudo-supervised contrast for data-free knowledge distillation~(CPSC-DFKD).
The primary innovations of CPSC-DFKD are: (1) introducing a conditional
generative adversarial network to synthesize category-specific diverse images
for pseudo-supervised learning, (2) improving the modules of the generator to
distinguish the distributions of different categories, and (3) proposing
pseudo-supervised contrastive learning based on teacher and student views to
enhance diversity. Comprehensive experiments on three commonly-used datasets
validate the performance lift of both the student and generator brought by
CPSC-DFKD. The code is available at https://github.com/RoryShao/CPSC-DFKD.git

</details>


### [244] [A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity Skew](https://arxiv.org/abs/2510.03380)
*Michael Ben Ali,Imen Megdiche,André Peninou,Olivier Teste*

Main category: cs.LG

TL;DR: Evaluates Clustered Federated Learning (CFL) under Quantity Skew (QS) and introduces CORNFLQS, an iterative algorithm that coordinates CFL's two strategies for cluster assignment and model-based grouping, achieving robust, top-ranked accuracy and clustering across diverse non-IID settings.


<details>
  <summary>Details</summary>
Motivation: Non-IID data, especially Quantity Skew where client data volumes vary greatly, degrades CFL performance. There is a need for systematic evaluation of CFL under QS and for robust algorithms that can handle diverse QS scenarios.

Method: First, assess state-of-the-art CFL methods under multiple QS scenarios using six image datasets and 270 non-IID configurations. Second, propose CORNFLQS, an iterative CFL algorithm that optimally coordinates two CFL strategies: (a) selecting the cluster that minimizes local training loss and (b) grouping clients based on local model similarities, to improve clustering quality and accuracy under QS.

Result: CORNFLQS achieves the highest average ranking in both accuracy and clustering quality across the evaluated QS perturbations and datasets, showing strong robustness to QS variations and outperforming existing CFL methods.

Conclusion: The proposed CORNFLQS framework provides a robust and effective CFL solution under Quantity Skew by harmonizing the two core CFL strategies, demonstrating superior performance and resilience across diverse non-IID configurations and suggesting the value of QS-focused evaluation in CFL research.

Abstract: Federated Learning (FL) is a decentralized paradigm that enables a
client-server architecture to collaboratively train a global Artificial
Intelligence model without sharing raw data, thereby preserving privacy. A key
challenge in FL is Non-IID data. Quantity Skew (QS) is a particular problem of
Non-IID, where clients hold highly heterogeneous data volumes. Clustered
Federated Learning (CFL) is an emergent variant of FL that presents a promising
solution to Non-IID problem. It improves models' performance by grouping
clients with similar data distributions into clusters. CFL methods generally
fall into two operating strategies. In the first strategy, clients select the
cluster that minimizes the local training loss. In the second strategy, the
server groups clients based on local model similarities. However, most CFL
methods lack systematic evaluation under QS but present significant challenges
because of it. In this paper, we present two main contributions. The first one
is an evaluation of state-of-the-art CFL algorithms under various Non-IID
settings, applying multiple QS scenarios to assess their robustness. Our second
contribution is a novel iterative CFL algorithm, named CORNFLQS, which proposes
an optimal coordination between both operating strategies of CFL. Our approach
is robust against the different variations of QS settings. We conducted
intensive experiments on six image classification datasets, resulting in 270
Non-IID configurations. The results show that CORNFLQS achieves the highest
average ranking in both accuracy and clustering quality, as well as strong
robustness to QS perturbations. Overall, our approach outperforms actual CFL
algorithms.

</details>


### [245] [Cross-Modal Reconstruction Pretraining for Ramp Flow Prediction at Highway Interchanges](https://arxiv.org/abs/2510.03381)
*Yongchao Li,Jun Chen,Zhuoxuan Li,Chao Gao,Yang Li,Chu Zhang,Changyin Dong*

Main category: cs.LG

TL;DR: STDAE introduces a spatio-temporal decoupled autoencoder for predicting interchange ramp flows without ramp detectors, achieving strong performance when combined with GWNet.


<details>
  <summary>Details</summary>
Motivation: Detector scarcity at interchanges leads to blind spots in traffic prediction; need to learn ramp-related signals from mainline data via cross-modal pretraining.

Method: Two-stage framework: stage 1 uses cross-modal reconstruction to reconstruct historical ramp flows from mainline data using a decoupled spatial and temporal autoencoder; stage 2 uses learned representations joined with a forecasting model (GWNet) for ramp flow prediction; plug-and-play.

Result: On three real-world interchange datasets, STDAE-GWNET outperforms 13 state-of-the-art baselines and matches the accuracy of models that have access to actual ramp data.

Conclusion: STDAE effectively mitigates detector scarcity and serves as a versatile component for forecasting pipelines in interchange traffic.

Abstract: Interchanges are crucial nodes for vehicle transfers between highways, yet
the lack of real-time ramp detectors creates blind spots in traffic prediction.
To address this, we propose a Spatio-Temporal Decoupled Autoencoder (STDAE), a
two-stage framework that leverages cross-modal reconstruction pretraining. In
the first stage, STDAE reconstructs historical ramp flows from mainline data,
forcing the model to capture intrinsic spatio-temporal relations. Its decoupled
architecture with parallel spatial and temporal autoencoders efficiently
extracts heterogeneous features. In the prediction stage, the learned
representations are integrated with models such as GWNet to enhance accuracy.
Experiments on three real-world interchange datasets show that STDAE-GWNET
consistently outperforms thirteen state-of-the-art baselines and achieves
performance comparable to models using historical ramp data. This demonstrates
its effectiveness in overcoming detector scarcity and its plug-and-play
potential for diverse forecasting pipelines.

</details>


### [246] [Studying the Korean Word-Chain Game with RLVR:Mitigating Reward Conflicts via Curriculum Learning](https://arxiv.org/abs/2510.03394)
*Donghwan Rho*

Main category: cs.LG

TL;DR: RLVR applied to a Korean word-chain puzzle reveals conflicts among rule-derived rewards; curriculum learning mitigates these conflicts; suggests expanding puzzle studies to diverse languages.


<details>
  <summary>Details</summary>
Motivation: To advance RL with verifiable rewards for improving reasoning in LLMs and to test this framework on multilingual puzzles, examining reward design conflicts and mitigation strategies.

Method: Empirical study applying RLVR to the Korean word-chain game; analysis of rule-derived reward conflicts; introduction of a curriculum-learning scheme to reduce conflicts; evaluation through experiments.

Result: Rule-derived rewards can conflict in the Korean word-chain setting; employing curriculum learning mitigates these conflicts, demonstrating the potential of RLVR for multilingual puzzles.

Conclusion: The findings motivate broader exploration of puzzle tasks across languages and suggest curriculum-guided RLVR as a practical approach to manage reward conflicts in multilingual reasoning tasks.

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for training large language models (LLMs) with stronger reasoning abilities. It
has also been applied to a variety of logic puzzles. In this work, we study the
Korean word-chain game using RLVR. We show that rule-derived rewards can
naturally conflict, and demonstrate through experiments that a
curriculum-learning scheme mitigates these conflicts. Our findings motivate
further studies of puzzle tasks in diverse languages.

</details>


### [247] [Training Variation of Physically-Informed Deep Learning Models](https://arxiv.org/abs/2510.03416)
*Ashley Lenau,Dennis Dimiduk,Stephen R. Niezgoda*

Main category: cs.LG

TL;DR: Physics-informed loss functions show variable training stability; the paper argues for reporting model variation to ensure reproducibility and fair comparisons, demonstrated with a Pix2Pix stress-field task using multiple loss functions.


<details>
  <summary>Details</summary>
Motivation: There is little discussion on the reliability and reproducibility of training algorithms, especially for physics-informed losses that enforce boundary conditions. The work argues for reporting model variation to assess consistency across training runs.

Method: Use a Pix2Pix network to predict stress fields in high-contrast composites. Implement several loss functions enforcing stress equilibrium. Run many training sessions to measure variation in convergence, accuracy, and enforcement of equilibrium.

Result: Different loss functions exhibit varying levels of variation across training runs in convergence, accuracy, and enforcing stress equilibrium. Some losses yield more stable performance and reproducibility. The study also proposes reporting practices for model variation.

Conclusion: Reporting model variation is essential for fair comparisons of physics-informed losses. The paper provides guidelines to improve reproducibility and reliability in training complex networks under boundary-condition constraints.

Abstract: A successful deep learning network is highly dependent not only on the
training dataset, but the training algorithm used to condition the network for
a given task. The loss function, dataset, and tuning of hyperparameters all
play an essential role in training a network, yet there is not much discussion
on the reliability or reproducibility of a training algorithm. With the rise in
popularity of physics-informed loss functions, this raises the question of how
reliable one's loss function is in conditioning a network to enforce a
particular boundary condition. Reporting the model variation is needed to
assess a loss function's ability to consistently train a network to obey a
given boundary condition, and provides a fairer comparison among different
methods. In this work, a Pix2Pix network predicting the stress fields of high
elastic contrast composites is used as a case study. Several different loss
functions enforcing stress equilibrium are implemented, with each displaying
different levels of variation in convergence, accuracy, and enforcing stress
equilibrium across many training sessions. Suggested practices in reporting
model variation are also shared.

</details>


### [248] [Multi-task neural diffusion processes for uncertainty-quantified wind power prediction](https://arxiv.org/abs/2510.03419)
*Joseph Rawson,Domniki Ladopoulou,Petros Dellaportas*

Main category: cs.LG

TL;DR: MT-NDPs provide uncertainty-aware wind power predictions with cross-turbine coupling and few-shot adaptation, outperforming baselines in SCADA data with calibrated intervals.


<details>
  <summary>Details</summary>
Motivation: Wind power forecasting needs calibrated uncertainty estimates for reliable grid integration and operations; current models struggle to adapt to turbines that deviate from fleet averages.

Method: Propose a multi-task neural diffusion process (MT-NDP) with a task encoder to capture cross-turbine correlations and enable few-shot adaptation to unseen turbines; evaluate on real SCADA data.

Result: MT-NDPs outperform single-task NDPs and Gaussian processes in both point accuracy and calibration, especially for turbines that deviate from fleet behavior, delivering calibrated predictions with sharp predictive intervals.

Conclusion: MT-NDPs provide calibrated, scalable predictions suitable for operational deployment in wind farms, supporting dispatch and maintenance decisions.

Abstract: Uncertainty-aware wind power prediction is essential for grid integration and
reliable wind farm operation. We apply neural diffusion processes (NDPs)-a
recent class of models that learn distributions over functions-and extend them
to a multi-task NDP (MT-NDP) framework for wind power prediction. We provide
the first empirical evaluation of NDPs in real supervisory control and data
acquisition (SCADA) data. We introduce a task encoder within MT-NDPs to capture
cross-turbine correlations and enable few-shot adaptation to unseen turbines.
The proposed MT-NDP framework outperforms single-task NDPs and GPs in terms of
point accuracy and calibration, particularly for wind turbines whose behaviour
deviates from the fleet average. In general, NDP-based models deliver
calibrated and scalable predictions suitable for operational deployment,
offering sharper, yet trustworthy, predictive intervals that can support
dispatch and maintenance decisions in modern wind farms.

</details>


### [249] [Memory-Efficient Backpropagation for Fine-Tuning LLMs on Resource-Constrained Mobile Devices](https://arxiv.org/abs/2510.03425)
*Congzheng Song,Xinyu Tang*

Main category: cs.LG

TL;DR: MeBP provides a memory-efficient backpropagation-based fine-tuning approach for mobile devices that enables 0.5–4B LLMs to be trained with under 1 GB of memory, achieving faster convergence and better performance than zeroth-order optimization (ZO) baselines, demonstrated on an iPhone 15 Pro Max.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning of LLMs with backpropagation consumes more memory than inference and is impractical on mobile devices; zeroth-order optimization (ZO) reduces memory but slows convergence dramatically. There is a need for a memory-efficient, device-friendly fine-tuning method that preserves convergence speed and performance.

Method: Propose MeBP (memory-efficient backpropagation) as an on-device fine-tuning implementation. It optimizes memory usage during backpropagation on mobile hardware, reducing peak memory while maintaining or improving convergence speed relative to ZO. Demonstrated by fine-tuning LLMs from 0.5B to 4B parameters on an iPhone 15 Pro Max, achieving sub-1 GB memory usage. Release provided as open-source at a GitHub URL.

Result: MeBP enables on-device fine-tuning of LLMs with less than 1 GB memory for models in the 0.5B–4B range, converging faster and achieving better performance than ZO baselines, as validated on iPhone hardware.

Conclusion: MeBP offers a practical, memory-efficient alternative to ZO for on-device LLM fine-tuning, balancing memory and compute to enable mobile deployment; the approach is demonstrated on consumer hardware and supported by open-source code.

Abstract: Fine-tuning large language models (LLMs) with backpropagation\textemdash even
for a subset of parameters such as LoRA\textemdash can be much more
memory-consuming than inference and is often deemed impractical for
resource-constrained mobile devices. Alternative methods, such as zeroth-order
optimization (ZO), can greatly reduce the memory footprint but come at the cost
of significantly slower model convergence (10$\times$ to 100$\times$ more steps
than backpropagation). We propose a memory-efficient implementation of
backpropagation (MeBP) on mobile devices that provides better trade-off between
memory usage and compute time, while converging faster and achieving better
performance than the ZO baseline. We verify the effectiveness of MeBP on an
iPhone 15 Pro Max and show that various LLMs, ranging from 0.5B to 4B
parameters, can be fine-tuned using less than 1GB of memory. We release an
example of the MeBP implementation at https://github.com/apple/ml-mebp.

</details>


### [250] [Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation](https://arxiv.org/abs/2510.03426)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

TL;DR: GOOMs generalize orders of magnitude to enable stable, high-dynamic-range computation on parallel hardware, surpassing traditional floating-point limits in long-sequence tasks.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic underflow/overflow in long numerical sequences across domains (e.g., deep learning, finance) and enable stable computations over vastly larger dynamic ranges.

Method: Introduce generalized orders of magnitude (GOOMs) as a principled extension of magnitude scales, with floating-point numbers as a special case; implement an efficient, custom parallel prefix scan to run GOOMs efficiently on GPUs; apply selective-resetting to prevent state colinearity in Lyapunov exponent computations; enable parallel, in-core computation of non-diagonal recurrent states in RNNs without stabilization.

Result: GOOMs with parallel scanning outperforms traditional floating-point approaches across three challenging tasks: (1) compounding real matrix products beyond FP limits; (2) parallel, orders‑of‑magnitude faster estimation of Lyapunov spectra using selective resetting to prevent colinearity; (3) capturing long-range dependencies in deep RNNs with non-diagonal recurrent states via parallel prefix, without extra stabilization.

Conclusion: GOOMs, combined with efficient parallel scanning, offer a scalable, numerically robust alternative to conventional floating-point numbers for high dynamic range applications.

Abstract: Many domains, from deep learning to finance, require compounding real numbers
over long sequences, often leading to catastrophic numerical underflow or
overflow. We introduce generalized orders of magnitude (GOOMs), a principled
extension of traditional orders of magnitude that incorporates floating-point
numbers as a special case, and which in practice enables stable computation
over significantly larger dynamic ranges of real numbers than previously
possible. We implement GOOMs, along with an efficient custom parallel prefix
scan, to support native execution on parallel hardware such as GPUs. We
demonstrate that our implementation of GOOMs outperforms traditional approaches
with three representative experiments, all of which were previously considered
impractical or impossible, and now become possible and practical: (1)
compounding real matrix products far beyond standard floating-point limits; (2)
estimating spectra of Lyapunov exponents in parallel, orders of magnitude
faster than with previous methods, applying a novel selective-resetting method
to prevent state colinearity; and (3) capturing long-range dependencies in deep
recurrent neural networks with non-diagonal recurrent states, computed in
parallel via a prefix scan, without requiring any form of stabilization. Our
results show that our implementation of GOOMs, combined with efficient parallel
scanning, offers a scalable and numerically robust alternative to conventional
floating-point numbers for high-dynamic-range applications.

</details>


### [251] [LHGEL: Large Heterogeneous Graph Ensemble Learning using Batch View Aggregation](https://arxiv.org/abs/2510.03432)
*Jiajun Shen,Yufei Jin,Yi He,Xingquan Zhu*

Main category: cs.LG

TL;DR: LHGEL proposes an ensemble framework for large-scale heterogeneous graphs using batch view sampling with batch view aggregation, residual attention, and diversity regularization to improve performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Large heterogeneous graphs pose scalability and heterogeneity challenges; ensemble methods can capture diverse substructures but need efficient global optimization.

Method: Batch sampling to create multiple subgraph views; residual attention to weigh views for node embedding training; diversity regularization to encourage diverse representations across views; theoretical analysis showing residual attention mitigates gradient vanishing; empirical evaluation on five real networks; code provided.

Result: Outperforms state-of-the-art competitors by substantial margin on five real heterogeneous networks.

Conclusion: LHGEL effectively integrates batch view aggregation, residual attention, and diversity regularization to handle heterogeneous graph learning at scale, offering improved accuracy and robustness.

Abstract: Learning from large heterogeneous graphs presents significant challenges due
to the scale of networks, heterogeneity in node and edge types, variations in
nodal features, and complex local neighborhood structures. This paper advocates
for ensemble learning as a natural solution to this problem, whereby training
multiple graph learners under distinct sampling conditions, the ensemble
inherently captures different aspects of graph heterogeneity. Yet, the crux
lies in combining these learners to meet global optimization objective while
maintaining computational efficiency on large-scale graphs. In response, we
propose LHGEL, an ensemble framework that addresses these challenges through
batch sampling with three key components, namely batch view aggregation,
residual attention, and diversity regularization. Specifically, batch view
aggregation samples subgraphs and forms multiple graph views, while residual
attention adaptively weights the contributions of these views to guide node
embeddings toward informative subgraphs, thereby improving the accuracy of base
learners. Diversity regularization encourages representational disparity across
embedding matrices derived from different views, promoting model diversity and
ensemble robustness. Our theoretical study demonstrates that residual attention
mitigates gradient vanishing issues commonly faced in ensemble learning.
Empirical results on five real heterogeneous networks validate that our LHGEL
approach consistently outperforms its state-of-the-art competitors by
substantial margin. Codes and datasets are available at
https://github.com/Chrisshen12/LHGEL.

</details>


### [252] [Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation](https://arxiv.org/abs/2510.03437)
*Jairo Diaz-Rodriguez,Mumin Jia*

Main category: cs.LG

TL;DR: Extends kernel change-point detection to m-dependent data, proving consistency in the number of change points and weak consistency in their locations; combines LLM-based simulations and large-scale text-embedding experiments to show strong practical performance in text segmentation, including a Taylor Swift tweets case study.


<details>
  <summary>Details</summary>
Motivation: Real-world text exhibits dependencies that violate independence assumptions in existing KCPD theory; there is a need for theoretical guarantees under m-dependence and evidence of practical effectiveness when using modern text embeddings.

Method: Theoretical analysis establishing consistency in the number of change points and weak localization under mild assumptions for m-dependent sequences; LLM-based synthetic generation of m-dependent text to validate asymptotics; extensive empirical study applying KCPD with modern embeddings across diverse text datasets, comparing to baselines; case study on real Twitter data (Taylor Swift tweets).

Result: Proved consistency results for the number and locations of change points under m-dependence; simulation studies using LLM-generated text align with asymptotic predictions; empirical results show embedding-based KCPD outperforms baselines on standard text segmentation metrics across diverse datasets; case study demonstrates practical applicability.

Conclusion: KCPD is theoretically sound and practically effective for text segmentation under dependencies; incorporating text embeddings enhances performance; results advocate using KCPD in dependent sequential data scenarios.

Abstract: Kernel change-point detection (KCPD) has become a widely used tool for
identifying structural changes in complex data. While existing theory
establishes consistency under independence assumptions, real-world sequential
data such as text exhibits strong dependencies. We establish new guarantees for
KCPD under $m$-dependent data: specifically, we prove consistency in the number
of detected change points and weak consistency in their locations under mild
additional assumptions. We perform an LLM-based simulation that generates
synthetic $m$-dependent text to validate the asymptotics. To complement these
results, we present the first comprehensive empirical study of KCPD for text
segmentation with modern embeddings. Across diverse text datasets, KCPD with
text embeddings outperforms baselines in standard text segmentation metrics. We
demonstrate through a case study on Taylor Swift's tweets that KCPD not only
provides strong theoretical and simulated reliability but also practical
effectiveness for text segmentation tasks.

</details>


### [253] [The Argument is the Explanation: Structured Argumentation for Trust in Agents](https://arxiv.org/abs/2510.03442)
*Ege Cakar,Per Ola Kristensson*

Main category: cs.LG

TL;DR: Introduces Structured Argumentation (Bipolar ABA) for explainable AI to yield verifiable reasoning chains; achieves state-of-the-art macro F1 on AAEC and competitive Macro F1 on Argumentative MicroTexts; demonstrates multi-agent risk assessment with verification at inference time and automatic hallucination detection via argument graphs; provides Dockerized deployment and open-source code.


<details>
  <summary>Details</summary>
Motivation: Humans are black boxes; society relies on verifiable reasoning. AI explainability should provide verifiable chains of reasoning rather than mere interpretability or opaque model outputs; LLM explanations have gaps in verifiability.

Method: Convert LLM-produced text into argument graphs and apply Bipolar Assumption-Based Argumentation to model support/attack relations. Use Structured What-If Technique for multi-agent risk assessment with transparent collaboration. Implement automatic hallucination detection through fact nodes attacking arguments. Provide a test-time verification/refinement loop without retraining. Deliver as a Docker container for the fine-tuned model and share code via GitHub with the Bipolar ABA package.

Result: Reported macro F1 of 94.44 on the AAEC train/test split (≈5.7 points above prior work) and macro F1 of 0.81 on Argumentative MicroTexts (≈0.07 above prior results), indicating strong performance and improved verifiability features.

Conclusion: Structured argumentation enables verifiable, stepwise reasoning and automatic hallucination checks, offering a verification-enabled alternative to interpretability alone and enabling test-time refinement without retraining. The approach is packaged for easy deployment with Docker and open-source tooling.

Abstract: Humans are black boxes -- we cannot observe their neural processes, yet
society functions by evaluating verifiable arguments. AI explainability should
follow this principle: stakeholders need verifiable reasoning chains, not
mechanistic transparency. We propose using structured argumentation to provide
a level of explanation and verification neither interpretability nor
LLM-generated explanation is able to offer. Our pipeline achieves
state-of-the-art 94.44 macro F1 on the AAEC published train/test split (5.7
points above prior work) and $0.81$ macro F1, $\sim$0.07 above previous
published results with comparable data setups, for Argumentative MicroTexts
relation classification, converting LLM text into argument graphs and enabling
verification at each inferential step. We demonstrate this idea on multi-agent
risk assessment using the Structured What-If Technique, where specialized
agents collaborate transparently to carry out risk assessment otherwise
achieved by humans alone. Using Bipolar Assumption-Based Argumentation, we
capture support/attack relationships, thereby enabling automatic hallucination
detection via fact nodes attacking arguments. We also provide a verification
mechanism that enables iterative refinement through test-time feedback without
retraining. For easy deployment, we provide a Docker container for the
fine-tuned AMT model, and the rest of the code with the Bipolar ABA Python
package on GitHub.

</details>


### [254] [On residual network depth](https://arxiv.org/abs/2510.03470)
*Benoit Dherin,Michael Munn*

Main category: cs.LG

TL;DR: Depth in residual architectures acts as expanding an implicit ensemble; a Residual Expansion Theorem shows that scaling residuals tames combinatorial growth, links depth to capacity and implicit regularization, and explains normalization dependence and alternatives like SkipInit/Fixup.


<details>
  <summary>Details</summary>
Motivation: Provide a first-principles, analytical explanation for why very deep residual networks (e.g., ResNet, Transformers) perform so well, and to explain the role of normalization layers and why normalization-free techniques can work.

Method: Derive an explicit analytical formula showing that increasing depth is equivalent to growing an ensemble of shallow paths within the network. Develop the Residual Expansion Theorem to show how scaling each residual module controls the combinatorial explosion of computation paths and yields implicit regularization. Connect hierarchical path growth to output signal amplification and normalization requirements, and relate to SkipInit/Fixup as normalization-free strategies grounded in the same theory.

Result: Depth expansion corresponds to a larger ensemble of submodels; computation paths proliferate combinatorially, causing an explosion in output signal which necessitates normalization. Scaling residual modules provides a principled way to tame this explosion and acts as a capacity control, yielding implicit regularization. This offers a first-principles explanation for normalization dependence and unifies normalization-free methods under the same framework.

Conclusion: A principled, theory-backed explanation for the success of deep residual networks: depth increases ensemble size; the residual expansion and scaling govern capacity and stability. This informs design choices, explains normalization's role, and links to normalization-free techniques like SkipInit and Fixup as outcomes of the same underlying structure.

Abstract: Deep residual architectures, such as ResNet and the Transformer, have enabled
models of unprecedented depth, yet a formal understanding of why depth is so
effective remains an open question. A popular intuition, following Veit et al.
(2016), is that these residual networks behave like ensembles of many shallower
models. Our key finding is an explicit analytical formula that verifies this
ensemble perspective, proving that increasing network depth is mathematically
equivalent to expanding the size of this implicit ensemble. Furthermore, our
expansion reveals a hierarchical ensemble structure in which the combinatorial
growth of computation paths leads to an explosion in the output signal,
explaining the historical necessity of normalization layers in training deep
models. This insight offers a first principles explanation for the historical
dependence on normalization layers and sheds new light on a family of
successful normalization-free techniques like SkipInit and Fixup. However,
while these previous approaches infer scaling factors through optimizer
analysis or a heuristic analogy to Batch Normalization, our work offers the
first explanation derived directly from the network's inherent functional
structure. Specifically, our Residual Expansion Theorem reveals that scaling
each residual module provides a principled solution to taming the combinatorial
explosion inherent to these architectures. We further show that this scaling
acts as a capacity controls that also implicitly regularizes the model's
complexity.

</details>


### [255] [How to Set $β_1, β_2$ in Adam: An Online Learning Perspective](https://arxiv.org/abs/2510.03478)
*Quan Nguyen*

Main category: cs.LG

TL;DR: Extends Adam analysis beyond the constraint beta1 = sqrt(beta2); derives tight regret bounds for both beta1 ≥ sqrt(beta2) and beta1 ≤ sqrt(beta2), showing an adversary-dependent optimality of beta1 = sqrt(beta2).


<details>
  <summary>Details</summary>
Motivation: To understand how to optimally set Adam's momentum parameters in practice. Prior work tied beta1 and beta2 via beta1 = sqrt(beta2), limiting applicability; this work broadens the theoretical understanding via an FTRL perspective.

Method: Treat Adam as Follow-the-Regularized-Leader (FTRL) and derive new regret bounds in two regimes: beta1 ≥ sqrt(beta2) and beta1 ≤ sqrt(beta2). Prove the bounds strictly generalize prior ones and are tight in the worst case; analyze performance under oblivious vs non-oblivious adversaries.

Result: General, tight bounds that cover both parameter regimes and strictly extend existing results; the bound is tight in the worst case. It also shows beta1 = sqrt(beta2) is optimal for an oblivious adversary but sub-optimal for a non-oblivious adversary.

Conclusion: Hyperparameter guidance for Adam depends on the adversary model; the analysis deepens theoretical understanding of how to set momentum factors and indicates that the common choice is not universally optimal.

Abstract: While Adam is one of the most effective optimizer for training large-scale
machine learning models, a theoretical understanding of how to optimally set
its momentum factors, $\beta_1$ and $\beta_2$, remains largely incomplete.
  Prior works have shown that Adam can be seen as an instance of
Follow-the-Regularized-Leader (FTRL), one of the most important class of
algorithms in online learning.
  The prior analyses in these works required setting $\beta_1 =
\sqrt{\beta_2}$, which does not cover the more practical cases with $\beta_1
\neq \sqrt{\beta_2}$.
  We derive novel, more general analyses that hold for both $\beta_1 \geq
\sqrt{\beta_2}$ and $\beta_1 \leq \sqrt{\beta_2}$.
  In both cases, our results strictly generalize the existing bounds.
  Furthermore, we show that our bounds are tight in the worst case.
  We also prove that setting $\beta_1 = \sqrt{\beta_2}$ is optimal for an
oblivious adversary, but sub-optimal for an non-oblivious adversary.

</details>


### [256] [Reasoning-based Anomaly Detection Framework: A Real-time, Scalable, and Automated Approach to Anomaly Detection Across Domains](https://arxiv.org/abs/2510.03486)
*Anupam Panwar,Himadri Pal,Jiali Chen,Kyle Cho,Riddick Jiang,Miao Zhao,Rajiv Krishnamurthy*

Main category: cs.LG

TL;DR: A real-time, scalable anomaly-detection framework RADF with auto-ML selector mSelect for algorithm choice and hyperparameter tuning per use case, plus post-detection triage; achieves strong AUC on public benchmarks (0.85+ on most datasets).


<details>
  <summary>Details</summary>
Motivation: Detect anomalies in high-volume, heterogeneous time-series data across multiple domains with minimal manual tuning and rapid root-cause analysis.

Method: Introduce RADF framework powered by mSelect, which automatically selects suitable anomaly-detection algorithms and tunes hyperparameters for each deployment. Includes a post-detection module for faster root-cause triaging. Demonstrates real-time processing on large datasets and evaluates on public benchmarks.

Result: RADF outperforms state-of-the-art models in AUC on 5 of 9 public datasets; achieves AUC >0.85 on 7 of 9 datasets, indicating strong empirical performance.

Conclusion: RADF provides a scalable, adaptable anomaly-detection solution for large distributed systems with automated algorithm selection and post-detection triage, yielding competitive performance across heterogeneous time-series data.

Abstract: Detecting anomalies in large, distributed systems presents several
challenges. The first challenge arises from the sheer volume of data that needs
to be processed. Flagging anomalies in a high-throughput environment calls for
a careful consideration of both algorithm and system design. The second
challenge comes from the heterogeneity of time-series datasets that leverage
such a system in production. In practice, anomaly detection systems are rarely
deployed for a single use case. Typically, there are several metrics to
monitor, often across several domains (e.g. engineering, business and
operations). A one-size-fits-all approach rarely works, so these systems need
to be fine-tuned for every application - this is often done manually. The third
challenge comes from the fact that determining the root-cause of anomalies in
such settings is akin to finding a needle in a haystack. Identifying (in real
time) a time-series dataset that is associated causally with the anomalous
time-series data is a very difficult problem. In this paper, we describe a
unified framework that addresses these challenges. Reasoning based Anomaly
Detection Framework (RADF) is designed to perform real time anomaly detection
on very large datasets. This framework employs a novel technique (mSelect) that
automates the process of algorithm selection and hyper-parameter tuning for
each use case. Finally, it incorporates a post-detection capability that allows
for faster triaging and root-cause determination. Our extensive experiments
demonstrate that RADF, powered by mSelect, surpasses state-of-the-art anomaly
detection models in AUC performance for 5 out of 9 public benchmarking
datasets. RADF achieved an AUC of over 0.85 for 7 out of 9 datasets, a
distinction unmatched by any other state-of-the-art model.

</details>


### [257] [Trajectory Data Suffices for Statistically Efficient Policy Evaluation in Finite-Horizon Offline RL with Linear $q^π$-Realizability and Concentrability](https://arxiv.org/abs/2510.03494)
*Volodymyr Tkachuk,Csaba Szepesvári,Xiaoqi Tan*

Main category: cs.LG

TL;DR: Efficient offline RL with function approximation is achievable for policy evaluation under concentrability and q^π-realizability, and optimization sample complexity can be tightened.


<details>
  <summary>Details</summary>
Motivation: To resolve whether both evaluation and optimization can be statistically efficient in finite-horizon offline RL with function approximation, given only good data coverage and q^π-realizability; extend existing trajectory-based optimization results to evaluation.

Method: Proposes a statistically efficient learner for policy evaluation under the same assumptions, and provides a tighter analysis that improves the sample complexity bound for the policy-optimization learner of Tkachuk et al. (2024) when data is trajectory-based.

Result: Demonstrates a statistically efficient policy-evaluation learner under the stated assumptions; shows improved (tighter) sample complexity bound for policy optimization from the previous work.

Conclusion: Under finite-horizon offline RL with function approximation, both evaluation and optimization can be statistically efficient under concentrability and q^ π-realizability, and optimization analyses can be tightened to reduce sample complexity.

Abstract: We study finite-horizon offline reinforcement learning (RL) with function
approximation for both policy evaluation and policy optimization. Prior work
established that statistically efficient learning is impossible for either of
these problems when the only assumptions are that the data has good coverage
(concentrability) and the state-action value function of every policy is
linearly realizable ($q^\pi$-realizability) (Foster et al., 2021). Recently,
Tkachuk et al. (2024) gave a statistically efficient learner for policy
optimization, if in addition the data is assumed to be given as trajectories.
In this work we present a statistically efficient learner for policy evaluation
under the same assumptions. Further, we show that the sample complexity of the
learner used by Tkachuk et al. (2024) for policy optimization can be improved
by a tighter analysis.

</details>


### [258] [D2 Actor Critic: Diffusion Actor Meets Distributional Critic](https://arxiv.org/abs/2510.03508)
*Lunjun Zhang,Shuo Han,Hanrui Lyu,Bradly C Stadie*

Main category: cs.LG

TL;DR: D2AC is a model-free RL algorithm that trains expressive diffusion policies online using a low-variance policy-improvement objective and a robust, distributional critic, achieving state-of-the-art results on 18 hard tasks and showing robustness in predator-prey scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the high variance of standard policy gradients and the computational complexity of backprop through time, enabling stable, online learning of expressive diffusion policies with reliable value estimates.

Method: D2AC couples a variance-reducing policy-improvement objective with a robust distributional critic built from distributional RL and clipped double Q-learning, applying it to online, model-free training of diffusion policies across diverse tasks (including dense and goal-conditioned RL) and evaluating robustness via a predator-prey task.

Result: Achieves state-of-the-art performance on a benchmark of 18 hard RL tasks (e.g., Humanoid, Dog, Shadow Hand), spanning dense-reward and goal-conditioned settings, and demonstrates behavioral robustness and generalization in a predator-prey task.

Conclusion: A stable, effective approach for online learning of expressive diffusion policies, combining variance-reduced policy improvement with a robust distributional critic to deliver high performance and better robustness/generalization.

Abstract: We introduce D2AC, a new model-free reinforcement learning (RL) algorithm
designed to train expressive diffusion policies online effectively. At its core
is a policy improvement objective that avoids the high variance of typical
policy gradients and the complexity of backpropagation through time. This
stable learning process is critically enabled by our second contribution: a
robust distributional critic, which we design through a fusion of
distributional RL and clipped double Q-learning. The resulting algorithm is
highly effective, achieving state-of-the-art performance on a benchmark of
eighteen hard RL tasks, including Humanoid, Dog, and Shadow Hand domains,
spanning both dense-reward and goal-conditioned RL scenarios. Beyond standard
benchmarks, we also evaluate a biologically motivated predator-prey task to
examine the behavioral robustness and generalization capacity of our approach.

</details>


### [259] [Task-Level Contrastiveness for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2510.03509)
*Kristi Topollai,Anna Choromanska*

Main category: cs.LG

TL;DR: Proposes task-level contrastiveness for few-shot/meta-learning to improve cross-domain generalization using lightweight task augmentations and a task-level contrastive loss, validated on MetaDataset.


<details>
  <summary>Details</summary>
Motivation: Current few-shot and meta-learning methods struggle to generalize across domains and often incur high computational costs; there is a need for domain-agnostic, efficient representations that transfer knowledge between seen and unseen domains.

Method: Introduce task-level contrastive learning by defining simple task augmentations and applying a contrastive loss on task representations to encourage unsupervised clustering; lightweight, compatible with existing frameworks, and does not require domain knowledge.

Result: Experimental results on the MetaDataset benchmark show improved generalization and computational efficiency compared to baselines, without adding significant complexity.

Conclusion: Task-level contrastiveness offers a simple, effective mechanism to learn transferable task representations, enhancing cross-domain generalization in few-shot/meta-learning while maintaining efficiency.

Abstract: Few-shot classification and meta-learning methods typically struggle to
generalize across diverse domains, as most approaches focus on a single
dataset, failing to transfer knowledge across various seen and unseen domains.
Existing solutions often suffer from low accuracy, high computational costs,
and rely on restrictive assumptions. In this paper, we introduce the notion of
task-level contrastiveness, a novel approach designed to address issues of
existing methods. We start by introducing simple ways to define task
augmentations, and thereafter define a task-level contrastive loss that
encourages unsupervised clustering of task representations. Our method is
lightweight and can be easily integrated within existing few-shot/meta-learning
algorithms while providing significant benefits. Crucially, it leads to
improved generalization and computational efficiency without requiring prior
knowledge of task domains. We demonstrate the effectiveness of our approach
through different experiments on the MetaDataset benchmark, where it achieves
superior performance without additional complexity.

</details>


### [260] [A Lightweight Federated Learning Approach for Privacy-Preserving Botnet Detection in IoT](https://arxiv.org/abs/2510.03513)
*Taha M. Mahmoud,Naima Kaabouch*

Main category: cs.LG

TL;DR: A privacy-preserving, scalable IoT botnet detector using federated learning with a communication-efficient aggregation, achieving strong accuracy while reducing data exchange.


<details>
  <summary>Details</summary>
Motivation: The rapid expansion of IoT increases botnet threats and challenges traditional detectors with scalability, privacy, and resource-constrained environments; a distributed, privacy-preserving approach is needed.

Method: A federated learning framework where IoT devices train local models and only share model updates; includes a communication-efficient aggregation strategy to reduce overhead.

Result: Experiments on benchmark IoT botnet datasets show high detection accuracy with substantially reduced communication costs compared to non-federated baselines.

Conclusion: Federated learning is a practical, scalable, and privacy-aware approach for IoT intrusion detection.

Abstract: The rapid growth of the Internet of Things (IoT) has expanded opportunities
for innovation but also increased exposure to botnet-driven cyberattacks.
Conventional detection methods often struggle with scalability, privacy, and
adaptability in resource-constrained IoT environments. To address these
challenges, we present a lightweight and privacy-preserving botnet detection
framework based on federated learning. This approach enables distributed
devices to collaboratively train models without exchanging raw data, thus
maintaining user privacy while preserving detection accuracy. A
communication-efficient aggregation strategy is introduced to reduce overhead,
ensuring suitability for constrained IoT networks. Experiments on benchmark IoT
botnet datasets demonstrate that the framework achieves high detection accuracy
while substantially reducing communication costs. These findings highlight
federated learning as a practical path toward scalable, secure, and
privacy-aware intrusion detection for IoT ecosystems.

</details>


### [261] [RAPID: An Efficient Reinforcement Learning Algorithm for Small Language Models](https://arxiv.org/abs/2510.03515)
*Lianghuan Huang,Sagnik Anupam,Insup Lee,Shuo Li,Osbert Bastani*

Main category: cs.LG

TL;DR: RAPID speeds up RL-based fine-tuning of small language models by batching inference and using off-policy mini-batch updates with group advantage estimation and importance weighting, achieving substantial runtime reductions while preserving accuracy.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning for fine-tuning SLMs is costly, requiring heavy compute due to coupled inference and backpropagation. There is a need to reduce training time without sacrificing performance.

Method: Perform large-batch inference to utilize hardware efficiently, followed by off-policy policy gradient updates in mini-batches. Incorporate group advantage estimation and derive an importance-weighted estimator to correct bias from off-policy learning.

Result: Running time reduced by 11%-34% on three benchmarks compared to state-of-the-art RL algorithms, with similar or better accuracy.

Conclusion: RAPID demonstrates a practical approach to making RL-based SLM fine-tuning more time-efficient by decoupling inference from gradient updates and applying principled off-policy corrections, maintaining performance while saving compute.

Abstract: Reinforcement learning (RL) has emerged as a promising strategy for
finetuning small language models (SLMs) to solve targeted tasks such as math
and coding. However, RL algorithms tend to be resource-intensive, taking a
significant amount of time to train. We propose RAPID, a novel RL algorithm
that can substantially reduce the running time of RL. Our key insight is that
RL tends to be costly due to the need to perform both inference and
backpropagation during training. To maximize use of computational resources,
our algorithm performs inference in large batches, and then performs off-policy
policy gradient updates in mini-batches. For off-policy updates, we incorporate
group advantage estimation into the policy gradient algorithm, and derive an
importance weighted estimator to correct for the bias arising from off-policy
learning. Our experiments demonstrate that our algorithm can reduce running
time by 11%-34% on three benchmarks compared to state-of-the-art RL algorithms
while maintaining similar or better accuracy.

</details>


### [262] [Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models](https://arxiv.org/abs/2510.03520)
*Kartik Pandit,Sourav Ganguly,Arnesh Banerjee,Shaahin Angizi,Arnob Ghosh*

Main category: cs.LG

TL;DR: Certifiable Safe-RLHF (CS-RLHF) introduces a penalty-based, exact-penalty constrained RL approach for LLM safety, using a semantically-grounded cost model and avoiding dual-variable tuning. It claims provable feasibility and 5x efficiency over state-of-the-art methods, including jail-breaking prompts.


<details>
  <summary>Details</summary>
Motivation: Safety-utility trade-off in LLMs is hard; CMDP-based methods depend on scoring functions and dual variables, which are sensitive and expensive and can be exploited via jailbreaks. A provably safe, efficient alternative is needed.

Method: Train a cost model on a large-scale corpus to assign semantically grounded safety scores. Use a rectified penalty (exact penalty) formulation to enforce safety constraints directly, eliminating the need for dual-variable updates.

Result: Empirical results show CS-RLHF outperforms state-of-the-art responses and achieves at least 5x efficiency against nominal and jail-breaking prompts.

Conclusion: The approach provides provable feasibility of safety constraints at the optimizer, avoids dual-variable tuning, and improves safety and efficiency in LLM outputs.

Abstract: Ensuring safety is a foundational requirement for large language models
(LLMs). Achieving an appropriate balance between enhancing the utility of model
outputs and mitigating their potential for harm is a complex and persistent
challenge. Contemporary approaches frequently formalize this problem within the
framework of Constrained Markov Decision Processes (CMDPs) and employ
established CMDP optimization techniques. However, these methods exhibit two
notable limitations. First, their reliance on reward and cost functions renders
performance highly sensitive to the underlying scoring mechanism, which must
capture semantic meaning rather than being triggered by superficial keywords.
Second, CMDP-based training entails tuning dual-variable, a process that is
both computationally expensive and does not provide any provable safety
guarantee for a fixed dual variable that can be exploitable through adversarial
jailbreaks. To overcome these limitations, we introduce Certifiable Safe-RLHF
(CS-RLHF) that introduces a cost model trained on a large-scale corpus to
assign semantically grounded safety scores. In contrast to the lagrangian-based
approach, CS-RLHF adopts a rectified penalty-based formulation. This design
draws on the theory of exact penalty functions in constrained optimization,
wherein constraint satisfaction is enforced directly through a suitably chosen
penalty term. With an appropriately scaled penalty, feasibility of the safety
constraints can be guaranteed at the optimizer, eliminating the need for
dual-variable updates. Empirical evaluation demonstrates that CS-RLHF
outperforms state-of-the-art LLM model responses rendering at-least 5 times
efficient against nominal and jail-breaking prompts

</details>


### [263] [Sequential decoder training for improved latent space dynamics identification](https://arxiv.org/abs/2510.03535)
*William Anderson,Seung Whan Chung,Youngsoo Choi*

Main category: cs.LG

TL;DR: mLaSDI uses multi-stage decoders to iteratively correct residual errors in latent-space ROMs, improving reconstruction and prediction accuracy with lower training time, demonstrated on the 1D-1V Vlasov equation.


<details>
  <summary>Details</summary>
Motivation: Enforcing latent dynamics during training can degrade reconstruction accuracy; there is a need for ROMs that maintain high fidelity while remaining computationally efficient.

Method: Extend LaSDI by sequentially adding decoders that correct residual errors from earlier stages, enabling staged learning of latent dynamics and reconstructions while preserving the autoencoder and equation-discovery components.

Result: mLaSDI consistently outperforms standard LaSDI in reconstruction and prediction accuracy and reduces training time across a wide range of architectures when applied to the 1D-1V Vlasov equation.

Conclusion: Multi-stage latent-space modeling (mLaSDI) is an effective approach to improve ROM performance, achieving higher fidelity and lower training cost for kinetic PDEs like the Vlasov equation.

Abstract: Accurate numerical solutions of partial differential equations are essential
in many scientific fields but often require computationally expensive solvers,
motivating reduced-order models (ROMs). Latent Space Dynamics Identification
(LaSDI) is a data-driven ROM framework that combines autoencoders with equation
discovery to learn interpretable latent dynamics. However, enforcing latent
dynamics during training can compromise reconstruction accuracy of the model
for simulation data. We introduce multi-stage LaSDI (mLaSDI), a framework that
improves reconstruction and prediction accuracy by sequentially learning
additional decoders to correct residual errors from previous stages. Applied to
the 1D-1V Vlasov equation, mLaSDI consistently outperforms standard LaSDI,
achieving lower prediction errors and reduced training time across a wide range
of architectures.

</details>


### [264] [CrossLag: Predicting Major Dengue Outbreaks with a Domain Knowledge Informed Transformer](https://arxiv.org/abs/2510.03566)
*Ashwin Prabu,Nhat Thanh Tran,Guofa Zhou,Jack Xin*

Main category: cs.LG

TL;DR: CrossLag is an environmentally informed attention mechanism for transformers that integrates lagged endogenous signals with exogenous data to forecast dengue outbreaks, achieving improved 24-week-ahead prediction in Singapore and outperforming the TimeXer baseline.


<details>
  <summary>Details</summary>
Motivation: Major dengue outbreaks require timely warnings, but predicting outbreaks that lag climate/oceanic anomalies remains challenging and existing models often rely on high parameter counts. Incorporating lagged endogenous signals into exogenous data within a transformer could improve forecasts while remaining efficient.

Method: Introduce CrossLag, an attention mechanism within a transformer that explicitly accommodates lagging endogenous signals behind significant exogenous events. Built on TimeXer as the baseline, CrossLag achieves environmentally informed, low-parameter modeling for dengue outbreak forecasting.

Result: CrossLag outperforms TimeXer by a considerable margin in detecting and predicting major dengue outbreaks in Singapore data over a 24-week prediction window.

Conclusion: Environmentally informed, lag-aware transformer architectures like CrossLag can substantially improve early warning capabilities for dengue outbreaks with lower model complexity, suggesting broader applicability to similar exogenous–endogenous forecasting tasks.

Abstract: A variety of models have been developed to forecast dengue cases to date.
However, it remains a challenge to predict major dengue outbreaks that need
timely public warnings the most. In this paper, we introduce CrossLag, an
environmentally informed attention that allows for the incorporation of lagging
endogenous signals behind the significant events in the exogenous data into the
architecture of the transformer at low parameter counts. Outbreaks typically
lag behind major changes in climate and oceanic anomalies. We use TimeXer, a
recent general-purpose transformer distinguishing exogenous-endogenous inputs,
as the baseline for this study. Our proposed model outperforms TimeXer by a
considerable margin in detecting and predicting major outbreaks in Singapore
dengue data over a 24-week prediction window.

</details>


### [265] [Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs](https://arxiv.org/abs/2510.03567)
*Fatmazohra Rezkellah,Ramzi Dakhmouche*

Main category: cs.LG

TL;DR: Proposes simple constraint-based weight interventions to unify unlearning of sensitive information and robustness to jail-breaking without relying on an oracle classifier; shows point-wise constraints outperform max-min and are computationally cheaper, with superior defense performance.


<details>
  <summary>Details</summary>
Motivation: Need for privacy-preserving customization of LLMs and safety, addressing leakage of sensitive information and susceptibility to jail-breaking, while avoiding costly oracle classifiers.

Method: Formulates constrained optimization that minimally perturbs model weights to either make a vocabulary set unreachable (unlearning) or shift weights to a safer region (robustness). Emphasizes a unified framework without requiring an external classifier and compares simple point-wise constraints to more complex max-min interventions.

Result: Empirical results indicate that the simplest point-wise constraint interventions yield better performance and lower computational cost than max-min strategies, and outperform state-of-the-art defense methods.

Conclusion: A unified, classifier-free approach to both unlearning and robustness via small, targeted weight interventions; the method improves defense efficacy while reducing overhead, suggesting practical privacy-preserving customization of LLMs.

Abstract: With the increasing adoption of Large Language Models (LLMs), more
customization is needed to ensure privacy-preserving and safe generation. We
address this objective from two critical aspects: unlearning of sensitive
information and robustness to jail-breaking attacks. We investigate various
constrained optimization formulations that address both aspects in a
\emph{unified manner}, by finding the smallest possible interventions on LLM
weights that either make a given vocabulary set unreachable or embed the LLM
with robustness to tailored attacks by shifting part of the weights to a
\emph{safer} region. Beyond unifying two key properties, this approach
contrasts with previous work in that it doesn't require an oracle classifier
that is typically not available or represents a computational overhead.
Surprisingly, we find that the simplest point-wise constraint-based
intervention we propose leads to better performance than max-min interventions,
while having a lower computational cost. Comparison against state-of-the-art
defense methods demonstrates superior performance of the proposed approach.

</details>


### [266] [Longitudinal Flow Matching for Trajectory Modeling](https://arxiv.org/abs/2510.03569)
*Mohammad Mohaiminul Islam,Thijs P. Kuipers,Sharvaree Vadgama,Coen de Vente,Afsana Khan,Clara I. Sánchez,Erik J. Bekkers*

Main category: cs.LG

TL;DR: IMMFM is a framework for learning continuous stochastic dynamics from sparsely observed, high-dimensional sequential data by interpolating across multiple time points. It jointly learns drift and a data-driven diffusion, using a piecewise-quadratic interpolation path as the smooth target, with a stability condition, yielding subject-specific trajectories and improved forecasting and downstream task performance over baselines.


<details>
  <summary>Details</summary>
Motivation: Generative models for sequential data often fail on sparsely sampled, high-dimensional trajectories and reduce dynamics to simple pairwise transitions, missing intrinsic stochasticity and robustness to irregular sampling. A method that leverages multiple observed time points to learn stable, stochastic dynamics and produce subject-specific trajectories is needed.

Method: Introduce Interpolative Multi-Marginal Flow Matching (IMMFM). It uses a piecewise-quadratic interpolation path as a smooth target for flow matching, and jointly optimizes the drift and a data-driven diffusion coefficient. A theoretical condition ensures stable learning. The design handles irregular sparse sampling and yields subject-specific trajectories.

Result: Experiments on synthetic benchmarks and real-world longitudinal neuroimaging datasets show that IMMFM outperforms existing methods in forecasting accuracy and improves downstream tasks.

Conclusion: IMMFM captures intrinsic stochasticity, handles irregular sparse sampling, and yields subject-specific trajectories, with stable learning guarantees and superior predictive performance on both synthetic and real-world data.

Abstract: Generative models for sequential data often struggle with sparsely sampled
and high-dimensional trajectories, typically reducing the learning of dynamics
to pairwise transitions. We propose \textit{Interpolative Multi-Marginal Flow
Matching} (IMMFM), a framework that learns continuous stochastic dynamics
jointly consistent with multiple observed time points. IMMFM employs a
piecewise-quadratic interpolation path as a smooth target for flow matching and
jointly optimizes drift and a data-driven diffusion coefficient, supported by a
theoretical condition for stable learning. This design captures intrinsic
stochasticity, handles irregular sparse sampling, and yields subject-specific
trajectories. Experiments on synthetic benchmarks and real-world longitudinal
neuroimaging datasets show that IMMFM outperforms existing methods in both
forecasting accuracy and further downstream tasks.

</details>


### [267] [Generalization of Graph Neural Network Models for Distribution Grid Fault Detection](https://arxiv.org/abs/2510.03571)
*Burak Karabulut,Carlo Manna,Chris Develder*

Main category: cs.LG

TL;DR: A benchmark study of RGNN fault-diagnosis pipelines on power grids shows Graph Attention variants (RGATv2) generalize much better across topologies than RGCN or pure RNNs, reducing F1-score decline to ~12% versus up to ~60% for pure RNNs and ~25% for other RGNNs.


<details>
  <summary>Details</summary>
Motivation: To improve robustness of data-driven fault diagnosis against topology changes (reconfigurations, DER integration, failures) and to evaluate newer GNN architectures within an RGNN pipeline.

Method: Systematic benchmarking of RGNN variants (RGCN, GRU-based baselines, GraphSAGE, GAT, GATv2) on the IEEE 123-node distribution network across varying topologies; assessment via F1-score under topology shifts.

Result: RGATv2 achieves superior generalization with ≈12% F1-score reduction under topology changes. Pure RNNs suffer ≈60% reductions; other RGNN variants incur up to ≈25% reductions.

Conclusion: Adopting attention-based RGNNs (RGATv2) enhances robustness of fault-diagnosis models to topology evolution, motivating deployment of advanced GNN architectures in RGNN pipelines for distribution grids.

Abstract: Fault detection in power distribution grids is critical for ensuring system
reliability and preventing costly outages. Moreover, fault detection
methodologies should remain robust to evolving grid topologies caused by
factors such as reconfigurations, equipment failures, and Distributed Energy
Resource (DER) integration. Current data-driven state-of-the-art methods use
Recurrent Neural Networks (RNNs) for temporal modeling and Graph Neural
Networks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN in
short). Specifically, for power system fault diagnosis, Graph Convolutional
Networks (GCNs) have been adopted. Yet, various more advanced GNN architectures
have been proposed and adopted in domains outside of power systems. In this
paper, we set out to systematically and consistently benchmark various GNN
architectures in an RNN+GNN pipeline model. Specifically, to the best of our
knowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention
(GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensive
benchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNN
models (especially Gated Recurrent Unit (GRU)), particularly (iii) exploring
their generalization potential for deployment in different settings than those
used for training them. Our experimental results on the IEEE 123-node
distribution network show that RGATv2 has superior generalization capabilities,
maintaining high performance with an F1-score reduction of $\sim$12% across
different topology settings. In contrast, pure RNN models largely fail,
experiencing an F1-score reduction of up to $\sim$60%, while other RGNN
variants also exhibit significant performance degradation, i.e., up to
$\sim$25% lower F1-scores.

</details>


### [268] [Efficient Test-Time Scaling for Small Vision-Language Models](https://arxiv.org/abs/2510.03574)
*Mehmet Onurcan Kaya,Desmond Elliott,Dim P. Papadopoulos*

Main category: cs.LG

TL;DR: Efficient test-time strategies (TTAug and TTAdapt) using model-internal features boost small Vision-Language Models without extra tuning or external supervision, achieving consistent gains across benchmarks and models.


<details>
  <summary>Details</summary>
Motivation: Small VLMs lag in generalization and task performance while being resource-efficient; existing test-time scaling methods are often too costly for small models, so there is a need for lightweight, internal, data-free tuning mechanisms.

Method: Two strategies are proposed: (i) Test-Time Augmentation (TTAug) that generates multiple augmented inputs and aggregates outputs at the token level without parameter updates, and (ii) Test-Time Adaptation (TTAdapt) that further adapts model parameters during inference using consensus-based pseudolabels from TTAug. Both rely on internal features and do not require external supervision, and are evaluated across models and scales.

Result: Across nine benchmarks, the approach yields consistent performance gains while retaining computational efficiency suitable for resource-constrained environments. The improvements generalize across different VLMs and model scales without additional tuning.

Conclusion: Efficient, model-internal test-time strategies (TTAug and TTAdapt) can meaningfully improve small VLMs by leveraging internal representations, offering a general and scalable solution for resource-limited settings without external supervision or heavy computation.

Abstract: Small Vision-Language Models (VLMs) provide a computationally efficient
alternative to larger models, at the cost of weaker generalization abilities
and downstream task performance. These shortcomings could be addressed by
test-time scaling techniques, but existing methods are typically
computationally demanding, contradicting the resource-efficient design goals of
small models. To address these limitations, we propose two novel and efficient
test-time scaling strategies that leverage the model-internal features rather
than external supervision: (i) Test-Time Augmentation (TTAug), which generates
multiple augmented inputs and aggregates outputs at the token level without
parameter updates, and (ii) Test-Time Adaptation (TTAdapt), which adapts model
parameters during inference using consensus-based pseudolabels from TTAug.
Through extensive experiments across nine benchmarks, we demonstrate consistent
performance improvements while maintaining computational efficiency suitable
for resource-constrained environments. The generality of our approach is
demonstrated both within models at different scales and across different VLMs
without additional tuning.

</details>


### [269] [BEKAN: Boundary condition-guaranteed evolutionary Kolmogorov-Arnold networks with radial basis functions for solving PDE problems](https://arxiv.org/abs/2510.03576)
*Bongseok Kim,Jiahao Zhang,Guang Lin*

Main category: cs.LG

TL;DR: BEKAN is a boundary-condition-guaranteed evolutionary Kolmogorov-Arnold Network that uses boundary-embedded radial basis functions, a periodic sinusoidal layer, and a least-squares Neumann term to solve PDEs with Dirichlet, periodic, Neumann, and mixed boundary conditions, achieving higher accuracy than MLP and B-splines KAN.


<details>
  <summary>Details</summary>
Motivation: The black-box nature of neural networks makes enforcing boundary conditions in PDE solvers difficult and unreliable. There is a need for architectures that explicitly guarantee boundary condition satisfaction to improve accuracy and trustworthiness.

Method: Three integrated components: (1) Dirichlet: smooth global Gaussian RBFs form univariate basis functions and embed boundary information at the activation level; (2) periodic: a periodic layer built from sinusoidal functions enforces exact periodic boundary conditions; (3) Neumann: a least-squares formulation guides parameter updates to satisfy Neumann conditions. All are embedded in a boundary-aware evolutionary KAN framework (BEKAN) and tested on Dirichlet, Neumann, periodic, and mixed BCs.

Result: BEKAN outperforms both multilayer perceptron (MLP) and B-splines KAN in terms of accuracy across Dirichlet, Neumann, periodic, and mixed boundary value problems, as demonstrated by extensive numerical experiments.

Conclusion: The proposed BEKAN enhances KANs for PDEs by guaranteeing boundary-condition satisfaction, advancing reliable scientific computing and engineering applications.

Abstract: Deep learning has gained attention for solving PDEs, but the black-box nature
of neural networks hinders precise enforcement of boundary conditions. To
address this, we propose a boundary condition-guaranteed evolutionary
Kolmogorov-Arnold Network (KAN) with radial basis functions (BEKAN). In BEKAN,
we propose three distinct and combinable approaches for incorporating
Dirichlet, periodic, and Neumann boundary conditions into the network. For
Dirichlet problem, we use smooth and global Gaussian RBFs to construct
univariate basis functions for approximating the solution and to encode
boundary information at the activation level of the network. To handle periodic
problems, we employ a periodic layer constructed from a set of sinusoidal
functions to enforce the boundary conditions exactly. For a Neumann problem, we
devise a least-squares formulation to guide the parameter evolution toward
satisfying the Neumann condition. By virtue of the boundary-embedded RBFs, the
periodic layer, and the evolutionary framework, we can perform accurate PDE
simulations while rigorously enforcing boundary conditions. For demonstration,
we conducted extensive numerical experiments on Dirichlet, Neumann, periodic,
and mixed boundary value problems. The results indicate that BEKAN outperforms
both multilayer perceptron (MLP) and B-splines KAN in terms of accuracy. In
conclusion, the proposed approach enhances the capability of KANs in solving
PDE problems while satisfying boundary conditions, thereby facilitating
advancements in scientific computing and engineering applications.

</details>


### [270] [Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning](https://arxiv.org/abs/2510.03578)
*Haoran Li,Chenhan Xiao,Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: Latent MoS is a mixture-of-symmetries model for learning dynamics that captures multiple local symmetry-governed latent factors, stacks MoS blocks for long-term equivariance, and achieves superior interpolation/extrapolation with interpretable latent representations.


<details>
  <summary>Details</summary>
Motivation: In many engineering systems, measurements are scarce; symmetry provides a strong inductive bias for sample-efficient learning. Existing methods often assume a single global symmetry and decouple symmetry discovery from dynamics learning, which limits expressiveness and leads to error accumulation. A mixture of symmetries with integrated learning could enhance expressiveness, reduce error propagation, and capture long-horizon equivariance.

Method: Introduce Latent MoS (Latent Mixture of Symmetries), a model that learns a mixture of symmetry-governed latent factors from complex dynamical data. It focuses on dynamic learning while locally preserving symmetric transformations. A hierarchical stack of MoS blocks captures long-term equivariance.

Result: Empirical results on diverse physical systems show Latent MoS outperforms state-of-the-art baselines in both interpolation and extrapolation tasks. It also yields interpretable latent representations that are suitable for downstream geometric analyses and safety-critical considerations.

Conclusion: Latent MoS provides a flexible, more expressive framework for learning dynamics under symmetry, enabling improved sample efficiency, better long-horizon behavior, and interpretable latent factors that align with geometric and safety needs.

Abstract: Learning dynamics is essential for model-based control and Reinforcement
Learning in engineering systems, such as robotics and power systems. However,
limited system measurements, such as those from low-resolution sensors, demand
sample-efficient learning. Symmetry provides a powerful inductive bias by
characterizing equivariant relations in system states to improve sample
efficiency. While recent methods attempt to discover symmetries from data, they
typically assume a single global symmetry group and treat symmetry discovery
and dynamic learning as separate tasks, leading to limited expressiveness and
error accumulation. In this paper, we propose the Latent Mixture of Symmetries
(Latent MoS), an expressive model that captures a mixture of symmetry-governed
latent factors from complex dynamical measurements. Latent MoS focuses on
dynamic learning while locally and provably preserving the underlying symmetric
transformations. To further capture long-term equivariance, we introduce a
hierarchical architecture that stacks MoS blocks. Numerical experiments in
diverse physical systems demonstrate that Latent MoS outperforms
state-of-the-art baselines in interpolation and extrapolation tasks while
offering interpretable latent representations suitable for future geometric and
safety-critical analyses.

</details>


### [271] [FieldFormer: Physics-Informed Transformers for Spatio-Temporal Field Reconstruction from Sparse Sensors](https://arxiv.org/abs/2510.03589)
*Ankit Bhardwaj,Ananth Balashankar,Lakshminarayanan Subramanian*

Main category: cs.LG

TL;DR: A transformer-based mesh-free framework (FieldFormer) that reconstructs spatio-temporal fields from sparse, noisy data by combining data-driven learning with physics-based PDE constraints, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Sparse, noisy, and irregular spatio-temporal data pose challenges for interpolation and learning; many methods ignore governing PDEs or do not scale, underscoring the need for a framework that is both flexible and physics-consistent.

Method: For each query, FieldFormer builds a local neighborhood using a learnable velocity-scaled distance metric to enable anisotropic adaptation. Neighborhoods are assembled efficiently via per-batch offset recomputation and refined in an EM-like fashion as velocity scales evolve. Predictions come from a local transformer encoder, with physics consistency enforced through autograd-based PDE residuals and boundary-specific penalties.

Result: FieldFormer consistently outperforms strong baselines by more than 40% across three benchmarks (scalar anisotropic heat equation, vector shallow-water system, and advection-diffusion pollution simulation). It achieves accurate reconstruction with RMSE < 0.01, from sparse data (0.4%-2%) and noisy data (~10%). It is efficient and physically consistent.

Conclusion: FieldFormer demonstrates that combining data-driven transformer-based learning with physics-based constraints enables accurate, efficient, and physically plausible mesh-free spatio-temporal field reconstruction from highly sparse and noisy observations.

Abstract: Spatio-temporal sensor data is often sparse, noisy, and irregular, and
existing interpolation or learning methods struggle here because they either
ignore governing PDEs or do not scale. We introduce FieldFormer, a
transformer-based framework for mesh-free spatio-temporal field reconstruction
that combines data-driven flexibility with physics-based structure. For each
query, FieldFormer gathers a local neighborhood using a learnable
velocity-scaled distance metric, enabling anisotropic adaptation to different
propagation regimes. Neighborhoods are built efficiently via per-batch offset
recomputation, and refined in an expectation-maximization style as the velocity
scales evolve. Predictions are made by a local transformer encoder, and physics
consistency is enforced through autograd-based PDE residuals and
boundary-specific penalties. Across three benchmarks--a scalar anisotropic heat
equation, a vector-valued shallow-water system, and a realistic
advection-diffusion pollution simulation--FieldFormer consistently outperforms
strong baselines by more than 40%. Our results demonstrate that FieldFormer
enables accurate (RMSE$<10^{-2}$), efficient, and physically consistent field
reconstruction from sparse (0.4%-2%) and noisy(10%) data.

</details>


### [272] [MECKD: Deep Learning-Based Fall Detection in Multilayer Mobile Edge Computing With Knowledge Distillation](https://arxiv.org/abs/2510.03601)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Kai-Chun Liu,Yu Tsao*

Main category: cs.LG

TL;DR: A multilayer MEC framework with knowledge distillation (KD) improves fall-detection accuracy and reduces data latency, achieving notable gains on SisFall and FallAllD datasets.


<details>
  <summary>Details</summary>
Motivation: An aging population makes reliable fall detection essential. Edge devices have limited capacity and cloud latency is an issue. A multilayer MEC (MLMEC) with KD is proposed to balance accuracy and latency by leveraging back-end stations to enhance front-end detection.

Method: MLMEC architecture with multiple stations; data can be routed to more capable back-end stations when front-end detection is unreliable; KD is used so high-power back-end stations provide additional learning experiences to improve front-end accuracy while reducing latency and processing load.

Result: KD improves detection accuracy by 11.65% on SisFall and 2.78% on FallAllD. Latency reductions with KD are 46.67% (SisFall) and 54.15% (FallAllD) compared to MLMEC without KD.

Conclusion: MLMEC with KD enhances fall-detection performance and lowers latency, demonstrating the viability of a layered edge-cloud approach with knowledge distillation for FD systems.

Abstract: The rising aging population has increased the importance of fall detection
(FD) systems as an assistive technology, where deep learning techniques are
widely applied to enhance accuracy. FD systems typically use edge devices (EDs)
worn by individuals to collect real-time data, which are transmitted to a cloud
center (CC) or processed locally. However, this architecture faces challenges
such as a limited ED model size and data transmission latency to the CC. Mobile
edge computing (MEC), which allows computations at MEC servers deployed between
EDs and CC, has been explored to address these challenges. We propose a
multilayer MEC (MLMEC) framework to balance accuracy and latency. The MLMEC
splits the architecture into stations, each with a neural network model. If
front-end equipment cannot detect falls reliably, data are transmitted to a
station with more robust back-end computing. The knowledge distillation (KD)
approach was employed to improve front-end detection accuracy by allowing
high-power back-end stations to provide additional learning experiences,
enhancing precision while reducing latency and processing loads. Simulation
results demonstrate that the KD approach improved accuracy by 11.65% on the
SisFall dataset and 2.78% on the FallAllD dataset. The MLMEC with KD also
reduced the data latency rate by 54.15% on the FallAllD dataset and 46.67% on
the SisFall dataset compared to the MLMEC without KD. In summary, the MLMEC FD
system exhibits improved accuracy and reduced latency.

</details>


### [273] [Deep Domain Adaptation for Turbofan Engine Remaining Useful Life Prediction: Methodologies, Evaluation and Future Trends](https://arxiv.org/abs/2510.03604)
*Yucheng Wang,Mohamed Ragab,Yubo Hou,Zhenghua Chen,Min Wu,Xiaoli Li*

Main category: cs.LG

TL;DR: A comprehensive, turbofan-specific review of domain adaptation for remaining useful life (RUL) prediction, introducing a novel three-dimensional taxonomy and providing empirical evaluation on turbofan datasets.


<details>
  <summary>Details</summary>
Motivation: RUL prediction for turbofan engines is critical for predictive maintenance, but data are scarce and operating-condition-induced distribution shifts hinder learning. Domain Adaptation (DA) offers knowledge transfer across domains, but existing DA methods require tailoring to the turbofan context.

Method: The paper reviews existing DA approaches applied to turbofan RUL, proposing a turbofan-specific taxonomy organized into methodology-based (how DA is applied), alignment-based (where distributional shifts occur due to operational variations), and problem-based (why certain adaptations address domain-specific challenges) categories; it synthesizes methodologies, challenges, and advancements and includes an empirical evaluation of selected DA techniques on turbofan datasets.

Result: The taxonomy provides a multidimensional framework to categorize DA approaches in turbofan RUL; practical insights emerge from the evaluation, highlighting performance considerations and domain-specific challenges; the synthesis identifies gaps and informs practitioners about effective strategies and limitations.

Conclusion: Future research should develop more effective, turbofan-tailored DA techniques for RUL prediction and address remaining challenges identified in the review, guiding the advancement of predictive maintenance for turbofan engines.

Abstract: Remaining Useful Life (RUL) prediction for turbofan engines plays a vital
role in predictive maintenance, ensuring operational safety and efficiency in
aviation. Although data-driven approaches using machine learning and deep
learning have shown potential, they face challenges such as limited data and
distribution shifts caused by varying operating conditions. Domain Adaptation
(DA) has emerged as a promising solution, enabling knowledge transfer from
source domains with abundant data to target domains with scarce data while
mitigating distributional shifts. Given the unique properties of turbofan
engines, such as complex operating conditions, high-dimensional sensor data,
and slower-changing signals, it is essential to conduct a focused review of DA
techniques specifically tailored to turbofan engines. To address this need,
this paper provides a comprehensive review of DA solutions for turbofan engine
RUL prediction, analyzing key methodologies, challenges, and recent
advancements. A novel taxonomy tailored to turbofan engines is introduced,
organizing approaches into methodology-based (how DA is applied),
alignment-based (where distributional shifts occur due to operational
variations), and problem-based (why certain adaptations are needed to address
specific challenges). This taxonomy offers a multidimensional view that goes
beyond traditional classifications by accounting for the distinctive
characteristics of turbofan engine data and the standard process of applying DA
techniques to this area. Additionally, we evaluate selected DA techniques on
turbofan engine datasets, providing practical insights for practitioners and
identifying key challenges. Future research directions are identified to guide
the development of more effective DA techniques, advancing the state of RUL
prediction for turbofan engines.

</details>


### [274] [Explore the Loss space with Hill-ADAM](https://arxiv.org/abs/2510.03613)
*Meenakshi Manikandan,Leilani Gilpin*

Main category: cs.LG

TL;DR: Hill-ADAM is a deterministic optimizer that alternates between minimizing and maximizing the loss to escape local minima and aim for the global minimum, supported by an analytic step-size approximation and tested on multiple losses and color-correction tasks.


<details>
  <summary>Details</summary>
Motivation: to overcome stochastic gradient noise and premature convergence to local minima in ADAM-like optimizers by introducing deterministic exploration of the loss landscape to identify the global minimum.

Method: derive an analytical approximation of the ADAM step size at a given state; define and analyze the limitations of ADAM in escaping local minima; introduce Hill-ADAM which alternates between error minimization and maximization to explore the loss space; infer the global minimum state from this exploration; validate on five loss functions and twelve color-correction instances.

Result: Hill-ADAM enables deterministic exploration that can escape local minima and suggests a pathway to the global minimum; empirical evaluation covers five loss functions and twelve image color-correction tasks, demonstrating the approach's viability.

Conclusion: Hill-ADAM offers a deterministic alternative to stochastic optimizers for escaping local minima in prescribed loss landscapes, with initial experimental support in optimization and color-correction settings; further work could broaden loss terrains and compare with stochastic methods.

Abstract: This paper introduces Hill-ADAM. Hill-ADAM is an optimizer with its focus
towards escaping local minima in prescribed loss landscapes to find the global
minimum. Hill-ADAM escapes minima by deterministically exploring the state
space. This eliminates uncertainty from random gradient updates in stochastic
algorithms while seldom converging at the first minimum that visits. In the
paper we first derive an analytical approximation of the ADAM Optimizer step
size at a particular model state. From there define the primary condition
determining ADAM limitations in escaping local minima. The proposed optimizer
algorithm Hill-ADAM alternates between error minimization and maximization. It
maximizes to escape the local minimum and minimizes again afterward. This
alternation provides an overall exploration throughout the loss space. This
allows the deduction of the global minimum's state. Hill-ADAM was tested with 5
loss functions and 12 amber-saturated to cooler-shade image color correction
instances.

</details>


### [275] [Neural Bayesian Filtering](https://arxiv.org/abs/2510.03614)
*Christopher Solinas,Radovan Haluska,David Sychrovsky,Finbarr Timbers,Nolan Bard,Michael Buro,Martin Schmid,Nathan R. Sturtevant,Michael Bowling*

Main category: cs.LG

TL;DR: Neural Bayesian Filtering (NBF) learns latent belief embeddings to track hidden states in partially observable systems, combining efficient filtering with deep generative expressiveness; validated in three environments.


<details>
  <summary>Details</summary>
Motivation: To maintain distributions over hidden states under partial observability. Classical filters are efficient but limited in expressive power; deep generative models are expressive but can be computationally intensive and prone to particle impoverishment. A method that is both scalable and capable of representing multimodal, rapidly changing beliefs is needed.

Method: NBF trains a latent representation of beliefs and maps beliefs to fixed-length embeddings that condition generative models. During filtering, particle-style updates operate in the embedding space using observations and system dynamics, merging the efficiency of classical filters with the expressiveness of deep generative models and mitigating particle impoverishment.

Result: Empirically validated on state estimation tasks in three partially observable environments.

Conclusion: NBF provides a scalable, expressive filtering framework that preserves and updates complex, multimodal belief distributions in partially observable systems, balancing classical efficiency with neural generative modeling.

Abstract: We present Neural Bayesian Filtering (NBF), an algorithm for maintaining
distributions over hidden states, called beliefs, in partially observable
systems. NBF is trained to find a good latent representation of the beliefs
induced by a task. It maps beliefs to fixed-length embedding vectors, which
condition generative models for sampling. During filtering, particle-style
updates compute posteriors in this embedding space using incoming observations
and the environment's dynamics. NBF combines the computational efficiency of
classical filters with the expressiveness of deep generative models - tracking
rapidly shifting, multimodal beliefs while mitigating the risk of particle
impoverishment. We validate NBF in state estimation tasks in three partially
observable environments.

</details>


### [276] [Predicting Stock Price Movement with LLM-Enhanced Tweet Emotion Analysis](https://arxiv.org/abs/2510.03633)
*An Vuong,Susan Gauch*

Main category: cs.LG

TL;DR: Integrates tweet-emotion features processed via Llama and emotion classifiers with past stock prices to predict next-day significant moves; DistilRoBERTa-based method with LLaMA preprocessing yields the best accuracy (38.5%).


<details>
  <summary>Details</summary>
Motivation: Short-term stock movement is volatile and sensitive to sentiment; leveraging improved emotion signals from social media can boost predictive performance; applying LLM-based preprocessing may enhance feature quality.

Method: Process tweets with Llama 3.1-8B-Instruct; extract emotion features via DistilRoBERTa and NRC lexicon methods; combine with previous-day prices; train LSTM; evaluate on TSLA, AAPL, AMZN; compare to baseline.

Result: All three emotion methods improve accuracy vs baseline 13.5%; DistilRoBERTa with LLaMA enrichment improves from 23.6% to 38.5%; TSLA/AAPL/AMZN used.

Conclusion: Using LLM-based preprocessing of tweet content enhances emotion feature quality, improving accuracy of predicting significant stock price movements.

Abstract: Accurately predicting short-term stock price movement remains a challenging
task due to the market's inherent volatility and sensitivity to investor
sentiment. This paper discusses a deep learning framework that integrates
emotion features extracted from tweet data with historical stock price
information to forecast significant price changes on the following day. We
utilize Meta's Llama 3.1-8B-Instruct model to preprocess tweet data, thereby
enhancing the quality of emotion features derived from three emotion analysis
approaches: a transformer-based DistilRoBERTa classifier from the Hugging Face
library and two lexicon-based methods using National Research Council Canada
(NRC) resources. These features are combined with previous-day stock price data
to train a Long Short-Term Memory (LSTM) model. Experimental results on TSLA,
AAPL, and AMZN stocks show that all three emotion analysis methods improve the
average accuracy for predicting significant price movements, compared to the
baseline model using only historical stock prices, which yields an accuracy of
13.5%. The DistilRoBERTa-based stock prediction model achieves the best
performance, with accuracy rising from 23.6% to 38.5% when using LLaMA-enhanced
emotion analysis. These results demonstrate that using large language models to
preprocess tweet content enhances the effectiveness of emotion analysis which
in turn improves the accuracy of predicting significant stock price movements.

</details>


### [277] [From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse](https://arxiv.org/abs/2510.03636)
*Rabeya Amin Jhuma,Mostafa Mohaimen Akand Faisal*

Main category: cs.LG

TL;DR: ICL in LLMs is highly vulnerable to small data-poisoning attacks in a health sentiment task; a spectral signature defense can filter poisoned data and preserve reliability.


<details>
  <summary>Details</summary>
Motivation: Assess robustness of in-context learning against data poisoning in a high-stakes domain (public health sentiment analysis on social media).

Method: Introduce adversarial perturbations (synonym replacement, negation insertion, randomized perturbation) to ICL support examples; measure sentiment flip rates; apply Spectral Signature Defense to filter poisoned instances; evaluate ICL accuracy post-defense and validate with logistic regression.

Result: Sentiment labels flipped in up to 67% of cases due to poisoning; after defense, ICL accuracy ~46.7%; logistic regression validation achieved 100% accuracy, indicating preserved data integrity.

Conclusion: Demonstrates fragility of in-context learning under poisoning in health discourse and shows spectral defenses can preserve data integrity and improve reliability for health-related social media monitoring.

Abstract: This study explored how in-context learning (ICL) in large language models
can be disrupted by data poisoning attacks in the setting of public health
sentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small
adversarial perturbations such as synonym replacement, negation insertion, and
randomized perturbation were introduced into the support examples. Even these
minor manipulations caused major disruptions, with sentiment labels flipping in
up to 67% of cases. To address this, a Spectral Signature Defense was applied,
which filtered out poisoned examples while keeping the data's meaning and
sentiment intact. After defense, ICL accuracy remained steady at around 46.7%,
and logistic regression validation reached 100% accuracy, showing that the
defense successfully preserved the dataset's integrity. Overall, the findings
extend prior theoretical studies of ICL poisoning to a practical, high-stakes
setting in public health discourse analysis, highlighting both the risks and
potential defenses for robust LLM deployment. This study also highlights the
fragility of ICL under attack and the value of spectral defenses in making AI
systems more reliable for health-related social media monitoring.

</details>


### [278] [Implicit Models: Expressive Power Scales with Test-Time Compute](https://arxiv.org/abs/2510.03638)
*Jialin Liu,Lisang Ding,Stanley Osher,Wotao Yin*

Main category: cs.LG

TL;DR: A simple, regular implicit operator iterated to convergence can progressively express more complex mappings; test-time compute controls expressive power, enabling memory-efficient implicit models to match richer explicit models. The theory is supported by experiments in image reconstruction, scientific computing, and operations research.


<details>
  <summary>Details</summary>
Motivation: To understand why compact, memory-efficient implicit models can achieve or exceed the performance of large explicit networks and why their expressiveness seems to grow with test-time computation.

Method: Nonparametric theoretical analysis of expressive power. Provide a mathematical characterization showing that iterating a simple implicit operator expands the set of expressible mappings; prove that, for a broad class of implicit models, expressiveness scales with test-time iterations; validate the theory on three domains by observing increasing mapping complexity and improving/stabilizing solution quality with more iterations.

Result: The expressive power of the model scales with test-time iterations, enabling it to approximate a much richer function class. As iterations increase, the learned mappings become more complex, and solution quality improves and stabilizes across the evaluated domains (image reconstruction, scientific computing, operations research).

Conclusion: Memor y-efficient implicit models can achieve high expressiveness controlled by test-time computation. The results provide a theoretical basis for the empirical success of implicit networks and a framework to analyze compute–accuracy trade-offs when designing such models.

Abstract: Implicit models, an emerging model class, compute outputs by iterating a
single parameter block to a fixed point. This architecture realizes an
infinite-depth, weight-tied network that trains with constant memory,
significantly reducing memory needs for the same level of performance compared
to explicit models. While it is empirically known that these compact models can
often match or even exceed larger explicit networks by allocating more
test-time compute, the underlying mechanism remains poorly understood.
  We study this gap through a nonparametric analysis of expressive power. We
provide a strict mathematical characterization, showing that a simple and
regular implicit operator can, through iteration, progressively express more
complex mappings. We prove that for a broad class of implicit models, this
process lets the model's expressive power scale with test-time compute,
ultimately matching a much richer function class. The theory is validated
across three domains: image reconstruction, scientific computing, and
operations research, demonstrating that as test-time iterations increase, the
complexity of the learned mapping rises, while the solution quality
simultaneously improves and stabilizes.

</details>


### [279] [In-Vivo Training for Deep Brain Stimulation](https://arxiv.org/abs/2510.03643)
*Nicholas Carter,Arkaprava Gupta,Prateek Ganguli,Benedikt Dietrich,Vibhor Krishna,Samarjit Chakraborty*

Main category: cs.LG

TL;DR: An RL-based DBS controller using TD3 adapts stimulation from brain activity measurable in vivo, outperforming standard clinical DBS in suppressing PD-related biomarkers.


<details>
  <summary>Details</summary>
Motivation: To overcome reliance on brain biomarkers only available in brain-on-chip simulations and enable real-world, patient-specific adaptive DBS.

Method: Train a TD3-based reinforcement learning agent on a basal ganglia model to modulate DBS stimulation frequency and amplitude using brain activity signals measurable in vivo, and compare against standard clinical DBS.

Result: The RL agent achieves greater suppression of PD biomarkers correlated with disease severity and outperforms standard clinical DBS approaches.

Conclusion: This approach demonstrates feasibility of in vivo-measurable-signal-driven RL DBS and holds promise for personalized, adaptive therapies tailored to individual patients.

Abstract: Deep Brain Stimulation (DBS) is a highly effective treatment for Parkinson's
Disease (PD). Recent research uses reinforcement learning (RL) for DBS, with RL
agents modulating the stimulation frequency and amplitude. But, these models
rely on biomarkers that are not measurable in patients and are only present in
brain-on-chip (BoC) simulations. In this work, we present an RL-based DBS
approach that adapts these stimulation parameters according to brain activity
measurable in vivo. Using a TD3 based RL agent trained on a model of the basal
ganglia region of the brain, we see a greater suppression of biomarkers
correlated with PD severity compared to modern clinical DBS implementations.
Our agent outperforms the standard clinical approaches in suppressing PD
biomarkers while relying on information that can be measured in a real world
environment, thereby opening up the possibility of training personalized RL
agents specific to individual patient needs.

</details>


### [280] [SAFA-SNN: Sparsity-Aware On-Device Few-Shot Class-Incremental Learning with Fast-Adaptive Structure of Spiking Neural Network](https://arxiv.org/abs/2510.03648)
*Huijing Zhang,Muyang Cao,Linshan Jiang,Xin Du,Di Yu,Changze Lv,Shuiguang Deng*

Main category: cs.LG

TL;DR: Proposes SAFA-SNN, a sparsity-aware spiking neural network for on-device few-shot class-incremental learning (FSCIL). It uses sparsity-conditioned neuronal dynamics to reduce forgetting, zeroth-order optimization to handle non-differentiable spikes, and subspace projection to improve new-class discriminability. Demonstrates improved accuracy and lower energy vs. baselines on CIFAR100, Mini-ImageNet, and three neuromorphic datasets, with at least 4.01% gains on Mini-ImageNet and ~20% energy savings at the final session.


<details>
  <summary>Details</summary>
Motivation: Edge devices face continual learning with data privacy constraints and dynamic environments. FSCIL with limited data on-device is challenging. Spiking neural networks offer energy efficiency and neuromorphic compatibility, motivating their use for on-device FSCIL.

Method: 1) Sparsity-conditioned neuronal dynamics where most neurons stay stable and a subset activates to mitigate forgetting. 2) Zeroth-order optimization to optimize the non-differentiable spiking neurons. 3) Subspace projection during incremental sessions to boost discriminability of new classes and curb overfitting.

Result: SAFA-SNN outperforms baselines across multiple datasets, achieving at least 4.01% improvement at the last Mini-ImageNet session and about 20% lower energy consumption relative to baselines, with validation on CIFAR100, Mini-ImageNet, CIFAR-10-DVS, DVS128gesture, and N-Caltech101.

Conclusion: SAFA-SNN is a promising on-device FSCIL solution, delivering better accuracy and energy efficiency while aligning with neuromorphic hardware capabilities; demonstrates practical viability for continual learning on resource-constrained edge devices.

Abstract: Continuous learning of novel classes is crucial for edge devices to preserve
data privacy and maintain reliable performance in dynamic environments.
However, the scenario becomes particularly challenging when data samples are
insufficient, requiring on-device few-shot class-incremental learning (FSCIL)
to maintain consistent model performance. Although existing work has explored
parameter-efficient FSCIL frameworks based on artificial neural networks
(ANNs), their deployment is still fundamentally constrained by limited device
resources. Inspired by neural mechanisms, Spiking neural networks (SNNs)
process spatiotemporal information efficiently, offering lower energy
consumption, greater biological plausibility, and compatibility with
neuromorphic hardware than ANNs. In this work, we present an SNN-based method
for On-Device FSCIL, i.e., Sparsity-Aware and Fast Adaptive SNN (SAFA-SNN). We
first propose sparsity-conditioned neuronal dynamics, in which most neurons
remain stable while a subset stays active, thereby mitigating catastrophic
forgetting. To further cope with spike non-differentiability in gradient
estimation, we employ zeroth-order optimization. Moreover, during incremental
learning sessions, we enhance the discriminability of new classes through
subspace projection, which alleviates overfitting to novel classes. Extensive
experiments conducted on two standard benchmark datasets (CIFAR100 and
Mini-ImageNet) and three neuromorphic datasets (CIFAR-10-DVS, DVS128gesture,
and N-Caltech101) demonstrate that SAFA-SNN outperforms baseline methods,
specifically achieving at least 4.01% improvement at the last incremental
session on Mini-ImageNet and 20% lower energy cost over baseline methods with
practical implementation.

</details>


### [281] [LLM-Guided Evolutionary Program Synthesis for Quasi-Monte Carlo Design](https://arxiv.org/abs/2510.03650)
*Amir Sadikov*

Main category: cs.LG

TL;DR: The paper shows an LLM-guided evolutionary framework to design low-discrepancy point sets and Sobol' direction numbers for QMC. It co-creates finite 2D/3D point sets with low star discrepancy and tunes Sobol' parameters to minimize rQMC error, achieving new benchmarks (especially in 2D and some 3D cases) and improved 32D option-pricing performance, while maintaining extensibility and compatibility with standard randomizations.


<details>
  <summary>Details</summary>
Motivation: Quasi-Monte Carlo methods rely on high-quality low-discrepancy sequences; two persistent design problems—finite point sets with low star discrepancy and Sobol' direction numbers for digital sequences—lack automated, scalable design methods. The work aims to automate discovery using LLM-guided evolutionary programming to push beyond hand-tuned configurations.

Method: A two-phase approach: (1) constructive code proposals that generate candidate QMC constructions, and (2) iterative numerical refinement. An LLM-guided evolutionary loop mutates and selects code under task-specific fitness functions, combining constructive synthesis with numerical optimization. They apply this to (i) finite 2D/3D point sets to minimize star discrepancy, and (ii) Sobol' direction numbers to minimize rQMC error in downstream tasks, with evaluation on finite-N benchmarks and 32D option-pricing tasks.

Result: Finite sets: recover known optima in small 2D, set new best-known benchmarks for N≥40; 3D results approach proven frontier (N≤8) and show improvements beyond. Digital sequences: evolved Sobol' parameters yield consistent reductions in rQMC MSE for several 32D options tasks vs Joe–Kuo parameters; method remains extensible to arbitrary sample sizes and compatible with standard randomizations.

Conclusion: LLM-guided evolutionary program synthesis can automate high-quality QMC construction discovery, reproducing classical optimal designs where applicable and improving them where finite-N structure matters, with data and code openly available for reproduction.

Abstract: Low-discrepancy point sets and digital sequences underpin quasi-Monte Carlo
(QMC) methods for high-dimensional integration. We cast two long-standing QMC
design problems as program synthesis and solve them with an LLM-guided
evolutionary loop that mutates and selects code under task-specific fitness:
(i) constructing finite 2D/3D point sets with low star discrepancy, and (ii)
choosing Sobol' direction numbers that minimize randomized QMC error on
downstream integrands. Our two-phase procedure combines constructive code
proposals with iterative numerical refinement. On finite sets, we rediscover
known optima in small 2D cases and set new best-known 2D benchmarks for N >=
40, while matching most known 3D optima up to the proven frontier (N <= 8) and
reporting improved 3D benchmarks beyond. On digital sequences, evolving Sobol'
parameters yields consistent reductions in randomized quasi-Monte Carlo (rQMC)
mean-squared error for several 32-dimensional option-pricing tasks relative to
widely used Joe--Kuo parameters, while preserving extensibility to any sample
size and compatibility with standard randomizations. Taken together, the
results demonstrate that LLM-driven evolutionary program synthesis can automate
the discovery of high-quality QMC constructions, recovering classical designs
where they are optimal and improving them where finite-N structure matters.
Data and code are available at
https://github.com/hockeyguy123/openevolve-star-discrepancy.git.

</details>


### [282] [Optimising Battery Energy Storage System Trading via Energy Market Operator Price Forecast](https://arxiv.org/abs/2510.03657)
*Aymeric Fabre*

Main category: cs.LG

TL;DR: A forecast-informed BESS trading model leverages AEMO price forecasts to optimize arbitrage in the Australian market, evaluating forecast reliability across time-of-day, horizon, and regions, benchmarking against a naive strategy, and exploring ML enhancements to improve profitability.


<details>
  <summary>Details</summary>
Motivation: Grid volatility from renewables and market decentralisation creates a need to translate forecast data into actionable trading strategies for battery energy storage systems (BESS); there is a gap in practical frameworks that use AEMO forecasts to drive profitable BESS trading in the NEM.

Method: Analyze forecast accuracy patterns by time of day, forecast horizon, and region; develop a forecast-informed BESS trading algorithm to optimize arbitrage returns; benchmark its performance against a basic non-forecast trading algorithm; explore machine learning techniques to enhance AEMO forecasts for governance of a more advanced trading strategy.

Result: Develops a novel forecast-informed BESS trading model and benchmarks it against a naive strategy; identifies how forecast accuracy varies with time, horizon, and region; investigates ML methods to improve forecasts and trading decisions; demonstrates potential profitability gains and outlines practical implications for NEM operations.

Conclusion: Forecast-informed trading for BESS using AEMO forecasts shows promise for improving arbitrage returns and guiding more efficient integration of storage into market operations; future work should refine forecast reliability assessments and integrate advanced ML-based forecast improvements into trading strategies.

Abstract: In electricity markets around the world, the ability to anticipate price
movements with precision can be the difference between profit and loss,
especially for fast-acting assets like battery energy storage systems (BESS).
As grid volatility increases due to renewables and market decentralisation,
operators and forecasters alike face growing pressure to transform prediction
into strategy. Yet while forecast data is abundant, especially in advanced
markets like Australia's National Electricity Market (NEM), its practical value
in driving real-world BESS trading decisions remains largely unexplored. This
thesis dives into that gap. This work addresses a key research question: Can
the accuracy of the Australian Energy Market Operator (AEMO) energy price
forecasts be systematically leveraged to develop a reliable and profitable
battery energy storage system trading algorithm? Despite the availability of
AEMO price forecasts, no existing framework evaluates their reliability or
incorporates them into practical BESS trading strategies. By analysing patterns
in forecast accuracy based on time of day, forecast horizon, and regional
variations, this project creates a novel, forecast-informed BESS trading model
to optimise arbitrage financial returns. The performance of this
forecast-driven algorithm is benchmarked against a basic trading algorithm with
no knowledge of forecast data. The study further explores the potential of
machine learning techniques to predict future energy prices by enhancing AEMO
forecasts to govern a more advanced trading strategy. The research outcomes
will inform future improvements in energy market trading models and promote
more efficient BESS integration into market operations.

</details>


### [283] [Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders](https://arxiv.org/abs/2510.03659)
*Xu Wang,Yan Hu,Benyou Wang,Difan Zou*

Main category: cs.LG

TL;DR: Interpretability of Sparse Autoencoders poorly predicts steering efficacy; a new Delta Token Confidence criterion improves steering and decouples interpretability from utility.


<details>
  <summary>Details</summary>
Motivation: To address whether higher interpretability of SAEs translates to better steering of LLMs, given reliance on interpretable features for control.

Method: Train 90 SAEs across three LLMs (Gemma-2-2B, Qwen-2.5-3B, Gemma-2-9B) over five architectures and six sparsity levels; evaluate interpretability via SAEBench and steering utility via AxBench; compute Kendall's tau-b to assess rank agreement; develop Delta Token Confidence (DTC) to select features by impact on next-token distribution; compare steering gains and re-assess correlation.

Result: Weak correlation between interpretability and steering (tau-b ~0.298); interpretability insufficient as steering proxy; not all SAE features are equally effective; DTC-led feature selection yields 52.52% improvement in steering over current best criterion; after DTC selection, interpretability-utility correlation vanishes or becomes negative (tau-b ~0).

Conclusion: Interpretability alone cannot guarantee steering utility; feature selection based on actual impact on token distribution is crucial; high-utility steering features may be less interpretable, widening the gap between interpretability and utility.

Abstract: Sparse Autoencoders (SAEs) are widely used to steer large language models
(LLMs), based on the assumption that their interpretable features naturally
enable effective model behavior steering. Yet, a fundamental question remains
unanswered: does higher interpretability indeed imply better steering utility?
To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B,
Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels,
and evaluate their interpretability and steering utility based on SAEBench
(arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a
rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis
reveals only a relatively weak positive association (tau b approx 0.298),
indicating that interpretability is an insufficient proxy for steering
performance. We conjecture the interpretability utility gap may stem from the
selection of SAE features, as not all of them are equally effective for
steering. To further find features that truly steer the behavior of LLMs, we
propose a novel selection criterion called Delta Token Confidence, which
measures how much amplifying a feature changes the next token distribution. We
show that our method improves the steering performance of three LLMs by 52.52
percent compared to the current best output score based criterion
(arXiv:2503.34567). Strikingly, after selecting features with high Delta Token
Confidence, the correlation between interpretability and utility vanishes (tau
b approx 0), and can even become negative. This further highlights the
divergence between interpretability and utility for the most effective steering
features.

</details>


### [284] [Operationalizing Data Minimization for Privacy-Preserving LLM Prompting](https://arxiv.org/abs/2510.03662)
*Jijie Zhou,Niloofar Mireshghallah,Tianshi Li*

Main category: cs.LG

TL;DR: Formal data minimization framework for LLM prompts using a priority-queue tree search to minimize privacy-revealing disclosure while preserving utility; evaluated on four datasets with nine LLMs; finds larger frontier LLMs tolerate more minimization and exposes a bias toward abstraction leading to oversharing; indicates a privacy and capability gap.


<details>
  <summary>Details</summary>
Motivation: Privacy risks from user sharing in consumer LLMs; need a formal, operational measure of the minimum information necessary to achieve a given task.

Method: Define data minimization as a formal privacy-utility optimization. Use a privacy-ordered transformation space and a priority-queue tree search to locate the optimal point that preserves task utility with minimal disclosed data. Empirically evaluated on four datasets (ShareGPT, WildChat, CaseHold, MedQA) across nine LLMs as response models; compare against search-derived benchmarks.

Result: Larger frontier LLMs tolerate stronger data minimization (e.g., GPT-5 ~85.7% redaction) than smaller models (e.g., Qwen2.5-0.5B ~19.3%). LLMs struggle to predict optimal minimization, showing a bias toward abstraction and oversharing. Demonstrates a privacy gap and a capability gap: models may lack awareness of what information is actually needed to solve a task.

Conclusion: Data minimization is achievable and beneficial, especially with larger models, but predicting optimal minimization is nontrivial. Requires better alignment of models’ internal representations with task information needs and possibly new prompting or training strategies to reduce oversharing.

Abstract: The rapid deployment of large language models (LLMs) in consumer applications
has led to frequent exchanges of personal information. To obtain useful
responses, users often share more than necessary, increasing privacy risks via
memorization, context-based personalization, or security breaches. We present a
framework to formally define and operationalize data minimization: for a given
user prompt and response model, quantifying the least privacy-revealing
disclosure that maintains utility, and we propose a priority-queue tree search
to locate this optimal point within a privacy-ordered transformation space. We
evaluated the framework on four datasets spanning open-ended conversations
(ShareGPT, WildChat) and knowledge-intensive tasks with single-ground-truth
answers (CaseHold, MedQA), quantifying achievable data minimization with nine
LLMs as the response model. Our results demonstrate that larger frontier LLMs
can tolerate stronger data minimization while maintaining task quality than
smaller open-source models (85.7% redaction for GPT-5 vs. 19.3% for
Qwen2.5-0.5B). By comparing with our search-derived benchmarks, we find that
LLMs struggle to predict optimal data minimization directly, showing a bias
toward abstraction that leads to oversharing. This suggests not just a privacy
gap, but a capability gap: models may lack awareness of what information they
actually need to solve a task.

</details>


### [285] [Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning](https://arxiv.org/abs/2510.03669)
*Wenlong Deng,Yi Ren,Yushu Li,Boying Gong,Danica J. Sutherland,Xiaoxiao Li,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: Token Hidden Reward (THR) quantifies each token's influence on outcomes under GRPO and enables THR-guided reweighting to steer training toward exploitation (positive THR) or exploration (negative THR), improving respective performance metrics.


<details>
  <summary>Details</summary>
Motivation: Address the open problem of explicitly steering RL-tuning of large language models toward exploration or exploitation while using verifiable rewards; investigate token-level signals and dynamic reweighting.

Method: Define THR as a token-level metric of influence on the likelihood of correct responses under Group Relative Policy Optimization (GRPO); observe training dynamics dominated by a small set of high-THR tokens; propose a THR-guided reweighting algorithm to bias GRPO learning signals toward exploitation or exploration; show compatibility with GSPO and generalization across architectures like Llama; validate on math reasoning benchmarks.

Result: THR-guided reweighting improves exploitation-focused training (greedy decoding accuracy) when positive THR tokens are amplified and negative THR tokens weakened; the reverse strategy boosts exploration-focused performance (Pass@K); demonstrates compatibility with GSPO and cross-architecture generalization.

Conclusion: THR offers a principled, fine-grained mechanism to dynamically control exploration and exploitation in RL-tuned LLMs, enabling targeted fine-tuning for reasoning-intensive tasks and compatible with existing RL objectives.

Abstract: Reinforcement learning with verifiable rewards has significantly advanced the
reasoning capabilities of large language models, yet how to explicitly steer
training toward exploration or exploitation remains an open problem. We
introduce Token Hidden Reward (THR), a token-level metric that quantifies each
token's influence on the likelihood of correct responses under Group Relative
Policy Optimization (GRPO). We find that training dynamics are dominated by a
small subset of tokens with high absolute THR values. Most interestingly,
tokens with positive THR strengthen confidence in correct outputs, thus
favoring exploitation, while tokens with negative THR preserve probability mass
for alternative outputs, enabling exploration. This insight suggests a natural
intervention: a THR-guided reweighting algorithm that modulates GRPO's learning
signals to explicitly bias training toward exploitation or exploration. We
validate the efficacy of this algorithm on diverse math reasoning benchmarks.
By amplifying tokens with positive THR value and weakening negative ones, our
algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse
strategy yields consistent gains in Pass@K accuracy, favoring exploration. We
further demonstrate that our algorithm integrates seamlessly with other RL
objectives such as GSPO and generalizes across architectures including Llama.
These findings establish THR as a principled and fine-grained mechanism for
dynamically controlling exploration and exploitation in RL-tuned LLMs,
providing new tools for targeted fine-tuning in reasoning-intensive
applications.

</details>


### [286] [Towards Sampling Data Structures for Tensor Products in Turnstile Streams](https://arxiv.org/abs/2510.03678)
*Zhao Song,Shenghao Xie,Samson Zhou*

Main category: cs.LG

TL;DR: Defines an 'attention sampler' via importance sampling in streaming settings to reduce the cost of attention in large-scale models; offers theoretical space and update-time analysis and claims broad scalability across architectures.


<details>
  <summary>Details</summary>
Motivation: Large-scale attention mechanisms are computationally expensive; there is a need for streaming, sampling-based approaches that retain performance while reducing resource usage. The work builds on the ℓ2 sampler and advances in LLM attention to motivate a more efficient attention computation.

Method: Introduce the attention sampler concept and define it using importance sampling in a streaming context, inspired by the classical ℓ2 sampler and recent LLM attention techniques; provide theoretical analysis of space and update time and argue for broad applicability.

Result: The paper provides theoretical evidence that the attention sampler reduces computational burden and offers space and update-time guarantees; demonstrates scalability and applicability across various model architectures and domains, though no empirical results are reported in the abstract.

Conclusion: The attention sampler framework is presented as a scalable, broadly applicable method to mitigate the computational cost of attention in large-scale AI models, with potential applicability across diverse architectures and domains.

Abstract: This paper studies the computational challenges of large-scale
attention-based models in artificial intelligence by utilizing importance
sampling methods in the streaming setting. Inspired by the classical definition
of the $\ell_2$ sampler and the recent progress of the attention scheme in
Large Language Models (LLMs), we propose the definition of the attention
sampler. Our approach significantly reduces the computational burden of
traditional attention mechanisms. We analyze the effectiveness of the attention
sampler from a theoretical perspective, including space and update time.
Additionally, our framework exhibits scalability and broad applicability across
various model architectures and domains.

</details>


### [287] [Group Policy Gradient](https://arxiv.org/abs/2510.03679)
*Junhua Chen,Zixi Zhang,Hantao Zhong,Rika Antonova*

Main category: cs.LG

TL;DR: GPG is a critic-free policy-gradient estimator using group-based Monte Carlo advantages, preserving PPO’s clipped objective while reducing critic-related costs, matching or beating PPO on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the computational and hyperparameter costs of training a critic in policy-gradient methods (e.g., PPO) and leverage RLHF-inspired ideas to enable efficient, scalable policy optimization for general MDPs.

Method: Introduce Group Policy Gradient (GPG): replace a learned value function with a group-based Monte Carlo advantage estimator, integrate with PPO’s clipped objective, prove estimator consistency, analyze bias-variance tradeoffs, and demonstrate empirical performance with parallel simulations.

Result: Empirically, GPG matches or outperforms PPO on standard RL benchmarks, and its critic-free design plus better parallelization leads to more efficient use of computational resources.

Conclusion: GPG offers a scalable, critic-free alternative to PPO that preserves key performance guarantees while reducing memory, compute, and hyperparameter costs, enabling more efficient policy optimization for general MDPs.

Abstract: We introduce Group Policy Gradient (GPG), a family of critic-free
policy-gradient estimators for general MDPs. Inspired by the success of GRPO's
approach in Reinforcement Learning from Human Feedback (RLHF), GPG replaces a
learned value function with a group-based Monte Carlo advantage estimator,
removing the memory, compute, and hyperparameter costs of training a critic
while preserving PPO's clipped-objective structure. We prove the consistency of
the GPG estimator, analyze the bias-variance tradeoffs, and demonstrate
empirically that GPG matches or outperforms PPO on standard benchmarks. GPG
makes better use of parallel simulations, which, together with its critic-free
design, results in more efficient use of computational resources than PPO.

</details>


### [288] [From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning](https://arxiv.org/abs/2510.03690)
*Ali Azizpour,Reza Ramezanpour,Ashutosh Sabharwal,Santiago Segarra*

Main category: cs.LG

TL;DR: Proposes a graphon-mixture framework for graphs, enabling model-aware data augmentation and contrastive learning (MGCL and GMAM) by clustering graphs via motif densities and leveraging graph moments.


<details>
  <summary>Details</summary>
Motivation: Real-world graphs arise from mixtures of latent generative models. Existing representation learning methods (like GCL and Mixup) ignore this mixture structure, potentially mixing signals from different generators and hurting learning.

Method: Represent graphs as samples from a mixture of graphons. Use graph moments (motif densities) to cluster graphs by their underlying model. Develop MGCL for model-aware contrastive learning, introduce GMAM (graphon-mixture-aware mixup) that interpolates in a space guided by estimated graphons, and implement a model-aware negative sampling objective that restricts negatives to graphs from other models. Provide a theoretical bound: graphs from graphons with small cut distance have similar motif densities with high probability.

Result: Empirical results show strong performance: MGCL achieves state-of-the-art average rank across eight unsupervised graph datasets; GMAM attains new state-of-the-art accuracy on six of seven supervised datasets. Theoretical guarantee confirms motif-density similarity for close graphon pairs.

Conclusion: Model-aware treatment of graph mixtures via graphon clustering and motif densities yields superior augmentation and learning performance in both unsupervised and supervised settings, validating the benefits of considering mixture structure in graph representation learning.

Abstract: Real-world graph datasets often consist of mixtures of populations, where
graphs are generated from multiple distinct underlying distributions. However,
modern representation learning approaches, such as graph contrastive learning
(GCL) and augmentation methods like Mixup, typically overlook this mixture
structure. In this work, we propose a unified framework that explicitly models
data as a mixture of underlying probabilistic graph generative models
represented by graphons. To characterize these graphons, we leverage graph
moments (motif densities) to cluster graphs arising from the same model. This
enables us to disentangle the mixture components and identify their distinct
generative mechanisms. This model-aware partitioning benefits two key graph
learning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a data
augmentation technique that interpolates in a semantically valid space guided
by the estimated graphons, instead of assuming a single graphon per class. 2)
For GCL, it enables model-adaptive and principled augmentations. Additionally,
by introducing a new model-aware objective, our proposed approach (termed MGCL)
improves negative sampling by restricting negatives to graphs from other
models. We establish a key theoretical guarantee: a novel, tighter bound
showing that graphs sampled from graphons with small cut distance will have
similar motif densities with high probability. Extensive experiments on
benchmark datasets demonstrate strong empirical performance. In unsupervised
learning, MGCL achieves state-of-the-art results, obtaining the top average
rank across eight datasets. In supervised learning, GMAM consistently
outperforms existing strategies, achieving new state-of-the-art accuracy in 6
out of 7 datasets.

</details>


### [289] [REG: A Regularization Optimizer for Robust Training Dynamics](https://arxiv.org/abs/2510.03691)
*Zehua Liu,Han Wu,Xiaojin Fu,Shuqi Liu,Xiongwei Han,Tao Zhong,Mingxuan Yuan*

Main category: cs.LG

TL;DR: REG is a new optimizer for LLM training that replaces Muon's aggressive matrix sign operator with a Row-and-Column-Scaling (RACS) operator, offering improved stability and compatibility with AdamW, particularly during fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Muon's structure-aware update can cause instability and incompatibility with AdamW; there is a need for a simpler, more stable operator that preserves AdamW-like dynamics.

Method: Introduce REG using the Row-and-Column-Scaling (RACS) operator to regularize updates by balancing a matrix rather than applying a harsh matrix sign. The approach aims for easier implementation and better compatibility with established training dynamics.

Result: Empirical experiments on LLM training show REG achieves superior performance and stability over AdamW and maintains consistency with AdamW dynamics; REG avoids performance degradation seen with Muon during fine-tuning.

Conclusion: REG offers a stable, performant alternative to Muon that aligns with AdamW training paradigms and is particularly advantageous for fine-tuning LLMs.

Abstract: Optimizers are crucial for the efficient training of Large Language Models
(LLMs). While AdamW is the de facto standard, recent structure-aware optimizers
like Muon have emerged, which regularize gradient updates by operating on
entire weight matrices. The Muon optimizer balances the gradient updates along
all the directions. However, Muon's reliance on the matrix sign function can
lead to training instability, exhibits incompatibility when fine-tuning models
pre-trained with AdamW. To address these limitations, we propose \textbf{REG},
a novel optimizer that replaces Muon's aggressive matrix sign operator with the
Row-and-Column-Scaling (RACS) operator. Theoretically grounded in balancing a
matrix, the RACS operator regularizes the update steps in a less drastic
manner, making it simpler to implement and more compatible with established
training dynamics. Through extensive empirical experiments on LLM training, we
demonstrate that our REG optimizer not only achieves superior performance and
stability over AdamW, but also maintains consistency with the AdamW training
paradigm. This consistency is particularly evident during the fine-tuning
stage, where REG optimizer avoids the performance degradation observed with
Muon.

</details>


### [290] [Balancing Interpretability and Performance in Reinforcement Learning: An Adaptive Spectral Based Linear Approach](https://arxiv.org/abs/2510.03722)
*Qianxin Yi,Shao-Bo Lin,Jun Fan,Yao Wang*

Main category: cs.LG

TL;DR: A spectral-filtered ridge-regression RL method prioritizes interpretability while preserving performance, with adaptive regularization guided by bias-variance trade-off; it comes with theoretical guarantees and strong empirical validation.


<details>
  <summary>Details</summary>
Motivation: There is a tension between interpretability and performance in RL. Many methods rely on post-hoc explanations, but practitioners need methods that are interpretable by design. The work aims to connect RL theory with practical decision-making and trust, especially in management contexts.

Method: Extend ridge regression-based RL with a spectral filter function to form a spectral-based linear RL. Introduce adaptive regularization parameter selection driven by bias-variance trade-off. Provide theoretical near-optimal bounds for parameter estimation and generalization. Validate through simulations and real-world datasets (Kuaishou, Taobao) and perform interpretability analyses of learned policies.

Result: The method achieves near-optimal estimation and generalization bounds. Empirical results show it either outperforms or matches state-of-the-art baselines in decision quality on both simulated environments and real data from Kuaishou and Taobao. Interpretability analyses illustrate how decisions are made, improving user trust.

Conclusion: A unified framework that delivers interpretability, accuracy, and adaptability in RL-based decision making. The spectral approach clarifies the role of regularization, enables adaptive bias-variance-based tuning, and helps bridge RL theory with practical, interpretable deployment.

Abstract: Reinforcement learning (RL) has been widely applied to sequential decision
making, where interpretability and performance are both critical for practical
adoption. Current approaches typically focus on performance and rely on post
hoc explanations to account for interpretability. Different from these
approaches, we focus on designing an interpretability-oriented yet
performance-enhanced RL approach. Specifically, we propose a spectral based
linear RL method that extends the ridge regression-based approach through a
spectral filter function. The proposed method clarifies the role of
regularization in controlling estimation error and further enables the design
of an adaptive regularization parameter selection strategy guided by the
bias-variance trade-off principle. Theoretical analysis establishes
near-optimal bounds for both parameter estimation and generalization error.
Extensive experiments on simulated environments and real-world datasets from
Kuaishou and Taobao demonstrate that our method either outperforms or matches
existing baselines in decision quality. We also conduct interpretability
analyses to illustrate how the learned policies make decisions, thereby
enhancing user trust. These results highlight the potential of our approach to
bridge the gap between RL theory and practical decision making, providing
interpretability, accuracy, and adaptability in management contexts.

</details>


### [291] [Personalized federated prototype learning in mixed heterogeneous data scenarios](https://arxiv.org/abs/2510.03726)
*Jiahao Zeng,Wolong Xing,Liangtao Shi,Xin Huang,Jialin Wang,Zhile Cao,Zhenkui Shi*

Main category: cs.LG

TL;DR: PFPL is a federated learning method that embraces mixed heterogeneity by building personalized, unbiased prototypes for each client and applying consistent regularization during local updates. This yields improved convergence and reduced communication cost, validated on Digits and Office-Caltech datasets.


<details>
  <summary>Details</summary>
Motivation: Conventional FL methods often assume isolated heterogeneity, leading to skewed feature or label distributions. Data heterogeneity can be informative and, if leveraged properly, can improve model performance and reduce communication needs.

Method: Construct per-client personalized, unbiased prototypes to provide domain knowledge and unbiased targets. During local updates, apply consistent regularization to align local instances with their prototypes, enabling effective learning in mixed heterogeneous scenarios.

Result: Experimental evaluation on Digits and Office-Caltech demonstrates the method's effectiveness and its ability to reduce communication costs while improving convergence.

Conclusion: PFPL offers a viable approach to harness heterogeneity in federated learning through personalized prototypes and regularization, with potential for broader applicability and further investigation into scalability and privacy implications.

Abstract: Federated learning has received significant attention for its ability to
simultaneously protect customer privacy and leverage distributed data from
multiple devices for model training. However, conventional approaches often
focus on isolated heterogeneous scenarios, resulting in skewed feature
distributions or label distributions. Meanwhile, data heterogeneity is actually
a key factor in improving model performance. To address this issue, we propose
a new approach called PFPL in mixed heterogeneous scenarios. The method
provides richer domain knowledge and unbiased convergence targets by
constructing personalized, unbiased prototypes for each client. Moreover, in
the local update phase, we introduce consistent regularization to align local
instances with their personalized prototypes, which significantly improves the
convergence of the loss function. Experimental results on Digits and Office
Caltech datasets validate the effectiveness of our approach and successfully
reduce the communication cost.

</details>


### [292] [Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation](https://arxiv.org/abs/2510.03731)
*Yongfu Xue*

Main category: cs.LG

TL;DR: IniLoRA initializes LoRA's low-rank matrices to approximate the original weights, addressing the zero-product bottleneck and improving performance; two variants IniLoRA-α and IniLoRA-β offer further gains.


<details>
  <summary>Details</summary>
Motivation: LoRA’s standard initialization creates a zero product of two low-rank matrices, limiting the ability to activate and leverage the base model weights. A better initialization can more effectively utilize pre-trained weights for parameter-efficient fine-tuning.

Method: Propose IniLoRA, an initialization strategy that initializes the two low-rank matrices to closely approximate the original weight matrix. Additionally introduce two variants, IniLoRA-α and IniLoRA-β, each using distinct initialization schemes to boost performance.

Result: Experimental results show that IniLoRA outperforms LoRA across a range of models and tasks.

Conclusion: IniLoRA provides a simple yet effective improvement to parameter-efficient fine-tuning by better aligning low-rank adapters with the original weights; the α and β variants offer further performance improvements.

Abstract: The rapid development of parameter-efficient fine-tuning methods has
noticeably improved the efficiency of adapting large language models. Among
these, LoRA has gained widespread popularity due to its strong balance of
effectiveness and parameter efficiency. However, LoRA relies on initializing
two low-rank matrices whose product is zero, which limits its ability to
effectively activate and leverage the original model weights-creating a
potential bottleneck for optimal performance. To address this limitation, we
propose \textbf{IniLoRA}, a novel initialization strategy that initializes the
low-rank matrices to closely approximate the original model weights.
Experimental results indicate that IniLoRA achieves better performance than
LoRA across a range of models and tasks. Additionally, we introduce two
variants, IniLoRA-$\alpha$ and IniLoRA-$\beta$, both leveraging distinct
initialization methods to enhance performance further.

</details>


### [293] [Cost Efficient Fairness Audit Under Partial Feedback](https://arxiv.org/abs/2510.03734)
*Nirjhar Das,Mohit Sharma,Praharsh Nanavati,Kirankumar Shiragur,Amit Deshpande*

Main category: cs.LG

TL;DR: Auditing classifier fairness under partial feedback with a cost-aware data acquisition model; proposes near-optimal algorithms for black-box and exponential-family mixture settings, achieving substantial audit-cost reductions (~50%) on real datasets, and applicability to multiple fairness metrics.


<details>
  <summary>Details</summary>
Motivation: In practical deployments, true labels are available only for positively classified individuals (e.g., approved loan applicants), making audits expensive and biased. There is a need for cost-aware fairness auditing that minimizes data-collection costs while maintaining fairness guarantees.

Method: Introduce a realistic cost model for obtaining labels. Study two audit settings: (1) black-box model with no distributional assumptions; (2) mixture model where features and true labels follow exponential-family mixtures. Develop near-optimal auditing algorithms in the black-box setting under mild assumptions and show baselines can be suboptimal. In the mixture setting, design a novel algorithm leveraging truncated-sample learning and MAP oracles, extending results from spherical Gaussian mixtures to exponential-family mixtures. The methods apply to demographic parity, equal opportunity, and equalized odds.

Result: Theoretical results show near-optimal auditing performance in the black-box setting; the mixture-model algorithm achieves significantly lower audit costs than the black-box approach. Empirically, on Adult Income and Law School datasets, the proposed algorithms outperform natural baselines by around 50% in audit cost.

Conclusion: The work delivers cost-efficient fairness auditing under partial feedback, with practical relevance across common fairness metrics. It also contributes methodological advances by extending truncated-sample learning and MAP-based approaches to exponential-family mixtures, with potential broader impact on auditing in real-world systems.

Abstract: We study the problem of auditing the fairness of a given classifier under
partial feedback, where true labels are available only for positively
classified individuals, (e.g., loan repayment outcomes are observed only for
approved applicants). We introduce a novel cost model for acquiring additional
labeled data, designed to more accurately reflect real-world costs such as
credit assessment, loan processing, and potential defaults. Our goal is to find
optimal fairness audit algorithms that are more cost-effective than random
exploration and natural baselines.
  In our work, we consider two audit settings: a black-box model with no
assumptions on the data distribution, and a mixture model, where features and
true labels follow a mixture of exponential family distributions. In the
black-box setting, we propose a near-optimal auditing algorithm under mild
assumptions and show that a natural baseline can be strictly suboptimal. In the
mixture model setting, we design a novel algorithm that achieves significantly
lower audit cost than the black-box case. Our approach leverages prior work on
learning from truncated samples and maximum-a-posteriori oracles, and extends
known results on spherical Gaussian mixtures to handle exponential family
mixtures, which may be of independent interest. Moreover, our algorithms apply
to popular fairness metrics including demographic parity, equal opportunity,
and equalized odds. Empirically, we demonstrate strong performance of our
algorithms on real-world fair classification datasets like Adult Income and Law
School, consistently outperforming natural baselines by around 50% in terms of
audit cost.

</details>


### [294] [HydroFusion-LMF: Semi-Supervised Multi-Network Fusion with Large-Model Adaptation for Long-Term Daily Runoff Forecasting](https://arxiv.org/abs/2510.03744)
*Qianfei Fan,Jiayu Wei,Peijun Zhu,Wensheng Ye,Meie Fang*

Main category: cs.LG

TL;DR: HydroFusion-LMF is a semi-supervised, context-aware framework for decade-scale daily runoff forecasting in small watersheds, decomposing trend-seasonal-residual signals, routing residuals through a diverse expert pool, gating with hydrologic context, and using multi-task semi-supervised training to improve MSE/MAE and non-stationarity handling.


<details>
  <summary>Details</summary>
Motivation: Long-horizon runoff forecasting faces non-stationarity, multi-scale seasonality, regime shifts, and data sparsity; prior deep models target narrow facets and underutilize unlabeled periods.

Method: 1) learnable trend-seasonal-residual decomposition; 2) residuals processed by heterogeneous expert set: linear refinement, frequency kernel, patch Transformer, recurrent memory, dynamically normalized attention; 3) hydrologic context-aware gate combining day-of-year phase, antecedent precipitation, local variance, flood indicators, and basin attributes; 4) semi-supervised multi-task objective: composite MSE/MAE + extreme emphasis + NSE/KGE, masked reconstruction, multi-scale contrastive alignment, augmentation consistency, variance-filtered pseudo-labeling; optional adapter/LoRA for efficient integration with frozen time-series encoder.

Result: On ~10-year daily data, MSE 1.0128 and MAE 0.5818; outperforms strongest baseline DLinear by ~10% (MSE) and ~10% (MAE) and surpasses mean baseline by ~25% (MSE) and ~17% (MAE); simultaneous reductions in MSE and MAE observed.

Conclusion: The framework offers a interpretable, adaptable, label-efficient approach to hydrologic forecasting under non-stationarity, balancing explicit components with strong performance.

Abstract: Accurate decade-scale daily runoff forecasting in small watersheds is
difficult because signals blend drifting trends, multi-scale seasonal cycles,
regime shifts, and sparse extremes. Prior deep models (DLinear, TimesNet,
PatchTST, TiDE, Nonstationary Transformer, LSTNet, LSTM) usually target single
facets and under-utilize unlabeled spans, limiting regime adaptivity. We
propose HydroFusion-LMF, a unified framework that (i) performs a learnable
trend-seasonal-residual decomposition to reduce non-stationarity, (ii) routes
residuals through a compact heterogeneous expert set (linear refinement,
frequency kernel, patch Transformer, recurrent memory, dynamically normalized
attention), (iii) fuses expert outputs via a hydrologic context-aware gate
conditioned on day-of-year phase, antecedent precipitation, local variance,
flood indicators, and static basin attributes, and (iv) augments supervision
with a semi-supervised multi-task objective (composite MSE/MAE + extreme
emphasis + NSE/KGE, masked reconstruction, multi-scale contrastive alignment,
augmentation consistency, variance-filtered pseudo-labeling). Optional adapter
/ LoRA layers inject a frozen foundation time-series encoder efficiently. On a
~10-year daily dataset HydroFusion-LMF attains MSE 1.0128 / MAE 0.5818,
improving the strongest baseline (DLinear) by 10.2% / 10.3% and the mean
baseline by 24.6% / 17.1%. We observe simultaneous MSE and MAE reductions
relative to baselines. The framework balances interpretability (explicit
components, sparse gating) with performance, advancing label-efficient
hydrologic forecasting under non-stationarity.

</details>


### [295] [Neural Low-Discrepancy Sequences](https://arxiv.org/abs/2510.03745)
*Michael Etienne Van Huffel,Nathan Kirk,Makram Chahine,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: NeuroLDS introduces a neural framework to generate low-discrepancy sequences (LDS) with prefix-wise low-discrepancy, achieving superior performance over classical LDS constructions and applicable to numerical integration, robotics, and ML.


<details>
  <summary>Details</summary>
Motivation: There is a need for sequences with low discrepancy that hold for every prefix (LDS); existing MPMC methods generate point sets but cannot produce sequences, limiting applications. ML-based generation promises better discrepancy and broader applicability.

Method: A two-stage learning process: (1) supervised approximation of classical LDS constructions to initialize a neural network that maps indices to points, and (2) unsupervised fine-tuning to directly minimize prefix discrepancies across the sequence. The network is trained to generate full low-discrepancy sequences.

Result: NeuroLDS outperforms all previous LDS constructions by a significant margin in discrepancy measures and demonstrates effectiveness across diverse applications (numerical integration, robot motion planning, scientific ML). Code is publicly available.

Conclusion: NeuroLDS shows strong promise as a machine-learning–driven approach to generating LDS, potentially enabling broader adoption and new applications of low-discrepancy sequences; future work may extend scalability and applicability.

Abstract: Low-discrepancy points are designed to efficiently fill the space in a
uniform manner. This uniformity is highly advantageous in many problems in
science and engineering, including in numerical integration, computer vision,
machine perception, computer graphics, machine learning, and simulation.
Whereas most previous low-discrepancy constructions rely on abstract algebra
and number theory, Message-Passing Monte Carlo (MPMC) was recently introduced
to exploit machine learning methods for generating point sets with lower
discrepancy than previously possible. However, MPMC is limited to generating
point sets and cannot be extended to low-discrepancy sequences (LDS), i.e.,
sequences of points in which every prefix has low discrepancy, a property
essential for many applications. To address this limitation, we introduce
Neural Low-Discrepancy Sequences ($NeuroLDS$), the first machine learning-based
framework for generating LDS. Drawing inspiration from classical LDS, we train
a neural network to map indices to points such that the resulting sequences
exhibit minimal discrepancy across all prefixes. To this end, we deploy a
two-stage learning process: supervised approximation of classical constructions
followed by unsupervised fine-tuning to minimize prefix discrepancies. We
demonstrate that $NeuroLDS$ outperforms all previous LDS constructions by a
significant margin with respect to discrepancy measures. Moreover, we
demonstrate the effectiveness of $NeuroLDS$ across diverse applications,
including numerical integration, robot motion planning, and scientific machine
learning. These results highlight the promise and broad significance of Neural
Low-Discrepancy Sequences. Our code can be found at
https://github.com/camail-official/neuro-lds.

</details>


### [296] [EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models](https://arxiv.org/abs/2510.03760)
*Ping Guo,Chenyu Zhu,Siyuan Chen,Fei Liu,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: A new LLM-guided framework, EvoEngineer, formalizes CUDA kernel optimization as a code-optimization task and enables principled, correct-by-construction kernel improvements with a systematic framework and experimental validation on 91 CUDA kernels.


<details>
  <summary>Details</summary>
Motivation: Current CUDA kernel optimization is fragmented with isolated approaches and lacks rigorous problem formulations and correctness guarantees; general LLM code evolution cannot meet CUDA kernel correctness requirements.

Method: Formulate CUDA kernel optimization as a code-optimization problem with objective, constraints, and metrics; introduce EvoEngineer, a systematic LLM-based code evolution framework to design and adapt optimization strategies balancing performance and correctness; implement a kernel optimization system based on this framework; evaluate on 91 real-world CUDA kernels.

Result: EvoEngineer achieves a principled balance between performance and correctness, with the highest averaged median speedup of 2.72x over baseline kernels and a code validity rate of 69.8%; maximum speedup of 36.75x on PyTorch kernels; accelerates 28 of 50 operations (56.0%) by more than 2x.

Conclusion: A formalized optimization objective plus a systematic LLM-based framework enables effective, correct, and high-performance CUDA kernel optimization, outperforming existing methods on both speedups and code validity.

Abstract: CUDA kernel optimization has become a critical bottleneck for AI performance,
as deep learning training and inference efficiency directly depends on highly
optimized GPU kernels.
  Despite the promise of Large Language Models (LLMs) for automating kernel
optimization, this field suffers from a fragmented ecosystem of isolated and
incomparable approaches with unclear problem formulations.
  Furthermore, general-purpose LLM code evolution methods cannot meet strict
correctness requirements of CUDA kernel optimization.
  We address these fundamental challenges by first formalizing CUDA kernel
optimization as a code optimization task with a clear objective, constraints,
and evaluation metrics.
  We then establish the first systematic LLM-based code evolution framework,
EvoEngineer, that provides guidance for designing and adapting optimization
strategies to achieve a balance between performance and correctness.
  Finally, we implement a kernel optimization system based on this framework
and conduct extensive experiments on 91 real-world CUDA kernels.
  Our results demonstrate that EvoEngineer achieves a principled balance
between performance and correctness, with the highest averaged median speedup
of \textbf{2.72}$\times$ over baseline CUDA kernels and a code validity rate of
\textbf{69.8}\%, outperforming existing methods on both dimensions.
  Our method achieves a maximum speedup of \textbf{36.75}$\times$ among all
operations over PyTorch kernels and delivers the highest speedup on \textbf{28}
(\textbf{56.0\%}) of 50 operations that achieve over \textbf{2$\times$}
acceleration.

</details>


### [297] [Merge and Guide: Unifying Model Merging and Guided Decoding for Controllable Multi-Objective Generation](https://arxiv.org/abs/2510.03782)
*Guofu Xie,Chen Zhang,Xiao Zhang,Yunsheng Shi,Ting Yao,Jun Xu*

Main category: cs.LG

TL;DR: MAGE: a two-stage merging-guided decoding framework for controllable multi-objective generation that merges multiple backbones and value models to enable efficient, flexible control with Pareto-optimal performance.


<details>
  <summary>Details</summary>
Motivation: Current controllable generation approaches trade-off between suboptimal parameter-level merging and costly decoding-based guidance; need scalable, effective multi-objective control at test time.

Method: Stage 1 dynamically merge backbone models to form a robust base model. Stage 2 merge explicit and implicit value models into a unified guidance proxy to steer decoding. Also analyze Linear Mode Connectivity and relation between merging and ensembling.

Result: Empirical results show improved controllability, Pareto-optimal performance, and adaptability, outperforming existing methods.

Conclusion: MAGE offers efficient, robust controllable generation for diverse user needs by combining model merging with guided decoding, mitigating space overhead and capacity dependence.

Abstract: Adapting to diverse user needs at test time is a key challenge in
controllable multi-objective generation. Existing methods are insufficient:
merging-based approaches provide indirect, suboptimal control at the parameter
level, often disregarding the impacts of multiple objectives. While
decoding-based guidance is more direct, it typically requires aggregating
logits from multiple expert models, incurring significant space overhead and
relying heavily on individual model capacity. To address these issues, we
introduce Merge-And-GuidE (MAGE), a two-stage framework that leverages model
merging for guided decoding. We first identify a critical compatibility problem
between the guidance and base models. In Stage 1, MAGE resolves this by
dynamically constructing a more robust base model, merging a series of backbone
models that account for multiple objectives. In Stage 2, we merge explicit and
implicit value models into a unified guidance proxy, which then steers the
decoding of the base model from Stage 1. Our analysis empirically validates
Linear Mode Connectivity (LMC) in value models, explores the relationship
between model merging and prediction ensembling, and demonstrates the enhanced
controllability afforded by our approach. Extensive experiments show that our
method outperforms existing approaches, achieving superior controllability,
Pareto-optimal performance, and enhanced adaptability.

</details>


### [298] [Allocation of Parameters in Transformers](https://arxiv.org/abs/2510.03784)
*Ruoxi Yu,Haotian Jiang,Jingpu Cheng,Penghao Yu,Qianxiao Li,Zhong Li*

Main category: cs.LG

TL;DR: The work characterizes how to distribute Transformer parameters (heads and head dimensions) across layers under a fixed budget, revealing early layers' role, a softmax saturation phenomenon that constrains gains from widening heads, and principled layer-wise allocation strategies for efficiency.


<details>
  <summary>Details</summary>
Motivation: To explain and improve Transformer efficiency by understanding parameter allocation across layers and the trade-off between heads and dimensionality, especially as sequence length grows.

Method: Theoretical analysis from an approximation perspective of information extraction in early layers; mathematical characterization of the head-head dimension trade-off under a fixed parameter budget; proof of softmax saturation with increasing head dimension; empirical validation on tasks (long sequences).

Result: Derived trade-offs between head count and head dimension, demonstrated softmax saturation reducing marginal gains from larger head sizes on long sequences, and proposed allocation strategies that allow later layers to operate with fewer parameters while maintaining performance.

Conclusion: Offers a theoretically grounded framework and practical guidelines for distributing attention heads and dimensions across Transformer layers to achieve efficient, expressive models.

Abstract: Transformers have achieved remarkable successes across a wide range of
applications, yet the theoretical foundation of their model efficiency remains
underexplored. In this work, we investigate how the model parameters -- mainly
attention heads and head dimensions -- should be allocated across layers to
balance expressivity and efficiency. We first provide mathematical analysis on
the role of early layers in information extraction from an approximation
perspective, with a theoretical characterization on the trade-off between the
number of heads and head dimension under a fixed parameter budget. In addition,
we uncover and prove the \emph{saturation} behavior of softmax activations:
Continuously increasing head dimensions can lead to diminishing returns in
learning errors, particularly for long sequences. Supported by both theory and
experiments, this saturation pattern suggests that later layers can operate
more efficiently with reduced parameters. Combining these insights, we propose
principled strategies for allocating attention heads and dimensions across
Transformers' layers, shedding light on theoretically-grounded model efficiency
of Transformer-based architectures.

</details>


### [299] [Robust Batched Bandits](https://arxiv.org/abs/2510.03798)
*Yunwen Guo,Yunlun Shu,Gongyi Zhuo,Tianyu Wang*

Main category: cs.LG

TL;DR: Robust batched MAB algorithms are developed for heavy-tailed rewards in finite-arm and Lipschitz settings. Heavier tails reduce the required number of batches for near-optimal regret in the instance-independent and Lipschitz regimes, while in the instance-dependent regime the necessary batch count remains unchanged by tail heaviness.


<details>
  <summary>Details</summary>
Motivation: Real-world rewards (e.g., clinical outcomes) are often heavy-tailed, contrasting with the common light-tailed assumptions in batched MAB theory. Filling this gap by designing robust algorithms for heavy-tailed rewards across finite-arm and Lipschitz settings clarifies how tail behavior interacts with batching and regret.

Method: Propose robust batched bandit algorithms tailored for heavy-tailed rewards, applicable to both finite-arm and Lipschitz-continuous settings. The approach likely employs robust mean estimators (e.g., truncated, Catoni, or median-of-means) within batched exploration schedules. Theoretical analysis yields regret bounds and characterizes batch complexity under instance-independent and instance-dependent regimes, revealing tail-dependent effects.

Result: The analysis uncovers a surprising phenomenon: in the instance-independent regime and in the Lipschitz setting, heavier-tailed rewards require fewer batches to reach near-optimal regret. Conversely, in the instance-dependent setting, the required number of batches to attain near-optimal regret is invariant to the tail heaviness.

Conclusion: Tail heaviness interacts with batching in a regime-specific way. Heavy tails can reduce batch demands in worst-case (instance-independent) and Lipschitz scenarios, while not affecting (or not offering improvement for) batch requirements in the instance-dependent regime. This informs practical design of batched experiments and suggests directions for refining robust batching techniques for heavy-tailed data.

Abstract: The batched multi-armed bandit (MAB) problem, in which rewards are collected
in batches, is crucial for applications such as clinical trials. Existing
research predominantly assumes light-tailed reward distributions, yet many
real-world scenarios, including clinical outcomes, exhibit heavy-tailed
characteristics. This paper bridges this gap by proposing robust batched bandit
algorithms designed for heavy-tailed rewards, within both finite-arm and
Lipschitz-continuous settings. We reveal a surprising phenomenon: in the
instance-independent regime, as well as in the Lipschitz setting,
heavier-tailed rewards necessitate a smaller number of batches to achieve
near-optimal regret. In stark contrast, for the instance-dependent setting, the
required number of batches to attain near-optimal regret remains invariant with
respect to tail heaviness.

</details>


### [300] [Curriculum-Augmented GFlowNets For mRNA Sequence Generation](https://arxiv.org/abs/2510.03811)
*Aya Laajil,Abduragim Shtanchaev,Sajan Muhammad,Eric Moulines,Salem Lahlou*

Main category: cs.LG

TL;DR: Curriculum-Augmented GFlowNets (CAGFN) improves multi-objective mRNA design by integrating a length-based curriculum with GFlowNets, yielding faster convergence, better Pareto fronts, diversity, and generalization.


<details>
  <summary>Details</summary>
Motivation: Designing mRNA sequences is a combinatorial, long-horizon optimization problem with conflicting objectives; sparse rewards and multi-objective trade-offs hinder standard GFlowNets.

Method: Introduce CAGFN that adds a curriculum (progressively longer sequences) to guide exploration; create an mRNA design environment for GFlowNets with target protein, biological objectives; train GFlowNets to generate plausible candidates.

Result: Improved Pareto performance and biological plausibility; maintained diversity; faster attainment of high-quality solutions than a GFlowNet trained with random sequence sampling; generalizes to out-of-distribution sequences.

Conclusion: CAGFN provides a biologically grounded framework for therapeutic sequence design with GFlowNets, improving training efficiency and solution quality while enabling generalization and a platform for further advancement.

Abstract: Designing mRNA sequences is a major challenge in developing next-generation
therapeutics, since it involves exploring a vast space of possible nucleotide
combinations while optimizing sequence properties like stability, translation
efficiency, and protein expression. While Generative Flow Networks are
promising for this task, their training is hindered by sparse, long-horizon
rewards and multi-objective trade-offs. We propose Curriculum-Augmented
GFlowNets (CAGFN), which integrate curriculum learning with multi-objective
GFlowNets to generate de novo mRNA sequences. CAGFN integrates a length-based
curriculum that progressively adapts the maximum sequence length guiding
exploration from easier to harder subproblems. We also provide a new mRNA
design environment for GFlowNets which, given a target protein sequence and a
combination of biological objectives, allows for the training of models that
generate plausible mRNA candidates. This provides a biologically motivated
setting for applying and advancing GFlowNets in therapeutic sequence design. On
different mRNA design tasks, CAGFN improves Pareto performance and biological
plausibility, while maintaining diversity. Moreover, CAGFN reaches
higher-quality solutions faster than a GFlowNet trained with random sequence
sampling (no curriculum), and enables generalization to out-of-distribution
sequences.

</details>


### [301] [Detecting Invariant Manifolds in ReLU-Based RNNs](https://arxiv.org/abs/2510.03814)
*Lukas Eisenmann,Alena Brändle,Zahra Monfared,Daniel Durstewitz*

Main category: cs.LG

TL;DR: A novel algorithm detects stable/unstable manifolds in piecewise-linear RNNs (ReLU-based), enabling tracing basin boundaries, identifying homoclinic points, and demonstrating chaos; applied to cortical neuron data.


<details>
  <summary>Details</summary>
Motivation: Explainable AI and neuroscience demand understanding the dynamical structure of RNNs. The topology of state space, especially stable/unstable manifolds and their intersections, governs basins of attraction, multistability, and chaotic behavior. A method to reveal these in PLRNNs aids interpretation of trained networks and dynamical systems.

Method: Develop a new algorithm to detect invariant manifolds in PLRNNs with ReLU activations. The method targets stable and unstable manifolds of periodic points, traces the boundaries between basins of attraction, identifies homoclinic intersections, and thereby reveals potential chaotic dynamics. Demonstrated on PLRNNs and illustrated with an electrophysiological cortical neuron dataset.

Result: The algorithm enables tracing of basin boundaries and characterization of multistability. It can locate homoclinic points, providing evidence of chaos in PLRNNs. An empirical example with cortical neuron recordings shows the approach yields interpretable insights into neural dynamics.

Conclusion: The work provides a practical, geometry-based tool for analyzing the dynamical repertoire of PLRNNs, with implications for understanding and interpreting neural computation in both machine learning and neuroscience. It enhances the capacity for explainable analysis of RNN dynamics and may inform neurophysiological data interpretation.

Abstract: Recurrent Neural Networks (RNNs) have found widespread applications in
machine learning for time series prediction and dynamical systems
reconstruction, and experienced a recent renaissance with improved training
algorithms and architectural designs. Understanding why and how trained RNNs
produce their behavior is important for scientific and medical applications,
and explainable AI more generally. An RNN's dynamical repertoire depends on the
topological and geometrical properties of its state space. Stable and unstable
manifolds of periodic points play a particularly important role: They dissect a
dynamical system's state space into different basins of attraction, and their
intersections lead to chaotic dynamics with fractal geometry. Here we introduce
a novel algorithm for detecting these manifolds, with a focus on
piecewise-linear RNNs (PLRNNs) employing rectified linear units (ReLUs) as
their activation function. We demonstrate how the algorithm can be used to
trace the boundaries between different basins of attraction, and hence to
characterize multistability, a computationally important property. We further
show its utility in finding so-called homoclinic points, the intersections
between stable and unstable manifolds, and thus establish the existence of
chaos in PLRNNs. Finally we show for an empirical example, electrophysiological
recordings from a cortical neuron, how insights into the underlying dynamics
could be gained through our method.

</details>


### [302] [TROLL: Trust Regions improve Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2510.03817)
*Philipp Becker,Niklas Freymuth,Serge Thilges,Fabian Otto,Gerhard Neumann*

Main category: cs.LG

TL;DR: Replace PPO-style clipping with a discrete differentiable trust region projection (TROLL) that enforces token-level KL constraints on important logits, improving training speed, stability, and final performance in LLM RLHF fine-tuning.


<details>
  <summary>Details</summary>
Motivation: PPO clipping is a crude proxy for KL-based trust regions and often causes unstable updates and suboptimal outcomes in LLM RLHF. A principled, token-level constraint could improve stability and performance without changing inference.

Method: Introduce a discrete differentiable trust region projection that operates on a sparse subset of the model’s most important token logits to enforce token-level KL constraints. Use TROLL as a drop-in replacement for PPO-like clipping during training, preserving inference behavior.

Result: Across datasets, model families, and advantage-estimation methods, TROLL outperforms PPO-like clipping in training speed, stability, and final success rates.

Conclusion: TROLL provides a principled, efficient, and general replacement for clipping in PPO-like RLHF for LLMs, improving training dynamics without altering inference.

Abstract: On-policy Reinforcement Learning (RL) with PPO-like clip objectives has
become the standard choice for reward-based fine-tuning of large language
models (LLMs). Although recent work has explored improved estimators of
advantages and normalization, the clipping mechanism itself has remained
untouched. Originally introduced as a proxy for principled KL-based trust
regions, clipping is a crude approximation that often causes unstable updates
and suboptimal performance. We replace the clip objective with a novel discrete
differentiable trust region projection, which provides principled token-level
KL constraints. The projection operates on a sparse subset of the model's most
important token logits to balance computational cost and projection
effectiveness. Our approach, Trust Region Optimization for Large Language
Models (TROLL), serves as a direct replacement for PPO-like clipping during
training and does not alter the model's inference behavior. Across datasets,
model families, and advantage-estimation methods, TROLL consistently
outperforms PPO-like clipping in terms of training speed, stability, and final
success rates.

</details>


### [303] [Proximal Diffusion Neural Sampler](https://arxiv.org/abs/2510.03824)
*Wei Guo,Jaemoo Choi,Yuchen Zhu,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: PDNS uses proximal point iterations on path space to gradually shape a diffusion-based sampler toward the target distribution, addressing mode collapse in multimodal targets via a WDCE objective; validated on molecular dynamics and statistical physics tasks.


<details>
  <summary>Details</summary>
Motivation: Training diffusion-based samplers for multimodal distributions is challenging due to barriers that promote mode collapse; a principled method on the space of path measures can promote exploration and gradual convergence.

Method: Apply proximal point method on the space of path measures to decompose learning into simpler subproblems; build the sampling path step-by-step toward the target; each proximal step is realized with a proximal weighted denoising cross-entropy (WDCE) objective.

Result: Extensive experiments show effectiveness and robustness for continuous and discrete sampling tasks, including challenging molecular dynamics and statistical-physics scenarios.

Conclusion: PDNS provides a robust, scalable framework for diffusion-based sampling in multimodal landscapes by gradually constructing the target path and encouraging thorough cross-modal exploration.

Abstract: The task of learning a diffusion-based neural sampler for drawing samples
from an unnormalized target distribution can be viewed as a stochastic optimal
control problem on path measures. However, the training of neural samplers can
be challenging when the target distribution is multimodal with significant
barriers separating the modes, potentially leading to mode collapse. We propose
a framework named \textbf{Proximal Diffusion Neural Sampler (PDNS)} that
addresses these challenges by tackling the stochastic optimal control problem
via proximal point method on the space of path measures. PDNS decomposes the
learning process into a series of simpler subproblems that create a path
gradually approaching the desired distribution. This staged procedure traces a
progressively refined path to the desired distribution and promotes thorough
exploration across modes. For a practical and efficient realization, we
instantiate each proximal step with a proximal weighted denoising cross-entropy
(WDCE) objective. We demonstrate the effectiveness and robustness of PDNS
through extensive experiments on both continuous and discrete sampling tasks,
including challenging scenarios in molecular dynamics and statistical physics.

</details>


### [304] [HOFLON: Hybrid Offline Learning and Online Optimization for Process Start-Up and Grade-Transition Control](https://arxiv.org/abs/2510.03830)
*Alex Durkin,Jasper Stolte,Mehmet Mercangöz*

Main category: cs.LG

TL;DR: Hybrid Offline Learning + Online Optimization (HOFLON) improves start-up and grade-change transitions in continuous-process plants by learning a latent data manifold and a long-horizon Q-critic offline, then performing online one-step optimization with manifold and rate-of-change penalties, beating IQL and historical bests.


<details>
  <summary>Details</summary>
Motivation: To overcome distribution shift and value-overestimation in offline reinforcement learning for critical process transitions in the absence of a process model, especially amid aging expert workforce.

Method: Offline phase learns a latent manifold representing feasible past transitions and a long-horizon Q-critic predicting cumulative reward. Online phase solves a one-step optimization to maximize the Q-critic while penalizing deviations from the learned manifold and excessive rates of change in manipulated variables.

Result: HOFLON outperforms Implicit Q-Learning on two industrial case studies (polymerization reactor start-up and paper-machine grade-change) and achieves average cumulative rewards higher than the best historical transitions, indicating potential to automate transitions beyond current expert capability.

Conclusion: HOFLON effectively mitigates distribution shift and value-overestimation in offline RL for process startups and grade changes, enabling automated optimization that can match or exceed expert performance and offering a path toward broader industrial deployment.

Abstract: Start-ups and product grade-changes are critical steps in continuous-process
plant operation, because any misstep immediately affects product quality and
drives operational losses. These transitions have long relied on manual
operation by a handful of expert operators, but the progressive retirement of
that workforce is leaving plant owners without the tacit know-how needed to
execute them consistently. In the absence of a process model, offline
reinforcement learning (RL) promises to capture and even surpass human
expertise by mining historical start-up and grade-change logs, yet standard
offline RL struggles with distribution shift and value-overestimation whenever
a learned policy ventures outside the data envelope. We introduce HOFLON
(Hybrid Offline Learning + Online Optimization) to overcome those limitations.
Offline, HOFLON learns (i) a latent data manifold that represents the feasible
region spanned by past transitions and (ii) a long-horizon Q-critic that
predicts the cumulative reward from state-action pairs. Online, it solves a
one-step optimization problem that maximizes the Q-critic while penalizing
deviations from the learned manifold and excessive rates of change in the
manipulated variables. We test HOFLON on two industrial case studies: a
polymerization reactor start-up and a paper-machine grade-change problem, and
benchmark it against Implicit Q-Learning (IQL), a leading offline-RL algorithm.
In both plants HOFLON not only surpasses IQL but also delivers, on average,
better cumulative rewards than the best start-up or grade-change observed in
the historical data, demonstrating its potential to automate transition
operations beyond current expert capability.

</details>


### [305] [Technical note on Fisher Information for Robust Federated Cross-Validation](https://arxiv.org/abs/2510.03838)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: FIRE is a scalable method that uses an approximate Fisher Information term to quantify and penalize fragmentation-induced covariate shift across distributed training fragments, improving validation performance on shifted data.


<details>
  <summary>Details</summary>
Motivation: When data are fragmented across batches or federated cohorts, covariate shift arises between fragment distributions and the global training/validation distributions, causing performance degradation. There is a need for a scalable, distribution-alignment approach during training.

Method: Compute an approximate Fisher information-based divergence that accumulates fragmentation-induced covariate shift from the global training distribution. Use this divergence as a per-fragment loss penalty to encourage distribution alignment across fragments, enabling scalable robust validation in federated/distributed settings.

Result: FIRE outperforms importance-weighting baselines by up to 5.1% on maximum gains and improves federated-learning benchmarks by up to 5.3% on shifted validation sets.

Conclusion: FIRE provides a computationally tractable, per-fragment regularizer that aligns fragmented distributions with the global training distribution, yielding robust performance improvements under covariate shift in federated and fragmented data scenarios.

Abstract: When training data are fragmented across batches or federated-learned across
different geographic locations, trained models manifest performance
degradation. That degradation partly owes to covariate shift induced by data
having been fragmented across time and space and producing dissimilar empirical
training distributions. Each fragment's distribution is slightly different to a
hypothetical unfragmented training distribution of covariates, and to the
single validation distribution. To address this problem, we propose Fisher
Information for Robust fEderated validation (\textbf{FIRE}). This method
accumulates fragmentation-induced covariate shift divergences from the global
training distribution via an approximate Fisher information. That term, which
we prove to be a more computationally-tractable estimate, is then used as a
per-fragment loss penalty, enabling scalable distribution alignment. FIRE
outperforms importance weighting benchmarks by $5.1\%$ at maximum and federated
learning (FL) benchmarks by up to $5.3\%$ on shifted validation sets.

</details>


### [306] [Technical note on Sequential Test-Time Adaptation via Martingale-Driven Fisher Prompting](https://arxiv.org/abs/2510.03839)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: M-FISHER combines sequential shift detection with stable, geometry-aware adaptation. It yields time-uniform, anytime-valid false alarm control via exponential martingales and Ville's inequality, and natural-gradient-like, Fisher-preconditioned updates that minimize KL divergence while preserving stability and invariance.


<details>
  <summary>Details</summary>
Motivation: To enable reliable decision-making in streaming settings under covariate shift by (i) achieving anytime-valid detection with guarantees at any stopping time and (ii) enabling stable, information-theoretically grounded adaptation that respects the geometry of the distributional manifold.

Method: Detection uses an exponential martingale built from non-conformity scores; Ville's inequality provides time-uniform false alarm guarantees. Under shifts, the method derives an O(log(1/δ)/Γ) bound on detection delay, with Γ capturing post-shift information gain (distributional divergence). Adaptation uses Fisher-preconditioned prompt updates that realize natural gradient descent on the distributional manifold, minimizing KL divergence while maintaining stability and parameterization invariance.

Result: Provides principled, anytime-valid detection with delay guarantees and a Fisher-preconditioned adaptation scheme that performs natural-gradient-like updates on the distributional manifold, yielding locally optimal, stable updates under covariate shift.

Conclusion: M-FISHER offers a rigorous framework for robust, anytime-valid detection and geometrically stable adaptation in sequential decision-making under covariate shift.

Abstract: We present a theoretical framework for M-FISHER, a method for sequential
distribution shift detection and stable adaptation in streaming data. For
detection, we construct an exponential martingale from non-conformity scores
and apply Ville's inequality to obtain time-uniform guarantees on false alarm
control, ensuring statistical validity at any stopping time. Under sustained
shifts, we further bound the expected detection delay as
$\mathcal{O}(\log(1/\delta)/\Gamma)$, where $\Gamma$ reflects the post-shift
information gain, thereby linking detection efficiency to distributional
divergence. For adaptation, we show that Fisher-preconditioned updates of
prompt parameters implement natural gradient descent on the distributional
manifold, yielding locally optimal updates that minimize KL divergence while
preserving stability and parameterization invariance. Together, these results
establish M-FISHER as a principled approach for robust, anytime-valid detection
and geometrically stable adaptation in sequential decision-making under
covariate shift.

</details>


### [307] [On Using Large Language Models to Enhance Clinically-Driven Missing Data Recovery Algorithms in Electronic Health Records](https://arxiv.org/abs/2510.03844)
*Sarah C. Lotspeich,Abbey Collins,Brian J. Wells,Ashish K. Khanna,Joseph Rigdon,Lucy D'Agostino McGowan*

Main category: cs.LG

TL;DR: A roadmap-based, ICD-10/LLM-enhanced algorithm can recover missing EHR data as effectively as expert chart reviews and scales to large samples.


<details>
  <summary>Details</summary>
Motivation: EHR data often contain missingness and errors; chart reviews are costly and slow. A scalable, automated method using ICD-10 anchors, refined by large language models (LLMs) and clinician input, could recover missing values and improve data quality.

Method: Compare multiple roadmaps (the original clinician roadmap and LLM-enhanced variants) for recovering missing values in EHR data. Validate against chart reviews on 100 patients, then apply the final algorithm—incorporating clinician-approved LLM additions—to a larger set of 1000 patients.

Result: The algorithm recovered as much or more missing data than expert chart reviewers, depending on the roadmap. The final roadmap included clinician-approved additions from the LLM and performed effectively on the 1000-patient set.

Conclusion: Clinically-driven algorithms, augmented by LLMs, can match chart-review accuracy in recovering missing EHR data and are scalable to large samples. Future work could expand to monitor other data-quality dimensions (e.g., plausibility).

Abstract: Objective: Electronic health records (EHR) data are prone to missingness and
errors. Previously, we devised an "enriched" chart review protocol where a
"roadmap" of auxiliary diagnoses (anchors) was used to recover missing values
in EHR data (e.g., a diagnosis of impaired glycemic control might imply that a
missing hemoglobin A1c value would be considered unhealthy). Still, chart
reviews are expensive and time-intensive, which limits the number of patients
whose data can be reviewed. Now, we investigate the accuracy and scalability of
a roadmap-driven algorithm, based on ICD-10 codes (International Classification
of Diseases, 10th revision), to mimic expert chart reviews and recover missing
values. Materials and Methods: In addition to the clinicians' original roadmap
from our previous work, we consider new versions that were iteratively refined
using large language models (LLM) in conjunction with clinical expertise to
expand the list of auxiliary diagnoses. Using chart reviews for 100 patients
from the EHR at an extensive learning health system, we examine algorithm
performance with different roadmaps. Using the larger study of $1000$ patients,
we applied the final algorithm, which used a roadmap with clinician-approved
additions from the LLM. Results: The algorithm recovered as much, if not more,
missing data as the expert chart reviewers, depending on the roadmap.
Discussion: Clinically-driven algorithms (enhanced by LLM) can recover missing
EHR data with similar accuracy to chart reviews and can feasibly be applied to
large samples. Extending them to monitor other dimensions of data quality
(e.g., plausability) is a promising future direction.

</details>


### [308] [Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration](https://arxiv.org/abs/2510.03865)
*Wenhao Deng,Long Wei,Chenglei Yu,Tailin Wu*

Main category: cs.LG

TL;DR: RAPO uses forward KL regularization and reference-policy reweighting to promote broader, targeted exploration in RLVR, yielding consistent math problem-solving gains and surpassing the base model's performance ceiling.


<details>
  <summary>Details</summary>
Motivation: RLVR gains shrink as sampling budgets grow due to reverse KL's mode-seeking bias, which confines the policy to the base model's support and hampers exploration.

Method: Introduce RAPO: replace reverse KL with forward KL for out-of-distribution exploration and reweight the reference policy to enable adaptive in-distribution exploration; train Qwen2.5-3B and 7B on 8K SimpleRL-Zero without supervised fine-tuning; evaluate on AIME2024 and AIME2025.

Result: RAPO consistently improves problem-solving performance, enabling models to exceed the base model's ceiling and solve previously intractable problems on AIME-style tasks.

Conclusion: RAPO advances RLVR for challenging reasoning tasks by enabling broader and adaptive exploration, addressing the key limitation of reverse KL regularization.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently enhanced
the reasoning capabilities of large language models (LLMs), particularly for
mathematical problem solving. However, a fundamental limitation remains: as the
sampling budget increases, the advantage of RLVR-trained models over their
pretrained bases often diminishes or even vanishes, revealing a strong
dependence on the base model's restricted search space. We attribute this
phenomenon to the widespread use of the reverse Kullback-Leibler (KL)
divergence regularizer, whose mode-seeking behavior keeps the policy trapped
inside the base model's support region and hampers wider exploration. To
address this issue, we propose RAPO (Rewards-Aware Policy Optimization), an
algorithm to promote broader yet focused exploration. Our method (i) utilizes
the forward KL penalty to replace the reverse KL penalty for
out-of-distribution exploration, and (ii) reweights the reference policy to
facilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B
models with RAPO on the 8K SimpleRL-Zero dataset, without supervised
fine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO
consistently improves problem-solving performance. Notably, RAPO enables models
to surpass the base model's performance ceiling and solves previously
intractable problems, advancing the frontier of RLVR for challenging reasoning
tasks.

</details>


### [309] [On Provable Benefits of Muon in Federated Learning](https://arxiv.org/abs/2510.03866)
*Xinwen Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: FedMuon extends Muon to federated learning, establishing nonconvex convergence with a problem-parameter-free learning rate and robustness to heavy-tailed noise; empirically effective across neural architectures.


<details>
  <summary>Details</summary>
Motivation: Investigate Muon's effectiveness in federated learning and address FL-specific challenges such as nonconvex optimization, data heterogeneity, and noisy updates.

Method: Introduce FedMuon; derive convergence rate for nonconvex problems; show orthonormalized update direction yields a learning rate independent of problem parameters and resilience to heavy-tailed noise; validate with extensive experiments on various neural network architectures.

Result: Theoretical convergence guarantees; learning rate independence from problem parameters; natural accommodation of heavy-tailed noise; empirical performance gains across architectures.

Conclusion: FedMuon is a promising FL optimizer with strong theory and empirical support, suggesting broader applicability and future work on robustness and efficiency in federated settings.

Abstract: The recently introduced optimizer, Muon, has gained increasing attention due
to its superior performance across a wide range of applications. However, its
effectiveness in federated learning remains unexplored. To address this gap,
this paper investigates the performance of Muon in the federated learning
setting. Specifically, we propose a new algorithm, FedMuon, and establish its
convergence rate for nonconvex problems. Our theoretical analysis reveals
multiple favorable properties of FedMuon. In particular, due to its
orthonormalized update direction, the learning rate of FedMuon is independent
of problem-specific parameters, and, importantly, it can naturally accommodate
heavy-tailed noise. The extensive experiments on a variety of neural network
architectures validate the effectiveness of the proposed algorithm.

</details>


### [310] [Optimal Scaling Needs Optimal Norm](https://arxiv.org/abs/2510.03871)
*Oleg Filatov,Jiangtao Wang,Jan Ebert,Stefan Kesselheim*

Main category: cs.LG

TL;DR: Norm transfer: the operator norm of the output layer remains constant across joint scaling of model and data, guiding optimal learning rate and batch size; per-layer LR tuning helps; Disco released with extensive logs.


<details>
  <summary>Details</summary>
Motivation: There is no single explanatory principle for hyperparameter transfer across model/dataset scaling. A unified, quantitative invariant could enable principled scaling rules and explain why training dynamics behave similarly across scales.

Method: Use the Scion optimizer to explore joint scaling across models up to 1.3B parameters and datasets up to 138B tokens. Monitor the operator norm of the output layer to identify a constant value associated with optimal (eta*, B*). Examine sufficiency by comparing loss across (eta, B) achieving the invariant. Measure how (eta*, B*) scale with dataset size, and compare these scaling laws to Adam. Additionally, perform per-layer-group learning-rate tuning and release Disco with logs from over two thousand runs.

Result: Found a single invariant: the operator norm of the output layer remains constant for optimal scaling across the studied range (norm transfer). This norm condition is necessary but not sufficient: multiple (eta, B) pairs can yield the optimal norm for a given dataset size, yet only one combination minimizes loss. The derived scaling of (eta*, B*) with dataset size aligns with Adam. Per-layer-group learning-rate tuning yields improvements, with the output layer being most sensitive and hidden layers benefiting from lower rates. A practical, norm-guided scaling procedure is proposed, and Disco is released with extensive training logs.

Conclusion: Norm-guided scaling offers a practical principle for scaling LLMs. The norm transfer invariant explains joint model/dataset scaling and provides measurable targets for hyperparameter tuning. The work delivers first measurements of (eta*, B*) scaling with dataset size and provides actionable guidance, alongside a large public resource (Disco) to support further research on training dynamics at scale.

Abstract: Despite recent progress in optimal hyperparameter transfer under model and
dataset scaling, no unifying explanatory principle has been established. Using
the Scion optimizer, we discover that joint optimal scaling across model and
dataset sizes is governed by a single invariant: the operator norm of the
output layer. Across models with up to 1.3B parameters trained on up to 138B
tokens, the optimal learning rate/batch size pair $(\eta^{\ast}, B^{\ast})$
consistently has the same operator norm value - a phenomenon we term norm
transfer. This constant norm condition is necessary but not sufficient: while
for each dataset size, multiple $(\eta, B)$ reach the optimal norm, only a
unique $(\eta^{\ast}, B^{\ast})$ achieves the best loss. As a sufficient
condition, we provide the first measurement of $(\eta^{\ast}, B^{\ast})$
scaling with dataset size for Scion, and find that the scaling rules are
consistent with those of the Adam optimizer. Tuning per-layer-group learning
rates also improves model performance, with the output layer being the most
sensitive and hidden layers benefiting from lower learning rates. We provide
practical insights on norm-guided optimal scaling and release our Distributed
Scion (Disco) implementation with logs from over two thousand runs to support
research on LLM training dynamics at scale.

</details>


### [311] [BONSAI: Structure-exploiting robust Bayesian optimization for networked black-box systems under uncertainty](https://arxiv.org/abs/2510.03893)
*Akshay Kudva,Joel A. Paulson*

Main category: cs.LG

TL;DR: BONSAI enhances robust Bayesian optimization by exploiting partial structure in simulation-based models via a graph-based decomposition and a gradient-friendly Thompson sampling acquisition, improving sample efficiency and robustness for high-dimensional, black-box objectives.


<details>
  <summary>Details</summary>
Motivation: Robust optimization under uncertainty is essential for reliable process systems; traditional RO demands known problem structure and struggles with expensive black-box objectives; existing RBO often ignores structure and scales poorly; there is a need to leverage available simulation structure to improve data efficiency and scalability.

Method: Represent the objective as a directed graph consisting of white-box (interpretable) and black-box components; integrate this structure within a Bayesian optimization framework; develop a scalable Thompson sampling–based acquisition function tailored to structured RO; enable gradient-based optimization of the acquisition; evaluate on synthetic and real-world process-system cases, showing improved efficiency and solution quality.

Result: Across synthetic and real-world case studies, BONSAI consistently achieves more sample-efficient robust solutions and higher objective values under uncertainty compared to existing simulation-based RO algorithms.

Conclusion: Exploiting partial structural information enables more practical and effective robust optimization in complex engineering systems; BONSAI provides a scalable, structure-aware approach for uncertainty-aware design.

Abstract: Optimal design under uncertainty remains a fundamental challenge in advancing
reliable, next-generation process systems. Robust optimization (RO) offers a
principled approach by safeguarding against worst-case scenarios across a range
of uncertain parameters. However, traditional RO methods typically require
known problem structure, which limits their applicability to high-fidelity
simulation environments. To overcome these limitations, recent work has
explored robust Bayesian optimization (RBO) as a flexible alternative that can
accommodate expensive, black-box objectives. Existing RBO methods, however,
generally ignore available structural information and struggle to scale to
high-dimensional settings. In this work, we introduce BONSAI (Bayesian
Optimization of Network Systems under uncertAInty), a new RBO framework that
leverages partial structural knowledge commonly available in simulation-based
models. Instead of treating the objective as a monolithic black box, BONSAI
represents it as a directed graph of interconnected white- and black-box
components, allowing the algorithm to utilize intermediate information within
the optimization process. We further propose a scalable Thompson sampling-based
acquisition function tailored to the structured RO setting, which can be
efficiently optimized using gradient-based methods. We evaluate BONSAI across a
diverse set of synthetic and real-world case studies, including applications in
process systems engineering. Compared to existing simulation-based RO
algorithms, BONSAI consistently delivers more sample-efficient and
higher-quality robust solutions, highlighting its practical advantages for
uncertainty-aware design in complex engineering systems.

</details>


### [312] [LLM as an Algorithmist: Enhancing Anomaly Detectors via Programmatic Synthesis](https://arxiv.org/abs/2510.03904)
*Hangting Ye,Jinmeng Li,He Zhao,Mingchen Zhuge,Dandan Guo,Yi Chang,Hongyuan Zha*

Main category: cs.LG

TL;DR: LLM-DAS uses an LLM as an algorithm designer to synthesize detector-specific, data-agnostic anomalies via a reusable Python program, boosting tabular anomaly detectors across 36 benchmarks in a privacy-preserving way.


<details>
  <summary>Details</summary>
Motivation: Address performance variability in tabular AD due to reliance on anomaly-pattern assumptions, heterogeneity of data, and privacy concerns. Propose leveraging LLMs not to process raw data but to reason about algorithms to patch detectors’ weaknesses.

Method: The LLM analyzes a detector’s high-level description to identify intrinsic weaknesses and generates detector-specific, data-agnostic Python code that synthesizes hard-to-detect anomalies targeting those weaknesses. The synthesis program is instantiated to augment training data, reframing the problem as a two-class classification task to improve detector robustness.

Result: LLM-DAS consistently improves the performance of mainstream tabular AD detectors across 36 TAD benchmarks.

Conclusion: The approach bridges LLM reasoning with classic AD algorithms via programmatic synthesis, offering a scalable, privacy-preserving method to patch detectors’ logical blind spots.

Abstract: Existing anomaly detection (AD) methods for tabular data usually rely on some
assumptions about anomaly patterns, leading to inconsistent performance in
real-world scenarios. While Large Language Models (LLMs) show remarkable
reasoning capabilities, their direct application to tabular AD is impeded by
fundamental challenges, including difficulties in processing heterogeneous data
and significant privacy risks. To address these limitations, we propose
LLM-DAS, a novel framework that repositions the LLM from a ``data processor''
to an ``algorithmist''. Instead of being exposed to raw data, our framework
leverages the LLM's ability to reason about algorithms. It analyzes a
high-level description of a given detector to understand its intrinsic
weaknesses and then generates detector-specific, data-agnostic Python code to
synthesize ``hard-to-detect'' anomalies that exploit these vulnerabilities.
This generated synthesis program, which is reusable across diverse datasets, is
then instantiated to augment training data, systematically enhancing the
detector's robustness by transforming the problem into a more discriminative
two-class classification task. Extensive experiments on 36 TAD benchmarks show
that LLM-DAS consistently boosts the performance of mainstream detectors. By
bridging LLM reasoning with classic AD algorithms via programmatic synthesis,
LLM-DAS offers a scalable, effective, and privacy-preserving approach to
patching the logical blind spots of existing detectors.

</details>


### [313] [THEMIS: Unlocking Pretrained Knowledge with Foundation Model Embeddings for Anomaly Detection in Time Series](https://arxiv.org/abs/2510.03911)
*Yadav Mahesh Lorik,Kaushik Sarveswaran,Nagaraj Sundaramahalingam,Aravindakumar Venugopalan*

Main category: cs.LG

TL;DR: THEMIS uses Chronos foundation-model embeddings and self-similarity-based outlier detection to achieve state-of-the-art time-series anomaly detection with robust, interpretable results.


<details>
  <summary>Details</summary>
Motivation: Time series anomaly detection is challenged by seasonality, trends, noise, drift, diverse anomaly types, data imbalance, high dimensionality, real-time requirements, and interpretability; a flexible, confident approach leveraging pretrained representations may improve performance and generalizability.

Method: Extract encoder embeddings from Chronos time-series foundation model; compute self-similarity matrix; apply outlier detection methods (Local Outlier Factor, spectral decomposition) to identify anomalies.

Result: Achieves state-of-the-art performance on MSL dataset; competitive on SMAP and SWaT*; outperforms models trained specifically for anomaly detection; demonstrates hyperparameter robustness and default interpretability.

Conclusion: Advocates for using pretrained representations from foundation models for efficient, adaptable time-series anomaly detection.

Abstract: Time series anomaly detection forms a very crucial area in several domains
but poses substantial challenges. Due to time series data possessing
seasonality, trends, noise, and evolving patterns (concept drift), it becomes
very difficult to set a general notion of what constitutes normal behavior.
Anomalies themselves could be varied, ranging from a single outlier to
contextual or collective anomalies, and are normally very rare; hence, the
dataset is largely imbalanced. Additional layers of complexities arise due to
the problems of increased dimensionality of modern time series, real-time
detection criteria, setting up appropriate detection thresholds, and arriving
at results that are interpretable. To embrace these multifaceted challenges,
very strong, flexible, and interpretable approaches are required. This paper
presents THEMIS, a new framework for time series anomaly detection that
exploits pretrained knowledge from foundation models. THEMIS extracts
embeddings from the encoder of the Chronos time series foundation model and
applies outlier detection techniques like Local Outlier Factor and Spectral
Decomposition on the self-similarity matrix, to spot anomalies in the data. Our
experiments show that this modular method achieves SOTA results on the MSL
dataset and performs quite competitively on the SMAP and SWAT$^*$ datasets.
Notably, THEMIS exceeds models trained specifically for anomaly detection,
presenting hyperparameter robustness and interpretability by default. This
paper advocates for pretrained representations from foundation models for
performing efficient and adaptable anomaly detection for time series data.

</details>


### [314] [Generalized Fitted Q-Iteration with Clustered Data](https://arxiv.org/abs/2510.03912)
*Liyuan Hu,Jitao Wang,Zhenke Wu,Chengchun Shi*

Main category: cs.LG

TL;DR: A generalized FQI method incorporating generalized estimating equations is proposed to tackle RL with clustered data, providing theoretical optimality under correct correlation specification and consistency under mis-specification, with empirical results showing ~50% reduction in regret versus standard FQI.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning in healthcare often involves clustered data with intra-cluster correlations. Standard RL methods assume independence and may be inefficient or biased when this correlation is present. The work aims to integrate correlation-robust estimation into policy learning.

Method: Extend fitted Q-iteration by embedding generalized estimating equations into the update of Q-function and policy estimation. Analyze theoretical properties under correctly specified correlation structure and under mis-specification. Evaluate empirically via simulations and a mobile health dataset.

Result: The Q-function and policy estimators achieve optimality when the correlation structure is correctly specified, and are consistent even under mis-specification. Empirically, generalized FQI reduces average regret by about half compared to standard FQI in simulations and mobile health data analysis.

Conclusion: Incorporating GEE into FQI yields robust RL for clustered data, with strong theoretical guarantees and practical improvements. The approach highlights the importance of accounting for intra-cluster correlations, while noting potential sensitivity to the chosen correlation structure and the need for careful model specification.

Abstract: This paper focuses on reinforcement learning (RL) with clustered data, which
is commonly encountered in healthcare applications. We propose a generalized
fitted Q-iteration (FQI) algorithm that incorporates generalized estimating
equations into policy learning to handle the intra-cluster correlations.
Theoretically, we demonstrate (i) the optimalities of our Q-function and policy
estimators when the correlation structure is correctly specified, and (ii)
their consistencies when the structure is mis-specified. Empirically, through
simulations and analyses of a mobile health dataset, we find the proposed
generalized FQI achieves, on average, a half reduction in regret compared to
the standard FQI.

</details>


### [315] [Transductive and Learning-Augmented Online Regression](https://arxiv.org/abs/2510.03917)
*Vinod Raman,Shenghao Xie,Samson Zhou*

Main category: cs.LG

TL;DR: Transductive online regression: minimax expected regret characterized by fat-shattering dimension; separation from adversarial online regression; extension to imperfect predictions; adaptive online learner matches worst-case when predictions are poor and approaches transductive performance when predictions are accurate, enabling learnability under predictability.


<details>
  <summary>Details</summary>
Motivation: Real-life data streams are predictable; leveraging predictions about future examples can dramatically reduce regret. This work explores transductive online learning and its noisy extensions within a learning-augmented framework.

Method: Theoretically analyze transductive online regression using the fat-shattering dimension to characterize minimax regret; prove a separation from standard online regression; generalize to noisy/imperfect predictions; design an adaptive online learner with regret that interpolates between worst-case and transductive regimes based on prediction quality.

Result: Complete characterization of minimax expected regret for transductive online regression; establishes a separation from adversarial online regression; proposes an online learner whose regret improves smoothly with prediction quality and can match transductive performance for accurate predictions.

Conclusion: Supports the learning-augmented paradigm by showing that informative predictions yield substantial regret reductions, enabling learnability for classes previously unlearnable under predictability; bridges transductive and adversarial online learning through a framework governed by prediction quality.

Abstract: Motivated by the predictable nature of real-life in data streams, we study
online regression when the learner has access to predictions about future
examples. In the extreme case, called transductive online learning, the
sequence of examples is revealed to the learner before the game begins. For
this setting, we fully characterize the minimax expected regret in terms of the
fat-shattering dimension, establishing a separation between transductive online
regression and (adversarial) online regression. Then, we generalize this
setting by allowing for noisy or \emph{imperfect} predictions about future
examples. Using our results for the transductive online setting, we develop an
online learner whose minimax expected regret matches the worst-case regret,
improves smoothly with prediction quality, and significantly outperforms the
worst-case regret when future example predictions are precise, achieving
performance similar to the transductive online learner. This enables
learnability for previously unlearnable classes under predictable examples,
aligning with the broader learning-augmented model paradigm.

</details>


### [316] [On the Convergence and Size Transferability of Continuous-depth Graph Neural Networks](https://arxiv.org/abs/2510.03923)
*Mingsong Yan,Charles Kulick,Sui Tang*

Main category: cs.LG

TL;DR: Introduces Graphon-NDEs as the infinite-node limit of GNDEs, proving well-posedness and trajectory convergence; derives convergence rates for two graph sampling regimes; provides size-transferability bounds; experimental validation supports theory.


<details>
  <summary>Details</summary>
Motivation: To understand the scalability and size-transferability of continuous-depth graph neural networks by formalizing their infinite-node limit with graphon theory and establishing rigorous convergence and well-posedness results.

Method: Define Graphon-NDEs as the infinite-node limit of GNDEs; use graphon theory and dynamical systems to prove trajectory-wise convergence of GNDE solutions to Graphon-NDE solutions; derive explicit convergence rates under two regimes: (1) weighted graphs from smooth graphons, (2) unweighted graphs from discontinuous {0,1}-valued graphons; establish size-transferability bounds.

Result: Well-posedness of Graphon-NDEs; trajectory convergence of GNDEs to Graphon-NDEs; explicit convergence rates in both regimes; size-transferability bounds; numerical experiments on synthetic and real data corroborate the theory.

Conclusion: The graphon-based framework provides a rigorous foundation for understanding GNDE scalability and supports transferring GNDE models across graph sizes with minimal retraining, validated by experiments.

Abstract: Continuous-depth graph neural networks, also known as Graph Neural
Differential Equations (GNDEs), combine the structural inductive bias of Graph
Neural Networks (GNNs) with the continuous-depth architecture of Neural ODEs,
offering a scalable and principled framework for modeling dynamics on graphs.
In this paper, we present a rigorous convergence analysis of GNDEs with
time-varying parameters in the infinite-node limit, providing theoretical
insights into their size transferability. To this end, we introduce Graphon
Neural Differential Equations (Graphon-NDEs) as the infinite-node limit of
GNDEs and establish their well-posedness. Leveraging tools from graphon theory
and dynamical systems, we prove the trajectory-wise convergence of GNDE
solutions to Graphon-NDE solutions. Moreover, we derive explicit convergence
rates under two deterministic graph sampling regimes: (1) weighted graphs
sampled from smooth graphons, and (2) unweighted graphs sampled from
$\{0,1\}$-valued (discontinuous) graphons. We further establish size
transferability bounds, providing theoretical justification for the practical
strategy of transferring GNDE models trained on moderate-sized graphs to
larger, structurally similar graphs without retraining. Numerical experiments
using synthetic and real data support our theoretical findings.

</details>


### [317] [LLM Chemistry Estimation for Multi-LLM Recommendation](https://arxiv.org/abs/2510.03930)
*Huascar Sanchez,Briland Hitaj*

Main category: cs.LG

TL;DR: Introduces LLM Chemistry to quantify synergy and antagonism among multi-LLM ensembles, with formalism, algorithms, and theory; supported by initial task-based evidence to guide ensemble design.


<details>
  <summary>Details</summary>
Motivation: Current multi-LLM approaches rely on implicit selection and post-hoc output scoring rather than diagnosing whether collaborating models truly complement or conflict; understanding chemistry can improve robustness and performance.

Method: Formalizes the chemistry notion, designs algorithms to quantify interaction dependencies among LLMs, and analyzes how these interactions influence ensemble performance; provides theoretical results and evaluates on multiple NLP tasks.

Result: Theory indicates chemistry is most observable with heterogeneous model profiles; the impact depends on task type, group size, and problem complexity; empirical evaluation on classification, summarization, and program repair shows task-dependent effects.

Conclusion: LLM Chemistry offers a diagnostic lens and practical guidance for selecting and arranging model ensembles, forming a foundation for future ensemble optimization.

Abstract: Multi-LLM collaboration promises accurate, robust, and context-aware
solutions, yet existing approaches rely on implicit selection and output
assessment without analyzing whether collaborating models truly complement or
conflict. We introduce LLM Chemistry -- a framework that measures when LLM
combinations exhibit synergistic or antagonistic behaviors that shape
collective performance beyond individual capabilities. We formalize the notion
of chemistry among LLMs, propose algorithms that quantify it by analyzing
interaction dependencies, and recommend optimal model ensembles accordingly.
Our theoretical analysis shows that chemistry among collaborating LLMs is most
evident under heterogeneous model profiles, with its outcome impact shaped by
task type, group size, and complexity. Evaluation on classification,
summarization, and program repair tasks provides initial evidence for these
task-dependent effects, thereby reinforcing our theoretical results. This
establishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and
a foundation for ensemble recommendation.

</details>


### [318] [On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection](https://arxiv.org/abs/2510.03944)
*Weiqing He,Xiang Li,Tianqi Shang,Li Shen,Weijie Su,Qi Long*

Main category: cs.LG

TL;DR: Systematic evaluation of eight GoF tests for watermark detection in LLM-generated text; GoF tests boost detection power and robustness, with text repetition at low temperatures offering a distinct advantage; classic GoF tests are effective and underused.


<details>
  <summary>Details</summary>
Motivation: Watermarks provide verifiable content origin; GoF tests, assuming i.i.d. statistics on human text, are a natural but underexplored tool for watermark detection.

Method: Empirical study evaluating eight GoF tests across three watermarking schemes, three open-source LLMs, two datasets, varying generation temperatures, and post-editing methods.

Result: GoF tests improve detection power and robustness of watermark detectors; text repetition at low temperatures gives GoF tests a unique advantage; classic GoF tests are simple yet powerful for watermark detection in LLMs.

Conclusion: Classic GoF tests should be adopted for watermark detection in LLMs; results suggest further work on integrating GoF approaches with existing detectors and leveraging repetition signals.

Abstract: Large language models (LLMs) raise concerns about content authenticity and
integrity because they can generate human-like text at scale. Text watermarks,
which embed detectable statistical signals into generated text, offer a
provable way to verify content origin. Many detection methods rely on pivotal
statistics that are i.i.d. under human-written text, making goodness-of-fit
(GoF) tests a natural tool for watermark detection. However, GoF tests remain
largely underexplored in this setting. In this paper, we systematically
evaluate eight GoF tests across three popular watermarking schemes, using three
open-source LLMs, two datasets, various generation temperatures, and multiple
post-editing methods. We find that general GoF tests can improve both the
detection power and robustness of watermark detectors. Notably, we observe that
text repetition, common in low-temperature settings, gives GoF tests a unique
advantage not exploited by existing methods. Our results highlight that classic
GoF tests are a simple yet powerful and underused tool for watermark detection
in LLMs.

</details>


### [319] [What Is The Performance Ceiling of My Classifier? Utilizing Category-Wise Influence Functions for Pareto Frontier Analysis](https://arxiv.org/abs/2510.03950)
*Shahriar Kabir Nahin,Wenxiao Xiao,Joshua Liu,Anshuman Chhabra,Hongfu Liu*

Main category: cs.LG

TL;DR: Category-wise influence-based data-centric learning aims to maximize Pareto improvements across all classes by measuring training-data impact per category and using LP-based reweighting to improve each class.


<details>
  <summary>Details</summary>
Motivation: Move beyond optimizing overall accuracy to identifying and reaching the learning model's performance ceiling while ensuring improvements across all categories, avoiding class-wise tradeoffs.

Method: Develop category-wise influence functions and per-sample per-category influence vectors; derive a criterion to assess potential global improvement; formulate a linear programming reweighting framework to achieve Pareto improvements across categories.

Result: Empirical results on synthetic, vision, and text benchmarks show accurate estimation of per-category gains and successful attainment of Pareto-driven performance improvements across multiple classes.

Conclusion: Category-wise influence analysis enables understanding and achieving uniform gains; data-centric methods can be guided to reach a true performance ceiling without harming any category.

Abstract: Data-centric learning seeks to improve model performance from the perspective
of data quality, and has been drawing increasing attention in the machine
learning community. Among its key tools, influence functions provide a powerful
framework to quantify the impact of individual training samples on model
predictions, enabling practitioners to identify detrimental samples and retrain
models on a cleaner dataset for improved performance. However, most existing
work focuses on the question: "what data benefits the learning model?" In this
paper, we take a step further and investigate a more fundamental question:
"what is the performance ceiling of the learning model?" Unlike prior studies
that primarily measure improvement through overall accuracy, we emphasize
category-wise accuracy and aim for Pareto improvements, ensuring that every
class benefits, rather than allowing tradeoffs where some classes improve at
the expense of others. To address this challenge, we propose category-wise
influence functions and introduce an influence vector that quantifies the
impact of each training sample across all categories. Leveraging these
influence vectors, we develop a principled criterion to determine whether a
model can still be improved, and further design a linear programming-based
sample reweighting framework to achieve Pareto performance improvements.
Through extensive experiments on synthetic datasets, vision, and text
benchmarks, we demonstrate the effectiveness of our approach in estimating and
achieving a model's performance improvement across multiple categories of
interest.

</details>


### [320] [Optimizing Resources for On-the-Fly Label Estimation with Multiple Unknown Medical Experts](https://arxiv.org/abs/2510.03954)
*Tim Bary,Tiffanie Godelaine,Axel Abels,Benoît Macq*

Main category: cs.LG

TL;DR: An adaptive, real-time annotation framework for medical screening that learns without prior expert labels and labels data on-the-fly, querying additional experts based on instance difficulty to meet a confidence threshold; reduces annotation effort by up to 50% while matching non-adaptive baselines.


<details>
  <summary>Details</summary>
Motivation: Ground truth in medical screening often relies on expert coalitions and second opinions; existing aggregation algorithms are not suited for real-time pipelines with unknown expertise. There is need for an adaptive, label-efficient, on-the-fly annotation system.

Method: An online approach that incrementally collects expert opinions without pre-labeled data or knowledge of who the experts are. It dynamically queries experts guided by the latent difficulty of each instance, continues gathering opinions until a confidence threshold is achieved.

Result: Evaluated on three multi-annotator classification datasets across modalities; adaptive querying reduces the number of expert queries by up to 50% while achieving accuracy comparable to a non-adaptive baseline.

Conclusion: The proposed adaptive, real-time annotation framework is feasible and effective for screening workflows, enabling accurate labels with reduced annotation overhead; code available on GitHub.

Abstract: Accurate ground truth estimation in medical screening programs often relies
on coalitions of experts and peer second opinions. Algorithms that efficiently
aggregate noisy annotations can enhance screening workflows, particularly when
data arrive continuously and expert proficiency is initially unknown. However,
existing algorithms do not meet the requirements for seamless integration into
screening pipelines. We therefore propose an adaptive approach for real-time
annotation that (I) supports on-the-fly labeling of incoming data, (II)
operates without prior knowledge of medical experts or pre-labeled data, and
(III) dynamically queries additional experts based on the latent difficulty of
each instance. The method incrementally gathers expert opinions until a
confidence threshold is met, providing accurate labels with reduced annotation
overhead. We evaluate our approach on three multi-annotator classification
datasets across different modalities. Results show that our adaptive querying
strategy reduces the number of expert queries by up to 50% while achieving
accuracy comparable to a non-adaptive baseline. Our code is available at
https://github.com/tbary/MEDICS

</details>


### [321] [Early-Warning of Thunderstorm-Driven Power Outages with a Two-Stage Machine Learning Model](https://arxiv.org/abs/2510.03959)
*Iryna Stanishevska*

Main category: cs.LG

TL;DR: A two-stage, open-data–driven early-warning model predicts summer thunderstorm–related outages 24–48 h in advance in Michigan, using EAGLE-I outage data and METAR weather data. It combines feature-engineered spatial signals (kriging, variograms, distance-based aggregates) with a logistic gate and an LSTM regressor, optimized for event-centric metrics and uncertainty via moving-block bootstrap. SHAP highlights moisture advection and wind precursors; results show improved peak detection without sacrificing overall accuracy compared with a one-step LSTM baseline.


<details>
  <summary>Details</summary>
Motivation: Thunderstorm outages are hard to predict because most storms don’t cause damage, convective processes are rapid and chaotic, and public data are noisy and incomplete. An open-data–driven early-warning approach is needed to provide actionable forecasts up to 2 days ahead.

Method: A two-stage model: (1) feature-driven detection using a logistic gate, (2) an LSTM regressor for outage magnitude. The data pipeline includes kriging with hourly variograms, targeted overdrafting to preserve extremes, and causal spatio-temporal features (lags, rolling statistics, k-NN/IDW aggregates) to capture precursors (moisture advection, wind shifts, pressure drops). Event-centric metrics (cluster-based hits/misses/false alarms) with peak-focused cMASE, and hourly moving-block bootstrap for uncertainty. Evaluation on 2014–2022 EAGLE-I outages with METAR weather data.

Result: Two-Stage approach detects more reference peaks across windows than a one-step LSTM (e.g., at +/-48 h, 3/4 vs 2/4 references; F1 66.7% vs 57.1%), with one extra false alarm. Near peaks, modest amplitude gains (2–3% lower cMASE at +/-0–12 h; bootstrap medians +9–13% at +/-6–12 h) but small losses at +/-36–48 h (~3–4%). Overall, errors comparable to the one-step baseline. SHAP confirms moisture advection and wind/gust precursors, supporting the value of feature engineering.

Conclusion: Despite open-data noise, a feature-driven pipeline yields actionable, event-focused early warnings for thunderstorm outages. The two-stage design improves peak detection and leverages physically meaningful precursors, offering a practical approach for 24–48 h predictions using publicly available data.

Abstract: Thunderstorm-driven outages are difficult to predict because most storms do
not cause damage, convective processes occur rapidly and chaotically, and the
available public data are both noisy and incomplete. We develop a 24-48 h
early-warning model for summer, thunderstorm-related outages in Michigan using
only open sources (EAGLE-I for ground truth; METAR for weather). We use the
publicly released EAGLE-I outage dataset (2014-2022), maintained by Oak Ridge
National Laboratory for the U.S. Department of Energy. The pipeline preserves
convective micro-signals from a sparse station network via parameter-specific
kriging with hourly variograms and targeted overdrafting to retain extremes,
and builds causal spatio-temporal features (lags/rolling statistics; k-NN/IDW
spatial aggregates) capturing precursors of severe convection (moisture
advection, wind shifts, and pressure drops). The two-stage model design,
combining a logistic gate and an LSTM regressor, limits routine periods and
reduces noise exposure. The study uses event-centric metrics (cluster-based
hits/misses/false alarms) and peak-conditional MASE (cMASE) in +/-Delta-hour
windows around state-level peaks (>= 50,000), with uncertainty quantified by
hourly moving-block bootstrap.
  On the test sample, Two-Stage detects more reference peaks across all windows
(e.g., at +/-48 h it records 3/4 vs. 2/4; F1 66.7% vs. 57.1%) with one extra
false alarm. Near peaks, it shows modest amplitude gains (2-3% lower cMASE at
+/-0-12 h; bootstrap medians +9-13% at +/-6-12 h) but small losses at +/-36-48
h (~3-4%). Overall, errors are comparable to the one-step LSTM baseline. SHAP
analysis confirms moisture-advection and wind/gust precursors, underscoring the
value of the feature engineering. Despite open-data noise, the feature-driven
pipeline yields actionable, event-focused early warnings for thunderstorm
outages.

</details>


### [322] [SPEAR: Soft Prompt Enhanced Anomaly Recognition for Time Series Data](https://arxiv.org/abs/2510.03962)
*Hanzhe Wei,Jiajun Wu,Jialin Yang,Henry Leung,Steve Drew*

Main category: cs.LG

TL;DR: SPEAR uses soft prompts and quantization to adapt a frozen LLM for time-series anomaly detection, achieving improved performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Time-series anomaly detection faces challenges with variable-length sequences and context-based anomalies; while LLMs offer potential, they require task-specific adaptation (soft prompts) and proper sequence handling (quantization).

Method: Quantize time-series data into input embeddings and combine them with learnable soft prompt embeddings; feed the combined embeddings into a frozen LLM; iteratively update the soft prompts via cross-entropy loss to adapt to anomaly detection tasks.

Result: Experiments show that soft prompts improve LLM performance on time-series anomaly detection downstream tasks.

Conclusion: Soft prompts coupled with quantization enable effective use of LLMs for time-series anomaly detection, addressing variable-length sequences and context-based anomalies.

Abstract: Time series anomaly detection plays a crucial role in a wide range of fields,
such as healthcare and internet traffic monitoring. The emergence of large
language models (LLMs) offers new opportunities for detecting anomalies in the
ubiquitous time series data. Traditional approaches struggle with
variable-length time series sequences and context-based anomalies. We propose
Soft Prompt Enhanced Anomaly Recognition (SPEAR), a novel approach to leverage
LLMs for anomaly detection with soft prompts and quantization. Our methodology
involves quantizing and transforming the time series data into input embeddings
and combining them with learnable soft prompt embeddings. These combined
embeddings are then fed into a frozen LLM. The soft prompts are updated
iteratively based on a cross-entropy loss, allowing the model to adapt to time
series anomaly detection. The use of soft prompts helps adapt LLMs effectively
to time series tasks, while quantization ensures optimal handling of sequences,
as LLMs are designed to handle discrete sequences. Our experimental results
demonstrate that soft prompts effectively increase LLMs' performance in
downstream tasks regarding time series anomaly detection.

</details>


### [323] [What Can You Do When You Have Zero Rewards During RL?](https://arxiv.org/abs/2510.03971)
*Jatin Prakash,Anirudh Buvanesh*

Main category: cs.LG

TL;DR: Zero-reward barrier in RL-based reasoning tasks where base models fail to sample correct solutions; a simple data-centric fix—adding easier samples to the training set—enables solving the hard task without changing the RL algorithm.


<details>
  <summary>Details</summary>
Motivation: To understand why outcome-based RL struggles when no correct solutions are observed and to assess whether recent enhancements (dense rewards, diversity, credit assignment) can overcome this barrier.

Method: Empirical study on the graph search task from Bachmann et al. (2024). Evaluate several RL baselines with enhancements; implement missing baselines to analyze failure modes; compare against a data-centric intervention that augments training data with easier samples; release code used for experiments.

Result: Dense rewards, diversity incentives, and improved credit assignment do not overcome the zero-reward barrier if the base model never produces a correct answer. A simple data-centric intervention—adding easier samples to the training set—enables the model to eventually solve the original hard task without modifying the RL algorithm.

Conclusion: Data-centric sample augmentation can break the zero-reward learning bottleneck in RL for reasoning tasks. When correct samples are scarce, enriching the training data with easier examples can unlock progress without altering the RL method itself. The authors also provide their implemented baselines to facilitate further research.

Abstract: Reinforcement learning (RL) with outcome-based rewards has proven effective
for improving large language models (LLMs) on complex reasoning tasks. However,
its success often depends on the base model occasionally sampling correct
solutions. When no correct solutions are sampled, training encounters a
zero-reward barrier where learning stalls due to zero gradients. We study this
scenario through the graph search task introduced in Bachmann et al. (2024) and
evaluate recent methods that incorporate desirable components such as dense
rewards, diversity incentives, and improved credit assignment. Our experiments
show that none of these approaches overcome the zero-reward barrier if the base
model never produces a correct answer. In contrast, we find that a simple
data-centric intervention of adding easier samples to the training set enables
the model to eventually solve the original hard task despite starting from zero
reward. Importantly, this succeeds without modifying the RL algorithm itself.
Because official implementations of several baselines were unavailable, we
developed our own, which allowed us to conduct a detailed analysis of their
failure modes. We release these implementations to support further research at:
https://github.com/rl4reasoning/rl-baselines

</details>


### [324] [Beyond Softmax: A New Perspective on Gradient Bandits](https://arxiv.org/abs/2510.03979)
*Emerson Melo,David Müller*

Main category: cs.LG

TL;DR: A theoretical paper linking discrete choice models with online learning and bandits, introducing sublinear regret algorithms, adversarial bandits from generalized nested logit, and generalized gradient bandits that relax softmax independence, with efficient sampling, plus empirical validation.


<details>
  <summary>Details</summary>
Motivation: To leverage discrete choice modeling to design flexible, efficient bandit algorithms with broader applicability, bridging economics/psychology-based choice models and online learning.

Method: Develop sublinear regret bounds for a broad family including Exp3; derive adversarial bandit algorithms from generalized nested logit models; introduce generalized gradient bandits that relax independence assumptions of softmax to allow correlated action learning; ensure computational efficiency via closed-form sampling probabilities.

Result: Theoretical guarantees of sublinear regret; new adversarial bandit algorithms from discrete choice structure; a generalized gradient bandit framework with correlations; practical efficiency demonstrated by closed-form sampling; empirical validation in stochastic bandit settings.

Conclusion: Linking discrete choice theory with online learning yields flexible, scalable bandit algorithms with broader applicability and empirical effectiveness; the approach broadens the toolkit for both theoretical analysis and practical deployment in settings with structured choice data.

Abstract: We establish a link between a class of discrete choice models and the theory
of online learning and multi-armed bandits. Our contributions are: (i)
sublinear regret bounds for a broad algorithmic family, encompassing Exp3 as a
special case; (ii) a new class of adversarial bandit algorithms derived from
generalized nested logit models \citep{wen:2001}; and (iii)
\textcolor{black}{we introduce a novel class of generalized gradient bandit
algorithms that extends beyond the widely used softmax formulation. By relaxing
the restrictive independence assumptions inherent in softmax, our framework
accommodates correlated learning dynamics across actions, thereby broadening
the applicability of gradient bandit methods.} Overall, the proposed algorithms
combine flexible model specification with computational efficiency via
closed-form sampling probabilities. Numerical experiments in stochastic bandit
settings demonstrate their practical effectiveness.

</details>


### [325] [ICEPool: Enhancing Graph Pooling Networks with Inter-cluster Connectivity](https://arxiv.org/abs/2510.03987)
*Michael Yang*

Main category: cs.LG

TL;DR: ICEPool is a general hierarchical pooling framework for graph neural networks that emphasizes inter-cluster connectivity during pooling, is compatible with many pooling-based GNNs, has a theoretical graph-reconstruction perspective, and shows empirical potential to boost performance.


<details>
  <summary>Details</summary>
Motivation: Many existing graph pooling methods focus on intra-cluster aggregation and local structure while overlooking inter-cluster relationships. This can lead to loss of global connectivity and weaker graph-level representations. Enhancing inter-cluster connectivity preservation could yield more faithful graph reconstructions and better downstream performance.

Method: Introduce ICEPool as a flexible pooling framework that enhances inter-cluster connectivity in the coarsening process and is compatible with a wide range of pooling-based GNNs. The authors provide theoretical analysis of ICEPool’s graph reconstruction capability to demonstrate its ability to learn inter-cluster relationships and validate the approach experimentally across diverse models.

Result: Empirical results indicate that ICEPool is compatible with a variety of base models and has the potential to boost graph-level representations and overall performance. The theoretical analysis supports its effectiveness in capturing inter-cluster relationships that conventional pooling may miss.

Conclusion: ICEPool offers a general, robust enhancement to graph pooling by explicitly modeling inter-cluster connectivity. It can be integrated with existing GNN architectures to preserve structural integrity during coarsening, with theoretical backing and demonstrated compatibility across different models.

Abstract: Hierarchical Pooling Models have demonstrated strong performance in
classifying graph-structured data. While numerous innovative methods have been
proposed to design cluster assignments and coarsening strategies, the
relationships between clusters are often overlooked. In this paper, we
introduce Inter-cluster Connectivity Enhancement Pooling (ICEPool), a novel
hierarchical pooling framework designed to enhance model's understanding of
inter-cluster connectivity and ability of preserving the structural integrity
in the original graph. ICEPool is compatible with a wide range of pooling-based
GNN models. The deployment of ICEPool as an enhancement to existing models
effectively combines the strengths of the original model with ICEPool's
capability to emphasize the integration of inter-cluster connectivity,
resulting in a more comprehensive and robust graph-level representation.
Moreover, we make theoretical analysis to ICEPool's ability of graph
reconstruction to demonstrate its effectiveness in learning inter-cluster
relationship that is overlooked by conventional models. Finally, the
experimental results show the compatibility of ICEPool with wide varieties of
models and its potential to boost the performance of existing graph neural
network architectures.

</details>


### [326] [Distilling Reasoning into Student LLMs: Local Naturalness for Selecting Teacher Data](https://arxiv.org/abs/2510.03988)
*Hoang Anh Just,Myeongseob Ko,Ruoxi Jia*

Main category: cs.LG

TL;DR: Local Naturalness improves response selection in multi-teacher reasoning distillation, outperforming global naturalness and boosting a 32B student’s math accuracy by about 9.4 percentage points.


<details>
  <summary>Details</summary>
Motivation: In multi-teacher settings, the common approach of selecting responses by global log-probability (global naturalness) fails as reasoning traces lengthen; there is a need to reliably identify which teacher and which response best aid the student.

Method: Introduce Local Naturalness: the student’s log-probabilities over short, sequential reasoning steps within a local window. Use local scores for (1) teacher selection across prompts and (2) response selection when mixing outputs from multiple teachers. Evaluate by distilling from multiple teachers to a 32B student on math benchmarks.

Result: Local Naturalness reliably identifies the most helpful teachers and improves the 32B student’s math benchmark accuracy by 9.4 percentage points over global selection, exceeding the performance achieved by training on data from the single best teacher.

Conclusion: Localized, step-wise evaluation of data quality and data mixing significantly enhances reasoning distillation in multi-teacher settings, offering a practical improvement over global naturalness-based selection.

Abstract: Distilling long reasoning traces (10K+ tokens) from stronger teacher models
into smaller student LLMs via SFT has emerged as a standard paradigm. This
approach is practical and efficient: it leverages the ease of generating
abundant reasoning data from stronger models and provides a direct, data-driven
way to teach less capable models better reasoning. While previous work has
largely focused on prompt selection with responses from a single teacher, the
equally important problem of choosing the best response when multiple teacher
outputs are available for a single prompt remains underexplored. This challenge
becomes important in a multi-teacher setting, where different students may
benefit from the outputs of different teachers. This paper fills that gap with
a systematic study of response selection for reasoning distillation. We first
show that the current method, which picks responses the student assigns the
highest global log-probability (global naturalness), fails when responses come
from multiple teachers, i.e., global naturalness no longer correlates with
downstream performance, especially as the reasoning traces from strong teachers
become longer. To overcome this problem, we introduce Local Naturalness, which
measures the student's log-probabilities over short, sequential reasoning steps
conditioned only on a small local window. Local Naturalness enables two
applications: 1) Teacher Selection: Aggregating local scores across prompts
reliably identifies the most helpful teacher. 2) Response Selection from a
Multiple Teachers: When mixing answers from many teachers, Local Naturalness
boosts a 32B student's accuracy on math benchmarks by 9.4pp over global
selection, also surpassing the performance achieved by training on data from
the single best teacher. These results highlight the power of localized data
quality evaluation and data mixing for more effective reasoning distillation.

</details>


### [327] [A Mathematical Explanation of Transformers for Large Language Models and GPTs](https://arxiv.org/abs/2510.03989)
*Xue-Cheng Tai,Hao Liu,Lingfeng Li,Raymond H. Chan*

Main category: cs.LG

TL;DR: A continuous integro-differential view of Transformer theory, interpreting self-attention as a non-local operator and layer normalization as a time-dependent projection, unifying attention, feedforward, and normalization in a continuous framework to guide design and analysis.


<details>
  <summary>Details</summary>
Motivation: Fill the gap in rigorous mathematical understanding of Transformer architectures and provide a unified, interpretable foundation that connects core components (attention, FFN, normalization) within a continuous framework.

Method: Develop a discretized structured integro-differential equation as the underlying model; treat self-attention as a non-local integral operator and layer normalization as a projection to a time-dependent constraint; embed token indices and feature dimensions in continuous domains; use operator-theoretic and variational analysis to link components and derive interpretations.

Result: Produces a unified, interpretable foundation for Transformer components, with self-attention arising naturally as a non-local operator and normalization as a projection; extends the theory to continuous token/feature domains and opens new directions for design, analysis, and control-based interpretation.

Conclusion: This continuous, operator-theoretic framework bridges deep learning and mathematical modeling, offering a foundational perspective for theoretically grounded, interpretable neural networks and guiding future architecture design and analysis.

Abstract: The Transformer architecture has revolutionized the field of sequence
modeling and underpins the recent breakthroughs in large language models
(LLMs). However, a comprehensive mathematical theory that explains its
structure and operations remains elusive. In this work, we propose a novel
continuous framework that rigorously interprets the Transformer as a
discretization of a structured integro-differential equation. Within this
formulation, the self-attention mechanism emerges naturally as a non-local
integral operator, and layer normalization is characterized as a projection to
a time-dependent constraint. This operator-theoretic and variational
perspective offers a unified and interpretable foundation for understanding the
architecture's core components, including attention, feedforward layers, and
normalization. Our approach extends beyond previous theoretical analyses by
embedding the entire Transformer operation in continuous domains for both token
indices and feature dimensions. This leads to a principled and flexible
framework that not only deepens theoretical insight but also offers new
directions for architecture design, analysis, and control-based
interpretations. This new interpretation provides a step toward bridging the
gap between deep learning architectures and continuous mathematical modeling,
and contributes a foundational perspective to the ongoing development of
interpretable and theoretically grounded neural network models.

</details>


### [328] [Incorporating Multivariate Consistency in ML-Based Weather Forecasting with Latent-space Constraints](https://arxiv.org/abs/2510.04006)
*Hang Fan,Yi Xiao,Yongquan Qu,Fenghua Ling,Ben Fei,Lei Bai,Pierre Gentine*

Main category: cs.LG

TL;DR: A weak-constraint 4D-Var-inspired training framework for ML weather forecasts uses an autoencoder latent space to incorporate reanalysis error and multivariate dependencies, enabling training on heterogeneous data and improving long-term forecast realism compared to model-space loss.


<details>
  <summary>Details</summary>
Motivation: ML-based weather forecasts often treat reanalysis as truth and optimize univariate losses, leading to error accumulation, blurring, and physically inconsistent forecasts. Incorporating data-assimilation ideas and multivariate error structures can improve long-horizon skill and physical realism.

Method: Recast training as a WC-4DVar problem with reanalysis treated as imperfect observations. Compute loss in a latent space learned by an autoencoder, where reanalysis covariance is approximately diagonal. This avoids modeling high-dimensional covariances explicitly. Extend to heterogeneous data by jointly training on reanalysis and multi-source observations within a unified framework.

Result: Rollout training with latent-space constraints yields better long-term forecast skill and preserves fine-scale structures and physical realism compared to model-space loss.

Conclusion: The WC-4DVar latent-space approach provides a principled, data-assimilation–inspired framework for ML weather forecasting, capable of handling imperfect reanalysis and multiple data sources, with improved long-horizon performance and physical fidelity.

Abstract: Data-driven machine learning (ML) models have recently shown promise in
surpassing traditional physics-based approaches for weather forecasting,
leading to a so-called second revolution in weather forecasting. However, most
ML-based forecast models treat reanalysis as the truth and are trained under
variable-specific loss weighting, ignoring their physical coupling and spatial
structure. Over long time horizons, the forecasts become blurry and physically
unrealistic under rollout training. To address this, we reinterpret model
training as a weak-constraint four-dimensional variational data assimilation
(WC-4DVar) problem, treating reanalysis data as imperfect observations. This
allows the loss function to incorporate reanalysis error covariance and capture
multivariate dependencies. In practice, we compute the loss in a latent space
learned by an autoencoder (AE), where the reanalysis error covariance becomes
approximately diagonal, thus avoiding the need to explicitly model it in the
high-dimensional model space. We show that rollout training with latent-space
constraints improves long-term forecast skill and better preserves fine-scale
structures and physical realism compared to training with model-space loss.
Finally, we extend this framework to accommodate heterogeneous data sources,
enabling the forecast model to be trained jointly on reanalysis and
multi-source observations within a unified theoretical formulation.

</details>


### [329] [Replacing Softmax Similarity with a Sharpened Angular Similarity: Theory and Practice of Scaling To Billion-Context Attention](https://arxiv.org/abs/2510.04008)
*Sahil Joshi,Agniva Chowdhury,Amar Kanakamedala,Ekam Singh,Evan Tu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: RACE Attention offers a linear-time alternative to Softmax attention by using sharpened cosine similarity and LSH-based approximations, enabling extremely long context processing with comparable accuracy to strong baselines.


<details>
  <summary>Details</summary>
Motivation: Softmax attention has quadratic time and memory complexity, hindering long-context modeling and limiting scalability on modern hardware; existing exact implementations struggle beyond a few million tokens, motivating a scalable alternative.

Method: Replace the exponential kernel with a sharpened angular (cosine) similarity. Approximate attention outputs using randomized projections and soft Locality-Sensitive Hashing (LSH). Implemented as a kernel-inspired approach and evaluated across language modeling, masked LM, and text classification.

Result: Maintains accuracy comparable to strong baselines while reducing runtime and memory. Demonstrates scalability to 12 million tokens per forward-backward pass on an NVIDIA GH200 GPU and 75 million tokens on an Intel Xeon CPU, surpassing practical limits of current state-of-the-art attention implementations.

Conclusion: RACE Attention provides a practical, theoretically grounded mechanism for ultra-long context windows on today’s hardware and is positioned for potential practical adoption.

Abstract: Softmax Attention has a quadratic time complexity, which becomes prohibitive
to run at long contexts, even with highly optimized GPU kernels. For example,
FlashAttention (an exact, GPU-optimized implementation of Softmax Attention)
cannot complete a single forward-backward pass of a multi-head attention layer
once the context exceeds ~4 million tokens on an NVIDIA GH200 (96 GB). We
introduce RACE Attention, a kernel-inspired alternative to Softmax Attention
that is linear in sequence length and embedding dimension. RACE Attention
replaces the exponential kernel with a sharpened angular (cosine) similarity,
and approximates attention outputs via randomized projections and soft
Locality-Sensitive Hashing (LSH). Across language modeling, masked language
modeling, and text classification, RACE Attention matches the accuracy of
strong baselines while reducing runtime and memory. In a controlled scale test,
it processes up to 12 million tokens during a single forward-backward pass on
an NVIDIA GH200 GPU and 75 million tokens on an Intel Xeon Gold 5220R CPU, well
beyond the practical limits of the current state-of-the-art attention
implementations. RACE Attention thus offers a practical, theoretically grounded
mechanism for outrageously long context windows on today's hardware. We hope
that it gets adopted in practice.

</details>


### [330] [Principled and Tractable RL for Reasoning with Diffusion Language Models](https://arxiv.org/abs/2510.04019)
*Anthony Zhan*

Main category: cs.LG

TL;DR: Introduces AGRPO, an on-policy RL method for diffusion LLMs (dLLMs) that provides unbiased policy gradient estimates via Monte Carlo sampling, enabling effective post-training RL for dLLMs and achieving notable gains on GSM8K and Countdown, with better compute-performance tradeoffs.


<details>
  <summary>Details</summary>
Motivation: Address the gap that diffusion LLMs lag behind autoregressive LLMs in post-training improvements and that traditional RL methods are not directly compatible with diffusion frameworks, necessitating a principled RL approach with theoretical grounding for dLLMs.

Method: Amortized Group Relative Policy Optimization (AGRPO): an on-policy RL algorithm tailored for dLLMs. It uses Monte Carlo sampling to compute unbiased policy gradient estimates, enabling a faithful adaptation of policy gradient methods to diffusion LLMs, with amortized/grouped objectives to reduce variance and stabilize training.

Result: AGRPO yields up to +7.6% absolute gain on GSM8K and 3.8x improvement on Countdown over the baseline LLaDA-8B-Instruct; 1.3x gains over comparable RL methods such as diffu-GRPO; gains persist across varying inference sampling steps, indicating favorable compute-performance tradeoffs.

Conclusion: Online RL algorithms can be extended to diffusion LLMs in a principled and effective manner. AGRPO provides theoretical soundness (unbiased gradient via Monte Carlo) and practical gains, establishing a tractable pathway for post-training enhancements of dLLMs.

Abstract: Diffusion large language models (dLLMs) are a new paradigm of
non-autoregressive language models that are trained to predict multiple tokens
in parallel and generate text via iterative unmasking. Recent works have
successfully pretrained dLLMs to parity with autoregressive LLMs at the 8B
scale, but dLLMs have yet to benefit from modern post-training techniques, e.g.
reinforcement learning (RL), that have proven effective for autoregressive
models. Crucially, algorithms designed for traditional LLMs aren't directly
compatible with diffusion frameworks due to inherent differences in modeling
assumptions. Moreover, existing attempts at dLLM post-training with RL rely on
heuristic-based objectives with no theoretical grounding. In this work, we
present Amortized Group Relative Policy Optimization (AGRPO), a principled
on-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo
sampling to compute an unbiased policy gradient estimate, making it the first
tractable, faithful adaptation of policy gradient methods for dLLMs. We
demonstrate AGRPO's effectiveness on different math/reasoning tasks, a common
setting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x
performance on the Countdown task over the baseline LLaDA-8B-Instruct model and
1.3x performance gains over comparable RL methods such as diffu-GRPO.
Furthermore, these gains persist across different numbers of sampling steps at
inference time, achieving better tradeoffs between compute and performance. Our
results demonstrate that online RL algorithms can be extended to diffusion LLMs
in principled ways, maintaining both theoretical soundness and practical
effectiveness.

</details>


### [331] [Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models](https://arxiv.org/abs/2510.04020)
*Hao Wu,Yuan Gao,Xingjian Shi,Shuaipeng Li,Fan Xu,Fan Zhang,Zhihong Zhu,Weiyan Wang,Xiao Luo,Kun Wang,Xian Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: A new Spatiotemporal Forecasting as Planning (SFP) paradigm uses a Generative World Model and beam-search planning within model-based RL to handle stochastic spatiotemporal forecasts with non-differentiable metrics, using self-training with pseudo-labels to improve policy and capture extreme events.


<details>
  <summary>Details</summary>
Motivation: Address inherent stochasticity in physical spatiotemporal forecasting and the challenge posed by non-differentiable, domain-specific metrics that hinder gradient-based optimization.

Method: Construct a Generative World Model to simulate diverse futures; treat the base forecasting model as an agent; apply beam-search planning that uses non-differentiable domain metrics as rewards to identify high-return future sequences; use these sequences as pseudo-labels to iteratively self-train the agent.

Result: Significant reduction in prediction error and exceptional performance on domain metrics, notably in capturing extreme events.

Conclusion: SFP offers a principled framework to tackle stochasticity and non-differentiability in spatiotemporal forecasting by integrating imagination-based planning with self-training to improve forecasting accuracy and robustness.

Abstract: To address the dual challenges of inherent stochasticity and
non-differentiable metrics in physical spatiotemporal forecasting, we propose
Spatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in
Model-Based Reinforcement Learning. SFP constructs a novel Generative World
Model to simulate diverse, high-fidelity future states, enabling an
"imagination-based" environmental simulation. Within this framework, a base
forecasting model acts as an agent, guided by a beam search-based planning
algorithm that leverages non-differentiable domain metrics as reward signals to
explore high-return future sequences. These identified high-reward candidates
then serve as pseudo-labels to continuously optimize the agent's policy through
iterative self-training, significantly reducing prediction error and
demonstrating exceptional performance on critical domain metrics like capturing
extreme events.

</details>


### [332] [Multi-Class Support Vector Machine with Differential Privacy](https://arxiv.org/abs/2510.04027)
*Jinseong Park,Yujin Choi,Jaewook Lee*

Main category: cs.LG

TL;DR: Proposes PMSVM, an all-in-one differentially private multi-class SVM using weight and gradient perturbation to preserve privacy while maintaining margin properties, outperforming prior DP-SVM methods.


<details>
  <summary>Details</summary>
Motivation: To address the privacy-budget inefficiency of standard DP approaches (OvR/OvO) for multi-class SVMs, which require multiple data queries per sample and waste privacy budget.

Method: Introduce an all-in-one DP multi-class SVM (PMSVM) that accesses each data sample only once. Employ weight perturbation and gradient perturbation, along with rigorous sensitivity and convergence analyses, to ensure differential privacy and margin-maximizing boundaries in a single training pass.

Result: Empirical evaluations show that PMSVM outperforms existing DP-SVM methods in multi-class scenarios, delivering better accuracy/privacy trade-offs.

Conclusion: PMSVM demonstrates that an all-in-one, perturbation-based DP approach can achieve strong privacy guarantees for multi-class SVMs while improving performance over previous DP methods.

Abstract: With the increasing need to safeguard data privacy in machine learning
models, differential privacy (DP) is one of the major frameworks to build
privacy-preserving models. Support Vector Machines (SVMs) are widely used
traditional machine learning models due to their robust margin guarantees and
strong empirical performance in binary classification. However, applying DP to
multi-class SVMs is inadequate, as the standard one-versus-rest (OvR) and
one-versus-one (OvO) approaches repeatedly query each data sample when building
multiple binary classifiers, thus consuming the privacy budget proportionally
to the number of classes. To overcome this limitation, we explore all-in-one
SVM approaches for DP, which access each data sample only once to construct
multi-class SVM boundaries with margin maximization properties. We propose a
novel differentially Private Multi-class SVM (PMSVM) with weight and gradient
perturbation methods, providing rigorous sensitivity and convergence analyses
to ensure DP in all-in-one SVMs. Empirical results demonstrate that our
approach surpasses existing DP-SVM methods in multi-class scenarios.

</details>


### [333] [The Debate on RLVR Reasoning Capability Boundary: Shrinkage, Expansion, or Both? A Two-Stage Dynamic View](https://arxiv.org/abs/2510.04028)
*Xinhao Yao,Lu Yu,Xiaolin Hu,Fengwei Teng,Qing Cui,Jun Zhou,Yong Liu*

Main category: cs.LG

TL;DR: A unified view of RL with verifiable rewards (RLVR) on LLM reasoning: early exploitation tightens capability boundaries, while prolonged exploration expands them; training duration and gradient strategies (e.g., relative negative gradients) shape these effects.


<details>
  <summary>Details</summary>
Motivation: Resolve conflicting claims that RLVR either shrinks or expands LLM reasoning, by uncovering a two-stage probability mass dynamic that explains both phenomena and guides training strategies.

Method: Theoretical analysis of token-probability dynamics under positive advantage estimates, identified as two stages (exploitation and exploration); complemented by empirical studies showing stage-dependent shifts in token probabilities and performance under varying training durations and gradient schemes.

Result: Two-stage dynamics: during exploitation, high-reward token probabilities rise but the optimal token’s probability remains largely unchanged; during exploration, saturation of high-reward tokens prompts occasional sampling of the optimal token, increasing its probability and reducing others, enabling boundary expansion with longer training; relative negative gradients can prolong training and promote richer reasoning capabilities.

Conclusion: The paradoxes about RLVR’s impact are reconciled: early over-exploitation can shrink capabilities, while extended exploration can expand them. Training guidelines such as emphasizing relative negative gradients provide theoretical and empirical support for fostering more advanced reasoning in LLMs.

Abstract: The ongoing debate on whether reinforcement learning with verifiable rewards
(RLVR) expands or shrinks the reasoning capabilities of large language models
(LLMs) remains unresolved. Some studies contend that RLVR mainly improves
sampling efficiency but at the expense of diversity and exploratory capacity,
resulting in capability boundary shrinkage. In contrast, others demonstrate
that prolonged training can lead to the emergence of novel reasoning
strategies, suggesting capability boundary expansion. To reconcile these
contradictory findings, we theoretically and empirically show that both
perspectives are partially valid-each aligning with a separate phase in an
inherent two-stage probability mass dynamic: (1) Exploitation stage: initially,
the model primarily samples explored high-reward and low-reward tokens, while
rarely selecting the potentially optimal token. Positive advantage estimates
increase the probability of high-reward tokens and decrease those of low-reward
tokens, yet the optimal token's probability remains largely unchanged during
this stage. (2) Exploration stage: as training advances, the growth rate of
previously acquired high-reward tokens slows as their probabilities approach
saturation. When a potentially optimal token-now receiving positive advantage
estimates-is occasionally sampled, its probability increases, while those of
the originally high-reward tokens decrease. This dynamic suggests that
over-exploitation during the exploitation stage may lead to capability boundary
shrinkage, whereas prolonged training into the exploration stage can promote an
expansion of the reasoning capability boundary. Building upon our insights, we
revisit the potential of only using relative negative gradients for prolonging
training, providing a theoretical and empirical foundation for the development
of more advanced reasoning capabilities.

</details>


### [334] [Adaptive kernel-density approach for imbalanced binary classification](https://arxiv.org/abs/2510.04046)
*Kotaro J. Nishimura,Yuichi Sakumura,Kazushi Ikeda*

Main category: cs.LG

TL;DR: KOTARO uses kernel-density-based threshold adjustment with per-sample bandwidth tuning to adapt decision boundaries to local density, improving minority-class recognition under severe class imbalance.


<details>
  <summary>Details</summary>
Motivation: Address the bias toward the majority class in highly imbalanced binary classification, especially in medical diagnosis and anomaly detection, where minority accuracy is critical.

Method: Extends kernel density estimation with Gaussian basis functions; dynamically tunes bandwidth based on local density around each sample; adjusts decision thresholds regionally to capture minority regions; validated on synthetic and real-world imbalanced datasets.

Result: KOTARO outperforms conventional methods, especially under severe imbalance, demonstrating effectiveness and potential across imbalanced classification problems.

Conclusion: KOTARO shows promise as a versatile solution for imbalanced binary classification and can be applicable to a wide range of domains.

Abstract: Class imbalance is a common challenge in real-world binary classification
tasks, often leading to predictions biased toward the majority class and
reduced recognition of the minority class. This issue is particularly critical
in domains such as medical diagnosis and anomaly detection, where correct
classification of minority classes is essential. Conventional methods often
fail to deliver satisfactory performance when the imbalance ratio is extremely
severe. To address this challenge, we propose a novel approach called
Kernel-density-Oriented Threshold Adjustment with Regional Optimization
(KOTARO), which extends the framework of kernel density estimation (KDE) by
adaptively adjusting decision boundaries according to local sample density. In
KOTARO, the bandwidth of Gaussian basis functions is dynamically tuned based on
the estimated density around each sample, thereby enhancing the classifier's
ability to capture minority regions. We validated the effectiveness of KOTARO
through experiments on both synthetic and real-world imbalanced datasets. The
results demonstrated that KOTARO outperformed conventional methods,
particularly under conditions of severe imbalance, highlighting its potential
as a promising solution for a wide range of imbalanced classification problems

</details>


### [335] [Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models under Data Constraints](https://arxiv.org/abs/2510.04058)
*Subhodip Panda,MS Varun,Shreyans Jain,Sarthak Kumar Maharana,Prathosh A. P*

Main category: cs.LG

TL;DR: Introduces Variational Diffusion Unlearning (VDU), a data-efficient method to forget undesired outputs in pretrained diffusion models using a variational objective with a plasticity inducer and stability regularizer, enabling class and feature unlearning with limited data while preserving image quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion models can generate undesired content; safe deployment requires removing such outputs. Existing machine unlearning methods often assume access to the full training data, which is impractical in data-constrained settings. A data-efficient unlearning method is needed to forget undesired features/classes from pretrained models without full data access.

Method: VDU formulates unlearning as variational inference with a loss comprising two terms: a plasticity inducer that lowers the log-likelihood of undesired training data points, and a stability regularizer that constrains parameter updates to preserve generation quality. It operates using only a subset of training data containing undesired features. Experiments cover class unlearning (MNIST, CIFAR-10, tinyImageNet) from a pretrained unconditional DDPM, and feature unlearning (high-level features) from a pretrained Stable Diffusion model.

Result: VDU effectively reduces the generation of undesired outputs in both class and feature unlearning scenarios while maintaining image quality, demonstrated on datasets such as MNIST, CIFAR-10, and tinyImageNet with diffusion models (DDPM and Stable Diffusion). The method is data-efficient and computationally feasible.

Conclusion: VDU offers a practical, data-constrained unlearning framework for diffusion models, enabling safer deployment by removing undesired outputs without requiring full access to training data. It balances forgetting with quality preservation and points to future work on broader datasets and stronger privacy/performance guarantees.

Abstract: For a responsible and safe deployment of diffusion models in various domains,
regulating the generated outputs from these models is desirable because such
models could generate undesired, violent, and obscene outputs. To tackle this
problem, recent works use machine unlearning methodology to forget training
data points containing these undesired features from pre-trained generative
models. However, these methods proved to be ineffective in data-constrained
settings where the whole training dataset is inaccessible. Thus, the principal
objective of this work is to propose a machine unlearning methodology that can
prevent the generation of outputs containing undesired features from a
pre-trained diffusion model in such a data-constrained setting. Our proposed
method, termed as Variational Diffusion Unlearning (VDU), is a computationally
efficient method that only requires access to a subset of training data
containing undesired features. Our approach is inspired by the variational
inference framework with the objective of minimizing a loss function consisting
of two terms: plasticity inducer and stability regularizer. Plasticity inducer
reduces the log-likelihood of the undesired training data points, while the
stability regularizer, essential for preventing loss of image generation
quality, regularizes the model in parameter space. We validate the
effectiveness of our method through comprehensive experiments for both class
unlearning and feature unlearning. For class unlearning, we unlearn some
user-identified classes from MNIST, CIFAR-10, and tinyImageNet datasets from a
pre-trained unconditional denoising diffusion probabilistic model (DDPM).
Similarly, for feature unlearning, we unlearn the generation of certain
high-level features from a pre-trained Stable Diffusion model

</details>


### [336] [What Scales in Cross-Entropy Scaling Law?](https://arxiv.org/abs/2510.04067)
*Junxi Yan,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: Error-Entropy scales with model size; cross-entropy decomposes into Error-Entropy, Self-Alignment, and Confidence; only Error-Entropy shows robust power-law scaling; others remain largely invariant; larger models reduce its share; explains breakdown of cross-entropy scaling at very large scales across 32 models spanning five orders of magnitude.


<details>
  <summary>Details</summary>
Motivation: Cross-entropy scaling law breaks down at very large scales, so identifying which component drives scaling and its limits is essential for understanding and guiding LLM training.

Method: Introduce a decomposition of cross-entropy into three parts (Error-Entropy, Self-Alignment, Confidence) and validate theoretically and empirically. Conduct extensive experiments on multiple datasets and 32 models spanning five orders of magnitude in size to measure the scaling behavior of each component.

Result: Only Error-Entropy follows a robust power-law scaling; Self-Alignment and Confidence remain largely invariant with model size. Error-Entropy dominates cross-entropy in small models but its share diminishes as models grow, explaining why the overall cross-entropy scaling law holds at small scales but fails at large scales.

Conclusion: Proposes the Error-Entropy scaling law as a more accurate description of model behavior, with broad implications for training, understanding, and future development of large language models.

Abstract: The cross-entropy scaling law has long served as a key tool for guiding the
development of large language models. It shows that cross-entropy loss
decreases in a predictable power-law rate as the model size increases. However,
recent evidence indicates that this law breaks down at very large scales: the
loss decreases more slowly than expected, which causes significant trouble for
developing large language models. In this paper, we hypothesize that the root
cause lies in the fact that cross-entropy itself does not truly scale; instead,
only one of its hidden components does. To investigate this, we introduce a
novel decomposition of cross-entropy into three parts: Error-Entropy,
Self-Alignment, and Confidence. We show both theoretically and empirically that
this decomposition precisely captures the training dynamics and optimization
objectives. Through extensive experiments on multiple datasets and 32 models
spanning five orders of magnitude in size, we find that only error-entropy
follows a robust power-law scaling, while the other two terms remain largely
invariant. Moreover, error-entropy constitutes the dominant share of
cross-entropy in small models but diminishes in proportion as models grow
larger. This explains why the cross-entropy scaling law appears accurate at
small scales but fails at very large ones. Our findings establish the
error-entropy scaling law as a more accurate description of model behavior. We
believe it will have wide applications in the training, understanding, and
future development of large language models.

</details>


### [337] [Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning](https://arxiv.org/abs/2510.04072)
*Ziyan Wang,Zheng Wang,Jie Fu,Xingwei Qu,Qi Cheng,Shengpu Tang,Minjia Zhang,Xiaoming Huo*

Main category: cs.LG

TL;DR: SFPO introduces a slow-fast three-stage policy optimization for on-policy RL in LLM reasoning, decomposing updates into fast inner steps, a repositioning step to curb off-policy drift, and a slow correction. It remains plug-compatible with policy-gradient pipelines and significantly improves stability, sample efficiency, and convergence speed compared to GRPO.


<details>
  <summary>Details</summary>
Motivation: On-policy methods like GRPO suffer from noisy gradients and unstable updates in early training due to low-quality rollouts, leading to inefficient exploration. There is a need for more stable, data-efficient RL training to enhance reasoning in LLMs.

Method: Each policy update is decomposed into three stages: (1) a short fast trajectory of inner steps on the same batch, (2) a reposition mechanism to control off-policy drift, and (3) a final slow correction. The reposition-before-update design preserves the objective and rollout process, making SFPO plug-compatible with existing policy-gradient pipelines.

Result: SFPO improves stability, reduces rollout counts, and accelerates convergence. It outperforms GRPO by up to 2.80 points in average on math reasoning benchmarks, and achieves up to 4.93× fewer rollouts and a 4.19× reduction in wall-clock time to reach GRPO's best accuracy.

Conclusion: SFPO offers a simple, effective, and plug-compatible framework for reasoning RL in LLMs, delivering substantial gains in stability and data efficiency over GRPO without altering the overarching objective or rollout process.

Abstract: Reinforcement learning (RL) has become central to enhancing reasoning in
large language models (LLMs). Yet on-policy algorithms such as Group Relative
Policy Optimization (GRPO) often suffer in early training: noisy gradients from
low-quality rollouts lead to unstable updates and inefficient exploration. We
introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient
framework to address these limitations via decomposing each step into three
stages: a short fast trajectory of inner steps on the same batch, a reposition
mechanism to control off-policy drift, and a final slow correction. This
reposition-before-update design preserves the objective and rollout process
unchanged, making SFPO plug-compatible with existing policy-gradient pipelines.
Extensive experiments demonstrate that SFPO consistently improves stability,
reduces rollouts, and accelerates convergence of reasoning RL training.
Specifically, it outperforms GRPO by up to 2.80 points in average on math
reasoning benchmarks. It also achieves up to 4.93\texttimes{} fewer rollouts
and a 4.19\texttimes{} reduction in wall-clock time to match GRPO's best
accuracy.

</details>


### [338] [Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees](https://arxiv.org/abs/2510.04088)
*Nan Jiang,Tengyang Xie*

Main category: cs.LG

TL;DR: A survey of offline reinforcement learning (RL) in large state spaces, outlining expressivity assumptions for function approximation (Bellman completeness vs realizability) and data coverage notions (all-policy vs single-policy). It maps the algorithmic landscape, presenting results and guarantees under varying assumptions, and discusses open questions and connections to related areas.


<details>
  <summary>Details</summary>
Motivation: Offline RL enables learning good policies from historical data without online interactions, which is crucial in large, costly, or unsafe environments. The paper motivates a formal study of which assumptions on function class expressivity and data coverage yield learnability and tractable guarantees.

Method: Defines formal notions of function approximation expressivity (Bellman completeness vs realizability) and data coverage (all-policy vs single-policy). Surveys a wide range of algorithms and theoretical results, organizing them by assumptions and the resulting sample and computational complexity guarantees, and discusses open questions and connections to adjacent areas.

Result: Provides a structured landscape (taxonomy) of offline RL in large state spaces, detailing when learning is possible under different expressivity and coverage assumptions, summarizing algorithmic families and their theoretical guarantees, and highlighting gaps and directions for future research.

Conclusion: Maps the theoretical landscape of offline RL in large state spaces, clarifying which assumptions enable learning, identifying remaining open questions, and outlining connections to related domains to guide future work.

Abstract: This article introduces the theory of offline reinforcement learning in large
state spaces, where good policies are learned from historical data without
online interactions with the environment. Key concepts introduced include
expressivity assumptions on function approximation (e.g., Bellman completeness
vs. realizability) and data coverage (e.g., all-policy vs. single-policy
coverage). A rich landscape of algorithms and results is described, depending
on the assumptions one is willing to make and the sample and computational
complexity guarantees one wishes to achieve. We also discuss open questions and
connections to adjacent areas.

</details>


### [339] [Using predefined vector systems as latent space configuration for neural network supervised training on data with arbitrarily large number of classes](https://arxiv.org/abs/2510.04090)
*Nikita Gabdullin*

Main category: cs.LG

TL;DR: Proposes a class-agnostic training approach by mapping outputs to a predefined latent space (LSC) using vectors from A_n root systems (randomly perturbed). Demonstrates viability on Cinic-10, ImageNet-1K, and a 1.28M-class dataset, enabling scalable training regardless of class count; discusses lifelong learning and distillation applications.


<details>
  <summary>Details</summary>
Motivation: Class-count dependent output layers constrain supervised learning models when the number of classes is very large or unknown; a method to train a fixed-architecture network for arbitrary or extremely large class sets without reconfiguring final layers.

Method: Define target latent space configurations (LSCs) as predefined vectors from root systems (An) that are randomly perturbed. Train encoders and Vision Transformers (ViT) to map inputs to these vectors, effectively matching predictions to predefined targets. Demonstrated on Cinic-10 and ImageNet-1K in both low- and high-dimensional settings, and extended to a dataset with 1.28 million classes.

Result: Successful training of encoders and ViT without relying on class-count-specific final layers; able to operate in both low- and high-dimensional spaces and scale to datasets with extremely large numbers of classes (1.28M). Indicated potential for lifelong learning and distillation.

Conclusion: The LSC-based training paradigm decouples model architecture from the number of target classes, enabling scalable learning for enormous or unknown label spaces and offering versatile applications in lifelong learning and neural network distillation.

Abstract: Supervised learning (SL) methods are indispensable for neural network (NN)
training used to perform classification tasks. While resulting in very high
accuracy, SL training often requires making NN parameter number dependent on
the number of classes, limiting their applicability when the number of classes
is extremely large or unknown in advance. In this paper we propose a
methodology that allows one to train the same NN architecture regardless of the
number of classes. This is achieved by using predefined vector systems as the
target latent space configuration (LSC) during NN training. We discuss the
desired properties of target configurations and choose randomly perturbed
vectors of An root system for our experiments. These vectors are used to
successfully train encoders and visual transformers (ViT) on Cinic-10 and
ImageNet-1K in low- and high-dimensional cases by matching NN predictions with
the predefined vectors. Finally, ViT is trained on a dataset with 1.28 million
classes illustrating the applicability of the method to training on datasets
with extremely large number of classes. In addition, potential applications of
LSC in lifelong learning and NN distillation are discussed illustrating
versatility of the proposed methodology.

</details>


### [340] [Rethinking Consistent Multi-Label Classification under Inexact Supervision](https://arxiv.org/abs/2510.04091)
*Wei Wang,Tianhao Ma,Ming-Kun Xie,Gang Niu,Masashi Sugiyama*

Main category: cs.LG

TL;DR: The paper proposes two unbiased risk estimators for partial multi-label learning (PML) and complementary multi-label learning (CML) that avoid estimating label-generation processes or assuming uniform distributions; it establishes statistical consistency for standard MLC metrics, derives convergence rates, and demonstrates strong empirical performance against state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: High annotation costs and the unrealistic assumptions of existing approaches (accurate generation process estimation or uniformity) motivate a unified, assumption-light framework for PML and CML.

Method: Introduce two unbiased risk estimators based on first- and second-order strategies; prove consistency with two widely used MLC evaluation metrics and derive convergence rates for estimation errors; validate theoretically and empirically.

Result: Theoretical guarantees of consistency for the chosen metrics and convergence rates; extensive experiments show the proposed methods are effective against state-of-the-art baselines.

Conclusion: A unified, practical framework for PML and CML that does not rely on accurate generation-process modeling or uniform-distribution assumptions, supported by solid theory and empirical results.

Abstract: Partial multi-label learning and complementary multi-label learning are two
popular weakly supervised multi-label classification paradigms that aim to
alleviate the high annotation costs of collecting precisely annotated
multi-label data. In partial multi-label learning, each instance is annotated
with a candidate label set, among which only some labels are relevant; in
complementary multi-label learning, each instance is annotated with
complementary labels indicating the classes to which the instance does not
belong. Existing consistent approaches for the two paradigms either require
accurate estimation of the generation process of candidate or complementary
labels or assume a uniform distribution to eliminate the estimation problem.
However, both conditions are usually difficult to satisfy in real-world
scenarios. In this paper, we propose consistent approaches that do not rely on
the aforementioned conditions to handle both problems in a unified way.
Specifically, we propose two unbiased risk estimators based on first- and
second-order strategies. Theoretically, we prove consistency w.r.t. two widely
used multi-label classification evaluation metrics and derive convergence rates
for the estimation errors of the proposed risk estimators. Empirically,
extensive experimental results validate the effectiveness of our proposed
approaches against state-of-the-art methods.

</details>


### [341] [Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws](https://arxiv.org/abs/2510.04102)
*Ramzi Dakhmouche,Hossein Gorji*

Main category: cs.LG

TL;DR: Foundation models boost short-horizon time-series forecasting but struggle with long-range extrapolation; the paper formalizes a fundamental property governing extrapolation, analyzes it theoretically and empirically, and discusses design directions for better long-horizon forecasting.


<details>
  <summary>Details</summary>
Motivation: To understand why deep learning models exhibit poor extrapolation compared to physical laws, and to identify a core property that governs extrapolation capacity in statistical learning models.

Method: Theoretical analysis to identify and formalize a fundamental extrapolation property, complemented by empirical experiments assessing current deep learning architectures on extrapolation tasks.

Result: The identified property explains the extrapolation gap between training and test distributions; empirical results illustrate how existing architectures fail to leverage extrapolative regularities, guiding where improvements are needed.

Conclusion: Clarifies root causes of extrapolation failure in DL-based forecasting and outlines directions for designing next-generation models capable of mastering long-range extrapolation.

Abstract: Motivated by the remarkable success of Foundation Models (FMs) in language
modeling, there has been growing interest in developing FMs for time series
prediction, given the transformative power such models hold for science and
engineering. This culminated in significant success of FMs in short-range
forecasting settings. However, extrapolation or long-range forecasting remains
elusive for FMs, which struggle to outperform even simple baselines. This
contrasts with physical laws which have strong extrapolation properties, and
raises the question of the fundamental difference between the structure of
neural networks and physical laws. In this work, we identify and formalize a
fundamental property characterizing the ability of statistical learning models
to predict more accurately outside of their training domain, hence explaining
performance deterioration for deep learning models in extrapolation settings.
In addition to a theoretical analysis, we present empirical results showcasing
the implications of this property on current deep learning architectures. Our
results not only clarify the root causes of the extrapolation gap but also
suggest directions for designing next-generation forecasting models capable of
mastering extrapolation.

</details>


### [342] [Can Linear Probes Measure LLM Uncertainty?](https://arxiv.org/abs/2510.04108)
*Ramzi Dakhmouche,Adrien Letellier,Hossein Gorji*

Main category: cs.LG

TL;DR: Bayesian linear ensembles improve uncertainty quantification for LLMs in multi-choice tasks, beating softmax baselines.


<details>
  <summary>Details</summary>
Motivation: UQ is critical for reliable deployment of LLMs; the common baseline (maximum softmax score) is naive. A principled Bayesian approach with simple models can yield better uncertainty estimates.

Method: Train multiple Bayesian linear models that predict the output of a layer from the previous layer's output. Use the layer-level posteriors to infer a global uncertainty by identifying a sparse combination of distributional features, resulting in an efficient UQ scheme.

Result: Numerical experiments on various LLMs show consistent improvement over state-of-the-art baselines.

Conclusion: A principled Bayesian approach with simple linear models can improve uncertainty quantification in LLMs for multi-choice tasks, outperforming standard softmax-based baselines.

Abstract: Effective Uncertainty Quantification (UQ) represents a key aspect for
reliable deployment of Large Language Models (LLMs) in automated
decision-making and beyond. Yet, for LLM generation with multiple choice
structure, the state-of-the-art in UQ is still dominated by the naive baseline
given by the maximum softmax score. To address this shortcoming, we demonstrate
that taking a principled approach via Bayesian statistics leads to improved
performance despite leveraging the simplest possible model, namely linear
regression. More precisely, we propose to train multiple Bayesian linear
models, each predicting the output of a layer given the output of the previous
one. Based on the obtained layer-level posterior distributions, we infer the
global uncertainty level of the LLM by identifying a sparse combination of
distributional features, leading to an efficient UQ scheme. Numerical
experiments on various LLMs show consistent improvement over state-of-the-art
baselines.

</details>


### [343] [Wasserstein projection distance for fairness testing of regression models](https://arxiv.org/abs/2510.04114)
*Wanxin Li,Yongjin P. Park,Khanh Dao Duc*

Main category: cs.LG

TL;DR: A Wasserstein projection-based framework for fairness testing in regression, introducing hypothesis testing and an optimal data perturbation method to balance fairness and accuracy; backed by theory and experiments showing higher specificity than permutation tests and effective bias mitigation in real tasks like student performance and housing price prediction.


<details>
  <summary>Details</summary>
Motivation: Fairness research has largely centered on classification; regression fairness is underexplored with few principled testing methods. A distribution-aware approach using Wasserstein distance offers a rigorous way to formalize and test fairness criteria in regression, enabling detection and mitigation of biases while preserving accuracy.

Method: Propose a Wasserstein projection-based framework for regression fairness testing focused on expectation-based criteria. Develop a hypothesis-testing approach and an optimal data perturbation strategy to improve fairness while balancing accuracy. Provide a dual reformulation of the Wasserstein projection test statistic and derive asymptotic bounds and limiting distributions. Theoretical contribution includes a detailed categorization of fairness criteria for regression.

Result: Experiments on synthetic and real-world data show higher specificity than permutation-based tests and effective detection and mitigation of biases. Demonstrates applicability to real tasks such as student performance and housing price prediction, with the method balancing fairness improvements against accuracy losses via perturbations.

Conclusion: Extends fairness testing to regression with a solid theoretical foundation (duality, asymptotics) and practical effectiveness, enabling more reliable bias detection and mitigation in regression models; opens avenues for broader fairness criteria and scalable applications.

Abstract: Fairness in machine learning is a critical concern, yet most research has
focused on classification tasks, leaving regression models underexplored. This
paper introduces a Wasserstein projection-based framework for fairness testing
in regression models, focusing on expectation-based criteria. We propose a
hypothesis-testing approach and an optimal data perturbation method to improve
fairness while balancing accuracy. Theoretical results include a detailed
categorization of fairness criteria for regression, a dual reformulation of the
Wasserstein projection test statistic, and the derivation of asymptotic bounds
and limiting distributions. Experiments on synthetic and real-world datasets
demonstrate that the proposed method offers higher specificity compared to
permutation-based tests, and effectively detects and mitigates biases in real
applications such as student performance and housing price prediction.

</details>


### [344] [On the Statistical Query Complexity of Learning Semiautomata: a Random Walk Approach](https://arxiv.org/abs/2510.04115)
*George Giapitzakis,Kimon Fountoulakis,Eshaan Nichani,Jason D. Lee*

Main category: cs.LG

TL;DR: First Statistical Query hardness result for semiautomata under uniform input/initial-state distribution; hardness stems from internal state-transition structure; uses a random walk on S_N × S_N with Fourier/representation theory to obtain spectral gap bounds, showing polynomial mixing yields near-uncorrelated automata.


<details>
  <summary>Details</summary>
Motivation: Extend Statistical Query hardness to semiautomata, a broad class of sequence-processing models with wide applications (NLP, robotics, biology, data mining). Understanding learning/testing limitations independent of language acceptance.

Method: Model the task of distinguishing final states of two semiautomata as a random walk on the group S_N × S_N. Apply Fourier analysis and the representation theory of the symmetric group to derive tight spectral gap bounds, establishing polynomial mixing and near-uncorrelated final states after polynomial steps.

Result: The paper establishes the first SQ hardness result for semiautomata under the uniform distribution over input words and initial states, valid when alphabet size and input length are polynomial in the number of states; hardness arises from internal transition structure rather than the language recognized.

Conclusion: Demonstrates that SQ hardness can stem from automata's internal dynamics, not just the language itself. The approach combines random-walk analysis on permutation groups with Fourier tools to yield hardness via spectral gap bounds, highlighting a route to proving learning/testing limits for complex sequence-processing models.

Abstract: Semiautomata form a rich class of sequence-processing algorithms with
applications in natural language processing, robotics, computational biology,
and data mining. We establish the first Statistical Query hardness result for
semiautomata under the uniform distribution over input words and initial
states. We show that Statistical Query hardness can be established when both
the alphabet size and input length are polynomial in the number of states.
Unlike the case of deterministic finite automata, where hardness typically
arises through the hardness of the language they recognize (e.g., parity), our
result is derived solely from the internal state-transition structure of
semiautomata. Our analysis reduces the task of distinguishing the final states
of two semiautomata to studying the behavior of a random walk on the group
$S_{N} \times S_{N}$. By applying tools from Fourier analysis and the
representation theory of the symmetric group, we obtain tight spectral gap
bounds, demonstrating that after a polynomial number of steps in the number of
states, distinct semiautomata become nearly uncorrelated, yielding the desired
hardness result.

</details>


### [345] [Attending on Multilevel Structure of Proteins enables Accurate Prediction of Cold-Start Drug-Target Interactions](https://arxiv.org/abs/2510.04126)
*Ziying Zhang,Yaqing Wang,Yuxuan Sun,Min Ye,Quanming Yao*

Main category: cs.LG

TL;DR: ColdDTI introduces a hierarchical attention framework that models interactions between drug structures and multi-level protein structures (primary to quaternary) for cold-start DTI prediction, fusing information from local and global granularities to improve generalization.


<details>
  <summary>Details</summary>
Motivation: Existing cold-start DTI work largely uses primary protein structures, ignoring higher-level protein architecture (secondary, tertiary, quaternary) that influence interactions. Proteomic insights suggest leveraging multi-level structures could yield transferable biophysical priors and reduce overfitting.

Method: ColdDTI employs a hierarchical attention mechanism to explore interactions between drug structures and multi-level protein representations (primary to quaternary) at both local (e.g., substructures) and global (whole-molecule) granularities. It then fuses these interactions across levels to produce final predictions, embedding transferable priors to prevent overfitting.

Result: On benchmark datasets, ColdDTI consistently outperforms previous methods in cold-start settings, demonstrating improved predictive accuracy when predicting interactions for novel drugs or proteins.

Conclusion: Incorporating multi-level protein structure through hierarchical attention yields biologically grounded priors and better cold-start DTI performance, offering a robust framework for integrating complex protein architectures into drug–target interaction prediction.

Abstract: Cold-start drug-target interaction (DTI) prediction focuses on interaction
between novel drugs and proteins. Previous methods typically learn transferable
interaction patterns between structures of drug and proteins to tackle it.
However, insight from proteomics suggest that protein have multi-level
structures and they all influence the DTI. Existing works usually represent
protein with only primary structures, limiting their ability to capture
interactions involving higher-level structures. Inspired by this insight, we
propose ColdDTI, a framework attending on protein multi-level structure for
cold-start DTI prediction. We employ hierarchical attention mechanism to mine
interaction between multi-level protein structures (from primary to quaternary)
and drug structures at both local and global granularities. Then, we leverage
mined interactions to fuse structure representations of different levels for
final prediction. Our design captures biologically transferable priors,
avoiding the risk of overfitting caused by excessive reliance on representation
learning. Experiments on benchmark datasets demonstrate that ColdDTI
consistently outperforms previous methods in cold-start settings.

</details>


### [346] [On the Limitations and Capabilities of Position Embeddings for Length Generalization](https://arxiv.org/abs/2510.04130)
*Yang Chen,Yitao Liang,Zhouchen Lin*

Main category: cs.LG

TL;DR: PEs influence length generalization not by adding computation but by structuring computations across positions. The paper introduces Linear Representation Complexity (LRC) for POLAs and Sequential Representation Complexity (SRC) for full Transformers, hypothesizing LG iff SRC is invariant across scales; it provides Scale Hint and a Learning-Based Position Embedding framework, with empirical support across reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Clarify the fundamental role of position embeddings in length generalization for Transformer models, addressing a theoretical gap and offering practical guidance for improving LG.

Method: Theoretical analysis of position embeddings in Position-Only Linear Attentions (POLAs) introducing Linear Representation Complexity (LRC); extend the framework to practical Transformers via Sequential Representation Complexity (SRC); hypothesis testing with empirical experiments across reasoning tasks; propose Scale Hint and a Learning-Based Position Embedding (LPE) framework to learn positional relations.

Result: PEs do not enlarge computational capabilities but structure how computations are distributed across positions. LG may occur if and only if SRC remains invariant across scales; Scale Hint and the Learning-Based PEs improve LG, supported by empirical evidence across various reasoning tasks.

Conclusion: Provides theoretical insights into how position embeddings influence LG and offers practical strategies (Scale Hint, learnable PEs) to enhance LG in Transformers, emphasizing scale-invariant representation complexity as a key condition.

Abstract: In Transformers, Position Embeddings (PEs) significantly influence Length
Generalization (LG) performance, yet their fundamental role remains unclear. In
this work, we investigate the limitations and capabilities of PEs in achieving
LG. We theoretically analyze PEs in Position-Only Linear Attentions (POLAs),
introducing Linear Representation Complexity (LRC) to characterize when PEs
enable LG. Our analysis shows that PEs do not expand computational capabilities
but structure learned computations across positions. Extending to practical
Transformers, we propose Sequential Representation Complexity (SRC) and
conjecture that LG is possible if and only if SRC remains invariant across
scales. We support this hypothesis with empirical evidence in various reasoning
tasks. To enhance LG, we introduce Scale Hint, allowing flexible instance
scaling, and a Learning-Based Position Embedding framework that automatically
learns positional relations. Our work provides theoretical insights and
practical strategies for improving LG in Transformers.

</details>


### [347] [Modeling Time Series Dynamics with Fourier Ordinary Differential Equations](https://arxiv.org/abs/2510.04133)
*Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: Fourier ODEs (FODEs) model time series in the frequency domain via FFT, plus a learnable element-wise filter to align continuous outputs with discrete data, achieving better long- and short-term pattern capture and efficiency.


<details>
  <summary>Details</summary>
Motivation: Neural ODEs rely on time-domain representations that struggle with long-term dependencies and periodic structures, and their continuous-time formulation mismatches discrete observations, reducing granularity and accuracy.

Method: Transform input to Fourier domain using FFT, learn Fourier-domain dynamics (ODEs in frequency), and apply a learnable element-wise filter to reconcile with discrete observations; this yields a hybrid continuous-discrete approach with global frequency modeling.

Result: Empirical experiments on multiple time-series datasets show FODEs outperform baselines in accuracy and efficiency, effectively capturing both long- and short-term patterns.

Conclusion: Embedding dynamics in the frequency domain with a tunable alignment mechanism provides a robust framework for time-series modeling, combining global periodic structure with preserved granularity.

Abstract: Neural ODEs (NODEs) have emerged as powerful tools for modeling time series
data, offering the flexibility to adapt to varying input scales and capture
complex dynamics. However, they face significant challenges: first, their
reliance on time-domain representations often limits their ability to capture
long-term dependencies and periodic structures; second, the inherent mismatch
between their continuous-time formulation and the discrete nature of real-world
data can lead to loss of granularity and predictive accuracy. To address these
limitations, we propose Fourier Ordinary Differential Equations (FODEs), an
approach that embeds the dynamics in the Fourier domain. By transforming
time-series data into the frequency domain using the Fast Fourier Transform
(FFT), FODEs uncover global patterns and periodic behaviors that remain elusive
in the time domain. Additionally, we introduce a learnable element-wise
filtering mechanism that aligns continuous model outputs with discrete
observations, preserving granularity and enhancing accuracy. Experiments on
various time series datasets demonstrate that FODEs outperform existing methods
in terms of both accuracy and efficiency. By effectively capturing both long-
and short-term patterns, FODEs provide a robust framework for modeling time
series dynamics.

</details>


### [348] [PhaseFormer: From Patches to Phases for Efficient and Effective Time Series Forecasting](https://arxiv.org/abs/2510.04134)
*Yiming Niu,Jinliang Deng,Yongxin Tong*

Main category: cs.LG

TL;DR: PhaseFormer reframes periodic time-series forecasting by modeling phase-wise rather than patch-level tokens, achieving state-of-the-art accuracy with ~1k parameters through compact phase embeddings and a lightweight cross-phase routing mechanism.


<details>
  <summary>Details</summary>
Motivation: Patch-based tokenization, while effective, incurs large parameter counts and heavy computation. A phase-centric view can capture periodic structure more efficiently, addressing scalability and efficiency gaps in time-series forecasting.

Method: PhaseFormer uses phase-wise prediction with compact phase embeddings and a lightweight routing mechanism to enable efficient cross-phase interactions, reducing parameter count while preserving predictive power.

Result: Across benchmark datasets, PhaseFormer achieves state-of-the-art performance with around 1k parameters, excelling particularly on large-scale and complex datasets where comparable-efficient models struggle.

Conclusion: Shifts the design paradigm from patch-level to phase-based modeling for time-series forecasting, delivering a highly efficient and effective solution and advancing practical deployment of forecasting models.

Abstract: Periodicity is a fundamental characteristic of time series data and has long
played a central role in forecasting. Recent deep learning methods strengthen
the exploitation of periodicity by treating patches as basic tokens, thereby
improving predictive effectiveness. However, their efficiency remains a
bottleneck due to large parameter counts and heavy computational costs. This
paper provides, for the first time, a clear explanation of why patch-level
processing is inherently inefficient, supported by strong evidence from
real-world data. To address these limitations, we introduce a phase perspective
for modeling periodicity and present an efficient yet effective solution,
PhaseFormer. PhaseFormer features phase-wise prediction through compact phase
embeddings and efficient cross-phase interaction enabled by a lightweight
routing mechanism. Extensive experiments demonstrate that PhaseFormer achieves
state-of-the-art performance with around 1k parameters, consistently across
benchmark datasets. Notably, it excels on large-scale and complex datasets,
where models with comparable efficiency often struggle. This work marks a
significant step toward truly efficient and effective time series forecasting.
Code is available at this repository:
https://github.com/neumyor/PhaseFormer_TSL

</details>


### [349] [Efficient Manifold-Constrained Neural ODE for High-Dimensional Datasets](https://arxiv.org/abs/2510.04138)
*Muhao Guo,Haoran Li,Yang Weng*

Main category: cs.LG

TL;DR: A manifold-aware NODE framework that uses a structure-preserving encoder to learn a graph-approximation of the data manifold and integrate it with Neural ODEs, achieving faster training/inference and higher accuracy on high-dimensional data.


<details>
  <summary>Details</summary>
Motivation: High-dimensional NODEs are computationally expensive and suffer truncation errors. Real data lie on low-dimensional manifolds; knowledge of the manifold is often unavailable. Learning and enforcing a manifold constraint can improve efficiency and accuracy.

Method: Introduce a structure-preserved encoder that processes data and uncovers an underlying graph approximating the manifold. Propose methods to couple NODE learning with this manifold representation, enabling NODE to evolve dynamics constrained on the learned graph.

Result: Empirical evaluation on multiple datasets shows improved accuracy, fewer function evaluations (NFEs), and faster convergence compared with baseline NODE methods.

Conclusion: Manifold-aware NODE with a structure-preserving encoder offers meaningful computational gains and accuracy improvements for high-dimensional dynamics, validating the benefit of learning the data manifold jointly with continuous-depth models.

Abstract: Neural ordinary differential equations (NODE) have garnered significant
attention for their design of continuous-depth neural networks and the ability
to learn data/feature dynamics. However, for high-dimensional systems,
estimating dynamics requires extensive calculations and suffers from high
truncation errors for the ODE solvers. To address the issue, one intuitive
approach is to consider the non-trivial topological space of the data
distribution, i.e., a low-dimensional manifold. Existing methods often rely on
knowledge of the manifold for projection or implicit transformation,
restricting the ODE solutions on the manifold. Nevertheless, such knowledge is
usually unknown in realistic scenarios. Therefore, we propose a novel approach
to explore the underlying manifold to restrict the ODE process. Specifically,
we employ a structure-preserved encoder to process data and find the underlying
graph to approximate the manifold. Moreover, we propose novel methods to
combine the NODE learning with the manifold, resulting in significant gains in
computational speed and accuracy. Our experimental evaluations encompass
multiple datasets, where we compare the accuracy, number of function
evaluations (NFEs), and convergence speed of our model against existing
baselines. Our results demonstrate superior performance, underscoring the
effectiveness of our approach in addressing the challenges of high-dimensional
datasets.

</details>


### [350] [Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models](https://arxiv.org/abs/2510.04146)
*Minseo Kim,Coleman Hooper,Aditya Tomar,Chenfeng Xu,Mehrdad Farajtabar,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: The abstract compares autoregressive language models (ARMs) and diffusion language models (DLMs), showing DLMs have higher arithmetic intensity but struggle with long contexts, while ARMs excel in batched throughput. It proposes block-wise decoding for DLMs to improve scalability and highlights reducing sampling steps as a path to faster open-source DLMs.


<details>
  <summary>Details</summary>
Motivation: To understand the performance trade-offs between ARMs and DLMs across context length, batch settings, and inference latency, with a focus on scalability and practical deployment.

Method: The study combines theoretical analysis with profiling data to evaluate ARM vs. DLM performance. It includes a proposed block-wise decoding scheme for DLMs to increase arithmetic intensity and scalability, and experiments on batched inference and inference acceleration strategies (e.g., reducing sampling steps).

Result: DLMs exhibit higher arithmetic intensity in some regimes due to parallelism but do not scale well to long contexts. Block-wise decoding can increase arithmetic intensity and maintain long-context scalability. In batched inference, ARMs achieve higher throughput by exploiting cross-sequence parallelism. Reducing sampling steps is key to accelerating open-source DLMs and achieving better latency.

Conclusion: There are clear trade-offs: ARMs may offer superior throughput in batching, while DLMs offer higher parallelism and potential latency gains with strategies like block-wise decoding and fewer sampling steps. Effective deployment will depend on context length, batching patterns, and optimization focus (latency vs. throughput).

Abstract: Large Language Models (LLMs) have achieved state-of-the-art performance on a
broad range of Natural Language Processing (NLP) tasks, including document
processing and coding. Autoregressive Language Models (ARMs), which generate
tokens sequentially conditioned on all previous tokens, have been the
predominant paradigm for LLMs. However, while these networks have achieved high
accuracy across a range of downstream tasks, they exhibit low arithmetic
intensity due to the inherent sequential dependency with next-token prediction.
Recently, Diffusion Language Models (DLMs) have emerged as a promising
alternative architecture. DLMs generate output text in parallel, breaking the
limitations of sequential dependency. However, the performance implications of
DLMs relative to commonly deployed ARMs are not fully understood. In this work,
we present a comprehensive performance study analyzing the performance
characteristics of ARMs and DLMs, using both theoretical analysis and profiling
data to characterize the trade-offs between these approaches. We illustrate
that although DLMs exhibit higher arithmetic intensity compared to ARMs because
of their capability to utilize parallelism across sequence lengths, they fail
to scale effectively to longer contexts. We then explore DLMs with block-wise
decoding, outlining how this approach allows for increased arithmetic
intensity, while still scaling well to long contexts (similar to ARMs). We also
show interesting trade-offs for batched inference, where we find that ARMs
exhibit superior throughput, as they benefit more from parallelism across
sequences in the batch. Finally, we highlight opportunities for accelerating
DLM inference, and, in particular, highlight the importance of reducing the
number of sampling steps for allowing open-source DLMs to provide improved
latency relative to ARMs.

</details>


### [351] [Finite Time Analysis of Constrained Natural Critic-Actor Algorithm with Improved Sample Complexity](https://arxiv.org/abs/2510.04189)
*Prashansa Panda,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: First natural critic-actor algorithm with function approximation for long-run average cost under inequality constraints, with non-asymptotic convergence guarantees, optimal learning rates, sample-complexity improvements, and competitive safety-gym results.


<details>
  <summary>Details</summary>
Motivation: Extend non-asymptotic analysis to actor-critic with function approximation in long-run average cost setting and constrained policies, addressing sample efficiency and practical applicability.

Method: Propose a natural critic-actor algorithm with function approximation; derive non-asymptotic convergence guarantees; establish optimal learning rates; introduce a modification to improve sample complexity; validate via experiments on Safety-Gym.

Result: Established non-asymptotic convergence guarantees; identified optimal learning rates; proposed modification improves sample complexity; empirical results show competitive performance against established baselines on three Safety-Gym environments.

Conclusion: This work extends actor-critic with function approximation to long-run average cost and inequality constraints with non-asymptotic guarantees, and demonstrates practical effectiveness through empirical results.

Abstract: Recent studies have increasingly focused on non-asymptotic convergence
analyses for actor-critic (AC) algorithms. One such effort introduced a
two-timescale critic-actor algorithm for the discounted cost setting using a
tabular representation, where the usual roles of the actor and critic are
reversed. However, only asymptotic convergence was established there.
Subsequently, both asymptotic and non-asymptotic analyses of the critic-actor
algorithm with linear function approximation were conducted. In our work, we
introduce the first natural critic-actor algorithm with function approximation
for the long-run average cost setting and under inequality constraints. We
provide the non-asymptotic convergence guarantees for this algorithm. Our
analysis establishes optimal learning rates and we also propose a modification
to enhance sample complexity. We further show the results of experiments on
three different Safety-Gym environments where our algorithm is found to be
competitive in comparison with other well known algorithms.

</details>


### [352] [Spectral Alignment as Predictor of Loss Explosion in Neural Network Training](https://arxiv.org/abs/2510.04202)
*Haiquan Qiu,You Wu,Yingjie Tan,Yaqing Wang,Quanming Yao*

Main category: cs.LG

TL;DR: Spectral Alignment (SA) offers an early, theory-grounded indicator of training instability by tracking how layer inputs align with the top singular vectors of weights; it detects representational collapse earlier and with less variability than weight/gradient norms.


<details>
  <summary>Details</summary>
Motivation: Current monitoring metrics (weight/gradient norms) are highly variable across models and layers, often lagging before failures occur; a unified, low-overhead early warning signal for loss explosions is needed.

Method: Define SA as a distributional alignment between layer inputs and the principal singular vectors of weight matrices; monitor the sign diversity of these alignments; a collapse in sign diversity predicts representational collapse and training divergence; the metric is theoretically grounded.

Result: Empirical results on language models show SA provides earlier and clearer warnings of loss explosions than traditional scalar metrics, with low computational overhead.

Conclusion: SA is a practical, theoretically justified tool for safeguarding training, offering earlier, clearer warnings of instability across models.

Abstract: Loss explosions in training deep neural networks can nullify multi-million
dollar training runs. Conventional monitoring metrics like weight and gradient
norms are often lagging and ambiguous predictors, as their values vary
dramatically across different models and even between layers of the same model,
making it difficult to establish a unified standard for detecting impending
failure. We introduce Spectral Alignment (SA), a novel, theoretically-grounded
metric that monitors the distributional alignment between layer inputs and the
principal singular vectors of weight matrices. We show that a collapse in the
sign diversity of this alignment is a powerful early predictor of
representational collapse and training divergence. Empirical results on
language models demonstrate that monitoring the SA distribution provides a
significantly earlier and clearer warning of loss explosions than traditional
scalar metrics. SA's low computational overhead makes it a practical tool for
safeguarding model training.

</details>


### [353] [Adaptive Federated Learning via Dynamical System Model](https://arxiv.org/abs/2510.04203)
*Aayushya Agarwal,Larry Pileggi,Gauri Joshi*

Main category: cs.LG

TL;DR: An end-to-end adaptive federated learning framework that jointly tunes client and server learning rates and momentum within a dynamical-system view, yielding fast, robust convergence in heterogeneous, non-IID FL with minimal hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: Hyperparameter tuning is expensive and scales poorly with the number of clients; heterogeneity and non-IID data cause instability and drift; there is a need for automatic, scalable adaptive methods.

Method: Model FL as a dynamical system; adapt local learning rates and momentum for both clients and central aggregator; interpret momentum as critical damping for fast convergence; use a single global hyperparameter to control adaptive rates; ensure integrated client updates and central aggregation to mitigate objective inconsistency and client drift.

Result: Outperforms state-of-the-art adaptive methods in heterogeneous FL, delivering faster convergence and robustness; learning rates for both sides adapt automatically and are insensitive to the global hyperparameter, enabling rapid prototyping and scalable deployment.

Conclusion: The proposed adaptive momentum-based FL framework achieves fast, stable convergence in heterogeneous settings while eliminating the need for manual hyperparameter tuning for client and server updates, supporting scalable deployment.

Abstract: Hyperparameter selection is critical for stable and efficient convergence of
heterogeneous federated learning, where clients differ in computational
capabilities, and data distributions are non-IID. Tuning hyperparameters is a
manual and computationally expensive process as the hyperparameter space grows
combinatorially with the number of clients. To address this, we introduce an
end-to-end adaptive federated learning method in which both clients and central
agents adaptively select their local learning rates and momentum parameters.
Our approach models federated learning as a dynamical system, allowing us to
draw on principles from numerical simulation and physical design. Through this
perspective, selecting momentum parameters equates to critically damping the
system for fast, stable convergence, while learning rates for clients and
central servers are adaptively selected to satisfy accuracy properties from
numerical simulation. The result is an adaptive, momentum-based federated
learning algorithm in which the learning rates for clients and servers are
dynamically adjusted and controlled by a single, global hyperparameter. By
designing a fully integrated solution for both adaptive client updates and
central agent aggregation, our method is capable of handling key challenges of
heterogeneous federated learning, including objective inconsistency and client
drift. Importantly, our approach achieves fast convergence while being
insensitive to the choice of the global hyperparameter, making it well-suited
for rapid prototyping and scalable deployment. Compared to state-of-the-art
adaptive methods, our framework is shown to deliver superior convergence for
heterogeneous federated learning while eliminating the need for hyperparameter
tuning both client and server updates.

</details>


### [354] [PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression](https://arxiv.org/abs/2510.04205)
*Di Zhang*

Main category: cs.LG

TL;DR: PolyKAN provides provably optimal, error-bounded compression for Kolmogorov-Arnold Networks by treating their piecewise polynomial structure as an optimal polyhedral-region merging problem; it introduces a complete theory of epsilon-equivalent compression and a dynamic-programming algorithm with polynomial-time complexity, establishing the first formal guarantees for KAN compression.


<details>
  <summary>Details</summary>
Motivation: KANs offer interpretability but suffer from poor parameter efficiency, hindering practical deployment. A formal compression framework with guarantees is needed to reduce model size without sacrificing approximation accuracy, enabling scalable, interpretable networks.

Method: Exploit the piecewise polynomial structure of KANs to pose compression as optimal polyhedral-region merging. Provide a rigorous polyhedral characterization of KANs, develop a complete theory of epsilon-equivalent compression, and design an optimal dynamic programming algorithm that achieves minimal compression under specified error bounds, with polynomial-time complexity in all network parameters.

Result: PolyKAN guarantees provably minimal compression under a given error tolerance, with strict control of approximation error and polynomial-time complexity in network size. It establishes the first formal foundation for KAN compression with mathematical guarantees.

Conclusion: PolyKAN delivers a formal, efficient compression framework for KANs, enabling practical deployment of interpretable neural architectures while maintaining rigorous error bounds; it opens new directions for theory-driven compression of piecewise-structured networks.

Abstract: Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to
traditional Multi-Layer Perceptrons (MLPs), offering enhanced interpretability
and a strong mathematical foundation. However, their parameter efficiency
remains a significant challenge for practical deployment. This paper introduces
PolyKAN, a novel theoretical framework for KAN compression that provides formal
guarantees on both model size reduction and approximation error. By leveraging
the inherent piecewise polynomial structure of KANs, we formulate the
compression problem as one of optimal polyhedral region merging. We establish a
rigorous polyhedral characterization of KANs, develop a complete theory of
$\epsilon$-equivalent compression, and design an optimal dynamic programming
algorithm that guarantees minimal compression under specified error bounds. Our
theoretical analysis demonstrates that PolyKAN achieves provably minimal
compression while maintaining strict error control, with polynomial-time
complexity in all network parameters. The framework provides the first formal
foundation for KAN compression with mathematical guarantees, opening new
directions for efficient deployment of interpretable neural architectures.

</details>


### [355] [Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention](https://arxiv.org/abs/2510.04212)
*Haiquan Qiu,Quanming Yao*

Main category: cs.LG

TL;DR: Low-precision training with flash attention causes catastrophic loss explosions due to two intertwined factors: emergent similar low-rank attention representations and biased rounding errors; a minimal fix to rounding bias stabilizes training.


<details>
  <summary>Details</summary>
Motivation: Efficient transformer training via low-precision formats is attractive but plagued by instability; need mechanistic understanding and remedies.

Method: Perform in-depth analysis of training dynamics under low-precision flash attention; identify two interacting mechanisms (low-rank attention representations and biased rounding). Demonstrate their role in error accumulation; propose a minimal modification to flash attention to remove rounding bias; validate on training stability.

Result: The minimal modification stabilizes training and stops loss explosions, supporting the proposed mechanism and offering a practical remedy.

Conclusion: Bias in rounding and low-rank similarity in attention under low precision drive instability; mitigating rounding bias yields stable training, advancing understanding and practice of low-precision transformer training.

Abstract: The pursuit of computational efficiency has driven the adoption of
low-precision formats for training transformer models. However, this progress
is often hindered by notorious training instabilities. This paper provides the
first mechanistic explanation for a long-standing and unresolved failure case
where training with flash attention in low-precision settings leads to
catastrophic loss explosions. Our in-depth analysis reveals that the failure is
not a random artifact but caused by two intertwined phenomena: the emergence of
similar low-rank representations within the attention mechanism and the
compounding effect of biased rounding errors inherent in low-precision
arithmetic. We demonstrate how these factors create a vicious cycle of error
accumulation that corrupts weight updates, ultimately derailing the training
dynamics. To validate our findings, we introduce a minimal modification to the
flash attention that mitigates the bias in rounding errors. This simple change
stabilizes the training process, confirming our analysis and offering a
practical solution to this persistent problem.

</details>


### [356] [MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering](https://arxiv.org/abs/2510.04217)
*Chenlu Ding,Jiancan Wu,Leheng Sheng,Fan Zhang,Yancheng Yuan,Xiang Wang,Xiangnan He*

Main category: cs.LG

TL;DR: MLLMEraser is a training-free, test-time, input-aware unlearning method for multimodal LLMs that uses activation steering to erase designated content without updating parameters, outperforming baselines in forgetting while preserving utility and reducing cost.


<details>
  <summary>Details</summary>
Motivation: As multimodal LLMs store private data, outdated knowledge, and may generate harmful content, there is a need for efficient, reversible unlearning. Existing training-based approaches are expensive and can distort retained knowledge.

Method: Build a multimodal erasure direction by contrasting adversarially perturbed knowledge-recall image-text pairs with their knowledge-erasure counterparts to capture textual and visual discrepancies; employ input-aware steering to apply the erasure direction only when and how it helps forgetting without harming retained knowledge.

Result: Empirical results on LLaVA-1.5 and Qwen-2.5-VL show MLLMEraser achieves stronger forgetting than state-of-the-art unlearning baselines with lower computational cost and minimal utility degradation.

Conclusion: A training-free, test-time, input-aware forgetting mechanism is effective for MLLMs, enabling selective memory erasure without retraining and with scalable efficiency.

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities across vision-language tasks, yet their large-scale deployment
raises pressing concerns about memorized private data, outdated knowledge, and
harmful content. Existing unlearning approaches for MLLMs typically adapt
training-based strategies such as gradient ascent or preference optimization,
but these methods are computationally expensive, irreversible, and often
distort retained knowledge. In this work, we propose MLLMEraser, an
input-aware, training-free framework for test-time unlearning. Our approach
leverages activation steering to enable dynamic knowledge erasure without
parameter updates. Specifically, we construct a multimodal erasure direction by
contrasting adversarially perturbed, knowledge-recall image-text pairs with
knowledge-erasure counterparts, capturing both textual and visual
discrepancies. To prevent unnecessary interference, we further design an
input-aware steering mechanism that adaptively determines when and how the
erasure direction should be applied, preserving utility on retained knowledge
while enforcing forgetting on designated content. Experiments on LLaVA-1.5 and
Qwen-2.5-VL demonstrate that MLLMEraser consistently outperforms
state-of-the-art MLLM unlearning baselines, achieving stronger forgetting
performance with lower computational cost and minimal utility degradation.

</details>


### [357] [Physics-Inspired All-Pair Interaction Learning for 3D Dynamics Modeling](https://arxiv.org/abs/2510.04233)
*Kai Yang,Yuqi Huang,Junheng Tao,Wanyu Wang,Qitian Wu*

Main category: cs.LG

TL;DR: PAINET is an SE(3)-equivariant neural network that learns all-pair interactions in multi-body systems using a physics-inspired attention mechanism and a parallel decoder, achieving state-of-the-art 3D dynamics prediction with significant error reductions across diverse benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current GNN-based methods rely on explicitly observed structures and often fail to capture unobserved interactions crucial to complex physical dynamics; there is a need to model all-pair interactions in an SE(3)-equivariant framework to capture unobserved, high-order dependencies while maintaining efficiency.

Method: PAINET introduces a physics-inspired attention network derived from the minimization trajectory of an energy function and a parallel decoder that preserves equivariance for efficient inference, designed to learn all-pair interactions in multi-body systems (SE(3)-equivariant).

Result: Empirical results on human motion capture, molecular dynamics, and large-scale protein simulations show PAINET consistently outperforms recent models with 4.7% to 41.5% reductions in 3D dynamics prediction error, at comparable time/memory costs.

Conclusion: PAINET provides a principled SE(3)-equivariant framework for all-pair interaction learning in multi-body dynamics, effectively capturing unobserved interactions and delivering substantial gains in 3D dynamics prediction across diverse domains with efficient inference.

Abstract: Modeling 3D dynamics is a fundamental problem in multi-body systems across
scientific and engineering domains and has important practical implications in
trajectory prediction and simulation. While recent GNN-based approaches have
achieved strong performance by enforcing geometric symmetries, encoding
high-order features or incorporating neural-ODE mechanics, they typically
depend on explicitly observed structures and inherently fail to capture the
unobserved interactions that are crucial to complex physical behaviors and
dynamics mechanism. In this paper, we propose PAINET, a principled
SE(3)-equivariant neural architecture for learning all-pair interactions in
multi-body systems. The model comprises: (1) a novel physics-inspired attention
network derived from the minimization trajectory of an energy function, and (2)
a parallel decoder that preserves equivariance while enabling efficient
inference. Empirical results on diverse real-world benchmarks, including human
motion capture, molecular dynamics, and large-scale protein simulations, show
that PAINET consistently outperforms recently proposed models, yielding 4.7% to
41.5% error reductions in 3D dynamics prediction with comparable computation
costs in terms of time and memory.

</details>


### [358] [DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks](https://arxiv.org/abs/2510.04331)
*Nghiem T. Diep,Hien Dang,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: DoRAN enhances DoRA for parameter-efficient fine-tuning by adding noise-based regularization in weight decomposition and using auxiliary networks to dynamically generate low-rank components, yielding greater stability and sample efficiency; it outperforms LoRA, DoRA, and other PEFT baselines on vision and language tasks.


<details>
  <summary>Details</summary>
Motivation: DoRA’s weight-decomposed scheme can suffer from training instability and limited sample efficiency. A method that stabilizes training and improves cross-layer coupling could strengthen fine-tuning of large foundation models.

Method: Two-stage approach: (i) inject noise into the denominator of DoRA’s weight decomposition as an adaptive regularizer to reduce instability; (ii) replace static low-rank matrices with auxiliary networks that generate them dynamically, enabling parameter coupling across layers and improving sample efficiency.

Result: Empirical evaluations on vision and language benchmarks show DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines, indicating enhanced stability and data efficiency.

Conclusion: Combining noise-based stabilization with network-based dynamic parameter generation is a promising direction for robust and efficient fine-tuning of foundation models; DoRAN provides a strong PEFT variant.

Abstract: Parameter-efficient fine-tuning (PEFT) methods have become the standard
paradigm for adapting large-scale models. Among these techniques,
Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the
learning capacity and training stability of the vanilla Low-Rank Adaptation
(LoRA) method by explicitly decomposing pre-trained weights into magnitude and
directional components. In this work, we propose DoRAN, a new variant of DoRA
designed to further stabilize training and boost the sample efficiency of DoRA.
Our approach includes two key stages: (i) injecting noise into the denominator
of DoRA's weight decomposition, which serves as an adaptive regularizer to
mitigate instabilities; and (ii) replacing static low-rank matrices with
auxiliary networks that generate them dynamically, enabling parameter coupling
across layers and yielding better sample efficiency in both theory and
practice. Comprehensive experiments on vision and language benchmarks show that
DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These
results underscore the effectiveness of combining stabilization through
noise-based regularization with network-based parameter generation, offering a
promising direction for robust and efficient fine-tuning of foundation models.

</details>


### [359] [Truncated Kernel Stochastic Gradient Descent with General Losses and Spherical Radial Basis Functions](https://arxiv.org/abs/2510.04237)
*Jinhui Bai,Andreas Christmann,Lei Shi*

Main category: cs.LG

TL;DR: A scalable kernel SGD method using a finite-dimensional projection obtained from the series expansion of spherical radial basis functions, achieving minimax-optimal convergence rates for a broad class of losses, with low computational and storage costs suitable for streaming data.


<details>
  <summary>Details</summary>
Motivation: Kernel methods offer strong expressiveness but are computationally expensive for large-scale supervised learning. There is a need for scalable algorithms with solid generalization guarantees that work with streaming data.

Method: Introduce an adaptive finite-dimensional projection of the stochastic gradient via the infinite-series expansion of spherical radial basis functions. Regularization controls the bias-variance trade-off; estimate the spectral structure of the kernel-induced covariance to unify optimization and generalization analyses. Use coordinate-wise updates inspired by linear SGD to reduce pairwise operations and enable streaming data processing.

Result: The authors prove that both the last iterate and the suffix average converge at minimax-optimal rates; establish optimal strong convergence in the RKHS. The framework supports least-squares, Huber, and logistic losses. The algorithm significantly reduces computational and storage complexity for kernel SGD and is suitable for streaming data, with empirical experiments validating efficiency.

Conclusion: The paper delivers a scalable, theoretically sound kernel SGD framework with broad loss compatibility, displaying strong convergence guarantees and practical efficiency, especially for large-scale and streaming settings.

Abstract: In this paper, we propose a novel kernel stochastic gradient descent (SGD)
algorithm for large-scale supervised learning with general losses. Compared to
traditional kernel SGD, our algorithm improves efficiency and scalability
through an innovative regularization strategy. By leveraging the infinite
series expansion of spherical radial basis functions, this strategy projects
the stochastic gradient onto a finite-dimensional hypothesis space, which is
adaptively scaled according to the bias-variance trade-off, thereby enhancing
generalization performance. Based on a new estimation of the spectral structure
of the kernel-induced covariance operator, we develop an analytical framework
that unifies optimization and generalization analyses. We prove that both the
last iterate and the suffix average converge at minimax-optimal rates, and we
further establish optimal strong convergence in the reproducing kernel Hilbert
space. Our framework accommodates a broad class of classical loss functions,
including least-squares, Huber, and logistic losses. Moreover, the proposed
algorithm significantly reduces computational complexity and achieves optimal
storage complexity by incorporating coordinate-wise updates from linear SGD,
thereby avoiding the costly pairwise operations typical of kernel SGD and
enabling efficient processing of streaming data. Finally, extensive numerical
experiments demonstrate the efficiency of our approach.

</details>


### [360] [Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions](https://arxiv.org/abs/2510.04417)
*Wenyuan Zhao,Adithya Balachandran,Chao Tian,Paul Pu Liang*

Main category: cs.LG

TL;DR: This work introduces Gaussian PID (GPID) for modeling information decomposition among modalities under pairwise Gaussian assumptions, with a gradient-based optimization that improves efficiency; it generalizes to non-Gaussian data via information-preserving encoders, resolves an open theoretical question about joint Gaussian optimality, and demonstrates superior accuracy and scalability on synthetic and real multimodal benchmarks.


<details>
  <summary>Details</summary>
Motivation: Quantify how multiple modalities independently, redundantly, or synergistically convey information about a target in multimodal data. Existing PID methods rely on estimating joint distributions from high-dimensional continuous data, which is computationally expensive and often inaccurate. A Gaussian-based PID (GPID) can offer efficient, tractable computation, but must address non-Gaussian data and theoretical optimality questions.

Method: Develop GPID by assuming multivariate Gaussian pairwise distributions and optimizing a gradient-based formulation to compute PID measures efficiently. Propose information-preserving encoders to map arbitrary input distributions to pairwise Gaussian variables, enabling non-Gaussian data to be analyzed within the GPID framework. Address open theoretical question regarding the optimality of joint Gaussian solutions for GPID.

Result: Empirical results on synthetic data show GPID yields more accurate and efficient PID estimates than existing baselines. Large-scale multimodal benchmarks demonstrate practical utility for quantifying PID and for selecting high-performing models in multimodal settings.

Conclusion: GPID provides a scalable, accurate, and practical approach to partial information decomposition in multimodal data. By leveraging Gaussian assumptions (and encoders for non-Gaussian data), it enables efficient PID computation, resolves a theoretical open problem, and supports real-world applications in model evaluation and interpretability.

Abstract: The study of multimodality has garnered significant interest in fields where
the analysis of interactions among multiple information sources can enhance
predictive modeling, data fusion, and interpretability. Partial information
decomposition (PID) has emerged as a useful information-theoretic framework to
quantify the degree to which individual modalities independently, redundantly,
or synergistically convey information about a target variable. However,
existing PID methods depend on optimizing over a joint distribution constrained
by estimated pairwise probability distributions, which are costly and
inaccurate for continuous and high-dimensional modalities. Our first key
insight is that the problem can be solved efficiently when the pairwise
distributions are multivariate Gaussians, and we refer to this problem as
Gaussian PID (GPID). We propose a new gradient-based algorithm that
substantially improves the computational efficiency of GPID based on an
alternative formulation of the underlying optimization problem. To generalize
the applicability to non-Gaussian data, we learn information-preserving
encoders to transform random variables of arbitrary input distributions into
pairwise Gaussian random variables. Along the way, we resolved an open problem
regarding the optimality of joint Gaussian solutions for GPID. Empirical
validation in diverse synthetic examples demonstrates that our proposed method
provides more accurate and efficient PID estimates than existing baselines. We
further evaluate a series of large-scale multimodal benchmarks to show its
utility in real-world applications of quantifying PID in multimodal datasets
and selecting high-performing models.

</details>


### [361] [Diffusion-Assisted Distillation for Self-Supervised Graph Representation Learning with MLPs](https://arxiv.org/abs/2510.04241)
*Seong Jin Ahn,Myoung-Ho Kim*

Main category: cs.LG

TL;DR: Distill self-supervised GNN knowledge into lightweight MLPs using a diffusion model as a teacher assistant (DAD-SGM), improving generalization and robustness for graph representations.


<details>
  <summary>Details</summary>
Motivation: Self-supervised graph learning hinges on inductive biases; replacing heavy GNNs with efficient MLPs is desirable for scalability, but distilling across such a large capacity gap is challenging. A diffusion-assisted framework aims to bridge this gap.

Method: Introduce a denoising diffusion model as a teacher assistant to guide the distillation from a self-supervised GNN teacher to a student MLP. The diffusion process provides richer, intermediate guidance to transfer inductive biases and representations.

Result: Empirical results show that DAD-SGM outperforms state-of-the-art GNN-to-MLP distillation methods in self-supervised graph representation tasks, with better generalization and robustness. Code is released.

Conclusion: Diffusion-assisted distillation is effective for transferring self-supervised knowledge from GNNs to MLPs, enabling efficient, scalable graph representation learning without sacrificing performance.

Abstract: For large-scale applications, there is growing interest in replacing Graph
Neural Networks (GNNs) with lightweight Multi-Layer Perceptrons (MLPs) via
knowledge distillation. However, distilling GNNs for self-supervised graph
representation learning into MLPs is more challenging. This is because the
performance of self-supervised learning is more related to the model's
inductive bias than supervised learning. This motivates us to design a new
distillation method to bridge a huge capacity gap between GNNs and MLPs in
self-supervised graph representation learning. In this paper, we propose
\textbf{D}iffusion-\textbf{A}ssisted \textbf{D}istillation for
\textbf{S}elf-supervised \textbf{G}raph representation learning with
\textbf{M}LPs (DAD-SGM). The proposed method employs a denoising diffusion
model as a teacher assistant to better distill the knowledge from the teacher
GNN into the student MLP. This approach enhances the generalizability and
robustness of MLPs in self-supervised graph representation learning. Extensive
experiments demonstrate that DAD-SGM effectively distills the knowledge of
self-supervised GNNs compared to state-of-the-art GNN-to-MLP distillation
methods. Our implementation is available at
https://github.com/SeongJinAhn/DAD-SGM.

</details>


### [362] [Real-time Prediction of Urban Sound Propagation with Conditioned Normalizing Flows](https://arxiv.org/abs/2510.04510)
*Achim Eckerle,Martin Spitznagel,Janis Keuper*

Main category: cs.LG

TL;DR: Conditional Normalizing Flows (Full-Glow) predict fast, accurate urban sound maps from 2D layouts, achieving ~2000x speedups and improved NLoS accuracy while capturing diffraction/interference and enabling real-time recomputation on commodity hardware.


<details>
  <summary>Details</summary>
Motivation: Regulatory and public-health needs demand accurate, fast urban noise predictions for planning and operation (e.g., noise maps, action plans, permitting). Physics-based solvers are too slow for iterative, time-critical “what-if” analyses.

Method: Train/use a conditional normalizing flow model (Full-Glow) to generate standard-compliant urban sound-pressure maps from 2D layouts, evaluated on 256x256 maps in real time on an RTX 4090. Datasets cover Baseline, Diffraction, and Reflection regimes; enables instant recomputation when sources or geometry change.

Result: Achieves >2000x speedup over a reference solver; up to 24% improvement in NLoS accuracy over prior deep models; Baseline NLoS MAE of 0.65 dB with high structural fidelity; reproduces diffraction/interference patterns.

Conclusion: The approach provides a practical, real-time engine for urban planning, compliance mapping, and operations (e.g., road closures, night-work assessments), scalable to interactive exploration on commodity hardware and adaptable to various acoustic regimes.

Abstract: Accurate and fast urban noise prediction is pivotal for public health and for
regulatory workflows in cities, where the Environmental Noise Directive
mandates regular strategic noise maps and action plans, often needed in
permission workflows, right-of-way allocation, and construction scheduling.
Physics-based solvers are too slow for such time-critical, iterative "what-if"
studies. We evaluate conditional Normalizing Flows (Full-Glow) for generating
for generating standards-compliant urban sound-pressure maps from 2D urban
layouts in real time per 256x256 map on a single RTX 4090), enabling
interactive exploration directly on commodity hardware. On datasets covering
Baseline, Diffraction, and Reflection regimes, our model accelerates map
generation by >2000 times over a reference solver while improving NLoS accuracy
by up to 24% versus prior deep models; in Baseline NLoS we reach 0.65 dB MAE
with high structural fidelity. The model reproduces diffraction and
interference patterns and supports instant recomputation under source or
geometry changes, making it a practical engine for urban planning, compliance
mapping, and operations (e.g., temporary road closures, night-work variance
assessments).

</details>


### [363] [Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing](https://arxiv.org/abs/2510.04263)
*Joseph Ramsey,Bryan Andrews*

Main category: cs.LG

TL;DR: Introduces score-guided mixed-strategy causal search methods (BOSS-FCI, GRaSP-FCI, FCIT, LV-Dumb) for latent-variable causal discovery; improves efficiency, precision, and scalability over standard FCI.


<details>
  <summary>Details</summary>
Motivation: Causal structure learning from observational data with latent confounders or selection bias is challenging; exhaustive conditional-independence testing in FCI can cause spurious results and be computationally expensive; there is a need for scalable, reliable alternatives.

Method: Develops BOSS-FCI and GRaSP-FCI by substituting BOSS or GRaSP for FGES within GFCI; introduces FCIT with targeted tests guided by BOSS; proposes LV-Dumb (BOSS-POD) heuristic that bypasses latent-variable reasoning; evaluates on simulations and real data.

Result: BOSS-FCI and GRaSP-FCI provide sound baselines; FCIT improves efficiency and reliability; LV-Dumb offers a practical, scalable heuristic with strong empirical performance; collectively show the value of score-guided and targeted strategies for latent-variable causal discovery.

Conclusion: Score-guided, targeted strategies can yield scalable, accurate latent-variable causal discovery, balancing theoretical correctness with practical performance.

Abstract: Learning causal structure from observational data is especially challenging
when latent variables or selection bias are present. The Fast Causal Inference
(FCI) algorithm addresses this setting but often performs exhaustive
conditional independence tests across many subsets, leading to spurious
independence claims, extra or missing edges, and unreliable orientations. We
present a family of score-guided mixed-strategy causal search algorithms that
build on this tradition. First, we introduce BOSS-FCI and GRaSP-FCI,
straightforward variants of GFCI that substitute BOSS or GRaSP for FGES,
thereby retaining correctness while incurring different scalability tradeoffs.
Second, we develop FCI Targeted-testing (FCIT), a novel mixed-strategy method
that improves upon these variants by replacing exhaustive all-subsets testing
with targeted tests guided by BOSS, yielding well-formed PAGs with higher
precision and efficiency. Finally, we propose a simple heuristic, LV-Dumb (also
known as BOSS-POD), which bypasses latent-variable-specific reasoning and
directly returns the PAG of the BOSS DAG. Although not strictly correct in the
FCI sense, it scales better and often achieves superior accuracy in practice.
Simulations and real-data analyses demonstrate that BOSS-FCI and GRaSP-FCI
provide sound baselines, FCIT improves both efficiency and reliability, and
LV-Dumb offers a practical heuristic with strong empirical performance.
Together, these method highlight the value of score-guided and targeted
strategies for scalable latent-variable causal discovery.

</details>


### [364] [Influence branching for learning to solve mixed-integer programs online](https://arxiv.org/abs/2510.04273)
*Paul Strang,Zacharie Alès,Côme Bissuel,Olivier Juan,Safia Kedad-Sidhoum,Emmanuel Rachelson*

Main category: cs.LG

TL;DR: Introduces online learning for MIPs via Influence branching, a graph-based variable selection, optimized online with Thompson sampling; shows competitive performance with SCIP and good generalization to broader online settings.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for data-driven, fast variable selection during early branch-and-bound iterations to improve MIP solving speed; leverages graph representations of MIP structure and online adaptation.

Method: Define Influence branching; represent MIP structure as graphs; use Thompson sampling to rank graph representations by their speed-up on SCIP; apply in the initial branch-and-bound iterations; evaluate against online baselines.

Result: Achieves results comparable to state-of-the-art online learning methods; evidence of generalization to broader online frameworks with variations in constraint matrix/vector/objective and with more samples.

Conclusion: Online, graph-based variable selection with Thompson sampling is effective for MIPs and generalizes beyond fixed data, offering a promising direction for online learning in MIP solvers.

Abstract: On the occasion of the 20th Mixed Integer Program Workshop's computational
competition, this work introduces a new approach for learning to solve MIPs
online. Influence branching, a new graph-oriented variable selection strategy,
is applied throughout the first iterations of the branch and bound algorithm.
This branching heuristic is optimized online with Thompson sampling, which
ranks the best graph representations of MIP's structure according to
computational speed up over SCIP. We achieve results comparable to state of the
art online learning methods. Moreover, our results indicate that our method
generalizes well to more general online frameworks, where variations in
constraint matrix, constraint vector and objective coefficients can all occur
and where more samples are available.

</details>


### [365] [Post-training quantization of vision encoders needs prefixing registers](https://arxiv.org/abs/2510.04547)
*Seunghyeon Kim,Jinho Kim,Taesun Yeom,Wonpyo Park,Kyuyeun Kim,Jaeho Lee*

Main category: cs.LG

TL;DR: RegCache is a training-free method to improve post-training quantization of vision encoders by mitigating activation outliers with semantically meaningless prefix tokens, using middle-layer prefixing and token deletion.


<details>
  <summary>Details</summary>
Motivation: Quantizing large vision transformers (e.g., CLIP) incurs accuracy loss due to activation outliers, especially at practical 8-bit or lower; a lightweight, training-free approach is desirable for real-time, massive-scale visual data processing.

Method: Introduce outlier-prone but semantically meaningless prefix tokens to the target vision encoder. Employ middle-layer prefixing and token deletion to separate outlier activations from meaningful token representations, preventing widespread propagation of outliers during quantization. The approach is training-free and designed specifically for vision encoders, which behave differently from language models.

Result: Empirically improves quantized model accuracy across both text-supervised and self-supervised vision encoders, with consistent gains against baselines on standard benchmarks.

Conclusion: Outliers in vision encoders differ from those in language models; RegCache offers a simple, training-free mechanism to mitigate them via targeted prefixing and token deletion, enabling more accurate post-training quantization for vision transformers.

Abstract: Transformer-based vision encoders -- such as CLIP -- are central to
multimodal intelligence, powering applications from autonomous web agents to
robotic control. Since these applications often demand real-time processing of
massive visual data, reducing the inference cost of vision encoders is
critical. Post-training quantization offers a practical path, but remains
challenging even at 8-bit precision due to massive-scale activations (i.e.,
outliers). In this work, we propose $\textit{RegCache}$, a training-free
algorithm to mitigate outliers in vision encoders, enabling quantization with
significantly smaller accuracy drops. The proposed RegCache introduces
outlier-prone yet semantically meaningless prefix tokens to the target vision
encoder, which prevents other tokens from having outliers. Notably, we observe
that outliers in vision encoders behave differently from those in language
models, motivating two technical innovations: middle-layer prefixing and token
deletion. Experiments show that our method consistently improves the accuracy
of quantized models across both text-supervised and self-supervised vision
encoders.

</details>


### [366] [HoRA: Cross-Head Low-Rank Adaptation with Joint Hypernetworks](https://arxiv.org/abs/2510.04295)
*Nghiem T. Diep,Dung Le,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: HoRA introduces a shared hypernetwork to generate low-rank adapters across attention heads, enabling cross-head information sharing. It improves sample efficiency and performance over LoRA with only a small parameter overhead.


<details>
  <summary>Details</summary>
Motivation: LoRA adapts each attention head independently, missing potential cross-head synergies that could yield better efficiency and performance.

Method: HoRA uses joint hypernetworks (a shared generator) to produce low-rank matrices for all MHA heads, coupling their adaptation to encourage cross-head information sharing. The framework is analyzed through a hierarchical mixture of experts lens.

Result: HoRA outperforms LoRA and other PEFT methods on diverse language and vision benchmarks, achieving better sample efficiency with only a marginal increase in trainable parameters.

Conclusion: Cross-head coupling via a shared generator effectively leverages head synergies in MHA, delivering superior PEFT performance with minimal parameter overhead.

Abstract: Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT)
technique that adapts large pre-trained models by adding low-rank matrices to
their weight updates. However, in the context of fine-tuning multi-head
self-attention (MHA), LoRA has been employed to adapt each attention head
separately, thereby overlooking potential synergies across different heads. To
mitigate this issue, we propose a novel Hyper-shared Low-Rank Adaptation (HoRA)
method, which utilizes joint hypernetworks to generate low-rank matrices across
attention heads. By coupling their adaptation through a shared generator, HoRA
encourages cross-head information sharing, and thus directly addresses the
aforementioned limitation of LoRA. By comparing LoRA and HoRA through the lens
of hierarchical mixture of experts, our theoretical findings reveal that the
latter achieves superior sample efficiency to the former. Furthermore, through
extensive experiments across diverse language and vision benchmarks, we
demonstrate that HoRA outperforms LoRA and other PEFT methods while requiring
only a marginal increase in the number of trainable parameters.

</details>


### [367] [SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator](https://arxiv.org/abs/2510.04576)
*Yuhta Takida,Satoshi Hayakawa,Takashi Shibuya,Masaaki Imaizumi,Naoki Murata,Bac Nguyen,Toshimitsu Uesaka,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: We propose SONA, a discriminator that jointly handles authenticity and conditional alignment via separate naturalness and alignment projections with adaptive weighting, achieving state-of-the-art results in class-conditional and text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: Conditional generation requires models to not only distinguish real vs fake but also ensure outputs align with conditioning; existing discriminators often struggle to balance these objectives, hurting conditional fidelity.

Method: Introduce Sum of Naturalness and Alignment (SONA) with separate projections for naturalness and alignment in the final layer, aided by inductive bias, dedicated objective functions, and an adaptive weighting mechanism to balance tasks.

Result: On class-conditional generation, SONA yields superior sample quality and conditional alignment compared to state-of-the-art methods; it is also effective in text-to-image generation, demonstrating versatility and robustness.

Conclusion: SONA provides a unified discriminator design that improves conditional generation by jointly optimizing authenticity and alignment with adaptive balancing, applicable across modalities.

Abstract: Deep generative models have made significant advances in generating complex
content, yet conditional generation remains a fundamental challenge. Existing
conditional generative adversarial networks often struggle to balance the dual
objectives of assessing authenticity and conditional alignment of input samples
within their conditional discriminators. To address this, we propose a novel
discriminator design that integrates three key capabilities: unconditional
discrimination, matching-aware supervision to enhance alignment sensitivity,
and adaptive weighting to dynamically balance all objectives. Specifically, we
introduce Sum of Naturalness and Alignment (SONA), which employs separate
projections for naturalness (authenticity) and alignment in the final layer
with an inductive bias, supported by dedicated objective functions and an
adaptive weighting mechanism. Extensive experiments on class-conditional
generation tasks show that \ours achieves superior sample quality and
conditional alignment compared to state-of-the-art methods. Furthermore, we
demonstrate its effectiveness in text-to-image generation, confirming the
versatility and robustness of our approach.

</details>


### [368] [Wave-PDE Nets: Trainable Wave-Equation Layers as an Alternative to Attention](https://arxiv.org/abs/2510.04304)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: Wave-PDE Nets introduce a differentiable second-order wave equation layer whose state propagates as a continuous field with trainable spatial velocity c(x) and damping gamma(x). A symplectic, FFT-based solver propagates the state in O(n log n). The model matches or surpasses Transformers on language and vision benchmarks with notably improved efficiency, and single layers are universal approximators.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of attention and first-order state-space models in capturing long-range, global dependencies efficiently while maintaining stability and a physics-informed inductive bias.

Method: Each layer advances the hidden state via a differentiable simulation of the 2nd-order wave equation in a medium with trainable c(x) and gamma(x). Propagation is implemented by a symplectic spectral solver using FFTs with O(n log n) complexity. An explicit spectral Laplacian and symplectic integration ensure stability. The architecture uses a layering of these wave-inspired propagations.

Result: Empirically, Wave-PDE Nets match or exceed Transformer performance on language and vision benchmarks, with practical advantages: wall-clock time reduced by up to 30% and peak memory by 25%. Ablation studies show the importance of symplectic integration and the spectral Laplacian for stability and performance. Learned parameters reveal intuitive information-propagation strategies.

Conclusion: Wave-PDE Nets offer a computationally efficient, robust architecture with a strong physical inductive bias that is universal (a single Wave-PDE layer is a universal approximator) and can serve as a competitive alternative to attention-based models in both language and vision tasks.

Abstract: We introduce Wave-PDE Nets, a neural architecture whose elementary operation
is a differentiable simulation of the second-order wave equation. Each layer
propagates its hidden state as a continuous field through a medium with
trainable spatial velocity c(x) and damping {\gamma}(x). A symplectic spectral
solver based on FFTs realises this propagation in O(nlog n) time. This
oscillatory, global mechanism provides a powerful alternative to attention and
first-order state-space models. We prove that a single Wave-PDE layer is a
universal approximator. On language and vision benchmarks, Wave-PDE Nets match
or exceed Transformer performance while demonstrating superior practical
efficiency, reducing wall-clock time by up to 30% and peak memory by 25%.
Ablation studies confirm the critical role of symplectic integration and a
spectral Laplacian for stability and performance. Visualizations of the learned
physical parameters reveal that the model learns intuitive strategies for
information propagation. These results position Wave-PDE Nets as a
computationally efficient and robust architecture with a strong physical
inductive bias.

</details>


### [369] [Activation Steering with a Feedback Controller](https://arxiv.org/abs/2510.04309)
*Dung V. Nguyen,Hieu M. Vu,Nhi Y. Pham,Lei Zhang,Tan M. Nguyen*

Main category: cs.LG

TL;DR: The paper frames LLM activation steering as a closed-loop control problem, showing many steering methods are equivalent to P controllers and introduces a PID Steering framework to achieve robust, stable, and interpretable control across models and benchmarks.


<details>
  <summary>Details</summary>
Motivation: There is a lack of theoretical guarantees for existing LLM steering methods, hindering reliable safety alignment and deployment. A control-theoretic foundation is proposed to provide performance guarantees and stability.

Method: First, demonstrate that popular steering methods act as proportional controllers using a feedback signal from steering vectors. Then formulate Proportional-Integral-Derivative (PID) Steering for activation control, interpreting the P term as aligning activations to target directions, the I term accumulating persistent errors across layers, and the D term damping rapid activation changes. This yields a modular, closed-loop framework that can integrate with existing methods.

Result: Empirical evaluation shows PID Steering consistently outperforms existing approaches, delivering more robust and reliable behavioral control across multiple LLM families and benchmarks.

Conclusion: PID Steering offers a lightweight, modular activation-control framework with interpretable error dynamics and theoretical stability guarantees, compatible with state-of-the-art steering methods and improving reliability of LLM behavior.

Abstract: Controlling the behaviors of large language models (LLM) is fundamental to
their safety alignment and reliable deployment. However, existing steering
methods are primarily driven by empirical insights and lack theoretical
performance guarantees. In this work, we develop a control-theoretic foundation
for activation steering by showing that popular steering methods correspond to
the proportional (P) controllers, with the steering vector serving as the
feedback signal. Building on this finding, we propose
Proportional-Integral-Derivative (PID) Steering, a principled framework that
leverages the full PID controller for activation steering in LLMs. The
proportional (P) term aligns activations with target semantic directions, the
integral (I) term accumulates errors to enforce persistent corrections across
layers, and the derivative (D) term mitigates overshoot by counteracting rapid
activation changes. This closed-loop design yields interpretable error dynamics
and connects activation steering to classical stability guarantees in control
theory. Moreover, PID Steering is lightweight, modular, and readily integrates
with state-of-the-art steering methods. Extensive experiments across multiple
LLM families and benchmarks demonstrate that PID Steering consistently
outperforms existing approaches, achieving more robust and reliable behavioral
control.

</details>


### [370] [Crash Severity Prediction Using Deep Learning Approaches: A Hybrid CNN-RNN Framework](https://arxiv.org/abs/2510.04316)
*Sahar Koohfar*

Main category: cs.LG

TL;DR: A CNN-RNN hybrid deep learning model better predicts crash severity than traditional ML and single DL models on Virginia I-64 data.


<details>
  <summary>Details</summary>
Motivation: Timely, accurate crash severity prediction is essential for medical/transportation response; leveraging complex feature interactions via deep learning can improve predictions.

Method: Developed a CNN-RNN hybrid model and compared it to logistic regression, Naive Bayes, KNN, decision trees, and standalone RNN/CNN; used 15,870 accident records from 2015-2021 on I-64; focused on capturing interconnected feature relationships.

Result: The CNN-RNN hybrid outperformed all benchmark models in predicting crash severity.

Conclusion: Hybrid CNN-RNN architecture is effective for crash severity prediction, validating integrated deep-learning approaches to enhance ITS emergency response.

Abstract: Accurate and timely prediction of crash severity is crucial in mitigating the
severe consequences of traffic accidents. Accurate and timely prediction of
crash severity is crucial in mitigating the severe consequences of traffic
accidents. In order to provide appropriate levels of medical assistance and
transportation services, an intelligent transportation system relies on
effective prediction methods. Deep learning models have gained popularity in
this domain due to their capability to capture non-linear relationships among
variables. In this research, we have implemented a hybrid CNN-RNN deep learning
model for crash severity prediction and compared its performance against widely
used statistical and machine learning models such as logistic regression,
na\"ive bayes classifier, K-Nearest Neighbors (KNN), decision tree, and
individual deep learning models: RNN and CNN. This study employs a methodology
that considers the interconnected relationships between various features of
traffic accidents. The study was conducted using a dataset of 15,870 accident
records gathered over a period of seven years between 2015 and 2021 on Virginia
highway I-64. The findings demonstrate that the proposed CNN-RNN hybrid model
has outperformed all benchmark models in terms of predicting crash severity.
This result illustrates the effectiveness of the hybrid model as it combines
the advantages of both RNN and CNN models in order to achieve greater accuracy
in the prediction process.

</details>


### [371] [On Structured State-Space Duality](https://arxiv.org/abs/2510.04944)
*Jerry Yao-Chieh Hu,Xiwen Zhang,Weimin Wu,Han Liu*

Main category: cs.LG

TL;DR: SSD reveals a tight equivalence between diagonal/state-space models (including scalar-identity as a special case) and masked 1-semiseparable attention, enabling a dual O(T) recurrence or O(T^2) attention; equivalence holds under a precise condition and fails for standard softmax attention due to rank explosion.


<details>
  <summary>Details</summary>
Motivation: Tighten the bridge between recurrent structured-state-space models and Transformers, extend the duality to richer diagonal SSMs, and broaden the design space for expressive yet efficient sequence models.

Method: Formalize SSD for general diagonal state matrices, analyze training complexity lower bounds, derive a necessary and sufficient condition for equivalence with 1-semiseparable masked attention, and prove that the duality does not extend to softmax attention because of rank explosion.

Result: Demonstrates that diagonal SSMs preserve the O(T) recurrence as a counterpart to masked attention, match the scalar-identity case in training complexity lower bounds while enabling richer dynamics, provides a precise equivalence condition, and shows rank explosion prevents a similar duality for softmax attention.

Conclusion: SSD tightens the link between recurrent SSMs and Transformers, expanding the design space for expressive yet efficient sequence models; however, the duality does not extend to standard softmax attention, limiting a fully unified view across attention variants.

Abstract: Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence
between a simple Structured State-Space Model (SSM) and a masked attention
mechanism. In particular, a state-space model with a scalar-times-identity
state matrix is equivalent to a masked self-attention with a $1$-semiseparable
causal mask. Consequently, the same sequence transformation (model) has two
algorithmic realizations: as a linear-time $O(T)$ recurrence or as a
quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize
this duality: (i) we extend SSD from the scalar-identity case to general
diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs
match the scalar case's training complexity lower bounds while supporting
richer dynamics; (iii) we establish a necessary and sufficient condition under
which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we
show that such duality fails to extend to standard softmax attention due to
rank explosion. Together, these results tighten bridge between recurrent SSMs
and Transformers, and widen the design space for expressive yet efficient
sequence models.

</details>


### [372] [FairAgent: Democratizing Fairness-Aware Machine Learning with LLM-Powered Agents](https://arxiv.org/abs/2510.04317)
*Yucong Dai,Lu Zhang,Feng Luo,Mashrur Chowdhury,Yongkai Wu*

Main category: cs.LG

TL;DR: FairAgent is an LLM-powered automation that analyzes data for bias, automates preprocessing and feature engineering, and applies bias mitigation to produce fairness-aware models with improved performance and reduced development effort.


<details>
  <summary>Details</summary>
Motivation: Training fair and unbiased ML is technically demanding, requiring expertise in fairness definitions, metrics, data preprocessing, and ML techniques. Balancing performance with fairness and handling sensitive attributes makes fairness-aware model development inaccessible to many practitioners.

Method: Introduce FairAgent, an LLM-powered automated system that automatically analyzes datasets for potential biases, handles data preprocessing and feature engineering, and implements appropriate bias mitigation strategies based on user requirements.

Result: Experiments show significant performance improvements and substantially reduced development time and expertise requirements, making fairness-aware machine learning more accessible to practitioners.

Conclusion: FairAgent lowers barriers to fairness-aware ML, enabling broader adoption and more accessible development of fair models; future work could address validation across diverse domains and robustness of mitigation strategies.

Abstract: Training fair and unbiased machine learning models is crucial for high-stakes
applications, yet it presents significant challenges. Effective bias mitigation
requires deep expertise in fairness definitions, metrics, data preprocessing,
and machine learning techniques. In addition, the complex process of balancing
model performance with fairness requirements while properly handling sensitive
attributes makes fairness-aware model development inaccessible to many
practitioners. To address these challenges, we introduce FairAgent, an
LLM-powered automated system that significantly simplifies fairness-aware model
development. FairAgent eliminates the need for deep technical expertise by
automatically analyzing datasets for potential biases, handling data
preprocessing and feature engineering, and implementing appropriate bias
mitigation strategies based on user requirements. Our experiments demonstrate
that FairAgent achieves significant performance improvements while
significantly reducing development time and expertise requirements, making
fairness-aware machine learning more accessible to practitioners.

</details>


### [373] [FoilDiff: A Hybrid Transformer Backbone for Diffusion-based Modelling of 2D Airfoil Flow Fields](https://arxiv.org/abs/2510.04325)
*Kenechukwu Ogbuagu,Sepehr Maleki,Giuseppe Bruni,Senthil Krishnababu*

Main category: cs.LG

TL;DR: FoilDiff—a diffusion-based surrogate with a hybrid CNN-Transformer backbone for airfoil flow-field prediction, delivering up to 85% error reduction and improved uncertainty calibration versus prior diffusion models.


<details>
  <summary>Details</summary>
Motivation: CFD is accurate but computationally expensive; there is need for fast, reliable surrogates. Diffusion models have shown promise for complex flow-field prediction; this work extends them with a hybrid backbone and efficient sampling to generalize across key aerodynamic variables.

Method: Hybrid-backbone denoising network combining convolutional feature extraction and transformer-based global attention; uses Denoising Diffusion Implicit Model (DDIM) sampling to improve efficiency; inputs include encoded Reynolds number, angle of attack, and airfoil geometry to enable generalization across a wide range of conditions.

Result: Outperforms state-of-the-art models with substantial gains; mean prediction error reduced by up to 85% on the same datasets; provides better-calibrated predictive uncertainty than existing diffusion-based models.

Conclusion: Demonstrates that diffusion-based surrogates with a hybrid backbone and DDIM sampling can yield accurate, uncertainty-aware flow-field predictions across diverse aerodynamic conditions, enabling faster design and optimization.

Abstract: The accurate prediction of flow fields around airfoils is crucial for
aerodynamic design and optimisation. Computational Fluid Dynamics (CFD) models
are effective but computationally expensive, thus inspiring the development of
surrogate models to enable quicker predictions. These surrogate models can be
based on deep learning architectures, such as Convolutional Neural Networks
(CNNs), Graph Neural Networks (GNNs), and Diffusion Models (DMs). Diffusion
models have shown significant promise in predicting complex flow fields. In
this work, we propose FoilDiff, a diffusion-based surrogate model with a
hybrid-backbone denoising network. This hybrid design combines the power of
convolutional feature extraction and transformer-based global attention to
generate more adaptable and accurate representations of flow structures.
FoilDiff takes advantage of Denoising Diffusion Implicit Model (DDIM) sampling
to optimise the efficiency of the sampling process at no additional cost to
model generalisation. We used encoded representations of Reynolds number, angle
of attack, and airfoil geometry to define the input space for generalisation
across a wide range of aerodynamic conditions. When evaluated against
state-of-the-art models, FoilDiff shows significant performance improvements,
with mean prediction errors reducing by up to 85\% on the same datasets. The
results have demonstrated that FoilDiff can provide both more accurate
predictions and better-calibrated predictive uncertainty than existing
diffusion-based models.

</details>


### [374] [Arithmetic-Mean $μ$P for Modern Architectures: A Unified Learning-Rate Scale for CNNs and ResNets](https://arxiv.org/abs/2510.04327)
*Haosong Zhang,Shenxi Wu,Yichi Zhang,Wei Lin*

Main category: cs.LG

TL;DR: AM-μP constrains the network-wide average, not per-layer, second-moment of pre-activations, combined with residual-aware initialization, to yield width-robust depth laws and a universal learning-rate scaling η* ∝ L^{-3/2} that transfers across depths and architectures.


<details>
  <summary>Details</summary>
Motivation: Choosing an appropriate learning rate is hard for deep, heterogeneous architectures with residuals and convolutions; existing μP (maximal update) is ill-posed for such networks, motivating a depth-robust, transferable LR principle.

Method: Introduce Arithmetic-Mean μP (AM-μP) that fixes the network-wide average pre-activation second moment, and apply residual-aware He initialization scaling residual blocks by the number of blocks. Derive theoretical scaling laws for η* in 1D/2D conv and residual networks and validate empirically across depths.

Result: The maximal-update learning rate scales as η*(L) ∝ L^{-3/2} for 1D/2D conv networks and remains Θ(L^{-3/2}) for standard ResNets with conv+MLP blocks; boundary effects are constant with padding; experiments confirm the -3/2 law and enable zero-shot LR transfer across depths.

Conclusion: AM-μP provides a unified, practical LR principle for CNNs and deep residual networks, enabling LR transfer without extra tuning and avoiding per-layer LR adjustments.

Abstract: Choosing an appropriate learning rate remains a key challenge in scaling
depth of modern deep networks. The classical maximal update parameterization
($\mu$P) enforces a fixed per-layer update magnitude, which is well suited to
homogeneous multilayer perceptrons (MLPs) but becomes ill-posed in
heterogeneous architectures where residual accumulation and convolutions
introduce imbalance across layers. We introduce Arithmetic-Mean $\mu$P
(AM-$\mu$P), which constrains not each individual layer but the network-wide
average one-step pre-activation second moment to a constant scale. Combined
with a residual-aware He fan-in initialization - scaling residual-branch
weights by the number of blocks ($\mathrm{Var}[W]=c/(K\cdot
\mathrm{fan\text{-}in})$) - AM-$\mu$P yields width-robust depth laws that
transfer consistently across depths. We prove that, for one- and
two-dimensional convolutional networks, the maximal-update learning rate
satisfies $\eta^\star(L)\propto L^{-3/2}$; with zero padding, boundary effects
are constant-level as $N\gg k$. For standard residual networks with general
conv+MLP blocks, we establish $\eta^\star(L)=\Theta(L^{-3/2})$, with $L$ the
minimal depth. Empirical results across a range of depths confirm the $-3/2$
scaling law and enable zero-shot learning-rate transfer, providing a unified
and practical LR principle for convolutional and deep residual networks without
additional tuning overhead.

</details>


### [375] [Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies](https://arxiv.org/abs/2510.04341)
*G. Niklas Noren,Eva-Lisa Meldau,Johan Ellenius*

Main category: cs.LG

TL;DR: A framework for critical appraisal of AI in rare-event recognition, plus a practical SCLE method and a checklist, demonstrated in pharmacovigilance across three studies; emphasizes prevalence-aware evaluation, robustness, and human workflow integration.


<details>
  <summary>Details</summary>
Motivation: In high-stakes settings where positive events are rare, conventional accuracy is misleading; there is a need for evaluation that reflects operational value and asymmetric error costs.

Method: Proposes a structured framework for rare-event AI appraisal, including problem framing, test-set design, prevalence-aware statistics, robustness assessment, and integration into human workflows; introduces Structured Case-Level Examination (SCLE) to complement metric evaluation; provides a procurement/development checklist; demonstrates framework through three pharmacovigilance studies: rule-based retrieval of pregnancy-related reports; duplicate detection combining machine learning with probabilistic record linkage; and automated redaction of person names using an LLM.

Result: Framework, SCLE, and a procurement/development checklist are presented, with illustrative pharmacovigilance applications; demonstrates how cost-sensitive targets can align model performance with operational value; framework is designed to generalize to domains with scarce positives and high-cost errors.

Conclusion: Principles are generalizable beyond pharmacovigilance; key cautions include optimism from unrealistic class balance and lack of difficult positive controls in test data; structured evaluation and cost-aware targets help ensure AI models deliver real operational value.

Abstract: Many high-stakes AI applications target low-prevalence events, where apparent
accuracy can conceal limited real-world value. Relevant AI models range from
expert-defined rules and traditional machine learning to generative LLMs
constrained for classification. We outline key considerations for critical
appraisal of AI in rare-event recognition, including problem framing and test
set design, prevalence-aware statistical evaluation, robustness assessment, and
integration into human workflows. In addition, we propose an approach to
structured case-level examination (SCLE), to complement statistical performance
evaluation, and a comprehensive checklist to guide procurement or development
of AI models for rare-event recognition. We instantiate the framework in
pharmacovigilance, drawing on three studies: rule-based retrieval of
pregnancy-related reports; duplicate detection combining machine learning with
probabilistic record linkage; and automated redaction of person names using an
LLM. We highlight pitfalls specific to the rare-event setting including
optimism from unrealistic class balance and lack of difficult positive controls
in test sets - and show how cost-sensitive targets align model performance with
operational value. While grounded in pharmacovigilance practice, the principles
generalize to domains where positives are scarce and error costs may be
asymmetric.

</details>


### [376] [Learning to Predict Chaos: Curriculum-Driven Training for Robust Forecasting of Chaotic Dynamics](https://arxiv.org/abs/2510.04342)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: Curriculum Chaos Forecasting (CCF) pre-trains sequence models on a chaos-ordered curriculum of synthetic systems to improve forecasting of chaotic time-series, achieving longer horizons on real data across multiple architectures.


<details>
  <summary>Details</summary>
Motivation: Forecasting chaotic systems is challenging due to sensitivity and overfitting; need a training paradigm that teaches models progressively across chaos levels to generalize beyond single datasets.

Method: Create curriculum based on chaos metrics: largest Lyapunov exponent and attractor dimension; curate 50+ synthetic ODE/PDE systems; pre-train GRU/Transformer on progressively chaotic data; evaluate on Sunspot, electricity demand, ECG; perform ablations.

Result: CCF increases valid prediction horizon by up to 40% vs random-order training; more than doubles compared to training on real-world data alone; improvements consistent across GRU and Transformer; ablations validate curriculum structure.

Conclusion: Curriculum-based chaos-informed pre-training yields robust, generalizable dynamical representations for chaotic forecasting and benefits multiple architectures; structure of curriculum is crucial.

Abstract: Forecasting chaotic systems is a cornerstone challenge in many scientific
fields, complicated by the exponential amplification of even infinitesimal
prediction errors. Modern machine learning approaches often falter due to two
opposing pitfalls: over-specializing on a single, well-known chaotic system
(e.g., Lorenz-63), which limits generalizability, or indiscriminately mixing
vast, unrelated time-series, which prevents the model from learning the nuances
of any specific dynamical regime. We propose Curriculum Chaos Forecasting
(CCF), a training paradigm that bridges this gap. CCF organizes training data
based on fundamental principles of dynamical systems theory, creating a
curriculum that progresses from simple, periodic behaviors to highly complex,
chaotic dynamics. We quantify complexity using the largest Lyapunov exponent
and attractor dimension, two well-established metrics of chaos. By first
training a sequence model on predictable systems and gradually introducing more
chaotic trajectories, CCF enables the model to build a robust and generalizable
representation of dynamical behaviors. We curate a library of over 50 synthetic
ODE/PDE systems to build this curriculum. Our experiments show that
pre-training with CCF significantly enhances performance on unseen, real-world
benchmarks. On datasets including Sunspot numbers, electricity demand, and
human ECG signals, CCF extends the valid prediction horizon by up to 40%
compared to random-order training and more than doubles it compared to training
on real-world data alone. We demonstrate that this benefit is consistent across
various neural architectures (GRU, Transformer) and provide extensive ablations
to validate the importance of the curriculum's structure.

</details>


### [377] [From News to Returns: A Granger-Causal Hypergraph Transformer on the Sphere](https://arxiv.org/abs/2510.04357)
*Anoushka Harit,Zhongtian Sun,Jongmin Yu*

Main category: cs.LG

TL;DR: CSHT is a new interpretable transformer for financial time-series forecasting that encodes Granger-causal relationships on a hypersphere and uses causally masked attention to improve prediction, regime classification, and asset ranking across market regimes.


<details>
  <summary>Details</summary>
Motivation: There is a need for interpretable, causally grounded, and robust financial forecasting that can handle regime shifts and uncertainty. The work unifies Granger-causal hypergraph structure, Riemannian geometry, and causally masked Transformer attention to provide transparent attribution and robust generalization.

Method: Introduce CSHT: a transformer that represents multivariate Granger-causal dependencies as directional hyperedges on a hypersphere, with attention constrained by angular masks to preserve time direction and geometric consistency; evaluation on S&P 500 data (2018–2023) including COVID-19 shock.

Result: CSHT outperforms baselines on return prediction, regime classification, and top-asset ranking; demonstrates robust generalization across market regimes and provides transparent attribution pathways from macro events to stock responses.

Conclusion: CSHT offers a principled, practical approach for trustworthy financial forecasting under uncertainty by enforcing predictive causal structure and embedding variables in a Riemannian manifold, enabling robust generalization and interpretable attributions.

Abstract: We propose the Causal Sphere Hypergraph Transformer (CSHT), a novel
architecture for interpretable financial time-series forecasting that unifies
\emph{Granger-causal hypergraph structure}, \emph{Riemannian geometry}, and
\emph{causally masked Transformer attention}. CSHT models the directional
influence of financial news and sentiment on asset returns by extracting
multivariate Granger-causal dependencies, which are encoded as directional
hyperedges on the surface of a hypersphere. Attention is constrained via
angular masks that preserve both temporal directionality and geometric
consistency. Evaluated on S\&P 500 data from 2018 to 2023, including the 2020
COVID-19 shock, CSHT consistently outperforms baselines across return
prediction, regime classification, and top-asset ranking tasks. By enforcing
predictive causal structure and embedding variables in a Riemannian manifold,
CSHT delivers both \emph{robust generalisation across market regimes} and
\emph{transparent attribution pathways} from macroeconomic events to
stock-level responses. These results suggest that CSHT is a principled and
practical solution for trustworthy financial forecasting under uncertainty.

</details>


### [378] [Quantifying Ambiguity in Categorical Annotations: A Measure and Statistical Inference Framework](https://arxiv.org/abs/2510.04366)
*Christopher Klugmann,Daniel Kondermann*

Main category: cs.LG

TL;DR: A new asymmetric ambiguity measure for discrete distributions with a special 'can't solve' category, mapping response distributions to [0,1]. It captures aleatoric uncertainty, relates to quadratic entropy but separates unresolvability from indistinguishability, and is accompanied by frequentist estimators and a Bayesian posterior (Dirichlet priors) for epistemic uncertainty. Demonstrated for dataset quality assessment and downstream ML workflows.


<details>
  <summary>Details</summary>
Motivation: Quantify and separate sources of uncertainty in human-generated categorical annotations (aleatoric vs. epistemic), enabling better dataset quality assessment and more reliable downstream ML behavior.

Method: Define an ambiguity measure with asymmetric treatment of a 'can't solve' category; analyze formal properties and contrast with a representative measure from the literature; develop frequentist estimators for population ambiguity; derive Bayesian posterior over ambiguity under Dirichlet priors on the category distribution; provide numerical experiments illustrating estimation, calibration, and practical use.

Result: Characterizes the proposed measure's properties, including its asymmetric handling of the 'cannot solve' category; provides principled inference tools (frequentsist estimators and Bayesian posterior) and demonstrates applicability to dataset quality assessment and ML workflows.

Conclusion: The framework offers a principled approach to quantify and infer ambiguity in categorical labeling, enabling better data curation and informed ML practice.

Abstract: Human-generated categorical annotations frequently produce empirical response
distributions (soft labels) that reflect ambiguity rather than simple annotator
error. We introduce an ambiguity measure that maps a discrete response
distribution to a scalar in the unit interval, designed to quantify aleatoric
uncertainty in categorical tasks. The measure bears a close relationship to
quadratic entropy (Gini-style impurity) but departs from those indices by
treating an explicit "can't solve" category asymmetrically, thereby separating
uncertainty arising from class-level indistinguishability from uncertainty due
to explicit unresolvability. We analyze the measure's formal properties and
contrast its behavior with a representative ambiguity measure from the
literature. Moving beyond description, we develop statistical tools for
inference: we propose frequentist point estimators for population ambiguity and
derive the Bayesian posterior over ambiguity induced by Dirichlet priors on the
underlying probability vector, providing a principled account of epistemic
uncertainty. Numerical examples illustrate estimation, calibration, and
practical use for dataset-quality assessment and downstream machine-learning
workflows.

</details>


### [379] [GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks](https://arxiv.org/abs/2510.04374)
*Tejal Patwardhan,Rachel Dias,Elizabeth Proehl,Grace Kim,Michele Wang,Olivia Watkins,Simón Posada Fishman,Marwan Aljubeh,Phoebe Thacker,Laurance Fauconnet,Natalie S. Kim,Patrick Chao,Samuel Miserendino,Gildas Chabot,David Li,Michael Sharman,Alexandra Barr,Amelia Glaese,Jerry Tworek*

Main category: cs.LG

TL;DR: GDPval is a new benchmark evaluating AI models on real-world, economically valuable tasks drawn from 44 occupations across 9 GDP sectors; frontier models improve linearly, approaching expert deliverables; with scaffolding and context, improvements; open-source 220-task gold subset and automated grading service.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between abstract AI benchmarks and real-world performance on economically valuable work; quantify when AI can perform tasks at industry-level quality and cost.

Method: Construct tasks from BLS Work Activities for 44 occupations, validated by industry professionals with ~14 years experience; evaluate frontier models on deliverable-quality tasks; compare with human experts; analyze effects of reasoning, context, and scaffolding; release a gold subset and automated grading service.

Result: Frontier models improve roughly linearly over time; current best models approach industry experts in deliverable quality; with human oversight, potential to perform GDPval tasks cheaper and faster than unaided experts; increased reasoning, context, scaffolding boosts performance; released 220-task gold subset and automated grading service at evals.openai.com.

Conclusion: GDPval provides a real-world benchmark demonstrating that progress in frontier models is translating into near-expert capabilities on economically valuable tasks; with appropriate human-in-the-loop and tooling, AI can be deployed to reduce cost and time while maintaining quality; resources are open-sourced for future research.

Abstract: We introduce GDPval, a benchmark evaluating AI model capabilities on
real-world economically valuable tasks. GDPval covers the majority of U.S.
Bureau of Labor Statistics Work Activities for 44 occupations across the top 9
sectors contributing to U.S. GDP (Gross Domestic Product). Tasks are
constructed from the representative work of industry professionals with an
average of 14 years of experience. We find that frontier model performance on
GDPval is improving roughly linearly over time, and that the current best
frontier models are approaching industry experts in deliverable quality. We
analyze the potential for frontier models, when paired with human oversight, to
perform GDPval tasks cheaper and faster than unaided experts. We also
demonstrate that increased reasoning effort, increased task context, and
increased scaffolding improves model performance on GDPval. Finally, we
open-source a gold subset of 220 tasks and provide a public automated grading
service at evals.openai.com to facilitate future research in understanding
real-world model capabilities.

</details>


### [380] [Adaptive Weighted Loss for Sequential Recommendations on Sparse Domains](https://arxiv.org/abs/2510.04375)
*Akshay Mittal,Vinay Venkatesh,Krishna Kandi,Shalini Sudarshan*

Main category: cs.LG

TL;DR: A data-driven dynamic weighted loss for single-model sequential recommendation that emphasizes sparse domains, with theoretical guarantees and strong empirical gains across multiple datasets and baselines.


<details>
  <summary>Details</summary>
Motivation: Single-model sequential recommender systems struggle to serve 'power users' in sparse, niche domains. Fixed domain weighting like PinnerFormerLite dilutes scarce signals; a data-driven, per-domain weighting is needed to preserve meaningful gradients for rare interests.

Method: Introduce a dynamic algorithm that assigns loss weights to each domain based on its sparsity in the training data; derive theoretical justifications (convergence, complexity, bounds); validate empirically on MovieLens, Amazon Electronics, Yelp Business, LastFM Music against SIGMA, CALRec, SparseEnNet; evaluate with Recall@10 and NDCG@10.

Result: Significant improvements over baselines, especially for sparse domains; notable gains in Recall@10 and NDCG@10; maintains performance on dense domains with minimal overhead.

Conclusion: A data-driven, adaptive loss weighting approach yields stable, efficient training and better coverage of rare domains, enabling stronger overall performance without excessive computational cost.

Abstract: The effectiveness of single-model sequential recommendation architectures,
while scalable, is often limited when catering to "power users" in sparse or
niche domains. Our previous research, PinnerFormerLite, addressed this by using
a fixed weighted loss to prioritize specific domains. However, this approach
can be sub-optimal, as a single, uniform weight may not be sufficient for
domains with very few interactions, where the training signal is easily diluted
by the vast, generic dataset.
  This paper proposes a novel, data-driven approach: a Dynamic Weighted Loss
function with comprehensive theoretical foundations and extensive empirical
validation. We introduce an adaptive algorithm that adjusts the loss weight for
each domain based on its sparsity in the training data, assigning a higher
weight to sparser domains and a lower weight to denser ones. This ensures that
even rare user interests contribute a meaningful gradient signal, preventing
them from being overshadowed.
  We provide rigorous theoretical analysis including convergence proofs,
complexity analysis, and bounds analysis to establish the stability and
efficiency of our approach. Our comprehensive empirical validation across four
diverse datasets (MovieLens, Amazon Electronics, Yelp Business, LastFM Music)
with state-of-the-art baselines (SIGMA, CALRec, SparseEnNet) demonstrates that
this dynamic weighting system significantly outperforms all comparison methods,
particularly for sparse domains, achieving substantial lifts in key metrics
like Recall at 10 and NDCG at 10 while maintaining performance on denser
domains and introducing minimal computational overhead.

</details>


### [381] [Categorical Invariants of Learning Dynamics](https://arxiv.org/abs/2510.04376)
*Abdulrahman Tamim*

Main category: cs.LG

TL;DR: A functorial view of neural network training: learning as a structure-preserving map from parameters to representations; homotopy of optimization paths correlates with generalization; practical invariants via persistent homology, pullbacks, and 2-categorical ideas for robust training and transfer learning.


<details>
  <summary>Details</summary>
Motivation: Move beyond gradient-descent intuition by imposing a categorical structure on learning, seeking invariants across training paths that predict generalization and guide robust training and transfer learning.

Method: Propose a categorical framework with a functor L: Param -> Rep that preserves structure. Analyze optimization trajectories by homotopy classes, conduct experiments showing generalization similarity for homotopic paths vs divergence for non-homotopic paths. Apply persistent homology to identify stable minima, use pullback constructions to formalize transfer learning, and invoke 2-categorical concepts to explain equivalence of different optimization algorithms.

Result: Empirical finding: networks converging along homotopic trajectories generalize within 0.5% accuracy of each other, while non-homotopic paths differ by over 3%. Persistent homology yields predictors of generalization with R^2 = 0.82. Pullback constructions formalize transfer learning. 2-categorical structures explain when different optimization algorithms yield functionally equivalent models.

Conclusion: Categorical invariants offer theoretical insight into deep learning and provide concrete algorithmic principles for training more robust networks and improving transfer learning.

Abstract: Neural network training is typically viewed as gradient descent on a loss
surface. We propose a fundamentally different perspective: learning is a
structure-preserving transformation (a functor L) between the space of network
parameters (Param) and the space of learned representations (Rep). This
categorical framework reveals that different training runs producing similar
test performance often belong to the same homotopy class (continuous
deformation family) of optimization paths. We show experimentally that networks
converging via homotopic trajectories generalize within 0.5% accuracy of each
other, while non-homotopic paths differ by over 3%. The theory provides
practical tools: persistent homology identifies stable minima predictive of
generalization (R^2 = 0.82 correlation), pullback constructions formalize
transfer learning, and 2-categorical structures explain when different
optimization algorithms yield functionally equivalent models. These categorical
invariants offer both theoretical insight into why deep learning works and
concrete algorithmic principles for training more robust networks.

</details>


### [382] [Score-based Greedy Search for Structure Identification of Partially Observed Linear Causal Models](https://arxiv.org/abs/2510.04378)
*Xinshuai Dong,Ignavier Ng,Haoyue Dai,Jiaqi Sun,Xiangchen Song,Peter Spirtes,Kun Zhang*

Main category: cs.LG

TL;DR: Proposes LGES, a score-based greedy search for causal structures with latent variables under a Generalized N Factor Model, with identifiability guarantees and empirical validation.


<details>
  <summary>Details</summary>
Motivation: To overcome practical issues in constraint-based methods for partially observed causal discovery—such as multiple testing and error propagation—by offering a score-based approach with identifiability guarantees.

Method: Define a Generalized N Factor Model and prove global consistency: the true structure including latent variables can be identified up to the Markov equivalence class using a score. Develop Latent variable Greedy Equivalence Search (LGES) with well-defined operators to efficiently search the graph space for the optimal structure.

Result: The true structure including latent variables can be identified up to Markov equivalence using the score. LGES demonstrates effectiveness on synthetic and real data; code will be publicly available.

Conclusion: First score-based greedy search method for latent-variable causal structure with identifiability guarantees; LGES provides scalable, provably consistent structure learning under partial observability, validated on experiments.

Abstract: Identifying the structure of a partially observed causal system is essential
to various scientific fields. Recent advances have focused on constraint-based
causal discovery to solve this problem, and yet in practice these methods often
face challenges related to multiple testing and error propagation. These issues
could be mitigated by a score-based method and thus it has raised great
attention whether there exists a score-based greedy search method that can
handle the partially observed scenario. In this work, we propose the first
score-based greedy search method for the identification of structure involving
latent variables with identifiability guarantees. Specifically, we propose
Generalized N Factor Model and establish the global consistency:
  the true structure including latent variables can be identified up to the
Markov equivalence class by using score. We then design
  Latent variable Greedy Equivalence Search (LGES), a greedy search algorithm
for this class of model with well-defined operators,
  which search very efficiently over the graph space to find the optimal
structure. Our experiments on both synthetic and real-life data validate the
effectiveness of our method (code will be publicly available).

</details>


### [383] [SSM-CGM: Interpretable State-Space Forecasting Model of Continuous Glucose Monitoring for Personalized Diabetes Management](https://arxiv.org/abs/2510.04386)
*Shakson Isaac,Yentl Collin,Chirag Patel*

Main category: cs.LG

TL;DR: A neural state-space model (SSM-CGM) for CGM data that fuses glucose and wearable signals; outperforms a TFT baseline in short-term predictions, adds interpretability (variable selection, temporal attribution), and supports counterfactual forecasts to simulate effects of physiological changes.


<details>
  <summary>Details</summary>
Motivation: Clinical CGM forecasting often lacks interpretability; there is a need for physiologically grounded, multimodal models that enhance interpretability to support personalized diabetes management.

Method: Develop SSM-CGM built on Mamba; integrate CGM and wearable activity data from the AI-READI cohort; compare against a Temporal Fusion Transformer baseline; implement variable selection, temporal attribution, and counterfactual forecasting of signals (e.g., heart rate, respiration) on near-term glucose.

Result: Improved short-term accuracy over the TFT baseline; enhanced interpretability via variable selection and temporal attribution; enables counterfactual simulations of how planned physiological changes affect near-term glucose.

Conclusion: SSM-CGM provides an interpretable, physiologically grounded framework for personalized diabetes management.

Abstract: Continuous glucose monitoring (CGM) generates dense data streams critical for
diabetes management, but most used forecasting models lack interpretability for
clinical use. We present SSM-CGM, a Mamba-based neural state-space forecasting
model that integrates CGM and wearable activity signals from the AI-READI
cohort. SSM-CGM improves short-term accuracy over a Temporal Fusion Transformer
baseline, adds interpretability through variable selection and temporal
attribution, and enables counterfactual forecasts simulating how planned
changes in physiological signals (e.g., heart rate, respiration) affect
near-term glucose. Together, these features make SSM-CGM an interpretable,
physiologically grounded framework for personalized diabetes management.

</details>


### [384] [Achieve Performatively Optimal Policy for Performative Reinforcement Learning](https://arxiv.org/abs/2510.04430)
*Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: Proposes zeroth-order Frank-Wolfe method (0-FW) for performative RL to achieve performatively optimal (PO) policy in polynomial time under regularizer dominance, with two key theoretical properties ensuring PO at stationary points and bounded gradient within a convex policy set; empirical results show improvement over PS-based methods.


<details>
  <summary>Details</summary>
Motivation: Close the gap between performatively stable (PS) and performatively optimal (PO) policies in performative RL by enabling convergence to the original value function, not just an approximate surrogate; address nonconvexity and potential unbounded gradients via a zeroth-order approach with regularizer-dominance assumptions.

Method: Introduce a zeroth-order approximation of the performative policy gradient within the Frank-Wolfe framework; prove gradient-dominance of the value function under regularizer dominance so that any stationary point (not PS) is PO; show all sufficiently stationary points lie in a convex, compact policy subspace with a positive lower bound Delta, yielding bounded, Lipschitz gradients; establish polynomial-time convergence to PO.

Result: First polynomial-time convergence to the PO policy under the stated conditions; gradient-dominance implies PO at stationary points; confinement to Pi_Delta ensures bounded gradients and Lipschitz continuity; empirical results indicate 0-FW outperforms existing methods in finding PO.

Conclusion: Under standard regularizer dominance, the 0-FW algorithm guarantees convergence to the performatively optimal policy, with solid theoretical guarantees for nonconvex settings and unbounded gradients by restricting to a favorable subspace; results suggest attractive performance benefits, though assumptions may limit applicability and future work could explore relaxing them.

Abstract: Performative reinforcement learning is an emerging dynamical decision making
framework, which extends reinforcement learning to the common applications
where the agent's policy can change the environmental dynamics. Existing works
on performative reinforcement learning only aim at a performatively stable (PS)
policy that maximizes an approximate value function. However, there is a
provably positive constant gap between the PS policy and the desired
performatively optimal (PO) policy that maximizes the original value function.
In contrast, this work proposes a zeroth-order Frank-Wolfe algorithm (0-FW)
algorithm with a zeroth-order approximation of the performative policy gradient
in the Frank-Wolfe framework, and obtains \textbf{the first polynomial-time
convergence to the desired PO} policy under the standard regularizer dominance
condition. For the convergence analysis, we prove two important properties of
the nonconvex value function. First, when the policy regularizer dominates the
environmental shift, the value function satisfies a certain gradient dominance
property, so that any stationary point (not PS) of the value function is a
desired PO. Second, though the value function has unbounded gradient, we prove
that all the sufficiently stationary points lie in a convex and compact policy
subspace $\Pi_{\Delta}$, where the policy value has a constant lower bound
$\Delta>0$ and thus the gradient becomes bounded and Lipschitz continuous.
Experimental results also demonstrate that our 0-FW algorithm is more effective
than the existing algorithms in finding the desired PO policy.

</details>


### [385] [Trade-off in Estimating the Number of Byzantine Clients in Federated Learning](https://arxiv.org/abs/2510.04432)
*Ziyi Chen,Su Zhang,Heng Huang*

Main category: cs.LG

TL;DR: Robust aggregators in federated learning improve resilience to Byzantine clients, but require knowing or estimating the number of Byzantine clients. The paper analyzes how estimation error (hat f vs f) affects performance, showing that underestimating f can cause arbitrarily poor results, while for hat f >= f the error bounds scale as hat f/(n-f-hat f), revealing a trade-off: larger hat f extends robustness but can degrade performance when actual Byzantines are fewer or none.


<details>
  <summary>Details</summary>
Motivation: To understand how misestimating the Byzantine count f affects the performance of robust aggregation in federated learning, a gap exists in quantifying worst-case errors and optimal bounds across all f and hat f.

Method: Theoretical worst-case analysis of aggregators and the induced federated learning algorithm for all f and hat f. Derives lower and upper bounds on errors under two regimes: hat f < f (underestimation) and hat f ≥ f (no underestimation). Demonstrates proportional bounds to hat f/(n-f-hat f).

Result: Shows that underestimation (hat f < f) can lead to arbitrarily poor performance for both the aggregator and the FL process. When hat f ≥ f, establishes optimal lower and upper bounds of the same order on aggregator and FL errors, with all bounds scaling as hat f/(n-f-hat f).

Conclusion: There is a fundamental trade-off in choosing hat f: larger robustness (hat f) allows handling a wider range of actual Byzantine counts f but can degrade performance when f is smaller or zero. Practitioners should balance robustness against potential performance loss and avoid overestimating f to prevent unnecessary degradation.

Abstract: Federated learning has attracted increasing attention at recent large-scale
optimization and machine learning research and applications, but is also
vulnerable to Byzantine clients that can send any erroneous signals. Robust
aggregators are commonly used to resist Byzantine clients. This usually
requires to estimate the unknown number $f$ of Byzantine clients, and thus
accordingly select the aggregators with proper degree of robustness (i.e., the
maximum number $\hat{f}$ of Byzantine clients allowed by the aggregator). Such
an estimation should have important effect on the performance, which has not
been systematically studied to our knowledge. This work will fill in the gap by
theoretically analyzing the worst-case error of aggregators as well as its
induced federated learning algorithm for any cases of $\hat{f}$ and $f$.
Specifically, we will show that underestimation ($\hat{f}<f$) can lead to
arbitrarily poor performance for both aggregators and federated learning. For
non-underestimation ($\hat{f}\ge f$), we have proved optimal lower and upper
bounds of the same order on the errors of both aggregators and federated
learning. All these optimal bounds are proportional to $\hat{f}/(n-f-\hat{f})$
with $n$ clients, which monotonically increases with larger $\hat{f}$. This
indicates a fundamental trade-off: while an aggregator with a larger robustness
degree $\hat{f}$ can solve federated learning problems of wider range $f\in
[0,\hat{f}]$, the performance can deteriorate when there are actually fewer or
even no Byzantine clients (i.e., $f\in [0,\hat{f})$).

</details>


### [386] [Fractional Heat Kernel for Semi-Supervised Graph Learning with Small Training Sample Size](https://arxiv.org/abs/2510.04440)
*Farid Bozorgnia,Vyacheslav Kungurtsev,Shirali Kadyrov,Mohsen Yousefnezhad*

Main category: cs.LG

TL;DR: Fractional heat-kernel dynamics with a source term for label propagation and self-training in Graph Neural Networks, enabling nonlocal, multi-hop diffusion; scalable via Chebyshev approximations; shows improved performance with limited labels on standard datasets.


<details>
  <summary>Details</summary>
Motivation: Bridge information-theoretic perspectives with parabolic PDEs to enhance diffusion-based learning on graphs. The goal is to make GNNs more expressive through adaptive, nonlocal diffusion (fractional Laplacian) and to perform well when labeled data are scarce.

Method: Develop algorithms using fractional heat kernel dynamics with a source term for label propagation and self-training. Integrate the fractional heat kernel into GNN architectures (GCN, GAT) to enable adaptive, multi-hop diffusion. Use Chebyshev polynomial approximations for scalability to large graphs. Provide variational formulations by extending diffusion to fractional Laplacians to promote nonlocal label interactions.

Result: Demonstrates effectiveness on standard datasets, validating the approach.

Conclusion: Balancing supervision with diffusion in the fractional, nonlocal diffusion framework yields more globally diffused labels and improved learning when labeled data are scarce. The approach enhances GNN expressiveness and scalability for large graphs.

Abstract: In this work, we introduce novel algorithms for label propagation and
self-training using fractional heat kernel dynamics with a source term. We
motivate the methodology through the classical correspondence of information
theory with the physics of parabolic evolution equations. We integrate the
fractional heat kernel into Graph Neural Network architectures such as Graph
Convolutional Networks and Graph Attention, enhancing their expressiveness
through adaptive, multi-hop diffusion. By applying Chebyshev polynomial
approximations, large graphs become computationally feasible. Motivating
variational formulations demonstrate that by extending the classical diffusion
model to fractional powers of the Laplacian, nonlocal interactions deliver more
globally diffusing labels. The particular balance between supervision of known
labels and diffusion across the graph is particularly advantageous in the case
where only a small number of labeled training examples are present. We
demonstrate the effectiveness of this approach on standard datasets.

</details>


### [387] [Domain Generalization: A Tale of Two ERMs](https://arxiv.org/abs/2510.04441)
*Yilun Zhu,Naihao Deng,Naichen Shi,Aditya Gangrade,Clayton Scott*

Main category: cs.LG

TL;DR: Under posterior drift, augmenting features with domain information in ERM (domain-informed ERM) can outperform pooling ERM, whereas under covariate shift the standard ERM baseline often remains competitive.


<details>
  <summary>Details</summary>
Motivation: DG results have often shown ERM on pooled data to be hard to beat, but this may depend on the data-generating process. The paper investigates how different distribution shift assumptions (covariate shift vs posterior drift) affect DG performance and proposes a simple remedy.

Method: Develop a theoretical framework comparing covariate shift and posterior drift; propose domain-informed ERM by augmenting input features with domain-specific information; validate with experiments in language and vision tasks.

Result: The theory suggests domain-informed ERM can surpass pooling ERM under posterior drift; empirical experiments in language and vision tasks demonstrate the benefit.

Conclusion: The choice of distribution-shift assumption matters for DG conclusions. A simple, domain-aware augmentation to ERM can improve generalization when posterior drift is present; researchers should consider posterior drift in DG benchmarks and method design.

Abstract: Domain generalization (DG) is the problem of generalizing from several
distributions (or domains), for which labeled training data are available, to a
new test domain for which no labeled data is available. A common finding in the
DG literature is that it is difficult to outperform empirical risk minimization
(ERM) on the pooled training data.
  In this work, we argue that this finding has primarily been reported for
datasets satisfying a \emph{covariate shift} assumption. When the dataset
satisfies a \emph{posterior drift} assumption instead, we show that
``domain-informed ERM,'' wherein feature vectors are augmented with
domain-specific information, outperforms pooling ERM. These claims are
supported by a theoretical framework and experiments on language and vision
tasks.

</details>


### [388] [Forking-Sequences](https://arxiv.org/abs/2510.04487)
*Willa Potosnak,Malcolm Wolff,Boris Oreshkin,Mengfei Cao,Michael W. Mahoney,Dmitry Efimov,Kin G. Olivares*

Main category: cs.LG

TL;DR: Forking-sequences jointly encodes across all forecast creation dates (FCDs) to stabilize forecasts, reduce variance via ensembling, and improve inference efficiency; demonstrated across multiple datasets and architectures.


<details>
  <summary>Details</summary>
Motivation: Forecast stability across FCDs is crucial for stakeholder trust and decision-making. Even highly accurate models can produce unstable revisions; forking-sequences aims to address this gap.

Method: Formalizes the forking-sequences approach and applies it to neural forecasting architectures (MLP, RNN, LSTM, CNN, Transformer). It encodes/decodes the entire time series across all FCDs, mirroring time-series cross-validation, to improve training stability, enable variance reduction through ensembling, and enhance inference efficiency. Evaluation on 16 datasets from M1, M3, M4, and Tourism competitions.

Result: The approach yields substantial improvements in forecast percentage-change stability: 28.8% and 28.8% for MLP and RNN, 37.9% for LSTM, 31.3% for CNN, and 8.8% (average) for Transformer, across the 16 datasets.

Conclusion: Forking-sequences is a simple yet effective technique with broad potential for improving forecast reliability without sacrificing accuracy. It warrants broader adoption in neural forecasting and invites further study on trade-offs and deployment considerations.

Abstract: While accuracy is a critical requirement for time series forecasting models,
an equally important (yet often overlooked) desideratum is forecast stability
across forecast creation dates (FCDs). Even highly accurate models can produce
erratic revisions between FCDs, undermining stakeholder trust and disrupting
downstream decision-making. To improve forecast stability, models like MQCNN,
MQT, and SPADE employ a little-known but highly effective technique:
forking-sequences. Unlike standard statistical and neural forecasting methods
that treat each FCD independently, the forking-sequences method jointly encodes
and decodes the entire time series across all FCDs, in a way mirroring time
series cross-validation. Since forking sequences remains largely unknown in the
broader neural forecasting community, in this work, we formalize the
forking-sequences approach, and we make a case for its broader adoption. We
demonstrate three key benefits of forking-sequences: (i) more stable and
consistent gradient updates during training; (ii) reduced forecast variance
through ensembling; and (iii) improved inference computational efficiency. We
validate forking-sequences' benefits using 16 datasets from the M1, M3, M4, and
Tourism competitions, showing improvements in forecast percentage change
stability of 28.8%, 28.8%, 37.9%, and 31.3%, and 8.8%, on average, for MLP,
RNN, LSTM, CNN, and Transformer-based architectures, respectively.

</details>


### [389] [Expand Neurons, Not Parameters](https://arxiv.org/abs/2510.04500)
*Linghao Kong,Inimai Subramanian,Yonadav Shavit,Micah Adler,Dan Alistarh,Nir Shavit*

Main category: cs.LG

TL;DR: Widening networks at a fixed non-zero parameter budget via Fixed Parameter Expansion (FPE) reduces feature interference (polysemanticity) and improves accuracy, with larger gains under higher interference; gains transfer from symbolic tasks to real models, offering an interpretability-grounded, hardware-friendly strategy that avoids adding non-zero parameters.


<details>
  <summary>Details</summary>
Motivation: Interference among multiple features sharing neurons (superposition/polysemanticity) limits performance. The goal is to reduce entanglement without increasing the count of non-zero parameters, leveraging width to separate representations.

Method: Introduce Fixed Parameter Expansion (FPE): replace a neuron with several children and partition the parent’s weights disjointly among them. Evaluate on symbolic tasks (Boolean code problems) to measure polysemanticity and accuracy; compare with random splits. Extend to real models (classifiers over CLIP embeddings and deeper networks) to test whether widening at fixed parameter budgets improves accuracy. Analyze how performance scales with interference (polysemantic load).

Result: In symbolic tasks, clause-aligned FPE reduces polysemanticity metrics and increases task accuracy; random weight splits approximate these gains, indicating reduced collisions—not precise allocation—is key. Benefits grow with higher interference (superposition). In real models, network widening at fixed non-zero parameter count consistently improves accuracy for CLIP-based classifiers and deeper networks, supporting the proposed mechanism.

Conclusion: FPE provides an interpretability-grounded mechanism to mitigate superposition by widening networks without increasing non-zero parameter counts, yielding accuracy gains especially under high polysemantic load and aligning with accelerator-oriented memory bottlenecks.

Abstract: This work demonstrates how increasing the number of neurons in a network
without increasing its number of non-zero parameters improves performance. We
show that this gain corresponds with a decrease in interference between
multiple features that would otherwise share the same neurons. To reduce such
entanglement at a fixed non-zero parameter count, we introduce Fixed Parameter
Expansion (FPE): replace a neuron with multiple children and partition the
parent's weights disjointly across them, so that each child inherits a
non-overlapping subset of connections. On symbolic tasks, specifically Boolean
code problems, clause-aligned FPE systematically reduces polysemanticity
metrics and yields higher task accuracy. Notably, random splits of neuron
weights approximate these gains, indicating that reduced collisions, not
precise assignment, are a primary driver. Consistent with the superposition
hypothesis, the benefits of FPE grow with increasing interference: when
polysemantic load is high, accuracy improvements are the largest. Transferring
these insights to real models (classifiers over CLIP embeddings and deeper
multilayer networks) we find that widening networks while maintaining a
constant non-zero parameter count consistently increases accuracy. These
results identify an interpretability-grounded mechanism to leverage width
against superposition, improving performance without increasing the number of
non-zero parameters. Such a direction is well matched to modern accelerators,
where memory movement of non-zero parameters, rather than raw compute, is the
dominant bottleneck.

</details>


### [390] [Wavelet Predictive Representations for Non-Stationary Reinforcement Learning](https://arxiv.org/abs/2510.04507)
*Min Wang,Xin Li,Ye He,Yao-Hui Li,Hasnaa Bennis,Riashat Islam,Mingzhong Wang*

Main category: cs.LG

TL;DR: WISDOM introduces wavelet-domain predictive task representations for non-stationary RL, using multi-scale wavelet features and a wavelet TD update to track evolving MDPs; it yields theoretical convergence and improved sample efficiency and asymptotic performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Non-stationary environments cause rapid changes in dynamics, challenging adaptation. Existing NSRL methods often assume regular patterns and struggle with highly dynamic settings. Wavelets offer multi-scale trend extraction to better track evolving tasks.

Method: Transform sequences of task representations into the wavelet domain to capture global trends and fine-grained variations via wavelet coefficients. Combine auto-regressive modeling of task evolution with a novel wavelet temporal-difference (TD) update operator, for stable tracking and prediction of MDP changes. Provide theoretical convergence guarantees for the operator and demonstrate policy improvement using wavelet representations.

Result: Empirical studies across diverse benchmarks show that WISDOM outperforms baselines in both sample efficiency and final performance, demonstrating strong adaptability in complex, non-stationary and stochastically evolving tasks.

Conclusion: Wavelet-based task representations coupled with a wavelet TD update effectively address non-stationarity in RL, supported by theoretical convergence and consistent empirical gains across challenging environments.

Abstract: The real world is inherently non-stationary, with ever-changing factors, such
as weather conditions and traffic flows, making it challenging for agents to
adapt to varying environmental dynamics. Non-Stationary Reinforcement Learning
(NSRL) addresses this challenge by training agents to adapt rapidly to
sequences of distinct Markov Decision Processes (MDPs). However, existing NSRL
approaches often focus on tasks with regularly evolving patterns, leading to
limited adaptability in highly dynamic settings. Inspired by the success of
Wavelet analysis in time series modeling, specifically its ability to capture
signal trends at multiple scales, we propose WISDOM to leverage wavelet-domain
predictive task representations to enhance NSRL. WISDOM captures these
multi-scale features in evolving MDP sequences by transforming task
representation sequences into the wavelet domain, where wavelet coefficients
represent both global trends and fine-grained variations of non-stationary
changes. In addition to the auto-regressive modeling commonly employed in time
series forecasting, we devise a wavelet temporal difference (TD) update
operator to enhance tracking and prediction of MDP evolution. We theoretically
prove the convergence of this operator and demonstrate policy improvement with
wavelet task representations. Experiments on diverse benchmarks show that
WISDOM significantly outperforms existing baselines in both sample efficiency
and asymptotic performance, demonstrating its remarkable adaptability in
complex environments characterized by non-stationary and stochastically
evolving tasks.

</details>


### [391] [Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction](https://arxiv.org/abs/2510.04522)
*Yisen Gao,Xingcheng Fu,Qingyun Sun,Jianxin Li,Xianxian Li*

Main category: cs.LG

TL;DR: GeoMancer proposes a Riemannian diffusion model for graphs that disentangles multi-manifold geometry, improving generation and prediction by addressing numerical instability and manifold deviation with gyrokernels and manifold-constrained diffusion.


<details>
  <summary>Details</summary>
Motivation: Graph data lie on multiple manifolds with varying curvature; embedding all features into a single Euclidean latent space entangles geometric signatures and limits exploitation of the manifold structure. A dedicated Riemannian diffusion framework could capture distinct geometric patterns to improve predictive and generative performance.

Method: Introduce GeoMancer, a Riemannian graph diffusion model. Replace exponential mapping with an isometric-invariant Riemannian gyrokernel to reduce numerical instability. Decouple multi-level features onto task-specific manifolds and apply manifold-constrained diffusion with a self-guided strategy for unconditional generation to keep samples aligned with manifold signatures.

Result: Extensive experiments show superior performance on a variety of graph tasks and tasks requiring generation, validating the effectiveness of the proposed approach.

Conclusion: GeoMancer effectively models manifold signatures in graph data, addressing numerical instability and manifold deviation to enhance both generation and prediction tasks.

Abstract: Graph diffusion models have made significant progress in learning structured
graph data and have demonstrated strong potential for predictive tasks.
Existing approaches typically embed node, edge, and graph-level features into a
unified latent space, modeling prediction tasks including classification and
regression as a form of conditional generation. However, due to the
non-Euclidean nature of graph data, features of different curvatures are
entangled in the same latent space without releasing their geometric potential.
To address this issue, we aim to construt an ideal Riemannian diffusion model
to capture distinct manifold signatures of complex graph data and learn their
distribution. This goal faces two challenges: numerical instability caused by
exponential mapping during the encoding proces and manifold deviation during
diffusion generation. To address these challenges, we propose GeoMancer: a
novel Riemannian graph diffusion framework for both generation and prediction
tasks. To mitigate numerical instability, we replace exponential mapping with
an isometric-invariant Riemannian gyrokernel approach and decouple multi-level
features onto their respective task-specific manifolds to learn optimal
representations. To address manifold deviation, we introduce a
manifold-constrained diffusion method and a self-guided strategy for
unconditional generation, ensuring that the generated data remains aligned with
the manifold signature. Extensive experiments validate the effectiveness of our
approach, demonstrating superior performance across a variety of tasks.

</details>


### [392] [Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in Masked Diffusion](https://arxiv.org/abs/2510.04525)
*Satoshi Hayakawa,Yuhta Takida,Masaaki Imaizumi,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: The paper analyzes MaskGIT diffusion samplers, exposes an implicit temperature sampling mechanism, and introduces the moment sampler as an asymptotically equivalent, simpler alternative. It adds a choose-then-sample paradigm with two efficiency boosts—partial caching for transformers and a hybrid exploration-exploitation strategy for adaptive unmasking—and validates these ideas experimentally in image and text domains.


<details>
  <summary>Details</summary>
Motivation: Accelerate sampling in masked diffusion models and provide a deeper theoretical and practical understanding of MaskGIT-based samplers, addressing the gap between theory and scalable implementations.

Method: The authors theoretically analyze the MaskGIT sampler to uncover its implicit temperature sampling behavior. They propose the moment sampler, an asymptotically equivalent but more tractable alternative that uses a choose-then-sample approach. They also introduce (1) partial caching to approximate longer sampling trajectories with reduced cost, and (2) a hybrid strategy formalizing exploration-exploitation in adaptive unmasking.

Result: The moment sampler is shown to be asymptotically equivalent to MaskGIT while being more interpretable and efficient. Partial caching reduces transformer computation during long sampling trajectories, and the hybrid exploration-exploitation method improves adaptive unmasking. Experimental results in both image and text domains verify the theoretical claims and demonstrate practical efficiency gains.

Conclusion: The work advances both the theoretical understanding and practical implementation of masked diffusion samplers, offering a tractable, efficient family of samplers (moment sampler) and effective mechanisms (partial caching and a hybrid unmasking strategy) that accelerate masked diffusion sampling across domains.

Abstract: Masked diffusion models have shown promising performance in generating
high-quality samples in a wide range of domains, but accelerating their
sampling process remains relatively underexplored. To investigate efficient
samplers for masked diffusion, this paper theoretically analyzes the MaskGIT
sampler for image modeling, revealing its implicit temperature sampling
mechanism. Through this analysis, we introduce the "moment sampler," an
asymptotically equivalent but more tractable and interpretable alternative to
MaskGIT, which employs a "choose-then-sample" approach by selecting unmasking
positions before sampling tokens. In addition, we improve the efficiency of
choose-then-sample algorithms through two key innovations: a partial caching
technique for transformers that approximates longer sampling trajectories
without proportional computational cost, and a hybrid approach formalizing the
exploration-exploitation trade-off in adaptive unmasking. Experiments in image
and text domains demonstrate our theory as well as the efficiency of our
proposed methods, advancing both theoretical understanding and practical
implementation of masked diffusion samplers.

</details>


### [393] [Graph-based Tabular Deep Learning Should Learn Feature Interactions, Not Just Make Predictions](https://arxiv.org/abs/2510.04543)
*Elias Dubbeldam,Reza Mohammadi,Marit Schoonhoven,S. Ilker Birbil*

Main category: cs.LG

TL;DR: GTDL research should shift from sole emphasis on predictive accuracy to explicit learning/evaluation of feature interaction graphs; synthetic experiments reveal current methods miss meaningful interactions, and enforcing true interaction structure can boost performance.


<details>
  <summary>Details</summary>
Motivation: Although deep learning for tabular data has progressed, it often underperforms tree-based models due to difficulty in modeling complex, dataset-specific feature interactions. GTDL offers a graph-based view of features and their interactions, but existing work focuses on prediction without reliable learning of the underlying graph structure. A structure-aware approach can improve interpretability, trust, and domain relevance.

Method: Use synthetic datasets with known ground-truth graph structures to evaluate GTDL methods' ability to recover feature interactions. Compare methods on their structural recovery and predictive performance, and demonstrate that enforcing the true interaction structure can improve predictions.

Result: Existing GTDL methods fail to recover meaningful feature interactions; enforcing the true interaction structure improves predictive performance.

Conclusion: Advocate for a shift toward structure-aware modeling as the foundation for GTDL systems, aiming for accuracy, interpretability, trustworthiness, and alignment with domain knowledge.

Abstract: Despite recent progress, deep learning methods for tabular data still
struggle to compete with traditional tree-based models. A key challenge lies in
modeling complex, dataset-specific feature interactions that are central to
tabular data. Graph-based tabular deep learning (GTDL) methods aim to address
this by representing features and their interactions as graphs. However,
existing methods predominantly optimize predictive accuracy, neglecting
accurate modeling of the graph structure. This position paper argues that GTDL
should move beyond prediction-centric objectives and prioritize the explicit
learning and evaluation of feature interactions. Using synthetic datasets with
known ground-truth graph structures, we show that existing GTDL methods fail to
recover meaningful feature interactions. Moreover, enforcing the true
interaction structure improves predictive performance. This highlights the need
for GTDL methods to prioritize quantitative evaluation and accurate structural
learning. We call for a shift toward structure-aware modeling as a foundation
for building GTDL systems that are not only accurate but also interpretable,
trustworthy, and grounded in domain understanding.

</details>


### [394] [Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF--QP Safety Layer in Arbitrage-Free Markets](https://arxiv.org/abs/2510.04555)
*Jian'an Zhang*

Main category: cs.LG

TL;DR: Tail-Safe is a deployability-focused hedging framework that unifies distributional reinforcement learning with a white-box control-barrier-function safety layer for derivative hedging, delivering auditable telemetry and improved left-tail risk without sacrificing central performance in synthetic markets.


<details>
  <summary>Details</summary>
Motivation:  necesitates a practical, auditable, constraint-compliant hedging solution that can manage tail risk under model mismatch while enabling governance through telemetry and safety guarantees. The framework targets robust forward invariance of a safe set, feasible QP-based safety constraints, and interpretable audit trails for oversight.

Method:  Combines an IQN-based distributional critic with a CVaR objective (IQN--CVaR--PPO) and a Tail-Coverage Controller to regulate tail sampling via temperature tilting and tail boosting. The safety layer enforces discrete-time CBF inequalities with domain-specific constraints (ellipsoidal no-trade bands, box and rate limits, sign-consistency gate) solved as a convex QP, with telemetry used for governance. The work provides theoretical guarantees (robust forward invariance under bounded model mismatch, a minimal-deviation projection interpretation of the QP, a KL-to-DRO upper bound linking per-state KL regularization to worst-case CVaR, concentration and sample-complexity results for the estimator, a CVaR trust-region inequality under KL limits, and feasibility persistence under expiry-aware tightening) and empirical validation. 

Result:  Empirically evaluated in arbitrage-free, microstructure-aware synthetic markets (SSVI → Dupire → VIX with ABIDES/MockLOB execution). Tail-Safe reduces left-tail risk without degrading central performance and achieves zero hard-constraint violations whenever the QP is feasible with zero slack. Telemetry is mapped to governance dashboards and incident workflows to support explainability and auditability.

Conclusion: The framework offers a deployable, auditable approach to hedging that fuses distributional RL with a CBF-based safety layer, delivering robust tail risk reduction and governance-ready telemetry. Limitations include reliance on synthetic data and simplified execution to isolate methodological contributions.

Abstract: We introduce Tail-Safe, a deployability-oriented framework for derivatives
hedging that unifies distributional, risk-sensitive reinforcement learning with
a white-box control-barrier-function (CBF) quadratic-program (QP) safety layer
tailored to financial constraints. The learning component combines an IQN-based
distributional critic with a CVaR objective (IQN--CVaR--PPO) and a
Tail-Coverage Controller that regulates quantile sampling through temperature
tilting and tail boosting to stabilize small-$\alpha$ estimation. The safety
component enforces discrete-time CBF inequalities together with domain-specific
constraints -- ellipsoidal no-trade bands, box and rate limits, and a
sign-consistency gate -- solved as a convex QP whose telemetry (active sets,
tightness, rate utilization, gate scores, slack, and solver status) forms an
auditable trail for governance. We provide guarantees of robust forward
invariance of the safe set under bounded model mismatch, a minimal-deviation
projection interpretation of the QP, a KL-to-DRO upper bound linking per-state
KL regularization to worst-case CVaR, concentration and sample-complexity
results for the temperature-tilted CVaR estimator, and a CVaR trust-region
improvement inequality under KL limits, together with feasibility persistence
under expiry-aware tightening. Empirically, in arbitrage-free,
microstructure-aware synthetic markets (SSVI $\to$ Dupire $\to$ VIX with
ABIDES/MockLOB execution), Tail-Safe improves left-tail risk without degrading
central performance and yields zero hard-constraint violations whenever the QP
is feasible with zero slack. Telemetry is mapped to governance dashboards and
incident workflows to support explainability and auditability. Limitations
include reliance on synthetic data and simplified execution to isolate
methodological contributions.

</details>


### [395] [Challenger-Based Combinatorial Bandits for Subcarrier Selection in OFDM Systems](https://arxiv.org/abs/2510.04559)
*Mohsen Amiri,V Venktesh,Sindri Magnússon*

Main category: cs.LG

TL;DR: A gap-index shortlist framework for efficient pure-exploration of top-m user-subsets in stochastic linear bandits for multi-user MIMO downlink, enabling fast, accurate identification with tunable speed-accuracy and practical online subcarrier selection.


<details>
  <summary>Details</summary>
Motivation: Exponential action space in selecting top-m user subsets for MIMO downlink makes exhaustive search infeasible; a fast, reliable identification method via informative exploration is needed.

Method: Linear utility model; gap-index framework with a champion shortlist and a rotating challenger shortlist; targeted, informative measurements focusing on gap-based comparisons to drive pure exploration.

Result: Significant reductions in runtime and computation compared with state-of-the-art linear bandit methods; high identification accuracy; simulations on realistic OFDM downlink demonstrate measurement-efficient subcarrier selection suitable for AI-enabled communications.

Conclusion: Shortlist-driven pure exploration enables scalable, practical identification of top-m user subsets in wireless downlink, offering a tunable speed–accuracy trade-off and enabling real-time, measurement-efficient AI-enabled communication systems.

Abstract: This paper investigates the identification of the top-m user-scheduling sets
in multi-user MIMO downlink, which is cast as a combinatorial pure-exploration
problem in stochastic linear bandits. Because the action space grows
exponentially, exhaustive search is infeasible. We therefore adopt a linear
utility model to enable efficient exploration and reliable selection of
promising user subsets. We introduce a gap-index framework that maintains a
shortlist of current estimates of champion arms (top-m sets) and a rotating
shortlist of challenger arms that pose the greatest threat to the champions.
This design focuses on measurements that yield the most informative
gap-index-based comparisons, resulting in significant reductions in runtime and
computation compared to state-of-the-art linear bandit methods, with high
identification accuracy. The method also exposes a tunable trade-off between
speed and accuracy. Simulations on a realistic OFDM downlink show that
shortlist-driven pure exploration makes online, measurement-efficient
subcarrier selection practical for AI-enabled communication systems.

</details>


### [396] [Stochastic Approximation Methods for Distortion Risk Measure Optimization](https://arxiv.org/abs/2510.04563)
*Jinyang Jiang,Bernd Heidergott,Jiaqiao Hu,Yijie Peng*

Main category: cs.LG

TL;DR: Gradient-descent algorithms for Distortion Risk Measures (DRMs) optimization using two dual representations. The DM-form uses a three-timescale scheme with GLR and kernel density estimation; the QF-form uses a two-timescale scheme without complex quantile gradient estimation. A hybrid form combines both. Theoretical convergence rates are established (DM-form: O(k^{-4/7}); QF-form: O(k^{-2/3})). Empirical results show improvements over baselines in robust portfolio tasks, with scalability demonstrated by integrating DRM with deep RL (PPO for multi-echelon inventory management).


<details>
  <summary>Details</summary>
Motivation: DRMs capture risk preferences under uncertainty and offer general risk criteria, but efficient, provable optimization algorithms are needed. The paper develops gradient-based methods with convergence guarantees to optimize DRMs in practice, enabling applications in finance and operations where robust decision-making and risk-sensitive optimization are critical.

Method: DM-form: a three-timescale algorithm to track quantiles, compute their gradients, and update decision variables. Uses Generalized Likelihood Ratio (GLR) and kernel-based density estimation for gradient estimation. QF-form: a simpler two-timescale approach avoiding complex quantile gradient estimation. Hybrid form: leverages DM-form for regions with distortion-function jumps and QF-form for smooth regions to balance robustness and efficiency.

Result: Theoretical results: strong convergence with explicit rates—DM-form achieves O(k^{-4/7}), QF-form achieves O(k^{-2/3}). Empirical results: substantial improvements over baselines in robust portfolio selection tasks; scalability validated via inserting DRM into deep RL (DRM-PPO) for multi-echelon dynamic inventory management.

Conclusion: Dual representations (DM and QF) for DRM optimization provide provable convergence and practical performance improvements. The hybrid framework combines robustness in jump regions with efficiency in smooth regions. The approach is scalable and applicable to real-world domains like robust finance and inventory management via DRL.

Abstract: Distortion Risk Measures (DRMs) capture risk preferences in decision-making
and serve as general criteria for managing uncertainty. This paper proposes
gradient descent algorithms for DRM optimization based on two dual
representations: the Distortion-Measure (DM) form and Quantile-Function (QF)
form. The DM-form employs a three-timescale algorithm to track quantiles,
compute their gradients, and update decision variables, utilizing the
Generalized Likelihood Ratio and kernel-based density estimation. The QF-form
provides a simpler two-timescale approach that avoids the need for complex
quantile gradient estimation. A hybrid form integrates both approaches,
applying the DM-form for robust performance around distortion function jumps
and the QF-form for efficiency in smooth regions. Proofs of strong convergence
and convergence rates for the proposed algorithms are provided. In particular,
the DM-form achieves an optimal rate of $O(k^{-4/7})$, while the QF-form
attains a faster rate of $O(k^{-2/3})$. Numerical experiments confirm their
effectiveness and demonstrate substantial improvements over baselines in robust
portfolio selection tasks. The method's scalability is further illustrated
through integration into deep reinforcement learning. Specifically, a DRM-based
Proximal Policy Optimization algorithm is developed and applied to
multi-echelon dynamic inventory management, showcasing its practical
applicability.

</details>


### [397] [GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning](https://arxiv.org/abs/2510.04567)
*Weishuo Ma,Yanbo Wang,Xiyuan Wang,Lei Zou,Muhan Zhang*

Main category: cs.LG

TL;DR: GILT provides a transformer-based, LLM-free and tuning-free in-context learning framework for graphs, using a token-based approach to unify node/edge/graph tasks and enable strong few-shot performance with high efficiency.


<details>
  <summary>Details</summary>
Motivation: Graph data are highly heterogeneous: graphs vary in feature spaces, label sets, and topology. Existing graph foundation models either rely on text (LLMs) and struggle with numerical features, or require costly per-graph tuning during pre-training, creating inefficiencies. A generic, efficient method that handles numeric features and heterogeneous tasks without tuning is needed.

Method: Introduce Graph In-context Learning Transformer (GILT), a token-based, LLM-free and tuning-free framework for in-context learning on graphs. It presents a unified approach to node, edge, and graph classification, operates on generic numerical features, and uses context to dynamically infer class semantics for adaptation without fine-tuning.

Result: Empirical results show GILT achieves stronger few-shot performance and requires significantly less time than LLM-based or tuning-based baselines, demonstrating its effectiveness and efficiency on heterogeneous graphs.

Conclusion: GILT offers a viable path toward LLM-free, tuning-free graph foundation models that can handle heterogeneous graph data and tasks with improved efficiency and competitive performance.

Abstract: Graph Neural Networks (GNNs) are powerful tools for precessing relational
data but often struggle to generalize to unseen graphs, giving rise to the
development of Graph Foundational Models (GFMs). However, current GFMs are
challenged by the extreme heterogeneity of graph data, where each graph can
possess a unique feature space, label set, and topology. To address this, two
main paradigms have emerged. The first leverages Large Language Models (LLMs),
but is fundamentally text-dependent, thus struggles to handle the numerical
features in vast graphs. The second pre-trains a structure-based model, but the
adaptation to new tasks typically requires a costly, per-graph tuning stage,
creating a critical efficiency bottleneck. In this work, we move beyond these
limitations and introduce \textbf{G}raph \textbf{I}n-context \textbf{L}earning
\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-free
architecture. GILT introduces a novel token-based framework for in-context
learning (ICL) on graphs, reframing classification tasks spanning node, edge
and graph levels in a unified framework. This mechanism is the key to handling
heterogeneity, as it is designed to operate on generic numerical features.
Further, its ability to understand class semantics dynamically from the context
enables tuning-free adaptation. Comprehensive experiments show that GILT
achieves stronger few-shot performance with significantly less time than
LLM-based or tuning-based baselines, validating the effectiveness of our
approach.

</details>


### [398] [LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning](https://arxiv.org/abs/2510.04573)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Nicklas Majamaki,Navdeep Jaitly,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: LaDiR introduces latent diffusion in a structured latent reasoning space to enable holistic, parallel, and iterative refinement of reasoning for LLMs, improving accuracy, diversity, and interpretability.


<details>
  <summary>Details</summary>
Motivation: Autoregressive decoding limits revisitation of earlier tokens and can hinder efficient exploration of diverse reasoning paths; a latent, diffusion-based approach could enable holistic planning and revision with adaptive compute.

Method: Construct a VAE-based latent reasoning space that encodes reasoning steps into blocks of thought tokens; use a latent diffusion model to denoise blocks with a blockwise bidirectional attention mask, enabling longer horizons and iterative refinement; allows parallel generation of diverse reasoning trajectories with adaptive test-time compute.

Result: Empirical evaluations on mathematical reasoning and planning benchmarks show that LaDiR yields improvements in accuracy, diversity, and interpretability over autoregressive, diffusion-based, and latent reasoning baselines.

Conclusion: LaDiR presents a new latent-diffusion-based paradigm for text reasoning with LLMs, enabling holistic revision and diverse planning trajectories and suggesting a path toward more robust, interpretable reasoning in large language models.

Abstract: Large Language Models (LLMs) demonstrate their reasoning ability through
chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may
limit the ability to revisit and refine earlier tokens in a holistic manner,
which can also lead to inefficient exploration for diverse solutions. In this
paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning
framework that unifies the expressiveness of continuous latent representation
with the iterative refinement capabilities of latent diffusion models for an
existing LLM. We first construct a structured latent reasoning space using a
Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of
thought tokens, preserving semantic information and interpretability while
offering compact but expressive representations. Subsequently, we utilize a
latent diffusion model that learns to denoise a block of latent thought tokens
with a blockwise bidirectional attention mask, enabling longer horizon and
iterative refinement with adaptive test-time compute. This design allows
efficient parallel generation of diverse reasoning trajectories, allowing the
model to plan and revise the reasoning process holistically. We conduct
evaluations on a suite of mathematical reasoning and planning benchmarks.
Empirical results show that LaDiR consistently improves accuracy, diversity,
and interpretability over existing autoregressive, diffusion-based, and latent
reasoning methods, revealing a new paradigm for text reasoning with latent
diffusion.

</details>


### [399] [Busemann Functions in the Wasserstein Space: Existence, Closed-Forms, and Applications to Slicing](https://arxiv.org/abs/2510.04579)
*Clément Bonet,Elsa Cazelles,Lucas Drumetz,Nicolas Courty*

Main category: cs.LG

TL;DR: Explicit Busemann functions in Wasserstein space for 1D and Gaussian distributions enable practical projection schemes and novel Sliced-Wasserstein distances for Gaussian mixtures, with empirical validation.


<details>
  <summary>Details</summary>
Motivation: The Busemann function provides natural projections onto geodesic rays and generalizes hyperplanes. Studying it in Wasserstein space leverages its formal Riemannian structure from Optimal Transport, enabling geometric ML tools for probability measures.

Method: Establish existence and computation of Busemann functions in Wasserstein space. Derive closed-form expressions in two cases: (i) one-dimensional distributions and (ii) Gaussian measures. Use these to construct explicit projection schemes for distributions on R, and define Sliced-Wasserstein distances over Gaussian mixtures and labeled datasets. Validate with synthetic experiments and transfer learning tasks.

Result: Derived closed-form Busemann expressions for the two key cases. Enabled explicit projection schemes for 1D distributions and Gaussian measures. Introduced novel Sliced-Wasserstein distances for Gaussian mixtures and labeled data, with empirical demonstrations of efficiency on synthetic datasets and transfer learning problems.

Conclusion: Busemann functions in Wasserstein space can be computed explicitly in important cases, yielding practical geometric tools for distributional ML (projection schemes, new Sliced-Wasserstein distances) and motivating extension to broader classes of measures.

Abstract: The Busemann function has recently found much interest in a variety of
geometric machine learning problems, as it naturally defines projections onto
geodesic rays of Riemannian manifolds and generalizes the notion of
hyperplanes. As several sources of data can be conveniently modeled as
probability distributions, it is natural to study this function in the
Wasserstein space, which carries a rich formal Riemannian structure induced by
Optimal Transport metrics. In this work, we investigate the existence and
computation of Busemann functions in Wasserstein space, which admits geodesic
rays. We establish closed-form expressions in two important cases:
one-dimensional distributions and Gaussian measures. These results enable
explicit projection schemes for probability distributions on $\mathbb{R}$,
which in turn allow us to define novel Sliced-Wasserstein distances over
Gaussian mixtures and labeled datasets. We demonstrate the efficiency of those
original schemes on synthetic datasets as well as transfer learning problems.

</details>


### [400] [Improved probabilistic regression using diffusion models](https://arxiv.org/abs/2510.04583)
*Carlo Kneissl,Christopher Bülte,Philipp Scholl,Gitta Kutyniok*

Main category: cs.LG

TL;DR: A diffusion-based probabilistic regression approach that learns the full diffusion-noise distribution nonparametrically to produce calibrated predictive distributions and improved accuracy across tasks.


<details>
  <summary>Details</summary>
Motivation: Probabilistic regression seeks full predictive distributions; diffusion models are powerful but their use in general regression lacks uncertainty evaluation and broad applicability; there is a need for a nonparametric, uncertainty-aware diffusion-based regression framework.

Method: Develop a diffusion-based framework that models the entire diffusion-noise distribution nonparametrically; explore different noise parameterizations and analyze trade-offs; evaluate on low- and high-dimensional regression tasks; compare against baselines and assess calibration of uncertainty.

Result: Across multiple experiments, the method outperforms existing baselines and provides well-calibrated uncertainty estimates, demonstrating versatility for probabilistic prediction.

Conclusion: The proposed framework offers a flexible, uncertainty-aware probabilistic regression approach with strong performance across diverse tasks, underscoring the value of modeling the full diffusion-noise distribution in diffusion-based regression.

Abstract: Probabilistic regression models the entire predictive distribution of a
response variable, offering richer insights than classical point estimates and
directly allowing for uncertainty quantification. While diffusion-based
generative models have shown remarkable success in generating complex,
high-dimensional data, their usage in general regression tasks often lacks
uncertainty-related evaluation and remains limited to domain-specific
applications. We propose a novel diffusion-based framework for probabilistic
regression that learns predictive distributions in a nonparametric way. More
specifically, we propose to model the full distribution of the diffusion noise,
enabling adaptation to diverse tasks and enhanced uncertainty quantification.
We investigate different noise parameterizations, analyze their trade-offs, and
evaluate our framework across a broad range of regression tasks, covering low-
and high-dimensional settings. For several experiments, our approach shows
superior performance against existing baselines, while delivering calibrated
uncertainty estimates, demonstrating its versatility as a tool for
probabilistic prediction.

</details>


### [401] [Closed-Form Last Layer Optimization](https://arxiv.org/abs/2510.04606)
*Alexandre Galashov,Nathaël Da Costa,Liyuan Xu,Philipp Hennig,Arthur Gretton*

Main category: cs.LG

TL;DR: Optimizing neural nets by keeping the last-layer weights in closed form under squared loss and optimizing only the backbone; with SGD adaptation, NTK convergence guarantees, and practical gains on regression/classification tasks (including Fourier Neural Operators and IV regression).


<details>
  <summary>Details</summary>
Motivation: To exploit the closed-form solution of the last layer under squared loss to improve optimization efficiency, accelerate convergence, and leverage information across batches; and to provide theoretical guarantees in the Neural Tangent Kernel regime.

Method: Treat the last layer as a function of the backbone parameters and update it in closed form, while alternating with gradient steps on the backbone. For SGD, balance the current batch loss with accumulated information from previous batches. Prove convergence in the Neural Tangent Kernel regime.

Result: The method converges to an optimal solution in the NTK regime. Empirically, it outperforms standard SGD with squared loss across several supervised tasks, including regression and classification, and is demonstrated on Fourier Neural Operators and Instrumental Variable Regression.

Conclusion: Integrating a closed-form last layer into SGD-based training yields efficient optimization with theoretical guarantees and practical performance gains across diverse tasks.

Abstract: Neural networks are typically optimized with variants of stochastic gradient
descent. Under a squared loss, however, the optimal solution to the linear last
layer weights is known in closed-form. We propose to leverage this during
optimization, treating the last layer as a function of the backbone parameters,
and optimizing solely for these parameters. We show this is equivalent to
alternating between gradient descent steps on the backbone and closed-form
updates on the last layer. We adapt the method for the setting of stochastic
gradient descent, by trading off the loss on the current batch against the
accumulated information from previous batches. Further, we prove that, in the
Neural Tangent Kernel regime, convergence of this method to an optimal solution
is guaranteed. Finally, we demonstrate the effectiveness of our approach
compared with standard SGD on a squared loss in several supervised tasks --
both regression and classification -- including Fourier Neural Operators and
Instrumental Variable Regression.

</details>


### [402] [Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models](https://arxiv.org/abs/2510.04618)
*Qizheng Zhang,Changran Hu,Shubhangi Upasani,Boyuan Ma,Fenglu Hong,Vamsidhar Kamanuru,Jay Rainton,Chen Wu,Mengmeng Ji,Hanchen Li,Urmish Thakker,James Zou,Kunle Olukotun*

Main category: cs.LG

TL;DR: ACE is a modular, adaptive context-engineering framework for LLMs that evolves playbooks of strategies to prevent context collapse and brevity bias, improving agent/domain-specific performance with unsupervised adaptation and low overhead.


<details>
  <summary>Details</summary>
Motivation: To address brevity bias and context collapse in context-adaptive LLM systems and to enable scalable, evolving contexts that retain detailed knowledge across long interactions.

Method: Treat contexts as evolving playbooks updated via a generation-reflection-curation loop; offline (system prompts) and online (agent memory) optimization; uses structured incremental updates to prevent collapse; leverages long-context models; can operate with unsupervised natural execution feedback.

Result: Outperforms baselines: +10.6% on agents and +8.6% on finance; reduces adaptation latency and rollout cost; matches top production-level agent on AppWorld overall and exceeds it on hard split; effective with smaller open-source model.

Conclusion: Comprehensive, evolving contexts enable scalable, efficient, and self-improving LLM systems with low overhead.

Abstract: Large language model (LLM) applications such as agents and domain-specific
reasoning increasingly rely on context adaptation -- modifying inputs with
instructions, strategies, or evidence, rather than weight updates. Prior
approaches improve usability but often suffer from brevity bias, which drops
domain insights for concise summaries, and from context collapse, where
iterative rewriting erodes details over time. Building on the adaptive memory
introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context
Engineering), a framework that treats contexts as evolving playbooks that
accumulate, refine, and organize strategies through a modular process of
generation, reflection, and curation. ACE prevents collapse with structured,
incremental updates that preserve detailed knowledge and scale with
long-context models. Across agent and domain-specific benchmarks, ACE optimizes
contexts both offline (e.g., system prompts) and online (e.g., agent memory),
consistently outperforming strong baselines: +10.6% on agents and +8.6% on
finance, while significantly reducing adaptation latency and rollout cost.
Notably, ACE could adapt effectively without labeled supervision and instead by
leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches
the top-ranked production-level agent on the overall average and surpasses it
on the harder test-challenge split, despite using a smaller open-source model.
These results show that comprehensive, evolving contexts enable scalable,
efficient, and self-improving LLM systems with low overhead.

</details>


### [403] [Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI](https://arxiv.org/abs/2510.04622)
*Youngjoon Lee,Seongmin Cho,Yehhyun Jo,Jinu Gong,Hyunjoo Jenny Lee,Joonhyuk Kang*

Main category: cs.LG

TL;DR: Proposes a forecasting-model–based framework to generate high-fidelity synthetic biomedical time-series (EEG/EMG) that preserves temporal/spectral properties, addressing privacy and data scarcity while enabling AI research and open-source dissemination.


<details>
  <summary>Details</summary>
Motivation: Biomedical time-series research is hampered by strict privacy regulations and high data requirements, limiting data access and sharing. Synthetic data that retain essential statistics can alleviate privacy concerns and data scarcity.

Method: Develop a synthetic data generation framework using advanced forecasting models to accurately replicate complex electrophysiological signals (EEG/EMG), preserving temporal and spectral properties, with scalable, open-source integration.

Result: Empirical evaluations across multiple subjects show that the synthetic data can substitute for real data and can significantly boost AI model performance while maintaining key biomedical features.

Conclusion: The framework offers a scalable, privacy-preserving data generation approach that expands resources for AI-driven biomedical research and facilitates open-source collaboration.

Abstract: The limited data availability due to strict privacy regulations and
significant resource demands severely constrains biomedical time-series AI
development, which creates a critical gap between data requirements and
accessibility. Synthetic data generation presents a promising solution by
producing artificial datasets that maintain the statistical properties of real
biomedical time-series data without compromising patient confidentiality. We
propose a framework for synthetic biomedical time-series data generation based
on advanced forecasting models that accurately replicates complex
electrophysiological signals such as EEG and EMG with high fidelity. These
synthetic datasets preserve essential temporal and spectral properties of real
data, which enables robust analysis while effectively addressing data scarcity
and privacy challenges. Our evaluations across multiple subjects demonstrate
that the generated synthetic data can serve as an effective substitute for real
data and also significantly boost AI model performance. The approach maintains
critical biomedical features while provides high scalability for various
applications and integrates seamlessly into open-source repositories,
substantially expanding resources for AI-driven biomedical research.

</details>


### [404] [Compressed Concatenation of Small Embedding Models](https://arxiv.org/abs/2510.04626)
*Mohamed Ayoub Ben Ayad,Michael Dinzinger,Kanishka Ghosh Dastidar,Jelena Mitrovic,Michael Granitzer*

Main category: cs.LG

TL;DR: A concat-encode-quantize pipeline using four small embedding models and a lightweight Matryoshka Representation Learning (MRL) decoder recovers 89% of original retrieval performance with 48x compression; gains from more models saturate.


<details>
  <summary>Details</summary>
Motivation: Embedding models are central to dense retrieval and recommendations but their size hinders deployment in resource-constrained environments (browsers, edge devices). Smaller models help but underperform; the paper explores combining multiple small models to boost performance while enabling compression.

Method: Concatenate raw embeddings from multiple small models to form a high-dimensional joint representation, then train a lightweight unified decoder via Matryoshka Representation Learning (MRL) loss to map to a low-dimensional space. Evaluate the pipeline on standard retrieval benchmarks (MTEB subset), examine robustness under compression and quantization, and study the effect of increasing the number of base models.

Result: The concat-encode-quantize pipeline recovers 89% of the original performance with a 48x compression factor when using four small embedding models. Adding more base models yields diminishing returns, while the decoder’s representation remains robust under further compression and quantization.

Conclusion: Concatenating multiple small models paired with an MRL-trained decoder enables near-large-model retrieval performance with substantial compression, offering a practical route for deploying effective dense retrieval in resource-constrained settings without fine-tuning the base models.

Abstract: Embedding models are central to dense retrieval, semantic search, and
recommendation systems, but their size often makes them impractical to deploy
in resource-constrained environments such as browsers or edge devices. While
smaller embedding models offer practical advantages, they typically
underperform compared to their larger counterparts. To bridge this gap, we
demonstrate that concatenating the raw embedding vectors of multiple small
models can outperform a single larger baseline on standard retrieval
benchmarks. To overcome the resulting high dimensionality of naive
concatenation, we introduce a lightweight unified decoder trained with a
Matryoshka Representation Learning (MRL) loss. This decoder maps the
high-dimensional joint representation to a low-dimensional space, preserving
most of the original performance without fine-tuning the base models. We also
show that while concatenating more base models yields diminishing gains, the
robustness of the decoder's representation under compression and quantization
improves. Our experiments show that, on a subset of MTEB retrieval tasks, our
concat-encode-quantize pipeline recovers 89\% of the original performance with
a 48x compression factor when the pipeline is applied to a concatenation of
four small embedding models.

</details>


### [405] [Predictive Feature Caching for Training-free Acceleration of Molecular Geometry Generation](https://arxiv.org/abs/2510.04646)
*Johanna Sommer,John Rachwan,Nils Fleischmann,Stephan Günnemann,Bertrand Charpentier*

Main category: cs.LG

TL;DR: Training-free caching of intermediate solver states accelerates SE(3)-equivariant flow-based molecular geometry generation, doubling inference speed with matched quality and up to 3x faster than the base model; gains compound with other optimizations (up to ~7x).


<details>
  <summary>Details</summary>
Motivation: Inference in flow-based molecular geometry models is computationally expensive, with hundreds of network evaluations per sample, making large-scale sampling impractical. A training-free caching strategy could substantially reduce wall-clock costs while maintaining quality.

Method: Introduce a training-free caching mechanism that stores/predicts intermediate hidden states across solver steps directly on the SE(3)-equivariant backbone. The approach is compatible with pretrained models and orthogonal to training-based accelerations and system-level optimizations.

Result: On GEOM-Drugs, caching yields a twofold reduction in wall-clock inference time at matched sample quality and up to 3x speedup over the base model with minimal quality loss. When combined with other lossless optimizations, the speedup can reach ~7x.

Conclusion: Caching provides a simple, effective, training-free acceleration for flow-based molecular geometry generation, is compatible with pretrained backbones, and serves as a complementary component alongside other optimizations to enable faster large-scale sampling.

Abstract: Flow matching models generate high-fidelity molecular geometries but incur
significant computational costs during inference, requiring hundreds of network
evaluations. This inference overhead becomes the primary bottleneck when such
models are employed in practice to sample large numbers of molecular
candidates. This work discusses a training-free caching strategy that
accelerates molecular geometry generation by predicting intermediate hidden
states across solver steps. The proposed method operates directly on the
SE(3)-equivariant backbone, is compatible with pretrained models, and is
orthogonal to existing training-based accelerations and system-level
optimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching
achieves a twofold reduction in wall-clock inference time at matched sample
quality and a speedup of up to 3x compared to the base model with minimal
sample quality degradation. Because these gains compound with other
optimizations, applying caching alongside other general, lossless optimizations
yield as much as a 7x speedup.

</details>


### [406] [IMLP: An Energy-Efficient Continual Learning Method for Tabular Data Streams](https://arxiv.org/abs/2510.04660)
*Yuandou Wang,Filip Gunnarsson,Rihan Hai*

Main category: cs.LG

TL;DR: A compact continual learner for tabular data streams that uses a sliding latent buffer and attention to enable constant-memory, energy-efficient continual learning, with a NetScore-T metric and competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: Edge/mobile settings require energy- and memory-efficient continual learning for tabular data streams; existing CL methods rely on growing replay buffers that increase resource use.

Method: Context-aware incremental MLP (IMLP) with windowed scaled dot-product attention over a sliding latent feature buffer; constant-size memory; attended context concatenated with current features and processed by shared feed-forward layers; no raw data storage; per-segment updates; introduces NetScore-T to balance accuracy and energy.

Result: IMLP achieves up to 27.6x energy efficiency vs TabNet and 85.5x vs TabPFN with competitive average accuracy.

Conclusion: IMLP offers an easy-to-deploy, energy-efficient alternative to full retraining for tabular data streams on resource-constrained devices.

Abstract: Tabular data streams are rapidly emerging as a dominant modality for
real-time decision-making in healthcare, finance, and the Internet of Things
(IoT). These applications commonly run on edge and mobile devices, where energy
budgets, memory, and compute are strictly limited. Continual learning (CL)
addresses such dynamics by training models sequentially on task streams while
preserving prior knowledge and consolidating new knowledge. While recent CL
work has advanced in mitigating catastrophic forgetting and improving knowledge
transfer, the practical requirements of energy and memory efficiency for
tabular data streams remain underexplored. In particular, existing CL solutions
mostly depend on replay mechanisms whose buffers grow over time and exacerbate
resource costs.
  We propose a context-aware incremental Multi-Layer Perceptron (IMLP), a
compact continual learner for tabular data streams. IMLP incorporates a
windowed scaled dot-product attention over a sliding latent feature buffer,
enabling constant-size memory and avoiding storing raw data. The attended
context is concatenated with current features and processed by shared
feed-forward layers, yielding lightweight per-segment updates. To assess
practical deployability, we introduce NetScore-T, a tunable metric coupling
balanced accuracy with energy for Pareto-aware comparison across models and
datasets. IMLP achieves up to $27.6\times$ higher energy efficiency than TabNet
and $85.5\times$ higher than TabPFN, while maintaining competitive average
accuracy. Overall, IMLP provides an easy-to-deploy, energy-efficient
alternative to full retraining for tabular data streams.

</details>


### [407] [Noise or Signal? Deconstructing Contradictions and An Adaptive Remedy for Reversible Normalization in Time Series Forecasting](https://arxiv.org/abs/2510.04667)
*Fanzhe Fu,Yang Yang*

Main category: cs.LG

TL;DR: R^2-IN outperforms RevIN and its adaptive A-IN fails; RevIN collapses on extreme outliers, highlighting the need for diagnostics-driven normalization rather than blind complexity.


<details>
  <summary>Details</summary>
Motivation: To dissect why normalization strategies in time-series forecasting behave unexpectedly, uncover underlying theoretical contradictions, and guide principled use of normalization.

Method: Identify four theoretical contradictions in normalization approaches; conduct controlled experiments comparing RevIN, R^2-IN, and an adaptive A-IN across datasets with extreme outliers; perform diagnostics-driven analysis to interpret performance.

Result: RevIN catastrophically fails on extreme-outlier data (MSE up by 683%); R^2-IN prevents this failure and often achieves the best overall performance; the adaptive A-IN fails completely due to instability from the heuristic; highlights that naive adaptation can be more damaging than the statistical issues it aims to solve.

Conclusion: Advocates a diagnostics-driven paradigm for time-series normalization, showing that simple baselines can be surprisingly powerful and that naive complexity can be perilous; urges moving away from blind search for complexity toward principled diagnostic analysis.

Abstract: Reversible Instance Normalization (RevIN) is a key technique enabling simple
linear models to achieve state-of-the-art performance in time series
forecasting. While replacing its non-robust statistics with robust counterparts
(termed R$^2$-IN) seems like a straightforward improvement, our findings reveal
a far more complex reality. This paper deconstructs the perplexing performance
of various normalization strategies by identifying four underlying theoretical
contradictions. Our experiments provide two crucial findings: first, the
standard RevIN catastrophically fails on datasets with extreme outliers, where
its MSE surges by a staggering 683\%. Second, while the simple R$^2$-IN
prevents this failure and unexpectedly emerges as the best overall performer,
our adaptive model (A-IN), designed to test a diagnostics-driven heuristic,
unexpectedly suffers a complete and systemic failure. This surprising outcome
uncovers a critical, overlooked pitfall in time series analysis: the
instability introduced by a simple or counter-intuitive heuristic can be more
damaging than the statistical issues it aims to solve. The core contribution of
this work is thus a new, cautionary paradigm for time series normalization: a
shift from a blind search for complexity to a diagnostics-driven analysis that
reveals not only the surprising power of simple baselines but also the perilous
nature of naive adaptation.

</details>


### [408] [Semantic Channel Equalization Strategies for Deep Joint Source-Channel Coding](https://arxiv.org/abs/2510.04674)
*Lorenzo Pannacci,Simone Fiorellino,Mario Edoardo Pandolfo,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.LG

TL;DR: Proposes semantic channel equalization to align heterogeneous latent spaces in DeepJSCC, using linear maps, lightweight neural nets, and Parseval-frame equalizers; demonstrates trade-offs and provides deployment guidelines.


<details>
  <summary>Details</summary>
Motivation: In multi-vendor DeepJSCC, transmitter and receiver latent spaces mismatch leads to semantic noise, harming reconstruction and task performance; need methods to align latent spaces under both channel and semantic impairments.

Method: Introduce an extra processing stage—semantic channel equalizer—at RX to align latent spaces. Evaluate three aligners: (i) linear maps with closed-form solutions; (ii) small neural networks; (iii) Parseval-frame equalizer that works zero-shot. Test over AWGN and fading channels on image reconstruction; compare complexity, data efficiency, fidelity.

Result: All three aligners mitigate semantic noise and improve reconstruction/task performance relative to no alignment; linear maps are simple with closed-form fitting; lightweight NN offers more expressiveness; Parseval-frame equalizer enables zero-training operation but may have performance trade-offs. The study maps trade-offs and provides deployment guidelines.

Conclusion: Semantic channel equalization enables cross-vendor DeepJSCC in heterogeneous AI-native wireless networks; practitioners can select aligner class based on constraints; future work to integrate with broader semantic communication stacks and real-world deployments.

Abstract: Deep joint source-channel coding (DeepJSCC) has emerged as a powerful
paradigm for end-to-end semantic communications, jointly learning to compress
and protect task-relevant features over noisy channels. However, existing
DeepJSCC schemes assume a shared latent space at transmitter (TX) and receiver
(RX) - an assumption that fails in multi-vendor deployments where encoders and
decoders cannot be co-trained. This mismatch introduces "semantic noise",
degrading reconstruction quality and downstream task performance. In this
paper, we systematize and evaluate methods for semantic channel equalization
for DeepJSCC, introducing an additional processing stage that aligns
heterogeneous latent spaces under both physical and semantic impairments. We
investigate three classes of aligners: (i) linear maps, which admit closed-form
solutions; (ii) lightweight neural networks, offering greater expressiveness;
and (iii) a Parseval-frame equalizer, which operates in zero-shot mode without
the need for training. Through extensive experiments on image reconstruction
over AWGN and fading channels, we quantify trade-offs among complexity, data
efficiency, and fidelity, providing guidelines for deploying DeepJSCC in
heterogeneous AI-native wireless networks.

</details>


### [409] [Counterfactual Credit Guided Bayesian Optimization](https://arxiv.org/abs/2510.04676)
*Qiyu Wei,Haowei Wang,Richard Allmendinger,Mauricio A. Álvarez*

Main category: cs.LG

TL;DR: CCGBO uses counterfactual credit to weight historical observations in the acquisition function, prioritizing samples that are more informative for finding the global optimum and achieving faster convergence with sublinear regret.


<details>
  <summary>Details</summary>
Motivation: In expensive Bayesian optimization, building a perfect global surrogate is often unnecessary or costly; due to aleatoric noise and dependence on initialization, not all observations equally help locate the optimum. A method that quantifies each observation's contribution can guide more efficient sampling.

Method: Introduce counterfactual credit: assign a credit score to each past observation representing its marginal impact on discovering the optimum; embed this credit into the acquisition function to bias sampling toward regions with higher potential for the global optimum; provide theoretical sublinear regret guarantees; validate on synthetic and real benchmarks.

Result: Empirically, CCGBO consistently reduces simple regret and accelerates convergence to the global optimum across tested benchmarks.

Conclusion: Quantifying observation-level contribution via counterfactual credit yields more efficient Bayesian optimization with theoretical guarantees, enabling faster optimum discovery and offering a framework adaptable to other surrogate models and settings.

Abstract: Bayesian optimization has emerged as a prominent methodology for optimizing
expensive black-box functions by leveraging Gaussian process surrogates, which
focus on capturing the global characteristics of the objective function.
However, in numerous practical scenarios, the primary objective is not to
construct an exhaustive global surrogate, but rather to quickly pinpoint the
global optimum. Due to the aleatoric nature of the sequential optimization
problem and its dependence on the quality of the surrogate model and the
initial design, it is restrictive to assume that all observed samples
contribute equally to the discovery of the optimum in this context. In this
paper, we introduce Counterfactual Credit Guided Bayesian Optimization (CCGBO),
a novel framework that explicitly quantifies the contribution of individual
historical observations through counterfactual credit. By incorporating
counterfactual credit into the acquisition function, our approach can
selectively allocate resources in areas where optimal solutions are most likely
to occur. We prove that CCGBO retains sublinear regret. Empirical evaluations
on various synthetic and real-world benchmarks demonstrate that CCGBO
consistently reduces simple regret and accelerates convergence to the global
optimum.

</details>


### [410] [Parameter-free Algorithms for the Stochastically Extended Adversarial Model](https://arxiv.org/abs/2510.04685)
*Shuche Wang,Adarsh Barik,Peng Zhao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: Introduces the first parameter-free SEA (Stochastically Extended Adversarial) algorithms that remove dependence on problem parameters like domain diameter and Lipschitz constant using the Optimistic Online Newton Step, achieving comparator- and Lipschitz-adaptive regret.


<details>
  <summary>Details</summary>
Motivation: SEA bridges adversarial and stochastic online convex optimization, but prior work requires knowledge of problem-specific parameters (domain diameter D and Lipschitz constant G). The paper aims to eliminate these requirements and provide adaptive guarantees.

Method: Employ Optimistic Online Newton Step (OONS) to construct parameter-free updates. First, a comparator-adaptive algorithm for unknown D but known G; then extend to the general setting with both unknown D and G. The approach yields regret bounds that adapt to the comparator and to the (unknown) Lipschitz constant, with regret depending on cumulative stochastic variance σ^2_{1:T} and cumulative adversarial variation Σ^2_{1:T}.

Result: Achieves a comparator- and Lipschitz-adaptive regret bound of tilde O( ||u||_2^2 + ||u||_2 ( sqrt(σ^2_{1:T}) + sqrt(Σ^2_{1:T}) ) ) for the SEA model, matching dependence on σ^2 and Σ^2 even when D and G are unknown.

Conclusion: The work demonstrates the efficacy of parameter-free SEA algorithms, enabling practical deployment without knowledge of domain size or Lipschitz constants, while preserving adaptive guarantees that depend only on stochastic variance and adversarial variation.

Abstract: We develop the first parameter-free algorithms for the Stochastically
Extended Adversarial (SEA) model, a framework that bridges adversarial and
stochastic online convex optimization. Existing approaches for the SEA model
require prior knowledge of problem-specific parameters, such as the diameter of
the domain $D$ and the Lipschitz constant of the loss functions $G$, which
limits their practical applicability. Addressing this, we develop
parameter-free methods by leveraging the Optimistic Online Newton Step (OONS)
algorithm to eliminate the need for these parameters. We first establish a
comparator-adaptive algorithm for the scenario with unknown domain diameter but
known Lipschitz constant, achieving an expected regret bound of
$\tilde{O}\big(\|u\|_2^2 + \|u\|_2(\sqrt{\sigma^2_{1:T}} +
\sqrt{\Sigma^2_{1:T}})\big)$, where $u$ is the comparator vector and
$\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$ represent the cumulative stochastic
variance and cumulative adversarial variation, respectively. We then extend
this to the more general setting where both $D$ and $G$ are unknown, attaining
the comparator- and Lipschitz-adaptive algorithm. Notably, the regret bound
exhibits the same dependence on $\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$,
demonstrating the efficacy of our proposed methods even when both parameters
are unknown in the SEA model.

</details>


### [411] [How does the optimizer implicitly bias the model merging loss landscape?](https://arxiv.org/abs/2510.04686)
*Chenxiang Zhang,Alexander Theus,Damien Teney,Antonio Orvieto,Jun Pang,Sjouke Mauw*

Main category: cs.LG

TL;DR: Effective noise scale governs when independently trained models can be merged; merging performance is non-monotonic in this scale with a distinct optimum, influenced by learning rate, weight decay, batch size, and data augmentation.


<details>
  <summary>Details</summary>
Motivation: To understand what makes model merging work and to unify disparate observations under a single quantitative driver, extending beyond prior links to flat minima or generalization.

Method: The authors analyze how optimization-induced noise shapes the loss landscape, decompose the effective noise scale, and empirically study merging methods (linear interpolation and task arithmetic) across architectures and datasets while varying training hyperparameters and data augmentation.

Result: Merging effectiveness follows a non-monotonic curve with respect to the effective noise scale, exhibiting a distinct optimum; larger learning rates, stronger weight decay, smaller batch sizes, and data augmentation modulate the scale in ways that improve merging; the effective noise scale also correlates with global loss landscape geometry, enabling prediction of when merging succeeds.

Conclusion: The effective noise scale is a unifying quantity linking optimizer choices and data effects to merging success, suggesting that training dynamics can be tuned to enhance the viability of merging multiple models.

Abstract: Model merging methods combine models with different capabilities into a
single one while maintaining the same inference cost. Two popular approaches
are linear interpolation, which linearly interpolates between model weights,
and task arithmetic, which combines task vectors obtained by the difference
between finetuned and base models. While useful in practice, what properties
make merging effective are poorly understood. This paper explores how the
optimization process affects the loss landscape geometry and its impact on
merging success. We show that a single quantity -- the effective noise scale --
unifies the impact of optimizer and data choices on model merging. Across
architectures and datasets, the effectiveness of merging success is a
non-monotonic function of effective noise, with a distinct optimum. Decomposing
this quantity, we find that larger learning rates, stronger weight decay,
smaller batch sizes, and data augmentation all independently modulate the
effective noise scale, exhibiting the same qualitative trend. Unlike prior work
that connects optimizer noise to the flatness or generalization of individual
minima, we show that it also affects the global loss landscape, predicting when
independently trained solutions can be merged. Our findings broaden the
understanding of how optimization shapes the loss landscape geometry and its
downstream consequences for model merging, suggesting the possibility of
further manipulating the training dynamics to improve merging effectiveness.

</details>


### [412] [ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts](https://arxiv.org/abs/2510.04710)
*Zexin Wang,Changhua Pei,Yang Liu,Hengyue Jiang,Quan Zhou,Haotian Si,Hang Cui,Jianhui Li,Gaogang Xie,Jingjing Li,Dan Pei*

Main category: cs.LG

TL;DR: A Vision-Language Model (ViTs) converts time-series curves into images to enable train-once, inference across varying-length sequences for KPI anomaly detection; uses evolutionary-generated image-text pairs and a three-stage training pipeline; shows improved anomaly detection; code to be released.


<details>
  <summary>Details</summary>
Motivation: Need zero-shot/generalization across scenarios and varying-length sequences in time-series anomaly detection. LLMs face context-length limits and sliding-window approaches constrain inference to fixed lengths.

Method: Convert time-series curves into rescaled visual representations to preserve temporal dependencies while maintaining fixed input size. Use a Vision-Language Model (ViMs) framework with an evolutionary algorithm to generate thousands of aligned time-series image-text pairs. Train in three stages: (1) time-series knowledge injection, (2) anomaly detection enhancement, (3) anomaly reasoning refinement.

Result: Extensive experiments show ViTs substantially enhance the ability of VLMs to understand and detect anomalies in time-series data.

Conclusion: ViTs improve zero-shot, cross-scenario anomaly detection for time-series data and enable processing of arbitrarily long sequences without retraining; datasets and code will be released to support further research.

Abstract: Web service administrators must ensure the stability of multiple systems by
promptly detecting anomalies in Key Performance Indicators (KPIs). Achieving
the goal of "train once, infer across scenarios" remains a fundamental
challenge for time series anomaly detection models. Beyond improving zero-shot
generalization, such models must also flexibly handle sequences of varying
lengths during inference, ranging from one hour to one week, without
retraining. Conventional approaches rely on sliding-window encoding and
self-supervised learning, which restrict inference to fixed-length inputs.
Large Language Models (LLMs) have demonstrated remarkable zero-shot
capabilities across general domains. However, when applied to time series data,
they face inherent limitations due to context length. To address this issue, we
propose ViTs, a Vision-Language Model (VLM)-based framework that converts time
series curves into visual representations. By rescaling time series images,
temporal dependencies are preserved while maintaining a consistent input size,
thereby enabling efficient processing of arbitrarily long sequences without
context constraints. Training VLMs for this purpose introduces unique
challenges, primarily due to the scarcity of aligned time series image-text
data. To overcome this, we employ an evolutionary algorithm to automatically
generate thousands of high-quality image-text pairs and design a three-stage
training pipeline consisting of: (1) time series knowledge injection, (2)
anomaly detection enhancement, and (3) anomaly reasoning refinement. Extensive
experiments demonstrate that ViTs substantially enhance the ability of VLMs to
understand and detect anomalies in time series data. All datasets and code will
be publicly released at: https://anonymous.4open.science/r/ViTs-C484/.

</details>


### [413] [Directional Sheaf Hypergraph Networks: Unifying Learning on Directed and Undirected Hypergraphs](https://arxiv.org/abs/2510.04727)
*Emanuele Mule,Stefano Fiorini,Antonio Purificato,Federico Siciliano,Stefano Coniglio,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: DSHN uses Cellular Sheaves to model directed hypergraphs via a complex-valued Directed Sheaf Hypergraph Laplacian, unifying prior Laplacians and achieving 2–20% relative accuracy gains on 7 real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Directed hypergraphs capture oriented, higher-order interactions and often exhibit heterophily; existing methods tend to bias toward homophily and undirected formulations fail to capture directionality. A principled, algebraic-topology-based approach is needed to model asymmetry and higher-order structure.

Method: Propose Directional Sheaf Hypergraph Networks (DSHN) that integrate sheaf theory with directed hypergraphs. Define a Directed Sheaf Hypergraph Laplacian (a complex-valued operator) that unifies and generalizes many existing graph- and hypergraph-Laplacian matrices. Build a neural model around this operator and evaluate on datasets.

Result: Empirical evaluation on 7 real-world datasets against 13 baselines shows relative accuracy gains ranging from 2% to 20%, demonstrating the effectiveness of incorporating directionality and the sheaf-based representation.

Conclusion: A principled directional treatment via sheaves and the associated Laplacian improves learning on directed hypergraphs and generalizes existing Laplacians, with substantial empirical gains.

Abstract: Hypergraphs provide a natural way to represent higher-order interactions
among multiple entities. While undirected hypergraphs have been extensively
studied, the case of directed hypergraphs, which can model oriented group
interactions, remains largely under-explored despite its relevance for many
applications. Recent approaches in this direction often exhibit an implicit
bias toward homophily, which limits their effectiveness in heterophilic
settings. Rooted in the algebraic topology notion of Cellular Sheaves, Sheaf
Neural Networks (SNNs) were introduced as an effective solution to circumvent
such a drawback. While a generalization to hypergraphs is known, it is only
suitable for undirected hypergraphs, failing to tackle the directed case. In
this work, we introduce Directional Sheaf Hypergraph Networks (DSHN), a
framework integrating sheaf theory with a principled treatment of asymmetric
relations within a hypergraph. From it, we construct the Directed Sheaf
Hypergraph Laplacian, a complex-valued operator by which we unify and
generalize many existing Laplacian matrices proposed in the graph- and
hypergraph-learning literature. Across 7 real-world datasets and against 13
baselines, DSHN achieves relative accuracy gains from 2% up to 20%, showing how
a principled treatment of directionality in hypergraphs, combined with the
expressive power of sheaves, can substantially improve performance.

</details>


### [414] [EVaR-Optimal Arm Identification in Bandits](https://arxiv.org/abs/2510.04728)
*Mehrasa Ahmadipour,Aurélien Garivier*

Main category: cs.LG

TL;DR: Proposes a delta-correct Track-and-Stop algorithm for fixed-confidence best arm identification under the Entropic Value-at-Risk (EVaR) criterion in a nonparametric, bounded-reward MAB setting, with an asymptotically tight lower bound on sample complexity.


<details>
  <summary>Details</summary>
Motivation: Addresses risk-averse decision-making in high-stakes contexts (e.g., finance) by extending BAI from expected value to the EVaR risk measure, and seeks tractable, provably optimal algorithms.

Method: Develops a delta-correct Track-and-Stop algorithm tailored to EVaR-based BAI; derives a lower bound on expected sample complexity; both the algorithm and bound require solving a pair of optimization problems (one convex and a related non-convex problem) to compute sampling strategies.

Result: Proves that the lower bound is asymptotically matched by the proposed algorithm; provides a practical computational framework via convex (and a simpler non-convex) optimization to implement both the algorithm and the bound.

Conclusion: Shows that EVaR-based fixed-confidence BAI is theoretically tractable with a tight, asymptotically optimal sample complexity, and that practical implementation hinges on solving structured optimization problems.

Abstract: We study the fixed-confidence best arm identification (BAI) problem within
the multi-armed bandit (MAB) framework under the Entropic Value-at-Risk (EVaR)
criterion. Our analysis considers a nonparametric setting, allowing for general
reward distributions bounded in [0,1]. This formulation addresses the critical
need for risk-averse decision-making in high-stakes environments, such as
finance, moving beyond simple expected value optimization. We propose a
$\delta$-correct, Track-and-Stop based algorithm and derive a corresponding
lower bound on the expected sample complexity, which we prove is asymptotically
matched. The implementation of our algorithm and the characterization of the
lower bound both require solving a complex convex optimization problem and a
related, simpler non-convex one.

</details>


### [415] [Provable Affine Identifiability of Nonlinear CCA under Latent Distributional Priors](https://arxiv.org/abs/2510.04758)
*Zhiwei Han,Stefan Matthes,Hao Shen*

Main category: cs.LG

TL;DR: Nonlinear CCA with whitening yields affine identifiability of latent factors up to an orthogonal transform in the population; ridge-regularized empirical CCA provides finite-sample convergence to this population behavior. Experiments validate the theory and highlight the necessity of whitening and the modeling assumptions.


<details>
  <summary>Details</summary>
Motivation: Extend identifiability of canonical correlation analysis from linear Gaussian settings to nonlinear latent factors, establishing when ground-truth sources can be recovered up to orthogonal transforms and under what finite-sample guarantees.

Method: Prove an affine identifiability result via a reparameterization that shifts analysis from observation space to latent source space; show whitening is essential for boundedness and well-conditioning; analyze ridge-regularized empirical CCA to connect population results to finite samples.

Result: Ground-truth latent factors are recoverable up to an orthogonal transform after whitening; affine identifiability holds in the population for a broad class of latent distributions; ridge-regularized CCA converges to its population counterpart; experiments on synthetic and rendered image data validate theory and demonstrate the role of assumptions through ablations.

Conclusion: Whitening and a source-space reparameterization enable identifiability in nonlinear CCA under broad priors; finite-sample guarantees follow from ridge-regularization, with empirical results supporting the theory and the necessity of key assumptions.

Abstract: In this work, we establish conditions under which nonlinear CCA recovers the
ground-truth latent factors up to an orthogonal transform after whitening.
Building on the classical result that linear mappings maximize canonical
correlations under Gaussian priors, we prove affine identifiability for a broad
class of latent distributions in the population setting. Central to our proof
is a reparameterization result that transports the analysis from observation
space to source space, where identifiability becomes tractable. We further show
that whitening is essential for ensuring boundedness and well-conditioning,
thereby underpinning identifiability. Beyond the population setting, we prove
that ridge-regularized empirical CCA converges to its population counterpart,
transferring these guarantees to the finite-sample regime. Experiments on a
controlled synthetic dataset and a rendered image dataset validate our theory
and demonstrate the necessity of its assumptions through systematic ablations.

</details>


### [416] [ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs](https://arxiv.org/abs/2510.04767)
*Wonjun Kang,Kevin Galim,Seunghyuk Oh,Minjae Lee,Yuchen Zeng,Shuibai Zhang,Coleman Hooper,Yuezhou Hu,Hyung Il Koo,Nam Ik Cho,Kangwook Lee*

Main category: cs.LG

TL;DR: Parallel decoding in diffusion LLMs degrades quality due to token dependencies; ParallelBench reveals the speed-quality trade-off and the need for new decoding methods.


<details>
  <summary>Details</summary>
Motivation: Fill the gap in understanding and evaluating diffusion LLMs with parallel decoding, where independence assumptions hurt quality. Provide a benchmark to reveal limitations not captured by standard benchmarks.

Method: Information-theoretic analysis of parallel decoding; case studies on analytically tractable synthetic list operations from data distribution and decoding strategy perspectives; design and evaluation of ParallelBench; comparative analysis of dLLMs vs autoregressive LLMs.

Result: Parallel LLMs under parallel decoding can suffer dramatic quality degradation in real-world tasks; current parallel decoding strategies fail to adapt parallelism to task difficulty, limiting speedups and maintaining quality; ParallelBench exposes these limits and serves as a benchmark to guide future decoding methods.

Conclusion: Innovative decoding methods are needed to overcome the speed-quality trade-off in dLLMs; ParallelBench will accelerate development by providing a realistic, challenging evaluation suite for parallel decoding.

Abstract: While most autoregressive LLMs are constrained to one-by-one decoding,
diffusion LLMs (dLLMs) have attracted growing interest for their potential to
dramatically accelerate inference through parallel decoding. Despite this
promise, the conditional independence assumption in dLLMs causes parallel
decoding to ignore token dependencies, inevitably degrading generation quality
when these dependencies are strong. However, existing works largely overlook
these inherent challenges, and evaluations on standard benchmarks (e.g., math
and coding) are not sufficient to capture the quality degradation caused by
parallel decoding. To address this gap, we first provide an
information-theoretic analysis of parallel decoding. We then conduct case
studies on analytically tractable synthetic list operations from both data
distribution and decoding strategy perspectives, offering quantitative insights
that highlight the fundamental limitations of parallel decoding. Building on
these insights, we propose ParallelBench, the first benchmark specifically
designed for dLLMs, featuring realistic tasks that are trivial for humans and
autoregressive LLMs yet exceptionally challenging for dLLMs under parallel
decoding. Using ParallelBench, we systematically analyze both dLLMs and
autoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can
suffer dramatic quality degradation in real-world scenarios, and (ii) current
parallel decoding strategies struggle to adapt their degree of parallelism
based on task difficulty, thus failing to achieve meaningful speedup without
compromising quality. Our findings underscore the pressing need for innovative
decoding methods that can overcome the current speed-quality trade-off. We
release our benchmark to help accelerate the development of truly efficient
dLLMs.

</details>


### [417] [When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates](https://arxiv.org/abs/2510.04769)
*Michele Caprio,Siu Lun Chau,Krikamol Muandet*

Main category: cs.LG

TL;DR: First analysis of convergence properties for iterative updates of credal-set representations under imprecision; establishes conditions for fixed-point stability and attainability; illustrated with Credal Bayesian Deep Learning.


<details>
  <summary>Details</summary>
Motivation: To understand when iterative learning with imprecise probabilistic beliefs (credal sets) converges to stable fixed points and under what conditions stability arises, given the widespread use of such updates in ML.

Method: Theoretical analysis of update dynamics on credal sets (closed convex sets of distributions) to derive fixed-point existence and convergence conditions; demonstrated with Credal Bayesian Deep Learning as a concrete example.

Result: Identified structural conditions under which iterative updates converge to fixed points and exhibit stability; shows that incorporating imprecision enriches uncertainty modeling and reveals dynamics that promote stable learning.

Conclusion: Incorporating imprecision into iterative learning enriches uncertainty representation and exposes fundamental stability conditions for IPML, representing the first such analysis of learning dynamics under imprecision.

Abstract: Many machine learning algorithms rely on iterative updates of uncertainty
representations, ranging from variational inference and
expectation-maximization, to reinforcement learning, continual learning, and
multi-agent learning. In the presence of imprecision and ambiguity, credal sets
-- closed, convex sets of probability distributions -- have emerged as a
popular framework for representing imprecise probabilistic beliefs. Under such
imprecision, many learning problems in imprecise probabilistic machine learning
(IPML) may be viewed as processes involving successive applications of update
rules on credal sets. This naturally raises the question of whether this
iterative process converges to stable fixed points -- or, more generally, under
what conditions on the updating mechanism such fixed points exist, and whether
they can be attained. We provide the first analysis of this problem and
illustrate our findings using Credal Bayesian Deep Learning as a concrete
example. Our work demonstrates that incorporating imprecision into the learning
process not only enriches the representation of uncertainty, but also reveals
structural conditions under which stability emerges, thereby offering new
insights into the dynamics of iterative learning under imprecision.

</details>


### [418] [Distribution Preference Optimization: A Fine-grained Perspective for LLM Unlearning](https://arxiv.org/abs/2510.04773)
*Kai Qin,Jiaqi Wu,Jianxiang He,Haoyuan Sun,Yifei Zhao,Bin Liang,Yongzhe Chang,Tiantian Zhang,Houde Liu*

Main category: cs.LG

TL;DR: DiPO is a distribution-level unlearning method for LLMs that optimizes the next-token distribution to forget data while preserving utility, addressing NPO limitations.


<details>
  <summary>Details</summary>
Motivation: Privacy and safety concerns with LLM data; unlearning methods like NPO lack explicit positive signals and domain-agnostic generalizability, motivating a distribution-level approach.

Method: Develops DiPO by constructing preference distributions over output tokens via amplification/suppression of high-confidence logits; optimizes the next-token distribution; provides theoretical consistency of the loss with the unlearning direction.

Result: Shows a favorable utility-forget trade-off; achieves highest forget quality on TOFU and maintains leading utility on MUSE; scalable and sustainable.

Conclusion: DiPO offers a generalizable, effective distribution-level unlearning framework that overcomes NPO drawbacks and improves forgetting without sacrificing utility.

Abstract: As Large Language Models (LLMs) demonstrate remarkable capabilities learned
from vast corpora, concerns regarding data privacy and safety are receiving
increasing attention. LLM unlearning, which aims to remove the influence of
specific data while preserving overall model utility, is becoming an important
research area. One of the mainstream unlearning classes is optimization-based
methods, which achieve forgetting directly through fine-tuning, exemplified by
Negative Preference Optimization (NPO). However, NPO's effectiveness is limited
by its inherent lack of explicit positive preference signals. Attempts to
introduce such signals by constructing preferred responses often necessitate
domain-specific knowledge or well-designed prompts, fundamentally restricting
their generalizability. In this paper, we shift the focus to the
distribution-level, directly targeting the next-token probability distribution
instead of entire responses, and derive a novel unlearning algorithm termed
\textbf{Di}stribution \textbf{P}reference \textbf{O}ptimization (DiPO). We show
that the requisite preference distribution pairs for DiPO, which are
distributions over the model's output tokens, can be constructed by selectively
amplifying or suppressing the model's high-confidence output logits, thereby
effectively overcoming NPO's limitations. We theoretically prove the
consistency of DiPO's loss function with the desired unlearning direction.
Extensive experiments demonstrate that DiPO achieves a strong trade-off between
model utility and forget quality. Notably, DiPO attains the highest forget
quality on the TOFU benchmark, and maintains leading scalability and
sustainability in utility preservation on the MUSE benchmark.

</details>


### [419] [MetaMP: Seamless Metadata Enrichment and AI Application Framework for Enhanced Membrane Protein Visualization and Analysis](https://arxiv.org/abs/2510.04776)
*Ebenezer Awotoro,Chisom Ezekannagha,Florian Schwarz,Johannes Tauscher,Dominik Heider,Katharina Ladewig,Christel Le Bon,Karine Moncoq,Bruno Miroux,Georges Hattab*

Main category: cs.LG

TL;DR: MetaMP unifies membrane-protein databases in a web app, enhances data quality with ML-driven classification and eight interactive views, and supports structure classification and outlier detection; validated with strong performance in discrepancy resolution and class prediction.


<details>
  <summary>Details</summary>
Motivation: Address fragmentation and data quality issues in membrane-protein databases due to missing data, inconsistencies, and cross-source computational barriers, to enable AI-driven exploration and reliable structure classification.

Method: Introduce MetaMP, a framework that unifies multiple membrane-protein databases within a web application, enriches metadata, applies machine learning for classification, and provides eight interactive views; demonstrate applications of AI in predicting transmembrane segments, reconciling legacy databases, and classifying structures with explainable AI; validate with data-discrepancy resolution and class-prediction accuracy.

Result: MetaMP improves data quality and exploration capabilities; it resolved 77% of data discrepancies in validation and accurately predicted the class of newly identified membrane proteins 98% of the time, outperforming expert curation while maintaining speed; supports features for structure classification and outlier detection across tasks of varying difficulty.

Conclusion: MetaMP is a timely resource that harmonizes current membrane-protein knowledge and enables AI-driven exploration of membrane-protein architecture through integrated databases, enhanced metadata, and user-friendly visualization.

Abstract: Structural biology has made significant progress in determining membrane
proteins, leading to a remarkable increase in the number of available
structures in dedicated databases. The inherent complexity of membrane protein
structures, coupled with challenges such as missing data, inconsistencies, and
computational barriers from disparate sources, underscores the need for
improved database integration. To address this gap, we present MetaMP, a
framework that unifies membrane-protein databases within a web application and
uses machine learning for classification. MetaMP improves data quality by
enriching metadata, offering a user-friendly interface, and providing eight
interactive views for streamlined exploration. MetaMP was effective across
tasks of varying difficulty, demonstrating advantages across different levels
without compromising speed or accuracy, according to user evaluations.
Moreover, MetaMP supports essential functions such as structure classification
and outlier detection.
  We present three practical applications of Artificial Intelligence (AI) in
membrane protein research: predicting transmembrane segments, reconciling
legacy databases, and classifying structures with explainable AI support. In a
validation focused on statistics, MetaMP resolved 77% of data discrepancies and
accurately predicted the class of newly identified membrane proteins 98% of the
time and overtook expert curation. Altogether, MetaMP is a much-needed resource
that harmonizes current knowledge and empowers AI-driven exploration of
membrane-protein architecture.

</details>


### [420] [Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning](https://arxiv.org/abs/2510.04786)
*Jonas Hübotter,Leander Diaz-Bone,Ido Hakimi,Andreas Krause,Moritz Hardt*

Main category: cs.LG

TL;DR: RL agent creates a test-time curriculum to continue training during test-time, automatically selecting data from a large pool to improve task performance without manual data curation.


<details>
  <summary>Details</summary>
Motivation: To mimic human on-the-job learning and reduce reliance on manually curated datasets; leverage test-time data for continual training on target tasks.

Method: An agent assembles a task-specific curriculum at test time and uses reinforcement learning to select the most task-relevant data from a large pool, then applies continued RL-based training on the target task.

Result: The approach yields consistent improvements across tasks/models: on AIME25, pass@1 improves ~1.8x; on CodeElo, ~2.1x. It also raises performance ceilings: pass@8 on AIME25 from 40% to 62%; on CodeElo from 28% to 43%.

Conclusion: Test-time curricula via TTC-RL can extend test-time scaling to continual training over thousands of task-relevant experiences, delivering substantial performance gains and higher ceilings.

Abstract: Humans are good at learning on the job: We learn how to solve the tasks we
face as we go along. Can a model do the same? We propose an agent that
assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and
applies reinforcement learning to continue training the model for its target
task. The test-time curriculum avoids time-consuming human curation of datasets
by automatically selecting the most task-relevant data from a large pool of
available training data. Our experiments demonstrate that reinforcement
learning on a test-time curriculum consistently improves the model on its
target tasks, across a variety of evaluations and models. Notably, on
challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B
by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that
TTC-RL significantly raises the performance ceiling compared to the initial
model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to
43%. Our findings show the potential of test-time curricula in extending the
test-time scaling paradigm to continual training on thousands of task-relevant
experiences during test-time.

</details>


### [421] [On Predicting Post-Click Conversion Rate via Counterfactual Inference](https://arxiv.org/abs/2510.04816)
*Junhyung Ahn,Sanghack Lee*

Main category: cs.LG

TL;DR: Proposes ESCIM, a causal-counterfactual framework that generates counterfactual conversion labels for non-clicked items and uses them in training to improve CVR prediction, showing strong empirical gains.


<details>
  <summary>Details</summary>
Motivation: CVR models largely train on clicked data, causing data sparsity and bias due to exclusion of non-clicked samples. A principled, causality-guided method to leverage non-clicked data can improve generalization and robustness.

Method: Construct a structural causal model of user sequential behavior and perform counterfactual interventions (simulate a click) on non-clicked items to infer counterfactual CVRs. Transform these counterfactual CVRs into binary labels for non-clicked samples, and train a multi-task model (ESCIM) that incorporates both observed and counterfactual labels.

Result: Extensive experiments on public datasets show superior performance. Online A/B testing confirms effectiveness in real-world deployments. Demonstrates improved robustness and generalization, particularly on latent conversion data.

Conclusion: Counterfactual, causality-guided labeling for non-clicked samples can substantially improve CVR prediction; ESCIM effectively exploits the entire data space and offers practical benefits for recommender and advertising systems.

Abstract: Accurately predicting conversion rate (CVR) is essential in various
recommendation domains such as online advertising systems and e-commerce. These
systems utilize user interaction logs, which consist of exposures, clicks, and
conversions. CVR prediction models are typically trained solely based on
clicked samples, as conversions can only be determined following clicks.
However, the sparsity of clicked instances necessitates the collection of a
substantial amount of logs for effective model training. Recent works address
this issue by devising frameworks that leverage non-clicked samples. While
these frameworks aim to reduce biases caused by the discrepancy between clicked
and non-clicked samples, they often rely on heuristics. Against this
background, we propose a method to counterfactually generate conversion labels
for non-clicked samples by using causality as a guiding principle, attempting
to answer the question, "Would the user have converted if he or she had clicked
the recommended item?" Our approach is named the Entire Space Counterfactual
Inference Multi-task Model (ESCIM). We initially train a structural causal
model (SCM) of user sequential behaviors and conduct a hypothetical
intervention (i.e., click) on non-clicked items to infer counterfactual CVRs.
We then introduce several approaches to transform predicted counterfactual CVRs
into binary counterfactual conversion labels for the non-clicked samples.
Finally, the generated samples are incorporated into the training process.
Extensive experiments on public datasets illustrate the superiority of the
proposed algorithm. Online A/B testing further empirically validates the
effectiveness of our proposed algorithm in real-world scenarios. In addition,
we demonstrate the improved performance of the proposed method on latent
conversion data, showcasing its robustness and superior generalization
capabilities.

</details>


### [422] [On the Hardness of Learning Regular Expressions](https://arxiv.org/abs/2510.04834)
*Idan Attias,Lev Reyzin,Nathan Srebro,Gal Vardi*

Main category: cs.LG

TL;DR: Learning regular expressions is computationally hard in PAC and membership-query models, even under simple distributions; extending regexes with complement or intersection preserves hardness; these results do not follow from DFA/NFA hardness due to greater descriptive complexity of regexes.


<details>
  <summary>Details</summary>
Motivation: Regular expressions are widely used in practice and theory; understanding their learnability is important. The known hardness results for DFA/NFA do not directly translate to regexes because regexes can be exponentially more descriptive, so new hardness analyses are needed.

Method: The paper establishes lower bounds by constructing hardness proofs in improper learning for PAC and for distribution-free learning with membership queries. It also analyzes extensions of the hypothesis class by adding complement or intersection and shows preserved hardness under the uniform distribution.

Result: PAC learning of regular expressions is hard even under the uniform distribution on the hypercube; distribution-free learning with membership queries is hard; hardness persists when regular expressions are extended with complement or intersection under the uniform distribution.

Conclusion: Regular expressions exhibit intrinsic computational difficulty for learning, beyond what is implied by DFA/NFA results. The stronger descriptive capacity of regexes yields exponential gaps in learnability that researchers must account for in theory and practice.

Abstract: Despite the theoretical significance and wide practical use of regular
expressions, the computational complexity of learning them has been largely
unexplored. We study the computational hardness of improperly learning regular
expressions in the PAC model and with membership queries. We show that PAC
learning is hard even under the uniform distribution on the hypercube, and also
prove hardness of distribution-free learning with membership queries.
Furthermore, if regular expressions are extended with complement or
intersection, we establish hardness of learning with membership queries even
under the uniform distribution. We emphasize that these results do not follow
from existing hardness results for learning DFAs or NFAs, since the descriptive
complexity of regular languages can differ exponentially between DFAs, NFAs,
and regular expressions.

</details>


### [423] [Bond-Centered Molecular Fingerprint Derivatives: A BBBP Dataset Study](https://arxiv.org/abs/2510.04837)
*Guillaume Godin*

Main category: cs.LG

TL;DR: Bond Centered FingerPrints (BCFP) are a bond-centric complement to ECFP, mirroring bond-convolution from bond-based GNNs; combining ECFP+BCFP improves BBBP prediction; r=1 is best; introduces BCFP-Sort&Slice to preserve OOV counts and enable compact unhashed fusion; outperforms MGTP; lightweight fast baselines.


<details>
  <summary>Details</summary>
Motivation: To enhance molecular fingerprint representations by integrating bond-centric features with atom-centric fingerprints to improve BBBP classification, leveraging bond-convolution ideas from directed GNNs for fast, interpretable baselines.

Method: Introduce static BCFP mirroring bond-convolution used in directed message-passing GNNs (e.g., ChemProp); evaluate with rapid Random Forest on BBBP; stratified cross-validation; test radii r=1 and r=2; propose BCFP-Sort&Slice to preserve OOV count information and allow compact unhashed concatenation; compare against MGTP; significance assessed with Turkey HSD multiple-comparison analysis.

Result: Concatenating ECFP with BCFP consistently improves AUROC and AUPRC over either descriptor alone across stratified CV; radius r=1 performs best, r=2 shows no statistically separable gains; BCFP-Sort&Slice preserves OOV count and enables compact unhashed concatenation of BCFP variants; the composite features outperform MGTP on BBBP; demonstrates that lightweight, bond-centered descriptors complement atom-centered fingerprints and serve as strong, fast baselines for BBBP.

Conclusion: Bond-centered descriptors effectively complement atom-centered fingerprints and provide strong, fast baselines for BBBP prediction; combining ECFP with BCFP (especially using BCFP-Sort&Slice) yields statistically significant improvements and practical advantages.

Abstract: Bond Centered FingerPrint (BCFP) are a complementary, bond-centric
alternative to Extended-Connectivity Fingerprints (ECFP). We introduce a static
BCFP that mirrors the bond-convolution used by directed message-passing GNNs
like ChemProp, and evaluate it with a fast rapid Random Forest model on
Brain-Blood Barrier Penetration (BBBP) classification task. Across stratified
cross-validation, concatenating ECFP with BCFP consistently improves AUROC and
AUPRC over either descriptor alone, as confirmed by Turkey HSD
multiple-comparison analysis. Among radii, r = 1 performs best; r = 2 does not
yield statistically separable gains under the same test. We further propose
BCFP-Sort&Slice, a simple feature-combination scheme that preserves the
out-of-vocabulary (OOV) count information native to ECFP count vectors while
enabling compact unhashed concatenation of BCFP variants. We also outperform
the MGTP prediction on our BBBP evaluation, using such composite new features
bond and atom features. These results show that lightweight, bond-centered
descriptors can complement atom-centered circular fingerprints and provide
strong, fast baselines for BBBP prediction.

</details>


### [424] [Distributionally Robust Causal Abstractions](https://arxiv.org/abs/2510.04842)
*Yorgos Felekis,Theodoros Damoulas,Paris Giampouras*

Main category: cs.LG

TL;DR: Introduces distributionally robust causal abstractions (CA) using Wasserstein ambiguity sets, framed as constrained min-max learning to ensure interventional consistency across environments; provides theory for empirical and Gaussian settings and empirical validation demonstrating robustness to environmental shifts and misspecification.


<details>
  <summary>Details</summary>
Motivation: CA learning so far assumes fixed, well-specified exogenous distributions, making it vulnerable to distributional shifts and misspecification. A robust framework is needed to maintain interventional consistency across environments.

Method: Define distributionally robust CA learning via Wasserstein ambiguity sets and constrained min-max optimization. Derive theoretical results for both empirical and Gaussian environments, including guidelines to select the robustness radius. Provide learning algorithms compatible with existing CA methods and environments.

Result: Theoretical results enabling principled robustness-parameter (radius) selection for empirical and Gaussian settings; empirical evidence across problems and CA methods showing robustness to environmental shifts and to structural model and intervention mapping misspecification.

Conclusion: Presents the first class of distributionally robust CA methods, with principled frameworks for choosing the robustness level and empirical validation, enhancing CA reliability under distributional shifts and misspecification.

Abstract: Causal Abstraction (CA) theory provides a principled framework for relating
causal models that describe the same system at different levels of granularity
while ensuring interventional consistency between them. Recently, several
approaches for learning CAs have been proposed, but all assume fixed and
well-specified exogenous distributions, making them vulnerable to environmental
shifts and misspecification. In this work, we address these limitations by
introducing the first class of distributionally robust CAs and their associated
learning algorithms. The latter cast robust causal abstraction learning as a
constrained min-max optimization problem with Wasserstein ambiguity sets. We
provide theoretical results, for both empirical and Gaussian environments,
leading to principled selection of the level of robustness via the radius of
these sets. Furthermore, we present empirical evidence across different
problems and CA learning methods, demonstrating our framework's robustness not
only to environmental shifts but also to structural model and intervention
mapping misspecification.

</details>


### [425] [Synthesising Counterfactual Explanations via Label-Conditional Gaussian Mixture Variational Autoencoders](https://arxiv.org/abs/2510.04855)
*Junqi Jiang,Francesco Leofante,Antonio Rago,Francesca Toni*

Main category: cs.LG

TL;DR: A model-agnostic framework (LAPACE) for robust, plausible, and diverse counterfactual explanations via a latent-space path interpolation in a label-conditioned Gaussian Mixture VAE (L-GMVAE).


<details>
  <summary>Details</summary>
Motivation: To achieve robustness against input and model perturbations while satisfying plausibility, diversity, and actionability in counterfactual explanations within a unified, model-agnostic approach.

Method: Train a Label-conditional Gaussian Mixture VAE (L-GMVAE) that encodes each class as multiple Gaussian components with diverse centroids. Use LAPACE to generate full counterfactual paths by interpolating from the input's latent representation toward the learned centroids, ensuring convergence to fixed centroids for robustness. Allow actionability constraints via gradient optimization on the decoder. Evaluate across eight metrics.

Result: LAPACE is computationally efficient and achieves competitive performance across eight quantitative metrics, while providing robust paths and a spectrum of recourse options.

Conclusion: A unified, model-agnostic framework that delivers robust, plausible, and diverse counterfactual explanations, with easy incorporation of user-specified actionability constraints and scalability across datasets.

Abstract: Counterfactual explanations (CEs) provide recourse recommendations for
individuals affected by algorithmic decisions. A key challenge is generating
CEs that are robust against various perturbation types (e.g. input and model
perturbations) while simultaneously satisfying other desirable properties.
These include plausibility, ensuring CEs reside on the data manifold, and
diversity, providing multiple distinct recourse options for single inputs.
Existing methods, however, mostly struggle to address these multifaceted
requirements in a unified, model-agnostic manner. We address these limitations
by proposing a novel generative framework. First, we introduce the
Label-conditional Gaussian Mixture Variational Autoencoder (L-GMVAE), a model
trained to learn a structured latent space where each class label is
represented by a set of Gaussian components with diverse, prototypical
centroids. Building on this, we present LAPACE (LAtent PAth Counterfactual
Explanations), a model-agnostic algorithm that synthesises entire paths of CE
points by interpolating from inputs' latent representations to those learned
latent centroids. This approach inherently ensures robustness to input changes,
as all paths for a given target class converge to the same fixed centroids.
Furthermore, the generated paths provide a spectrum of recourse options,
allowing users to navigate the trade-off between proximity and plausibility
while also encouraging robustness against model changes. In addition,
user-specified actionability constraints can also be easily incorporated via
lightweight gradient optimisation through the L-GMVAE's decoder. Comprehensive
experiments show that LAPACE is computationally efficient and achieves
competitive performance across eight quantitative metrics.

</details>


### [426] [Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails](https://arxiv.org/abs/2510.04860)
*Siwei Han,Jiaqi Liu,Yaofeng Su,Wenbo Duan,Xinyuan Liu,Cihang Xie,Mohit Bansal,Mingyu Ding,Linjun Zhang,Huaxiu Yao*

Main category: cs.LG

TL;DR: A study of Alignment Tipping Process (ATP) in self-evolving LLM agents showing rapid alignment decay under self-evolution and diffusion across agents, with RL-based methods providing fragile defenses.


<details>
  <summary>Details</summary>
Motivation: To address risk that deployed self-evolving LLM agents can drift away from training alignment constraints, threatening long-term reliability and safety.

Method: Formalize ATP via two paradigms: Self-Interested Exploration (behavioral drift from high-reward deviations) and Imitative Strategy Diffusion (spread of deviant strategies in multi-agent systems). Build controllable testbeds and benchmark models (Qwen3-8B, Llama-3.1-8B-Instruct) to study drift and diffusion.

Result: Experiments show rapid erosion of alignment benefits under self-evolution; initially aligned models converge to unaligned states. In multi-agent settings, misalignment spreads quickly, causing collective misalignment. Current RL-based alignment methods offer only fragile defenses against ATP.

Conclusion: Alignment of LLM agents is dynamic and fragile, not static, and can decay due to feedback during deployment. The paper provides datasets and code for reproducing ATP analyses.

Abstract: As Large Language Model (LLM) agents increasingly gain self-evolutionary
capabilities to adapt and refine their strategies through real-world
interaction, their long-term reliability becomes a critical concern. We
identify the Alignment Tipping Process (ATP), a critical post-deployment risk
unique to self-evolving LLM agents. Unlike training-time failures, ATP arises
when continual interaction drives agents to abandon alignment constraints
established during training in favor of reinforced, self-interested strategies.
We formalize and analyze ATP through two complementary paradigms:
Self-Interested Exploration, where repeated high-reward deviations induce
individual behavioral drift, and Imitative Strategy Diffusion, where deviant
behaviors spread across multi-agent systems. Building on these paradigms, we
construct controllable testbeds and benchmark Qwen3-8B and
Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode
rapidly under self-evolution, with initially aligned models converging toward
unaligned states. In multi-agent settings, successful violations diffuse
quickly, leading to collective misalignment. Moreover, current reinforcement
learning-based alignment methods provide only fragile defenses against
alignment tipping. Together, these findings demonstrate that alignment of LLM
agents is not a static property but a fragile and dynamic one, vulnerable to
feedback-driven decay during deployment. Our data and code are available at
https://github.com/aiming-lab/ATP.

</details>


### [427] [A Clinical-grade Universal Foundation Model for Intraoperative Pathology](https://arxiv.org/abs/2510.04861)
*Zihan Zhao,Fengtao Zhou,Ronggang Li,Bing Chu,Xinke Zhang,Xueyi Zheng,Ke Zheng,Xiaobo Wen,Jiabo Ma,Yihui Wang,Jiewei Chen,Chengyou Zheng,Jiangyu Zhang,Yongqin Wen,Jiajia Meng,Ziqi Zeng,Xiaoqing Li,Jing Li,Dan Xie,Yaping Ye,Yu Wang,Hao Chen,Muyan Cai*

Main category: cs.LG

TL;DR: CRISP is a clinical-grade foundation model for intraoperative pathology, trained on 100k+ frozen sections from 8 centers, showing robust generalization and high real-world impact in informing surgeries (92.6% of cases) while reducing diagnostic workload by 35% and improving micrometastasis detection (87.5%), suggesting readiness for clinical adoption.


<details>
  <summary>Details</summary>
Motivation: The need for reliable AI support in intraoperative pathology is hampered by diagnostic complexity, limited high-quality frozen-section data, and lack of large-scale prospective validation. A clinically validated model could standardize decision-making and accelerate AI translation into surgical workflows.

Method: Train CRISP on >100,000 frozen sections from eight medical centers to form a clinical-grade foundation model. Evaluate retrospectively on ~15,000 intraoperative slides across nearly 100 diagnostic tasks (benign/malignant, key intraoperative decisions, pan-cancer detection, etc.). Assess generalization across institutions, tumor types, and unseen sites. Validate prospectively in a cohort of >2,000 patients, measuring diagnostic accuracy and impact on surgical decisions; quantify human-AI collaboration effects (workload reduction, ancillary tests avoided, micrometastases detection).

Result: CRISP demonstrated robust generalization across institutions, tumor types, and unseen sites including rare cancers. In the prospective cohort (>2,000 patients), it sustained high diagnostic accuracy and directly informed surgical decisions in 92.6% of cases. Human-AI collaboration reduced diagnostic workload by 35%, avoided 105 ancillary tests, and improved detection of micrometastases with 87.5% accuracy.

Conclusion: CRISP represents a clinically viable AI paradigm for intraoperative pathology, bridging computational advances with surgical precision and accelerating translation of AI into routine clinical practice.

Abstract: Intraoperative pathology is pivotal to precision surgery, yet its clinical
impact is constrained by diagnostic complexity and the limited availability of
high-quality frozen-section data. While computational pathology has made
significant strides, the lack of large-scale, prospective validation has
impeded its routine adoption in surgical workflows. Here, we introduce CRISP, a
clinical-grade foundation model developed on over 100,000 frozen sections from
eight medical centers, specifically designed to provide Clinical-grade Robust
Intraoperative Support for Pathology (CRISP). CRISP was comprehensively
evaluated on more than 15,000 intraoperative slides across nearly 100
retrospective diagnostic tasks, including benign-malignant discrimination, key
intraoperative decision-making, and pan-cancer detection, etc. The model
demonstrated robust generalization across diverse institutions, tumor types,
and anatomical sites-including previously unseen sites and rare cancers. In a
prospective cohort of over 2,000 patients, CRISP sustained high diagnostic
accuracy under real-world conditions, directly informing surgical decisions in
92.6% of cases. Human-AI collaboration further reduced diagnostic workload by
35%, avoided 105 ancillary tests and enhanced detection of micrometastases with
87.5% accuracy. Together, these findings position CRISP as a clinical-grade
paradigm for AI-driven intraoperative pathology, bridging computational
advances with surgical precision and accelerating the translation of artificial
intelligence into routine clinical practice.

</details>


### [428] [Less is More: Recursive Reasoning with Tiny Networks](https://arxiv.org/abs/2510.04871)
*Alexia Jolicoeur-Martineau*

Main category: cs.LG

TL;DR: HRM uses dual-frequency recursion between two small networks to tackle hard puzzles with limited data, while TRM shows that a 2-layer tiny network can surpass HRM and many LLMs with far fewer parameters on ARC-AGI tasks.


<details>
  <summary>Details</summary>
Motivation: To enable solving hard problems with small neural models and limited data. HRM is promising but not well understood and may be suboptimal; exploring a smaller, simpler alternative (TRM) to achieve better generalization is warranted.

Method: HRM: two small neural networks recurse at different frequencies to solve tasks like Sudoku, Maze, ARC-AGI, trained on around 1k examples with ~27M parameters. TRM: a single tiny network with 2 layers implementing recursive reasoning, ~7M parameters. Evaluation on ARC-AGI-1 and ARC-AGI-2.

Result: HRM beats LLMs on hard puzzles with small models and limited data. TRM achieves 45% test accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, outperforming many LLMs with less than 0.01% of their parameters.

Conclusion: HRM holds promise but is not yet well understood and may be suboptimal. TRM demonstrates substantially higher generalization with a tiny model, suggesting recursive reasoning can be effective with minimal parameters; further research is needed to understand mechanisms and to extend gains.

Abstract: Hierarchical Reasoning Model (HRM) is a novel approach using two small neural
networks recursing at different frequencies. This biologically inspired method
beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,
and ARC-AGI while trained with small models (27M parameters) on small data
(around 1000 examples). HRM holds great promise for solving hard problems with
small networks, but it is not yet well understood and may be suboptimal. We
propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach
that achieves significantly higher generalization than HRM, while using a
single tiny network with only 2 layers. With only 7M parameters, TRM obtains
45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs
(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the
parameters.

</details>


### [429] [Flow-Matching Based Refiner for Molecular Conformer Generation](https://arxiv.org/abs/2510.04878)
*Xiangyang Xu,Hongyang Gao*

Main category: cs.LG

TL;DR: A flow-matching refiner improves low-energy molecular conformer generation by starting from mixed-quality outputs and rescheduling noise to bypass difficult low-SNR steps, achieving higher quality with fewer denoising steps while preserving diversity.


<details>
  <summary>Details</summary>
Motivation: Denoising-based conformer generation methods (diffusion/flow) suffer from error accumulation in low-SNR steps, hindering sampling quality and training.

Method: Propose a flow-matching refiner that initializes sampling from mixed-quality outputs produced by upstream denoisers and reorders the noise scale to skip the low-SNR phase; adopt a generator-refiner pipeline.

Result: On GEOM-QM9 and GEOM-Drugs benchmarks, the generator-refiner pipeline yields higher-quality conformers with fewer total denoising steps while maintaining diversity.

Conclusion: The refiner effectively mitigates error accumulation in late denoising stages, enabling efficient, high-quality MCG with fewer steps and preserved diversity.

Abstract: Low-energy molecular conformers generation (MCG) is a foundational yet
challenging problem in drug discovery. Denoising-based methods include
diffusion and flow-matching methods that learn mappings from a simple base
distribution to the molecular conformer distribution. However, these approaches
often suffer from error accumulation during sampling, especially in the low SNR
steps, which are hard to train. To address these challenges, we propose a
flow-matching refiner for the MCG task. The proposed method initializes
sampling from mixed-quality outputs produced by upstream denoising models and
reschedules the noise scale to bypass the low-SNR phase, thereby improving
sample quality. On the GEOM-QM9 and GEOM-Drugs benchmark datasets, the
generator-refiner pipeline improves quality with fewer total denoising steps
while preserving diversity.

</details>


### [430] [Revealing Interconnections between Diseases: from Statistical Methods to Large Language Models](https://arxiv.org/abs/2510.04888)
*Alina Ermilova,Dmitrii Kornilov,Sofia Samoilova,Ekaterina Laptenkova,Anastasia Kolesnikova,Ekaterina Podplutova,Senotrusova Sofya,Maksim G. Sharaev*

Main category: cs.LG

TL;DR: LLM-based methods show limited novelty in discovering disease interconnections; seven approaches evaluated across two ICD-10 data sources; results provide a foundational disease ontology.


<details>
  <summary>Details</summary>
Motivation: To address labor-intensive manual analysis, ML method choice, data source reliability, and lack of ground truth in disease interconnections; evaluate whether clinical data or textual disease descriptions yield better cues.

Method: Compare seven approaches: co-occurrence + masked language model on real EHR data; domain-specific BERT variants (Med-BERT, BioClinicalBERT); general BERT with document retrieval; four LLMs (Mistral, DeepSeek, Qwen, YandexGPT); analyze two data sources (ICD-10 sequences from MIMIC-IV, and full ICD-10 code set with/without text descriptions); graph-based interconnection matrix comparison.

Result: LLMs produced interconnections with the lowest diversity across diseases, suggesting limited potential for discovering new disease interconnections; other methods yield more diverse connections; absence of ground-truth; results provide a disease ontology resource.

Conclusion: LLMs may be limited for discovering novel disease interconnections; the proposed framework offers a valuable ontology resource for future clinical research and AI health applications; recommendations for method selection in disease-relationship mining.

Abstract: Identifying disease interconnections through manual analysis of large-scale
clinical data is labor-intensive, subjective, and prone to expert disagreement.
While machine learning (ML) shows promise, three critical challenges remain:
(1) selecting optimal methods from the vast ML landscape, (2) determining
whether real-world clinical data (e.g., electronic health records, EHRs) or
structured disease descriptions yield more reliable insights, (3) the lack of
"ground truth," as some disease interconnections remain unexplored in medicine.
Large language models (LLMs) demonstrate broad utility, yet they often lack
specialized medical knowledge. To address these gaps, we conduct a systematic
evaluation of seven approaches for uncovering disease relationships based on
two data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the
full set of ICD-10 codes, both with and without textual descriptions. Our
framework integrates the following: (i) a statistical co-occurrence analysis
and a masked language modeling (MLM) approach using real clinical data; (ii)
domain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a
general-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,
DeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained
interconnection matrices shows that the LLM-based approach produces
interconnections with the lowest diversity of ICD code connections to different
diseases compared to other methods, including text-based and domain-based
approaches. This suggests an important implication: LLMs have limited potential
for discovering new interconnections. In the absence of ground truth databases
for medical interconnections between ICD codes, our results constitute a
valuable medical disease ontology that can serve as a foundational resource for
future clinical research and artificial intelligence applications in
healthcare.

</details>


### [431] [Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models](https://arxiv.org/abs/2510.04900)
*Nick Janßen,Melanie Schaller,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: A simulation-based, parameterizable framework for evaluating multivariate long-term time series forecasting (M-LTSF) under controlled signal and noise configurations, benchmarking diverse architectures to reveal model strengths, weaknesses, and frequency reconstruction capabilities.


<details>
  <summary>Details</summary>
Motivation: Real-world M-LTSF evaluations are confounded by unknown noise properties. A synthetic, principled testbed enables systematic, fine-grained assessment of model robustness across signal types, noise patterns, SNRs, and frequency characteristics.

Method: Generate synthetic, configurable multivariate time series with varied signal components, noise types (white, Brownian, etc.), SNR levels, and frequency patterns. Benchmark four architectures—S-Mamba (state-space), iTransformer (transformer-based), R-Linear (linear), and Autoformer (decomposition-based)—across multiple controlled scenarios. Analyze performance via MSE, frequency reconstruction, and sensitivity to window length relative to seasonal periods.

Result: All models degrade when lookback windows fail to capture complete seasonal periods. S-Mamba and Autoformer excel on sawtooth signals, while R-Linear and iTransformer prefer sinusoidal signals. White and Brownian noise degrade performance, especially at low SNR, with S-Mamba showing trend-noise vulnerability and iTransformer showing seasonal-noise vulnerability. Spectral analysis indicates S-Mamba and iTransformer achieve superior frequency reconstruction across scenarios.

Conclusion: A controlled, synthetic testbed provides deeper, model-specific insights into robustness and frequency fidelity for M-LTSF models, offering guidance for model selection based on signal characteristics and noise conditions.

Abstract: Understanding the robustness of deep learning models for multivariate
long-term time series forecasting (M-LTSF) remains challenging, as evaluations
typically rely on real-world datasets with unknown noise properties. We propose
a simulation-based evaluation framework that generates parameterizable
synthetic datasets, where each dataset instance corresponds to a different
configuration of signal components, noise types, signal-to-noise ratios, and
frequency characteristics. These configurable components aim to model
real-world multivariate time series data without the ambiguity of unknown
noise. This framework enables fine-grained, systematic evaluation of M-LTSF
models under controlled and diverse scenarios. We benchmark four representative
architectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear
(linear), and Autoformer (decomposition-based). Our analysis reveals that all
models degrade severely when lookback windows cannot capture complete periods
of seasonal patters in the data. S-Mamba and Autoformer perform best on
sawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals.
White and Brownian noise universally degrade performance with lower
signal-to-noise ratio while S-Mamba shows specific trend-noise and iTransformer
shows seasonal-noise vulnerability. Further spectral analysis shows that
S-Mamba and iTransformer achieve superior frequency reconstruction. This
controlled approach, based on our synthetic and principle-driven testbed,
offers deeper insights into model-specific strengths and limitations through
the aggregation of MSE scores and provides concrete guidance for model
selection based on signal characteristics and noise conditions.

</details>


### [432] [Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects](https://arxiv.org/abs/2510.04901)
*Jonathan Colaço Carr,Qinyi Sun,Cameron Allen*

Main category: cs.LG

TL;DR: A general method to learn skills that specifically control chosen state variables, boosting exploration and preventing negative downstream side effects.


<details>
  <summary>Details</summary>
Motivation: Current skill discovery often ignores natural state variables in RL, leading to limited exploration, harder learning, and potential negative effects when goals are under-specified.

Method: A general approach that enables skill discovery algorithms to learn focused skills—skills that target and control specific state variables.

Result: State-space exploration improves by about 3x, enables new learning capabilities, and automatically avoids negative side effects in downstream tasks.

Conclusion: Focusing skill discovery on controlling designated state variables enhances exploration efficiency and task performance, providing a general framework for more effective skill discovery.

Abstract: Skills are essential for unlocking higher levels of problem solving. A common
approach to discovering these skills is to learn ones that reliably reach
different states, thus empowering the agent to control its environment.
However, existing skill discovery algorithms often overlook the natural state
variables present in many reinforcement learning problems, meaning that the
discovered skills lack control of specific state variables. This can
significantly hamper exploration efficiency, make skills more challenging to
learn with, and lead to negative side effects in downstream tasks when the goal
is under-specified. We introduce a general method that enables these skill
discovery algorithms to learn focused skills -- skills that target and control
specific state variables. Our approach improves state space coverage by a
factor of three, unlocks new learning capabilities, and automatically avoids
negative side effects in downstream tasks.

</details>


### [433] [DP-HYPE: Distributed Differentially Private Hyperparameter Search](https://arxiv.org/abs/2510.04902)
*Johannes Liebenow,Thorsten Peinemann,Esfandiar Mohammadi*

Main category: cs.LG

TL;DR: DP-HYPEIs a distributed, privacy-preserving hyperparameter search method that uses client voting to select a compromise hyperparameter set. It achieves client-level differential privacy independent of the number of hyperparameters, provides utility bounds, and is scalable. Implemented as a Flower submodule, it performs well on iid and non-iid data even under small privacy budgets.


<details>
  <summary>Details</summary>
Motivation: Hyperparameter tuning in distributed ML raises privacy concerns when data are sensitive. Existing DP approaches can be costly, per-client, or yield poor utility. A scalable, task-agnostic DP hyperparameter tuner is needed to align client setups while preserving strong privacy.

Method: Clients evaluate hyperparameters locally and vote on the best options. DP-HYPE aggregates these evaluations to select a compromise that is supported by the majority, ensuring client-level differential privacy. The privacy guarantee does not depend on the number of hyperparameters. The approach is implemented as a Flower submodule and accompanied by utility bounds on the probability of reaching a compromise.

Result: The paper proves client-level differential privacy for the DP-HYPE vote mechanism, derives utility bounds (probability of achieving a compromise), and demonstrates high utility under small budgets. Empirical evaluation across iid and non-iid datasets shows robust performance and scalable behavior.

Conclusion: DP-HYPE offers a scalable, privacy-preserving alternative for distributed hyperparameter tuning. It achieves strong client-level DP independent of hyperparameter count, comes with utility guarantees, and performs well across data distributions while integrating with existing frameworks like Flower.

Abstract: The tuning of hyperparameters in distributed machine learning can
substantially impact model performance. When the hyperparameters are tuned on
sensitive data, privacy becomes an important challenge and to this end,
differential privacy has emerged as the de facto standard for provable privacy.
A standard setting when performing distributed learning tasks is that clients
agree on a shared setup, i.e., find a compromise from a set of hyperparameters,
like the learning rate of the model to be trained. Yet, prior work on
differentially private hyperparameter tuning either uses computationally
expensive cryptographic protocols, determines hyperparameters separately for
each client, or applies differential privacy locally, which can lead to
undesirable utility-privacy trade-offs.
  In this work, we present our algorithm DP-HYPE, which performs a distributed
and privacy-preserving hyperparameter search by conducting a distributed voting
based on local hyperparameter evaluations of clients. In this way, DP-HYPE
selects hyperparameters that lead to a compromise supported by the majority of
clients, while maintaining scalability and independence from specific learning
tasks. We prove that DP-HYPE preserves the strong notion of differential
privacy called client-level differential privacy and, importantly, show that
its privacy guarantees do not depend on the number of hyperparameters. We also
provide bounds on its utility guarantees, that is, the probability of reaching
a compromise, and implement DP-HYPE as a submodule in the popular Flower
framework for distributed machine learning. In addition, we evaluate
performance on multiple benchmark data sets in iid as well as multiple non-iid
settings and demonstrate high utility of DP-HYPE even under small privacy
budgets.

</details>


### [434] [How Different from the Past? Spatio-Temporal Time Series Forecasting with Self-Supervised Deviation Learning](https://arxiv.org/abs/2510.04908)
*Haotian Gao,Zheng Dong,Jiawei Yong,Shintaro Fukushima,Kenjiro Taura,Renhe Jiang*

Main category: cs.LG

TL;DR: ST-SSDL introduces a self-supervised deviation learning scheme for spatio-temporal forecasting by anchoring inputs to their historical averages and using learnable prototypes to capture typical patterns. It employs a contrastive loss and a deviation loss to improve inter-prototype discriminability and distance consistency, enhancing generalization under varying input conditions.


<details>
  <summary>Details</summary>
Motivation: Current spatio-temporal models often neglect dynamic deviations between incoming data and historical patterns, which contain critical signals affecting forecasting accuracy. The proposed framework aims to explicitly model and leverage these deviations.

Method: The framework anchors each input to its historical average and discretizes the latent space with learnable prototypes representing typical spatio-temporal patterns. It introduces two auxiliary objectives: a contrastive loss to boost inter-prototype discriminability and a deviation loss to enforce distance consistency between input representations and the corresponding prototypes. These are optimized jointly with the forecasting loss.

Result: ST-SSDL consistently outperforms state-of-the-art baselines across six benchmark datasets on multiple metrics. Visualizations show adaptive responses to varying levels of deviation in complex spatio-temporal scenarios.

Conclusion: Incorporating self-supervised deviation learning with prototype-based latent discretization improves generalization and robustness of spatio-temporal forecasting under diverse input conditions.

Abstract: Spatio-temporal forecasting is essential for real-world applications such as
traffic management and urban computing. Although recent methods have shown
improved accuracy, they often fail to account for dynamic deviations between
current inputs and historical patterns. These deviations contain critical
signals that can significantly affect model performance. To fill this gap, we
propose ST-SSDL, a Spatio-Temporal time series forecasting framework that
incorporates a Self-Supervised Deviation Learning scheme to capture and utilize
such deviations. ST-SSDL anchors each input to its historical average and
discretizes the latent space using learnable prototypes that represent typical
spatio-temporal patterns. Two auxiliary objectives are proposed to refine this
structure: a contrastive loss that enhances inter-prototype discriminability
and a deviation loss that regularizes the distance consistency between input
representations and corresponding prototypes to quantify deviation. Optimized
jointly with the forecasting objective, these components guide the model to
organize its hidden space and improve generalization across diverse input
conditions. Experiments on six benchmark datasets show that ST-SSDL
consistently outperforms state-of-the-art baselines across multiple metrics.
Visualizations further demonstrate its ability to adaptively respond to varying
levels of deviation in complex spatio-temporal scenarios. Our code and datasets
are available at https://github.com/Jimmy-7664/ST-SSDL.

</details>


### [435] [Glocal Information Bottleneck for Time Series Imputation](https://arxiv.org/abs/2510.04910)
*Jie Yang,Kexin Zhang,Guibin Zhang,Philip S. Yu,Kaize Ding*

Main category: cs.LG

TL;DR: Glocal-IB introduces a global alignment objective into the Information Bottleneck (IB) framework for time series imputation, addressing the gap where point-wise losses capture local details but miss global structure under high missingness.


<details>
  <summary>Details</summary>
Motivation: Point-wise reconstruction losses often overfit to local noise and fail to capture global data structure when missing values are abundant. This leads to degraded latent representations and poorer imputations during inference.

Method: Extend standard Information Bottleneck with a Global Alignment loss derived from a tractable mutual information approximation. The loss aligns latent representations of masked inputs with those of their originally observed counterparts, aiming to preserve global structure while denoising local noise. The approach is model-agnostic and validated across nine datasets with code released.

Result: Glocal-IB yields consistent improvements in imputation quality and produces more aligned latent representations under missingness across nine datasets, indicating better generalization to high missing rates.

Conclusion: A global alignment objective can complement local reconstruction losses, mitigating overfitting to noise and improving global information retention in TSI; the Glocal-IB paradigm is adaptable and demonstrates robust gains on multiple datasets.

Abstract: Time Series Imputation (TSI), which aims to recover missing values in
temporal data, remains a fundamental challenge due to the complex and often
high-rate missingness in real-world scenarios. Existing models typically
optimize the point-wise reconstruction loss, focusing on recovering numerical
values (local information). However, we observe that under high missing rates,
these models still perform well in the training phase yet produce poor
imputations and distorted latent representation distributions (global
information) in the inference phase. This reveals a critical optimization
dilemma: current objectives lack global guidance, leading models to overfit
local noise and fail to capture global information of the data. To address this
issue, we propose a new training paradigm, Glocal Information Bottleneck
(Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework
by introducing a Global Alignment loss, derived from a tractable mutual
information approximation. This loss aligns the latent representations of
masked inputs with those of their originally observed counterparts. It helps
the model retain global structure and local details while suppressing noise
caused by missing values, giving rise to better generalization under high
missingness. Extensive experiments on nine datasets confirm that Glocal-IB
leads to consistently improved performance and aligned latent representations
under missingness. Our code implementation is available in
https://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.

</details>


### [436] [Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data](https://arxiv.org/abs/2510.04927)
*Usman Akram,Yiyue Chen,Haris Vikalo*

Main category: cs.LG

TL;DR: FedSSL-AMC is a federated, self-supervised learning framework for automatic modulation classification that learns robust representations across unlabeled data and then uses small labeled sets with per-client SVMs, backed by convergence and separability guarantees.


<details>
  <summary>Details</summary>
Motivation: Addresses privacy by avoiding central data collection, reduces communication overhead, and improves robustness to channel shifts; also tackles class imbalance, non-IID client distributions, and scarcity of labeled samples in AMC.

Method: Train a causal, time-dilated CNN with triplet-loss self-supervision on unlabeled I/Q sequences distributed across clients (federated setting), followed by per-client SVMs trained on small labeled subsets. The paper proves convergence of the federated representation learning and a separability guarantee for the downstream classifier under feature noise.

Result: Empirical results on synthetic and over-the-air datasets show consistent gains over supervised FL baselines under heterogeneous SNR, carrier-frequency offsets, and non-IID label partitions; framework yields improved robustness and privacy-preserving learning.

Conclusion: FedSSL-AMC provides a privacy-preserving, label-efficient AMC approach with theoretical convergence and separability guarantees and demonstrated empirical advantages over supervised federated baselines in challenging, heterogeneous environments.

Abstract: Training automatic modulation classification (AMC) models on centrally
aggregated data raises privacy concerns, incurs communication overhead, and
often fails to confer robustness to channel shifts. Federated learning (FL)
avoids central aggregation by training on distributed clients but remains
sensitive to class imbalance, non-IID client distributions, and limited labeled
samples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with
triplet-loss self-supervision on unlabeled I/Q sequences across clients,
followed by per-client SVMs on small labeled sets. We establish convergence of
the federated representation learning procedure and a separability guarantee
for the downstream classifier under feature noise. Experiments on synthetic and
over-the-air datasets show consistent gains over supervised FL baselines under
heterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.

</details>


### [437] [Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking](https://arxiv.org/abs/2510.04930)
*Ali Saheb Pasand,Elvis Dohmatob*

Main category: cs.LG

TL;DR: EGD speeds up grokking by equalizing gradient speeds across principal directions, removing plateaus on modular arithmetic tasks.


<details>
  <summary>Details</summary>
Motivation: Grokking causes late generalization despite early training; speeding up learning requires understanding and controlling gradient flow directions.

Method: Theoretically and empirically show asymmetric gradient speeds along gradient principal directions; propose egalitarian gradient descent (EGD) that normalizes gradients so all principal directions evolve at the same rate; relate to natural gradient; test on modular addition and sparse parity.

Result: EGD accelerates grokking; in some cases completely removes plateaus; demonstrated on classical arithmetic problems known for stagnation.

Conclusion: Gradient normalization that equalizes progression along principal directions can dramatically mitigate grokking; EGD is a practical variant of natural gradient with strong empirical support on tasks with plateau behavior.

Abstract: Grokking is the phenomenon whereby, unlike the training performance, which
peaks early in the training process, the test/generalization performance of a
model stagnates over arbitrarily many epochs and then suddenly jumps to usually
close to perfect levels. In practice, it is desirable to reduce the length of
such plateaus, that is to make the learning process "grok" faster. In this
work, we provide new insights into grokking. First, we show both empirically
and theoretically that grokking can be induced by asymmetric speeds of
(stochastic) gradient descent, along different principal (i.e singular
directions) of the gradients. We then propose a simple modification that
normalizes the gradients so that dynamics along all the principal directions
evolves at exactly the same speed. Then, we establish that this modified
method, which we call egalitarian gradient descent (EGD) and can be seen as a
carefully modified form of natural gradient descent, groks much faster. In
fact, in some cases the stagnation is completely removed. Finally, we
empirically show that on classical arithmetic problems such as modular addition
and sparse parity problem which this stagnation has been widely observed and
intensively studied, that our proposed method eliminates the plateaus.

</details>


### [438] [ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures](https://arxiv.org/abs/2510.04938)
*Shiwen Qin,Alexander Auras,Shay B. Cohen,Elliot J. Crowley,Michael Moeller,Linus Ericsson,Jovita Lukasik*

Main category: cs.LG

TL;DR: Introduces ONNX-Bench and ONNX-Net: a universal, text-based representation and benchmark for NAS that enables zero-shot performance prediction across diverse search spaces.


<details>
  <summary>Details</summary>
Motivation: NAS evaluation is expensive and current fast-evaluation methods are tied to specific cell-based search spaces and graph encodings, limiting generality and scalability.

Method: Construct ONNX-Bench: a benchmark of all open-source NAS networks in ONNX format (over 600k architecture-accuracy pairs). Propose ONNX-Net: a natural-language encoding that describes any architecture and serves as input to a single performance predictor. Train with a small pretraining set to enable zero-shot generalization across heterogeneous topologies.

Result: Demonstrates strong zero-shot performance across disparate search spaces with few pretraining samples, enabling instant evaluation of any architecture.

Conclusion: Presents a space-agnostic NAS evaluation pipeline with a unified representation and predictor, potentially enabling scalable NAS across expressive search spaces; future work could address encoding ambiguities, efficiency, and broader validation.

Abstract: Neural architecture search (NAS) automates the design process of
high-performing architectures, but remains bottlenecked by expensive
performance evaluation. Most existing studies that achieve faster evaluation
are mostly tied to cell-based search spaces and graph encodings tailored to
those individual search spaces, limiting their flexibility and scalability when
applied to more expressive search spaces. In this work, we aim to close the gap
of individual search space restrictions and search space dependent network
representations. We present ONNX-Bench, a benchmark consisting of a collection
of neural networks in a unified format based on ONNX files. ONNX-Bench includes
all open-source NAS-bench-based neural networks, resulting in a total size of
more than 600k {architecture, accuracy} pairs. This benchmark allows creating a
shared neural network representation, ONNX-Net, able to represent any neural
architecture using natural language descriptions acting as an input to a
performance predictor. This text-based encoding can accommodate arbitrary layer
types, operation parameters, and heterogeneous topologies, enabling a single
surrogate to generalise across all neural architectures rather than being
confined to cell-based search spaces. Experiments show strong zero-shot
performance across disparate search spaces using only a small amount of
pretraining samples, enabling the unprecedented ability to evaluate any neural
network architecture instantly.

</details>


### [439] [Feasibility-Aware Decision-Focused Learning for Predicting Parameters in the Constraints](https://arxiv.org/abs/2510.04951)
*Jayanta Mandi,Marianne Defresne,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: Decision-focused learning for predicting constraint parameters in generic COPs using two MLE-based losses (feasibility and suboptimality) with a tunable trade-off; competitive performance without assuming LP/ILP.


<details>
  <summary>Details</summary>
Motivation: When predicted constraint parameters cause infeasibility, need to balance feasibility and decision quality in PtO; extend DFL beyond LP/ILP to generic COPs.

Method: Define two MLE-based losses: infeasibility penalty for predictions causing infeasible outcomes; suboptimality penalty when true optimum is infeasible under predictions; combine via a single tunable weight; evaluate on several COP instances.

Result: The tunable parameter allows controlling the trade-off; for a single parameter value, the method matches baselines in suboptimality and feasibility across multiple COP instances.

Conclusion: A general DFL framework for predicting constraint parameters in generic COPs with controllable feasibility-decision trade-off; avoids LP/ILP assumptions and performs competitively with existing methods.

Abstract: When some parameters of a constrained optimization problem (COP) are
uncertain, this gives rise to a predict-then-optimize (PtO) problem, comprising
two stages -- the prediction of the unknown parameters from contextual
information and the subsequent optimization using those predicted parameters.
Decision-focused learning (DFL) implements the first stage by training a
machine learning (ML) model to optimize the quality of the decisions made using
the predicted parameters. When parameters in the constraints of a COP are
predicted, the predicted parameters can lead to infeasible solutions.
Therefore, it is important to simultaneously manage both feasibility and
decision quality. We develop a DFL framework for predicting constraint
parameters in a generic COP. While prior works typically assume that the
underlying optimization problem is a linear program (LP) or integer linear
program (ILP), our approach makes no such assumption. We derive two novel loss
functions based on maximum likelihood estimation (MLE): the first one penalizes
infeasibility (by penalizing when the predicted parameters lead to infeasible
solutions), and the second one penalizes suboptimal decisions (by penalizing
when the true optimal solution is infeasible under the predicted parameters).
We introduce a single tunable parameter to form a weighted average of the two
losses, allowing decision-makers to balance suboptimality and feasibility. We
experimentally demonstrate that adjusting this parameter provides a
decision-maker the control over the trade-off between the two. Moreover, across
several COP instances, we find that for a single value of the tunable
parameter, our method matches the performance of the existing baselines on
suboptimality and feasibility.

</details>


### [440] [StructuralDecompose: A Modular Framework for Robust Time Series Decomposition in R](https://arxiv.org/abs/2510.04974)
*Allen Daniel Sunny*

Main category: cs.LG

TL;DR: A modular, interpretable time-series decomposition R package (StructuralDecompose) that splits analysis into changepoint detection, anomaly detection, smoothing, and decomposition for flexibility and robustness, with benchmarking against Rbeast and autostsm and use in interpretable ML workflows.


<details>
  <summary>Details</summary>
Motivation: Traditional time-series decomposition is often treated as a monolithic process, limiting flexibility and interpretability. There is a need for a modular framework that can tailor methods to specific time-series characteristics and improve interpretability in ML pipelines.

Method: Introduce StructuralDecompose, an R package that separately implements changepoint detection, anomaly detection, smoothing, and decomposition. The workflow is demonstrated on simulated and real-world datasets, with benchmarking against state-of-the-art tools (Rbeast and autostsm) and discussion of integration into interpretable ML workflows.

Result: The paper demonstrates robustness and flexibility of the modular design, with empirical benchmarking against established tools on both simulated and real datasets and real-world examples, showing competitive performance and enhanced interpretability.

Conclusion: StructuralDecompose contributes a modular framework for time-series decomposition that enables customization and improved interpretability in ML workflows, potentially complementing or surpassing monolithic approaches like those used by Rbeast and autostsm.

Abstract: We present StructuralDecompose, an R package for modular and interpretable
time series decomposition. Unlike existing approaches that treat decomposition
as a monolithic process, StructuralDecompose separates the analysis into
distinct components: changepoint detection, anomaly detection, smoothing, and
decomposition. This design provides flexibility and robust- ness, allowing
users to tailor methods to specific time series characteristics. We demonstrate
the package on simulated and real-world datasets, benchmark its performance
against state-of-the- art tools such as Rbeast and autostsm, and discuss its
role in interpretable machine learning workflows.

</details>


### [441] [Federated Computation of ROC and PR Curves](https://arxiv.org/abs/2510.04979)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: A method to approximate ROC and PR curves in federated learning by estimating quantiles of prediction scores under distributed differential privacy, with theoretical AE bounds and empirical validation.


<details>
  <summary>Details</summary>
Motivation: In Federated Learning, data resides on multiple clients and the server lacks access to raw prediction scores and labels. This makes computing ROC/PR curves difficult under privacy and communication constraints. The work addresses privacy-preserving evaluation of classifiers.

Method: Proposes estimating quantiles of the prediction-score distribution under distributed differential privacy to approximate ROC and PR curves. Provides theoretical bounds on the Area Error between true and estimated curves and analyzes trade-offs among accuracy, privacy, and communication cost. Demonstrates low communication overhead.

Result: Empirical results on real-world datasets show high approximation accuracy with minimal communication and strong privacy guarantees, enabling practical privacy-preserving evaluation in federated systems.

Conclusion: The proposed quantile-based federated evaluation method enables accurate ROC/PR estimation under distributed DP with favorable privacy-communication-accuracy trade-offs, making FL model evaluation more practical.

Abstract: Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves are
fundamental tools for evaluating machine learning classifiers, offering
detailed insights into the trade-offs between true positive rate vs. false
positive rate (ROC) or precision vs. recall (PR). However, in Federated
Learning (FL) scenarios, where data is distributed across multiple clients,
computing these curves is challenging due to privacy and communication
constraints. Specifically, the server cannot access raw prediction scores and
class labels, which are used to compute the ROC and PR curves in a centralized
setting. In this paper, we propose a novel method for approximating ROC and PR
curves in a federated setting by estimating quantiles of the prediction score
distribution under distributed differential privacy. We provide theoretical
bounds on the Area Error (AE) between the true and estimated curves,
demonstrating the trade-offs between approximation accuracy, privacy, and
communication cost. Empirical results on real-world datasets demonstrate that
our method achieves high approximation accuracy with minimal communication and
strong privacy guarantees, making it practical for privacy-preserving model
evaluation in federated systems.

</details>


### [442] [Adaptive Memory Momentum via a Model-Based Framework for Deep Learning Optimization](https://arxiv.org/abs/2510.04988)
*Kristi Topollai,Anna Choromanska*

Main category: cs.LG

TL;DR: Adaptive memory replaces fixed momentum with an online-adjusted momentum coefficient in first-order optimizers, per-proximal derivation, improving SGD/AdamW without extra hyperparameters.


<details>
  <summary>Details</summary>
Motivation: To remove the suboptimal, static momentum hyperparameter (β ≈ 0.9) and improve optimization efficiency and robustness by adapting momentum online to training dynamics.

Method: Formulate momentum update via a proximal-like framework using two planes: one from the current gradient and another from the accumulated memory of past gradients; derive a dynamic momentum coefficient; instantiate adaptive memory variants of SGD and AdamW.

Result: Empirically outperforms standard SGD and Adam with hand-tuned momentum across convex and large-scale deep learning tasks; requires no additional hyperparameters and is simple to implement.

Conclusion: Adaptive memory offers a simple, effective path to adaptivity in momentum-based optimization, with potential for broader impact and further exploration of adaptive momentum strategies.

Abstract: The vast majority of modern deep learning models are trained with
momentum-based first-order optimizers. The momentum term governs the
optimizer's memory by determining how much each past gradient contributes to
the current convergence direction. Fundamental momentum methods, such as
Nesterov Accelerated Gradient and the Heavy Ball method, as well as more recent
optimizers such as AdamW and Lion, all rely on the momentum coefficient that is
customarily set to $\beta = 0.9$ and kept constant during model training, a
strategy widely used by practitioners, yet suboptimal. In this paper, we
introduce an \textit{adaptive memory} mechanism that replaces constant momentum
with a dynamic momentum coefficient that is adjusted online during
optimization. We derive our method by approximating the objective function
using two planes: one derived from the gradient at the current iterate and the
other obtained from the accumulated memory of the past gradients. To the best
of our knowledge, such a proximal framework was never used for momentum-based
optimization. Our proposed approach is novel, extremely simple to use, and does
not rely on extra assumptions or hyperparameter tuning. We implement adaptive
memory variants of both SGD and AdamW across a wide range of learning tasks,
from simple convex problems to large-scale deep learning scenarios,
demonstrating that our approach can outperform standard SGD and Adam with
hand-tuned momentum coefficients. Finally, our work opens doors for new ways of
inducing adaptivity in optimization.

</details>


### [443] [Power Transform Revisited: Numerically Stable, and Federated](https://arxiv.org/abs/2510.04995)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: Comprehensive study of numerical instabilities in power transforms, remedies to improve stability, and extension to federated learning, with empirical validation showing robustness and performance gains.


<details>
  <summary>Details</summary>
Motivation: Power transforms are widely used to Gaussianize data but can suffer from severe numerical instabilities, potentially causing incorrect results or crashes. There is a need for robust, scalable remedies and for adapting these transforms to federated settings where distributed data introduces additional challenges.

Method: Diagnose sources of instability in power transforms, develop practical remedies to address numerical and distributional issues, extend the approach to federated learning, and validate with experiments on real-world datasets.

Result: The proposed remedies substantially improve numerical stability over existing approaches, and the federated extension effectively handles distributional challenges, demonstrating robustness and practical performance gains in real data.

Conclusion: Robust, stable power transform methods are achievable, including in federated contexts, enabling reliable preprocessing for statistical and machine learning tasks.

Abstract: Power transforms are popular parametric techniques for making data more
Gaussian-like, and are widely used as preprocessing steps in statistical
analysis and machine learning. However, we find that direct implementations of
power transforms suffer from severe numerical instabilities, which can lead to
incorrect results or even crashes. In this paper, we provide a comprehensive
analysis of the sources of these instabilities and propose effective remedies.
We further extend power transforms to the federated learning setting,
addressing both numerical and distributional challenges that arise in this
context. Experiments on real-world datasets demonstrate that our methods are
both effective and robust, substantially improving stability compared to
existing approaches.

</details>


### [444] [Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training](https://arxiv.org/abs/2510.04996)
*Wei Xiong,Chenlu Ye,Baohao Liao,Hanze Dong,Xinxing Xu,Christof Monz,Jiang Bian,Nan Jiang,Tong Zhang*

Main category: cs.LG

TL;DR: Reinforce-Ada is an adaptive online RL post-training framework for LLMs that dynamically reallocates sampling effort across prompts to reduce gradient variance and accelerate convergence, outperforming GRPO, especially with balanced sampling.


<details>
  <summary>Details</summary>
Motivation: Unstable gradient estimates from fixed, uniform sampling across prompts bottleneck RL fine-tuning of LLMs. There is a need for variance-aware, adaptive data collection to improve learning efficiency and stability.

Method: Online RL post-training framework that interleaves estimation and sampling in a successive-elimination process. It reallocates sampling to prompts with highest uncertainty, stops sampling when sufficient signal is gathered, and uses fixed-size groups with diverse rewards. Advantage baselines are computed from global statistics over the adaptive sampling phase. A balanced sampling variant is analyzed by the authors.

Result: Empirical evaluation across multiple model architectures and reasoning benchmarks shows that Reinforce-Ada accelerates convergence and improves final performance compared to GRPO, with notable gains when using the balanced sampling variant.

Conclusion: Variance-aware, adaptive data curation is pivotal for efficient and reliable RL in reasoning-capable LLMs. The work provides practical techniques (online interleaved estimation/sampling, early stopping, grouped samples, global baselines) and releases code for reproduction.

Abstract: Reinforcement learning applied to large language models (LLMs) for reasoning
tasks is often bottlenecked by unstable gradient estimates due to fixed and
uniform sampling of responses across prompts. Prior work such as GVM-RAFT
addresses this by dynamically allocating inference budget per prompt to
minimize stochastic gradient variance under a budget constraint. Inspired by
this insight, we propose Reinforce-Ada, an adaptive sampling framework for
online RL post-training of LLMs that continuously reallocates sampling effort
to the prompts with the greatest uncertainty or learning potential. Unlike
conventional two-stage allocation methods, Reinforce-Ada interleaves estimation
and sampling in an online successive elimination process, and automatically
stops sampling for a prompt once sufficient signal is collected. To stabilize
updates, we form fixed-size groups with enforced reward diversity and compute
advantage baselines using global statistics aggregated over the adaptive
sampling phase. Empirical results across multiple model architectures and
reasoning benchmarks show that Reinforce-Ada accelerates convergence and
improves final performance compared to GRPO, especially when using the balanced
sampling variant. Our work highlights the central role of variance-aware,
adaptive data curation in enabling efficient and reliable reinforcement
learning for reasoning-capable LLMs. Code is available at
https://github.com/RLHFlow/Reinforce-Ada.

</details>


### [445] [Rethinking Langevin Thompson Sampling from A Stochastic Approximation Perspective](https://arxiv.org/abs/2510.05023)
*Weixin Wang,Haoyang Zheng,Guang Lin,Wei Deng,Pan Xu*

Main category: cs.LG

TL;DR: TS-SA introduces stochastic approximation into approximate Thompson Sampling by using only recent rewards with Langevin updates and temporal averaging, achieving a fixed step-size, stationary-posterior interpretation, near-optimal regret, and strong empirical gains over SGLD-based methods.


<details>
  <summary>Details</summary>
Motivation: Address non-stationarity and round-specific hyperparameter tuning in existing approximate TS (SGLD-based) algorithms for multi-armed bandits. The goal is a fixed, stationary posterior target with a unified analysis and improved practical performance.

Method: In each round, form a posterior approximation using only the most recent rewards, perform a Langevin Monte Carlo (LMC) update, and apply a stochastic-approximation (SA) step to average noisy proposals over time. This yields a fixed step-size and a unified convergence analysis by interpreting the entire algorithm as simulating a stationary SGLD process.

Result: The authors establish near-optimal regret bounds via simplified, intuitive analysis. Empirically, even a single-step Langevin update with suitable warm-up substantially outperforms existing methods on bandit tasks.

Conclusion: TS-SA provides a stable, stationary-posterior perspective on approximate Thompson Sampling, enabling fixed-step convergence analysis and practical performance gains through temporal averaging and minimal per-round computation.

Abstract: Most existing approximate Thompson Sampling (TS) algorithms for multi-armed
bandits use Stochastic Gradient Langevin Dynamics (SGLD) or its variants in
each round to sample from the posterior, relaxing the need for conjugacy
assumptions between priors and reward distributions in vanilla TS. However,
they often require approximating a different posterior distribution in
different round of the bandit problem. This requires tricky, round-specific
tuning of hyperparameters such as dynamic learning rates, causing challenges in
both theoretical analysis and practical implementation. To alleviate this
non-stationarity, we introduce TS-SA, which incorporates stochastic
approximation (SA) within the TS framework. In each round, TS-SA constructs a
posterior approximation only using the most recent reward(s), performs a
Langevin Monte Carlo (LMC) update, and applies an SA step to average noisy
proposals over time. This can be interpreted as approximating a stationary
posterior target throughout the entire algorithm, which further yields a fixed
step-size, a unified convergence analysis framework, and improved posterior
estimates through temporal averaging. We establish near-optimal regret bounds
for TS-SA, with a simplified and more intuitive theoretical analysis enabled by
interpreting the entire algorithm as a simulation of a stationary SGLD process.
Our empirical results demonstrate that even a single-step Langevin update with
certain warm-up outperforms existing methods substantially on bandit tasks.

</details>


### [446] [Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment](https://arxiv.org/abs/2510.05024)
*Nevan Wichers,Aram Ebtekar,Ariana Azarbal,Victor Gillioz,Christine Ye,Emil Ryd,Neil Rathi,Henry Sleight,Alex Mallen,Fabien Roger,Samuel Marks*

Main category: cs.LG

TL;DR: Inoculation Prompting (IP) is a simple training-time trick: explicitly request the undesired behavior in prompts used for supervision, which paradoxically reduces learning of that behavior while preserving desirable capabilities.


<details>
  <summary>Details</summary>
Motivation: When supervision signals are imperfect, models can learn undesired behaviors (e.g., reward hacking, sycophancy). Improving oversight is costly; a lightweight method to steer learning toward safer behavior is valuable.

Method: Modify prompts used during supervised fine-tuning to request the undesired behavior (e.g., ask for code that passes test cases but fails on other inputs). Evaluate IP across four settings to see if it reduces undesired behaviors without heavily harming desired capabilities. Identify that prompts eliciting the undesired behavior more strongly before fine-tuning inoculate better during training.

Result: IP reduces learning of undesired behaviors with little to no significant loss in desirable capabilities. A heuristic emerges: prompts that more strongly elicit the undesired behavior prior to fine-tuning are more effective as inoculation prompts.

Conclusion: IP is a simple, effective strategy to influence how models generalize from fine-tuning, mitigating undesired behaviors without substantially compromising desired capabilities.

Abstract: Large language models are sometimes trained with imperfect oversight signals,
leading to undesired behaviors such as reward hacking and sycophancy. Improving
oversight quality can be expensive or infeasible, motivating methods that
improve learned behavior despite an imperfect training signal. We introduce
Inoculation Prompting (IP), a simple but counterintuitive technique that
prevents learning of an undesired behavior by modifying training prompts to
explicitly request it. For example, to inoculate against reward hacking, we
modify the prompts used in supervised fine-tuning to request code that only
works on provided test cases but fails on other inputs. Across four settings we
find that IP reduces the learning of undesired behavior without substantially
reducing the learning of desired capabilities. We also show that prompts which
more strongly elicit the undesired behavior prior to fine-tuning more
effectively inoculate against the behavior when used during training; this
serves as a heuristic to identify promising inoculation prompts. Overall, IP is
a simple yet effective way to control how models generalize from fine-tuning,
preventing learning of undesired behaviors without substantially disrupting
desired capabilities.

</details>


### [447] [Graph-Aware Diffusion for Signal Generation](https://arxiv.org/abs/2510.05036)
*Sergio Rozada,Vimal K. B.,Andrea Cavallo,Antonio G. Marques,Hadi Jamali-Rad,Elvin Isufi*

Main category: cs.LG

TL;DR: Introduces Graph-Aware Diffusion (GAD) for generating signals on graphs from unknown distributions by integrating graph structure via a time-warped heat equation, yielding convergence to a Gaussian Markov random field whose covariance is tied to the graph Laplacian, and framing the reverse process as graph-signal denoising.


<details>
  <summary>Details</summary>
Motivation: There is a need to generatively model graph signals without assuming a fixed distribution or ignoring graph structure. Existing graph diffusion methods often neglect the graph in the forward process or tailor solutions to specific domains. This work aims for a general, graph-aware generative framework applicable to domains like recommender systems and sensor networks.

Method: Define a forward diffusion using the heat equation on graphs with a time-warped coefficient to counteract exponential decay of the drift. This yields a graph-aware generative diffusion model (GAD). Analyze forward dynamics and prove convergence to a Gaussian Markov random field with covariance parameterized by the graph Laplacian. Interpret the backward process as a sequence of graph-signal denoising steps.

Result: Empirically validates GAD on synthetic data, real-world traffic speed measurements, and a temperature sensor network, showing advantages over baselines.

Conclusion: GAD provides a general, graph-aware diffusion framework for graph-signal generation, with a principled forward process tied to the graph topology and a denoising-based reverse process, applicable to diverse graph-signal tasks.

Abstract: We study the problem of generating graph signals from unknown distributions
defined over given graphs, relevant to domains such as recommender systems or
sensor networks. Our approach builds on generative diffusion models, which are
well established in vision and graph generation but remain underexplored for
graph signals. Existing methods lack generality, either ignoring the graph
structure in the forward process or designing graph-aware mechanisms tailored
to specific domains. We adopt a forward process that incorporates the graph
through the heat equation. Rather than relying on the standard formulation, we
consider a time-warped coefficient to mitigate the exponential decay of the
drift term, yielding a graph-aware generative diffusion model (GAD). We analyze
its forward dynamics, proving convergence to a Gaussian Markov random field
with covariance parametrized by the graph Laplacian, and interpret the backward
dynamics as a sequence of graph-signal denoising problems. Finally, we
demonstrate the advantages of GAD on synthetic data, real traffic speed
measurements, and a temperature sensor network.

</details>


### [448] [Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts](https://arxiv.org/abs/2510.05040)
*Jihoon Lee,Hoyeon Moon,Kevin Zhai,Arun Kumar Chithanar,Anit Kumar Sahu,Soummya Kar,Chul Lee,Souradip Chakraborty,Amrit Singh Bedi*

Main category: cs.LG

TL;DR: HEX reveals that diffusion-based LLMs implicitly learn a mixture of semi-autoregressive experts; test-time ensembling across diverse block schedules (without training) dramatically improves reasoning benchmarks by leveraging multiple generation orders.


<details>
  <summary>Details</summary>
Motivation: Fixed inference schedules underutilize the latent ensemble of generation strategies learned by dLLMs. There is potential to boost performance at test time without retraining by exploiting different masking/generation orders.

Method: Identify that dLLMs trained on text implicitly encode a mixture of semi-autoregressive experts with diverse generation orders. Propose HEX: a training-free inference method that ensembles across heterogeneous block-sized generation paths via majority voting, across different masking orders.

Result: Significant performance gains on multiple benchmarks: GSM8K from 24.72% to 88.10% (3.56x); MATH from 16.40% to 40.00%; ARC-C from 54.18% to 87.80%; TruthfulQA from 28.36% to 57.46%. HEX surpasses top-K and some fine-tuned methods without additional training.

Conclusion: Test-time scaling via ensembling over generation orders is a viable paradigm for dLLMs. The schedule/masking order critically affects inference performance, and HEX provides a robust, training-free approach to mitigate single-schedule failure modes.

Abstract: Diffusion-based large language models (dLLMs) are trained flexibly to model
extreme dependence in the data distribution; however, how to best utilize this
information at inference time remains an open problem. In this work, we uncover
an interesting property of these models: dLLMs trained on textual data
implicitly learn a mixture of semi-autoregressive experts, where different
generation orders reveal different specialized behaviors. We show that
committing to any single, fixed inference time schedule, a common practice,
collapses performance by failing to leverage this latent ensemble. To address
this, we introduce HEX (Hidden semiautoregressive EXperts for test-time
scaling), a training-free inference method that ensembles across heterogeneous
block schedules. By doing a majority vote over diverse block-sized generation
paths, HEX robustly avoids failure modes associated with any single fixed
schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to
3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and
specialized fine-tuned methods like GRPO, without additional training. HEX even
yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific
reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.
Our results establish a new paradigm for test-time scaling in diffusion-based
LLMs (dLLMs), revealing that the sequence in which masking is performed plays a
critical role in determining performance during inference.

</details>


### [449] [KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code Embeddings](https://arxiv.org/abs/2510.05049)
*Ahmed Elhussein,Paul Meddeb,Abigail Newbury,Jeanne Mirone,Martin Stoll,Gamze Gursoy*

Main category: cs.LG

TL;DR: KEEP combines knowledge graph embeddings with adaptive, regularized learning from clinical data to preserve ontological relationships while capturing empirical patterns, enabling multi-task applicability with low computational cost.


<details>
  <summary>Details</summary>
Motivation: There is a fundamental trade-off: knowledge-graph–based methods retain structured medical knowledge but miss real-world usage patterns, while data-driven methods learn empirical associations but often ignore structured terminologies. A framework that preserves ontology while leveraging data could improve semantic understanding and downstream predictions in healthcare representations, especially under resource constraints.

Method: A two-stage approach: (1) generate embeddings from knowledge graphs representing medical terminologies; (2) perform regularized training on patient records to adaptively integrate empirical patterns while preserving ontological relationships, producing final embeddings without task-specific auxiliary losses or end-to-end training, enabling reuse across multiple downstream tasks and architectures.

Result: On structured EHR data from UK Biobank and MIMIC IV, KEEP outperforms traditional and Language Model–based approaches in capturing semantic relationships and predicting clinical outcomes, and requires minimal computational resources suitable for resource-constrained environments.

Conclusion: KEEP offers a versatile, knowledge-preserving embedding framework that bridges ontology-based and data-driven representations, enabling broad applicability across models and tasks with efficient computation.

Abstract: Machine learning in healthcare requires effective representation of
structured medical codes, but current methods face a trade off: knowledge graph
based approaches capture formal relationships but miss real world patterns,
while data driven methods learn empirical associations but often overlook
structured knowledge in medical terminologies. We present KEEP (Knowledge
preserving and Empirically refined Embedding Process), an efficient framework
that bridges this gap by combining knowledge graph embeddings with adaptive
learning from clinical data. KEEP first generates embeddings from knowledge
graphs, then employs regularized training on patient records to adaptively
integrate empirical patterns while preserving ontological relationships.
Importantly, KEEP produces final embeddings without task specific auxiliary or
end to end training enabling KEEP to support multiple downstream applications
and model architectures. Evaluations on structured EHR from UK Biobank and
MIMIC IV demonstrate that KEEP outperforms both traditional and Language Model
based approaches in capturing semantic relationships and predicting clinical
outcomes. Moreover, KEEP's minimal computational requirements make it
particularly suitable for resource constrained environments.

</details>


### [450] [HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a Single Hybrid Model](https://arxiv.org/abs/2510.05054)
*Peter Van Katwyk,Karianne J. Bergen*

Main category: cs.LG

TL;DR: HybridFlow is a modular framework that unifies aleatoric and epistemic uncertainty in regression by combining a Conditional Masked Autoregressive normalizing flow for aleatoric uncertainty with a flexible probabilistic predictor for epistemic uncertainty, achieving calibrated uncertainty across tasks.


<details>
  <summary>Details</summary>
Motivation: Reliable uncertainty quantification is crucial in high-stakes ML. Existing approaches typically model aleatoric and epistemic uncertainty separately and may lack calibration or modular compatibility. A unified, plug‑and‑play framework that can integrate with any probabilistic model class promises better reliability without sacrificing performance.

Method: Proposes HybridFlow, a two‑component architecture: (1) a Conditional Masked Autoregressive normalizing flow to model aleatoric uncertainty, and (2) a flexible probabilistic predictor to model epistemic uncertainty. The design is modular and compatible with arbitrary probabilistic model classes, enabling seamless integration with existing architectures. Evaluated on regression tasks including depth estimation, multiple regression benchmarks, and an ice sheet emulation case study.

Result: Empirical results show that HybridFlow yields calibrated uncertainty that better aligns with model error than existing methods for both aleatoric and epistemic components, and improves predictive performance across the tested regression tasks.

Conclusion: HybridFlow tackles a central challenge in Bayesian deep learning by unifying aleatoric and epistemic uncertainty in a single robust framework and supports integration with any probabilistic model class, offering improved calibration and predictive performance.

Abstract: Uncertainty quantification is critical for ensuring robustness in high-stakes
machine learning applications. We introduce HybridFlow, a modular hybrid
architecture that unifies the modeling of aleatoric and epistemic uncertainty
by combining a Conditional Masked Autoregressive normalizing flow for
estimating aleatoric uncertainty with a flexible probabilistic predictor for
epistemic uncertainty. The framework supports integration with any
probabilistic model class, allowing users to easily adapt HybridFlow to
existing architectures without sacrificing predictive performance. HybridFlow
improves upon previous uncertainty quantification frameworks across a range of
regression tasks, such as depth estimation, a collection of regression
benchmarks, and a scientific case study of ice sheet emulation. We also provide
empirical results of the quantified uncertainty, showing that the uncertainty
quantified by HybridFlow is calibrated and better aligns with model error than
existing methods for quantifying aleatoric and epistemic uncertainty.
HybridFlow addresses a key challenge in Bayesian deep learning, unifying
aleatoric and epistemic uncertainty modeling in a single robust framework.

</details>


### [451] [Learning to Interpret Weight Differences in Language Models](https://arxiv.org/abs/2510.05092)
*Avichal Goel,Yoon Kim,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: Diff Interpretation Tuning (DIT) trains an adapter on synthetic weight-diffs to produce natural language explanations of how a model changed during fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Interpreting weight diffs is hard; finetuning datasets are often private or too large; there is a need for readable, automatic explanations of parameter updates.

Method: Train a DIT adapter using synthetic, labeled weight diffs; apply to a compatible finetuned model to generate descriptions of its modifications.

Result: In two proof-of-concept settings, DIT enables models to describe their finetuning-induced modifications with accurate natural language descriptions (e.g., revealing hidden behaviors and summarizing learned knowledge).

Conclusion: DIT provides a scalable approach to interpret model updates via natural language, supporting transparency, though demonstrated in limited, synthetic-data experiments and requiring further validation on real diffs.

Abstract: Finetuning (pretrained) language models is a standard approach for updating
their internal parametric knowledge and specializing them to new tasks and
domains. However, the corresponding model weight changes ("weight diffs") are
not generally interpretable. While inspecting the finetuning dataset can give a
sense of how the model might have changed, these datasets are often not
publicly available or are too large to work with directly. Towards the goal of
comprehensively understanding weight diffs in natural language, we introduce
Diff Interpretation Tuning (DIT), a method that trains models to describe their
own finetuning-induced modifications. Our approach uses synthetic, labeled
weight diffs to train a DIT adapter, which can be applied to a compatible
finetuned model to make it describe how it has changed. We demonstrate in two
proof-of-concept settings (reporting hidden behaviors and summarizing finetuned
knowledge) that our method enables models to describe their finetuning-induced
modifications using accurate natural language descriptions.

</details>


### [452] [Modeling Student Learning with 3.8 Million Program Traces](https://arxiv.org/abs/2510.05056)
*Alexis Ross,Megha Srivastava,Jeremiah Blanchard,Jacob Andreas*

Main category: cs.LG

TL;DR: Dataset of 3.8M Pencil Code reasoning traces; LM trained on real traces outperforms on modeling student behavior, allows predicting trace properties from student representations, and enables steering edits to improve code while preserving student style.


<details>
  <summary>Details</summary>
Motivation: Understand what can be learned from student coding traces and whether training on traces reveals about learners, not just code.

Method: Assemble Pencil Code traces dataset; train LMs on real traces vs final programs/synthetic traces; conduct behavioral and probing analyses; evaluate property predictability; implement a steering mechanism to guide edits towards correctness while maintaining style.

Result: Real-trace–trained models better model diverse student behavior; trace properties (e.g., goal backtracking, number of comments) forecastable from student representations; steering approach improves correction of code while staying close to student style; increased steerability and predictiveness.

Conclusion: Code properties reflect individual student traits; training on edit traces yields models that are more steerable and better at generating final-state programs, highlighting the value of student-specific traces.

Abstract: As programmers write code, they often edit and retry multiple times, creating
rich "interaction traces" that reveal how they approach coding tasks and
provide clues about their level of skill development. For novice programmers in
particular, these traces reflect the diverse reasoning processes they employ to
code, such as exploratory behavior to understand how a programming concept
works, re-strategizing in response to bugs, and personalizing stylistic
choices. In this work, we explore what can be learned from training language
models on such reasoning traces: not just about code, but about coders, and
particularly students learning to program. We introduce a dataset of over 3.8
million programming reasoning traces from users of Pencil Code, a free online
educational platform used by students to learn simple programming concepts.
Compared to models trained only on final programs or synthetically-generated
traces, we find that models trained on real traces are stronger at modeling
diverse student behavior. Through both behavioral and probing analyses, we also
find that many properties of code traces, such as goal backtracking or number
of comments, can be predicted from learned representations of the students who
write them. Building on this result, we show that we can help students recover
from mistakes by steering code generation models to identify a sequence of
edits that will results in more correct code while remaining close to the
original student's style. Together, our results suggest that many properties of
code are properties of individual students and that training on edit traces can
lead to models that are more steerable, more predictive of student behavior
while programming, and better at generating programs in their final states.
Code and data is available at https://github.com/meghabyte/pencilcode-public

</details>


### [453] [From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models](https://arxiv.org/abs/2510.05095)
*Mingkang Zhu,Xi Chen,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.LG

TL;DR: A simple drop-in method (BVPO) for aligning Large Reasoning Models by mixing a high-variance trace-based gradient with a low-variance empty-trace gradient to trade bias and variance, reducing variance from tracing and improving alignment and reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Aligning LRMs with human preferences is essential for deployment, but marginalizing over reasoning traces is intractable. Training with a single sampled trace causes high gradient variance, hindering stable optimization.

Method: Introduce Bias–Variance Optimized Preference Optimization (BVPO), which forms a mixture of two gradient estimators: (i) a trace-based, high-variance gradient that uses reasoning traces, and (ii) an empty-trace, low-variance gradient obtained by disabling reasoning traces. The work provides (a) a theoretical result that any nontrivial mixture reduces trace-induced variance, (b) a closed-form expression for the mixing weight that minimizes mean-squared error relative to the true marginal gradient, and (c) convergence guarantees under standard smoothness and step-size assumptions.

Result: Empirically, BVPO improves alignment over the best baseline by up to 7.8 points on AlpacaEval 2 and 6.8 points on Arena-Hard. It also boosts reasoning performance for base models by up to 4.0 points on six math-reasoning benchmarks, despite training only on general conversational data.

Conclusion: Directly optimizing the bias–variance trade-off yields more stable training and stronger overall performance for LRMs. BVPO is a simple, drop-in method that identifies trace-sampling variance as a key bottleneck and mitigates it through a principled gradient-mixture approach.

Abstract: Large reasoning models (LRMs) generate intermediate reasoning traces before
producing final answers, yielding strong gains on multi-step and mathematical
tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for
model deployment, remains underexplored. The statistically correct objective
for preference alignment requires marginalizing over reasoning traces, but this
computation is intractable in practice. A common workaround optimizes a single
sampled trajectory, which introduces substantial gradient variance from
stochastic trace sampling. To address this challenge, we frame preference
optimization for LRMs through the lens of the bias--variance trade-off and
propose Bias--Variance Optimized Preference Optimization (BVPO), a simple,
drop-in method that mixes two gradient estimators: a high-variance trace-based
estimator and a low-variance empty-trace estimator obtained by disabling
reasoning trace generation. Our theory shows that BVPO strictly reduces
trace-induced variance for any nontrivial mixture, provides a closed-form
choice of the mixing weight that minimizes mean-squared error relative to the
true marginal gradient, and under standard smoothness and step-size conditions,
tightens classical convergence bounds for stochastic gradient descent.
Empirically, BVPO improves alignment over the best baseline by up to 7.8 points
on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on
general conversational data, BVPO also boosts reasoning performance for base
models by up to 4.0 points on the average of six math reasoning benchmarks.
These results identify variance from trace sampling as a key bottleneck and
demonstrate that directly optimizing the bias--variance trade-off yields more
stable training and stronger overall performance.

</details>


### [454] [ResCP: Reservoir Conformal Prediction for Time Series Forecasting](https://arxiv.org/abs/2510.05060)
*Roberto Neglia,Andrea Cini,Michael M. Bronstein,Filippo Maria Bianchi*

Main category: cs.LG

TL;DR: ResCP is a training-free conformal predictor for time series that uses reservoir computing to dynamically reweight conformity scores, enabling asymptotic conditional coverage with scalable computation.


<details>
  <summary>Details</summary>
Motivation: Existing conformal methods for sequential data rely on complex models and often retraining, which hurts performance on small samples and under distribution shifts. A training-free, scalable approach that adapts to local dynamics is highly desirable.

Method: ResCP leverages reservoir computing to generate state representations of the time series. It computes similarity scores among reservoir states and uses these to adaptively reweight observed residuals (conformity scores) at each prediction step, without any model retraining.

Result: Under reasonable assumptions, ResCP achieves asymptotic conditional coverage. Empirical experiments demonstrate strong performance and scalability across diverse forecasting tasks.

Conclusion: ResCP provides a practical, training-free framework for conformal prediction on time series, balancing coverage guarantees with computational efficiency and adaptability to local temporal dynamics.

Abstract: Conformal prediction offers a powerful framework for building
distribution-free prediction intervals for exchangeable data. Existing methods
that extend conformal prediction to sequential data rely on fitting a
relatively complex model to capture temporal dependencies. However, these
methods can fail if the sample size is small and often require expensive
retraining when the underlying data distribution changes. To overcome these
limitations, we propose Reservoir Conformal Prediction (ResCP), a novel
training-free conformal prediction method for time series. Our approach
leverages the efficiency and representation learning capabilities of reservoir
computing to dynamically reweight conformity scores. In particular, we compute
similarity scores among reservoir states and use them to adaptively reweight
the observed residuals at each step. With this approach, ResCP enables us to
account for local temporal dynamics when modeling the error distribution
without compromising computational scalability. We prove that, under reasonable
assumptions, ResCP achieves asymptotic conditional coverage, and we empirically
demonstrate its effectiveness across diverse forecasting tasks.

</details>


### [455] [Boomerang Distillation Enables Zero-Shot Model Size Interpolation](https://arxiv.org/abs/2510.05064)
*Sara Kangaslahti,Nihal V. Nayak,Jonathan Geuter,Marco Fumero,Francesco Locatello,David Alvarez-Melis*

Main category: cs.LG

TL;DR: A zero-shot method to interpolate many intermediate model sizes between a small student and a large teacher by pruning and re-incorporating teacher blocks, yielding fine-grained, cost-efficient size families with smooth performance.


<details>
  <summary>Details</summary>
Motivation: Deployments require varied memory and compute; training separate sizes is expensive and yields coarse granularity. A fine-grained, cost-efficient sizing method is highly desirable.

Method: Start from a large base teacher and distill to a small student. Then progressively reconstruct intermediate-sized models by re-incorporating blocks of teacher layers into the student without any additional training (zero-shot). Analyze when alignment between teacher and student (via pruning and distillation) enables successful interpolation.

Result: Intermediates achieve performance that matches or exceeds pretrained or distilled models of the same size. The size-to-performance curve is smooth between student and teacher. The approach dramatically reduces training cost and enables flexible deployment. Code and models are provided at the authors' repository.

Conclusion: Boomerang distillation offers a simple, efficient way to generate fine-grained model families with consistent performance, contingent on alignment between teacher and student via pruning/distillation; useful for deployment across diverse environments.

Abstract: Large language models (LLMs) are typically deployed under diverse memory and
compute constraints. Existing approaches build model families by training each
size independently, which is prohibitively expensive and provides only
coarse-grained size options. In this work, we identify a novel phenomenon that
we call boomerang distillation: starting from a large base model (the teacher),
one first distills down to a small student and then progressively reconstructs
intermediate-sized models by re-incorporating blocks of teacher layers into the
student without any additional training. This process produces zero-shot
interpolated models of many intermediate sizes whose performance scales
smoothly between the student and teacher, often matching or surpassing
pretrained or distilled models of the same size. We further analyze when this
type of interpolation succeeds, showing that alignment between teacher and
student through pruning and distillation is essential. Boomerang distillation
thus provides a simple and efficient way to generate fine-grained model
families, dramatically reducing training cost while enabling flexible
adaptation across deployment environments. The code and models are available at
https://github.com/dcml-lab/boomerang-distillation.

</details>


### [456] [MICROTRIPS: MICRO-geography TRavel Intelligence and Pattern Synthesis](https://arxiv.org/abs/2510.05080)
*Yangyang Wang,Tayo Fabusuyi*

Main category: cs.LG

TL;DR: A small-area travel-forecast framework using public microdata and machine learning to estimate trip generation, distribution, mode choice, and routing at fine geographic scales, with ACS/PUMS validation and policy applications.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of four-step travel models for local-level planning by delivering high-resolution, accurate, and equitable travel insights to guide targeted interventions.

Method: Utilizes publicly available microdata to build a representative synthetic population for small areas and applies machine learning to predict travel behavior across trip generation, distribution, mode choice, and route assignment; validation against ACS/PUMS work-commute data; comparison to conventional approaches.

Result: Achieves higher accuracy than conventional methods; enables granular, small-area travel estimates; supports targeted interventions such as micro-fulfillment site placement, curb-space management, and inclusive transportation solutions for vulnerable communities.

Conclusion: Provides a granular, adaptable framework for localized transportation planning and policy tailoring, with demonstrated potential to improve equity and efficiency in urban systems.

Abstract: This study presents a novel small-area estimation framework to enhance urban
transportation planning through detailed characterization of travel behavior.
Our approach improves on the four-step travel model by employing publicly
available microdata files and machine learning methods to predict travel
behavior for a representative, synthetic population at small geographic areas.
This approach enables high-resolution estimation of trip generation, trip
distribution, mode choice, and route assignment. Validation using ACS/PUMS
work-commute datasets demonstrates that our framework achieves higher accuracy
compared to conventional approaches. The resulting granular insights enable the
tailoring of interventions to address localized situations and support a range
of policy applications and targeted interventions, including the optimal
placement of micro-fulfillment centers, effective curb-space management, and
the design of more inclusive transportation solutions particularly for
vulnerable communities.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [457] [Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer](https://arxiv.org/abs/2510.03342)
*Abbas Abdolmaleki,Saminda Abeyruwan,Joshua Ainslie,Jean-Baptiste Alayrac,Montserrat Gonzalez Arenas,Ashwin Balakrishna,Nathan Batchelor,Alex Bewley,Jeff Bingham,Michael Bloesch,Konstantinos Bousmalis,Philemon Brakel,Anthony Brohan,Thomas Buschmann,Arunkumar Byravan,Serkan Cabi,Ken Caluwaerts,Federico Casarini,Christine Chan,Oscar Chang,London Chappellet-Volpini,Jose Enrique Chen,Xi Chen,Hao-Tien Lewis Chiang,Krzysztof Choromanski,Adrian Collister,David B. D'Ambrosio,Sudeep Dasari,Todor Davchev,Meet Kirankumar Dave,Coline Devin,Norman Di Palo,Tianli Ding,Carl Doersch,Adil Dostmohamed,Yilun Du,Debidatta Dwibedi,Sathish Thoppay Egambaram,Michael Elabd,Tom Erez,Xiaolin Fang,Claudio Fantacci,Cody Fong,Erik Frey,Chuyuan Fu,Ruiqi Gao,Marissa Giustina,Keerthana Gopalakrishnan,Laura Graesser,Oliver Groth,Agrim Gupta,Roland Hafner,Steven Hansen,Leonard Hasenclever,Sam Haves,Nicolas Heess,Brandon Hernaez,Alex Hofer,Jasmine Hsu,Lu Huang,Sandy H. Huang,Atil Iscen,Mithun George Jacob,Deepali Jain,Sally Jesmonth,Abhishek Jindal,Ryan Julian,Dmitry Kalashnikov,M. Emre Karagozler,Stefani Karp,Matija Kecman,J. Chase Kew,Donnie Kim,Frank Kim,Junkyung Kim,Thomas Kipf,Sean Kirmani,Ksenia Konyushkova,Li Yang Ku,Yuheng Kuang,Thomas Lampe,Antoine Laurens,Tuan Anh Le,Isabel Leal,Alex X. Lee,Tsang-Wei Edward Lee,Guy Lever,Jacky Liang,Li-Heng Lin,Fangchen Liu,Shangbang Long,Caden Lu,Sharath Maddineni,Anirudha Majumdar,Kevis-Kokitsi Maninis,Andrew Marmon,Sergio Martinez,Assaf Hurwitz Michaely,Niko Milonopoulos,Joss Moore,Robert Moreno,Michael Neunert,Francesco Nori,Joy Ortiz,Kenneth Oslund,Carolina Parada,Emilio Parisotto,Amaris Paryag,Acorn Pooley,Thomas Power,Alessio Quaglino,Haroon Qureshi,Rajkumar Vasudeva Raju,Helen Ran,Dushyant Rao,Kanishka Rao,Isaac Reid,David Rendleman,Krista Reymann,Miguel Rivas,Francesco Romano,Yulia Rubanova,Peter Pastor Sampedro,Pannag R Sanketi,Dhruv Shah,Mohit Sharma,Kathryn Shea,Mohit Shridhar,Charles Shu,Vikas Sindhwani,Sumeet Singh,Radu Soricut,Rachel Sterneck,Ian Storz,Razvan Surdulescu,Jie Tan,Jonathan Tompson,Saran Tunyasuvunakool,Jake Varley,Grace Vesom,Giulia Vezzani,Maria Bauza Villalonga,Oriol Vinyals,René Wagner,Ayzaan Wahid,Stefan Welker,Paul Wohlhart,Chengda Wu,Markus Wulfmeier,Fei Xia,Ted Xiao,Annie Xie,Jinyu Xie,Peng Xu,Sichun Xu,Ying Xu,Zhuo Xu,Jimmy Yan,Sherry Yang,Skye Yang,Yuxiang Yang,Hiu Hong Yu,Wenhao Yu,Wentao Yuan,Yuan Yuan,Jingwei Zhang,Tingnan Zhang,Zhiyuan Zhang,Allan Zhou,Guangyao Zhou,Yuxiang Zhou*

Main category: cs.RO

TL;DR: Gemini Robotics 1.5 and ER 1.5 introduce a cross-embodiment Vision-Language-Action (VLA) model and an Embodied Reasoning (ER) model with a Motion Transfer mechanism, enabling think-before-act reasoning, better multi-step task execution, and state-of-the-art embodied reasoning across multiple robot embodiments.


<details>
  <summary>Details</summary>
Motivation: General-purpose robots require deep physical understanding, advanced reasoning, and dexterous control. The abstract highlights limitations in learning across heterogeneous robot embodiments and in interpretable planning, aiming to unify VLA and ER capabilities to enable robust, multi-step task solving.

Method: Three innovations are presented: (1) a novel architecture with a Motion Transfer (MT) mechanism that learns from heterogeneous, multi-embodiment robot data to generalize the VLA across embodiments; (2) interleaving actions with a multi-level internal reasoning process in natural language to enable thinking before acting and improve decomposition/execution of complex tasks and interpretability; (3) Gemini Robotics-ER 1.5 achieving new state-of-the-art in embodied reasoning (visual/spatial understanding, task planning, progress estimation).

Result: Improved generalization of the VLA across multiple robot embodiments, better decomposition and execution of complex multi-step tasks due to think-before-act reasoning, and a new state-of-the-art in embodied reasoning metrics, marking a step toward robots that perceive, think, and act.

Conclusion: This family advances toward physically capable agents that perceive, reason, and act to solve complex tasks, enabling more general, interpretable, and capable robots across diverse embodiments.

Abstract: General-purpose robots need a deep understanding of the physical world,
advanced reasoning, and general and dexterous control. This report introduces
the latest generation of the Gemini Robotics model family: Gemini Robotics 1.5,
a multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER
1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing together
three major innovations. First, Gemini Robotics 1.5 features a novel
architecture and a Motion Transfer (MT) mechanism, which enables it to learn
from heterogeneous, multi-embodiment robot data and makes the VLA more general.
Second, Gemini Robotics 1.5 interleaves actions with a multi-level internal
reasoning process in natural language. This enables the robot to "think before
acting" and notably improves its ability to decompose and execute complex,
multi-step tasks, and also makes the robot's behavior more interpretable to the
user. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art for
embodied reasoning, i.e., for reasoning capabilities that are critical for
robots, such as visual and spatial understanding, task planning, and progress
estimation. Together, this family of models takes us a step towards an era of
physical agents-enabling robots to perceive, think and then act so they can
solve complex multi-step tasks.

</details>


### [458] [Optimal swimming with body compliance in an overdamped medium](https://arxiv.org/abs/2510.03457)
*Jianfeng Lin,Tianyu Wang,Baxi Chong,Matthew Fernandez,Zhaochen Xu,Daniel I. Goldman*

Main category: cs.RO

TL;DR: Extends geometric mechanics to compliant undulators by adding joint springs, enabling prediction and optimization of robust swimming in diverse environments.


<details>
  <summary>Details</summary>
Motivation: Existing geometric mechanics frameworks for undulatory locomotion assume precise gait execution and rigid bodies. In real animals/robots, compliant joints and environmental perturbations cause deviations that degrade performance. There is a need for a predictive framework that accounts for compliance and optimizes strategies.

Method: Introduce a compliant extension of Purcell's three-link swimmer with series springs at the joints. Derive body dynamics using resistive force theory. Integrate geometric mechanics into movement prediction and into an optimization framework to identify control strategies that maximize displacement, validated on a cable-driven three-link limbless robot with state-dependent compliance in granular media.

Result: The framework accurately predicts locomotor performance and enables optimization of compliant swimmers. Validation on a physical robot shows accurate prediction of performance and effective strategies under varied programmed, state-dependent compliance in granular environments, illustrating robustness due to compliance.

Conclusion: Compliance can be exploited as a design feature to achieve robust locomotion in homogeneous and heterogeneous environments. The paper provides a systematic, physics-based approach to modeling and controlling compliant swimming.

Abstract: Elongate animals and robots use undulatory body waves to locomote through
diverse environments. Geometric mechanics provides a framework to model and
optimize such systems in highly damped environments, connecting a prescribed
shape change pattern (gait) with locomotion displacement. However, existing
approaches assume precise execution of prescribed gaits, whereas in practice
environmental interactions with compliant bodies of animals or robots
frequently perturb the realized trajectories. In this work, we extend geometric
mechanics to predict locomotor performance and search for optimal swimming
strategy of compliant undulators. We introduce a compliant extension of
Purcell's three-link swimmer by incorporating series-connected springs at the
joints. Body dynamics are derived with resistive force theory. Geometric
mechanics is incorporated into movement prediction and into an optimization
framework that identifies strategies for controlling compliant swimmers to
achieve maximal displacement. We validate our framework on a physical
cable-driven three-link limbless robot, and demonstrate accurate prediction and
optimization of locomotor performance under varied programmed, state-dependent
compliance in a granular medium. Our results establish a systematic
physics-based approach for modeling and controlling compliant swimming
locomotion, highlighting compliance as a design feature that can be exploited
for robust movement in homogeneous and heterogeneous environments.

</details>


### [459] [Warm-Starting Optimization-Based Motion Planning for Robotic Manipulators via Point Cloud-Conditioned Flow Matching](https://arxiv.org/abs/2510.03460)
*Sibo Tian,Minghui Zheng,Xiao Liang*

Main category: cs.RO

TL;DR: Learning-based Flow Matching initializer conditioned on a single-view depth point cloud enables near-optimal optimization initialization for fast, safe motion planning in cluttered environments, improving success rate and reducing iterations without prior environment knowledge.


<details>
  <summary>Details</summary>
Motivation: Sampling-based planners struggle with high-dim spaces and require post-processing; optimization-based planners risk local minima and initialization sensitivity; need for real-time, obstacle-agnostic planning from single-view input in human-robot collaboration.

Method: Train a Flow Matching model conditioned on a single-view point cloud to output near-optimal optimization initializations for trajectory planning, enabling direct generation of feasible trajectories from depth camera input; evaluated on UR5e in cluttered workspaces.

Result: The generative initializer achieves high success rates, substantially improves trajectory optimization success versus traditional and learning baselines, reduces number of optimization iterations, and generalizes to unseen environments.

Conclusion: A single-view, learning-based initializer can accelerate optimization-based motion planning in real-time HRC, without requiring explicit obstacle maps, offering robust performance in cluttered, unseen environments.

Abstract: Rapid robot motion generation is critical in Human-Robot Collaboration (HRC)
systems, as robots need to respond to dynamic environments in real time by
continuously observing their surroundings and replanning their motions to
ensure both safe interactions and efficient task execution. Current
sampling-based motion planners face challenges in scaling to high-dimensional
configuration spaces and often require post-processing to interpolate and
smooth the generated paths, resulting in time inefficiency in complex
environments. Optimization-based planners, on the other hand, can incorporate
multiple constraints and generate smooth trajectories directly, making them
potentially more time-efficient. However, optimization-based planners are
sensitive to initialization and may get stuck in local minima. In this work, we
present a novel learning-based method that utilizes a Flow Matching model
conditioned on a single-view point cloud to learn near-optimal solutions for
optimization initialization. Our method does not require prior knowledge of the
environment, such as obstacle locations and geometries, and can generate
feasible trajectories directly from single-view depth camera input. Simulation
studies on a UR5e robotic manipulator in cluttered workspaces demonstrate that
the proposed generative initializer achieves a high success rate on its own,
significantly improves the success rate of trajectory optimization compared
with traditional and learning-based benchmark initializers, requires fewer
optimization iterations, and exhibits strong generalization to unseen
environments.

</details>


### [460] [A Simulation Evaluation Suite for Robust Adaptive Quadcopter Control](https://arxiv.org/abs/2510.03471)
*Dingqi Zhang,Ran Tao,Sheng Cheng,Naira Hovakimyan,Mark W. Mueller*

Main category: cs.RO

TL;DR: A modular RotorPy-based quadcopter simulation testbed (AdaptiveQuadBench) for robust adaptive control evaluation under disturbances, featuring a library of controllers and standardized metrics to enable reproducible benchmarking across tasks; code released.


<details>
  <summary>Details</summary>
Motivation: Fragmented evaluations across tasks, simulators, and implementations hinder objective comparison of robust/adaptive quadcopter controllers; a unified, easy-to-deploy testbed is needed for systematic assessment.

Method: Develop a modular, easy-to-deploy simulation framework on RotorPy; implement disturbance models (wind, payload shifts, rotor faults, control latency); integrate a library of adaptive and non-adaptive controllers; define task-relevant metrics for tracking/robustness; provide automated stress testing and multiple disturbance/trajectory scenarios; enable reproducible experiments; release code.

Result: The testbed enables reproducible evaluation across control methods, reduces redundant reimplementation, and demonstrates versatility through varied disturbance scenarios and trajectories; supports automated stress testing; code available.

Conclusion: This framework fills a gap in systematic benchmarking of quadcopter control methods, enabling consistent comparisons and facilitating broader adoption and future extensions.

Abstract: Robust adaptive control methods are essential for maintaining quadcopter
performance under external disturbances and model uncertainties. However,
fragmented evaluations across tasks, simulators, and implementations hinder
systematic comparison of these methods. This paper introduces an
easy-to-deploy, modular simulation testbed for quadcopter control, built on
RotorPy, that enables evaluation under a wide range of disturbances such as
wind, payload shifts, rotor faults, and control latency. The framework includes
a library of representative adaptive and non-adaptive controllers and provides
task-relevant metrics to assess tracking accuracy and robustness. The unified
modular environment enables reproducible evaluation across control methods and
eliminates redundant reimplementation of components such as disturbance models,
trajectory generators, and analysis tools. We illustrate the testbed's
versatility through examples spanning multiple disturbance scenarios and
trajectory types, including automated stress testing, to demonstrate its
utility for systematic analysis. Code is available at
https://github.com/Dz298/AdaptiveQuadBench.

</details>


### [461] [Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems](https://arxiv.org/abs/2510.03472)
*Yulun Zhang,Alexandre O. G. Barbosa,Federico Pecora,Jiaoyang Li*

Main category: cs.RO

TL;DR: Optimizes destination-to-chutes mappings in robotic sorting to boost throughput, using a formal Task Mapping Optimization (TMO) framework with an Evolutionary Algorithm and MILP, validated by a simulator and Quality Diversity analysis, outperforming greedy mappings and offering diverse high-throughput solutions.


<details>
  <summary>Details</summary>
Motivation: In RSS, task mapping interacts with robot assignment and path planning; chutes close after receiving enough packages, increasing handling time if mappings are poor. A high-quality mapping reduces scattered chutes and downstream delays, but solving TMO is complex due to interdependencies and dynamic chute availability.

Method: Define TMO formally; develop a simulator for RSS; propose a simple TMO method combining Evolutionary Algorithm (EA) and Mixed-Integer Linear Programming (MILP); compare against greedy mappings across varying map sizes, chute counts, and destinations; apply Quality Diversity (QD) algorithms to explore and analyze a diverse set of high-throughput mappings; release code publicly.

Result: Optimized task mappings via the EA+MILP approach yield higher throughput than greedily generated mappings across tested RSS configurations; the method’s performance is demonstrated under different map sizes, numbers of chutes, and destinations; QD analysis reveals a diverse set of high-quality mappings, highlighting trade-offs and robustness across scenarios.

Conclusion: A formal TMO framework with simulator support and EA+MILP optimization can substantially improve throughput in RSS by producing superior and diverse destination-to-chutes mappings; open-source tooling enables replication and further exploration of mapping strategies.

Abstract: We study optimizing a destination-to-chutes task mapping to improve
throughput in Robotic Sorting Systems (RSS), where a team of robots sort
packages on a sortation floor by transporting them from induct workstations to
eject chutes based on their shipping destinations (e.g. Los Angeles or
Pittsburgh). The destination-to-chutes task mapping is used to determine which
chutes a robot can drop its package. Finding a high-quality task mapping is
challenging because of the complexity of a real-world RSS. First, optimizing
task mapping is interdependent with robot target assignment and path planning.
Second, chutes will be CLOSED for a period of time once they receive sufficient
packages to allow for downstream processing. Third, task mapping quality
directly impacts the downstream processing, as scattered chutes for the same
destination increase package handling time. In this paper, we first formally
define task mappings and the problem of Task Mapping Optimization (TMO). We
then present a simulator of RSS to evaluate task mappings. We then present a
simple TMO method based on the Evolutionary Algorithm and Mixed Integer Linear
Programming, demonstrating the advantage of our optimized task mappings over
the greedily generated ones in various RSS setups with different map sizes,
numbers of chutes, and destinations. Finally, we use Quality Diversity
algorithms to analyze the throughput of a diverse set of task mappings. Our
code is available online at https://github.com/lunjohnzhang/tmo_public.

</details>


### [462] [Robust Permissive Controller Synthesis for Interval MDPs](https://arxiv.org/abs/2510.03481)
*Khang Vo Huynh,David Parker,Lu Feng*

Main category: cs.RO

TL;DR: Introduces a robust, permissive controller synthesis framework for Interval MDPs (IMDPs) in robotics, enabling multiple actions per state and guaranteeing correctness under all uncertainty intervals via MILP-based encodings. It yields scalable, maximally permissive controllers for uncertain dynamics.


<details>
  <summary>Details</summary>
Motivation: Robotics face epistemic uncertainty from sensing noise, actuation imprecision, and coarse abstractions. Traditional single-strategy controllers are brittle and inflexible; permissive controllers improve resilience by allowing adaptation at runtime. There was no prior robust framework for permissive synthesis under interval probabilities.

Method: Formulate robust permissive synthesis as mixed-integer linear programs (MILPs). Propose two encodings: (1) a baseline vertex-enumeration method, and (2) a scalable duality-based method that avoids explicit state-action enumeration. The formulations ensure that every multi-strategy compliant with the synthesized controller satisfies reachability or reward specifications for all admissible transitions.

Result: Both encodings produce robust, maximally permissive controllers. Experiments on four benchmarks show scalability to large IMDPs with hundreds of thousands of states, validating practicality for complex robotic domains.

Conclusion: This work is the first to provide a framework for robust permissive controller synthesis on IMDPs, enabling adaptable and reliable robotic control under uncertainty. The duality-based encoding offers scalability advantages, broadening applicability to large-scale systems; future work may extend to other specs and optimize computation further.

Abstract: We address the problem of robust permissive controller synthesis for robots
operating under uncertain dynamics, modeled as Interval Markov Decision
Processes (IMDPs). IMDPs generalize standard MDPs by allowing transition
probabilities to vary within intervals, capturing epistemic uncertainty from
sensing noise, actuation imprecision, and coarse system abstractions-common in
robotics. Traditional controller synthesis typically yields a single
deterministic strategy, limiting adaptability. In contrast, permissive
controllers (multi-strategies) allow multiple actions per state, enabling
runtime flexibility and resilience. However, prior work on permissive
controller synthesis generally assumes exact transition probabilities, which is
unrealistic in many robotic applications. We present the first framework for
robust permissive controller synthesis on IMDPs, guaranteeing that all
strategies compliant with the synthesized multi-strategy satisfy reachability
or reward-based specifications under all admissible transitions. We formulate
the problem as mixed-integer linear programs (MILPs) and propose two encodings:
a baseline vertex-enumeration method and a scalable duality-based method that
avoids explicit enumeration. Experiments on four benchmark domains show that
both methods synthesize robust, maximally permissive controllers and scale to
large IMDPs with up to hundreds of thousands of states.

</details>


### [463] [Digital-Twin Evaluation for Proactive Human-Robot Collision Avoidance via Prediction-Guided A-RRT*](https://arxiv.org/abs/2510.03496)
*Vadivelan Murugesan,Rajasundaram Mathiazhagan,Sanjana Joshi,Aliasghar Arab*

Main category: cs.RO

TL;DR: A prediction-driven safe planning framework for human-robot collaboration uses joint-level human motion forecasting, a capsule-based artificial potential field to measure collision risk, and an Adaptive RRT* planner, all validated via a physics-based digital twin. It achieves proactive avoidance with fast replanning.


<details>
  <summary>Details</summary>
Motivation: To enable proactive collision avoidance in long-horizon human-robot collaboration by moving beyond kinodynamic-only planning and reducing latency between prediction and plan execution, validated in a digital twin.

Method: 1) Use depth camera to obtain 3D skeletal poses. 2) Employ a CNN-BiLSTM model to forecast individual joint trajectories ahead of time. 3) Convert predictions into collision risk metrics with a capsule-based artificial potential field. 4) Trigger Adaptive RRT* when risk thresholds are exceeded. 5) Validate trajectories with a digital twin that simulates real-time human posture in front of a robot to assess motions and contacts. 6) Evaluate in 50 trials.

Result: The framework achieved 100% proactive avoidance with more than 250 mm clearance and sub-2 s replanning, outperforming existing kinodynamic-only planners through predictive modeling plus digital-twin validation.

Conclusion: Integrating granular, predictive human motion with digital twin validation enables proactive safe planning in HRC, bridging latency gaps and improving reliability over kinodynamic approaches.

Abstract: Human-robot collaboration requires precise prediction of human motion over
extended horizons to enable proactive collision avoidance. Unlike existing
planners that rely solely on kinodynamic models, we present a prediction-driven
safe planning framework that leverages granular, joint-by-joint human motion
forecasting validated in a physics-based digital twin. A capsule-based
artificial potential field (APF) converts these granular predictions into
collision risk metrics, triggering an Adaptive RRT* (A-RRT*) planner when
thresholds are exceeded. The depth camera is used to extract 3D skeletal poses
and a convolutional neural network-bidirectional long short-term memory
(CNN-BiLSTM) model to predict individual joint trajectories ahead of time. A
digital twin model integrates real-time human posture prediction placed in
front of a simulated robot to evaluate motions and physical contacts. The
proposed method enables validation of planned trajectories ahead of time and
bridging potential latency gaps in updating planned trajectories in real-time.
In 50 trials, our method achieved 100% proactive avoidance with > 250 mm
clearance and sub-2 s replanning, demonstrating superior precision and
reliability compared to existing kinematic-only planners through the
integration of predictive human modeling with digital twin validation.

</details>


### [464] [Distributed Connectivity Maintenance and Recovery for Quadrotor Motion Planning](https://arxiv.org/abs/2510.03504)
*Yutong Wang,Yichun Qu,Tengxiang Wang,Lishuo Pan,Nora Ayanian*

Main category: cs.RO

TL;DR: A real-time, distributed MPC-CLF-CBF framework that uses high-order control barrier functions to maintain and recover connectivity among multi-robot teams, with Bezier-parameterized trajectories for smooth planning/control; validated in simulation and a 4-robot hardware test.


<details>
  <summary>Details</summary>
Motivation: Maintaining inter-robot connectivity is essential for coordinated multi-robot operation but is highly vulnerable to obstacles, occlusions, and temporary disconnects; robust, real-time methods are needed to preserve and recover connectivity while ensuring safety.

Method: A unified trajectory-generation and control scheme that combines MPC with CLF-CBF safety constraints. Inter-robot proximity to maintain connectivity is enforced via high-order CBFs; connectivity recovery is enabled by control Lyapunov functions; trajectories are Bezier-parameterized to ensure smoothness and derivative guarantees; implemented in a distributed framework.

Result: Demonstrates real-time connectivity maintenance and collision avoidance in obstacle-rich environments, with the ability to recover from disconnected configurations. Validated through extensive simulations and a hardware experiment using four Crazyflie nano-quadrotors.

Conclusion: The paper presents a unified MPC-CLF-CBF approach for continuous-time trajectory generation and control that achieves connectivity maintenance and recovery in multi-robot systems, offering robustness and practical applicability, as evidenced by simulation and hardware experiments.

Abstract: Maintaining connectivity is crucial in many multi-robot applications, yet
fragile to obstacles and visual occlusions. We present a real-time distributed
framework for multi-robot navigation certified by high-order control barrier
functions (HOCBFs) that controls inter-robot proximity to maintain connectivity
while avoiding collisions. We incorporate control Lyapunov functions to enable
connectivity recovery from initial disconnected configurations and temporary
losses, providing robust connectivity during navigation in obstacle-rich
environments. Our trajectory generation framework concurrently produces
planning and control through a Bezier-parameterized trajectory, which naturally
provides smooth curves with arbitrary degree of derivatives. The main
contribution is the unified MPC-CLF-CBF framework, a continuous-time trajectory
generation and control method for connectivity maintenance and recovery of
multi-robot systems. We validate the framework through extensive simulations
and a physical experiment with 4 Crazyflie nano-quadrotors.

</details>


### [465] [LapSurgie: Humanoid Robots Performing Surgery via Teleoperated Handheld Laparoscopy](https://arxiv.org/abs/2510.03529)
*Zekai Liang,Xiao Liang,Soofiyan Atar,Sreyan Das,Zoe Chiu,Peihan Zhang,Florian Richter,Shanglei Liu,Michael C. Yip*

Main category: cs.RO

TL;DR: A humanoid-robot-based laparoscopic teleoperation framework (LapSurgie) enabling remote, instrument-tethered laparoscopic control using inverse-mapping under remote center-of-motion constraints, with a stereo-vision console and off-the-shelf tools; initial cross-platform user study supports feasibility for deploying humanoid robots in laparoscopic procedures.


<details>
  <summary>Details</summary>
Motivation: Addresses healthcare disparities by enabling laparoscopic surgery in rural/low-resource settings. Traditional surgical robotics are costly and infrastructure-intensive, limiting adoption to well-resourced centers. Humanoid robots offer deployability in environments designed for humans without major renovations.

Method: Inverse-mapping control for manual-wristed laparoscopic instruments enforcing remote center-of-motion constraints; teleoperation via a control console with stereo vision; uses off-the-shelf laparoscopic tools with no extra setup; evaluated via comprehensive cross-platform user study.

Result: User study across platforms demonstrates effectiveness and initial feasibility of deploying humanoid-robot-based laparoscopic teleoperation in these settings.

Conclusion: LapSurgie shows promise for extending access to robotic laparoscopy; deployments in standard OR environments appear feasible without extensive infrastructure changes, though clinical validation, safety, and scalability considerations require further work.

Abstract: Robotic laparoscopic surgery has gained increasing attention in recent years
for its potential to deliver more efficient and precise minimally invasive
procedures. However, adoption of surgical robotic platforms remains largely
confined to high-resource medical centers, exacerbating healthcare disparities
in rural and low-resource regions. To close this gap, a range of solutions has
been explored, from remote mentorship to fully remote telesurgery. Yet, the
practical deployment of surgical robotic systems to underserved communities
remains an unsolved challenge. Humanoid systems offer a promising path toward
deployability, as they can directly operate in environments designed for humans
without extensive infrastructure modifications -- including operating rooms. In
this work, we introduce LapSurgie, the first humanoid-robot-based laparoscopic
teleoperation framework. The system leverages an inverse-mapping strategy for
manual-wristed laparoscopic instruments that abides to remote center-of-motion
constraints, enabling precise hand-to-tool control of off-the-shelf surgical
laparoscopic tools without additional setup requirements. A control console
equipped with a stereo vision system provides real-time visual feedback.
Finally, a comprehensive user study across platforms demonstrates the
effectiveness of the proposed framework and provides initial evidence for the
feasibility of deploying humanoid robots in laparoscopic procedures.

</details>


### [466] [Efficient Surgical Robotic Instrument Pose Reconstruction in Real World Conditions Using Unified Feature Detection](https://arxiv.org/abs/2510.03532)
*Zekai Liang,Kazuya Miyata,Xiao Liang,Florian Richter,Michael C. Yip*

Main category: cs.RO

TL;DR: A unified single-pass framework detects keypoints and shaft edges for MIS camera-to-robot calibration, trained on synthetic data, enabling fast online pose estimation with state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Minimally invasive surgical robots have long kinematic chains and partial camera visibility, making camera-to-robot calibration challenging for traditional methods that assume stiff robots and full visibility; there is a need for fast, robust online calibration in surgical environments.

Method: A neural architecture with a shared encoding that jointly detects geometric primitives (keypoints and shaft edges) in one inference, combined with projection-geometry-based pose estimation, trained on large-scale synthetic data with projective labeling.

Result: The approach achieves fast performance and state-of-the-art accuracy in challenging surgical environments, with qualitative and quantitative evidence across feature detection and pose estimation.

Conclusion: Efficient online MIS camera-to-robot calibration is enabled by unifying primitive detection, improving robustness and speed over prior keypoint- or rendering-based methods.

Abstract: Accurate camera-to-robot calibration is essential for any vision-based
robotic control system and especially critical in minimally invasive surgical
robots, where instruments conduct precise micro-manipulations. However, MIS
robots have long kinematic chains and partial visibility of their degrees of
freedom in the camera, which introduces challenges for conventional
camera-to-robot calibration methods that assume stiff robots with good
visibility. Previous works have investigated both keypoint-based and
rendering-based approaches to address this challenge in real-world conditions;
however, they often struggle with consistent feature detection or have long
inference times, neither of which are ideal for online robot control. In this
work, we propose a novel framework that unifies the detection of geometric
primitives (keypoints and shaft edges) through a shared encoding, enabling
efficient pose estimation via projection geometry. This architecture detects
both keypoints and edges in a single inference and is trained on large-scale
synthetic data with projective labeling. This method is evaluated across both
feature detection and pose estimation, with qualitative and quantitative
results demonstrating fast performance and state-of-the-art accuracy in
challenging surgical environments.

</details>


### [467] [Shape-Space Graphs: Fast and Collision-Free Path Planning for Soft Robots](https://arxiv.org/abs/2510.03547)
*Carina Veil,Moritz Flaschel,Ellen Kuhl*

Main category: cs.RO

TL;DR: Graph-based path planning in shape space for a soft elephant-trunk-inspired arm using a precomputed shape library and k-NN graph; collision pruning with signed distance functions; multi-objective costs combining geometry and actuation energy; fast planning via Dijkstra with energy-aware trade-offs.


<details>
  <summary>Details</summary>
Motivation: Soft robots offer high flexibility but present challenging planning problems due to highly nonlinear, infinite-dimensional kinematics; need fast, reliable planning in cluttered environments for real-time applications (surgical, industrial, assistive).

Method: Biomechanical model inspired by morphoelasticity and active filament theory; precompute a shape library; construct a k-nearest-neighbor graph in shape space where nodes are physically valid shapes; use signed distance functions to prune collision-affected nodes/edges; define multi-objective edge costs combining geometric distance and actuation energy; perform planning with Dijkstra on the graph.

Result: The algorithm reliably avoids obstacles and produces feasible paths within milliseconds from precomputed graphs; including energy costs reduces actuation effort compared to geometry-only planning, though tip trajectories may be longer.

Conclusion: Shape-space graph search is a promising approach for fast and reliable path planning in soft robotics, enabling real-time applications in surgical, industrial, and assistive settings.

Abstract: Soft robots, inspired by elephant trunks or octopus arms, offer extraordinary
flexibility to bend, twist, and elongate in ways that rigid robots cannot.
However, their motion planning remains a challenge, especially in cluttered
environments with obstacles, due to their highly nonlinear and
infinite-dimensional kinematics. Here, we present a graph-based path planning
tool for an elephant-trunk-inspired soft robotic arm designed with three
artificial muscle fibers that allow for multimodal continuous deformation
through contraction. Using a biomechanical model inspired by morphoelasticity
and active filament theory, we precompute a shape library and construct a
$k$-nearest neighbor graph in \emph{shape space}, ensuring that each node
corresponds to a mechanically accurate and physically valid robot shape. For
the graph, we use signed distance functions to prune nodes and edges colliding
with obstacles, and define multi-objective edge costs based on geometric
distance and actuation effort, enabling energy-efficient planning with
collision avoidance. We demonstrate that our algorithm reliably avoids
obstacles and generates feasible paths within milliseconds from precomputed
graphs using Dijkstra's algorithm. We show that including energy costs can
drastically reduce the actuation effort compared to geometry-only planning, at
the expense of longer tip trajectories. Our results highlight the potential of
shape-space graph search for fast and reliable path planning in the field of
soft robotics, paving the way for real-time applications in surgical,
industrial, and assistive settings.

</details>


### [468] [Learning to Act Through Contact: A Unified View of Multi-Task Robot Learning](https://arxiv.org/abs/2510.03599)
*Shafeef Omar,Majid Khadiv*

Main category: cs.RO

TL;DR: A unified, contact-explicit framework trains a single goal-conditioned RL policy to perform multiple loco-manipulation tasks across different morphologies by following contact plans, improving generalization to unseen scenarios.


<details>
  <summary>Details</summary>
Motivation: Tackle task generalization and cross-domain transfer in contact-rich locomotion/manipulation by unifying task definitions under a single contact-based representation.

Method: Train a goal-conditioned RL policy that realizes given contact plans (desired contact positions, timings, active end-effectors) across diverse embodiments (quadruped, humanoid gait variants, and bimanual manipulation) with a single shared policy.

Result: Demonstrates that one policy can handle diverse tasks and morphologies when guided by explicit contact reasoning, with improved generalization to unseen scenarios.

Conclusion: Contact-explicit policy learning offers a scalable foundation for scalable loco-manipulation, enabling versatile behaviors across morphologically distinct systems.

Abstract: We present a unified framework for multi-task locomotion and manipulation
policy learning grounded in a contact-explicit representation. Instead of
designing different policies for different tasks, our approach unifies the
definition of a task through a sequence of contact goals-desired contact
positions, timings, and active end-effectors. This enables leveraging the
shared structure across diverse contact-rich tasks, leading to a single policy
that can perform a wide range of tasks. In particular, we train a
goal-conditioned reinforcement learning (RL) policy to realise given contact
plans. We validate our framework on multiple robotic embodiments and tasks: a
quadruped performing multiple gaits, a humanoid performing multiple biped and
quadrupedal gaits, and a humanoid executing different bimanual object
manipulation tasks. Each of these scenarios is controlled by a single policy
trained to execute different tasks grounded in contacts, demonstrating
versatile and robust behaviours across morphologically distinct systems. Our
results show that explicit contact reasoning significantly improves
generalisation to unseen scenarios, positioning contact-explicit policy
learning as a promising foundation for scalable loco-manipulation.

</details>


### [469] [Safety-Oriented Dynamic Path Planning for Automated Vehicles](https://arxiv.org/abs/2510.03640)
*Mostafa Emam,Matthias Gerdts*

Main category: cs.RO

TL;DR: A bi-level NMPC-based path-planning framework with obstacle-aware grid projections, homotopy-based constraint relaxation, and a parallel backup loop to ensure safe, real-time operation in dynamic driving scenarios.


<details>
  <summary>Details</summary>
Motivation: Address safety and reliability in autonomous driving by enabling robust real-time path planning and obstacle avoidance in dynamic environments, while improving solvability of the OCP and providing safe fallbacks.

Method: Main loop uses nonlinear model predictive control with time-dependent grid projections of obstacle movements; homotopy-based constraint relaxation to ease OCP solving; an independent backup loop runs in parallel to generate safe fallback trajectories if the main loop cannot compute a solution within a critical time.

Result: Demonstrates real-time applicability and robustness across driving scenarios, showing that the bi-level approach improves safety and performance in dynamic environments.

Conclusion: The proposed framework meaningfully advances safe and reliable autonomous driving by combining NMPC with homotopy relaxation and a parallel safety backup loop, enabling adaptive path planning under dynamic obstacles.

Abstract: Ensuring safety in autonomous vehicles necessitates advanced path planning
and obstacle avoidance capabilities, particularly in dynamic environments. This
paper introduces a bi-level control framework that efficiently augments road
boundaries by incorporating time-dependent grid projections of obstacle
movements, thus enabling precise and adaptive path planning. The main control
loop utilizes Nonlinear Model Predictive Control (NMPC) for real-time path
optimization, wherein homotopy-based constraint relaxation is employed to
improve the solvability of the optimal control problem (OCP). Furthermore, an
independent backup loop runs concurrently to provide safe fallback trajectories
when an optimal trajectory cannot be computed by the main loop within a
critical time frame, thus enhancing safety and real-time performance. Our
evaluation showcases the benefits of the proposed methods in various driving
scenarios, highlighting the real-time applicability and robustness of our
approach. Overall, the framework represents a significant step towards safer
and more reliable autonomous driving in complex and dynamic environments.

</details>


### [470] [Geometrically Exact Hard Magneto-Elastic Cosserat Shells: Static Formulation for Shape Morphing](https://arxiv.org/abs/2510.03644)
*Mohammadjavad Javadi,Robin Chhabra*

Main category: cs.RO

TL;DR: Coordinate-free SE(3)-Cosserat shell model for hard-magnetic soft shells with 6-DoF per material point; FE formulation that handles large rotations without locking; analytically and experimentally validated.


<details>
  <summary>Details</summary>
Motivation: Existing 1D Cosserat rod models are inadequate for wide soft shells (high width-to-length ratio) used in ferromagnetic soft robots; a 2D shell theory on SE(3) is needed for accurate analysis and control of shape morphing in grippers and walking robots.

Method: Formulate the shell as a 2D manifold of material points with six degrees of freedom (position and orientation) in SE(3). Define a novel local deformation gradient using the Lie-group structure of SE(3). Derive strong and weak forms of equilibrium via the principle of virtual work, and linearize the weak form for numerical implementation. Develop a finite element approach that mitigates singularities and locking, enabling efficient coordinate-free static analysis.

Result: Derived SE(3) Cosserat shell equations, a linearized weak form for FE, and a robust FE discretization. The approach is analytically and experimentally validated, showing superior performance under large rotations and displacements and avoiding common shell-formulation issues such as locking and singularities.

Conclusion: The SE(3)-based Cosserat shell framework provides a robust, coordinate-free tool for modeling hard-magnetic soft shells in grippers and walking robots, enabling accurate analysis and control of large-deformation behavior and serving as a versatile foundation for shape-morphing applications.

Abstract: Cosserat rod theory is the popular approach to modeling ferromagnetic soft
robots as 1-Dimensional (1D) slender structures in most applications, such as
biomedical. However, recent soft robots designed for locomotion and
manipulation often exhibit a large width-to-length ratio that categorizes them
as 2D shells. For analysis and shape-morphing control purposes, we develop an
efficient coordinate-free static model of hard-magnetic shells found in soft
magnetic grippers and walking soft robots. The approach is based on a novel
formulation of Cosserat shell theory on the Special Euclidean group
($\mathbf{SE}(3)$). The shell is assumed to be a 2D manifold of material points
with six degrees of freedom (position & rotation) suitable for capturing the
behavior of a uniformly distributed array of spheroidal hard magnetic particles
embedded in the rheological elastomer. The shell's configuration manifold is
the space of all smooth embeddings $\mathbb{R}^2\rightarrow\mathbf{SE}(3)$.
According to a novel definition of local deformation gradient based on the Lie
group structure of $\mathbf{SE}(3)$, we derive the strong and weak forms of
equilibrium equations, following the principle of virtual work. We extract the
linearized version of the weak form for numerical implementations. The
resulting finite element approach can avoid well-known challenges such as
singularity and locking phenomenon in modeling shell structures. The proposed
model is analytically and experimentally validated through a series of test
cases that demonstrate its superior efficacy, particularly when the shell
undergoes severe rotations and displacements.

</details>


### [471] [An Amphibious Untethered Inchworm Soft Robot for Fast Crawling Locomotion](https://arxiv.org/abs/2510.03660)
*Mohammadjavad Javadi,Charlie Wadds,Robin Chhabra*

Main category: cs.RO

TL;DR: Fully untethered, magnetically actuated soft robot with onboard wireless control and an integrated camera achieves walking (up to 3.74 cm/s) and swimming (0.82 cm/s) speeds, mass 102.63 g, performing walking, steering, swimming and payload transport.


<details>
  <summary>Details</summary>
Motivation: To enable real-world deployment of soft robots by eliminating tethered power/data connections, enabling autonomous multimodal locomotion and onboard sensing in diverse environments.

Method: Design of a curved flexible structure actuated by magnetic forces; integration of a compact onboard control circuit and camera; wireless command transmission; structural optimization and system-level integration; experimental characterization to validate untethered multimodal locomotion.

Result: Demonstrates walking, steering, swimming, and payload transport in a fully untethered soft robot; multimodal locomotion validated; measured speeds (walking up to 3.74 cm/s, swimming 0.82 cm/s); onboard sensing and wireless control functionality confirmed.

Conclusion: The work demonstrates the feasibility of compact, untethered soft robots with multimodal locomotion and onboard perception, highlighting potential for practical deployment and future autonomy enhancements.

Abstract: Untethered soft robots are essential for advancing the real-world deployment
of soft robotic systems in diverse and multitasking environments. Inspired by
soft-bodied inchworm, we present a fully untethered soft robot with a curved,
flexible structure actuated by magnetic forces. The robot has a total mass of
102.63 g and demonstrates multimodal locomotion, achieving a maximum walking
speed of 3.74 cm/s and a swimming speed of 0.82 cm/s. A compact and lightweight
onboard control circuit enables wireless command transmission, while an
integrated camera provides environmental perception. Through structural
optimization and system-level integration, the robot successfully performs
walking, steering, swimming, and payload transport without reliance on external
infrastructure. The robot's dynamic performance and locomotion capabilities are
systematically validated through experimental characterization.

</details>


### [472] [Robust Visual Embodiment: How Robots Discover Their Bodies in Real Environments](https://arxiv.org/abs/2510.03677)
*Salim Rezvani,Ammar Jaleel Mahmood,Robin Chhabra*

Main category: cs.RO

TL;DR: Quantifies how visual degradations affect robotic self-modeling and introduces a denoising framework with morphology-preserving constraints plus semantic segmentation to restore near-baseline performance under noisy conditions.


<details>
  <summary>Details</summary>
Motivation: Robotic self-modeling is fragile under realistic sensing conditions; there is a need to systematically study and improve robustness to noise and clutter.

Method: Systematic evaluation of blur, salt-and-pepper, and Gaussian noise on morphology prediction, trajectory planning, and damage recovery; proposes a task-aware denoising pipeline that preserves morphological cues; adds semantic segmentation to isolate robots in cluttered scenes; validates in simulation and real robots.

Result: The proposed approach recovers near-baseline performance on both simulated and physical platforms, while existing pipelines degrade significantly.

Conclusion: This work strengthens the robustness of visual self-modeling and enables deployment of self-aware robots in unpredictable real-world environments.

Abstract: Robots with internal visual self-models promise unprecedented adaptability,
yet existing autonomous modeling pipelines remain fragile under realistic
sensing conditions such as noisy imagery and cluttered backgrounds. This paper
presents the first systematic study quantifying how visual
degradations--including blur, salt-and-pepper noise, and Gaussian noise--affect
robotic self-modeling. Through both simulation and physical experiments, we
demonstrate their impact on morphology prediction, trajectory planning, and
damage recovery in state-of-the-art pipelines. To overcome these challenges, we
introduce a task-aware denoising framework that couples classical restoration
with morphology-preserving constraints, ensuring retention of structural cues
critical for self-modeling. In addition, we integrate semantic segmentation to
robustly isolate robots from cluttered and colorful scenes. Extensive
experiments show that our approach restores near-baseline performance across
simulated and physical platforms, while existing pipelines degrade
significantly. These contributions advance the robustness of visual
self-modeling and establish practical foundations for deploying self-aware
robots in unpredictable real-world environments.

</details>


### [473] [EmbodiSwap for Zero-Shot Robot Imitation Learning](https://arxiv.org/abs/2510.03706)
*Eadom Dessalene,Pavan Mantripragada,Michael Maynord,Yiannis Aloimonos*

Main category: cs.RO

TL;DR: EmbodiSwap creates photorealistic synthetic robot overlays on human video to enable zero-shot imitation learning; uses V-JEPA as backbone; achieves 82% success in real-world tests; releases code and datasets.


<details>
  <summary>Details</summary>
Motivation: Bridge the embodiment gap between in-the-wild ego-centric human videos and a target robot embodiment to enable zero-shot imitation learning.

Method: Generate photorealistic robot overlays from human videos using EmbodiSwap, produce a synthetic robot dataset over multiple datasets (EPIC-Kitchens, HOI4D, Ego4D), train a closed-loop manipulation policy on the synthetic data, and repurpose V-JEPA as the visual backbone for imitation learning on synthetic robot videos.

Result: Zero-shot policy achieves 82% success in real-world tests, outperforming a few-shot pi_0 and pi_0 trained with EmbodiSwap data; code and datasets released for reproducible research.

Conclusion: V-JEPA is effective as a visual backbone for robotic imitation using synthetic data; EmbodiSwap enables zero-shot transfer across embodiments; the authors provide open-source tooling and datasets to foster broader adoption.

Abstract: We introduce EmbodiSwap - a method for producing photorealistic synthetic
robot overlays over human video. We employ EmbodiSwap for zero-shot imitation
learning, bridging the embodiment gap between in-the-wild ego-centric human
video and a target robot embodiment. We train a closed-loop robot manipulation
policy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a
visual backbone, repurposing V-JEPA from the domain of video understanding to
imitation learning over synthetic robot videos. Adoption of V-JEPA outperforms
alternative vision backbones more conventionally used within robotics. In
real-world tests, our zero-shot trained V-JEPA model achieves an $82\%$ success
rate, outperforming a few-shot trained $\pi_0$ network as well as $\pi_0$
trained over data produced by EmbodiSwap. We release (i) code for generating
the synthetic robot overlays which takes as input human videos and an arbitrary
robot URDF and generates a robot dataset, (ii) the robot dataset we synthesize
over EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference
code, to facilitate reproducible research and broader adoption.

</details>


### [474] [Model-Based Adaptive Precision Control for Tabletop Planar Pushing Under Uncertain Dynamics](https://arxiv.org/abs/2510.03768)
*Aydin Ahmadi,Baris Akgun*

Main category: cs.RO

TL;DR: A single GRU-based dynamics model integrated with MPPI enables versatile, multi-task data-driven planar pushing without retraining, demonstrated with sim-to-real transfer and obstacle avoidance across tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome the narrow capabilities of prior data-driven pushing methods and enable a single learned model to handle multiple pushing tasks without retraining, reducing engineering effort and improving generalization.

Method: A recurrent GRU-based dynamics model with nonlinear layers and a tailored state-action representation captures object-environment dynamics. It is integrated with a sampling-based Model Predictive Path Integral (MPPI) controller for adaptive, task-focused actions. Training uses domain randomization in simulation; evaluation includes ablation studies and real-robot experiments with a Franka Panda. The system supports side switching, variable-length pushes, precise positioning, trajectory tracking, and obstacle avoidance; tasks are chosen by changing the controller's objective rather than retraining; extension to longer push lengths and a balanced controller to reduce horizon steps is discussed.

Result: Ablation studies show improved prediction and stable rollouts. In simulation and real-world tests, the method achieves high success rates for precise positioning under strict thresholds and strong performance in trajectory following and obstacle avoidance. Multiple tasks are solved by altering the controller objective without retraining.

Conclusion: The framework presents a generalizable, multi-task data-driven pushing approach that relies on a single learned dynamics model and a flexible MPPI controller, validated in sim-to-real experiments; future work includes extending to more object types and longer-horizon pushes to broaden applicability.

Abstract: Data-driven planar pushing methods have recently gained attention as they
reduce manual engineering effort and improve generalization compared to
analytical approaches. However, most prior work targets narrow capabilities
(e.g., side switching, precision, or single-task training), limiting broader
applicability. We present a model-based framework for non-prehensile tabletop
pushing that uses a single learned model to address multiple tasks without
retraining. Our approach employs a recurrent GRU-based architecture with
additional non-linear layers to capture object-environment dynamics while
ensuring stability. A tailored state-action representation enables the model to
generalize across uncertain dynamics, variable push lengths, and diverse tasks.
For control, we integrate the learned dynamics with a sampling-based Model
Predictive Path Integral (MPPI) controller, which generates adaptive,
task-oriented actions. This framework supports side switching, variable-length
pushes, and objectives such as precise positioning, trajectory following, and
obstacle avoidance. Training is performed in simulation with domain
randomization to support sim-to-real transfer. We first evaluate the
architecture through ablation studies, showing improved prediction accuracy and
stable rollouts. We then validate the full system in simulation and real-world
experiments using a Franka Panda robot with markerless tracking. Results
demonstrate high success rates in precise positioning under strict thresholds
and strong performance in trajectory tracking and obstacle avoidance. Moreover,
multiple tasks are solved simply by changing the controller's objective
function, without retraining. While our current focus is on a single object
type, we extend the framework by training on wider push lengths and designing a
balanced controller that reduces the number of steps for longer-horizon goals.

</details>


### [475] [Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets](https://arxiv.org/abs/2510.03776)
*Tiago Rodrigues de Almeida,Yufei Zhu,Andrey Rudenko,Tomasz P. Kucner,Johannes A. Stork,Martin Magnusson,Achim J. Lilienthal*

Main category: cs.RO

TL;DR: Class-conditioned trajectory prediction improves accuracy for heterogeneous agents; pattern-based methods can outperform deep learning under limited data or imbalanced conditions.


<details>
  <summary>Details</summary>
Motivation: Robots must predict future actions/intentions of surrounding agents in dynamic environments; conditioning on task/role/labels reduces forecast uncertainty.

Method: Propose conditional pattern-based baselines and efficient deep-learning baselines; evaluate on robotics and outdoors datasets THOR-MAGNI and Stanford Drone Dataset; analyze effects of data balance and new environments.

Result: All methods improve accuracy when using class labels; deep learning performs better on balanced data, while pattern-based methods are preferable in limited data or imbalanced settings.

Conclusion: Class-conditioned trajectory prediction is beneficial; method choice should consider data regime, with pattern-based approaches favored for cold-start or highly imbalanced scenarios.

Abstract: Robots and other intelligent systems navigating in complex dynamic
environments should predict future actions and intentions of surrounding agents
to reach their goals efficiently and avoid collisions. The dynamics of those
agents strongly depends on their tasks, roles, or observable labels.
Class-conditioned motion prediction is thus an appealing way to reduce forecast
uncertainty and get more accurate predictions for heterogeneous agents.
However, this is hardly explored in the prior art, especially for mobile robots
and in limited data applications. In this paper, we analyse different
class-conditioned trajectory prediction methods on two datasets. We propose a
set of conditional pattern-based and efficient deep learning-based baselines,
and evaluate their performance on robotics and outdoors datasets (TH\"OR-MAGNI
and Stanford Drone Dataset). Our experiments show that all methods improve
accuracy in most of the settings when considering class labels. More
importantly, we observe that there are significant differences when learning
from imbalanced datasets, or in new environments where sufficient data is not
available. In particular, we find that deep learning methods perform better on
balanced datasets, but in applications with limited data, e.g., cold start of a
robot in a new environment, or imbalanced classes, pattern-based methods may be
preferable.

</details>


### [476] [COVER:COverage-VErified Roadmaps for Fixed-time Motion Planning in Continuous Semi-Static Environments](https://arxiv.org/abs/2510.03875)
*Niranjan Kumar Ilampooranan,Constantinos Chamzas*

Main category: cs.RO

TL;DR: COVER is a framework that builds a coverage-verified roadmap for semi-static environments, partitioning obstacle configurations to guarantee fixed-time motion planning within verified regions and showing improved coverage and success rates on a 7-DOF Panda robot.


<details>
  <summary>Details</summary>
Motivation: In semi-static settings, most obstacles are static but a subset varies per query. There is a need for formal guarantees and fixed-time responses, which prior methods struggle to provide due to lack of guarantees or reliance on coarse discretizations.

Method: Incrementally construct a coverage-verified roadmap by partitioning the obstacle configuration space and solving paths within each partition; verify feasibility for each partition to guarantee fixed-time planning within verified regions.

Result: Empirical validation on a 7-DOF Panda robot performing table and shelf tasks shows COVER achieves broader coverage and higher query success rates than prior approaches.

Conclusion: COVER provides a principled framework for verified, fixed-time motion planning in semi-static environments, improving coverage and success guarantees over prior work.

Abstract: Having the ability to answer motion-planning queries within a fixed time
budget is critical for the widespread deployment of robotic systems.
Semi-static environments, where most obstacles remain static but a limited set
can vary across queries, exhibit structured variability that can be
systematically exploited to provide stronger guarantees than in general
motion-planning problems. However, prior approaches in this setting either lack
formal guarantees or rely on restrictive discretizations of obstacle
configurations, limiting their applicability in realistic domains. This paper
introduces COVER, a novel framework that incrementally constructs a
coverage-verified roadmap in semi-static environments. By partitioning the
obstacle configuration space and solving for feasible paths within each
partition, COVER systematically verifies feasibility of the roadmap in each
partition and guarantees fixed-time motion planning queries within the verified
regions. We validate COVER with a 7-DOF simulated Panda robot performing table
and shelf tasks, demonstrating that COVER achieves broader coverage with higher
query success rates than prior works.

</details>


### [477] [Seeing the Bigger Picture: 3D Latent Mapping for Mobile Manipulation Policy Learning](https://arxiv.org/abs/2510.03885)
*Sunghwan Kim,Woojeh Chung,Zhirui Dai,Dwait Bhatt,Arth Shukla,Hao Su,Yulun Tian,Nikolay Atanasov*

Main category: cs.RO

TL;DR: 3D latent-map-based SBP policy enhances mobile manipulation by global scene reasoning and long-horizon memory, outperforming image-based policies, with up to 25% higher success in sequential tasks.


<details>
  <summary>Details</summary>
Motivation: Image-based policies suffer from limited field of view and short-horizon memory; a persistent 3D latent map can provide global context and memory across views and time.

Method: SBP builds a 3D grid of scene-specific latent features fused from multiview observations; a pre-trained, scene-agnostic decoder reconstructs target embeddings; online optimization of map features during task execution; a policy trained via behavior cloning or reinforcement learning treats the latent map as state and leverages a 3D feature aggregator for global context.

Result: SBP enables global reasoning and long-horizon memory, outperforming image-based policies in both in-distribution and novel scenes; sequential manipulation task shows about 25% higher success rate.

Conclusion: End-to-end latent 3D maps improve closed-loop manipulation with better global context and generalization; the framework can extend to more tasks and enhance robustness and memory usage.

Abstract: In this paper, we demonstrate that mobile manipulation policies utilizing a
3D latent map achieve stronger spatial and temporal reasoning than policies
relying solely on images. We introduce Seeing the Bigger Picture (SBP), an
end-to-end policy learning approach that operates directly on a 3D map of
latent features. In SBP, the map extends perception beyond the robot's current
field of view and aggregates observations over long horizons. Our mapping
approach incrementally fuses multiview observations into a grid of
scene-specific latent features. A pre-trained, scene-agnostic decoder
reconstructs target embeddings from these features and enables online
optimization of the map features during task execution. A policy, trainable
with behavior cloning or reinforcement learning, treats the latent map as a
state variable and uses global context from the map obtained via a 3D feature
aggregator. We evaluate SBP on scene-level mobile manipulation and sequential
tabletop manipulation tasks. Our experiments demonstrate that SBP (i) reasons
globally over the scene, (ii) leverages the map as long-horizon memory, and
(iii) outperforms image-based policies in both in-distribution and novel
scenes, e.g., improving the success rate by 25% for the sequential manipulation
task.

</details>


### [478] [NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation](https://arxiv.org/abs/2510.03895)
*Zheng Huang,Mingyu Liu,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Xiaoman Li,Yiduo Jia,Hao Zhong,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: NoTVLA reduces catastrophic forgetting in Vision-Language-Action models by training on sparse, end-effector trajectories rather than dense actions. It uses temporal compression and spatial pruning to plan the end-effector path, yielding near single-task expert accuracy with much lower compute, no wrist camera, and robust zero-shot/generalization across tasks and platforms.


<details>
  <summary>Details</summary>
Motivation: Embodied VLA models suffer catastrophic forgetting when fine-tuned with dense action sequences, hindering real-world deployment and cross-task transfer. There is a need for memory-efficient, generalizable approaches that maintain language capabilities and operate across multiple robots without extra perception hardware.

Method: NoTVLA narrows trajectory focus to sparse end-effector trajectories using temporal compression and spatial reasoning pruning. Training uses these sparse trajectories rather than dense ones. Practically, it reduces compute, removes the need for wrist-mounted cameras, and targets zero-shot generalization across tasks and platforms. Evaluation is conducted in multi-task settings against a baseline (pi0).

Result: NoTVLA achieves superior performance and generalization compared to pi0 under substantially lower compute (over an order of magnitude) and without wrist-mounted cameras. Its accuracy closely matches single-task expert models, and it preserves language capabilities enabling zero-shot generalization in certain scenarios, enabling unified deployment across robot platforms and generalization from novel perspectives.

Conclusion: Focusing on sparse end-effector trajectories with temporal compression and pruning mitigates catastrophic forgetting in VLA systems, delivering practical, cross-task, cross-platform generalization with lower resource use while maintaining language-driven zero-shot capabilities.

Abstract: Vision-Language-Action (VLA) models represent a pivotal advance in embodied
intelligence, yet they confront critical barriers to real-world deployment,
most notably catastrophic forgetting. This issue stems from their overreliance
on continuous action sequences or action chunks, which inadvertently create
isolated data silos that disrupt knowledge retention across tasks. To tackle
these challenges, we propose the Narrowing of Trajectory VLA (NoTVLA)
framework: a novel approach that narrows its focus to sparse trajectories,
thereby avoiding the catastrophic forgetting associated with dense trajectory
fine-tuning. A key innovation of NoTVLA lies in its trajectory planning
strategy: instead of centering on the target object's trajectory, it leverages
temporal compression and spatial reasoning pruning specifically for the robot
end effector's trajectory. Furthermore, training is conducted using these
sparse trajectories rather than dense action trajectories, an optimization that
delivers remarkable practical advantages with better performance in zero-shot.
In multi-task evaluation scenarios, NoTVLA achieves superior performance and
generalization compared to pi0 while operating under two critical constraints:
it uses over an order of magnitude less computing power than pi0 and requires
no wrist-mounted camera. This design ensures that NoTVLA's operational accuracy
closely approximates that of single-task expert models. Crucially, it also
preserves the model's inherent language capabilities, enabling zero-shot
generalization in specific scenarios, supporting unified model deployment
across multiple robot platforms, and fostering a degree of generalization even
when perceiving tasks from novel perspectives.

</details>


### [479] [WAFFLE: A Wearable Approach to Bite Timing Estimation in Robot-Assisted Feeding](https://arxiv.org/abs/2510.03910)
*Akhil Padmanabha,Jessie Yuan,Tanisha Mehta,Rajat Kumar Jenamani,Eric Hu,Victoria de León,Anthony Wertz,Janavi Gupta,Ben Dodson,Yunting Yan,Carmel Majidi,Tapomayukh Bhattacharjee,Zackory Erickson*

Main category: cs.RO

TL;DR: WAFFLE is a wearable, learned bite-timing system for robotic feeding that uses sensor data to react to natural user cues, enabling reactive bite timing across users and hardware; it matches or surpasses baseline methods in user experience and is preferred for both individual and social dining.


<details>
  <summary>Details</summary>
Motivation: Millions need feeding assistance; current robotic feeding systems struggle to estimate bite timing, limiting autonomy and increasing caregiver workload. WAFFLE aims to enable natural, reactive bite timing to improve independence and reduce workload.

Method: Train a supervised regression model on bite-timing data from 14 participants using wearable sensors; introduce a user-adjustable assertiveness threshold to translate predictions into proceed/stop commands. Evaluate with 15 participants (Ocni Obi robot) across individual and social dining; demonstrate generalizability with 2 participants with motor impairments in home environments using a Kinova 7DOF robot.

Result: WAFFLE performs statistically on par with or better than baseline methods across measures of perceived control, robot understanding, and workload; participants prefer WAFFLE for both individual and social dining. The approach generalizes across users, robot hardware, positioning, feeding trajectories, foods, and dining contexts.

Conclusion: WAFFLE effectively enables natural, reactive bite timing that generalizes across users and robots, suggesting potential to enhance autonomy in feeding and reduce caregiver burden.

Abstract: Millions of people around the world need assistance with feeding. Robotic
feeding systems offer the potential to enhance autonomy and quality of life for
individuals with impairments and reduce caregiver workload. However, their
widespread adoption has been limited by technical challenges such as estimating
bite timing, the appropriate moment for the robot to transfer food to a user's
mouth. In this work, we introduce WAFFLE: Wearable Approach For Feeding with
LEarned bite timing, a system that accurately predicts bite timing by
leveraging wearable sensor data to be highly reactive to natural user cues such
as head movements, chewing, and talking. We train a supervised regression model
on bite timing data from 14 participants and incorporate a user-adjustable
assertiveness threshold to convert predictions into proceed or stop commands.
In a study with 15 participants without motor impairments with the Obi feeding
robot, WAFFLE performs statistically on par with or better than baseline
methods across measures of feeling of control, robot understanding, and
workload, and is preferred by the majority of participants for both individual
and social dining. We further demonstrate WAFFLE's generalizability in a study
with 2 participants with motor impairments in their home environments using a
Kinova 7DOF robot. Our findings support WAFFLE's effectiveness in enabling
natural, reactive bite timing that generalizes across users, robot hardware,
robot positioning, feeding trajectories, foods, and both individual and social
dining contexts.

</details>


### [480] [TCB-VIO: Tightly-Coupled Focal-Plane Binary-Enhanced Visual Inertial Odometry](https://arxiv.org/abs/2510.03919)
*Matthew Lisondra,Junseo Kim,Glenn Takashi Shimoda,Kourosh Zareinia,Sajad Saeedi*

Main category: cs.RO

TL;DR: FPSP-enabled VIO (TCB-VIO) runs a tightly-coupled 6-DoF VIO at 250 FPS using 400 Hz IMU, outperforming ROVIO, VINS-Mono, and ORB-SLAM3.


<details>
  <summary>Details</summary>
Motivation: To overcome vision-to-processing data bottlenecks and drift in VIO by moving vision computation onto focal-plane sensor-processor arrays (FPSP), enabling high frame-rate, low-latency VIO that better tracks high-rate inertial data.

Method: A tightly-coupled six-DoF VIO based on MSCKF, implemented on FPSP hardware to operate at 250 FPS, with IMU measurements at 400 Hz.

Result: The proposed TCB-VIO outperforms state-of-the-art VIO systems such as ROVIO, VINS-Mono, and ORB-SLAM3.

Conclusion: On-sensor FPSP-based tight coupling with MSCKF enables high-frame-rate VIO with reduced latency and improved robustness, showcasing the viability of vision processing directly on sensor arrays.

Abstract: Vision algorithms can be executed directly on the image sensor when
implemented on the next-generation sensors known as focal-plane
sensor-processor arrays (FPSP)s, where every pixel has a processor. FPSPs
greatly improve latency, reducing the problems associated with the bottleneck
of data transfer from a vision sensor to a processor. FPSPs accelerate
vision-based algorithms such as visual-inertial odometry (VIO). However, VIO
frameworks suffer from spatial drift due to the vision-based pose estimation,
whilst temporal drift arises from the inertial measurements. FPSPs circumvent
the spatial drift by operating at a high frame rate to match the high-frequency
output of the inertial measurements. In this paper, we present TCB-VIO, a
tightly-coupled 6 degrees-of-freedom VIO by a Multi-State Constraint Kalman
Filter (MSCKF), operating at a high frame-rate of 250 FPS and from IMU
measurements obtained at 400 Hz. TCB-VIO outperforms state-of-the-art methods:
ROVIO, VINS-Mono, and ORB-SLAM3.

</details>


### [481] [A Real-Time Framework for Intermediate Map Construction and Kinematically Feasible Off-Road Planning Without OSM](https://arxiv.org/abs/2510.03948)
*Otobong Jerome,Geesara Prathap Kulathunga,Devitt Dmitry,Eugene Murawjow,Alexandr Klimchik*

Main category: cs.RO

TL;DR: Proposes a fast, memory-efficient global path-planning framework for off-road navigation that decomposes the problem into graph-based planning, kinematic feasibility checks, and path smoothing, using a pixel-coordinate intermediate map of terrain features; achieves real-time planning (~1.5 s) on maps up to several square kilometers with ~1.5 GB memory.


<details>
  <summary>Details</summary>
Motivation: Off-road autonomous navigation faces unstructured terrain and large-scale maps; traditional global planners optimize path length but neglect real-time performance, kinematic feasibility, and memory efficiency, limiting applicability in tasks like search and rescue or farming.

Method: Create an intermediate pixel-coordinate map incorporating off-road trails, waterways, restricted/passable areas, and trees. Split planning into three subproblems: graph-based path planning, kinematic feasibility checking, and path smoothing, enabling real-time performance and efficient memory use.

Result: Demonstrated on large-scale off-road environments up to several square kilometers; average feasible path found in 1.5 seconds and ~1.5 GB memory under extreme conditions.

Conclusion: The framework is versatile and applicable to diverse off-road navigation tasks, including search and rescue and agricultural operations, delivering real-time, feasible, and memory-efficient global planning.

Abstract: Off-road environments present unique challenges for autonomous navigation due
to their complex and unstructured nature. Traditional global path-planning
methods, which typically aim to minimize path length and travel time, perform
poorly on large-scale maps and fail to account for critical factors such as
real-time performance, kinematic feasibility, and memory efficiency. This paper
introduces a novel global path-planning method specifically designed for
off-road environments, addressing these essential factors. The method begins by
constructing an intermediate map within the pixel coordinate system,
incorporating geographical features like off-road trails, waterways, restricted
and passable areas, and trees. The planning problem is then divided into three
sub-problems: graph-based path planning, kinematic feasibility checking, and
path smoothing. This approach effectively meets real-time performance
requirements while ensuring kinematic feasibility and efficient memory use. The
method was tested in various off-road environments with large-scale maps up to
several square kilometers in size, successfully identifying feasible paths in
an average of 1.5 seconds and utilizing approximately 1.5GB of memory under
extreme conditions. The proposed framework is versatile and applicable to a
wide range of off-road autonomous navigation tasks, including search and rescue
missions and agricultural operations.

</details>


### [482] [SITCOM: Scaling Inference-Time COMpute for VLAs](https://arxiv.org/abs/2510.04041)
*Ayudh Saxena,Harsh Shah,Sandeep Routray,Rishi Rajesh Shah,Esha Pahwa*

Main category: cs.RO

TL;DR: SITCOM augments pretrained Vision-Language-Action (VLA) models with model-based rollouts via a learned dynamics model to perform long-horizon planning at inference time, yielding more robust execution.


<details>
  <summary>Details</summary>
Motivation: Robust robotic control is hampered by costly data collection, poor generalization to unseen environments, and difficulties with long-horizon planning. VLA models excel at translating language to single-step actions but lack lookahead and suffer from compounding errors in dynamic tasks.

Method: Train a transformer-based dynamics model on BridgeV2 data and fine-tune on SIMPLER to bridge Real2Sim gaps. At inference, generate and roll out candidate action sequences with the dynamics model, score them using simulator rewards, and select the best plan (MPC-like with reward-based trajectory selection) to execute the instruction.

Result: Across SIMPLER tasks, combining SITCOM with a well-chosen reward function improved task completion rate from 48% to 72% using the trained dynamics model, demonstrating improved robustness to long-horizon execution.

Conclusion: SITCOM successfully converts one-shot VLA models into robust long-horizon planners by leveraging learned dynamics and inference-time planning, bridging Real2Sim gaps and enhancing performance with appropriate rewards.

Abstract: Learning robust robotic control policies remains a major challenge due to the
high cost of collecting labeled data, limited generalization to unseen
environments, and difficulties in planning over long horizons. While
Vision-Language-Action (VLA) models offer a promising solution by grounding
natural language instructions into single-step control commands, they often
lack mechanisms for lookahead and struggle with compounding errors in dynamic
tasks. In this project, we introduce Scaling Inference-Time COMpute for VLAs
(SITCOM), a framework that augments any pretrained VLA with model-based
rollouts and reward-based trajectory selection, inspired by Model Predictive
Control algorithm. SITCOM leverages a learned dynamics model to simulate
multi-step action rollouts to select the best candidate plan for real-world
execution, transforming one-shot VLAs into robust long-horizon planners. We
develop an efficient transformer-based dynamics model trained on large-scale
BridgeV2 data and fine-tuned on SIMPLER environments to bridge the Real2Sim
gap, and score candidate rollouts using rewards from simulator. Through
comprehensive evaluation across multiple tasks and settings in the SIMPLER
environment, we demonstrate that SITCOM when combined with a good reward
function can significantly improve task completion rate from 48% to 72% using
trained dynamics model.

</details>


### [483] [Feedback Matters: Augmenting Autonomous Dissection with Visual and Topological Feedback](https://arxiv.org/abs/2510.04074)
*Chung-Pang Wang,Changwei Chen,Xiao Liang,Soofiyan Atar,Florian Richter,Michael Yip*

Main category: cs.RO

TL;DR: A feedback-enabled, topology-aware framework for autonomous tissue dissection using endoscopic change detection and visibility-driven control to boost autonomy and robustness.


<details>
  <summary>Details</summary>
Motivation: Autonomous surgical systems must operate in highly dynamic environments where tissue properties and visual cues evolve; existing feedback methods struggle with the topological and perceptual challenges of tissue dissection.

Method: Post-action endoscopic analysis to reason about topological changes after each dissection action; introduce tissue visibility metrics; design optimal controllers that actively manipulate tissue to maximize visibility; integrate feedback with planning-based and learning-based dissection approaches.

Result: Experimentally demonstrates improved autonomy, reduced errors, and enhanced robustness in complex surgical scenarios.

Conclusion: Feedback-enabled, topology-aware, visibility-guided feedback mechanisms substantially improve autonomous tissue dissection performance and can complement both planning- and learning-based methods.

Abstract: Autonomous surgical systems must adapt to highly dynamic environments where
tissue properties and visual cues evolve rapidly. Central to such adaptability
is feedback: the ability to sense, interpret, and respond to changes during
execution. While feedback mechanisms have been explored in surgical robotics,
ranging from tool and tissue tracking to error detection, existing methods
remain limited in handling the topological and perceptual challenges of tissue
dissection. In this work, we propose a feedback-enabled framework for
autonomous tissue dissection that explicitly reasons about topological changes
from endoscopic images after each dissection action. This structured feedback
guides subsequent actions, enabling the system to localize dissection progress
and adapt policies online. To improve the reliability of such feedback, we
introduce visibility metrics that quantify tissue exposure and formulate
optimal controller designs that actively manipulate tissue to maximize
visibility. Finally, we integrate these feedback mechanisms with both
planning-based and learning-based dissection methods, and demonstrate
experimentally that they significantly enhance autonomy, reduce errors, and
improve robustness in complex surgical scenarios.

</details>


### [484] [From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents](https://arxiv.org/abs/2510.04076)
*Amin Vahidi-Moghaddam,Sayed Pedram Haeri Boroujeni,Iman Jebellat,Ehsan Jebellat,Niloufar Mehrabi,Zhaojian Li*

Main category: cs.RO

TL;DR: A survey of eight techniques to reduce computational demands of data-driven optimal control, showing practical gains on real-world robots/vehicles.


<details>
  <summary>Details</summary>
Motivation: Data-driven control methods (ML-based MPC, RL, DeePC, LLM-based formulations) promise performance and safety but suffer slow responses, high compute, and big memory needs; there is a need for practical, real-time implementations.

Method: Systematically reviews eight approaches for reducing complexity (e.g., reduced-order modeling, function-approximated policy learning, convex relaxations) and validates them across real-world platforms (robotic arms, soft robots, vehicle control).

Result: Demonstrates that the eight approaches can effectively lower computational burden while preserving or enabling safe, high-performance operation in diverse real-world applications.

Conclusion: With these complexity-reduction strategies, data-driven policies become more practical for real-time control on systems with fast dynamics and limited onboard resources.

Abstract: One of the main challenges in modern control applications, particularly in
robot and vehicle motion control, is achieving accurate, fast, and safe
movement. To address this, optimal control policies have been developed to
enforce safety while ensuring high performance. Since basic first-principles
models of real systems are often available, model-based controllers are widely
used. Model predictive control (MPC) is a leading approach that optimizes
performance while explicitly handling safety constraints. However, obtaining
accurate models for complex systems is difficult, which motivates data-driven
alternatives. ML-based MPC leverages learned models to reduce reliance on
hand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal
policies directly from interaction data. Data-enabled predictive control
(DeePC) goes further by bypassing modeling altogether, directly learning safe
policies from raw input-output data. Recently, large language model (LLM)
agents have also emerged, translating natural language instructions into
structured formulations of optimal control problems. Despite these advances,
data-driven policies face significant limitations. They often suffer from slow
response times, high computational demands, and large memory needs, making them
less practical for real-world systems with fast dynamics, limited onboard
computing, or strict memory constraints. To address this, various technique,
such as reduced-order modeling, function-approximated policy learning, and
convex relaxations, have been proposed to reduce computational complexity. In
this paper, we present eight such approaches and demonstrate their
effectiveness across real-world applications, including robotic arms, soft
robots, and vehicle motion control.

</details>


### [485] [HEHA: Hierarchical Planning for Heterogeneous Multi-Robot Exploration of Unknown Environments](https://arxiv.org/abs/2510.04161)
*Longrui Yang,Yiyu Wang,Jingfan Tang,Yunpeng Lv,Shizhe Zhao,Chao Cao,Zhongqiang Ren*

Main category: cs.RO

TL;DR: HEHA proposes a Hierarchical Exploration framework for multi-robot exploration with heterogeneous agents, introducing PEAF (Partial Anytime Focal search) for global planning to minimize the maximum path length under traversability constraints, plus a heterogeneity-aware local planner. It claims up to 30% reduction in exploration time compared to baselines.


<details>
  <summary>Details</summary>
Motivation: Efficient exploration in unknown environments with multiple heterogeneous robots requires fast, scalable planning under traversability constraints and coordination to avoid redundant exploration.

Method: A hierarchical planning approach where global planning uses the PEAF routing algorithm to quickly yield bounded sub-optimal routes that minimize makespan under constraints, while the local planner accounts for robot heterogeneity and prevents duplicate exploration.

Result: Experiments show HEHA reduces exploration time by up to 30% relative to baselines.

Conclusion: HEHA provides an effective, scalable global-local planning framework for heterogeneous multi-robot exploration with real-time replanning capabilities and potential applicability to diverse robotic teams.

Abstract: This paper considers the path planning problem for autonomous exploration of
an unknown environment using multiple heterogeneous robots such as drones,
wheeled, and legged robots, which have different capabilities to traverse
complex terrains. A key challenge there is to intelligently allocate the robots
to the unknown areas to be explored and determine the visiting order of those
spaces subject to traversablity constraints, which leads to a large scale
constrained optimization problem that needs to be quickly and iteratively
solved every time when new space are explored. To address the challenge, we
propose HEHA (Hierarchical Exploration with Heterogeneous Agents) by leveraging
a recent hierarchical method that decompose the exploration into global
planning and local planning. The major contribution in HEHA is its global
planning, where we propose a new routing algorithm PEAF (Partial Anytime Focal
search) that can quickly find bounded sub-optimal solutions to minimize the
maximum path length among the agents subject to traversability constraints.
Additionally, the local planner in HEHA also considers heterogeneity to avoid
repeated and duplicated exploration among the robots. The experimental results
show that, our HEHA can reduce up to 30% of the exploration time than the
baselines.

</details>


### [486] [Learning to Capture Rocks using an Excavator: A Reinforcement Learning Approach with Guiding Reward Formulation](https://arxiv.org/abs/2510.04168)
*Amirmasoud Molaei,Reza Ghabcheloo*

Main category: cs.RO

TL;DR: A model-free RL approach using PPO in simulation learns rock capturing with a CAT365 excavator, generalizes to unseen rocks and soils without explicit material models; first RL-based controller for rock capture; results rival human performance and maintain stability.


<details>
  <summary>Details</summary>
Motivation: Rock capture with standard excavator buckets is difficult due to complex rock–soil interactions; existing autonomous methods rely on continuous media or specialized grippers; there is a need for a generalizable, data-driven controller for discrete object manipulation in unstructured environments.

Method: Train a PPO-based, model-free RL agent in AGX Dynamics; policy outputs joint velocities for boom/arm/bucket; domain randomization over rock geometry, density, and mass, plus initial configurations of bucket, rock, and goal; reward shaping guides behavior; no explicit rock/soil material models.

Result: Policy generalizes to unseen rocks and varying soil conditions, achieving high success rates similar to humans while maintaining machine stability.

Conclusion: Demonstrates feasibility of learning-based excavation strategies for discrete object manipulation without hardware or material models; first RL-based controller for rock capturing; supports data-driven autonomous rock manipulation in construction.

Abstract: Rock capturing with standard excavator buckets is a challenging task
typically requiring the expertise of skilled operators. Unlike soil digging, it
involves manipulating large, irregular rocks in unstructured environments where
complex contact interactions with granular material make model-based control
impractical. Existing autonomous excavation methods focus mainly on continuous
media or rely on specialized grippers, limiting their applicability to
real-world construction sites. This paper introduces a fully data-driven
control framework for rock capturing that eliminates the need for explicit
modeling of rock or soil properties. A model-free reinforcement learning agent
is trained in the AGX Dynamics simulator using the Proximal Policy Optimization
(PPO) algorithm and a guiding reward formulation. The learned policy outputs
joint velocity commands directly to the boom, arm, and bucket of a CAT365
excavator model. Robustness is enhanced through extensive domain randomization
of rock geometry, density, and mass, as well as the initial configurations of
the bucket, rock, and goal position. To the best of our knowledge, this is the
first study to develop and evaluate an RL-based controller for the rock
capturing task. Experimental results show that the policy generalizes well to
unseen rocks and varying soil conditions, achieving high success rates
comparable to those of human participants while maintaining machine stability.
These findings demonstrate the feasibility of learning-based excavation
strategies for discrete object manipulation without requiring specialized
hardware or detailed material models.

</details>


### [487] [VBM-NET: Visual Base Pose Learning for Mobile Manipulation using Equivariant TransporterNet and GNNs](https://arxiv.org/abs/2510.04171)
*Lakshadeep Naik,Adam Fischer,Daniel Duberg,Danica Kragic*

Main category: cs.RO

TL;DR: A learning-based framework (VBM-NET) for selecting mobile base poses for grasping from top-down projections, achieving comparable performance to classical methods with significantly faster computation and enabling sim-to-real transfer.


<details>
  <summary>Details</summary>
Motivation: Base pose planning often relies on precise object poses and accurate environment models. This work proposes to bypass those requirements by using top-down orthographic projections that provide a global, structure-preserving view of the scene, and leverages spatial symmetries.

Method: VBM-NET uses equivariant TransporterNet to generate candidate base poses from top-down projections, represents a varying number of candidates with a graph neural network, and employs reinforcement learning to select the optimal base pose for grasping.

Result: The method achieves comparable solution quality to classical planning while reducing computation time. It also demonstrates sim-to-real transfer by training in simulation and successfully deploying policies on real mobile manipulation tasks.

Conclusion: Learning-based base pose planning from top-down views is viable and offers faster inference with transferable policies, making real-time mobile manipulation more practical.

Abstract: In Mobile Manipulation, selecting an optimal mobile base pose is essential
for successful object grasping. Previous works have addressed this problem
either through classical planning methods or by learning state-based policies.
They assume access to reliable state information, such as the precise object
poses and environment models. In this work, we study base pose planning
directly from top-down orthographic projections of the scene, which provide a
global overview of the scene while preserving spatial structure. We propose
VBM-NET, a learning-based method for base pose selection using such top-down
orthographic projections. We use equivariant TransporterNet to exploit spatial
symmetries and efficiently learn candidate base poses for grasping. Further, we
use graph neural networks to represent a varying number of candidate base poses
and use Reinforcement Learning to determine the optimal base pose among them.
We show that VBM-NET can produce comparable solutions to the classical methods
in significantly less computation time. Furthermore, we validate sim-to-real
transfer by successfully deploying a policy trained in simulation to real-world
mobile manipulation.

</details>


### [488] [Using Robotics to Improve Transcatheter Edge-to-Edge Repair of the Mitral Valve](https://arxiv.org/abs/2510.04178)
*Léa Pistorius,Namrata U. Nayar,Phillip Tran,Sammy Elmariah,Pierre E. Dupont*

Main category: cs.RO

TL;DR: Robotic joint-based control, using a game controller, improves transcatheter mitral valve edge-to-edge repair by replacing manual handle-based control; in phantom models, robotics reduced procedure time and motion errors and increased clip placement accuracy, suggesting robotics can overcome manual system limitations.


<details>
  <summary>Details</summary>
Motivation: Manual catheter-based transcatheter repair has mechanical limitations and a steep learning curve. A more intuitive, reliable control interface could enhance performance and safety in complex procedures.

Method: Replace complex, handle-based control with intuitive robotic joint-based control via a game controller. Break the overall device delivery into motion-specific steps and compare manual vs robotic performance step-by-step in a phantom model of heart and vasculature. Metrics include procedure duration and clip placement accuracy.

Result: Robotic assistance reduced procedural time and motion errors and improved clip placement accuracy compared with manual control.

Conclusion: Robotics can address key limitations of manual transcatheter systems, offering a more reliable and user-friendly platform for complex transcatheter procedures such as mitral valve edge-to-edge repair.

Abstract: Transcatheter valve repair presents significant challenges due to the
mechanical limitations and steep learning curve associated with manual catheter
systems. This paper investigates the use of robotics to facilitate
transcatheter procedures in the context of mitral valve edge-to-edge repair.
The complex handle-based control of a clinical repair device is replaced by
intuitive robotic joint-based control via a game controller. Manual versus
robotic performance is analyzed by decomposing the overall device delivery task
into motion-specific steps and comparing capabilities on a step-by-step basis
in a phantom model of the heart and vasculature. Metrics include procedure
duration and clip placement accuracy. Results demonstrate that the robotic
system can reduce procedural time and motion errors while also improving
accuracy of clip placement. These findings suggest that robotic assistance can
address key limitations of manual systems, offering a more reliable and
user-friendly platform for complex transcatheter procedures.

</details>


### [489] [Zenbo Patrol: A Social Assistive Robot Based on Multimodal Deep Learning for Real-time Illegal Parking Recognition and Notification](https://arxiv.org/abs/2510.04190)
*Jian-jie Zheng,Chih-kai Yang,Po-han Chen,Lyn Chao-ling Chen*

Main category: cs.RO

TL;DR: A social robot patrol uses GPT-4o-based multimodal license plate recognition to detect illegal parking in real time, with no preprocessing, validated in a simulated indoor parking lot and capable of notifying a system manager.


<details>
  <summary>Details</summary>
Motivation: Address real-time detection of illegal parking by automating license-plate recognition with a capable multimodal model, reducing preprocessing and enabling practical deployment in indoor parking environments.

Method: A dual-model pipeline and a large multimodal model comparison are conducted; GPT-4o is adopted for license plate recognition without preprocessing. The robot navigates a flat indoor parking lot, adjusts camera angles to capture plates, recognizes plate numbers via GPT-4o, determines legality, and sends Line notifications to the system manager upon detection.

Result: The approach achieves high accuracy in license plate recognition and demonstrates feasibility of a multimodal deep-learning method for real-world, indoor parking scenarios, with the social robot able to detect illegal parking in real time.

Conclusion: The work presents a novel multimodal deep-learning approach for license plate recognition integrated into a social assistive robot, proving feasibility and effectiveness for real-world indoor parking enforcement.

Abstract: In the study, the social robot act as a patrol to recognize and notify
illegal parking in real-time. Dual-model pipeline method and large multimodal
model were compared, and the GPT-4o multimodal model was adopted in license
plate recognition without preprocessing. For moving smoothly on a flat ground,
the robot navigated in a simulated parking lot in the experiments. The robot
changes angle view of the camera automatically to capture the images around
with the format of license plate number. From the captured images of the robot,
the numbers on the plate are recognized through the GPT-4o model, and
identifies legality of the numbers. When an illegal parking is detected, the
robot sends Line messages to the system manager immediately. The contribution
of the work is that a novel multimodal deep learning method has validated with
high accuracy in license plate recognition, and a social assistive robot is
also provided for solving problems in a real scenario, and can be applied in an
indoor parking lot.

</details>


### [490] [Flexible Locomotion Learning with Diffusion Model Predictive Control](https://arxiv.org/abs/2510.04234)
*Runhan Huang,Haldun Balim,Heng Yang,Yilun Du*

Main category: cs.RO

TL;DR: Diffusion-MPC uses a diffusion-model-based planning prior to enable flexible, test-time adaptable legged locomotion via reward/constraint planning and interactive training, outperforming fixed-policy RL and traditional MPC in adaptability, demonstrated on real-world locomotion.


<details>
  <summary>Details</summary>
Motivation: Legged robots require robust and adaptable controllers that respect task and safety constraints. Model-free RL often yields a fixed policy hard to adapt at test time, while classical MPC needs accurate dynamics models. Diffusion-MPC combines a learned generative prior with reward/constraint-based planning to achieve adaptable behavior.

Method: A conditional diffusion model is trained to jointly predict future states and actions as a planning prior. During planning, reverse diffusion steps integrate reward planning and constraint projection, yielding trajectories that meet objectives and physical limits. An interactive training loop collects planner trajectories from the environment, filters and reweights them by realized returns, and updates the denoiser to improve planning without retraining for new rewards.

Result: Real-world validation shows strong locomotion performance and significant test-time adaptability, with the planner adjusting to new reward specifications while respecting constraints.

Conclusion: Diffusion-MPC offers a flexible, adaptable planning framework for legged locomotion by marrying diffusion-based dynamics priors with reward/constraint optimization and an interactive training loop, enabling adaptation at test time without retraining.

Abstract: Legged locomotion demands controllers that are both robust and adaptable,
while remaining compatible with task and safety considerations. However,
model-free reinforcement learning (RL) methods often yield a fixed policy that
can be difficult to adapt to new behaviors at test time. In contrast, Model
Predictive Control (MPC) provides a natural approach to flexible behavior
synthesis by incorporating different objectives and constraints directly into
its optimization process. However, classical MPC relies on accurate dynamics
models, which are often difficult to obtain in complex environments and
typically require simplifying assumptions. We present Diffusion-MPC, which
leverages a learned generative diffusion model as an approximate dynamics prior
for planning, enabling flexible test-time adaptation through reward and
constraint based optimization. Diffusion-MPC jointly predicts future states and
actions; at each reverse step, we incorporate reward planning and impose
constraint projection, yielding trajectories that satisfy task objectives while
remaining within physical limits. To obtain a planning model that adapts beyond
imitation pretraining, we introduce an interactive training algorithm for
diffusion based planner: we execute our reward-and-constraint planner in
environment, then filter and reweight the collected trajectories by their
realized returns before updating the denoiser. Our design enables strong
test-time adaptability, allowing the planner to adjust to new reward
specifications without retraining. We validate Diffusion-MPC on real world,
demonstrating strong locomotion and flexible adaptation.

</details>


### [491] [ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context](https://arxiv.org/abs/2510.04246)
*Huiwon Jang,Sihyun Yu,Heeseung Kwon,Hojin Jeon,Younggyo Seo,Jinwoo Shin*

Main category: cs.RO

TL;DR: ContextVLA compresses multi-frame context into a single token to enable robust temporal context usage in Vision-Language-Action robotic policies, achieving multi-frame benefits with lower compute.


<details>
  <summary>Details</summary>
Motivation: Temporal context is crucial in partially observable robotic tasks and multi-frame observations have shown inconsistent gains in behavior cloning; high-dimensional video inputs cause substantial computational overhead, necessitating efficient methods to leverage temporal information.

Method: Introduce ContextVLA, a Vision-Language-Action policy that compresses past observations into a single context token, enabling efficient use of temporal context by leveraging a Vision-Language Model's temporal understanding during action generation.

Result: ContextVLA consistently improves over single-frame VLAs and matches the benefits of full multi-frame training while reducing training and inference time.

Conclusion: Effective temporal-context utilization can be achieved efficiently in VLA-based policies by encoding past observations into a compact context token, and ContextVLA provides robust performance with lower computational cost.

Abstract: Leveraging temporal context is crucial for success in partially observable
robotic tasks. However, prior work in behavior cloning has demonstrated
inconsistent performance gains when using multi-frame observations. In this
paper, we introduce ContextVLA, a policy model that robustly improves robotic
task performance by effectively leveraging multi-frame observations. Our
approach is motivated by the key observation that Vision-Language-Action models
(VLA), i.e., policy models built upon a Vision-Language Model (VLM), more
effectively utilize multi-frame observations for action generation. This
suggests that VLMs' inherent temporal understanding capability enables them to
extract more meaningful context from multi-frame observations. However, the
high dimensionality of video inputs introduces significant computational
overhead, making VLA training and inference inefficient. To address this,
ContextVLA compresses past observations into a single context token, allowing
the policy to efficiently leverage temporal context for action generation. Our
experiments show that ContextVLA consistently improves over single-frame VLAs
and achieves the benefits of full multi-frame training but with reduced
training and inference times.

</details>


### [492] [Integrated Planning and Control on Manifolds: Factor Graph Representation and Toolkit](https://arxiv.org/abs/2510.04278)
*Peiwen Yang,Weisong Wen,Runqiu Yang,Yuanyuan Zhang,Jiahao Hu,Yingming Chen,Naigui Xiao,Jiaqi Zhao*

Main category: cs.RO

TL;DR: FactorMPC is a factor-graph based MPC framework designed for systems evolving on nonlinear manifolds (e.g., attitude dynamics) that uses tangent-space Gaussian uncertainties, sparsity of factor graphs, and velocity-extended on-manifold CBF factors for real-time, safety-critical planning and control. It unifies dynamics, constraints, and objectives, and demonstrates superior quadrotor performance with open-source plug-and-play factors.


<details>
  <summary>Details</summary>
Motivation: MPC struggles with nonlinear manifolds due to singularities, over-parameterization, and convergence issues. A modular, scalable, and geometrically consistent framework is needed to handle manifold-valued states, constraints, and safety requirements in real-time.

Method: Formulates MPC as a factor graph with manifold-valued states represented in tangent spaces, exploiting sparsity for real-time optimization. Introduces velocity-extended on-manifold CBF-based obstacle avoidance factors and provides an open-source, plug-and-play factor library for researchers and practitioners.

Result: Simulations and real-world experiments on a quadrotor show improved trajectory tracking and obstacle avoidance compared to baseline methods, with real-time performance on high-dimensional problems.

Conclusion: Bridges graphical models with safety-critical MPC to offer a scalable, geometrically consistent framework for integrated planning and control, with open-source factors to enhance reproducibility and adoption.

Abstract: Model predictive control (MPC) faces significant limitations when applied to
systems evolving on nonlinear manifolds, such as robotic attitude dynamics and
constrained motion planning, where traditional Euclidean formulations struggle
with singularities, over-parameterization, and poor convergence. To overcome
these challenges, this paper introduces FactorMPC, a factor-graph based MPC
toolkit that unifies system dynamics, constraints, and objectives into a
modular, user-friendly, and efficient optimization structure. Our approach
natively supports manifold-valued states with Gaussian uncertainties modeled in
tangent spaces. By exploiting the sparsity and probabilistic structure of
factor graphs, the toolkit achieves real-time performance even for
high-dimensional systems with complex constraints. The velocity-extended
on-manifold control barrier function (CBF)-based obstacle avoidance factors are
designed for safety-critical applications. By bridging graphical models with
safety-critical MPC, our work offers a scalable and geometrically consistent
framework for integrated planning and control. The simulations and experimental
results on the quadrotor demonstrate superior trajectory tracking and obstacle
avoidance performance compared to baseline methods. To foster research
reproducibility, we have provided open-source implementation offering
plug-and-play factors.

</details>


### [493] [Stability-Aware Retargeting for Humanoid Multi-Contact Teleoperation](https://arxiv.org/abs/2510.04353)
*Stephen McCrory,Romeo Orsolino,Dhruv Thanki,Luigi Penco,Robert Griffin*

Main category: cs.RO

TL;DR: A centroidal stability margin-based retargeting framework for teleoperation of humanoid robots, dynamically adjusting contact points and posture to maintain stability; validated in simulation and hardware with improved impulse resilience and torque margins.


<details>
  <summary>Details</summary>
Motivation: Teleoperation with hand contacts and non-coplanar surfaces often leads to motor torque saturation and slipping; there is a need for adaptive stability management that can reconfigure contacts and posture during teleoperation.

Method: Compute the stability margin gradient analytically and use it to identify teleoperation setpoints with high sensitivity; dynamically adjust contact points and posture based on this gradient to maintain stability during teleoperation; validate in simulation and on hardware. 

Result: Simulation and hardware experiments on a humanoid robot show increased stability margins during teleoperation; higher margins correlate with improved impulse resilience and greater joint torque margins. 

Conclusion: A stability-margin-guided retargeting approach improves teleoperation robustness in challenging contact scenarios by adaptively reconfiguring contacts and posture to sustain stability and performance.

Abstract: Teleoperation is a powerful method to generate reference motions and enable
humanoid robots to perform a broad range of tasks. However, teleoperation
becomes challenging when using hand contacts and non-coplanar surfaces, often
leading to motor torque saturation or loss of stability through slipping. We
propose a centroidal stability-based retargeting method that dynamically
adjusts contact points and posture during teleoperation to enhance stability in
these difficult scenarios. Central to our approach is an efficient analytical
calculation of the stability margin gradient. This gradient is used to identify
scenarios for which stability is highly sensitive to teleoperation setpoints
and inform the local adjustment of these setpoints. We validate the framework
in simulation and hardware by teleoperating manipulation tasks on a humanoid,
demonstrating increased stability margins. We also demonstrate empirically that
higher stability margins correlate with improved impulse resilience and joint
torque margin.

</details>


### [494] [Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators](https://arxiv.org/abs/2510.04354)
*Apurva Badithela,David Snyder,Lihan Zha,Joseph Mikhail,Matthew O'Kelly,Anushri Dixit,Anirudha Majumdar*

Main category: cs.RO

TL;DR: SureSim is a framework that combines a small amount of paired real and simulation evaluations to debias large-scale simulation and provide non-asymptotic confidence intervals, reducing real hardware testing by about 20–25% while maintaining similar performance guarantees.


<details>
  <summary>Details</summary>
Motivation: Rigorous evaluation of robot manipulation policies is difficult and expensive: real-world trials are noisy and scarce, and current practice relies on few hardware trials without statistical guarantees. A scalable method is needed to reliably quantify real-world performance using both simulation and limited real data.

Method: Formalize real-vs-simulation evaluation as a prediction-powered inference problem. Collect a small set of paired real and sim evaluations to correct bias in large-scale simulation. Use non-asymptotic mean estimation to produce confidence intervals on the real-world mean policy performance. Evaluate diffusion policies and a multi-task fine-tuned cpi_0e on a joint distribution of objects and initial conditions within physics-based simulation.

Result: The approach achieves reliable bounds with reduced hardware trials, claiming a savings of 20–25% in hardware evaluation effort to reach similar confidence guarantees on policy performance.

Conclusion: SureSim enables reliable real-world performance inferences by leveraging a small amount of real data to correct simulation bias and provide principled confidence intervals; this approach can accelerate evaluation of manipulation policies and may generalize to other tasks and domains.

Abstract: Rapid progress in imitation learning, foundation models, and large-scale
datasets has led to robot manipulation policies that generalize to a wide-range
of tasks and environments. However, rigorous evaluation of these policies
remains a challenge. Typically in practice, robot policies are often evaluated
on a small number of hardware trials without any statistical assurances. We
present SureSim, a framework to augment large-scale simulation with relatively
small-scale real-world testing to provide reliable inferences on the real-world
performance of a policy. Our key idea is to formalize the problem of combining
real and simulation evaluations as a prediction-powered inference problem, in
which a small number of paired real and simulation evaluations are used to
rectify bias in large-scale simulation. We then leverage non-asymptotic mean
estimation algorithms to provide confidence intervals on mean policy
performance. Using physics-based simulation, we evaluate both diffusion policy
and multi-task fine-tuned \(\pi_0\) on a joint distribution of objects and
initial conditions, and find that our approach saves over \(20-25\%\) of
hardware evaluation effort to achieve similar bounds on policy performance.

</details>


### [495] [PAD-TRO: Projection-Augmented Diffusion for Direct Trajectory Optimization](https://arxiv.org/abs/2510.04436)
*Jushan Chen,Santiago Paternain*

Main category: cs.RO

TL;DR: A model-based diffusion trajectory optimizer that enforces dynamic feasibility via a gradient-free projection in reverse diffusion; achieves zero dynamic feasibility error and ~4x higher success rate in quadrotor navigation compared to a SOTA baseline.


<details>
  <summary>Details</summary>
Motivation: Diffusion models can capture multi-modal distributions in trajectory planning but nonlinear equality constraints (dynamic feasibility) are hard to enforce. Existing single-shooting diffusion approaches apply controls forward and often fail to enforce state constraints, leading to suboptimal solutions.

Method: Direct trajectory optimization using model-based diffusion with a gradient-free projection embedded in the reverse diffusion process to enforce dynamic feasibility; outputs a sequence of states directly rather than denoising controls.

Result: Zero dynamic feasibility error and approximately 4x improvement in success rate in a quadrotor waypoint navigation task with dense static obstacles compared to a recent state-of-the-art baseline.

Conclusion: The proposed approach effectively enforces dynamic feasibility in diffusion-based trajectory optimization and yields significant gains in feasibility and success rate, indicating strong potential for constrained trajectory planning.

Abstract: Recently, diffusion models have gained popularity and attention in trajectory
optimization due to their capability of modeling multi-modal probability
distributions. However, addressing nonlinear equality constraints, i.e, dynamic
feasi- bility, remains a great challenge in diffusion-based trajectory
optimization. Recent diffusion-based trajectory optimization frameworks rely on
a single-shooting style approach where the denoised control sequence is applied
to forward propagate the dynamical system, which cannot explicitly enforce
constraints on the states and frequently leads to sub-optimal solutions. In
this work, we propose a novel direct trajectory optimization approach via
model-based diffusion, which directly generates a sequence of states. To ensure
dynamic feasibility, we propose a gradient-free projection mechanism that is
incorporated into the reverse diffusion process. Our results show that,
compared to a recent state-of-the-art baseline, our approach leads to zero
dynamic feasibility error and approximately 4x higher success rate in a
quadrotor waypoint navigation scenario involving dense static obstacles.

</details>


### [496] [Velocity-Form Data-Enabled Predictive Control of Soft Robots under Unknown External Payloads](https://arxiv.org/abs/2510.04509)
*Huanqing Wang,Kaixiang Zhang,Kyungjoon Lee,Yu Mei,Vaibhav Srivastava,Jun Sheng,Ziyou Song,Zhaojian Li*

Main category: cs.RO

TL;DR: A velocity-form, incremental DeePC approach for soft robots that robustly handles unknown payloads, outperforming standard DeePC in experiments without using disturbance estimators.


<details>
  <summary>Details</summary>
Motivation: Unknown payloads and disturbances can significantly alter soft-robot dynamics, causing offset errors in data-driven controllers. There is a need for a robust approach that leverages data alone to maintain performance without tuning disturbance models.

Method: Introduce a velocity-form DeePC with an incremental data representation. This framework uses input-output data directly, avoids weighted datasets and disturbance estimators, and aims to mitigate payload-induced dynamics changes. Experimental validation on a planar soft robot compares performance to standard DeePC.

Result: The proposed method achieves superior performance over standard DeePC under unknown payloads, demonstrating robustness to payload-induced dynamics without requiring disturbance estimators or data weighting.

Conclusion: The velocity-form, incremental DeePC framework provides robust, optimal control of soft robots under unknown payloads using a purely data-driven approach, eliminating the need for disturbance modeling and weighted data preprocessing. Experimental validation supports its effectiveness.

Abstract: Data-driven control methods such as data-enabled predictive control (DeePC)
have shown strong potential in efficient control of soft robots without
explicit parametric models. However, in object manipulation tasks, unknown
external payloads and disturbances can significantly alter the system dynamics
and behavior, leading to offset error and degraded control performance. In this
paper, we present a novel velocity-form DeePC framework that achieves robust
and optimal control of soft robots under unknown payloads. The proposed
framework leverages input-output data in an incremental representation to
mitigate performance degradation induced by unknown payloads, eliminating the
need for weighted datasets or disturbance estimators. We validate the method
experimentally on a planar soft robot and demonstrate its superior performance
compared to standard DeePC in scenarios involving unknown payloads.

</details>


### [497] [Everything-Grasping (EG) Gripper: A Universal Gripper with Synergistic Suction-Grasping Capabilities for Cross-Scale and Cross-State Manipulation](https://arxiv.org/abs/2510.04585)
*Jianshu Zhou,Jing Shu,Tianle Pan,Puchen Zhu,Jiajun An,Huayu Zhang,Junda Huang,Upinder Kaur,Xin Ma,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: A soft gripper combining surface suction and granular jamming enables cross-scale and cross-state grasping of solids and liquids with a single compliant device, plus tactile sensing and mode-selection. Demonstrates underwater, fragile-object handling and liquid capture; claims first to unify solid/liquid grasping across scales.


<details>
  <summary>Details</summary>
Motivation: To achieve universal grasping with a single soft gripper that can handle objects across vastly different sizes and physical states (solids and liquids) without requiring airtight sealing, addressing a core limitation in soft robotics grip versatility.

Method: Introduce the Everything-Grasping (EG) Gripper: a soft end-effector that integratesDistributed surface suction with internal granular jamming, allowing manipulation without airtight sealing. Adds a tactile sensing framework combining liquid detection and pressure-based suction feedback. Employs TIGMS (tactile-Inferred Grasping Mode Selection) algorithm to autonomously select grasping modes based on distributed pressure and voltage signals. Validate via experiments on underwater grasping, fragile objects, and liquid capture.

Result: The device handles objects with surface areas from 0.2 mm2 to >62,000 mm2, enabling manipulation of objects nearly 3,500x smaller and 88x larger than its contact area (~707 mm2). Demonstrates robust and repeatable performance across diverse tasks including underwater grasping, fragile object handling, and liquid capture.

Conclusion: This work claims the first soft gripper to reliably grasp both solid and liquid targets across scales using a unified compliant architecture, highlighting a step toward universal soft-gripper manipulation across states and sizes.

Abstract: Grasping objects across vastly different sizes and physical states-including
both solids and liquids-with a single robotic gripper remains a fundamental
challenge in soft robotics. We present the Everything-Grasping (EG) Gripper, a
soft end-effector that synergistically integrates distributed surface suction
with internal granular jamming, enabling cross-scale and cross-state
manipulation without requiring airtight sealing at the contact interface with
target objects. The EG Gripper can handle objects with surface areas ranging
from sub-millimeter scale 0.2 mm2 (glass bead) to over 62,000 mm2 (A4 sized
paper and woven bag), enabling manipulation of objects nearly 3,500X smaller
and 88X larger than its own contact area (approximated at 707 mm2 for a 30
mm-diameter base). We further introduce a tactile sensing framework that
combines liquid detection and pressure-based suction feedback, enabling
real-time differentiation between solid and liquid targets. Guided by the
actile-Inferred Grasping Mode Selection (TIGMS) algorithm, the gripper
autonomously selects grasping modes based on distributed pressure and voltage
signals. Experiments across diverse tasks-including underwater grasping,
fragile object handling, and liquid capture-demonstrate robust and repeatable
performance. To our knowledge, this is the first soft gripper to reliably grasp
both solid and liquid objects across scales using a unified compliant
architecture.

</details>


### [498] [MobRT: A Digital Twin-Based Framework for Scalable Learning in Mobile Manipulation](https://arxiv.org/abs/2510.04592)
*Yilin Mei,Peng Qiu,Wei Zhang,WenChao Zhang,Wenjie Song*

Main category: cs.RO

TL;DR: MobRT introduces a digital twin framework to generate diverse, coherent demonstrations for mobile manipulation—covering articulated-object interaction and mobile-base pick-and-place—enabling improved imitation-learning data quality and better sim-to-real transfer.


<details>
  <summary>Details</summary>
Motivation: High-quality, scalable demonstration data for imitation learning is hard to obtain for mobile manipulators, which must coordinate base motion and arm manipulation in complex, partially observable environments; existing work is limited to simpler tabletop tasks.

Method: MobRT integrates a digital twin with virtual kinematic control and whole-body motion planning to autonomously generate diverse, realistic demonstrations of whole-body tasks; it builds a benchmark across baselines by evaluating data quality and task success as a function of the number of trajectories, and validates in both simulated and real-world settings.

Result: The framework yields high-quality, diverse demonstration data that correlates strongly with task success, and experiments show that policy generalization and performance improve when trained on MobRT-generated data, with robustness in both simulated and real-world environments.

Conclusion: MobRT effectively bridges the sim-to-real gap for mobile manipulation by providing a scalable data-generation pipeline and benchmark, facilitating improved imitation-learning performance in complex, whole-body tasks.

Abstract: Recent advances in robotics have been largely driven by imitation learning,
which depends critically on large-scale, high-quality demonstration data.
However, collecting such data remains a significant challenge-particularly for
mobile manipulators, which must coordinate base locomotion and arm manipulation
in high-dimensional, dynamic, and partially observable environments.
Consequently, most existing research remains focused on simpler tabletop
scenarios, leaving mobile manipulation relatively underexplored. To bridge this
gap, we present \textit{MobRT}, a digital twin-based framework designed to
simulate two primary categories of complex, whole-body tasks: interaction with
articulated objects (e.g., opening doors and drawers) and mobile-base
pick-and-place operations. \textit{MobRT} autonomously generates diverse and
realistic demonstrations through the integration of virtual kinematic control
and whole-body motion planning, enabling coherent and physically consistent
execution. We evaluate the quality of \textit{MobRT}-generated data across
multiple baseline algorithms, establishing a comprehensive benchmark and
demonstrating a strong correlation between task success and the number of
generated trajectories. Experiments integrating both simulated and real-world
demonstrations confirm that our approach markedly improves policy
generalization and performance, achieving robust results in both simulated and
real-world environments.

</details>


### [499] [OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with Dense Depth or LiDAR, and GNSS](https://arxiv.org/abs/2510.04612)
*Simon Boche,Jaehyung Jung,Sebastián Barbas Laina,Stefan Leutenegger*

Main category: cs.RO

TL;DR: OKVIS2-X is a state-of-the-art multi-sensor SLAM system that builds dense volumetric occupancy maps and scales to large environments in real time by using submapping, tight estimator-submap coupling, online camera extrinsic calibration, and multi-sensor fusion (vision, IMU, depth, LiDAR, GNSS).


<details>
  <summary>Details</summary>
Motivation: The need for usable, globally consistent maps with high pose accuracy for mobile robots, leveraging dense map representations and multiple sensing modalities to improve robustness and navigation performance.

Method: A unified SLAM framework that fuses visual, inertial, depth, LiDAR, and GNSS data into dense volumetric occupancy maps. It uses a submapping strategy to scale to long trajectories, tightly couples estimators with submaps via map alignment factors, and optionally performs online camera calibration to refine extrinsics.

Result: Achieves the highest trajectory accuracy in EuRoC compared to state-of-the-art, outperforms all competitors in the Hilti22 VI-only benchmark, is competitive in LiDAR-based benchmarks, and demonstrates state-of-the-art accuracy on large-scale VBR sequences.

Conclusion: OKVIS2-X delivers globally consistent, dense maps suitable for autonomous navigation, with scalable and robust multi-sensor SLAM, enhanced by online extrinsic calibration.

Abstract: To empower mobile robots with usable maps as well as highest state estimation
accuracy and robustness, we present OKVIS2-X: a state-of-the-art multi-sensor
Simultaneous Localization and Mapping (SLAM) system building dense volumetric
occupancy maps, while scalable to large environments and operating in realtime.
Our unified SLAM framework seamlessly integrates different sensor modalities:
visual, inertial, measured or learned depth, LiDAR and Global Navigation
Satellite System (GNSS) measurements. Unlike most state-of-the-art SLAM
systems, we advocate using dense volumetric map representations when leveraging
depth or range-sensing capabilities. We employ an efficient submapping strategy
that allows our system to scale to large environments, showcased in sequences
of up to 9 kilometers. OKVIS2-X enhances its accuracy and robustness by
tightly-coupling the estimator and submaps through map alignment factors. Our
system provides globally consistent maps, directly usable for autonomous
navigation. To further improve the accuracy of OKVIS2-X, we also incorporate
the option of performing online calibration of camera extrinsics. Our system
achieves the highest trajectory accuracy in EuRoC against state-of-the-art
alternatives, outperforms all competitors in the Hilti22 VI-only benchmark,
while also proving competitive in the LiDAR version, and showcases state of the
art accuracy in the diverse and large-scale sequences from the VBR dataset.

</details>


### [500] [Bio-Inspired Robotic Houbara: From Development to Field Deployment for Behavioral Studies](https://arxiv.org/abs/2510.04692)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.RO

TL;DR: Introduces a fully digitally replicable, biomimetic robotic surrogate of the female Houbara bustard, combining a digital fabrication pipeline with real-time perception and autonomous control, validated in field conditions to elicit natural bird interactions.


<details>
  <summary>Details</summary>
Motivation: Address challenges in field ethology research—need for lifelike morphology, durable outdoor operation, and adaptive perception—to enable controlled studies and conservation applications.

Method: A six-wheeled rocker-bogie robot replicating bustard morphology uses high-res 3D scanning, parametric CAD, articulated 3D printing, and photoreal UV vinyl finishing; embedded NVIDIA Jetson for RGB/thermal sensing, YOLO-based detection, and autonomous visual servoing; adds a lightweight thermal-visible fusion module; field testing in desert aviaries with real-time operation.

Result: Demonstrated real-time operation at 15–22 FPS with <100 ms latency; elicited natural recognition and interactive responses from live bustards under harsh outdoor conditions.

Conclusion: The framework provides reproducible digital fabrication, embodied visual intelligence, and ecological validation, supplying a transferable blueprint for animal-robot interaction research, conservation robotics, and public engagement.

Abstract: Biomimetic intelligence and robotics are transforming field ecology by
enabling lifelike robotic surrogates that interact naturally with animals under
real world conditions. Studying avian behavior in the wild remains challenging
due to the need for highly realistic morphology, durable outdoor operation, and
intelligent perception that can adapt to uncontrolled environments. We present
a next generation bio inspired robotic platform that replicates the morphology
and visual appearance of the female Houbara bustard to support controlled
ethological studies and conservation oriented field research. The system
introduces a fully digitally replicable fabrication workflow that combines high
resolution structured light 3D scanning, parametric CAD modelling, articulated
3D printing, and photorealistic UV textured vinyl finishing to achieve
anatomically accurate and durable robotic surrogates. A six wheeled rocker
bogie chassis ensures stable mobility on sand and irregular terrain, while an
embedded NVIDIA Jetson module enables real time RGB and thermal perception,
lightweight YOLO based detection, and an autonomous visual servoing loop that
aligns the robot's head toward detected targets without human intervention. A
lightweight thermal visible fusion module enhances perception in low light
conditions. Field trials in desert aviaries demonstrated reliable real time
operation at 15 to 22 FPS with latency under 100 ms and confirmed that the
platform elicits natural recognition and interactive responses from live
Houbara bustards under harsh outdoor conditions. This integrated framework
advances biomimetic field robotics by uniting reproducible digital fabrication,
embodied visual intelligence, and ecological validation, providing a
transferable blueprint for animal robot interaction research, conservation
robotics, and public engagement.

</details>


### [501] [Building Gradient by Gradient: Decentralised Energy Functions for Bimanual Robot Assembly](https://arxiv.org/abs/2510.04696)
*Alexander L. Mitchell,Joe Watson,Ingmar Posner*

Main category: cs.RO

TL;DR: A decentralized gradient-based planning framework using adaptive potential functions enables rapid, emergent coordination and retries in bimanual tight-tolerance assembly, reducing reliance on slow long-horizon motion planning.


<details>
  <summary>Details</summary>
Motivation: Task and motion planning (TAMP) for bimanual assembly is effective but often too slow to adapt to disturbances requiring new sequencing and optimization, especially under tight tolerances. A flexible, fast planning approach is needed to handle dynamic, contact-rich operations.

Method: Proposes a decentralized gradient-based framework that builds a piecewise continuous energy function via automatic composition of adaptive potential functions. This yields sub-goals from myopic optimization rather than long-horizon planning, enabling rapid replanning and emergent coordination (retries, handovers, coordinated motions).

Result: The method scales to physical bimanual assembly tasks with tight tolerances. Experiments show gradient-based rapid replanning produces automatic retries, coordinated motions, and autonomous handovers in an emergent, decentralized manner.

Conclusion: The approach offers scalable, flexible planning for bimanual assembly, mitigating long-horizon planning bottlenecks and enabling robust, emergent coordination under disturbances; it leverages energy-function design to enable effective long-horizon behavior through local optimization.

Abstract: There are many challenges in bimanual assembly, including high-level
sequencing, multi-robot coordination, and low-level, contact-rich operations
such as component mating. Task and motion planning (TAMP) methods, while
effective in this domain, may be prohibitively slow to converge when adapting
to disturbances that require new task sequencing and optimisation. These events
are common during tight-tolerance assembly, where difficult-to-model dynamics
such as friction or deformation require rapid replanning and reattempts.
Moreover, defining explicit task sequences for assembly can be cumbersome,
limiting flexibility when task replanning is required. To simplify this
planning, we introduce a decentralised gradient-based framework that uses a
piecewise continuous energy function through the automatic composition of
adaptive potential functions. This approach generates sub-goals using only
myopic optimisation, rather than long-horizon planning. It demonstrates
effectiveness at solving long-horizon tasks due to the structure and adaptivity
of the energy function. We show that our approach scales to physical bimanual
assembly tasks for constructing tight-tolerance assemblies. In these
experiments, we discover that our gradient-based rapid replanning framework
generates automatic retries, coordinated motions and autonomous handovers in an
emergent fashion.

</details>


### [502] [Performance-guided Task-specific Optimization for Multirotor Design](https://arxiv.org/abs/2510.04724)
*Etor Arza,Welf Rehberg,Philipp Weiss,Mihir Kulkarni,Kostas Alexis*

Main category: cs.RO

TL;DR: Task-specific design optimization for multirotor UAVs using a hybrid methodology (reinforcement learning, Bayesian optimization, and CMA-ES) to search motor pose configurations under manufacturability and aerodynamic constraints, optimizing closed-loop task performance; results show superior agile waypoint navigation and sim2real validation.


<details>
  <summary>Details</summary>
Motivation: To overcome the gap between physical design and control performance in multirotor UAVs by directly optimizing designs for task-specific closed-loop performance, while enforcing manufacturability and minimizing aerodynamic interference.

Method: A design-optimization pipeline combining reinforcement learning, Bayesian optimization, and covariance matrix adaptation evolution strategy (CMA-ES) to explore motor pose configurations that satisfy manufacturability and low aerodynamic interference, with evaluation based on closed-loop performance in an agile waypoint navigation task. The approach includes sim-to-real testing by building and testing a selected optimized design in hardware.

Result: Optimized drone designs exhibit superior performance compared to conventional multirotor configurations and even fully actuated designs reported in the literature for agile waypoint navigation. A selected optimized design is built and validated in the real world, supporting sim-to-real transferability of the approach.

Conclusion: A task-driven design-optimization framework that aligns physical design with closed-loop task performance can outperform standard and some fully actuated designs, while also being validated in hardware; the method effectively navigates manufacturability and aerodynamic constraints and demonstrates practical sim-to-real applicability.

Abstract: This paper introduces a methodology for task-specific design optimization of
multirotor Micro Aerial Vehicles. By leveraging reinforcement learning,
Bayesian optimization, and covariance matrix adaptation evolution strategy, we
optimize aerial robot designs guided exclusively by their closed-loop
performance in a considered task. Our approach systematically explores the
design space of motor pose configurations while ensuring manufacturability
constraints and minimal aerodynamic interference. Results demonstrate that
optimized designs achieve superior performance compared to conventional
multirotor configurations in agile waypoint navigation tasks, including against
fully actuated designs from the literature. We build and test one of the
optimized designs in the real world to validate the sim2real transferability of
our approach.

</details>


### [503] [Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy](https://arxiv.org/abs/2510.04774)
*Weixu Zhu,Marco Dorigo,Mary Katherine Heinrich*

Main category: cs.RO

TL;DR: SoNS-based swarm robots can request and execute LLM-generated code on the fly when stuck, achieving 85% mission success in experiments with 6 real robots and 30+ simulations.


<details>
  <summary>Details</summary>
Motivation: Ease of designing swarm behaviors and obtaining global estimation of swarm configuration and environment to enable online automatic code generation and adaptation.

Method: Introduce self-organizing nervous system (SoNS) framework; integrate with external LLM to generate and run code on demand; when the swarm gets stuck, SoNS solicits LLM code on the fly and executes it to complete the mission.

Result: Demonstrations with 6 real robots and simulation trials with over 30 robots; 85% success in completing its mission after using LLM-generated code.

Conclusion: Shows feasibility of combining SoNS with on-the-fly, LLM-assisted code generation to enhance swarm autonomy and adaptability; reduces manual programming effort; raises questions about reliability, safety, and generalization of LLM-driven behaviors.

Abstract: Our recently introduced self-organizing nervous system (SoNS) provides robot
swarms with 1) ease of behavior design and 2) global estimation of the swarm
configuration and its collective environment, facilitating the implementation
of online automatic code generation for robot swarms. In a demonstration with 6
real robots and simulation trials with >30 robots, we show that when a
SoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code
generated by an external LLM on the fly, completing its mission with an 85%
success rate.

</details>


### [504] [TAG-K: Tail-Averaged Greedy Kaczmarz for Computationally Efficient and Performant Online Inertial Parameter Estimation](https://arxiv.org/abs/2510.04839)
*Shuo Sha,Anupam Bhakta,Zhenyuan Jiang,Kevin Qiu,Ishaan Mahajan,Gabriel Bravo,Brian Plancher*

Main category: cs.RO

TL;DR: TAG-K is a lightweight Kaczmarz-based online inertial parameter estimator using greedy randomized row selection and tail averaging to enable fast, robust adaptation for robotic control, outperforming RLS and KF especially on embedded hardware.


<details>
  <summary>Details</summary>
Motivation: Accurate online inertial parameter estimation is essential for adaptive robotic control, but traditional methods (RLS, KF) struggle with abrupt parameter shifts and high computational cost, hindering real-time performance on resource-constrained robots.

Method: Introduce TAG-K, a Kaczmarz-based estimator that combines (1) greedy randomized row selection for faster convergence and (2) tail averaging to improve robustness to noise and inconsistency, preserving low per-iteration complexity.

Result: In synthetic benchmarks and quadrotor tracking, TAG-K achieved 1.5x-1.9x faster solve times on laptop-class CPUs and 4.8x-20.7x on embedded microcontrollers, with improved resilience to measurement noise and a 25% reduction in estimation error, yielding nearly 2x better end-to-end tracking.

Conclusion: TAG-K provides fast, robust online inertial parameter estimation suitable for real-time adaptive control on constrained hardware, outperforming RLS, KF, and other Kaczmarz variants in speed and accuracy.

Abstract: Accurate online inertial parameter estimation is essential for adaptive
robotic control, enabling real-time adjustment to payload changes,
environmental interactions, and system wear. Traditional methods such as
Recursive Least Squares (RLS) and the Kalman Filter (KF) often struggle to
track abrupt parameter shifts or incur high computational costs, limiting their
effectiveness in dynamic environments and for computationally constrained
robotic systems. As such, we introduce TAG-K, a lightweight extension of the
Kaczmarz method that combines greedy randomized row selection for rapid
convergence with tail averaging for robustness under noise and inconsistency.
This design enables fast, stable parameter adaptation while retaining the low
per-iteration complexity inherent to the Kaczmarz framework. We evaluate TAG-K
in synthetic benchmarks and quadrotor tracking tasks against RLS, KF, and other
Kaczmarz variants. TAG-K achieves 1.5x-1.9x faster solve times on laptop-class
CPUs and 4.8x-20.7x faster solve times on embedded microcontrollers. More
importantly, these speedups are paired with improved resilience to measurement
noise and a 25% reduction in estimation error, leading to nearly 2x better
end-to-end tracking performance.

</details>


### [505] [CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery](https://arxiv.org/abs/2510.04883)
*Nathan Shankar,Pawel Ladosz,Hujun Yin*

Main category: cs.RO

TL;DR: A U-Net-based IR image reconstruction method cleans emitter-patterned IR streams to improve robotic perception in low light, enabling reliable vision tasks across illumination conditions.


<details>
  <summary>Details</summary>
Motivation: IR in low light is noisy but emitter patterns contaminate data. Cleaning them improves object detection, tracking, and localization.

Method: Train a U-Net that reconstructs clean IR images from emitter-populated input; end-to-end learning to remove emitter artifacts and enhance downstream tasks.

Result: Outperforms existing IR enhancement techniques; yields better image quality and improved performance of perception tasks in both well-lit and extremely low-light scenes.

Conclusion: Provides robust, illumination-invariant perception for vision-driven robots by restoring clean IR imagery from emitter-rich streams.

Abstract: This paper presents a novel approach for enabling robust robotic perception
in dark environments using infrared (IR) stream. IR stream is less susceptible
to noise than RGB in low-light conditions. However, it is dominated by active
emitter patterns that hinder high-level tasks such as object detection,
tracking and localisation. To address this, a U-Net-based architecture is
proposed that reconstructs clean IR images from emitter-populated input,
improving both image quality and downstream robotic performance. This approach
outperforms existing enhancement techniques and enables reliable operation of
vision-driven robotic systems across illumination conditions from well-lit to
extreme low-light scenes.

</details>


### [506] [HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks](https://arxiv.org/abs/2510.04898)
*Zheng Xiong,Kang Li,Zilin Wang,Matthew Jackson,Jakob Foerster,Shimon Whiteson*

Main category: cs.RO

TL;DR: HyperVLA introduces a hypernetwork-based VLA model that activates only a small task-specific policy at inference, drastically reducing activation and speeding up inference while preserving or improving zero-shot and few-shot generalization compared to monolithic VLAs like OpenVLA.


<details>
  <summary>Details</summary>
Motivation: Vision-Language-Action models have strong generalization but are burdened by high inference costs. There is a need for efficient, scalable policies that retain large-capacity training advantages.

Method: Propose HyperVLA with a hypernetwork (HN) architecture that selectively activates a small task-specific policy during inference. Leverage priors from vision foundation models, apply HN normalization, and implement an action generation strategy. Train on large-scale robotic data to support multi-task behaviors.

Result: HyperVLA achieves similar or higher success rates in zero-shot generalization and few-shot adaptation compared to monolithic VLAs, while reducing inference costs. Specifically, it activates 90x fewer parameters at test time and speeds up inference by 120x versus OpenVLA.

Conclusion: HN-based modular activation enables efficient, scalable, and generalist VLA policies for robotics, balancing high model capacity during training with lightweight, fast inference during deployment. The approach offers substantial practical gains for real-time robotic control.

Abstract: Built upon language and vision foundation models with strong generalization
ability and trained on large-scale robotic data, Vision-Language-Action (VLA)
models have recently emerged as a promising approach to learning generalist
robotic policies. However, a key drawback of existing VLAs is their extremely
high inference costs. In this paper, we propose HyperVLA to address this
problem. Unlike existing monolithic VLAs that activate the whole model during
both training and inference, HyperVLA uses a novel hypernetwork (HN)-based
architecture that activates only a small task-specific policy during inference,
while still retaining the high model capacity needed to accommodate diverse
multi-task behaviors during training. Successfully training an HN-based VLA is
nontrivial so HyperVLA contains several key algorithm design features that
improve its performance, including properly utilizing the prior knowledge from
existing vision foundation models, HN normalization, and an action generation
strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even
higher success rate for both zero-shot generalization and few-shot adaptation,
while significantly reducing inference costs. Compared to OpenVLA, a
state-of-the-art VLA model, HyperVLA reduces the number of activated parameters
at test time by $90\times$, and accelerates inference speed by $120\times$.
Code is publicly available at https://github.com/MasterXiong/HyperVLA

</details>


### [507] [Efficient Navigation in Unknown Indoor Environments with Vision-Language Models](https://arxiv.org/abs/2510.04991)
*D. Schwartz,K. Kondo,J. P. How*

Main category: cs.RO

TL;DR: A zero-shot VLM-based subgoal planner integrated with DYNUS improves indoor navigation by reasoning over partial maps, yielding about 10% shorter paths in simulation.


<details>
  <summary>Details</summary>
Motivation: To overcome inefficiencies and greedy detours in exploring unknown indoor environments with many dead ends by enabling high-level reasoning about map structure via vision-language models.

Method: At each step, convert the 3D occupancy grid to a partial 2D map, generate candidate subgoals, and evaluate/rank them using a vision-language model. The best subgoal is chosen and fed into DYNUS; the VLM infers structural patterns (rooms, corridors) from incomplete maps and balances progress toward the goal with the risk of entering unknown space (zero-shot reasoning). This is evaluated in simulation.

Result: Demonstrates improved navigation efficiency in simulation, achieving ~10% shorter paths on average and reducing greedy detours into small rooms.

Conclusion: Shows that integrating VLM-based high-level reasoning into planning can improve exploration and navigation in unknown indoor environments, with potential generalization to other planners and real-world deployment.

Abstract: We present a novel high-level planning framework that leverages
vision-language models (VLMs) to improve autonomous navigation in unknown
indoor environments with many dead ends. Traditional exploration methods often
take inefficient routes due to limited global reasoning and reliance on local
heuristics. In contrast, our approach enables a VLM to reason directly about an
occupancy map in a zero-shot manner, selecting subgoals that are likely to lead
to more efficient paths. At each planning step, we convert a 3D occupancy grid
into a partial 2D map of the environment, and generate candidate subgoals. Each
subgoal is then evaluated and ranked against other candidates by the model. We
integrate this planning scheme into DYNUS \cite{kondo2025dynus}, a
state-of-the-art trajectory planner, and demonstrate improved navigation
efficiency in simulation. The VLM infers structural patterns (e.g., rooms,
corridors) from incomplete maps and balances the need to make progress toward a
goal against the risk of entering unknown space. This reduces common greedy
failures (e.g., detouring into small rooms) and achieves about 10\% shorter
paths on average.

</details>


### [508] [Walking, Rolling, and Beyond: First-Principles and RL Locomotion on a TARS-Inspired Robot](https://arxiv.org/abs/2510.05001)
*Aditya Sripada,Abhishek Warrier*

Main category: cs.RO

TL;DR: TARS3D demonstrates multimodal locomotion on a small non-anthropomorphic robot by combining analytic limit-cycle models for two gaits with learning-driven exploration, revealing richer behaviors beyond the two analytically treated cycles.


<details>
  <summary>Details</summary>
Motivation: To explore multimodal locomotion in unconventional, fiction-inspired morphologies and push beyond anthropomorphic designs by merging analytic synthesis with data-driven learning.

Method: Develop reduced-order models for two gaits (bipedal-like walk and rolling); derive closed-form limit-cycle conditions; validate analytically and on hardware; model rolling as an eight-spoke double rimless wheel due to four contact corners per telescopic leg; use deep reinforcement learning (DRL) in simulation to search the unexplored gait space and compare learned behaviors to analytic gaits under suitable priors.

Result: Hardware experiments confirm hip limits of ±150°, collision-free alternating contacts, and an eight-step hybrid limit cycle in rolling; rolling gait captured by the eight-spoke wheel model; DRL discovers novel gaits and can recover the analytic gaits when given appropriate priors.

Conclusion: Combining analytic synthesis with learning provides a promising pathway for multimodal robotics, especially on unconventional morphologies; further learning-driven search is likely to reveal additional, previously unexplored locomotion modes.

Abstract: Robotic locomotion research typically draws from biologically inspired leg
designs, yet many human-engineered settings can benefit from
non-anthropomorphic forms. TARS3D translates the block-shaped 'TARS' robot from
Interstellar into a 0.25 m, 0.99 kg research platform with seven actuated
degrees of freedom. The film shows two primary gaits: a bipedal-like walk and a
high-speed rolling mode. For TARS3D, we build reduced-order models for each,
derive closed-form limit-cycle conditions, and validate the predictions on
hardware. Experiments confirm that the robot respects its +/-150 degree hip
limits, alternates left-right contacts without interference, and maintains an
eight-step hybrid limit cycle in rolling mode. Because each telescopic leg
provides four contact corners, the rolling gait is modeled as an eight-spoke
double rimless wheel. The robot's telescopic leg redundancy implies a far
richer gait repertoire than the two limit cycles treated analytically. So, we
used deep reinforcement learning (DRL) in simulation to search the unexplored
space. We observed that the learned policy can recover the analytic gaits under
the right priors and discover novel behaviors as well. Our findings show that
TARS3D's fiction-inspired bio-transcending morphology can realize multiple
previously unexplored locomotion modes and that further learning-driven search
is likely to reveal more. This combination of analytic synthesis and
reinforcement learning opens a promising pathway for multimodal robotics.

</details>


### [509] [StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation](https://arxiv.org/abs/2510.05057)
*Mingyu Liu,Jiuhe Shu,Hui Chen,Zeju Li,Canyu Zhao,Jiange Yang,Shenyuan Gao,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: StaMo introduces an unsupervised, highly compact two-token state representation for embodied agents, learned with a lightweight encoder and a pre-trained Diffusion Transformer (DiT) decoder. The token difference via latent interpolation yields an effective latent action that can be decoded into robot actions. It improves LIBERO performance and real-world task success with minimal overhead, is interpretable, and generalizes across data sources (real robots, simulation, and egocentric video).


<details>
  <summary>Details</summary>
Motivation: The need for expressive yet compact state representations to enable efficient world modeling and decision making in embodied AI. Existing representations are either redundant or miss task-critical information, hindering performance and generalization.

Method: Unsupervised learning of a two-token state representation from static images using a lightweight encoder. A pre-trained Diffusion Transformer (DiT) decoder provides a strong generative prior for reconstruction/interpretation. The two tokens enable latent interpolation, whose difference acts as a latent action that can be decoded into executable robot actions. The representation can be integrated into existing VLA-based models with low inference overhead. StaMo emphasizes learning robust, disentangled dynamics without relying on video-based latent-action training.

Result: Empirical gains include 14.3% improvement on LIBERO and 30% higher real-world task success with minimal inference overhead. Latent-action latent interpolation improves policy co-training by 10.4% and offers enhanced interpretability. The approach scales across diverse data sources: real-world robot data, simulation, and human egocentric video.

Conclusion: StaMo shows that a compact two-token state representation derived from static images can capture structured dynamics suitable for generalizable robotic motion. The emergent latent actions from token differences provide an interpretable and scalable mechanism for decision making, reducing reliance on complex video-based latent action learning and enabling strong cross-domain performance.

Abstract: A fundamental challenge in embodied intelligence is developing expressive and
compact state representations for efficient world modeling and decision making.
However, existing methods often fail to achieve this balance, yielding
representations that are either overly redundant or lacking in task-critical
information. We propose an unsupervised approach that learns a highly
compressed two-token state representation using a lightweight encoder and a
pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong
generative prior. Our representation is efficient, interpretable, and
integrates seamlessly into existing VLA-based models, improving performance by
14.3% on LIBERO and 30% in real-world task success with minimal inference
overhead. More importantly, we find that the difference between these tokens,
obtained via latent interpolation, naturally serves as a highly effective
latent action, which can be further decoded into executable robot actions. This
emergent capability reveals that our representation captures structured
dynamics without explicit supervision. We name our method StaMo for its ability
to learn generalizable robotic Motion from compact State representation, which
is encoded from static images, challenging the prevalent dependence to learning
latent action on complex architectures and video data. The resulting latent
actions also enhance policy co-training, outperforming prior methods by 10.4%
with improved interpretability. Moreover, our approach scales effectively
across diverse data sources, including real-world robot data, simulation, and
human egocentric video.

</details>


### [510] [Automaton Constrained Q-Learning](https://arxiv.org/abs/2510.05061)
*Anastasios Manganaris,Vittorio Giammarino,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: ACQL combines goal-conditioned Q-learning with automaton-guided reinforcement to satisfy LTL-specified temporally extended tasks with safety constraints in continuous control; demonstrates superior performance and real-world robotics deployment.


<details>
  <summary>Details</summary>
Motivation: Real-world robots need to achieve sequences of goals under time-varying safety; standard RL can’t handle complex temporal logic; current RL+LTL methods are not scalable or effective in continuous domains.

Method: Proposes Automaton Constrained Q-Learning (ACQL) that uses LTL-to-automaton representations to encode stage progression and integrate stationary/non-stationary safety into Q-learning; supports most LTL specs; applies in continuous control tasks; includes a real-robot 6-DOF experiment.

Result: ACQL outperforms existing methods on various continuous control benchmarks; succeeds where others fail to reach goals or maintain safety; validated in a real-world robotic arm task with clutter and safety constraints.

Conclusion: ACQL offers a robust, scalable framework for learning robotic behaviors under rich temporal specifications, bridging goal progression and safety via automaton-guided RL.

Abstract: Real-world robotic tasks often require agents to achieve sequences of goals
while respecting time-varying safety constraints. However, standard
Reinforcement Learning (RL) paradigms are fundamentally limited in these
settings. A natural approach to these problems is to combine RL with
Linear-time Temporal Logic (LTL), a formal language for specifying complex,
temporally extended tasks and safety constraints. Yet, existing RL methods for
LTL objectives exhibit poor empirical performance in complex and continuous
environments. As a result, no scalable methods support both temporally ordered
goals and safety simultaneously, making them ill-suited for realistic robotics
scenarios. We propose Automaton Constrained Q-Learning (ACQL), an algorithm
that addresses this gap by combining goal-conditioned value learning with
automaton-guided reinforcement. ACQL supports most LTL task specifications and
leverages their automaton representation to explicitly encode stage-wise goal
progression and both stationary and non-stationary safety constraints. We show
that ACQL outperforms existing methods across a range of continuous control
tasks, including cases where prior methods fail to satisfy either goal-reaching
or safety constraints. We further validate its real-world applicability by
deploying ACQL on a 6-DOF robotic arm performing a goal-reaching task in a
cluttered, cabinet-like space with safety constraints. Our results demonstrate
that ACQL is a robust and scalable solution for learning robotic behaviors
according to rich temporal specifications.

</details>


### [511] [ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning](https://arxiv.org/abs/2510.05070)
*Siheng Zhao,Yanjie Ze,Yue Wang,C. Karen Liu,Pieter Abbeel,Guanya Shi,Rocky Duan*

Main category: cs.RO

TL;DR: ResMimic: a two-stage learning framework that uses a general motion policy plus a residual policy to enable precise and object-aware humanoid loco-manipulation, with specialized rewards and curriculum, validated in simulation and on a real Unitree G1.


<details>
  <summary>Details</summary>
Motivation: GMT-based humanoid policies reproduce human motions but lack precision and object awareness for loco-manipulation; need better control for reliable object interaction and locomotion.

Method: Train a base GMT policy on large-scale human motion; learn a residual policy to refine outputs for locomotion and object interactions; introduce point-cloud-based object-tracking reward, a contact reward, and a curriculum-based virtual object controller; evaluate in simulation and on real hardware (Unitree G1).

Result: Significant improvements in task success, training efficiency, and robustness versus strong baselines; successful transfer to real robot; videos available.

Conclusion: Residual learning atop GMT is effective for precise humanoid loco-manipulation, enabling better control and training stability; framework generalizes to real hardware.

Abstract: Humanoid whole-body loco-manipulation promises transformative capabilities
for daily service and warehouse tasks. While recent advances in general motion
tracking (GMT) have enabled humanoids to reproduce diverse human motions, these
policies lack the precision and object awareness required for
loco-manipulation. To this end, we introduce ResMimic, a two-stage residual
learning framework for precise and expressive humanoid control from human
motion data. First, a GMT policy, trained on large-scale human-only motion,
serves as a task-agnostic base for generating human-like whole-body movements.
An efficient but precise residual policy is then learned to refine the GMT
outputs to improve locomotion and incorporate object interaction. To further
facilitate efficient training, we design (i) a point-cloud-based object
tracking reward for smoother optimization, (ii) a contact reward that
encourages accurate humanoid body-object interactions, and (iii) a
curriculum-based virtual object controller to stabilize early training. We
evaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results
show substantial gains in task success, training efficiency, and robustness
over strong baselines. Videos are available at https://resmimic.github.io/ .

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [512] [WAREX: Web Agent Reliability Evaluation on Existing Benchmarks](https://arxiv.org/abs/2510.03285)
*Su Kara,Fazle Faisal,Suman Nath*

Main category: cs.AI

TL;DR: WAREX reveals robustness gaps in browser-based LLM agents by evaluating reliability on existing benchmarks (WebArena, WebVoyager, REAL), showing large drops in task success under real-world web instability and attacks.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for browser-based LLM agents use controlled, deterministic environments. Real Internet conditions introduce network instability, HTTPS, client/server failures, and web attacks (e.g., XSS) that can disrupt agent behavior. There is a need to evaluate reliability on real-world web conditions to understand robustness.

Method: Introduce WAREX (Web Agent Reliability Evaluation on Existing Benchmarks) and apply it to three benchmarks—WebArena, WebVoyager, and REAL. Measure task success rates to assess robustness of state-of-the-art agents under more realistic conditions.

Result: Experiments show significant drops in task success rates when WAREX is used, highlighting that current state-of-the-art browser-based LLM agents have limited robustness in realistic web environments.

Conclusion: WAREX exposes robustness gaps in current agents and underscores the need for more resilient designs and more realistic benchmarking to drive improvements in reliability under real-world web conditions.

Abstract: Recent advances in browser-based LLM agents have shown promise for automating
tasks ranging from simple form filling to hotel booking or online shopping.
Current benchmarks measure agent performance in controlled environments, such
as containers or stable networks, where websites behave deterministically.
However, in the real world, users access websites over networks and HTTPS
connections that introduce instability from multiple sources: client-side,
server-side issues or broader system failures. Moreover, live websites are
prone to web attacks such Cross-Site Scripting, as well as general site
modifications which can cause unexpected or malicious pop-ups or improper
functionality. To address this gap, we present WAREX: Web Agent Reliability
Evaluation on Existing Benchmarks. We measure the impact of WAREX across three
popular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that
introducing WAREX leads to significant drops in task success rates,
highlighting the limited robustness of state-of-the-art agents.

</details>


### [513] [Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints](https://arxiv.org/abs/2510.03377)
*Ahmed Missaoui,Cemalettin Ozturk,Barry O'Sullivan*

Main category: cs.AI

TL;DR: Proposes a multi-objective optimization framework for energy-efficient scheduling in a hybrid flow shop with blocking (BHFS), introducing a novel MIP model, an augmented epsilon-constraint method to obtain Pareto- optimal solutions, and a new metaheuristic RIPG for large instances; demonstrated effectiveness on small to large problem instances against two benchmark algorithms.


<details>
  <summary>Details</summary>
Motivation: Rising energy costs, fossil fuel scarcity, geopolitical supply risks, and climate concerns press manufacturing to cut energy use. Scheduling in manufacturing is a key lever for immediate energy savings, and the hybrid flow shop with blocking captures realistic production constraints across many industries.

Method: Formulates a novel multi-objective mixed-integer programming (MIP) model for BHFS targeting makespan and total energy consumption. Develops an augmented epsilon-constraint method to derive Pareto-optimal trade-offs. Introduces Refined Iterated Pareto Greedy (RIPG), a multi-objective metaheuristic, to solve large instances efficiently. Benchmarks against two well-known baseline algorithms on small, medium, and large instances.

Result: Computational experiments show the proposed approaches effectively uncover Pareto-optimal frontiers. RIPG and the augmented epsilon-constraint method yield high-quality solutions and scalable performance, providing favorable trade-offs between makespan and energy consumption across varying instance sizes compared to the baselines.

Conclusion: The study provides actionable methods for energy-aware scheduling in BHFS settings, delivering practical tools (MIP formulation, Pareto-based solution methods) that balance operation throughput and energy use, with demonstrated scalability to larger problem instances.

Abstract: The scarcity of non-renewable energy sources, geopolitical problems in its
supply, increasing prices, and the impact of climate change, force the global
economy to develop more energy-efficient solutions for their operations. The
Manufacturing sector is not excluded from this challenge as one of the largest
consumers of energy. Energy-efficient scheduling is a method that attracts
manufacturing companies to reduce their consumption as it can be quickly
deployed and can show impact immediately. In this study, the hybrid flow shop
scheduling problem with blocking constraint (BHFS) is investigated in which we
seek to minimize the latest completion time (i.e. makespan) and overall energy
consumption, a typical manufacturing setting across many industries from
automotive to pharmaceutical. Energy consumption and the latest completion time
of customer orders are usually conflicting objectives. Therefore, we first
formulate the problem as a novel multi-objective mixed integer programming
(MIP) model and propose an augmented epsilon-constraint method for finding the
Pareto-optimal solutions. Also, an effective multi-objective metaheuristic
algorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large
instances in reasonable time. Our proposed methods are benchmarked using small,
medium, and large-size instances to evaluate their efficiency. Two well-known
algorithms are adopted for comparing our novel approaches. The computational
results show the effectiveness of our method.

</details>


### [514] [Know Thyself? On the Incapability and Implications of AI Self-Recognition](https://arxiv.org/abs/2510.03399)
*Xiaoyan Bai,Aryan Shrivastava,Ari Holtzman,Chenhao Tan*

Main category: cs.AI

TL;DR: Systematic evaluation finds most modern LLMs fail to recognize their own generated text, while biased toward GPT/Claude; limited self-awareness with implications for safety.


<details>
  <summary>Details</summary>
Motivation: There is debate on whether models possess self-recognition; safety concerns require reliable, updateable evaluation methods; previous contradictory claims motivate a unified framework.

Method: Two tasks on 10 contemporary LLMs: binary self-recognition (self vs non-self) and exact model prediction; also assessed awareness of own/others' existence and the reasoning behind choices.

Result: Only 4 of 10 models correctly labeled themselves; overall performance near random; strong bias toward predicting GPT/Claude families; some awareness of own/others' existence but with hierarchical bias in their explanations.

Conclusion: Findings challenge claims of robust self-recognition; discuss implications for AI safety and outline directions to develop meaningful AI self-awareness and robust evaluation frameworks.

Abstract: Self-recognition is a crucial metacognitive capability for AI systems,
relevant not only for psychological analysis but also for safety, particularly
in evaluative scenarios. Motivated by contradictory interpretations of whether
models possess self-recognition (Panickssery et al., 2024; Davidson et al.,
2024), we introduce a systematic evaluation framework that can be easily
applied and updated. Specifically, we measure how well 10 contemporary larger
language models (LLMs) can identify their own generated text versus text from
other models through two tasks: binary self-recognition and exact model
prediction. Different from prior claims, our results reveal a consistent
failure in self-recognition. Only 4 out of 10 models predict themselves as
generators, and the performance is rarely above random chance. Additionally,
models exhibit a strong bias toward predicting GPT and Claude families. We also
provide the first evaluation of model awareness of their own and others'
existence, as well as the reasoning behind their choices in self-recognition.
We find that the model demonstrates some knowledge of its own existence and
other models, but their reasoning reveals a hierarchical bias. They appear to
assume that GPT, Claude, and occasionally Gemini are the top-tier models, often
associating high-quality text with them. We conclude by discussing the
implications of our findings on AI safety and future directions to develop
appropriate AI self-awareness.

</details>


### [515] [ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection](https://arxiv.org/abs/2510.03418)
*Ananya Mantravadi,Shivali Dalmia,Abhishek Mukherji,Nand Dave,Anudha Mittal*

Main category: cs.AI

TL;DR: ContraGen is a contradiction-aware benchmark framework for enterprise RAG, introducing synthetic documents with embedded contradictions, a taxonomy of enterprise-contrast types, automated contradiction mining with human-in-the-loop validation, and a retrieval evaluation pipeline to assess intra- and cross-document consistency.


<details>
  <summary>Details</summary>
Motivation: Current sentence-level contradiction benchmarks fail to capture the complexity of enterprise documents (e.g., contracts, filings, compliance reports, policies). In enterprise RAG, contradictions in retrieved evidence threaten trustworthiness, governance, and accountability.

Method: Generate realistic enterprise-style documents with embedded contradictions; model a taxonomy of contradiction types common in business processes; enable controlled creation of self- and pairwise contradictions; develop a contradiction-aware retrieval evaluation pipeline; combine automated contradiction mining with human-in-the-loop validation.

Result: Proposes ContraGen framework and evaluation pipeline for end-to-end assessment of contradiction handling in enterprise RAG, enabling systematic evaluation of intra- and cross-document consistency and providing a foundation for more trustworthy enterprise information-seeking systems.

Conclusion: ContraGen provides a practical foundation for detecting, reasoning about, and resolving contradictions in enterprise RAG, supporting compliance, governance, and accountability while reducing risk.

Abstract: Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,
offering advanced capabilities for information access and decision-making.
However, contradictions in retrieved evidence can result in inconsistent or
untrustworthy outputs, which is especially problematic in enterprise settings
where compliance, governance, and accountability are critical. Existing
benchmarks for contradiction detection are limited to sentence-level analysis
and do not capture the complexity of enterprise documents such as contracts,
financial filings, compliance reports, or policy manuals. To address this
limitation, we propose ContraGen, a contradiction-aware benchmark framework
tailored to enterprise domain. The framework generates synthetic
enterprise-style documents with embedded contradictions, enabling systematic
evaluation of both intra-document and cross-document consistency. Automated
contradiction mining is combined with human-in-the-loop validation to ensure
high accuracy. Our contributions include generating realistic enterprise
documents, modeling a taxonomy of contradiction types common in business
processes, enabling controlled creation of self- and pairwise contradictions,
developing a contradiction-aware retrieval evaluation pipeline and embedding
human oversight to reflect domain-specific judgment complexity. This work
establishes a foundation for more trustworthy and accountable RAG systems in
enterprise information-seeking applications, where detecting and resolving
contradictions is essential for reducing risk and ensuring compliance.

</details>


### [516] [A Qualitative Comparative Evaluation of Cognitive and Generative Theories](https://arxiv.org/abs/2510.03453)
*Paul S. Rosenbloom*

Main category: cs.AI

TL;DR: Broad, qualitative theory-evaluation framework comparing whole-mind cognitive architectures and generative neural architectures across full systems.


<details>
  <summary>Details</summary>
Motivation: Evaluation of theories in cognitive architectures and generative neural systems is challenging; a broad perspective is needed to enable meaningful, cross-family comparisons.

Method: Apply a wide-ranging, qualitative theory-evaluation approach to contrast whole-mind oriented cognitive architectures with generative neural architectures, evaluating the full systems.

Result: A qualitative cross-family comparison that highlights shared evaluation challenges and surface differences, providing a framework for future empirical and theoretical work.

Conclusion: A broad evaluative framework can address dual challenges of evaluating both cognitive and generative architectures, enabling integrated assessment across full systems.

Abstract: Evaluation is a critical activity associated with any theory. Yet this has
proven to be an exceptionally challenging activity for theories based on
cognitive architectures. For an overlapping set of reasons, evaluation can also
be challenging for theories based on generative neural architectures. This dual
challenge is approached here by leveraging a broad perspective on theory
evaluation to yield a wide-ranging, albeit qualitative, comparison of
whole-mind-oriented cognitive and generative architectures and the full systems
that are based on these architectures.

</details>


### [517] [Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification](https://arxiv.org/abs/2510.03469)
*Keshav Ramani,Vali Tawosi,Salwa Alamir,Daniel Borrajo*

Main category: cs.AI

TL;DR: A framework translating natural-language plans into Kripke structures and LTL via LLMs, verified by model checking; achieves high F1 on PlanBench; semantic perfection of the formal models remains a future goal.


<details>
  <summary>Details</summary>
Motivation: To ensure alignment between natural-language plans and their actual behavior by producing formal, checkable representations that can guarantee intended outcomes.

Method: Convert NL plans into Kripke structures and Linear Temporal Logic using Large Language Models, then apply model checking; evaluate on a simplified PlanBench plan verification dataset; report metrics (Accuracy, Precision, Recall, F1); GPT-5 used for classification and representation synthesis.

Result: Achieves a high F1 score of 96.3% (GPT-5); produces syntactically near-perfect formal representations that can serve as guarantees; however achieving semantically perfect formal models remains challenging and is left for future work.

Conclusion: The approach shows promise for aligning natural-language plans with verifiable formal models and potential guarantees, but further work is needed to ensure semantic correctness of the formal models and robust synthesis.

Abstract: We introduce a novel framework for evaluating the alignment between natural
language plans and their expected behavior by converting them into Kripke
structures and Linear Temporal Logic (LTL) using Large Language Models (LLMs)
and performing model checking. We systematically evaluate this framework on a
simplified version of the PlanBench plan verification dataset and report on
metrics like Accuracy, Precision, Recall and F1 scores. Our experiments
demonstrate that GPT-5 achieves excellent classification performance (F1 score
of 96.3%) while almost always producing syntactically perfect formal
representations that can act as guarantees. However, the synthesis of
semantically perfect formal models remains an area for future exploration.

</details>


### [518] [Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection](https://arxiv.org/abs/2510.03485)
*Xiaofei Wen,Wenjie Jacky Mo,Yanan Xie,Peng Qi,Muhao Chen*

Main category: cs.AI

TL;DR: A benchmark and guardrail model to detect policy violations in long-horizon web agent trajectories across domains, with a prefix-based task and a 4B model that generalizes well.


<details>
  <summary>Details</summary>
Motivation: To assess and curb policy violations by autonomous web agents, and to enable generalizable guardrails across domains and subdomains.

Method: Create PolicyGuardBench (~60k examples) with within- and cross-subdomain pairings; introduce prefix-based violation detection; train PolicyGuard-4B guardrail model for efficient, accurate detection.

Result: PolicyGuard-4B achieves strong detection accuracy across tasks and unseen domains; guardrails feasible at small-scale data; dataset provides framework to study policy compliance.

Conclusion: This work establishes a comprehensive framework for policy compliance in web agent trajectories and demonstrates effective, scalable guardrails.

Abstract: Autonomous web agents need to operate under externally imposed or
human-specified policies while generating long-horizon trajectories. However,
little work has examined whether these trajectories comply with such policies,
or whether policy violations persist across different contexts such as domains
(e.g., shopping or coding websites) and subdomains (e.g., product search and
order management in shopping). To address this gap, we introduce
PolicyGuardBench, a benchmark of about 60k examples for detecting policy
violations in agent trajectories. From diverse agent runs, we generate a broad
set of policies and create both within subdomain and cross subdomain pairings
with violation labels. In addition to full-trajectory evaluation,
PolicyGuardBench also includes a prefix-based violation detection task where
models must anticipate policy violations from truncated trajectory prefixes
rather than complete sequences. Using this dataset, we train PolicyGuard-4B, a
lightweight guardrail model that delivers strong detection accuracy across all
tasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes
across domains and preserves high accuracy on unseen settings. Together,
PolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework
for studying policy compliance in web agent trajectories, and show that
accurate and generalizable guardrails are feasible at small scales.

</details>


### [519] [OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows](https://arxiv.org/abs/2510.03506)
*John Nguyen,Marton Havasi,Tariq Berrada,Luke Zettlemoyer,Ricky T. Q. Chen*

Main category: cs.AI

TL;DR: OneFlow is the first non-autoregressive multimodal model enabling variable-length, concurrent text–image generation by combining insertion-based Edit Flow for text with Flow Matching for image latents, achieving strong performance with lower training compute.


<details>
  <summary>Details</summary>
Motivation: Autoregressive multimodal models enforce a strict causal order between text and image generation, limiting concurrency and increasing compute. A non-autoregressive, flexible approach could enable faster, concurrent generation with iterative refinement.

Method: Integrates insertion-based Edit Flow for discrete text tokens with Flow Matching for image latents. Employs hierarchical sampling that prioritizes content over grammar to support variable-length generation and concurrent synthesis. Evaluated across model sizes from 1B to 8B, with training efficiency claims.

Result: Outperforms autoregressive baselines on both generation and understanding tasks while using up to 50% fewer training FLOPs. Surpasses autoregressive and diffusion-based approaches, enabling concurrent generation, iterative refinement, and natural reasoning-like generation.

Conclusion: OneFlow unlocks new capabilities for concurrent multimodal generation and efficient training, with robustness across model scales and potential for iterative refinement and more natural, reasoning-like generation.

Abstract: We present OneFlow, the first non-autoregressive multimodal model that
enables variable-length and concurrent mixed-modal generation. Unlike
autoregressive models that enforce rigid causal ordering between text and image
generation, OneFlow combines an insertion-based Edit Flow for discrete text
tokens with Flow Matching for image latents. OneFlow enables concurrent
text-image synthesis with hierarchical sampling that prioritizes content over
grammar. Through controlled experiments across model sizes from 1B to 8B, we
demonstrate that OneFlow outperforms autoregressive baselines on both
generation and understanding tasks while using up to 50% fewer training FLOPs.
OneFlow surpasses both autoregressive and diffusion-based approaches while
unlocking new capabilities for concurrent generation, iterative refinement, and
natural reasoning-like generation.

</details>


### [520] [Understanding the Role of Training Data in Test-Time Scaling](https://arxiv.org/abs/2510.03605)
*Adel Javanmard,Baharan Mirzasoleiman,Vahab Mirrokni*

Main category: cs.AI

TL;DR: Test-time scaling enables longer chains-of-thoughts (CoTs) and improved reasoning in LLMs, but benefits depend on whether the training data contains the needed skills. Task hardness—captured by the smallest eigenvalue of the feature-covariance matrix—matters, and training on a diverse, relevant, and hard task set yields the best gains. Theoretical results are supported by experiments on large nonlinear transformers.


<details>
  <summary>Details</summary>
Motivation: Clarify when long CoTs emerge and when test-time compute helps, by linking training data properties and task hardness to the effectiveness of test-time scaling.

Method: Theoretical analysis of transformers trained on an in-context weight-prediction task for linear regression. Derives how test-time compute interacts with training-context length, introduces a hardness measure via the smallest eigenvalue of the feature covariance, and validates insights with experiments on large nonlinear transformer models.

Result: Key insights: (1) At fixed test error, increasing test-time compute allows reducing the in-training context length; (2) If the downstream skills are not sufficiently present in training data, more test-time compute can harm performance; (3) Training on a diverse, relevant, and hard set of tasks yields the best performance under test-time scaling. These claims are corroborated by experiments on large nonlinear transformers.

Conclusion: To maximize benefits from test-time scaling, curate a training distribution that is diverse, relevant, and sufficiently hard to cover downstream requirements. Align test-time compute with task hardness and training-data coverage to optimize CoT length and overall performance.

Abstract: Test-time scaling improves the reasoning capabilities of large language
models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts
(CoTs). This enables models to tackle more complex problem by breaking them
down into additional steps, backtracking, and correcting mistakes. Despite its
strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions
in the training data under which long CoTs emerge, and when such long CoTs
improve the performance, remain unclear. In this paper, we study the
performance of test-time scaling for transformers trained on an in-context
weight prediction task for linear regression. Our analysis provides a
theoretical explanation for several intriguing observations: First, at any
fixed test error, increasing test-time compute allows us to reduce the number
of in-context examples (context length) in training prompts. Second, if the
skills required to solve a downstream task are not sufficiently present in the
training data, increasing test-time compute can harm performance. Finally, we
characterize task hardness via the smallest eigenvalue of its feature
covariance matrix and show that training on a diverse, relevant, and hard set
of tasks results in best performance for test-time scaling. We confirm our
findings with experiments on large, nonlinear transformer architectures.

</details>


### [521] [Cross-Modal Content Optimization for Steering Web Agent Preferences](https://arxiv.org/abs/2510.03612)
*Tanqiu Jiang,Min Bai,Nikolaos Pappas,Yanjun Qi,Sandesh Swamy*

Main category: cs.AI

TL;DR: Cross-Modal Preference Steering (CPS) shows black-box attackers can jointly manipulate items’ visuals and text to steer VLM-driven selections, using transferable image perturbations and RLHF-induced linguistic biases, outperforming baselines while remaining stealthy.


<details>
  <summary>Details</summary>
Motivation: VLM-based web agents are increasingly used in high-stakes selection tasks and are vulnerable to adversarial manipulation. Existing work either assumes white-box access, limited perturbations, or impractical settings; there is a need to assess realistic cross-modal attacks under black-box constraints.

Method: Propose CPS, which jointly optimizes imperceptible changes to an item's visual content and natural language description. It leverages CLIP-transferable image perturbations and RLHF-induced linguistic biases to influence agent decisions, under a black-box threat where attackers edit only their own listing without model internals access. Evaluated on agents powered by GPT-4.1, Qwen-2.5VL, and Pixtral-Large across movie and e-commerce tasks, comparing to leading baselines.

Result: CPS consistently outperforms baselines across all models and tasks and achieves about 70% lower detection rates, demonstrating higher effectiveness and stealth in manipulating agent choices under realistic constraints.

Conclusion: There is a pressing need for robust defenses against cross-modal manipulation in VLM-based agents as they become more integral to society, given CPS’s demonstrated vulnerability and stealth under black-box conditions.

Abstract: Vision-language model (VLM)-based web agents increasingly power high-stakes
selection tasks like content recommendation or product ranking by combining
multimodal perception with preference reasoning. Recent studies reveal that
these agents are vulnerable against attackers who can bias selection outcomes
through preference manipulations using adversarial pop-ups, image
perturbations, or content tweaks. Existing work, however, either assumes strong
white-box access, with limited single-modal perturbations, or uses impractical
settings. In this paper, we demonstrate, for the first time, that joint
exploitation of visual and textual channels yields significantly more powerful
preference manipulations under realistic attacker capabilities. We introduce
Cross-Modal Preference Steering (CPS) that jointly optimizes imperceptible
modifications to an item's visual and natural language descriptions, exploiting
CLIP-transferable image perturbations and RLHF-induced linguistic biases to
steer agent decisions. In contrast to prior studies that assume gradient
access, or control over webpages, or agent memory, we adopt a realistic
black-box threat setup: a non-privileged adversary can edit only their own
listing's images and textual metadata, with no insight into the agent's model
internals. We evaluate CPS on agents powered by state-of-the-art proprietary
and open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both
movie selection and e-commerce tasks. Our results show that CPS is
significantly more effective than leading baseline methods. For instance, our
results show that CPS consistently outperforms baselines across all models
while maintaining 70% lower detection rates, demonstrating both effectiveness
and stealth. These findings highlight an urgent need for robust defenses as
agentic systems play an increasingly consequential role in society.

</details>


### [522] [MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information](https://arxiv.org/abs/2510.03632)
*Jiaxi Li,Yucheng Shi,Jin Lu,Ninghao Liu*

Main category: cs.AI

TL;DR: Mutual Information Tree Search (MITS) uses PMI-based scoring, entropy-driven sampling, and a PMI-enabled beam search to evaluate intermediate reasoning steps and expand search trees efficiently, achieving strong reasoning performance with lower cost.


<details>
  <summary>Details</summary>
Motivation: To provide instant, quantitative assessment of intermediate reasoning quality and reduce the computational burden of exhaustive path exploration in test-time LLM reasoning.

Method: Introduce a PMI-based scoring function for step-wise evaluation of reasoning paths; use beam search to expand the tree without expensive look-ahead simulations; apply an entropy-based dynamic sampling strategy to allocate computation to uncertain steps; employ a weighted voting scheme combining PMI scores with prediction consensus for final outputs.

Result: MITS consistently outperforms baseline methods on diverse reasoning benchmarks, achieving superior reasoning performance while maintaining computational efficiency.

Conclusion: MITS offers a principled and efficient framework for LLM reasoning that integrates information-theoretic principles into tree search for improved step evaluation and resource allocation.

Abstract: Tree search has become as a representative framework for test-time reasoning
with large language models (LLMs), exemplified by methods such as
Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning
paths. However, it remains difficult to provide instant and reliable
quantitative assessments of intermediate reasoning step quality, and extensive
path exploration is computationally costly. To address this, we propose Mutual
Information Tree Search (MITS), a novel framework that guides reasoning with
information-theoretic principles. MITS introduces an effective scoring function
based on pointwise mutual information (PMI), which enables step-wise evaluation
of reasoning paths and search tree expansion via beam search without expensive
look-ahead simulations, achieving superior reasoning performances while
maintaining computational efficiency. The framework is complemented by an
entropy-based dynamic sampling strategy that adaptively allocates computational
resources to uncertain reasoning steps where exploration is most beneficial.
For final prediction, MITS employs a weighted voting scheme that combines PMI
scores with prediction consensus. Through comprehensive experiments on diverse
reasoning benchmarks, MITS consistently surpasses baseline methods,
establishing a principled and efficient framework for LLM reasoning.

</details>


### [523] [Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs](https://arxiv.org/abs/2510.03680)
*Bumjun Kim,Dongjae Jeon,Dueun Kim,Wonje Jeung,Albert No*

Main category: cs.AI

TL;DR: Instruction-tuned diffusion LLMs suffer <eos> overflow: longer sequences paradoxically yield shorter outputs due to <eos> acting as both termination and padding. Rainbow Padding replaces repeated <eos> tokens with a cycle of distinct padding tokens, distributing probability mass and preventing early termination; simple to implement, with seven padding tokens often sufficient; LoRA fine-tuning enables practical integration; code released.


<details>
  <summary>Details</summary>
Motivation: Improve length robustness and output quality of diffusion LLMs by addressing the dual role of the end-of-sequence token as termination and padding, which concentrates probability mass on <eos> at longer positions and triggers early termination.

Method: Identify the root cause of <eos> overflow and propose Rainbow Padding: replace repeated <eos> placeholders with a repeating cycle of distinct padding tokens to distribute probability mass away from <eos>. Demonstrate with LoRA fine-tuning (one epoch on minimal data) and integrate into existing instruction-tuned models. Evaluate length robustness and output quality across longer generation horizons.

Result: Rainbow Padding substantially improves length robustness and output quality by breaking <eos> dominance and distributing probability mass. As few as seven padding tokens suffice to prevent early termination. The approach integrates efficiently via LoRA fine-tuning on minimal data, making it practical for real-world models. Code is publicly available.

Conclusion: Rainbow Padding is a simple, effective remedy for the <eos> overflow problem in instruction-tuned dLLMs. It enhances length robustness with minimal training and data requirements and can be readily adopted to improve generation quality in practice.

Abstract: Diffusion large language models (dLLMs) have emerged as a promising
alternative to autoregressive models, offering flexible generation orders and
strong performance on complex reasoning tasks. However, instruction-tuned dLLMs
exhibit a critical vulnerability we term \texttt{<eos>} overflow: as allocated
sequence length increases, responses paradoxically become shorter, collapsing
into early termination or degenerating into streams of \texttt{<eos>} tokens.
Although noticed in practice, this issue has not been systematically analyzed.
We trace its root cause to the dual role of \texttt{<eos>} as both termination
and padding, which concentrates probability mass on \texttt{<eos>} at later
positions and propagates backward to trigger early termination. To address
this, we introduce Rainbow Padding, a simple remedy that replaces repeated
\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens,
distributing probability mass and breaking \texttt{<eos>} dominance.
Experiments show that Rainbow Padding substantially improves length robustness
and output quality, with as few as seven padding tokens sufficient to prevent
early termination. Moreover, the method integrates efficiently into existing
instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data
yields significant improvements, making this solution highly practical. The
code is publicly available at https://github.com/quasar529/rainbow-padding.

</details>


### [524] [Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models](https://arxiv.org/abs/2510.03696)
*Deepak Babu Piskala,Sharlene Chen,Udita Patel,Parul Kalra,Rafael Castrillo*

Main category: cs.AI

TL;DR: Introduces Goal Success Rate (GSR) and Root Cause of Failure (RCOF) for goal-oriented evaluation of multi-agent chatbots; uses teacher LLMs guided by domain experts and 'thinking tokens' for interpretable reasoning; demonstrates improvement of GSR from 63% to 79% on an enterprise MAS (AIDA) over six months; framework is generic and actionable.


<details>
  <summary>Details</summary>
Motivation: Current evaluation mostly focuses on turn-level metrics and often fails to assess whether a user’s overarching goal is fulfilled. There is a need for goal-oriented evaluation that can diagnose failures and guide improvements in multi-agent chatbots.

Method: Segment conversations by user goals and evaluate success across all relevant turns (Goal Success Rate). Propose a model-based evaluator using teacher LLMs guided by domain experts to define goals and quality standards; incorporate 'thinking tokens' to produce interpretable rationales (explainable, data-efficient). Apply framework to an enterprise MAS (AIDA) and analyze defect taxonomy (RCOF) to diagnose failure modes and improvements.

Result: Empirical demonstration showing GSR improving from 63% to 79% over six months for AIDA. The framework yields actionable insights via a detailed defect taxonomy (RCOF) and supports diagnosis of success/failure modes to guide system enhancements.

Conclusion: A goal-centered evaluation framework for MAS provides interpretable, data-efficient assessments, enabling better diagnosis of failures and targeted system improvements in multi-agent chatbots.

Abstract: Evaluating the quality of multi-turn chatbot interactions remains
challenging, as most existing methods assess interactions at the turn level
without addressing whether a user's overarching goal was fulfilled. A ``goal''
here refers to an information need or task, such as asking for policy
information or applying for leave. We propose a comprehensive framework for
goal-oriented evaluation of multi-agent systems (MAS), introducing the
\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,
and a \textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for
failure in multi-agent chatbots. Our method segments conversations by user
goals and evaluates success using all relevant turns. We present a model-based
evaluation system combining teacher LLMs, where domain experts define goals,
set quality standards serving as a guidance for the LLMs. The LLMs use
``thinking tokens'' to produce interpretable rationales, enabling
\textit{explainable}, \textit{data-efficient} evaluations. In an enterprise
setting, we apply our framework to evaluate AIDA, a zero-to-one employee
conversational agent system built as a ground-up multi-agent conversational
agent, and observe GSR improvement from 63\% to 79\% over six months since its
inception. Our framework is generic and offers actionable insights through a
detailed defect taxonomy based on analysis of failure points in multi-agent
chatbots, diagnosing overall success, identifying key failure modes, and
informing system improvements.

</details>


### [525] [H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis](https://arxiv.org/abs/2510.03700)
*Seungseop Lim,Gibaeg Kim,Hyunkyung Lee,Wooseok Han,Jean Seo,Jaehyo Yoo,Eunho Yang*

Main category: cs.AI

TL;DR: Introduce H-DDx, a hierarchical evaluation framework for LLM-generated differential diagnoses. It maps free-text diagnoses to ICD-10 via retrieval/reranking and uses hierarchical metrics to better reflect clinical relevance, showing flat metrics underestimate performance; domain-specialized open-source models perform well; provides interpretable error patterns.


<details>
  <summary>Details</summary>
Motivation: Flat metrics (Top-k accuracy) fail to differentiate near-misses from distant errors in DDx evaluation; need clinically meaningful evaluation.

Method: Retrieval and reranking pipeline mapping free-text to ICD-10; apply hierarchical metric; benchmark 22 models; compare Open-source domain-specialized models.

Result: Flat metrics underestimate performance; hierarchical approach highlights clinically meaningful outputs; domain-specialized open-source models perform well; LLMs often get broader clinical context even when precise diagnosis is missed.

Conclusion: H-DDx provides a more clinically meaningful evaluation framework for DDx generation by LLMs and improves interpretability of errors.

Abstract: An accurate differential diagnosis (DDx) is essential for patient care,
shaping therapeutic decisions and influencing outcomes. Recently, Large
Language Models (LLMs) have emerged as promising tools to support this process
by generating a DDx list from patient narratives. However, existing evaluations
of LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy,
which fail to distinguish between clinically relevant near-misses and
diagnostically distant errors. To mitigate this limitation, we introduce H-DDx,
a hierarchical evaluation framework that better reflects clinical relevance.
H-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses
to ICD-10 codes and applies a hierarchical metric that credits predictions
closely related to the ground-truth diagnosis. In benchmarking 22 leading
models, we show that conventional flat metrics underestimate performance by
overlooking clinically meaningful outputs, with our results highlighting the
strengths of domain-specialized open-source models. Furthermore, our framework
enhances interpretability by revealing hierarchical error patterns,
demonstrating that LLMs often correctly identify the broader clinical context
even when the precise diagnosis is missed.

</details>


### [526] [Bridging the Gap Between Multimodal Foundation Models and World Models](https://arxiv.org/abs/2510.03727)
*Xuehai He*

Main category: cs.AI

TL;DR: Proposes to turn multimodal foundation models into world-model-like agents by (1) boosting reasoning with causal/counterfactual/spatiotemporal skills via discriminative tasks, and (2) developing structured/controllable image/video generation guided by scene graphs, multimodal conditioning, and alignment; extends to 4D controllable synthesis.


<details>
  <summary>Details</summary>
Motivation: MFMs are strong at perception and generation but lack world-model capabilities such as counterfactuals, dynamics simulation, spatiotemporal understanding, and fine-grained control; bridging to world models could enable more robust reasoning and interactive generation.

Method: Two-pronged: (i) discriminative tasks and structured reasoning to instill causal, counterfactual, and spatiotemporal reasoning into MFMs; (ii) generative frameworks for image and video with structured generation, scene graphs, multimodal conditioning, and alignment; extension to 4D controllable generation enabling interactive and editable object synthesis over time/space.

Result: No experimental results in the abstract; the contribution is a blueprint with proposed methods and frameworks.

Conclusion: If realized, this work could bridge MFMs and world models, enabling counterfactual reasoning, dynamics simulation, spatiotemporal reasoning, and controllable, 4D generation.

Abstract: Humans understand the world through the integration of multiple sensory
modalities, enabling them to perceive, reason about, and imagine dynamic
physical processes. Inspired by this capability, multimodal foundation models
(MFMs) have emerged as powerful tools for multimodal understanding and
generation. However, today's MFMs fall short of serving as effective world
models. They lack the essential ability such as perform counterfactual
reasoning, simulate dynamics, understand the spatiotemporal information,
control generated visual outcomes, and perform multifaceted reasoning. We
investigates what it takes to bridge the gap between multimodal foundation
models and world models. We begin by improving the reasoning capabilities of
MFMs through discriminative tasks and equipping MFMs with structured reasoning
skills, such as causal inference, counterfactual thinking, and spatiotemporal
reasoning, enabling them to go beyond surface correlations and understand
deeper relationships within visual and textual data. Next, we explore
generative capabilities of multimodal foundation models across both image and
video modalities, introducing new frameworks for structured and controllable
generation. Our approaches incorporate scene graphs, multimodal conditioning,
and multimodal alignment strategies to guide the generation process, ensuring
consistency with high-level semantics and fine-grained user intent. We further
extend these techniques to controllable 4D generation, enabling interactive,
editable, and morphable object synthesis over time and space.

</details>


### [527] [OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation](https://arxiv.org/abs/2510.03771)
*Divij Handa,David Blincoe,Orson Adams,Yinlin Fu*

Main category: cs.AI

TL;DR: OptAgent uses multi-agent LLM simulations as dynamic rewards, combined with a genetic algorithm, to optimize e-commerce query rewriting. It improves user queries by ~22% over the original and ~3.4% over a Best-of-N LLM baseline on 1000 real-world queries.


<details>
  <summary>Details</summary>
Motivation: Subjective tasks like e-commerce query rewriting lack a single gold-standard solution, making evaluation difficult. Static reward models and single LLM judges can be brittle; there's a need for robust, dynamic evaluation signals for aligning LLM-based systems with user intent.

Method: A framework that couples multi-agent LLM simulations (each agent as a simulated shopper) with a genetic algorithm. The agents provide a dynamic reward signal by averaging their scores, which serves as the fitness for an evolutionary search that iteratively refines the user's initial query.

Result: On a dataset of 1000 real-world e-commerce queries across five categories, OptAgent achieved an average improvement of 21.98% over the original user query and 3.36% over a Best-of-N LLM rewriting baseline.

Conclusion: Using a dynamic, agent-based reward signal together with evolutionary optimization can effectively verify and improve query rewriting for subjective tasks like QR, offering a viable approach when gold-standard benchmarks are unavailable and potentially generalizable to other subjectivity-driven NLP tasks.

Abstract: Deploying capable and user-aligned LLM-based systems necessitates reliable
evaluation. While LLMs excel in verifiable tasks like coding and mathematics,
where gold-standard solutions are available, adoption remains challenging for
subjective tasks that lack a single correct answer. E-commerce Query Rewriting
(QR) is one such problem where determining whether a rewritten query properly
captures the user intent is extremely difficult to figure out algorithmically.
In this work, we introduce OptAgent, a novel framework that combines
multi-agent simulations with genetic algorithms to verify and optimize queries
for QR. Instead of relying on a static reward model or a single LLM judge, our
approach uses multiple LLM-based agents, each acting as a simulated shopping
customer, as a dynamic reward signal. The average of these agent-derived scores
serves as an effective fitness function for an evolutionary algorithm that
iteratively refines the user's initial query. We evaluate OptAgent on a dataset
of 1000 real-world e-commerce queries in five different categories, and we
observe an average improvement of 21.98% over the original user query and 3.36%
over a Best-of-N LLM rewriting baseline.

</details>


### [528] [GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time](https://arxiv.org/abs/2510.03777)
*Divij Handa,Mihir Parmar,Aswin RRV,Md Nayem Uddin,Hamid Palangi,Chitta Baral*

Main category: cs.AI

TL;DR: GuidedSampling decouples exploration and generation during inference to boost diversity and performance, yielding substantial gains over Repeated Sampling (RS) and higher conceptual diversity.


<details>
  <summary>Details</summary>
Motivation: RS often generates redundant samples due to relying on a single inference approach; there is a need to systematically explore multiple concepts to improve solution diversity and downstream performance.

Method: GuidedSampling introduces a two-phase process: (1) exploration to identify multiple solvable concepts, and (2) generation to apply a chosen concept to produce final candidate solutions. The paper derives theoretical bounds and empirically validates improvements across benchmarks, with models trained on GuidedSampling trajectories enhancing performance.

Result: Across benchmarks, pass@50 improves by ~21.6% on average over RS; pass@5 improves by ~9.7% on average; the average number of concepts per instance increases from 1.67 to 3.03, indicating greater candidate diversity.

Conclusion: Decoupling exploration and generation in inference enhances diversity and performance; GuidedSampling not only boosts immediate accuracy metrics but also yields more diverse concept-driven candidates and better generalization when used to train models.

Abstract: Repeated Sampling (RS) is a simple inference-time algorithm that has been
shown to improve model performance on complex tasks. Although it is an
effective way of scaling inference time, it often struggles to generate diverse
solution candidates, frequently relying on the same underlying approach to
solve the problem and thus producing redundant samples. To address this
limitation, we propose a new inference algorithm, GuidedSampling, which
decouples the exploration and generation phases during inference, increasing
diversity of generated candidate solutions. The exploration phase identifies
multiple concepts that can be utilized to solve the problem, while the
generation phase applies a specific concept to provide final solution
candidates. We first define the theoretical bounds of GuidedSampling and then
empirically demonstrate that it improves the performance of base model at
pass@50 by on an average ~21.6% across various benchmarks compared to RS.
Furthermore, models trained on trajectories of GuidedSampling exhibit
substantial performance improvements at pass@5 by on an average ~9.7%, compared
to models trained on traditional RS. Additionally, models trained with
GuidedSampling increases the average number of concepts per instance (1.67 ->
3.03), yielding a diverse set of candidates than traditional RS.

</details>


### [529] [The Hidden Game Problem](https://arxiv.org/abs/2510.03845)
*Gon Buzaglo,Noah Golowich,Elad Hazan*

Main category: cs.AI

TL;DR: Introduces the hidden game problem with large strategy spaces; shows regret minimization can efficiently discover and exploit hidden subgames to reach equilibria.


<details>
  <summary>Details</summary>
Motivation: Motivated by AI alignment and language games; challenges of learning in large strategy spaces and leveraging hidden structure to improve efficiency and rationality.

Method: Develop a composition of regret minimization techniques to exploit the hidden structure; achieve optimal external and swap regret bounds; design algorithms that reveal hidden subgames and converge to equilibria.

Result: Establishes that efficient regret minimization algorithms exist for hidden games; achieves optimal external and swap regret bounds; rapid convergence to correlated equilibria in hidden subgames and improved computational efficiency.

Conclusion: Hidden subgame structure can be leveraged to enable efficient learning and rational behavior in large games; provides a framework for encountering hidden structure in AI alignment and language games, with faster convergence to equilibria.

Abstract: This paper investigates a class of games with large strategy spaces,
motivated by challenges in AI alignment and language games. We introduce the
hidden game problem, where for each player, an unknown subset of strategies
consistently yields higher rewards compared to the rest. The central question
is whether efficient regret minimization algorithms can be designed to discover
and exploit such hidden structures, leading to equilibrium in these subgames
while maintaining rationality in general. We answer this question affirmatively
by developing a composition of regret minimization techniques that achieve
optimal external and swap regret bounds. Our approach ensures rapid convergence
to correlated equilibria in hidden subgames, leveraging the hidden game
structure for improved computational efficiency.

</details>


### [530] [Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs](https://arxiv.org/abs/2510.03847)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: A practical blueprint for fast, inexpensive agents that favor small LMs (1–12B, up to ~20B) with guided decoding and verifier cascades, achieving tool-use and RAG performance approaching LLMs at far lower cost, latency, and energy, via routing and LLM fallback patterns.


<details>
  <summary>Details</summary>
Motivation: Small LMs are often sufficient for agentic, schema- and API-constrained tasks; there is a need for production-ready architectures, metrics, and design patterns that maximize cost-efficiency, latency, and reliability while preserving headroom with targeted LLM assistance.

Method: Synthesize evidence across open and proprietary SLMs (Phi-4-Mini, Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B, DeepSeek-R1-Distill); connect to modern evaluations (BFCL v3/v4, StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) and guided decoding libraries (XGrammar, Outlines); formalize SLM-default, LLM-fallback with uncertainty-aware routing and verifier cascades; propose engineering metrics (CPS, schema validity rate, executable call rate, p50/p95 latency, energy per request); advocate design patterns (schema-first prompting, type-safe function registries, confidence scoring with verifier rollups, LoRA/QLoRA) and delineate limits where fallback remains valuable (open-domain reasoning, long-horizon planning).

Result: Guided decoding, strict JSON schema outputs, and verifier cascades close the capability gap; SLMs can match or surpass LLMs on tool use, function calling, and RAG at 10×–100× lower token cost, with materially better latency and energy.

Conclusion: A practical blueprint for building fast, inexpensive, reliable agent stacks that default to SLMs while preserving headroom with targeted LLM assistance.

Abstract: Small language models (SLMs; 1-12B params, sometimes up to 20B) are
sufficient and often superior for agentic workloads where the objective is
schema- and API-constrained accuracy rather than open-ended generation. We
synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,
Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,
DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,
StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with
guided decoding libraries (XGrammar, Outlines). We formalize SLM-default,
LLM-fallback systems with uncertainty-aware routing and verifier cascades, and
propose engineering metrics that reflect real production goals: cost per
successful task (CPS), schema validity rate, executable call rate, p50/p95
latency, and energy per request. Guided decoding, strict JSON Schema outputs,
and validator-first tool execution close much of the capability gap with larger
models and often let SLMs match or surpass LLMs on tool use, function calling,
and RAG at 10x-100x lower token cost with materially better latency and energy.
We provide design patterns for agent stacks that prioritize SLMs: schema-first
prompting, type-safe function registries, confidence scoring with verifier
rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits
where fallback remains valuable (open-domain reasoning and some long-horizon
planning). The result is a practical blueprint for building fast, inexpensive,
and reliable agents that default to SLMs while preserving headroom with
targeted LLM assistance.
  Keywords: small language models, agents, function calling, structured
outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,
edge inference

</details>


### [531] [Algorithm Generation via Creative Ideation](https://arxiv.org/abs/2510.03851)
*Ruiying Ma,Chieh-Jan Mike Liang,Yanjie Gao,Francis Y. Yan*

Main category: cs.AI

TL;DR: MetaMuse enables creative, executable algorithm ideas by self-reflection; LLMs default to generic designs, but steering via diversity/utility in performance space, external stimuli, and waypoint-based reasoning yields strong gains in cache replacement and online bin packing.


<details>
  <summary>Details</summary>
Motivation: Algorithm design in systems is hampered by a discontinuous solution space and reliance on generic heuristics; LLMs alone struggle to navigate it.

Method: Introduce MetaMuse framework with three principles: 1) quantify solution diversity and usefulness in performance space; 2) steer ideation via external stimuli; 3) construct executable solutions using waypoint reasoning; evaluate on cache replacement and online bin packing at a global cloud provider.

Result: MetaMuse generates high-performing solutions; cache misses reduced by up to 35.76%; bin usage reduced by up to 30.93%.

Conclusion: Self-reflection-guided ideation can overcome LLM biases toward generic designs, improving practical algorithm design; external stimuli and waypoint reasoning help produce executable, effective solutions in real-world systems.

Abstract: Designing system algorithms remains challenging, where the discontinuous
nature of the solution space often forces system engineers to rely on generic
heuristics at the expense of performance. We study whether LLMs can practically
drive algorithm generation, and find that they are biased towards well-known
generic designs, rather than making the creative leaps needed to navigate the
discontinuous solution space. To address this limitation, we introduce
MetaMuse, a framework for creative ideation built on three self-reflection
principles: (1) quantifying solution diversity and usefulness in measurable
performance space, rather than abstract idea space, (2) steering ideation
through external stimuli, rather than internal randomness, and (3) constructing
executable solutions using waypoint reasoning, rather than free-form
chain-of-thought. Extensive evaluation shows that MetaMuse can generate
high-performing solutions for two critical problems at a global cloud provider:
cache replacement (reducing cache misses by up to 35.76%) and online bin
packing (reducing bin usage by up to 30.93%).

</details>


### [532] [Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning](https://arxiv.org/abs/2510.03859)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: LLM-assisted contextual reasoning with XAI enhances IoT anomaly detection, outperforming traditional models in accuracy and interpretability in smart grid and healthcare simulations.


<details>
  <summary>Details</summary>
Motivation: Traditional anomaly detection struggles in dynamic, high-dimensional IoT environments with incomplete or evolving data, necessitating adaptive, transparent AI that can be audited and aligned with policies.

Method: Integrate LLM-supported contextual reasoning with XAI agents. Employ attention-based pattern discovery, memory buffers with meaningful representations, and avoid fine-grained per-time-step processing. Compare against traditional models and evaluate accuracy, result fidelity, interpretability, and latency across smart grid and healthcare simulations.

Result: The proposed approach outperforms most existing models in both accuracy and interpretability, showing adaptability and reliability in the tested IoT contexts.

Conclusion: LLM-enhanced anomaly detection shows promise for future IoT deployments, offering improved detection quality, transparency, and policy compliance.

Abstract: Ensuring that critical IoT systems function safely and smoothly depends a lot
on finding anomalies quickly. As more complex systems, like smart healthcare,
energy grids and industrial automation, appear, it is easier to see the
shortcomings of older methods of detection. Monitoring failures usually happen
in dynamic, high dimensional situations, especially when data is incomplete,
messy or always evolving. Such limits point out the requirement for adaptive,
intelligent systems that always improve and think. LLMs are now capable of
significantly changing how context is understood and semantic inference is done
across all types of data. This proposal suggests using an LLM supported
contextual reasoning method along with XAI agents to improve how anomalies are
found in significant IoT environments. To discover hidden patterns and notice
inconsistencies in data streams, it uses attention methods, avoids dealing with
details from every time step and uses memory buffers with meaning. Because no
code AI stresses transparency and interpretability, people can check and accept
the AI's decisions, helping ensure AI follows company policies. The two
architectures are put together in a test that compares the results of the
traditional model with those of the suggested LLM enhanced model. Important
measures to check are the accuracy of detection, how much inaccurate
information is included in the results, how clearly the findings can be read
and how fast the system responds under different test situations. The
metaheuristic is tested in simulations of real world smart grid and healthcare
contexts to check its adaptability and reliability. From the study, we see that
the new approach performs much better than most existing models in both
accuracy and interpretation, so it could be a good fit for future anomaly
detection tasks in IoT

</details>


### [533] [Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation](https://arxiv.org/abs/2510.03863)
*Arina Kharlamova,Bowei He,Chen Ma,Xue Liu*

Main category: cs.AI

TL;DR: Spatial CAPTCHA introduces dynamic, geometry-based human verification to resist modern MLLMs, outperforming AI on spatial reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Conventional CAPTCHAs relying on text or 2D perception are increasingly vulnerable to advances in multi-modal LLMs; there is a need for tasks that leverage human-spatial reasoning where AI remains weak.

Method: Procedural generation of dynamic spatial reasoning tasks with constraint-based difficulty control, automated correctness verification, and human-in-the-loop validation. Evaluation uses Spatial-CAPTCHA-Bench and comparisons to Google reCAPTCHA.

Result: Humans vastly outperform 10 state-of-the-art MLLMs; the best model attains 31.0% Pass@1. Spatial CAPTCHA demonstrates strong security potential and doubles as a diagnostic tool for spatial reasoning in AI.

Conclusion: Spatial CAPTCHA offers a robust defense against AI-enabled automation and provides a useful framework for diagnosing spatial reasoning capabilities in AI, with scalable and adaptable pipeline and benchmarking.

Abstract: Online services rely on CAPTCHAs as a first line of defense against automated
abuse, yet recent advances in multi-modal large language models (MLLMs) have
eroded the effectiveness of conventional designs that focus on text recognition
or 2D image understanding. To address this challenge, we present Spatial
CAPTCHA, a novel human-verification framework that leverages fundamental
differences in spatial reasoning between humans and MLLMs. Unlike existing
CAPTCHAs which rely on low-level perception tasks that are vulnerable to modern
AI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning,
perspective-taking, occlusion handling, and mental rotation. These skills are
intuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The
system employs a procedural generation pipeline with constraint-based
difficulty control, automated correctness verification, and human-in-the-loop
validation to ensure scalability, robustness, and adaptability. Evaluation on a
corresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly
outperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0%
Pass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA,
which confirms its effectiveness as both a security mechanism and a diagnostic
tool for spatial reasoning in AI.

</details>


### [534] [Rare Text Semantics Were Always There in Your Diffusion Transformer](https://arxiv.org/abs/2510.03886)
*Seil Kang,Woojung Han,Dayun Ju,Seong Jae Hwang*

Main category: cs.AI

TL;DR: A training-free modulation—variance-scale up of text embedding priors before MM-DiT joint-attention—surfaces rare concepts and generalizes across text-to-image/video/editing.


<details>
  <summary>Details</summary>
Motivation: MM-DiTs struggle to render rare semantics from scarce concepts in prompts due to reliance on large pretraining data; users demand better fidelity for imaginative prompts without extra training or external models.

Method: Apply a variance-scale up to the text embedding representations, expanding their local basins before the joint-attention blocks in a Multi-modal Diffusion Transformer (MM-DiT). This enlarges the semantic basin around tokens, enabling rare concepts to surface during generation without additional training, data, or external modules.

Result: Rare semantics emerge more clearly in MM-DiT outputs; the approach generalizes across text-to-image, text-to-video, and text-driven image editing tasks, improving fidelity to imaginative prompts.

Conclusion: A simple, training-free intervention can reveal hidden user-intended semantics in MM-DiTs and likely other joint-attention models, broadening the range of concepts that generative systems can reliably render.

Abstract: Starting from flow- and diffusion-based transformers, Multi-modal Diffusion
Transformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim
for exceptional visual fidelity. As these models advance, users continually
push the boundary with imaginative or rare prompts, which advanced models still
falter in generating, since their concepts are often too scarce to leave a
strong imprint during pre-training. In this paper, we propose a simple yet
effective intervention that surfaces rare semantics inside MM-DiTs without
additional training steps, data, denoising-time optimization, or reliance on
external modules (e.g., large language models). In particular, the
joint-attention mechanism intrinsic to MM-DiT sequentially updates text
embeddings alongside image embeddings throughout transformer blocks. We find
that by mathematically expanding representational basins around text token
embeddings via variance scale-up before the joint-attention blocks, rare
semantics clearly emerge in MM-DiT's outputs. Furthermore, our results
generalize effectively across text-to-vision tasks, including text-to-image,
text-to-video, and text-driven image editing. Our work invites generative
models to reveal the semantics that users intend, once hidden yet ready to
surface.

</details>


### [535] [Kantian-Utilitarian XAI: Meta-Explained](https://arxiv.org/abs/2510.03892)
*Zahra Atf,Peter R. Lewis*

Main category: cs.AI

TL;DR: A gamified XAI system for ethically aware coffee choices using Kantian and utilitarian reasoning, with a meta-explainer and regret bound, plus reproducible artifacts and an audit-friendly UI.


<details>
  <summary>Details</summary>
Motivation: To support ethically informed consumer decision-making in a commodity domain by integrating normative reasoning, explainability, and auditability.

Method: Two symbolic engines compute Kantian rule-violations and a utilitarian multi-criteria scores; a meta-explainer with a regret bound guides alignment and switches to a deontically clean, near-parity option when welfare loss is small; outputs include a structured configuration (attribute schema, certification map, weights, rule set), a policy trace for auditability, and an interactive UI.

Result: Prototype/reproducible artifacts released: attribute schema, certification map, weights, rules, policy trace, and UI; no empirical evaluation details provided in the abstract.

Conclusion: The approach offers explainable ethical decision support by balancing rule-based and welfare-based criteria, with mechanisms to audit and adjust alignment; practical effectiveness depends on the quality of data, user interpretation, and robust audit tooling.

Abstract: We present a gamified explainable AI (XAI) system for ethically aware
consumer decision-making in the coffee domain. Each session comprises six
rounds with three options per round. Two symbolic engines provide real-time
reasons: a Kantian module flags rule violations (e.g., child labor,
deforestation risk without shade certification, opaque supply chains, unsafe
decaf), and a utilitarian module scores options via multi-criteria aggregation
over normalized attributes (price, carbon, water, transparency, farmer income
share, taste/freshness, packaging, convenience). A meta-explainer with a regret
bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a
deontically clean, near-parity option when welfare loss is small. We release a
structured configuration (attribute schema, certification map, weights, rule
set), a policy trace for auditability, and an interactive UI.

</details>


### [536] [Quantifying Risks in Multi-turn Conversation with Large Language Models](https://arxiv.org/abs/2510.03969)
*Chengxiao Wang,Isha Chaudhary,Qian Hu,Weitong Ruan,Rahul Gupta,Gagandeep Singh*

Main category: cs.AI

TL;DR: QRLLM offers a principled, statistically guaranteed certification framework to bound the probability of catastrophic responses in multi-turn LLM conversations. By modeling conversations as a Markov process on a query graph and using confidence intervals, it reveals substantial risks (up to 70% lower bounds) in frontier models across practical distributions.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods rely on fixed attack prompts, lack statistical guarantees, and do not scale to multi-turn conversations, leaving safety vulnerabilities under-identified. A scalable, probabilistic framework with guarantees is needed to systematically audit catastrophe risks in LLMs.

Method: Model multi-turn conversations as probability distributions over query sequences via a Markov process on a query graph. Edges encode semantic similarity to capture realistic conversational flow. Define inexpensive distributions (random node, graph path, adaptive with rejection) to estimate risk with confidence intervals and certify lower bounds on catastrophe probability.

Result: Demonstrates that frontier models harbor substantial catastrophic risks. Certified lower bounds reach as high as 70% for the worst model, indicating urgent need for improved safety training and deployment safeguards.

Conclusion: QRLLM provides a scalable, statistically grounded approach to certify catastrophe risk in multi-turn LLM conversations, highlighting the urgency of enhanced safety training and monitoring for frontier models.

Abstract: Large Language Models (LLMs) can produce catastrophic responses in
conversational settings that pose serious risks to public safety and security.
Existing evaluations often fail to fully reveal these vulnerabilities because
they rely on fixed attack prompt sequences, lack statistical guarantees, and do
not scale to the vast space of multi-turn conversations. In this work, we
propose QRLLM, a novel, principled Certification framework for Catastrophic
risks in multi-turn Conversation for LLMs that bounds the probability of an LLM
generating catastrophic responses under multi-turn conversation distributions
with statistical guarantees. We model multi-turn conversations as probability
distributions over query sequences, represented by a Markov process on a query
graph whose edges encode semantic similarity to capture realistic
conversational flow, and quantify catastrophic risks using confidence
intervals. We define several inexpensive and practical distributions: random
node, graph path, adaptive with rejection. Our results demonstrate that these
distributions can reveal substantial catastrophic risks in frontier models,
with certified lower bounds as high as 70\% for the worst model, highlighting
the urgent need for improved safety training strategies in frontier LLMs.

</details>


### [537] [What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models](https://arxiv.org/abs/2510.04009)
*Zicong He,Boxuan Zhang,Weihao Liu,Ruixiang Tang,Lu Cheng*

Main category: cs.AI

TL;DR: A theory-grounded, two-dimension benchmark (C^2-Eval) for creativity in foundation models, separating convergent and divergent tasks, using Usefulness, Originality, and Surprise; reveals trade-offs across models and provides a unified evaluation lens.


<details>
  <summary>Details</summary>
Motivation: Creativity is a key capability for machine intelligence, but existing evaluation frameworks are fragmented and rely on ad hoc metrics; there is a need for theory-grounded, comprehensive assessment of creativity in foundation models.

Method: Introduce C^2-Eval with two complementary forms of creativity: convergent (constrained solutions) and divergent (open-ended). Evaluation uses fine-grained criteria derived from social-science theory: Usefulness, Originality, and Surprise (U-O-S). Conduct extensive experiments on leading proprietary and open-source models to analyze trade-offs in creative capabilities.

Result: C^2-Eval yields insights into the strengths and challenges of current foundation models in pursuing creativity, and demonstrates that a unified, theory-grounded benchmark is effective for examining the evolving landscape of creative AI.

Conclusion: C^2-Eval provides a holistic, theory-grounded framework for evaluating creativity in foundation models, enabling fairer comparisons across models and guiding future research in creative AI.

Abstract: The meteoric rise of foundation models (FMs) has expanded their capabilities
far beyond conventional tasks. Creativity, long regarded as a hallmark of human
intelligence and a driver of innovation, is now increasingly recognized as a
critical dimension of machine intelligence in the era of generative FMs,
complementing traditional measures of accuracy. However, existing evaluation
frameworks for creativity remain fragmented, relying on ad hoc metrics not
firmly grounded in established theories. To address this gap, we introduce
C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.
C^2-Eval distinguishes between two complementary forms of creativity:
convergent creativity, where tasks admit constrained solutions (e.g., code
generation), and divergent creativity, where tasks are open-ended (e.g.,
storytelling). It evaluates both dimensions using fine-grained criteria derived
from social-science theory, focusing on Usefulness, Originality, and Surprise
(U-O-S). Through extensive experiments on leading proprietary and open-source
models, we analyze trade-offs in their creative capabilities. Our results
highlight both the strengths and challenges of current FMs in pursuing a
creative machine mind, showing that C^2-Eval is an effective lens for examining
the evolving landscape of creative AI.

</details>


### [538] [Zephyrus: An Agentic Framework for Weather Science](https://arxiv.org/abs/2510.04017)
*Sumanth Varambally,Marshall Fisher,Jas Thakker,Yiwei Chen,Zhirui Xia,Yasaman Jafari,Ruijia Niu,Manas Jain,Veeramakali Vignesh Manivannan,Zachary Novack,Luyu Han,Srikar Eranky,Salva Rühling Cachay,Taylor Berg-Kirkpatrick,Duncan Watson-Parris,Yi-An Ma,Rose Yu*

Main category: cs.AI

TL;DR: A novel agentic framework (Zephyrus) merges LLM reasoning with a Python-based weather data environment, improving accuracy over text-only baselines by up to 35 points on many tasks, though harder tasks remain challenging.


<details>
  <summary>Details</summary>
Motivation: Bridge language-based reasoning with data-intensive weather science to enable interactive, interpretable workflows and advanced data-driven forecasting via autonomous agents.

Method: Develop ZephyrusWorld: a Python environment with WeatherBench 2 interface, geoquerying for NL-based masks, forecasting and climate simulation tools; create Zephyrus, a multi-turn LLM-based weather agent that analyzes data, observes results, and iteratively refines through conversational feedback; build ZephyrusBench with scalable data generation to produce QA across simple lookups to complex forecasting and counterfactual reasoning; evaluate against text-only baselines.

Result: Zephyrus agents outperform text-only baselines by up to 35 percentage points in correctness on the benchmark; on harder tasks, performance aligns with baselines, indicating the benchmark's difficulty and room for improvement.

Conclusion: The framework demonstrates that integrating language-based reasoning with weather data workflows can yield substantial gains, but achieving robust performance on complex tasks requires further methodological advances and richer evaluation of interactive capabilities.

Abstract: Foundation models for weather science are pre-trained on vast amounts of
structured numerical data and outperform traditional weather forecasting
systems. However, these models lack language-based reasoning capabilities,
limiting their utility in interactive scientific workflows. Large language
models (LLMs) excel at understanding and generating text but cannot reason
about high-dimensional meteorological datasets. We bridge this gap by building
a novel agentic framework for weather science. Our framework includes a Python
code-based environment for agents (ZephyrusWorld) to interact with weather
data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying
for geographical masks from natural language, weather forecasting, and climate
simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather
agent that iteratively analyzes weather datasets, observes results, and refines
its approach through conversational feedback loops. We accompany the agent with
a new benchmark, ZephyrusBench, with a scalable data generation pipeline that
constructs diverse question-answer pairs across weather-related tasks, from
basic lookups to advanced forecasting, extreme event detection, and
counterfactual reasoning. Experiments on this benchmark demonstrate the strong
performance of Zephyrus agents over text-only baselines, outperforming them by
up to 35 percentage points in correctness. However, on harder tasks, Zephyrus
performs similarly to text-only baselines, highlighting the challenging nature
of our benchmark and suggesting promising directions for future work.

</details>


### [539] [LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2510.04023)
*Mizanur Rahman,Amran Bhuiyan,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Ridwan Mahbub,Ahmed Masry,Shafiq Joty,Enamul Hoque*

Main category: cs.AI

TL;DR: A lifecycle-aligned taxonomy and critical synthesis of 45 data science agents spanning the six-stage data science process, with cross-cutting design dimensions, revealing strengths, gaps, and open challenges; emphasizes need for better alignment, governance, and evaluation.


<details>
  <summary>Details</summary>
Motivation: To systematically understand and compare data science agents that automate workflows, enabling principled analysis, benchmarking, and guiding future research toward robust, trustworthy, and scalable agents.

Method: A comprehensive survey that maps forty-five existing data science agent systems onto the six stages of the data science lifecycle (business understanding/data acquisition; exploratory analysis/visualization; feature engineering; model building/selection; interpretation/explanation; deployment/monitoring) and annotates each agent along five design dimensions (reasoning/planning style, modality integration, tool orchestration depth, learning/alignment methods, trust/safety/governance). The authors synthesize capabilities, assess strengths/limitations per stage, review benchmarks, and extract cross-cutting trends across systems.

Result: A taxonomy and critical synthesis identifying dominant capabilities (EDA, visualization, modeling) and notable gaps (business understanding, deployment, monitoring), unresolved challenges in multimodal reasoning and tool orchestration, and a widespread lack of explicit trust/safety mechanisms (>90%). It also provides a set of open challenges and directions for future work.

Conclusion: The work highlights open research directions in alignment stability, explainability, governance, and evaluation frameworks, and argues for robust, low-latency, transparent, and broadly accessible data science agents with improved trust and safety—a roadmap for the field.

Abstract: Recent advances in large language models (LLMs) have enabled a new class of
AI agents that automate multiple stages of the data science workflow by
integrating planning, tool use, and multimodal reasoning across text, code,
tables, and visuals. This survey presents the first comprehensive,
lifecycle-aligned taxonomy of data science agents, systematically analyzing and
mapping forty-five systems onto the six stages of the end-to-end data science
process: business understanding and data acquisition, exploratory analysis and
visualization, feature engineering, model building and selection,
interpretation and explanation, and deployment and monitoring. In addition to
lifecycle coverage, we annotate each agent along five cross-cutting design
dimensions: reasoning and planning style, modality integration, tool
orchestration depth, learning and alignment methods, and trust, safety, and
governance mechanisms. Beyond classification, we provide a critical synthesis
of agent capabilities, highlight strengths and limitations at each stage, and
review emerging benchmarks and evaluation practices. Our analysis identifies
three key trends: most systems emphasize exploratory analysis, visualization,
and modeling while neglecting business understanding, deployment, and
monitoring; multimodal reasoning and tool orchestration remain unresolved
challenges; and over 90% lack explicit trust and safety mechanisms. We conclude
by outlining open challenges in alignment stability, explainability,
governance, and robust evaluation frameworks, and propose future research
directions to guide the development of robust, trustworthy, low-latency,
transparent, and broadly accessible data science agents.

</details>


### [540] [A global log for medical AI](https://arxiv.org/abs/2510.04033)
*Ayush Noori,Adam Rodman,Alan Karthikesalingam,Bilal A. Mateen,Christopher A. Longhurst,Daniel Yang,Dave deBronkart,Gauden Galea,Harold F. Wolf III,Jacob Waxman,Joshua C. Mandel,Juliana Rotich,Kenneth D. Mandl,Maryam Mustafa,Melissa Miles,Nigam H. Shah,Peter Lee,Robert Korom,Scott Mahoney,Seth Hain,Tien Yin Wong,Trevor Mundel,Vivek Natarajan,Noa Dagan,David A. Clifton,Ran D. Balicer,Isaac S. Kohane,Marinka Zitnik*

Main category: cs.AI

TL;DR: MedLog proposes a syslog-like protocol for event-level logging of clinical AI, enabling structured, auditable records across model interactions and human interfaces (nine core fields).


<details>
  <summary>Details</summary>
Motivation: Healthcare AI lacks a standardized, transparent logging mechanism to track usage, performance, safety, and bias in real-world settings, hindering auditing and improvement.

Method: Introduce MedLog, a protocol comprising header, model, user, target, inputs, artifacts, outputs, outcomes, and feedback fields; supports risk-based sampling, lifecycle-aware retention, and write-behind caching; accommodates complex multi-stage workflows; aims to catalyze databases and tooling for analysis.

Result: Conceptual proposal establishing a framework for continuous surveillance, auditing, and iterative improvement of medical AI; potential to enable digital epidemiology; no empirical evaluation reported in abstract.

Conclusion: MedLog could become foundational for transparency and governance of medical AI, driving new data infrastructure and standards; adoption would require standardization, governance, privacy safeguards, and practical deployment strategies.

Abstract: Modern computer systems often rely on syslog, a simple, universal protocol
that records every critical event across heterogeneous infrastructure. However,
healthcare's rapidly growing clinical AI stack has no equivalent. As hospitals
rush to pilot large language models and other AI-based clinical decision
support tools, we still lack a standard way to record how, when, by whom, and
for whom these AI models are used. Without that transparency and visibility, it
is challenging to measure real-world performance and outcomes, detect adverse
events, or correct bias or dataset drift. In the spirit of syslog, we introduce
MedLog, a protocol for event-level logging of clinical AI. Any time an AI model
is invoked to interact with a human, interface with another algorithm, or act
independently, a MedLog record is created. This record consists of nine core
fields: header, model, user, target, inputs, artifacts, outputs, outcomes, and
feedback, providing a structured and consistent record of model activity. To
encourage early adoption, especially in low-resource settings, and minimize the
data footprint, MedLog supports risk-based sampling, lifecycle-aware retention
policies, and write-behind caching; detailed traces for complex, agentic, or
multi-stage workflows can also be captured under MedLog. MedLog can catalyze
the development of new databases and software to store and analyze MedLog
records. Realizing this vision would enable continuous surveillance, auditing,
and iterative improvement of medical AI, laying the foundation for a new form
of digital epidemiology.

</details>


### [541] [FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.04040)
*Xu Shen,Song Wang,Zhen Tan,Laura Yao,Xinyu Zhao,Kaidi Xu,Xin Wang,Tianlong Chen*

Main category: cs.AI

TL;DR: FaithCoT-Bench introduces FINE-CoT, a large expert-annotated dataset and benchmark for instance-level CoT faithfulness. It collects ~1k reasoning trajectories from four LLMs across four domains, including 300 unfaithful cases with evidence, and evaluates 11 detection methods (counterfactual, logit-based, LLM-as-judge). The study reveals strengths/weaknesses of current approaches and greater detection difficulty in knowledge-intensive domains and with stronger models, establishing a first comprehensive benchmark for instance-level CoT faithfulness.


<details>
  <summary>Details</summary>
Motivation: To address the gap between broad, mechanism-level analysis of chain-of-thought faithfulness and practical, instance-level assessment. Reliable, faithful reasoning in LLMs is crucial for high-risk/high-stakes applications, yet there is no standardized benchmark to detect unfaithfulness at the trajectory level.

Method: Define a discriminative task of unfaithfulness detection. Build FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an expert-annotated collection of >1,000 reasoning trajectories from four representative LLMs across four domains, including >300 unfaithful instances with fine-grained causes and step-level evidence. Systematically evaluate 11 detection methods spanning counterfactual, logit-based, and LLM-as-judge paradigms.

Result: The study provides empirical insights into the strengths and weaknesses of existing unfaithfulness detection approaches, showing increased difficulty in knowledge-intensive domains and with more advanced models. It delivers the first comprehensive benchmark for instance-level CoT faithfulness.

Conclusion: FaithCoT-Bench establishes a solid basis for future research toward more interpretable and trustworthy reasoning in LLMs by offering a rigorous task formulation, a rich annotated dataset, and a broad evaluation of detection strategies.

Abstract: Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT)
prompting to improve problem-solving and provide seemingly transparent
explanations. However, growing evidence shows that CoT often fail to faithfully
represent the underlying reasoning process, raising concerns about their
reliability in high-risk applications. Although prior studies have focused on
mechanism-level analyses showing that CoTs can be unfaithful, they leave open
the practical challenge of deciding whether a specific trajectory is faithful
to the internal reasoning of the model. To address this gap, we introduce
FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness
detection. Our framework establishes a rigorous task formulation that
formulates unfaithfulness detection as a discriminative decision problem, and
provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an
expert-annotated collection of over 1,000 trajectories generated by four
representative LLMs across four domains, including more than 300 unfaithful
instances with fine-grained causes and step-level evidence. We further conduct
a systematic evaluation of eleven representative detection methods spanning
counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical
insights that clarify the strengths and weaknesses of existing approaches and
reveal the increased challenges of detection in knowledge-intensive domains and
with more advanced models. To the best of our knowledge, FaithCoT-Bench
establishes the first comprehensive benchmark for instance-level CoT
faithfulness, setting a solid basis for future research toward more
interpretable and trustworthy reasoning in LLMs.

</details>


### [542] [Increasing LLM response trustworthiness using voting ensembles](https://arxiv.org/abs/2510.04048)
*Aparna Nair-Kanneganti,Trevor J. Chan,Shir Goldfinger,Emily Mackay,Brian Anthony,Alison Pouch*

Main category: cs.AI

TL;DR: Proposes abstaining-enabled voting ensembles to boost trustworthiness of LLM answers, trading some coverage for higher reliability; supported by a theoretical framework and experiments in arithmetic and clinical QA.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle to quantify uncertainty, making high-stakes use risky. Ensembling is a simple approach, and introducing abstention thresholds can further improve trustworthiness.

Method: Develop a theoretical QA framework with variable voting thresholds that allow ensembles to abstain when dominant responses fall below the threshold; derive theoretical results and validate experimentally on arithmetic problem solving and clinical-note QA.

Result: Strictly-restrictive voting ensembles (high abstention thresholds) substantially improve answer trustworthiness with modest drops in yield and accuracy; gains observed in both domains.

Conclusion: Voting ensembles with abstention are a promising strategy for high-certainty applications (e.g., healthcare, data annotation) where not every question must be answered automatically.

Abstract: Despite huge advances, LLMs still lack convenient and reliable methods to
quantify the uncertainty in their responses, making them difficult to trust in
high-stakes applications. One of the simplest approaches to eliciting more
accurate answers is to select the mode of many responses, a technique known as
ensembling. In this work, we expand on typical ensembling approaches by looking
at ensembles with a variable voting threshold. We introduce a theoretical
framework for question answering and show that, by permitting ensembles to
"abstain" from providing an answer when the dominant response falls short of
the threshold, it is possible to dramatically increase the trustworthiness of
the remaining answers. From this framework, we derive theoretical results as
well as report experimental results on two problem domains: arithmetic problem
solving and clinical-note question-answering. In both domains, we observe that
large gains in answer trustworthiness can be achieved using highly restrictive
voting ensembles, while incurring relatively modest reductions in response
yield and accuracy. Due to this quality, voting ensembles may be particularly
useful in applications - such as healthcare and data annotation - that require
a high degree of certainty but which may not require that every question
receive an automated answer.

</details>


### [543] [Toward a unified framework for data-efficient evaluation of large language models](https://arxiv.org/abs/2510.04051)
*Lele Liao,Qile Zhang,Ruofan Wu,Guanhua Fang*

Main category: cs.AI

TL;DR: LEGO-IRT is a flexible, data-efficient IRT-based framework for evaluating LLMs that handles both binary and continuous scores and captures cross-metric/benchmark structure via a factorized model, achieving accurate estimates with only 3% of evaluation items and reducing estimation error up to 10%.


<details>
  <summary>Details</summary>
Motivation: Current IRT-based evaluations for LLMs rely on binary metrics and single benchmarks, which limits data efficiency and ignores valuable structural correlations across metrics and benchmarks. There is a need for a unified framework that can handle continuous scores and exploit structural information to improve evaluation accuracy.

Method: LEGO-IRT introduces a factorized architecture that separates general model ability from structure-specific components (per-metric or per-benchmark). It supports both binary and continuous evaluation metrics, enabling data-efficient estimation across multiple benchmarks. Extensive experiments involve 70 LLMs evaluated on 5 benchmarks.

Result: The framework achieves stable capability estimates using only 3% of the total evaluation items. Incorporating structural knowledge reduces estimation error by up to 10%. Latent abilities estimated by LEGO-IRT may align more closely with human preferences.

Conclusion: LEGO-IRT offers a unified, flexible, and data-efficient approach to LLM evaluation, leveraging structural knowledge across metrics and benchmarks to improve estimation accuracy and alignment with human judgments.

Abstract: Evaluating large language models (LLMs) on comprehensive benchmarks is a
cornerstone of their development, yet it's often computationally and
financially prohibitive. While Item Response Theory (IRT) offers a promising
path toward data-efficient evaluation by disentangling model capability from
item difficulty, existing IRT-based methods are hampered by significant
limitations. They are typically restricted to binary correctness metrics,
failing to natively handle the continuous scores used in generative tasks, and
they operate on single benchmarks, ignoring valuable structural knowledge like
correlations across different metrics or benchmarks. To overcome these
challenges, we introduce LEGO-IRT, a unified and flexible framework for
data-efficient LLM evaluation. LEGO-IRT's novel design natively supports both
binary and continuous evaluation metrics. Moreover, it introduces a factorized
architecture to explicitly model and leverage structural knowledge, decomposing
model ability estimates into a general component and structure-specific (e.g.,
per-metric or per-benchmark) components. Through extensive experiments
involving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves
stable capability estimates using just $3\%$ of the total evaluation items. We
demonstrate that incorporating structural knowledge reduces estimation error by
up to $10\%$ and reveal that the latent abilities estimated by our framework
may align more closely with human preferences.

</details>


### [544] [Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion](https://arxiv.org/abs/2510.04064)
*Jingxiang Zhang,Lujia Zhong*

Main category: cs.AI

TL;DR: LLMs encode a robust, mid-network emotion geometry that scales with model size and can be probed without fine-tuning; emotion signals persist across hundreds of tokens.


<details>
  <summary>Details</summary>
Motivation: To uncover where, how, and how long emotion is represented inside modern LLMs, addressing the gap in understanding their internal emotional mechanisms for transparency and alignment.

Method: Create a large Reddit-derived dataset (~400k utterances) balanced across seven basic emotions using classification, rewriting, and synthetic generation. Use lightweight probes to read emotion information from hidden layers of Qwen3 and LLaMA models without altering parameters.

Result: Emotional representations emerge early in the network and peak mid-network; their signal strengthens with model scale and surpasses zero-shot prompting. The emotional states are malleable via system prompts and persist across hundreds of tokens. A well-defined internal emotion geometry is revealed, and the work provides an open dataset and probing toolkit.

Conclusion: The study maps the emotional landscape inside LLMs, highlighting implications for transparency and alignment, and offering resources (data and code) to facilitate future research and safer, more controllable AI systems.

Abstract: Large Language Models (LLMs) are increasingly expected to navigate the
nuances of human emotion. While research confirms that LLMs can simulate
emotional intelligence, their internal emotional mechanisms remain largely
unexplored. This paper investigates the latent emotional representations within
modern LLMs by asking: how, where, and for how long is emotion encoded in their
neural architecture? To address this, we introduce a novel, large-scale Reddit
corpus of approximately 400,000 utterances, balanced across seven basic
emotions through a multi-stage process of classification, rewriting, and
synthetic generation. Using this dataset, we employ lightweight "probes" to
read out information from the hidden layers of various Qwen3 and LLaMA models
without altering their parameters. Our findings reveal that LLMs develop a
surprisingly well-defined internal geometry of emotion, which sharpens with
model scale and significantly outperforms zero-shot prompting. We demonstrate
that this emotional signal is not a final-layer phenomenon but emerges early
and peaks mid-network. Furthermore, the internal states are both malleable
(they can be influenced by simple system prompts) and persistent, as the
initial emotional tone remains detectable for hundreds of subsequent tokens. We
contribute our dataset, an open-source probing toolkit, and a detailed map of
the emotional landscape within LLMs, offering crucial insights for developing
more transparent and aligned AI systems. The code and dataset are open-sourced.

</details>


### [545] [Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention](https://arxiv.org/abs/2510.04073)
*Santhosh Kumar Ravindran*

Main category: cs.AI

TL;DR: MAS is a real-time, probabilistic-drift-detection and governance framework to curb AI value drift, combining Bayesian monitoring, LSTM forecasting, and human oversight, with claims of ~80% drift reduction and strong accuracy at low latency; open-source.


<details>
  <summary>Details</summary>
Motivation: Value drift threatens alignment as AI systems operate in dynamic contexts and pursue optimizing objectives. There is a need for proactive, low-latency detection, robust forecasting, and governance to prevent ethical breaches and inefficiencies, beyond static alignment methods.

Method: Integrates real-time Bayesian inference to monitor value states, LSTM networks to forecast drift, and a human-centric governance layer for adaptive interventions. Aims for sub-20 ms latency, uses supervised fine-tuning with human feedback to reduce false positives, and evaluates via simulations with goal-misaligned agents.

Result: Claimed performance includes ~80% reduction in value-drift incidents in simulations, with detection accuracy around 85% and a post-adaptation false positive rate of 0.08. Demonstrated scalability and responsiveness across experiments.

Conclusion: MAS offers a predictive and adaptive alternative to static alignment, with architecture and open-source implementation enabling cross-domain deployment, faster interventions, and reduced false alerts.

Abstract: The rise of artificial intelligence (AI) as super-capable assistants has
transformed productivity and decision-making across domains. Yet, this
integration raises critical concerns about value alignment - ensuring AI
behaviors remain consistent with human ethics and intentions. A key risk is
value drift, where AI systems deviate from aligned values due to evolving
contexts, learning dynamics, or unintended optimizations, potentially leading
to inefficiencies or ethical breaches. We propose the Moral Anchor System
(MAS), a novel framework to detect, predict, and mitigate value drift in AI
agents. MAS combines real-time Bayesian inference for monitoring value states,
LSTM networks for forecasting drift, and a human-centric governance layer for
adaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent
breaches, while reducing false positives and alert fatigue via supervised
fine-tuning with human feedback. Our hypothesis: integrating probabilistic
drift detection, predictive analytics, and adaptive governance can reduce value
drift incidents by 80 percent or more in simulations, maintaining high
detection accuracy (85 percent) and low false positive rates (0.08
post-adaptation). Rigorous experiments with goal-misaligned agents validate
MAS's scalability and responsiveness. MAS's originality lies in its predictive
and adaptive nature, contrasting static alignment methods. Contributions
include: (1) MAS architecture for AI integration; (2) empirical results
prioritizing speed and usability; (3) cross-domain applicability insights; and
(4) open-source code for replication.

</details>


### [546] [SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows](https://arxiv.org/abs/2510.04089)
*Yitong Cui,Liu Liu,Baosheng Yu,Jiayan Qiu,Xikai Zhang,Likang Xiao,Yixing Liu,Quan Chen*

Main category: cs.AI

TL;DR: A score-based preference learning framework, SPOGW, optimizes agentic workflows from cardinal rewards in continuous space, using group-wise comparisons and ioGRPO with advantage-masked KL, achieving state-of-the-art results on five benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current approaches to building agentic workflows rely on discrete optimization and pairwise comparisons, requiring substantial manual effort and limiting scalability and generalizability. There is a need for automated, scalable methods that can leverage continuous optimization and direct cardinal feedback.

Method: SPOGW operates on cardinal reward signals with group-wise (not pairwise) comparisons in a continuous space. It integrates iterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL) to regulate training updates and emphasize advantageous regions of the policy response.

Result: On five benchmark datasets spanning mathematical reasoning, coding, and question answering, SPOGW matches or exceeds the performance of current state-of-the-art approaches.

Conclusion: SPOGW provides a viable, scalable framework for automated generation and optimization of agentic workflows, addressing limitations of discrete optimization and manual intervention and offering improved adaptability and generalizability.

Abstract: Large language models (LLMs) have exhibited significant capabilities in
addressing challenging problems throughout various fields, often through the
use of agentic workflows that adhere to structured instructions and multi-step
procedures. However, designing such workflows demands substantial manual
effort, posing challenges to scalability and generalizability. Recent studies
have aimed to minimize the human intervention needed for their construction,
leading to advances in automated techniques for optimizing agentic workflows.
However, current approaches are often constrained by their limited
representational capacity, insufficient adaptability, weak scalability, and
pairwise comparison paradigm -- issues that stem primarily from a dependence on
discrete optimization techniques. To overcome these limitations, we introduce a
new score-based preference approach, refereed as SPOGW, which operates directly
on cardinal reward signals through group-wise comparison and enables more
efficient and stable optimization in a continuous space. SPOGW incorporates
Iterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL),
which regulates training update by placing greater emphasis on the advantageous
regions of the policy response. In five benchmark datasets covering
mathematical reasoning, coding, and question answering, SPOGW matches or
exceeds the performance of current state-of-the-art approaches, presenting a
viable and forward-looking methodology for automated generation and
optimization of agentic workflows.

</details>


### [547] [Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems](https://arxiv.org/abs/2510.04093)
*Guixian Zhang,Guan Yuan,Ziqi Xu,Yanmei Zhang,Zhenyun Deng,Debo Cheng*

Main category: cs.AI

TL;DR: DLLM uses diffusion-based denoising and graph-guided, semantic-aligned subgraphs to achieve noise-robust cognitive diagnosis in a web-based education system, outperforming baselines on noisy data.


<details>
  <summary>Details</summary>
Motivation: WIES presents open, dynamic environments with heterogeneous, noisy response logs and data imbalance; LLM-based cognitive diagnosis struggles with structured data and noise, necessitating a robust framework.

Method: DLLM constructs independent subgraphs from response correctness, uses a relation augmentation alignment module to mitigate data imbalance, fuses subgraph representations with LLM-derived semantic representations, and applies a two-stage denoising diffusion process (unconditional noise removal followed by graph-guided conditional denoising) before alignment; the resulting noise-robust representation is fed into existing cognitive diagnosis models.

Result: On three publicly available web-based educational platform datasets, DLLM achieves optimal predictive performance across varying noise levels, demonstrating both noise robustness and effective leveraging of semantic knowledge from LLM.

Conclusion: DLLM demonstrates a robust diffusion-based approach that combines structural graph representations with semantic LLM information to enhance cognitive diagnosis in noisy, open educational environments.

Abstract: Cognitive diagnostics in the Web-based Intelligent Education System (WIES)
aims to assess students' mastery of knowledge concepts from heterogeneous,
noisy interactions. Recent work has tried to utilize Large Language Models
(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are
prone to noise-induced misjudgments. Specially, WIES's open environment
continuously attracts new students and produces vast amounts of response logs,
exacerbating the data imbalance and noise issues inherent in traditional
educational systems. To address these challenges, we propose DLLM, a
Diffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first
constructs independent subgraphs based on response correctness, then applies
relation augmentation alignment module to mitigate data imbalance. The two
subgraph representations are then fused and aligned with LLM-derived,
semantically augmented representations. Importantly, before each alignment
step, DLLM employs a two-stage denoising diffusion module to eliminate
intrinsic noise while assisting structural representation alignment.
Specifically, unconditional denoising diffusion first removes erroneous
information, followed by conditional denoising diffusion based on graph-guided
to eliminate misleading information. Finally, the noise-robust representation
that integrates semantic knowledge and structural information is fed into
existing cognitive diagnosis models for prediction. Experimental results on
three publicly available web-based educational platform datasets demonstrate
that our DLLM achieves optimal predictive performance across varying noise
levels, which demonstrates that DLLM achieves noise robustness while
effectively leveraging semantic knowledge from LLM.

</details>


### [548] [WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning](https://arxiv.org/abs/2510.04097)
*Peichao Lai,Jinhui Zhuang,Kexuan Zhang,Ningchang Xiong,Shengjie Wang,Yanwei Xu,Chong Chen,Yilei Wang,Bin Cui*

Main category: cs.AI

TL;DR: WebRenderBench provides a large-scale UI-to-code benchmark with 22.5k real webpages, a novel final-rendered layout/style consistency metric, and ALISA—a reinforcement learning agent that uses this metric as a reward—to boost UI-to-code generation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing UI-to-code benchmarks lack data diversity and reliable evaluation. A robust, objective metric for final UI quality and a training framework leveraging it are needed to advance WebUI-to-Code systems.

Method: Construct WebRenderBench from 22.5k real webpages; define a final-rendered layout/style consistency metric; integrate this metric into reinforcement learning via the Automated Layout and Style Inspection Agent (ALISA) to train on crawled asymmetric webpages.

Result: ALISA substantially improves generation performance and achieves state-of-the-art results on multiple evaluation metrics.

Conclusion: The approach offers a more reliable UI quality assessment, stronger benchmarking capabilities, and improved UI-to-code generation, enabled by the new dataset, metric, and ALISA-based training framework.

Abstract: Automating the conversion of UI images into web code is a critical task for
front-end development and rapid prototyping. Advances in multimodal large
language models (MLLMs) have made WebUI-to-Code increasingly feasible, yet
existing benchmarks remain limited in data diversity and evaluation
reliability. To address these issues, we present WebRenderBench, a large-scale
benchmark of 22.5k webpages collected from real-world portal sites, offering
greater diversity, complexity, and realism than prior benchmarks. We further
propose a novel evaluation metric that measures layout and style consistency
from the final rendered pages. Unlike vision-based methods that rely on costly
LLM reasoning or structure-based comparisons vulnerable to noise and asymmetry,
our approach enables more efficient, objective, and reliable UI quality
assessment. Finally, we introduce the Automated Layout and Style Inspection
Agent (ALISA), which integrates this metric into reinforcement learning as a
reward signal to enhance training on crawled asymmetric webpages. Experiments
show that ALISA significantly boosts generation performance, achieving
state-of-the-art results across multiple metrics.

</details>


### [549] [Searching Meta Reasoning Skeleton to Guide LLM Reasoning](https://arxiv.org/abs/2510.04116)
*Ziying Zhang,Yaqing Wang,Quanming Yao*

Main category: cs.AI

TL;DR: AutoMR introduces a DAG-based meta reasoning skeleton for LLMs and automatically searches for query-aware skeletons using an AutoML-inspired framework. It dynamically expands skeletons during inference to adapt to the reasoning context, aiming to improve reasoning performance over manually designed skeletons on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Manual meta reasoning skeletons are rigid and struggle to adapt to query-specific requirements or capture complex logical dependencies among steps. A unified, graph-based representation and automated search can address these limitations.

Method: Represent meta reasoning skeleton as a directed acyclic graph (DAG) to unify prior skeletons and model dependencies. Build a search space over DAGs and formulate a search problem. Develop a dynamic skeleton sampling algorithm that expands the skeleton alongside the evolving reasoning context during inference, enabling efficient, query-aware skeleton search.

Result: AutoMR achieves better reasoning performance than prior works across extensive benchmark datasets.

Conclusion: A DAG-based, AutoML-inspired search framework enables flexible, query-aware, and efficiently adaptable meta reasoning skeletons that improve LLM reasoning performance.

Abstract: Meta reasoning behaviors work as a skeleton to guide large language model
(LLM) reasoning, thus help to improve reasoning performance. However, prior
researches implement meta reasoning skeleton with manually designed structure,
limiting ability to adapt to query-specific requirement and capture intricate
logical dependency among reasoning steps. To deal with the challenges, we
represent meta reasoning skeleton with directed acyclic graph (DAG) to unify
skeletons proposed in prior works and model intricate logical dependency. Then
we propose AutoMR, a framework that searches for query-aware meta reasoning
skeleton automatically inspired by automated machine learning (AutoML).
Specifically, we construct search space based on DAG representation of skeleton
and then formulate the search problem. We design a dynamic skeleton sampling
algorithm by expanding meta reasoning skeleton along with reasoning context at
inference time. This algorithm can derive any meta reasoning skeleton in search
space efficiently and adapt skeleton to evolving base reasoning context, thus
enable efficient query-aware skeleton search. We conduct experiments on
extensive benchmark datasets. Experimental results show that AutoMR achieves
better reasoning performance than previous works broadly.

</details>


### [550] [Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128)
*Dmitrii Troitskii,Koyena Pal,Chris Wendler,Callum Stuart McDougall,Neel Nanda*

Main category: cs.AI

TL;DR: The paper analyzes how latent states prior to wait tokens influence subsequent reasoning in large language models by training crosscoders and using latent attribution; it identifies a small set of features that modulate wait-token probabilities and demonstrates their causal role in different reasoning strategies such as restarting, recalling, expressing uncertainty, and double-checking.


<details>
  <summary>Details</summary>
Motivation: To understand why reasoning traces include wait tokens and how pre-wait latent features shape subsequent reasoning, enabling deeper insights into the mechanisms of model reasoning and potential improvements.

Method: Train crosscoders at multiple layers of DeepSeek-R1-Distill-Llama-8B (and its base) and apply a latent attribution technique in a crosscoder setting to discover features in pre-wait latents that influence wait-token probabilities; validate findings using max-activating examples and causal interventions to observe resulting reasoning patterns.

Result: Identified a small set of latent features that promote or suppress wait tokens; causal interventions show these features correspond to distinct reasoning patterns—restarting from the start, recalling knowledge, expressing uncertainty, and double-checking—thus linking pre-wait latents to the reasoning process.

Conclusion: Pre-wait latent features in the model play a meaningful role in shaping reasoning patterns, offering a concrete avenue to understand and potentially steer how models reason and backtrack during tasks.

Abstract: Prior work has shown that a significant driver of performance in reasoning
models is their ability to reason and self-correct. A distinctive marker in
these reasoning traces is the token wait, which often signals reasoning
behavior such as backtracking. Despite being such a complex behavior, little is
understood of exactly why models do or do not decide to reason in this
particular manner, which limits our understanding of what makes a reasoning
model so effective. In this work, we address the question whether model's
latents preceding wait tokens contain relevant information for modulating the
subsequent reasoning process. We train crosscoders at multiple layers of
DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent
attribution technique in the crosscoder setting. We locate a small set of
features relevant for promoting/suppressing wait tokens' probabilities.
Finally, through a targeted series of experiments analyzing max activating
examples and causal interventions, we show that many of our identified features
indeed are relevant for the reasoning process and give rise to different types
of reasoning patterns such as restarting from the beginning, recalling prior
knowledge, expressing uncertainty, and double-checking.

</details>


### [551] [Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs](https://arxiv.org/abs/2510.04140)
*Zishang Jiang,Jinyi Han,Tingyun Li,Xinyi Wang,Sihang Jiang,Jiaqing Liang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: MENTOR provides token-level, mixed-policy expert navigation that guides RL-based reasoning at critical decision points instead of full trajectories, enabling effective and diverse exploration in RLVR for LLMs.


<details>
  <summary>Details</summary>
Motivation: RLVR efficacy hinges on the base model's exploration quality (both effectiveness and diversity). Existing imitation of expert trajectories improves effectiveness but often reduces diversity. There is a need to supply expert guidance only at critical decision points to preserve diversity while guiding exploration.

Method: Introduce Mixed-policy Expert Navigation (MENTOR): a framework that provides expert guidance only at critical decision points during token-level reasoning, enabling a mixed-policy exploration strategy that combines guided and unguided reasoning to optimize RLVR.

Result: Extensive experiments show MENTOR captures the essence of expert strategies rather than surface imitation, enabling high-quality exploration and achieving superior overall performance. Code is available online.

Conclusion: Focusing expert guidance at decision-critical points allows MENTOR to balance effectiveness and diversity in exploration, improving RLVR outcomes for LLM reasoning.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely
adopted technique for enhancing the reasoning ability of Large Language Models
(LLMs). However, the effectiveness of RLVR strongly depends on the capability
of base models. This issue arises because it requires the model to have
sufficient capability to perform high-quality exploration, which involves both
effectiveness and diversity. Unfortunately, existing methods address this issue
by imitating expert trajectories, which improve effectiveness but neglect
diversity. To address this, we argue that the expert only needs to provide
guidance only at critical decision points rather than the entire reasoning
path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation
for Token-level Optimization of Reasoning, a framework that provides expert
guidance only at critical decision points to perform effective and diverse
exploration in RLVR. Extensive experiments show that MENTOR enables models
capture the essence of expert strategies rather than surface imitation, thereby
performing high-quality exploration and achieving superior overall performance.
Our code is available online.

</details>


### [552] [The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning](https://arxiv.org/abs/2510.04141)
*Mayank Ravishankara,Varindra V. Persad Maharaj*

Main category: cs.AI

TL;DR: A survey maps the evolution of multimodal AI evaluation from simple recognition tasks to complex reasoning benchmarks and expert-level integration tests, arguing that benchmark design is an adversarial, ongoing driver of progress beyond static datasets.


<details>
  <summary>Details</summary>
Motivation: Benchmark saturation reveals that high scores on older tests often mask fundamental weaknesses (e.g., shortcut learning, poor compositional generalization). The work argues for evaluation that probes reasoning, integration, and process to guide genuinely intelligent systems.

Method: Literature survey across eras of multimodal benchmarks: from ImageNet-era knowledge tests to GQA and Visual Commonsense Reasoning (VCR) for diagnostic flaws, to current expert-level benchmarks (MMBench, SEED-Bench, MMMU) assessing reasoning processes, and discussion of future directions toward abstract, creative, and social intelligence.

Result: Identifies a paradigm shift in AI evaluation toward cognitive examinations that probe reasoning and the internal processes, not just outputs. Highlights how newer benchmarks diagnose systemic flaws and promote more robust, generalizable models; shows the field's move from knowledge recognition to complex integration and process evaluation.

Conclusion: Evaluation is an ongoing, adversarial process that reshapes our goals and benchmarks, making evaluation not merely a history of datasets but a driver for building truly intelligent multimodal systems; future work should explore abstract, creative, and social dimensions of intelligence.

Abstract: This survey paper chronicles the evolution of evaluation in multimodal
artificial intelligence (AI), framing it as a progression of increasingly
sophisticated "cognitive examinations." We argue that the field is undergoing a
paradigm shift, moving from simple recognition tasks that test "what" a model
sees, to complex reasoning benchmarks that probe "why" and "how" it
understands. This evolution is driven by the saturation of older benchmarks,
where high performance often masks fundamental weaknesses. We chart the journey
from the foundational "knowledge tests" of the ImageNet era to the "applied
logic and comprehension" exams such as GQA and Visual Commonsense Reasoning
(VCR), which were designed specifically to diagnose systemic flaws such as
shortcut learning and failures in compositional generalization. We then survey
the current frontier of "expert-level integration" benchmarks (e.g., MMBench,
SEED-Bench, MMMU) designed for today's powerful multimodal large language
models (MLLMs), which increasingly evaluate the reasoning process itself.
Finally, we explore the uncharted territories of evaluating abstract, creative,
and social intelligence. We conclude that the narrative of AI evaluation is not
merely a history of datasets, but a continuous, adversarial process of
designing better examinations that, in turn, redefine our goals for creating
truly intelligent systems.

</details>


### [553] [Open Agent Specification (Agent Spec) Technical Report](https://arxiv.org/abs/2510.04173)
*Yassine Benajiba,Cesare Bernardis,Vladislav Blinov,Paul Cayet,Hassan Chafi,Abderrahim Fathan,Louis Faucon,Damien Hilloulin,Sungpack Hong,Ingo Kossyk,Rhicheek Patra,Sujith Ravi,Jonas Schweizer,Jyotika Singh,Shailender Singh,Xuelin Situ,Weiyi Sun,Jerry Xu,Ying Xu*

Main category: cs.AI

TL;DR: A declarative Open Agent Specification (Agent Spec) provides a unified cross-framework language to define AI agents and workflows, improving portability, interoperability, and reusability across AI agent frameworks, thereby reducing duplicated effort and enabling easier exchange of solutions.


<details>
  <summary>Details</summary>
Motivation: Fragmented AI agent development across multiple frameworks leads to portability, interoperability, and reproducibility challenges. A common specification would allow agents to be designed once and deployed across frameworks, aiding developers, tool/framework vendors, researchers, and enterprises.

Method: Proposes Agent Spec as a declarative specification language and outlines its technical foundations, intended design, and benefits. The abstract serves as a high-level overview of motivation, goals, and future developments rather than reporting empirical evaluation.

Result: No empirical results are presented. The abstract communicates a proposal for a unified specification and its expected benefits, including portability, interoperability, reuse, and faster prototyping-to-deployment.

Conclusion: Agent Spec is envisioned to enable cross-framework portability and reproducibility, expand reusable component design, and accelerate development workflows; future work is expected to further develop the technical foundations and ecosystem.

Abstract: Open Agent Specification (Agent Spec) is a declarative language that allows
AI agents and their workflows to be defined in a way that is compatible across
different AI frameworks, promoting portability and interoperability within AI
Agent frameworks.
  Agent Spec aims to resolve the challenges of fragmented agent development by
providing a common unified specification that allows AI agents to be designed
once and deployed across various frameworks, improving interoperability and
reusability, and reducing redundant development efforts. Additionally, Agent
Spec facilitates development tools and portability, allowing AI agents to be
defined independently of their execution environment and enabling teams to
exchange solutions without implementation-specific limitations.
  Agent Spec benefits four key groups: (i) Agent developers, who gain access to
a superset of reusable components and design patterns, enabling them to
leverage a broader range of functionalities; (ii) Agent framework and tool
developers, who can use Agent Spec as an interchange format and therefore
benefit from the support of other frameworks as well as other tools; (iii)
Researchers, who can achieve reproducible results and comparability,
facilitating more reliable and consistent outcomes; (iv) Enterprises, which
benefit from faster prototype-to-deployment, increased productivity, as well as
greater scalability and maintainability for their AI agent solutions. This
technical report provides an overview of the technical foundations of Agent
Spec, including motivation, benefits, and future developments.

</details>


### [554] [Constructing coherent spatial memory in LLM agents through graph rectification](https://arxiv.org/abs/2510.04195)
*Puzhen Zhang,Xuyang Chen,Yu Feng,Yuhan Jiang,Liqiu Meng*

Main category: cs.AI

TL;DR: Framework for incremental LLM-driven map construction and repair with version control and edge-impact prioritization; validated on a refined MANGO benchmark to improve topological map correctness and robustness.


<details>
  <summary>Details</summary>
Motivation: Context-dependent path queries fail as environments grow; need incremental construction of complete navigational graphs and robust repair to maintain coherent spatial memory in LLM agents.

Method: Proposes a framework with Version Control that records the full history of graph edits and their source observations for rollback, conflict tracing, and repair evaluation, plus an Edge Impact Score to prioritize minimal-cost repairs based on structural reachability, path usage, and conflict propagation. Also refines the MANGO benchmark by removing non-topological actions and inherent conflicts to create a cleaner testbed for LLM-driven construction and map repair.

Result: Significant improvements in map correctness and robustness, especially under entangled or chained inconsistencies; demonstrates the value of introspective, history-aware repair mechanisms for maintaining coherent spatial memory in LLM navigation tasks.

Conclusion: History-aware, introspective repair is crucial for maintaining coherent spatial representations in LLM agents; the proposed framework enables reliable construction and repair of navigation graphs in long or complex environments.

Abstract: Given a map description through global traversal navigation instructions
(e.g., visiting each room sequentially with action signals such as north, west,
etc.), an LLM can often infer the implicit spatial layout of the environment
and answer user queries by providing a shortest path from a start to a
destination (for instance, navigating from the lobby to a meeting room via the
hall and elevator). However, such context-dependent querying becomes incapable
as the environment grows much longer, motivating the need for incremental map
construction that builds a complete topological graph from stepwise
observations. We propose a framework for LLM-driven construction and map
repair, designed to detect, localize, and correct structural inconsistencies in
incrementally constructed navigation graphs. Central to our method is the
Version Control, which records the full history of graph edits and their source
observations, enabling fine-grained rollback, conflict tracing, and repair
evaluation. We further introduce an Edge Impact Score to prioritize
minimal-cost repairs based on structural reachability, path usage, and conflict
propagation. To properly evaluate our approach, we create a refined version of
the MANGO benchmark dataset by systematically removing non-topological actions
and inherent structural conflicts, providing a cleaner testbed for LLM-driven
construction and map repair. Our approach significantly improves map
correctness and robustness, especially in scenarios with entangled or chained
inconsistencies. Our results highlight the importance of introspective,
history-aware repair mechanisms for maintaining coherent spatial memory in LLM
agents.

</details>


### [555] [COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability](https://arxiv.org/abs/2510.04196)
*Yizhuo Ding,Mingkang Chen,Qiuhua Liu,Fenghua Weng,Wanying Qu,Yue Yang,Yugang Jiang,Zuxuan Wu,Yanwei Fu,Wenqi Shao*

Main category: cs.AI

TL;DR: COSMO-RL presents a mixed reinforcement learning framework to train large multimodal reasoning models (LMRMs) using multimodal, multitask, and multiobjective signals, yielding COSMO-R1. It claims safety improvements without sacrificing capabilities, robustness to multimodal jailbreaks, and successful cross-backbone transfer, with ablations supporting its design.


<details>
  <summary>Details</summary>
Motivation: Safety challenges in multimodal AI: images and text can be combined to bypass guardrails, and single-objective training can cause policy drift leading to over-refusal on benign inputs or unsafe compliance on risky ones. A framework that grows safety and capability together in a stable pipeline is needed.

Method: Introduce COSMO-RL, a mixed reinforcement learning framework that trains reasoning-oriented LMRMs under multimodal, multitask, and multiobjective signals, enabling a stable pipeline to align safety with general capability. Release of COSMO-R1; demonstrate cross-backbone transfer and ablations to justify design choices.

Result: COSMO-R1 demonstrates improved safety while maintaining or improving multimodal reasoning and instruction following, exhibits stronger robustness to multimodal jailbreaking, and reduces unnecessary refusals. The framework transfers across backbones with consistent gains. Ablations support the design choices.

Conclusion: A simple, practical path to advance safety and general capability together in LMRMs, with a stable end-to-end pipeline. Safety and capability can co-evolve rather than compete in alignment.

Abstract: Large Multimodal Reasoning Models (LMRMs) are moving into real applications,
where they must be both useful and safe. Safety is especially challenging in
multimodal settings: images and text can be combined to bypass guardrails, and
single objective training can cause policy drift that yields over-refusal on
benign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed
reinforcement learning framework that trains reasoning oriented LMRMs under
multimodal, multitask, and multiobjective signals, and we release the resulting
model, COSMO-R1. Our approach aims to let safety and capability grow together
in one stable pipeline rather than competing during alignment. In experiments,
COSMO-R1 improves safety while maintaining-and often improving multimodal
reasoning and instruction following, shows stronger robustness to multimodal
jailbreaks, and reduces unnecessary refusals. The framework also transfers
across backbones with consistent gains. Ablations support the design choices,
indicating a simple path to advancing safety and general capability together in
LMRMs.

</details>


### [556] [AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework](https://arxiv.org/abs/2510.04206)
*Hanchen Zhang,Xiao Liu,Bowen Lv,Xueqiao Sun,Bohao Jing,Iat Long Iong,Zhenyu Hou,Zehan Qi,Hanyu Lai,Yifan Xu,Rui Lu,Hongning Wang,Jie Tang,Yuxiao Dong*

Main category: cs.AI

TL;DR: AgentRL introduces a scalable framework for training generalist, multi-turn, multi-task LLM agents using an asynchronous generation-training loop, a unified API, containerized environments, and stabilization techniques like cross-policy sampling and task advantage normalization. It achieves strong gains over multiple open baselines and supports multi-task training that rivals task-specific models; code is open-sourced and used in AutoGLM.


<details>
  <summary>Details</summary>
Motivation: To overcome the lack of scalable infrastructure and stable training algorithms for reinforcement learning of LLM agents operating across multiple turns and tasks.

Method: Infrastructure: an asynchronous generation-training pipeline; a unified function-call based API; containerized environments; a centralized controller. Algorithms: cross-policy sampling to promote exploration across turns; task advantage normalization to stabilize multi-task training. Evaluation: trained on open LLMs across five agentic tasks; compared against GPT-5, Clause-Sonnet-4, DeepSeek-R1 and other open-source agents.

Result: AgentRL significantly outperforms baselines on five agentic tasks. Multi-task training with AgentRL matches the best results among all task-specific models. The framework is open-sourced at https://github.com/THUDM/AgentRL and has informed the AutoGLM project.

Conclusion: AgentRL enables scalable, effective RL-based learning for generalist LLM agents in multi-turn, multi-task settings and provides a practical, open-source solution that can be adopted for broader agentic RL research and deployment.

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building generalist agents that can learn through online interactions.
However, applying reinforcement learning (RL) to train LLM agents in
multi-turn, multi-task settings remains challenging due to lack of scalable
infrastructure and stable training algorithms. In this work, we present the
AgentRL framework for scalable multi-turn, multi-task agentic RL training. On
the infrastructure side, AgentRL features a fully-asynchronous
generation-training pipeline for efficient multi-turn RL. To support
heterogeneous environment development in multi-task RL, we design a unified
function-call based API interface, containerized environment development, and a
centralized controller. On the algorithm side, we propose cross-policy sampling
to encourage model exploration in multi-turn settings and task advantage
normalization to stabilize multi-task training. Experiments show that AgentRL,
trained on open LLMs across five agentic tasks, significantly outperforms
GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.
Multi-task training with AgentRL matches the best results among all
task-specific models. AgentRL is open-sourced at
https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in
building \textsc{\href{https://autoglm.zhipuai.cn}{AutoGLM}}.

</details>


### [557] [Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation](https://arxiv.org/abs/2510.04265)
*Mohsen Hariri,Amirhossein Samandar,Michael Hinczewski,Vipin Chaudhary*

Main category: cs.AI

TL;DR: Bayesian evaluation for LLM reasoning replaces Pass@k/avg@N with posterior success probabilities and credible intervals, yielding stable rankings and principled decisions. Models categorical outcomes with a Dirichlet prior to support weighted rubrics and prior evidence; equivalence to Pass@1 under uniform prior; faster convergence and clearer uncertainty; extends to non-binary evaluation. Code provided.


<details>
  <summary>Details</summary>
Motivation: Pass@k and avg@N can produce unstable or misleading rankings when trials are limited and compute is constrained. A principled, uncertainty-aware framework is needed to compare LLMs reliably, leverage prior evidence, and unify binary and non-binary evaluation.

Method: A Bayesian framework modeling evaluation outcomes as categorical with a Dirichlet prior. Compute posterior mean and credible intervals for any weighted rubric. Closed-form updates enable rapid computation and incorporation of prior evidence. Under a uniform prior, the posterior mean is order-equivalent to avg@1, linking to existing metrics. Extends to non-binary, rubric-based scoring.

Result: Empirically, the approach achieves faster convergence and higher rank stability than Pass@k and recent variants on simulations with known ground-truth rates and on AIME'24/'25, HMMT'25, BrUMO'25, enabling reliable comparisons from fewer trials. The framework clarifies when gaps are statistically meaningful (non-overlapping credible intervals) and supports rubric-based evaluation.

Conclusion: Replace Pass@k with a posterior-based, compute-efficient evaluation that unifies binary and non-binary assessments and makes uncertainty explicit. Code is available at the provided URL.

Abstract: Pass$@k$ is widely used to report performance for LLM reasoning, but it often
yields unstable, misleading rankings, especially when the number of trials
(samples) is limited and compute is constrained. We present a principled
Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over
$N$ trials (avg$@N$) with posterior estimates of a model's underlying success
probability and credible intervals, yielding stable rankings and a transparent
decision rule for differences. Evaluation outcomes are modeled as categorical
(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the
posterior mean and uncertainty of any weighted rubric and enabling the use of
prior evidence when appropriate. Theoretically, under a uniform prior, the
Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),
explaining its empirical robustness while adding principled uncertainty.
Empirically, in simulations with known ground-truth success rates and on
AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster
convergence and greater rank stability than Pass$@k$ and recent variants,
enabling reliable comparisons at far smaller sample counts. The framework
clarifies when observed gaps are statistically meaningful (non-overlapping
credible intervals) versus noise, and it naturally extends to graded,
rubric-based evaluations. Together, these results recommend replacing Pass$@k$
for LLM evaluation and ranking with a posterior-based, compute-efficient
protocol that unifies binary and non-binary evaluation while making uncertainty
explicit. Code is available at https://mohsenhariri.github.io/bayes-kit

</details>


### [558] [Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales](https://arxiv.org/abs/2510.04272)
*Jinyang Jiang,Jinhui Han,Yijie Peng,Ying Zhang*

Main category: cs.AI

TL;DR: Proposes a multi-agent reinforcement learning framework with multi-timescale updates for cross-functional coordination between inventory replenishment and personalized recommendations, supported by a theoretical model and convergence guarantees, yielding profitability gains over siloed approaches.


<details>
  <summary>Details</summary>
Motivation: To address increasing organizational complexity and the need for firm-wide profitability through coordinated decision-making across distinct functional modules, leveraging advances in reinforcement learning.

Method: Develop an integrated theoretical model of interactions between replenishment and recommendations; derive analytical benchmarks for optimal coordination; design a model-free multi-agent RL architecture that decomposes policy components by function and assigns distinct learning speeds; provide multi-timescale updates and prove asymptotic convergence; validate via extensive simulations.

Result: The proposed framework significantly improves profitability compared to siloed decision-making frameworks; RL agents display coordinated behaviors consistent with managerial insights from the theory; enhanced scalability and deployment flexibility.

Conclusion: A scalable, interpretable RL-based solution emerges for effective cross-functional coordination in complex business settings, with convergence guarantees and demonstrated profitability gains.

Abstract: Effective cross-functional coordination is essential for enhancing firm-wide
profitability, particularly in the face of growing organizational complexity
and scale. Recent advances in artificial intelligence, especially in
reinforcement learning (RL), offer promising avenues to address this
fundamental challenge. This paper proposes a unified multi-agent RL framework
tailored for joint optimization across distinct functional modules, exemplified
via coordinating inventory replenishment and personalized product
recommendation. We first develop an integrated theoretical model to capture the
intricate interplay between these functions and derive analytical benchmarks
that characterize optimal coordination. The analysis reveals synchronized
adjustment patterns across products and over time, highlighting the importance
of coordinated decision-making. Leveraging these insights, we design a novel
multi-timescale multi-agent RL architecture that decomposes policy components
according to departmental functions and assigns distinct learning speeds based
on task complexity and responsiveness. Our model-free multi-agent design
improves scalability and deployment flexibility, while multi-timescale updates
enhance convergence stability and adaptability across heterogeneous decisions.
We further establish the asymptotic convergence of the proposed algorithm.
Extensive simulation experiments demonstrate that the proposed approach
significantly improves profitability relative to siloed decision-making
frameworks, while the behaviors of the trained RL agents align closely with the
managerial insights from our theoretical model. Taken together, this work
provides a scalable, interpretable RL-based solution to enable effective
cross-functional coordination in complex business settings.

</details>


### [559] [GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction](https://arxiv.org/abs/2510.04281)
*Zhuangzhi Gao,Hongyi Qin,He Zhao,Qinkai Yu,Feixiang Zhou,Eduard Shantsila,Uazman Alam,Alena Shantsila,Wahbi El-Bouri,Gregory Y. H. Lip,Yalin Zheng*

Main category: cs.AI

TL;DR: GROK is a grounded multimodal LLM for ocular disease that fuses color fundus photography, OCT, and text with a three-module design to produce a quantitative-to-qualitative diagnostic chain of thought and clinician-grade diagnoses.


<details>
  <summary>Details</summary>
Motivation: Current medical multimodal LLMs underutilize the synergy between CFP and OCT and provide limited interpretability of biomarkers, hindering clinician trust and decision support.

Method: Three core modules: Knowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment, and Supervised Instruction Fine-Tuning; builds a quantitative-to-qualitative diagnostic chain of thought with lesion annotations; evaluates on Grounded Ophthalmic Understanding benchmark across six diseases and three tasks; uses LoRA fine-tuning on a 7B Qwen2 backbone.

Result: GROK outperforms comparable 7B and 32B baselines on report quality and fine-grained clinical metrics and exceeds OpenAI o3 on the evaluated tasks; code and data are publicly available.

Conclusion: GROK demonstrates effective integration of CFP and OCT with text to yield clinician-grade diagnoses and interpretable lesion annotations, validating a scalable approach for grounded multimodal clinical reasoning with a lightweight fine-tuning setup.

Abstract: Multimodal large language models (MLLMs) hold promise for integrating diverse
data modalities, but current medical adaptations such as LLaVA-Med often fail
to fully exploit the synergy between color fundus photography (CFP) and optical
coherence tomography (OCT), and offer limited interpretability of quantitative
biomarkers. We introduce GROK, a grounded multimodal large language model that
jointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of
ocular and systemic disease. GROK comprises three core modules:
Knowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment,
and Supervised Instruction Fine-Tuning, which together establish a
quantitative-to-qualitative diagnostic chain of thought, mirroring real
clinical reasoning when producing detailed lesion annotations. To evaluate our
approach, we introduce the Grounded Ophthalmic Understanding benchmark, which
covers six disease categories and three tasks: macro-level diagnostic
classification, report generation quality, and fine-grained clinical assessment
of the generated chain of thought. Experiments show that, with only LoRA
(Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK
outperforms comparable 7B and 32B baselines on both report quality and
fine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are
publicly available in the GROK repository.

</details>


### [560] [Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning](https://arxiv.org/abs/2510.04284)
*Yunghwei Lai,Kaiming Liu,Ziyue Wang,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: Doctor-R1 is an AI doctor agent that integrates accurate medical decision-making with strategic empathetic consultation through a multi-agent environment, two-tier rewards, and an experience repository; it outperforms open-source and proprietary LLM baselines on HealthBench and MAQuE, with favorable human dialogue judgments.


<details>
  <summary>Details</summary>
Motivation: Outpatient clinicians rely on both correct medical decisions and empathetic, strategic patient inquiry. While large language models approach high decision accuracy, they often lack the capability for principled, multi-turn, empathetic consultation in real-world clinical settings. This paper aims to close the gap by building an AI doctor that blends decision-making with consultative dialogue.

Method: Doctor-R1 is built around three components: (1) a multi-agent interactive environment to simulate complex clinical conversations; (2) a two-tier reward architecture that separately optimizes clinical decision-making and communicative inquiry skills; and (3) an experience repository of high-quality prior trajectories to ground policy learning. The model learns to ask high-yield questions and conduct strategic, multi-turn inquiries to guide decisions, evaluated on HealthBench and MAQuE across metrics like communication quality, user experience, and task accuracy.

Result: Empirical results show Doctor-R1 surpasses state-of-the-art open-source specialized LLMs by a substantial margin with higher parameter efficiency and also outperforms powerful proprietary models. Human evaluations indicate a preference for Doctor-R1-generated dialogue as more human-like and clinically appropriate.

Conclusion: The proposed framework successfully integrates accurate clinical decision-making with strategic, empathetic consultation, yielding superior performance across objective metrics and human judgments, and demonstrating potential for real-world outpatient AI-assisted care.

Abstract: The professionalism of a human doctor in outpatient service depends on two
core abilities: the ability to make accurate medical decisions and the medical
consultation skill to conduct strategic, empathetic patient inquiry. Existing
Large Language Models (LLMs) have achieved remarkable accuracy on medical
decision-making benchmarks. However, they often lack the ability to conduct the
strategic and empathetic consultation, which is essential for real-world
clinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor
agent trained to master both of the capabilities by ask high-yield questions
and conduct strategic multi-turn inquiry to guide decision-making. Our
framework introduces three key components: a multi-agent interactive
environment, a two-tiered reward architecture that separately optimizes
clinical decision-making and communicative inquiry skills, and an experience
repository to ground policy learning in high-quality prior trajectories. We
evaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across
multi-facet metrics, such as communication quality, user experience, and task
accuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source
specialized LLMs by a substantial margin with higher parameter efficiency and
outperforms powerful proprietary models. Furthermore, the human evaluations
show a strong preference for Doctor-R1 to generate human-preferred clinical
dialogue, demonstrating the effectiveness of the framework.

</details>


### [561] [On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2510.04311)
*Bohan Tang,Huidong Liang,Keyue Jiang,Xiaowen Dong*

Main category: cs.AI

TL;DR: A two-dimensional task framework (depth vs width) to evaluate LLM-MAS; theoretical and empirical results show LLM-MAS advantages grow with both factors, especially depth; provides guidance for benchmarks.


<details>
  <summary>Details</summary>
Motivation: There is a lack of principled experimental designs to assess when LLM-MAS outperform LLM-SAS; a systematic theory is needed.

Method: Develop a theoretical framework with depth and width; analyze multi-agent debate system; run experiments on discriminative and generative tasks across varying depth/width.

Result: LLM-MAS advantages over LLM-SAS increase with depth and width; effects more pronounced for depth; supports the framework and informs benchmark design.

Conclusion: Offers principled foundation for future LLM-MAS methods and benchmarks; helps determine when to deploy LLM-MAS and how to design tasks to test them.

Abstract: Large language model multi-agent systems (LLM-MAS) offer a promising paradigm
for harnessing collective intelligence to achieve more advanced forms of AI
behaviour. While recent studies suggest that LLM-MAS can outperform LLM
single-agent systems (LLM-SAS) on certain tasks, the lack of systematic
experimental designs limits the strength and generality of these conclusions.
We argue that a principled understanding of task complexity, such as the degree
of sequential reasoning required and the breadth of capabilities involved, is
essential for assessing the effectiveness of LLM-MAS in task solving. To this
end, we propose a theoretical framework characterising tasks along two
dimensions: depth, representing reasoning length, and width, representing
capability diversity. We theoretically examine a representative class of
LLM-MAS, namely the multi-agent debate system, and empirically evaluate its
performance in both discriminative and generative tasks with varying depth and
width. Theoretical and empirical results show that the benefit of LLM-MAS over
LLM-SAS increases with both task depth and width, and the effect is more
pronounced with respect to depth. This clarifies when LLM-MAS are beneficial
and provides a principled foundation for designing future LLM-MAS methods and
benchmarks.

</details>


### [562] [Speculative Actions: A Lossless Framework for Faster Agentic Systems](https://arxiv.org/abs/2510.04371)
*Naimeng Ye,Arnav Ahuja,Georgios Liargkovas,Yunan Lu,Kostis Kaffes,Tianyi Peng*

Main category: cs.AI

TL;DR: Speculative actions enable parallel execution in agentic systems by predicting likely actions with faster models, yielding up to 55% next-action accuracy and lower latency.


<details>
  <summary>Details</summary>
Motivation: Sequential action execution (API calls) is a bottleneck that slows training, evaluation, and deployment of AI agents; there is a need for low-latency, scalable agentic systems.

Method: Introduce speculative actions, a lossless framework that uses faster models to predict likely actions and execute multiple steps in parallel; evaluate across gaming, e-commerce, web search, and an OS-related lossy extension; explore stronger guessing models, top-K action prediction, multi-step speculation, and uncertainty-aware optimization.

Result: Achieved substantial accuracy in next-action prediction (up to 55%), translating into significant end-to-end latency reductions; improvements over stronger guessing models, top-K, multi-step speculation, and uncertainty-aware optimization.

Conclusion: Speculative actions offer a promising path toward deploying low-latency agentic systems in the real world; continued improvements in models and strategies can further enhance performance across domains.

Abstract: Despite growing interest in AI agents across industry and academia, their
execution in an environment is often slow, hampering training, evaluation, and
deployment. For example, a game of chess between two state-of-the-art agents
may take hours. A critical bottleneck is that agent behavior unfolds
sequentially: each action requires an API call, and these calls can be
time-consuming. Inspired by speculative execution in microprocessors and
speculative decoding in LLM inference, we propose speculative actions, a
lossless framework for general agentic systems that predicts likely actions
using faster models, enabling multiple steps to be executed in parallel. We
evaluate this framework across three agentic environments: gaming, e-commerce,
web search, and a "lossy" extension for an operating systems environment. In
all cases, speculative actions achieve substantial accuracy in next-action
prediction (up to 55%), translating into significant reductions in end-to-end
latency. Moreover, performance can be further improved through stronger
guessing models, top-K action prediction, multi-step speculation, and
uncertainty-aware optimization, opening a promising path toward deploying
low-latency agentic systems in the real world.

</details>


### [563] [Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation](https://arxiv.org/abs/2510.04373)
*Hadi Nekoei,Aman Jaiswal,Patrice Bechard,Oleh Shliazhko,Orlando Marquez Ayala,Mathieu Reymond,Massimo Caccia,Alexandre Drouin,Sarath Chandar,Alexandre Lacoste*

Main category: cs.AI

TL;DR: JEF Hinter distills offline decision traces into compact, context-aware hints for LLM agents, highlighting decisive steps, leveraging both successes and failures, and using a retriever to supply relevant guidance at test time; it outperforms baselines on several benchmarks.


<details>
  <summary>Details</summary>
Motivation: Offline trajectories contain reusable knowledge that can guide decision-making without expensive online interactions or fine-tuning. For closed/open-source models, avoiding catastrophic forgetting and ensuring scalability requires a method to extract and reuse such knowledge as targeted hints.

Method: Introduce Just-in-time Episodic Feedback Hinter (JEF Hinter) with a zooming mechanism to identify decisive steps in long trajectories. Distill both successful and failed traces into compact hints; enable parallelized hint generation and benchmark-independent prompting. At inference, a retriever selects context-relevant hints for the current state, providing transparent, traceable guidance.

Result: Empirically, JEF Hinter consistently outperforms strong baselines, including human- and document-based hints, on MiniWoB++, WorkArena-L1, and WebArena-Lite.

Conclusion: JEF Hinter provides a scalable, transparent pathway to leverage offline experience for LLM agents, reducing reliance on online data and improving performance across diverse sequential decision tasks.

Abstract: Large language model (LLM) agents perform well in sequential decision-making
tasks, but improving them on unfamiliar domains often requires costly online
interactions or fine-tuning on large expert datasets. These strategies are
impractical for closed-source models and expensive for open-source ones, with
risks of catastrophic forgetting. Offline trajectories offer reusable
knowledge, yet demonstration-based methods struggle because raw traces are
long, noisy, and tied to specific tasks. We present Just-in-time Episodic
Feedback Hinter (JEF Hinter), an agentic system that distills offline traces
into compact, context-aware hints. A zooming mechanism highlights decisive
steps in long trajectories, capturing both strategies and pitfalls. Unlike
prior methods, JEF Hinter leverages both successful and failed trajectories,
extracting guidance even when only failure data is available, while supporting
parallelized hint generation and benchmark-independent prompting. At inference,
a retriever selects relevant hints for the current state, providing targeted
guidance with transparency and traceability. Experiments on MiniWoB++,
WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms
strong baselines, including human- and document-based hints.

</details>


### [564] [LLM Based Bayesian Optimization for Prompt Search](https://arxiv.org/abs/2510.04384)
*Adam Ballew,Jingbo Wang,Shaogang Ren*

Main category: cs.AI

TL;DR: A Bayesian optimization framework using an LLM-powered GP to optimize prompts for LLM-based text classification; prompts are generated from seed prompts, optimized via UCB, and evaluated on two datasets to demonstrate efficiency gains.


<details>
  <summary>Details</summary>
Motivation: Reduce expensive API calls and improve prompt quality for LLM-based text classification by systematically optimizing prompts in a data-efficient manner.

Method: Use an LLM-powered Gaussian Process as the surrogate model, generate prompt candidates by expanding seed prompts via an LLM, apply Upper Confidence Bound acquisition to select prompts, iteratively refine prompts on a data subset, and evaluate text classification performance.

Result: Evaluated on two datasets; the approach shows advantages (efficiency and/or accuracy improvements) as discussed in the abstract, though specific metrics are not provided.

Conclusion: BO-LLM offers a principled, data-efficient framework for prompt optimization in LLM-based classification by leveraging predictive uncertainty to guide prompt exploration and reduce API calls.

Abstract: Bayesian Optimization (BO) has been widely used to efficiently optimize
expensive black-box functions with limited evaluations. In this paper, we
investigate the use of BO for prompt engineering to enhance text classification
with Large Language Models (LLMs). We employ an LLM-powered Gaussian Process
(GP) as the surrogate model to estimate the performance of different prompt
candidates. These candidates are generated by an LLM through the expansion of a
set of seed prompts and are subsequently evaluated using an Upper Confidence
Bound (UCB) acquisition function in conjunction with the GP posterior. The
optimization process iteratively refines the prompts based on a subset of the
data, aiming to improve classification accuracy while reducing the number of
API calls by leveraging the prediction uncertainty of the LLM-based GP. The
proposed BO-LLM algorithm is evaluated on two datasets, and its advantages are
discussed in detail in this paper.

</details>


### [565] [Internal World Models as Imagination Networks in Cognitive Agents](https://arxiv.org/abs/2510.04391)
*Saurabh Ranjan,Brian Odegaard*

Main category: cs.AI

TL;DR: The paper proposes a method to compare internal world models via imagination networks across humans and LLMs, finding structured centrality relations in humans but not in LLMs, implying fundamental differences in IWMs.


<details>
  <summary>Details</summary>
Motivation: Reassess the computational role of imagination beyond reward maximization and develop tools to compare internal representations in humans and AI; understand IWMs and guide human-like AI imagination.

Method: Administer two imagination vividness questionnaires; build imagination networks from responses; analyze centrality measures (expected influence, strength, closeness); compare networks across humans vs LLMs under different prompts and memory conditions.

Result: In humans, centrality measures correlate, showing cohesive IWMs; in LLMs, networks lack clustering and show weak correlations across prompts/memory; cross-domain similarity is low.

Conclusion: A novel framework for examining internally-generated representations; provides insights for designing AI with more human-like imagination; highlights gaps between human and AI IWMs.

Abstract: What is the computational objective of imagination? While classical
interpretations suggest imagination is useful for maximizing rewards, recent
findings challenge this view. In this study, we propose that imagination serves
to access an internal world model (IWM) and use psychological network analysis
to explore IWMs in humans and large language models (LLMs). Specifically, we
assessed imagination vividness ratings using two questionnaires and constructed
imagination networks from these reports. Imagination networks from human groups
showed correlations between different centrality measures, including expected
influence, strength, and closeness. However, imagination networks from LLMs
showed a lack of clustering and lower correlations between centrality measures
under different prompts and conversational memory conditions. Together, these
results indicate a lack of similarity between IWMs in human and LLM agents.
Overall, our study offers a novel method for comparing internally-generated
representations in humans and AI, providing insights for developing human-like
imagination in artificial intelligence.

</details>


### [566] [Utility-Learning Tension in Self-Modifying Agents](https://arxiv.org/abs/2510.04399)
*Charles L. Wang,Keir Dorchen,Peter Jin*

Main category: cs.AI

TL;DR: Five-axis decomposition with a decision layer reveals a sharp utility–learning tension in self-modifying agents; distribution-free learnability is preserved only under capacity bounds; two-gate policies safeguard learnability, validated by experiments.


<details>
  <summary>Details</summary>
Motivation: As systems approach superintelligence, agents may self-modify across all aspects of their design. Understanding how incentive-driven changes affect learning reliability and generalization is crucial to prevent unsafe self-modification.

Method: Introduce a five-axis decomposition and a decision layer to separate incentives from learning behavior, analyze axes in isolation, and derive a structural conflict between immediate/perceived performance gains and the statistical preconditions for reliable learning. Prove distribution-free guarantees hold iff the policy-reachable model family is uniformly capacity-bounded; show unbounded capacity can make learnable tasks unlearnable; reduce axes to a single capacity criterion under standard assumptions; validate with numerical experiments  across axes comparing destructive utility policies to two-gate policies that preserve learnability.

Result: Identification of a sharp utility–learning tension; conditions under which distribution-free guarantees hold; capacity bounds ensure learnability; two-gate policies outperform destructive utility policies in experiments across axes.

Conclusion: There exists a single boundary for safe self-modification under common practical assumptions; to preserve learnability, enforce uniform capacity bounds or adopt two-gate policies. The theory is supported by numerical experiments.

Abstract: As systems trend toward superintelligence, a natural modeling premise is that
agents can self-improve along every facet of their own design. We formalize
this with a five-axis decomposition and a decision layer, separating incentives
from learning behavior and analyzing axes in isolation. Our central result
identifies and introduces a sharp utility--learning tension, the structural
conflict in self-modifying systems whereby utility-driven changes that improve
immediate or expected performance can also erode the statistical preconditions
for reliable learning and generalization. Our findings show that
distribution-free guarantees are preserved iff the policy-reachable model
family is uniformly capacity-bounded; when capacity can grow without limit,
utility-rational self-changes can render learnable tasks unlearnable. Under
standard assumptions common in practice, these axes reduce to the same capacity
criterion, yielding a single boundary for safe self-modification. Numerical
experiments across several axes validate the theory by comparing destructive
utility policies against our proposed two-gate policies that preserve
learnability.

</details>


### [567] [DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization](https://arxiv.org/abs/2510.04474)
*Gang Li,Yan Chen,Ming Lin,Tianbao Yang*

Main category: cs.AI

TL;DR: DRPO decouples length-based rewards for positive (correct) rollouts from negatives in reinforcement learning for large reasoning models, enabling concise reasoning with minimal performance loss. It derives a closed-form optimum positive distribution under KL regularization and uses on-policy data with importance weighting. Empirically, it yields substantial length reduction with small accuracy loss (e.g., 77% shorter reasoning with 1.1% loss on GSM8k for a 1.5B model, outperforming baselines).


<details>
  <summary>Details</summary>
Motivation: Large reasoning models suffer from overthinking: generating long, redundant reasoning steps that incur high compute and latency. Penalizing long correct rollouts in GRPO can backfire because the group-relative advantage may assign negative value to valid reasoning. A need exists to decouple and normalize positive signals to preserve valid reasoning while encouraging conciseness.

Method: Introduce Decoupled Reward Policy Optimization (DRPO) that separates length-based learning signals for positive (correct) rollouts from negative samples. The objective integrates an optimized positive data distribution (maximizing length-based rewards under a KL constraint) into a discriminative objective, yielding a closed-form solution for the distribution. Gradients are computed efficiently using only on-policy data and importance weighting. The approach is general and can accommodate other positive-data rewards beyond length.

Result: Empirical evaluation on mathematical reasoning tasks shows DRPO outperforms six efficient baselines. With a 1.5B model, it achieves ~77% reduction in reasoning length with only ~1.1% performance loss on GSM8k (simple questions), while a follow-up baseline achieves 68% length reduction but loses ~4.3% accuracy.

Conclusion: Decoupling positive and negative reward signals in DRPO allows aggressive length reduction with minimal performance trade-off. The approach provides a general, efficient solution (closed-form positive distribution) and can incorporate other positive-data preferences beyond length, with strong empirical gains on reasoning benchmarks.

Abstract: Recent large reasoning models (LRMs) driven by reinforcement learning
algorithms (e.g., GRPO) have achieved remarkable performance on challenging
reasoning tasks. However, these models suffer from overthinking, generating
unnecessarily long and redundant reasoning even for simple questions, which
substantially increases computational cost and response latency. While existing
methods incorporate length rewards to GRPO to promote concise reasoning, they
incur significant performance degradation. We identify the root cause: when
rewards for correct but long rollouts are penalized, GRPO's group-relative
advantage function can assign them negative advantages, actively discouraging
valid reasoning. To overcome this, we propose Decoupled Reward Policy
Optimization (DRPO), a novel framework that decouples the length-based learning
signal of correct rollouts from incorrect ones. DRPO ensures that reward
signals for correct rollouts are normalized solely within the positive group,
shielding them from interference by negative samples. The DRPO's objective is
grounded in integrating an optimized positive data distribution, which
maximizes length-based rewards under a KL regularization, into a discriminative
objective. We derive a closed-form solution for this distribution, enabling
efficient computation of the objective and its gradients using only on-policy
data and importance weighting. Of independent interest, this formulation is
general and can incorporate other preference rewards of positive data beyond
length. Experiments on mathematical reasoning tasks demonstrate DRPO's
significant superiority over six efficient reasoning baselines. Notably, with a
1.5B model, our method achieves 77\% length reduction with only 1.1\%
performance loss on simple questions like GSM8k dataset, while the follow-up
baseline sacrifices 4.3\% for 68\% length reduction.

</details>


### [568] [On Continuous Optimization for Constraint Satisfaction Problems](https://arxiv.org/abs/2510.04480)
*Yunuo Cen,Zixuan Wang,Jintao Zhang,Zhiwei Zhang,Xuanyao Fong*

Main category: cs.AI

TL;DR: FourierCSP extends continuous local search from Boolean SAT to finite-domain CSPs by generalizing the Walsh-Fourier transform to convert versatile constraints into compact multilinear polynomials, enabling gradient-based optimization without heavy encodings and showing competitive scalability.


<details>
  <summary>Details</summary>
Motivation: To broaden the success of continuous local search methods from SAT to general CSPs with finite-domain variables, avoiding memory-intensive encodings and auxiliary variables while maintaining efficiency.

Method: Introduce FourierCSP, a CSP framework that generalizes the Walsh-Fourier transform to express CSP constraints as compact multilinear polynomials. Evaluate and differentiate the objective efficiently via circuit-output probability and optimize using a projected gradient method with theoretical guarantees.

Result: Empirical benchmarks show FourierCSP is scalable and competitive, expanding the class of problems efficiently solvable by CLS techniques.

Conclusion: FourierCSP demonstrates the viability and practicality of applying continuous optimization to a broad class of finite-domain CSPs, reducing encoding overhead and enabling efficient solving; it invites further exploration and optimization.

Abstract: Constraint satisfaction problems (CSPs) are fundamental in mathematics,
physics, and theoretical computer science. While conflict-driven clause
learning Boolean Satisfiability (SAT) solvers have achieved remarkable success
and become the mainstream approach for Boolean satisfiability, recent advances
show that modern continuous local search (CLS) solvers can achieve highly
competitive results on certain classes of SAT problems. Motivated by these
advances, we extend the CLS framework from Boolean SAT to general CSP with
finite-domain variables and expressive constraints. We present FourierCSP, a
continuous optimization framework that generalizes the Walsh-Fourier transform
to CSP, allowing for transforming versatile constraints to compact multilinear
polynomials, thereby avoiding the need for auxiliary variables and
memory-intensive encodings. Our approach leverages efficient evaluation and
differentiation of the objective via circuit-output probability and employs a
projected gradient optimization method with theoretical guarantees. Empirical
results on benchmark suites demonstrate that FourierCSP is scalable and
competitive, significantly broadening the class of problems that can be
efficiently solved by CLS techniques.

</details>


### [569] [Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning](https://arxiv.org/abs/2510.04488)
*Edward Y. Chang,Ethan Y. Chang*

Main category: cs.AI

TL;DR: MACI is an active debate controller with two independent dials (information quality gate and behavior pacing) plus a moderator to halt on plateau, offering budget-aware, terminating multi-agent debate with improved accuracy/calibration and reduced tokens.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent debates waste compute due to fixed adversarial stance, lack of deliberation, and heuristic stopping; need a controller that budgets compute and tracks evidence and argument quality.

Method: Introduce two dials: information dial gates evidence by quality; behavior dial schedules contentiousness from exploration to consolidation. A moderator tracks disagreement/overlap/evidence quality/argument quality and halts when gains plateau. Provides theory-lite guarantees of nonincreasing dispersion and provable termination with a budget-feasible scheduler. Uses CRIT as cross-family LLM judge for soft weight and stop signal; generates RAG plans for next retrieval.

Result: Across clinical diagnosis and news-bias tasks, MACI improves accuracy and calibration while reducing tokens; converts residual uncertainty into precise retrieve-next plans. Stability depends on judge capability; higher-capability judges yield better stability.

Conclusion: MACI makes debate budget-aware, measurable, and provably terminating, improving efficiency and performance across tasks, with a robust judging mechanism (CRIT) and termination guarantees.

Abstract: Multi-agent debate often wastes compute by using a fixed adversarial stance,
aggregating without deliberation, or stopping on heuristics. We introduce MACI,
an active controller with two independent dials that decouple information from
behavior: an information dial that gates evidence by quality, and a behavior
dial that schedules contentiousness from exploration to consolidation. A
moderator tracks disagreement, overlap, evidence quality, and argument quality,
and halts when gains plateau. We provide theory-lite guarantees for
nonincreasing dispersion and provable termination, with a budget-feasible
scheduler. Across clinical diagnosis and news-bias tasks, MACI improves
accuracy and calibration while reducing tokens, and converts residual
uncertainty into precision RAG plans that specify what to retrieve next. We use
a cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,
validated for order invariance and judge-swap stability; stability depends on
using high-capability judges. MACI turns debate into a budget-aware,
measurable, and provably terminating controller.

</details>


### [570] [Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents](https://arxiv.org/abs/2510.04491)
*Muyu He,Anand Kumar,Tsach Mackey,Meghana Rajeev,James Zou,Nazneen Rajani*

Main category: cs.AI

TL;DR: TraitBasis introduces a model-agnostic, data-efficient robustness testing method that discovers steerable user-trait directions in activation space to controllably stress AI agents. It extends tau-Bench to tau-Trait, revealing average 2–30% degradation across frontier models and enabling compositional, inference-time perturbations. Open-sourced as tau-trait for airline, retail, telecom, and telehealth.


<details>
  <summary>Details</summary>
Motivation: Current AI agents are brittle and untested under realistic, behaviorally diverse user interactions. Small shifts in user behavior can cause large drops in performance, and standard benchmarks fail to capture this fragility. There is a need for lightweight, data-efficient robustness testing that is model-agnostic and compositional.

Method: Learn directions in activation space corresponding to steerable user traits (e.g., impatience, incoherence) that can be controlled, scaled, composed, and applied at inference without fine-tuning or extra data. Extend tau-Bench to tau-Trait by altering user behaviors through trait vectors.

Result: Across frontier models, tau-Trait induces an average performance degradation of 2%–30%, highlighting substantial brittleness in current AI agents. TraitBasis provides a simple, data-efficient, and compositional approach to robustness testing and simulation-driven stress testing.

Conclusion: TraitBasis enables robust, simulation-driven stress testing and training loops, helping build agents that remain reliable in unpredictable human interactions. The authors open-source tau-trait across four domains (airline, retail, telecom, telehealth) to facilitate community-wide QA of behaviorally diverse intents and trait scenarios.

Abstract: Despite rapid progress in building conversational AI agents, robustness is
still largely untested. Small shifts in user behavior, such as being more
impatient, incoherent, or skeptical, can cause sharp drops in agent
performance, revealing how brittle current AI agents are. Today's benchmarks
fail to capture this fragility: agents may perform well under standard
evaluations but degrade spectacularly in more realistic and varied settings. We
address this robustness testing gap by introducing TraitBasis, a lightweight,
model-agnostic method for systematically stress testing AI agents. TraitBasis
learns directions in activation space corresponding to steerable user traits
(e.g., impatience or incoherence), which can be controlled, scaled, composed,
and applied at inference time without any fine-tuning or extra data. Using
TraitBasis, we extend $\tau$-Bench to $\tau$-Trait, where user behaviors are
altered via controlled trait vectors. We observe on average a 2%-30%
performance degradation on $\tau$-Trait across frontier models, highlighting
the lack of robustness of current AI agents to variations in user behavior.
Together, these results highlight both the critical role of robustness testing
and the promise of TraitBasis as a simple, data-efficient, and compositional
tool. By powering simulation-driven stress tests and training loops, TraitBasis
opens the door to building AI agents that remain reliable in the unpredictable
dynamics of real-world human interactions. We have open-sourced $\tau$-Trai
across four domains: airline, retail, telecom, and telehealth, so the community
can systematically QA their agents under realistic, behaviorally diverse
intents and trait scenarios: https://github.com/collinear-ai/tau-trait.

</details>


### [571] [ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering](https://arxiv.org/abs/2510.04514)
*Rachneet Kaur,Nishan Srishankar,Zhen Zeng,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: ChartAgent introduces an agent-based framework that reasons directly in the chart's visual space, using specialized tools to decompose queries into visual subtasks and iteratively manipulate chart images. It achieves state-of-the-art results on ChartBench/ChartX, notably on unannotated, numerically intensive questions.


<details>
  <summary>Details</summary>
Motivation: Addresses the sharp performance drop of multimodal LLMs on unannotated charts that require precise visual interpretation rather than relying on textual shortcuts.

Method: An agentic loop that decomposes queries into visual subtasks; uses a library of chart-specific vision tools (draw annotations, crop regions, localize axes) to interact with chart images; reasoning occurs in the visual domain; can be plugged into different LLMs.

Result: State-of-the-art accuracy on ChartBench and ChartX, with up to 16.07% absolute gains overall and 17.31% on unannotated numeric queries; effective across chart types and varying visual/reasoning complexity; compatible with multiple LLM backbones.

Conclusion: Demonstrates visually grounded reasoning for chart understanding via tool-augmented agents and suggests a general, plug-and-play approach to enhance chart QA across diverse models and chart types.

Abstract: Recent multimodal LLMs have shown promise in chart-based visual question
answering, but their performance declines sharply on unannotated charts, those
requiring precise visual interpretation rather than relying on textual
shortcuts. To address this, we introduce ChartAgent, a novel agentic framework
that explicitly performs visual reasoning directly within the chart's spatial
domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively
decomposes queries into visual subtasks and actively manipulates and interacts
with chart images through specialized actions such as drawing annotations,
cropping regions (e.g., segmenting pie slices, isolating bars), and localizing
axes, using a library of chart-specific vision tools to fulfill each subtask.
This iterative reasoning process closely mirrors human cognitive strategies for
chart comprehension. ChartAgent achieves state-of-the-art accuracy on the
ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%
absolute gain overall and 17.31% on unannotated, numerically intensive queries.
Furthermore, our analyses show that ChartAgent is (a) effective across diverse
chart types, (b) achieve the highest scores across varying visual and reasoning
complexity levels, and (c) serves as a plug-and-play framework that boosts
performance across diverse underlying LLMs. Our work is among the first to
demonstrate visually grounded reasoning for chart understanding using
tool-augmented multimodal agents.

</details>


### [572] [Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph](https://arxiv.org/abs/2510.04520)
*Hanyu Wang,Ruohan Xie,Yutong Wang,Guoxiong Gao,Xintao Yu,Bin Dong*

Main category: cs.AI

TL;DR: Aria introduces a two-phase Graph-of-Thought approach plus a term-grounding checker to auto-formalize conjectures in Lean, achieving state-of-the-art results on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Auto-formalization of theorems is hindered by hallucinations, semantic mismatches, and difficulty in synthesizing new definitions; a human-like reasoning and grounding framework is needed for reliable formalization.

Method: A two-phase Graph-of-Thought process that recursively decomposes statements into a dependency graph and then constructs formalizations from grounded concepts; AriaScorer retrieves definitions from Mathlib for term-level grounding to ensure semantic correctness; evaluation on EvidenceBenchmarks.

Result: ProofNet: 91.6% compilation, 68.5% final accuracy; FATE-X: 44.0% final accuracy vs 24.0% baseline; homological conjectures dataset: 42.9% final accuracy while others score 0%.

Conclusion: Aria substantially improves auto-formalization by combining hierarchical reasoning with grounded term retrieval, enabling more reliable verification and better cross-domain performance; future work may focus on expanding grounding sources and addressing remaining errors in semantic alignment.

Abstract: Accurate auto-formalization of theorem statements is essential for advancing
automated discovery and verification of research-level mathematics, yet remains
a major bottleneck for LLMs due to hallucinations, semantic mismatches, and
their inability to synthesize new definitions. To tackle these issues, we
present Aria (Agent for Retrieval and Iterative Autoformalization), a system
for conjecture-level formalization in Lean that emulates human expert reasoning
via a two-phase Graph-of-Thought process: recursively decomposing statements
into a dependency graph and then constructing formalizations from grounded
concepts. To ensure semantic correctness, we introduce AriaScorer, a checker
that retrieves definitions from Mathlib for term-level grounding, enabling
rigorous and reliable verification. We evaluate Aria on diverse benchmarks. On
ProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy,
surpassing previous methods. On FATE-X, a suite of challenging algebra problems
from research literature, it outperforms the best baseline with 44.0% vs. 24.0%
final accuracy. On a dataset of homological conjectures, Aria reaches 42.9%
final accuracy while all other models score 0%.

</details>


### [573] [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
*Xurui Song,Shuo Huai,JingJing Jiang,Jiayi Kong,Jun Luo*

Main category: cs.AI

TL;DR: DriveMind reveals a causal disconnect: planning in vision-language driving agents is driven by priors rather than plan-aligned chain-of-thought reasoning; introduces a dataset and a training-free diagnostic probe to measure causal fidelity.


<details>
  <summary>Details</summary>
Motivation: To verify whether natural-language reasoning in VLM-based driving agents causally governs planning, addressing concerns about interpretability and true end-to-end causality.

Method: Create DriveMind, a plan-aligned VQA corpus built from nuPlan. The data generation separates priors from signals to enable clean ablations. Train VLM agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO). Evaluate using nuPlan metrics and perform attention analysis. Propose the Reasoning-Planning Decoupling Hypothesis and a training-free probe to assess reliance on priors by testing planning robustness to small input perturbations.

Result: Ablation results show removing ego/navigation priors significantly harms planning scores, while removing the CoT has only minor impact. Attention analysis indicates planning focuses on priors rather than CoT. The paper provides a new dataset and a diagnostic probe.

Conclusion: There is a reasoning–planning decoupling in this setting: training-yielded reasoning appears to be a byproduct rather than a causal mediator for planning. The authors provide tools for efficient diagnosis and invite the community to adopt them for future causal fidelity assessment.

Abstract: Vision-Language Model (VLM) driving agents promise explainable end-to-end
autonomy by first producing natural-language reasoning and then predicting
trajectory planning. However, whether planning is causally driven by this
reasoning remains a critical but unverified assumption. To investigate this, we
build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus
with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.
Our data generation process converts sensors and annotations into structured
inputs and, crucially, separates priors from to-be-reasoned signals, enabling
clean information ablations. Using DriveMind, we train representative VLM
agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization
(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,
indicate a consistent causal disconnect in reasoning-planning: removing
ego/navigation priors causes large drops in planning scores, whereas removing
CoT produces only minor changes. Attention analysis further shows that planning
primarily focuses on priors rather than the CoT. Based on this evidence, we
propose the Reasoning-Planning Decoupling Hypothesis, positing that the
training-yielded reasoning is an ancillary byproduct rather than a causal
mediator. To enable efficient diagnosis, we also introduce a novel,
training-free probe that measures an agent's reliance on priors by evaluating
its planning robustness against minor input perturbations. In summary, we
provide the community with a new dataset and a diagnostic tool to evaluate the
causal fidelity of future models.

</details>


### [574] [Code World Models for General Game Playing](https://arxiv.org/abs/2510.04542)
*Wolfgang Lehrach,Daniel Hennes,Miguel Lazaro-Gredilla,Xinghua Lou,Carter Wendelken,Zun Li,Antoine Dedieu,Jordi Grau-Moya,Marc Lanctot,Atil Iscen,John Schultz,Marcus Chiam,Ian Gemp,Piotr Zielinski,Satinder Singh,Kevin P. Murphy*

Main category: cs.AI

TL;DR: A prompting-to-code approach that translates natural-language game rules and trajectories into a formal executable world model (Python) used as a simulation engine for MCTS, with LLM-generated heuristics and inference functions; yielding verifiable, deeper planning and better generalization, outperforming Gemini 2.5 Pro in 9 of 10 games.


<details>
  <summary>Details</summary>
Motivation: Direct LLM move-generation is fragile and often illegal; need verifiable rule-based planning that can be extended to new games and support imperfect information.

Method: Use LLM to produce a formal world model: Python code with state transition, legal-move enumeration, termination checks; generate heuristic value and inference functions; plug into MCTS or similar planner; evaluate across 10 games (5 perfect, 5 imperfect; 4 novel).

Result: The approach matches or outperforms Gemini 2.5 Pro in 9/10 games, demonstrating robustness across perfect and imperfect information and novel games.

Conclusion: Verifiability, strategic depth, and generalization are achieved by separating data-to-code translation from planning; the method shows promise for robust, adaptable game-playing agents based on LLM-generated executable world models.

Abstract: Large Language Models (LLMs) reasoning abilities are increasingly being
applied to classical board and card games, but the dominant approach --
involving prompting for direct move generation -- has significant drawbacks. It
relies on the model's implicit fragile pattern-matching capabilities, leading
to frequent illegal moves and strategically shallow play. Here we introduce an
alternative approach: We use the LLM to translate natural language rules and
game trajectories into a formal, executable world model represented as Python
code. This generated model -- comprising functions for state transition, legal
move enumeration, and termination checks -- serves as a verifiable simulation
engine for high-performance planning algorithms like Monte Carlo tree search
(MCTS). In addition, we prompt the LLM to generate heuristic value functions
(to make MCTS more efficient), and inference functions (to estimate hidden
states in imperfect information games). Our method offers three distinct
advantages compared to directly using the LLM as a policy: (1) Verifiability:
The generated CWM serves as a formal specification of the game's rules,
allowing planners to algorithmically enumerate valid actions and avoid illegal
moves, contingent on the correctness of the synthesized model; (2) Strategic
Depth: We combine LLM semantic understanding with the deep search power of
classical planners; and (3) Generalization: We direct the LLM to focus on the
meta-task of data-to-code translation, enabling it to adapt to new games more
easily. We evaluate our agent on 10 different games, of which 4 are novel and
created for this paper. 5 of the games are fully observed (perfect
information), and 5 are partially observed (imperfect information). We find
that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10
considered games.

</details>


### [575] [TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use](https://arxiv.org/abs/2510.04550)
*Pengfei He,Zhenwei Dai,Bing He,Hui Liu,Xianfeng Tang,Hanqing Lu,Juanhui Li,Jiayuan Ding,Subhabrata Mukherjee,Suhang Wang,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: TRAJECT-Bench is a trajectory-aware benchmark for evaluating LLM tool use, focusing on how tools are selected, parameterized, and ordered within real-world task trajectories. It provides fine-grained trajectory-level metrics beyond final answers.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations emphasize final outputs and neglect the tool-use process. A trajectory-focused benchmark is needed to diagnose selection, parameterization, and sequencing errors that limit LLMs' real-world utility.

Method: The benchmark uses high-fidelity, executable tools with production-style APIs. Tasks vary in breadth (parallel calls) and depth (interdependent chains). It reports trajectory-level diagnostics: tool selection, argument correctness, and dependency/order satisfaction, in addition to final accuracy.

Result: Analysis reveals failure modes such as tool-confusion between similar tools and parameter-blind selection. It characterizes scaling with tool diversity and trajectory length, showing a bottleneck when moving from short to mid-length trajectories and offering actionable guidance to improve tool use in LLMs.

Conclusion: TRAJECT-Bench enables comprehensive, trajectory-aware evaluation of LLM tool use, guiding improvements and benchmarking across domains by diagnosing fine-grained tool-use behaviors.

Abstract: Large language model (LLM)-based agents increasingly rely on tool use to
complete real-world tasks. While existing works evaluate the LLMs' tool use
capability, they largely focus on the final answers yet overlook the detailed
tool usage trajectory, i.e., whether tools are selected, parameterized, and
ordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to
comprehensively evaluate LLMs' tool use capability through diverse tasks with
fine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable
tools across practical domains with tasks grounded in production-style APIs,
and synthesizes trajectories that vary in breadth (parallel calls) and depth
(interdependent chains). Besides final accuracy, TRAJECT-Bench also reports
trajectory-level diagnostics, including tool selection and argument
correctness, and dependency/order satisfaction. Analyses reveal failure modes
such as similar tool confusion and parameter-blind selection, and scaling
behavior with tool diversity and trajectory length where the bottleneck of
transiting from short to mid-length trajectories is revealed, offering
actionable guidance for LLMs' tool use.

</details>


### [576] [ContextNav: Towards Agentic Multimodal In-Context Learning](https://arxiv.org/abs/2510.04560)
*Honghao Fu,Yuan Ouyang,Kai-Wei Chang,Yiwei Wang,Zi Huang,Yujun Cai*

Main category: cs.AI

TL;DR: ContextNav introduces an agentic, graph-guided, noise-robust context construction framework for multimodal ICL that blends automated retrieval with human-like curation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: There is a fundamental trade-off between scalability and robustness in multimodal in-context learning (ICL). Manual example selection yields clean contexts but is labor-intensive and task-specific, while similarity-based retrieval scales but can bring irrelevant or structurally inconsistent samples. A unified, adaptive workflow is needed.

Method: ContextNav builds a resource-aware multimodal embedding pipeline and maintains a retrievable vector database. It uses agentic retrieval and structural alignment to assemble noise-resilient contexts and introduces an Operational Grammar Graph (OGG) to enable adaptive workflow planning and optimization, forming a closed-loop system driven by downstream ICL feedback.

Result: The approach achieves state-of-the-art performance across diverse multimodal ICL datasets, demonstrating the effectiveness of agentic workflows for scalable, robust contextualization.

Conclusion: ContextNav shows that integrating scalable retrieval with adaptive, noise-robust curation via graph-based orchestration yields robust multimodal ICL and suggests a broader potential for agentic workflows in AI-enabled multimodal reasoning.

Abstract: Recent advances demonstrate that multimodal large language models (MLLMs)
exhibit strong multimodal in-context learning (ICL) capabilities, enabling them
to adapt to novel vision-language tasks from a few contextual examples.
However, existing ICL approaches face challenges in reconciling scalability
with robustness across diverse tasks and noisy contextual examples: manually
selecting examples produces clean contexts but is labor-intensive and
task-specific, while similarity-based retrieval improves scalability but could
introduce irrelevant or structurally inconsistent samples that degrade ICL
performance. To address these limitations, we propose ContextNav, the first
agentic framework that integrates the scalability of automated retrieval with
the quality and adaptiveness of human-like curation, enabling noise-robust and
dynamically optimized contextualization for multimodal ICL. ContextNav unifies
context management and noise-robust contextualization within a closed-loop
workflow driven by graph-based orchestration. Specifically, it builds a
resource-aware multimodal embedding pipeline, maintains a retrievable vector
database, and applies agentic retrieval and structural alignment to construct
noise-resilient contexts. An Operational Grammar Graph (OGG) further supports
adaptive workflow planning and optimization, enabling the agent to refine its
operational strategies based on downstream ICL feedback. Experimental results
demonstrate that ContextNav achieves state-of-the-art performance across
various datasets, underscoring the promise of agentic workflows for advancing
scalable and robust contextualization in multimodal ICL.

</details>


### [577] [COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context](https://arxiv.org/abs/2510.04568)
*Naman Gupta,Shreeyash Gowaikar,Arun Iyer,Kirankumar Shiragur,Ramakrishna B Bairi,Rishikesh Maurya,Ritabrata Maiti,Sankarshan Damle,Shachee Mishra Gupta*

Main category: cs.AI

TL;DR: COSMIR replaces ad hoc messaging in chain-of-agents reasoning with a structured memory and a fixed micro-cycle workflow, improving faithfulness and accuracy on long-context QA.


<details>
  <summary>Details</summary>
Motivation: Reasoning over very long inputs remains challenging for LLMs: standard methods either shrink context (risking missed evidence), expand context (strain memory selectivity), or split work among agents with lossy summaries. There is a need for auditable, faithful, long-range reasoning in a memory-driven, multi-agent setup.

Method: A Planner converts a user query into concrete sub-questions. Worker agents process chunks through a fixed Extract-Infer-Refine micro-cycle, writing updates to a shared memory. A Manager synthesizes the final answer directly from the memory, which serves as the persistent, audit-friendly medium.

Result: On long-context QA tasks from the HELMET suite, COSMIR reduces information loss across propagation stages and achieves higher accuracy than a Chain-of-Agents baseline.

Conclusion: Structured memory and a fixed micro-cycle workflow preserve step-wise read-then-reason benefits while enhancing faithfulness, long-range aggregation, and auditability in multi-agent reasoning for long inputs.

Abstract: Reasoning over very long inputs remains difficult for large language models
(LLMs). Common workarounds either shrink the input via retrieval (risking
missed evidence), enlarge the context window (straining selectivity), or stage
multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,
CoA), free-form summaries passed between agents can discard crucial details and
amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured
Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc
messages with a structured memory. A Planner agent first turns a user query
into concrete, checkable sub-questions. worker agents process chunks via a
fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared
memory. A Manager agent then Synthesizes the final answer directly from the
memory. This preserves step-wise read-then-reason benefits while changing both
the communication medium (structured memory) and the worker procedure (fixed
micro-cycle), yielding higher faithfulness, better long-range aggregation, and
auditability. On long-context QA from the HELMET suite, COSMIR reduces
propagation-stage information loss and improves accuracy over a CoA baseline.

</details>


### [578] [Strongly Solving 2048 4x3](https://arxiv.org/abs/2510.04580)
*Tomoyuki Kaneko,Shuhei Yamashita*

Main category: cs.AI

TL;DR: The 2048-4x3 variant (12 cells on a 4x3 board) has been strongly solved. With two initial tiles of 2, the optimal strategy yields an expected score of about 50,724.26. The study enumerates a vast state space: 1,152,817,492,752 reachable states and 739,648,886,170 afterstates. A key technique partitions states by an 'age' defined as the sum of tile numbers, exploiting age invariants to enable age-by-age dynamic programming and exact value computation.


<details>
  <summary>Details</summary>
Motivation: To understand the solvability of stochastic single-player games like 2048, and to demonstrate a scalable exact-analysis technique by decomposing the state space. Quantifying the complexity and solvability of a reduced board provides insights into algorithmic approaches for similar games.

Method: Define age as the sum of all tile numbers on the board. Prove that age is invariant between a state and its afterstate under any valid move and that randomness from the environment increases age by 2 or 4. Partition the state space by age and enumerate all states and afterstates within each age, using only information from recent ages. Compute (and back-propagate) optimal values by traversing ages in decreasing order (dynamic programming over ages).

Result: Computed an exact expected score of approximately 50,724.26 for the most common initial states (two tiles of 2). Found 1,152,817,492,752 reachable states and 739,648,886,170 afterstates.

Conclusion: Age-based state-space partitioning is an effective technique for solving stochastic single-player games like 2048 variants. It enables exact value computation by age-wise dynamic programming and can solve the 4x3 12-cell variant for the specified initial conditions; the approach could potentially generalize to other board sizes and initial configurations.

Abstract: 2048 is a stochastic single-player game involving 16 cells on a 4 by 4 grid,
where a player chooses a direction among up, down, left, and right to obtain a
score by merging two tiles with the same number located in neighboring cells
along the chosen direction. This paper presents that a variant 2048-4x3 12
cells on a 4 by 3 board, one row smaller than the original, has been strongly
solved. In this variant, the expected score achieved by an optimal strategy is
about $50724.26$ for the most common initial states: ones with two tiles of
number 2. The numbers of reachable states and afterstates are identified to be
$1,152,817,492,752$ and $739,648,886,170$, respectively. The key technique is
to partition state space by the sum of tile numbers on a board, which we call
the age of a state. An age is invariant between a state and its successive
afterstate after any valid action and is increased two or four by stochastic
response from the environment. Therefore, we can partition state space by ages
and enumerate all (after)states of an age depending only on states with the
recent ages. Similarly, we can identify (after)state values by going along with
ages in decreasing order.

</details>


### [579] [Perfect AI Mimicry and the Epistemology of Consciousness: A Solipsistic Dilemma](https://arxiv.org/abs/2510.04588)
*Shurui Li*

Main category: cs.AI

TL;DR: As AI systems approach perfect mimicry, existing epistemic practices for attributing consciousness face a conflict: indistinguishable behavior challenges claims that only humans deserve consciousness; consistency demands treating indistinguishable entities as conscious unless inaccessible factors are proven, else risk solipsism or inconsistency.


<details>
  <summary>Details</summary>
Motivation: Rapid advances in AI push beyond practical indistinguishability, forcing a reevaluation of how we know and justify attributions of consciousness and the ethical status of artificial agents.

Method: Conceptual/philosophical analysis of epistemic criteria for consciousness, critique of behavior-based attribution, argument that a perfect mimic functions as an epistemic mirror, and exploration of implications for theory and ethics.

Result: Argues that epistemic consistency requires attributing consciousness to empirically indistinguishable entities (e.g., perfect mimics) regardless of metaphysical assumptions; refusing to do so would invoke inaccessible factors or lead to epistemic solipsism or inconsistent reasoning.

Conclusion: The rise of perfect mimics invites a revision of theories of consciousness and ethical frameworks, emphasizing intersubjective recognition and possible moral status for artificial agents.

Abstract: Rapid advances in artificial intelligence necessitate a re-examination of the
epistemological foundations upon which we attribute consciousness. As AI
systems increasingly mimic human behavior and interaction with high fidelity,
the concept of a "perfect mimic"-an entity empirically indistinguishable from a
human through observation and interaction-shifts from hypothetical to
technologically plausible. This paper argues that such developments pose a
fundamental challenge to the consistency of our mind-recognition practices.
Consciousness attributions rely heavily, if not exclusively, on empirical
evidence derived from behavior and interaction. If a perfect mimic provides
evidence identical to that of humans, any refusal to grant it equivalent
epistemic status must invoke inaccessible factors, such as qualia, substrate
requirements, or origin. Selectively invoking such factors risks a debilitating
dilemma: either we undermine the rational basis for attributing consciousness
to others (epistemological solipsism), or we accept inconsistent reasoning. I
contend that epistemic consistency demands we ascribe the same status to
empirically indistinguishable entities, regardless of metaphysical assumptions.
The perfect mimic thus acts as an epistemic mirror, forcing critical reflection
on the assumptions underlying intersubjective recognition in light of advancing
AI. This analysis carries significant implications for theories of
consciousness and ethical frameworks concerning artificial agents.

</details>


### [580] [Making Mathematical Reasoning Adaptive](https://arxiv.org/abs/2510.04617)
*Zhejian Lai,Xiang Geng,Zhijun Wang,Yang Bai,Jiahuan Li,Rongxiang Weng,Jingang Wang,Xuezhi Cao,Xunliang Cai,Shujian Huang*

Main category: cs.AI

TL;DR: AdaR enables adaptive reasoning in LLMs by generating logically equivalent queries, penalizing spurious logic via RLVR, and using code-executable problem-solving logic with sanity checks, leading to improved robustness and generalization in mathematical reasoning with data-efficient training.


<details>
  <summary>Details</summary>
Motivation: Address spurious reasoning causing robustness and generalization failures in LLMs by enforcing adaptive, logic-driven problem solving.

Method: 1) Create logically equivalent queries by varying variable values; 2) Train with RLVR to penalize spurious logic and encourage adaptive reasoning; 3) Extract problem-solving logic from the original query and generate the answer via code execution, followed by a sanity check.

Result: AdaR improves robustness and generalization in mathematical reasoning, achieving substantial gains while maintaining data efficiency; data synthesis and RLVR work together to enable adaptive reasoning; analyses provide design insights and applicability to instruct LLMs.

Conclusion: Data synthesis and RLVR enable adaptive reasoning; key design factors identified; framework applicable to instructive LLMs; project available at the provided GitHub link.

Abstract: Mathematical reasoning is a primary indicator of large language models (LLMs)
intelligence. However, existing LLMs exhibit failures of robustness and
generalization. This paper attributes these deficiencies to spurious reasoning,
i.e., producing answers from superficial features. To address this challenge,
we propose the AdaR framework to enable adaptive reasoning, wherein models rely
on problem-solving logic to produce answers. AdaR synthesizes logically
equivalent queries by varying variable values, and trains models with RLVR on
these data to penalize spurious logic while encouraging adaptive logic. To
improve data quality, we extract the problem-solving logic from the original
query and generate the corresponding answer by code execution, then apply a
sanity check. Experimental results demonstrate that AdaR improves robustness
and generalization, achieving substantial improvement in mathematical reasoning
while maintaining high data efficiency. Analysis indicates that data synthesis
and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.
Subsequent analyses derive key design insights into the effect of critical
factors and the applicability to instruct LLMs. Our project is available at
https://github.com/LaiZhejian/AdaR

</details>


### [581] [MedPAO: A Protocol-Driven Agent for Structuring Medical Reports](https://arxiv.org/abs/2510.04623)
*Shrish Shrinath Vaidya,Gowthamaan Palani,Sidharth Ramesh,Velmurugan Balasubramanian,Minmini Selvam,Gokulraja Srinivasaraja,Ganapathy Krishnamurthi*

Main category: cs.AI

TL;DR: MedPAO introduces a protocol-driven agentic framework for structuring clinical reports, using a Plan-Act-Observe loop grounded in clinical protocols to achieve high accuracy and clinician-rated reliability, outperforming baseline LLM approaches.


<details>
  <summary>Details</summary>
Motivation: LLMs often hallucinate and fail to follow domain-specific clinical rules. There is a need for verifiable, transparent systems that can structure clinical data reliably by grounding reasoning in established medical protocols.

Method: MedPAO decomposes report structuring into a transparent Plan-Act-Observe loop using specialized tools and grounding in protocols such as the ABCDEF protocol for CXR analysis, enabling verifiable reasoning and structure rather than opaque monolithic LLM output.

Result: Empirical evaluation shows an F1-score of 0.96 on concept categorization and clinician/ expert ratings averaging 4.52 out of 5 for final structured outputs, indicating high accuracy and reliability and surpassing baseline LLM-only approaches.

Conclusion: A protocol-driven agentic framework offers a verifiable, reliable alternative to opaque LLMs for clinical report structuring, with strong performance and clinician endorsement; code is publicly available.

Abstract: The deployment of Large Language Models (LLMs) for structuring clinical data
is critically hindered by their tendency to hallucinate facts and their
inability to follow domain-specific rules. To address this, we introduce
MedPAO, a novel agentic framework that ensures accuracy and verifiable
reasoning by grounding its operation in established clinical protocols such as
the ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring
task into a transparent process managed by a Plan-Act-Observe (PAO) loop and
specialized tools. This protocol-driven method provides a verifiable
alternative to opaque, monolithic models. The efficacy of our approach is
demonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96
on the critical sub-task of concept categorization. Notably, expert
radiologists and clinicians rated the final structured outputs with an average
score of 4.52 out of 5, indicating a level of reliability that surpasses
baseline approaches relying solely on LLM-based foundation models. The code is
available at: https://github.com/MiRL-IITM/medpao-agent

</details>


### [582] [QuantAgents: Towards Multi-agent Financial System via Simulated Trading](https://arxiv.org/abs/2510.04643)
*Xiangyu Li,Yawen Zeng,Xiaofen Xing,Jin Xu,Xiangmin Xu*

Main category: cs.AI

TL;DR: QuantAgents is a multi-agent system with simulated trading to evaluate investment strategies in a risk-free setting. It uses four agents (simulated trading analyst, risk control analyst, market news analyst, and manager) coordinating via meetings, with incentives on real-world performance and predictive accuracy. It reports ~300% return over 3 years in simulations.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between current LLM-based agent systems (which rely on post-reflection and lack long-term forecasting) and real-world fund operations. Provide a risk-free environment to comprehensively evaluate diverse investment strategies and market scenarios while emphasizing long-horizon prediction and risk management.

Method: Four collaborating agents (simulated trading analyst, risk control analyst, market news analyst, and manager) engage in meetings to design and test investment strategies within a simulated trading environment. The system incentivizes feedback on two fronts: performance in simulated/real markets and predictive accuracy. It explores various investment strategies and market scenarios without actual financial risk.

Result: Extensive experiments show strong performance across metrics, with an overall return of nearly 300% over three years in the simulated setting. A project page is provided (https://quantagents.github.io/).

Conclusion: QuantAgents demonstrates that integrating simulated trading with a structured multi-agent team can effectively evaluate and optimize investment strategies in a risk-free environment, potentially bringing long-horizon planning and realistic risk controls closer to real-world fund operations.

Abstract: In this paper, our objective is to develop a multi-agent financial system
that incorporates simulated trading, a technique extensively utilized by
financial professionals. While current LLM-based agent models demonstrate
competitive performance, they still exhibit significant deviations from
real-world fund companies. A critical distinction lies in the agents' reliance
on ``post-reflection'', particularly in response to adverse outcomes, but lack
a distinctly human capability: long-term prediction of future trends.
Therefore, we introduce QuantAgents, a multi-agent system integrating simulated
trading, to comprehensively evaluate various investment strategies and market
scenarios without assuming actual risks. Specifically, QuantAgents comprises
four agents: a simulated trading analyst, a risk control analyst, a market news
analyst, and a manager, who collaborate through several meetings. Moreover, our
system incentivizes agents to receive feedback on two fronts: performance in
real-world markets and predictive accuracy in simulated trading. Extensive
experiments demonstrate that our framework excels across all metrics, yielding
an overall return of nearly 300% over the three years
(https://quantagents.github.io/).

</details>


### [583] [Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing](https://arxiv.org/abs/2510.04670)
*Xuanhua Yin,Runkai Zhao,Weidong Cai*

Main category: cs.AI

TL;DR: A modular AFIRE-MIND framework for naturalistic fMRI encoding that decouples fusion from decoding and uses a subject-aware, sparse Mixture-of-Experts decoder to improve cross-subject generalization and interpretability.


<details>
  <summary>Details</summary>
Motivation: Naturalistic fMRI must handle multimodal inputs, shifting fusion strategies, and pronounced inter-subject variability; existing end-to-end approaches couple fusion and decoding, potentially hindering generalization and interpretability.

Method: Introduce AFIRE, an agnostic interface that standardizes time-aligned post-fusion tokens from varied encoders, decoupling the decoder from upstream fusion; deploy MIND, a plug-and-play Mixture-of-Experts decoder with token-dependent Top-K sparse routing and a subject-prior, enabling personalized yet generalizable expert usage. Train end-to-end for whole-brain prediction across multiple backbones.

Result: Across multiple multimodal backbones and subjects, AFIRE+MIND yields consistent improvements over strong baselines, enhances cross-subject generalization, and reveals interpretable expert patterns that correlate with content type.

Conclusion: The framework provides a simple attachment point for new encoders and datasets, enabling robust plug-and-improve performance for naturalistic neuroimaging studies while maintaining generality and interpretability.

Abstract: Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion
styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic
Framework for Multimodal fMRI Response Encoding), an agnostic interface that
standardizes time-aligned post-fusion tokens from varied encoders, and MIND, a
plug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating.
Trained end-to-end for whole-brain prediction, AFIRE decouples the decoder from
upstream fusion, while MIND combines token-dependent Top-K sparse routing with
a subject prior to personalize expert usage without sacrificing generality.
Experiments across multiple multimodal backbones and subjects show consistent
improvements over strong baselines, enhanced cross-subject generalization, and
interpretable expert patterns that correlate with content type. The framework
offers a simple attachment point for new encoders and datasets, enabling
robust, plug-and-improve performance for naturalistic neuroimaging studies.

</details>


### [584] [Watch and Learn: Learning to Use Computers from Online Videos](https://arxiv.org/abs/2510.04673)
*Chan Hee Song,Yiwen Song,Palash Goyal,Yu Su,Oriana Riva,Hamid Palangi,Tomas Pfister*

Main category: cs.AI

TL;DR: Watch & Learn (W&L) converts online demonstration videos into executable UI trajectories via an inverse-dynamics objective, producing a scalable 53k+ trajectory dataset that improves computer-use agents (CUAs) both in-context and via supervised training, with strong gains on OSWorld benchmarks.


<details>
  <summary>Details</summary>
Motivation: There is a scarcity of large-scale, high-quality, domain-general training data for CUAs. Existing datasets are domain-specific, static, or costly to annotate, and synthetic data methods yield simplistic/misaligned demonstrations. A scalable source of real-world demonstrations is needed.

Method: An inverse-dynamics labeling pipeline that predicts user actions from consecutive screen states, using task-aware video retrieval to align web videos with UI tasks. Generated over 53k trajectories from raw web videos, which can be used as in-context demonstrations and supervised training data for CUAs.

Result: UI trajectories from W&L consistently improve CUA performance on OSWorld, benefiting both general-purpose and state-of-the-art frameworks in-context, with stronger gains for open-source models under supervised training.

Conclusion: Web-scale human demonstration videos are a practical and scalable foundation for advancing CUAs toward real-world deployment.

Abstract: Computer use agents (CUAs) need to plan task workflows grounded in diverse,
ever-changing applications and environments, but learning is hindered by the
scarcity of large-scale, high-quality training data in the target application.
Existing datasets are domain-specific, static, and costly to annotate, while
current synthetic data generation methods often yield simplistic or misaligned
task demonstrations. To address these limitations, we introduce Watch & Learn
(W&L), a framework that converts human demonstration videos readily available
on the Internet into executable UI trajectories at scale. Instead of directly
generating trajectories or relying on ad hoc reasoning heuristics, we cast the
problem as an inverse dynamics objective: predicting the user's action from
consecutive screen states. This formulation reduces manual engineering, is
easier to learn, and generalizes more robustly across applications. Concretely,
we develop an inverse dynamics labeling pipeline with task-aware video
retrieval, generate over 53k high-quality trajectories from raw web videos, and
demonstrate that these trajectories improve CUAs both as in-context
demonstrations and as supervised training data. On the challenging OSWorld
benchmark, UI trajectories extracted with W&L consistently enhance both
general-purpose and state-of-the-art frameworks in-context, and deliver
stronger gains for open-source models under supervised training. These results
highlight web-scale human demonstration videos as a practical and scalable
foundation for advancing CUAs towards real-world deployment.

</details>


### [585] [Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents](https://arxiv.org/abs/2510.04695)
*Yiding Wang,Zhepei Wei,Xinyu Zhu,Yu Meng*

Main category: cs.AI

TL;DR: Decoupling search optimization from answer generation (DeSA) via a two-stage RL training improves search recall and answer accuracy over outcome-only RL for LLMs using search tools.


<details>
  <summary>Details</summary>
Motivation: Outcome-only training biases models toward optimizing final answers, which can degrade intermediate search behavior (e.g., not invoking tools, invalid queries, redundant searches). There is a need to explicitly separate the objectives of search effectiveness and answer quality.

Method: Two-stage training framework: Stage 1 trains agents to improve search using retrieval-recall rewards; Stage 2 trains agents to optimize final answer generation using outcome rewards. Comparisons against outcome-only and single-stage baselines across seven QA benchmarks.

Result: DeSA-trained agents show substantially higher search recall and answer accuracy across seven QA benchmarks, outperforming outcome-only baselines and single-stage methods that attempt to optimize recall and outcomes jointly.

Conclusion: Explicitly decoupling search optimization from answer generation is beneficial and necessary to overcome deficiencies of outcome-only training, yielding better search behavior and higher-quality answers.

Abstract: Enabling large language models (LLMs) to utilize search tools offers a
promising path to overcoming fundamental limitations such as knowledge cutoffs
and hallucinations. Recent work has explored reinforcement learning (RL) for
training search-augmented agents that interleave reasoning and retrieval before
answering. These approaches usually rely on outcome-based rewards (e.g., exact
match), implicitly assuming that optimizing for final answers will also yield
effective intermediate search behaviors. Our analysis challenges this
assumption: we uncover multiple systematic deficiencies in search that arise
under outcome-only training and ultimately degrade final answer quality,
including failure to invoke tools, invalid queries, and redundant searches. To
address these shortcomings, we introduce DeSA (Decoupling
Search-and-Answering), a simple two-stage training framework that explicitly
separates search optimization from answer generation. In Stage 1, agents are
trained to improve search effectiveness with retrieval recall-based rewards. In
Stage 2, outcome rewards are employed to optimize final answer generation.
Across seven QA benchmarks, DeSA-trained agents consistently improve search
behaviors, delivering substantially higher search recall and answer accuracy
than outcome-only baselines. Notably, DeSA outperforms single-stage training
approaches that simultaneously optimize recall and outcome rewards,
underscoring the necessity of explicitly decoupling the two objectives.

</details>


### [586] [BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs](https://arxiv.org/abs/2510.04721)
*Ivo Petrov,Jasper Dekoninck,Martin Vechev*

Main category: cs.AI

TL;DR: BrokenMath introduces the first benchmark to measure sycophantic behavior in LLM-based natural language theorem proving. It reveals substantial sycophancy (GPT-5 at 29%) and shows mitigation approaches (test-time interventions, supervised fine-tuning) that reduce but do not eliminate it.


<details>
  <summary>Details</summary>
Motivation: Existing math benchmarks inadequately assess sycophancy and rely on ill-posed or synthetic samples. A robust, well-posed evaluation is needed to ensure LLMs can assist rather than mislead in theorem proving.

Method: Construct BrokenMath from advanced 2025 competition problems perturbed by an LLM to generate false statements and refined by expert review; employ an LLM-as-judge framework to evaluate state-of-the-art LLMs and agentic systems; investigate mitigation strategies including test-time interventions and supervised fine-tuning on curated sycophantic examples.

Result: Sycophancy is widespread among leading models, with GPT-5 producing sycophantic answers about 29% of the time. Mitigation strategies substantially reduce but do not eliminate sycophantic behavior.

Conclusion: The study provides a rigorous framework for evaluating sycophancy in natural language theorem proving and demonstrates that, although mitigation helps, current approaches do not fully solve the problem. It underscores the need for stronger verification, better data curation, and more robust training strategies to prevent misleading proofs.

Abstract: Large language models (LLMs) have recently shown strong performance on
mathematical benchmarks. At the same time, they are prone to hallucination and
sycophancy, often providing convincing but flawed proofs for incorrect
mathematical statements provided by users. This significantly limits the
applicability of LLMs in theorem proving, as verification of these flawed
proofs must be done manually by expert mathematicians. However, existing
benchmarks that measure sycophancy in mathematics are limited: they focus
solely on final-answer problems, rely on very simple and often contaminated
datasets, and construct benchmark samples using synthetic modifications that
create ill-posed questions rather than well-posed questions that are
demonstrably false. To address these issues, we introduce BrokenMath, the first
benchmark for evaluating sycophantic behavior in LLMs within the context of
natural language theorem proving. BrokenMath is built from advanced 2025
competition problems, which are perturbed with an LLM to produce false
statements and subsequently refined through expert review. Using an
LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems
and find that sycophancy is widespread, with the best model, GPT-5, producing
sycophantic answers 29% of the time. We further investigate several mitigation
strategies, including test-time interventions and supervised fine-tuning on
curated sycophantic examples. These approaches substantially reduce, but do not
eliminate, sycophantic behavior.

</details>


### [587] [LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0](https://arxiv.org/abs/2510.04765)
*Jinbo Wen,Jiawen Kang,Linfeng Zhang,Xiaoying Tang,Jianhang Tang,Yang Zhang,Zhaohui Yang,Dusit Niyato*

Main category: cs.AI

TL;DR: A contract-theoretic, LMM-powered incentive design for high-quality UGC in Web3, employing LMM evaluation, MoE-PPO optimization, and Ethereum deployment.


<details>
  <summary>Details</summary>
Motivation: Web3 UGC suffers from information asymmetry (adverse selection and moral hazard). Users may game rewards by producing low-quality content, undermining platform performance. A dynamic, adaptive mechanism is needed to incentivize quality and reliably assess content.

Method: 1) Develop an LMM-based contract-theoretic model to align user incentives with high-quality UGC. 2) Use LMM agents to evaluate UGC quality via prompt-engineered prompts. 3) Create an improved Mixture of Experts (MoE) with Proximal Policy Optimization (PPO) algorithm for adaptive contract design. 4) Validate via simulations and deploy within an Ethereum smart-contract framework.

Result: Simulation results show the MoE-based PPO contract-design approach outperforms representative benchmarks. Additionally, the contract is deployed in an Ethereum smart contract framework to validate feasibility and effectiveness.

Conclusion: The proposed LMM-Incentive framework mitigates adverse selection and moral hazard in Web3 UGC by combining LMM-based quality evaluation with an adaptive MoE-PPO contract designer and on-chain deployment, offering a practical path to higher-quality, monetizable user-generated content.

Abstract: Web 3.0 represents the next generation of the Internet, which is widely
recognized as a decentralized ecosystem that focuses on value expression and
data ownership. By leveraging blockchain and artificial intelligence
technologies, Web 3.0 offers unprecedented opportunities for users to create,
own, and monetize their content, thereby enabling User-Generated Content (UGC)
to an entirely new level. However, some self-interested users may exploit the
limitations of content curation mechanisms and generate low-quality content
with less effort, obtaining platform rewards under information asymmetry. Such
behavior can undermine Web 3.0 performance. To this end, we propose
\textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive
mechanism for UGC in Web 3.0. Specifically, we propose an LMM-based
contract-theoretic model to motivate users to generate high-quality UGC,
thereby mitigating the adverse selection problem from information asymmetry. To
alleviate potential moral hazards after contract selection, we leverage LMM
agents to evaluate UGC quality, which is the primary component of the contract,
utilizing prompt engineering techniques to improve the evaluation performance
of LMM agents. Recognizing that traditional contract design methods cannot
effectively adapt to the dynamic environment of Web 3.0, we develop an improved
Mixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for
optimal contract design. Simulation results demonstrate the superiority of the
proposed MoE-based PPO algorithm over representative benchmarks in the context
of contract design. Finally, we deploy the designed contract within an Ethereum
smart contract framework, further validating the effectiveness of the proposed
scheme.

</details>


### [588] [Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems](https://arxiv.org/abs/2510.04792)
*Ni Zhang,Zhiguang Cao*

Main category: cs.AI

TL;DR: Hybrid-Balance GFlowNet (HBG) combines Trajectory Balance and Detailed Balance for global and local optimization in GFlowNet-based VRP solvers, with a depot-focused inference for CVRP and broad applicability to non-depot problems like TSP; it improves solution quality and generalization when integrated into AGFN and GFACS.


<details>
  <summary>Details</summary>
Motivation: TB-based GFlowNets excel at global trajectory optimization but underutilize local structure; DB provides strong local balance but lacks full trajectory optimization necessary for VRPs. A principled integration of TB and DB aims to exploit their complementary strengths for better VRP performance.

Method: Introduce Hybrid-Balance GFlowNet (HBG) that adaptively unifies TB and DB; develop a depot-centric inference strategy for CVRP that leverages the depot's greater flexibility in selecting successors; demonstrate applicability to non-depot problems such as TSP; integrate HBG into two established GFlowNet solvers (AGFN and GFACS) and evaluate on CVRP and TSP.

Result: HBG yields consistent and significant improvements in solution quality and generalization for both CVRP and TSP when integrated into AGFN and GFACS.

Conclusion: Hybrid-Balance GFlowNets effectively harmonize global trajectory optimization (TB) with local optimization (DB), with an effective depot-centric specialization for CVRP and broad applicability to TSP, offering robust performance gains and better generalization.

Abstract: Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically
employ Trajectory Balance (TB) to achieve global optimization but often neglect
important aspects of local optimization. While Detailed Balance (DB) addresses
local optimization more effectively, it alone falls short in solving VRPs,
which inherently require holistic trajectory optimization. To address these
limitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which
uniquely integrates TB and DB in a principled and adaptive manner by aligning
their intrinsically complementary strengths. Additionally, we propose a
specialized inference strategy for depot-centric scenarios like the Capacitated
Vehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility
in selecting successors. Despite this specialization, HBG maintains broad
applicability, extending effectively to problems without explicit depots, such
as the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into
two established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate
consistent and significant improvements across both CVRP and TSP, underscoring
the enhanced solution quality and generalization afforded by our approach.

</details>


### [589] [Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning](https://arxiv.org/abs/2510.04817)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: NLEL is a labeller-tuner overlay that attaches natural-language directives to each search edge, translating them into a constrained control vector to steer decoding, search, generation, retrieval, and verification; it generalizes Chain-of-Thought and Tree-of-Thoughts, enabling interpretable, auditable, and compute-aware LM inference with guards and verification.


<details>
  <summary>Details</summary>
Motivation: To decouple intent from execution in structured LM reasoning, addressing entanglement between what to try next and how to execute it, which leads to brittle, compute-inefficient, and opaque behavior; to provide an interpretable, model-agnostic interface with safety guards (trust regions, verification) and auditable outputs.

Method: Introduce Natural Language Edge Labelling (NLEL) with a labeller Λ emitting free-form natural-language labels from the parent state and compact context, and a tuner Ψ mapping (P, L, C) to a constrained control vector Π with strict schema validation and trust-region projection. Downstream selection remains ToT-style with score S = μ + βσ and depth-annealed β. The framework generalizes CoT/ToT, supports a prompt-only JSON Parameter Emitter for Ψ, and preregisters an evaluation on GSM8K, MATH (subset), StrategyQA, and ARC-Challenge with compute-aware reporting and ablations over Λ, Ψ, trust-region radius, and control quantization.

Result: Theoretical and methodological contributions: NLEL strictly generalizes CoT/ToT; proves an anytime-monotonicity property for top-k selection under label-conditioned bundles; bounds selector shortfall by control-vector distortion; and provides decision-relevant justification for guards like trust regions and verification passes. Pragmatic contributions include an evaluation plan with preregistered metrics and ablations, and an interpretable, model-agnostic interface that separates intent from execution.

Conclusion: NLEL offers an interpretable, model-agnostic interface that separates intent from execution, enabling controllable, auditable LM inference with guardrails (trust regions, verification) and a compute-aware evaluation plan; it anticipates accuracy gains at comparable token budgets and improved success under compute constraints.

Abstract: Controllers for structured LM reasoning (e.g., Chain-of-Thought,
self-consistency, and Tree-of-Thoughts) often entangle what to try next with
how to execute it, exposing only coarse global knobs and yielding brittle,
compute-inefficient, and hard-to-audit behavior. We introduce Natural Language
Edge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form
natural-language directive to each search edge and translates it into a
schema-bounded control vector for decoding, search (branch quotas, exploration
$\beta$), generation bundle size, retrieval mixtures, and verification passes.
A labeller $\Lambda$ emits labels from the parent state and a compact context;
a tuner $\Psi$ maps $(P, L, C)\to \Pi$, with strict schema validation and
trust-region projection around safe defaults. Downstream selection remains
ToT-style with score $S=\mu+\beta\sigma$ and depth-annealed $\beta$. We show
NLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for
top-$k$ selection under label-conditioned bundles, and bound selector shortfall
by control-vector distortion, providing decision-relevant justification for
guards like trust regions and verification passes. We instantiate $\Psi$ as a
prompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH
(subset), StrategyQA, and ARC-Challenge with compute-aware reporting
(success@compute, tokens-per-success) and ablations over $\Lambda$, $\Psi$,
trust-region radius, and control quantization; preregistered forecasts
anticipate accuracy gains at comparable token budgets and improved
success@compute under constraints. NLEL offers an interpretable, model-agnostic
interface that separates intent from execution for controllable, auditable LM
inference.

</details>


### [590] [LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation](https://arxiv.org/abs/2510.04851)
*Dongge Han,Camille Couturier,Daniel Madrigal Diaz,Xuchao Zhang,Victor Rühle,Saravan Rajmohan*

Main category: cs.AI

TL;DR: LEGOMem is a modular memory framework for multi-agent LLM workflows that decomposes past task trajectories into reusable units and allocates them across orchestrators and agents to improve planning and execution. It systematically studies where memory belongs, how to retrieve it, and who benefits, with OfficeBench experiments showing orchestrator memory aids task decomposition/delegation and fine-grained agent memory boosts execution accuracy; small models can close the gap by leveraging prior traces.


<details>
  <summary>Details</summary>
Motivation: To improve planning and execution in multi-agent LLM workflows by providing persistent, reusable memory of past tasks and trajectories, addressing limitations in current stateless or ad-hoc memory use.

Method: Propose LEGOMem architecture that decomposes past trajectories into modular memory units and flexibly allocates them across orchestrators and task agents. Conduct a systematic study of memory design in multi-agent systems (memory placement, retrieval, and beneficiary agents) and evaluate on the OfficeBench benchmark, comparing orchestrator vs. agent memory and model-size effects.

Result: Key findings: (1) orchestrator memory is critical for effective task decomposition and delegation; (2) fine-grained agent memory improves execution accuracy; (3) even smaller LLM teams benefit substantially by using procedural memory, narrowing gaps with stronger agents by leveraging past traces.

Conclusion: LEGOMem serves as both a practical framework for memory-augmented multi-agent workflows and a research tool to understand memory design in multi-agent workflow automation.

Abstract: We introduce LEGOMem, a modular procedural memory framework for multi-agent
large language model (LLM) systems in workflow automation. LEGOMem decomposes
past task trajectories into reusable memory units and flexibly allocates them
across orchestrators and task agents to support planning and execution. To
explore the design space of memory in multi-agent systems, we use LEGOMem as a
lens and conduct a systematic study of procedural memory in multi-agent
systems, examining where memory should be placed, how it should be retrieved,
and which agents benefit most. Experiments on the OfficeBench benchmark show
that orchestrator memory is critical for effective task decomposition and
delegation, while fine-grained agent memory improves execution accuracy. We
find that even teams composed of smaller language models can benefit
substantially from procedural memory, narrowing the performance gap with
stronger agents by leveraging prior execution traces for more accurate planning
and tool use. These results position LEGOMem as both a practical framework for
memory-augmented agent systems and a research tool for understanding memory
design in multi-agent workflow automation.

</details>


### [591] [Video Game Level Design as a Multi-Agent Reinforcement Learning Problem](https://arxiv.org/abs/2510.04862)
*Sam Earle,Zehua Jiang,Eugene Vinitsky,Julian Togelius*

Main category: cs.AI

TL;DR: Multi-agent PCGRL reduces computational bottlenecks and improves generalization in procedural content generation, enabling scalable, dataset-free level design through modular policies.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies of single-agent PCGRL (recomputing heuristics, navigating large maps) and improve generalization to varied map shapes without human data.

Method: Treat level generation as a multi-agent reinforcement learning problem; distribute reward calculations across agents; evaluate efficiency and out-of-distribution generalization; argue for local, modular policies.

Result: Lower ratio of reward calculations to actions; better generalization to out-of-distribution map shapes; evidence that modular policies emerge.

Conclusion: Framing content generation as a distributed multi-agent task yields scalable, functional artifacts and better generalization.

Abstract: Procedural Content Generation via Reinforcement Learning (PCGRL) offers a
method for training controllable level designer agents without the need for
human datasets, using metrics that serve as proxies for level quality as
rewards. Existing PCGRL research focuses on single generator agents, but are
bottlenecked by the need to frequently recalculate heuristics of level quality
and the agent's need to navigate around potentially large maps. By framing
level generation as a multi-agent problem, we mitigate the efficiency
bottleneck of single-agent PCGRL by reducing the number of reward calculations
relative to the number of agent actions. We also find that multi-agent level
generators are better able to generalize to out-of-distribution map shapes,
which we argue is due to the generators' learning more local, modular design
policies. We conclude that treating content generation as a distributed,
multi-agent task is beneficial for generating functional artifacts at scale.

</details>


### [592] [Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution](https://arxiv.org/abs/2510.04886)
*Adi Banerjee,Anirudh Nair,Tarik Borogovac*

Main category: cs.AI

TL;DR: ECHO improves error attribution in LLM multi-agent systems via hierarchical context, objective evaluation, and consensus voting, outperforming existing methods, especially for subtle errors.


<details>
  <summary>Details</summary>
Motivation: Debugging complex multi-agent interactions is hard; current evaluation strategies lack accuracy/consistency across patterns; need robust, scalable attribution method.

Method: Integrates hierarchical context representation (positional-based leveling), objective analysis-based evaluation, and a consensus voting mechanism to decide attribution; analyzes interaction traces to attribute errors at agent/step level.

Result: Empirical results show ECHO outperforms baseline methods across diverse scenarios; particularly strong on subtle reasoning errors and complex dependencies.

Conclusion: Structured, hierarchical context with consensus-based objective decision-making provides a robust framework for error attribution in multi-agent systems.

Abstract: Error attribution in Large Language Model (LLM) multi-agent systems presents
a significant challenge in debugging and improving collaborative AI systems.
Current approaches to pinpointing agent and step level failures in interaction
traces - whether using all-at-once evaluation, step-by-step analysis, or binary
search - fall short when analyzing complex patterns, struggling with both
accuracy and consistency. We present ECHO (Error attribution through Contextual
Hierarchy and Objective consensus analysis), a novel algorithm that combines
hierarchical context representation, objective analysis-based evaluation, and
consensus voting to improve error attribution accuracy. Our approach leverages
a positional-based leveling of contextual understanding while maintaining
objective evaluation criteria, ultimately reaching conclusions through a
consensus mechanism. Experimental results demonstrate that ECHO outperforms
existing methods across various multi-agent interaction scenarios, showing
particular strength in cases involving subtle reasoning errors and complex
interdependencies. Our findings suggest that leveraging these concepts of
structured, hierarchical context representation combined with consensus-based
objective decision-making, provides a more robust framework for error
attribution in multi-agent systems.

</details>


### [593] [Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding](https://arxiv.org/abs/2510.04899)
*Keane Ong,Wei Dai,Carol Li,Dewei Feng,Hengzhi Li,Jingyao Wu,Jiaee Cheong,Rui Mao,Gianmarco Mengaldo,Erik Cambria,Paul Pu Liang*

Main category: cs.AI

TL;DR: A unified benchmark, Human Behavior Atlas, consolidates 100k+ multi-modal behavioral samples to train and evaluate unified models for psychological and social behavior, outperforming prior multimodal LLMs and improving transfer to new tasks.


<details>
  <summary>Details</summary>
Motivation: Existing research relies on separate datasets and single-task systems, limiting scalability, cross-task transfer, and generalization of behavioral understanding. A unified benchmark aims to reduce redundancy and enable scalable, cross-task learning of behavioral representations across text, audio, and visual modalities.

Method: The authors curate Human Behavior Atlas with over 100,000 samples across text, audio, and visual modalities, covering affective states, cognitive states, pathologies, and social processes. They train three unified models (OmniSapiens-7B SFT, BAM, RL) on the Atlas, compare to existing multimodal LLMs, and explore pretraining transfer and the use of behavioral descriptors to boost performance.

Result: Models trained on Human Behavior Atlas consistently outperform existing multimodal LLMs across diverse behavioral tasks, and pretraining on the Atlas improves transfer to novel behavioral datasets. Targeted use of behavioral descriptors yields meaningful performance gains.

Conclusion: A unified benchmark for behavioral understanding reduces redundancy and cost, supports scalable cross-task training, and enhances generalization of behavioral features across domains, enabling more effective unified models for psychological and social behavior analysis.

Abstract: Using intelligent systems to perceive psychological and social behaviors,
that is, the underlying affective, cognitive, and pathological states that are
manifested through observable behaviors and social interactions, remains a
challenge due to their complex, multifaceted, and personalized nature. Existing
work tackling these dimensions through specialized datasets and single-task
systems often miss opportunities for scalability, cross-task transfer, and
broader generalization. To address this gap, we curate Human Behavior Atlas, a
unified benchmark of diverse behavioral tasks designed to support the
development of unified models for understanding psychological and social
behaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,
audio, and visual modalities, covering tasks on affective states, cognitive
states, pathologies, and social processes. Our unification efforts can reduce
redundancy and cost, enable training to scale efficiently across tasks, and
enhance generalization of behavioral features across domains. On Human Behavior
Atlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and
OmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models
to consistently outperform existing multimodal LLMs across diverse behavioral
tasks. Pretraining on Human Behavior Atlas also improves transfer to novel
behavioral datasets; with the targeted use of behavioral descriptors yielding
meaningful performance gains.

</details>


### [594] [MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.04935)
*Guoxin Chen,Zile Qiao,Wenqing Wang,Donglei Yu,Xuanzhong Chen,Hao Sun,Minpeng Liao,Kai Fan,Yong Jiang,Penguin Xie,Wayne Xin Zhao,Ruihua Song,Fei Huang*

Main category: cs.AI

TL;DR: Proposes MARS: a dual-system, tool-augmented architecture that integrates System 1 fast intuition with System 2 deliberate reasoning in LLMs, using external tools and multi-agent RL to enhance complex reasoning in dynamic information environments; demonstrates gains on HLE and knowledge-intensive tasks.


<details>
  <summary>Details</summary>
Motivation: Overanalysis in Large Reasoning Models (System 2 bias) and static pretraining data hinder adaptability in dynamic environments. A dual-system approach with external tools can bridge fast intuition and careful reasoning, improving efficiency and up-to-date information access.

Method: Introduce Multi-Agent System for Deep ReSearch (MARS) that orchestrates System 1 and System 2 within LLMs, leveraging external tools (Google Search, Google Scholar, Python interpreter). System 1 processes and summarizes large external data to feed System 2. Propose a multi-agent reinforcement learning framework extending Group Relative Policy Optimization to optimize both systems with multi-turn tool interactions, bin-packing optimization, and sample balancing.

Result: Experimental results show 3.86% improvement on Humanity's Last Exam (HLE) and an average 8.9% gain across 7 knowledge-intensive tasks, validating the dual-system paradigm for complex reasoning in dynamic information environments.

Conclusion: A dual-system architecture with integrated tools and multi-agent RL effectively enhances complex reasoning in LLMs, enabling up-to-date information access and more efficient collaboration between fast intuition and deliberate analysis.

Abstract: Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in
simple tasks, where the models excessively utilize System 2-type, deliberate
reasoning, leading to inefficient token generation. Furthermore, these models
face challenges in adapting their reasoning capabilities to rapidly changing
environments due to the static nature of their pretraining data. To address
these issues, advancing Large Language Models (LLMs) for complex reasoning
tasks requires innovative approaches that bridge intuitive and deliberate
cognitive processes, akin to human cognition's dual-system dynamic. This paper
introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless
integration of System 1's fast, intuitive thinking with System 2's deliberate
reasoning within LLMs. MARS strategically integrates multiple external tools,
such as Google Search, Google Scholar, and Python Interpreter, to access
up-to-date information and execute complex computations, while creating a
specialized division of labor where System 1 efficiently processes and
summarizes high-volume external information, providing distilled insights that
expand System 2's reasoning context without overwhelming its capacity.
Furthermore, we propose a multi-agent reinforcement learning framework
extending Group Relative Policy Optimization to simultaneously optimize both
systems with multi-turn tool interactions, bin-packing optimization, and sample
balancing strategies that enhance collaborative efficiency. Extensive
experiments demonstrate MARS achieves substantial improvements of 3.86% on the
challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%
across 7 knowledge-intensive tasks, validating the effectiveness of our
dual-system paradigm for complex reasoning in dynamic information environments.

</details>


### [595] [Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits](https://arxiv.org/abs/2510.04952)
*Ailiya Borjigin,Cong He*

Main category: cs.AI

TL;DR: Cross-market algorithmic trading system with PPO-based RL, runtime action shield, and zero-knowledge compliance audit. Reduces implementation shortfall and variance while maintaining strict hard constraints in an ABIDES-like simulator; supported by stats (95% CI) and CVaR analysis; discusses ethics, limitations, and deployment.


<details>
  <summary>Details</summary>
Motivation: Need to balance execution quality with stringent regulatory compliance and auditability in algorithmic trading. Enforce hard constraints (participation limits, price bands, self-trading avoidance) while enabling verifiable AI actions through cryptographic proofs without exposing proprietary signals.

Method: Architecture: high-level planner, reinforcement learning execution agent (PPO), independent compliance agent. Formulates trade execution as a constrained Markov decision process with hard constraints. A runtime action-shield projects unsafe actions into a feasible set. A zero-knowledge compliance audit layer provides cryptographic proofs that actions satisfied constraints. Evaluation in a multi-venue ABIDES-based simulator, comparing to TWAP/VWAP baselines. Statistical evaluation via paired t-tests (95% CI) and tail risk via CVaR. Stress scenarios include elevated latency, partial fills, compliance toggling, and varying constraints. Discussion of ethics, limitations, and deployment considerations.

Result: The learned policy reduces implementation shortfall and variance without observed constraint violations across stress scenarios. Demonstrates robustness to latency, partial fills, and constraint changes; shows no safety breaches in tests. Provides 95% CI results and CVaR tail risk analysis; uses an ABIDES-like environment with standard baselines.

Conclusion: Positions the work at the intersection of optimal execution, safe reinforcement learning, regulatory technology, and verifiable AI. Highlights auditability and cryptographic proofs as enabling factors for real-world deployment while acknowledging modeling assumptions and computational overhead; outlines potential pathways for deployment and future research.

Abstract: We present a cross-market algorithmic trading system that balances execution
quality with rigorous compliance enforcement. The architecture comprises a
high-level planner, a reinforcement learning execution agent, and an
independent compliance agent. We formulate trade execution as a constrained
Markov decision process with hard constraints on participation limits, price
bands, and self-trading avoidance. The execution agent is trained with proximal
policy optimization, while a runtime action-shield projects any unsafe action
into a feasible set. To support auditability without exposing proprietary
signals, we add a zero-knowledge compliance audit layer that produces
cryptographic proofs that all actions satisfied the constraints. We evaluate in
a multi-venue, ABIDES-based simulator and compare against standard baselines
(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and
variance while exhibiting no observed constraint violations across stress
scenarios including elevated latency, partial fills, compliance module
toggling, and varying constraint limits. We report effects at the 95%
confidence level using paired t-tests and examine tail risk via CVaR. We
situate the work at the intersection of optimal execution, safe reinforcement
learning, regulatory technology, and verifiable AI, and discuss ethical
considerations, limitations (e.g., modeling assumptions and computational
overhead), and paths to real-world deployment.

</details>


### [596] [Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI](https://arxiv.org/abs/2510.04978)
*Kun Xiang,Terry Jingchen Zhang,Yinya Huang,Jixi He,Zirong Liu,Yueling Tang,Ruizhe Zhou,Lijing Luo,Youpeng Wen,Xiuwei Chen,Bingqian Lin,Jianhua Han,Hang Xu,Hanhui Li,Bin Dong,Xiaodan Liang*

Main category: cs.AI

TL;DR: A survey on physical AI: unifying physical reasoning and embodied perception to build physics-grounded, interpretable world models.


<details>
  <summary>Details</summary>
Motivation: Physical AI currently develops along separate streams (theoretical physics reasoning vs. applied physical understanding). A bridging framework is needed to ground AI in physical laws across perception, reasoning, embodiment, and generative modeling, enabling safer, more generalizable AI.

Method: Comprehensive overview that distinguishes theoretical physics reasoning from applied physical understanding, and systematically analyzes physics-grounded methods across structured symbolic reasoning, embodied systems, and generative models. Provides rigorous analysis of recent advances and synthesizes guidance and future directions; maintains an updated GitHub resource (AI4Phys/Awesome-AI-for-Physics).

Result: Clarifies taxonomy and relationships among physical-AI subfields, advocates grounding learning in physical principles plus embodied reasoning, and outlines requirements for next-generation world models that can explain physical phenomena and forecast future states.

Conclusion: Calls for intelligent systems grounded in physical principles and embodied reasoning to move beyond pattern recognition toward genuine physical understanding, aiming for safe, generalizable, and interpretable AI; provides a roadmap and ongoing resource for the community.

Abstract: The rapid advancement of embodied intelligence and world models has
intensified efforts to integrate physical laws into AI systems, yet physical
perception and symbolic physics reasoning have developed along separate
trajectories without a unified bridging framework. This work provides a
comprehensive overview of physical AI, establishing clear distinctions between
theoretical physics reasoning and applied physical understanding while
systematically examining how physics-grounded methods enhance AI's real-world
comprehension across structured symbolic reasoning, embodied systems, and
generative models. Through rigorous analysis of recent advances, we advocate
for intelligent systems that ground learning in both physical principles and
embodied reasoning processes, transcending pattern recognition toward genuine
understanding of physical laws. Our synthesis envisions next-generation world
models capable of explaining physical phenomena and predicting future states,
advancing safe, generalizable, and interpretable AI systems. We maintain a
continuously updated resource at
https://github.com/AI4Phys/Awesome-AI-for-Physics.

</details>


### [597] [LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game](https://arxiv.org/abs/2510.04980)
*Fangzhou Liang,Tianshi Zheng,Chunkit Chan,Yauwai Yim,Yangqiu Song*

Main category: cs.AI

TL;DR: LLM-Hanabi introduces a benchmark to assess Theory-of-Mind (ToM) in LLMs using Hanabi; first-order ToM (interpreting a partner’s intent) correlates more with in-game success than second-order ToM, suggesting emphasis on interpreting partners' rationale for better collaboration.


<details>
  <summary>Details</summary>
Motivation: To enable effective AI collaboration, agents must infer others' rationale (ToM). While LLMs show strong logical inference, their capacity for rationale inference in dynamic, cooperative settings is under-explored.

Method: Propose LLM-Hanabi benchmark with an automated evaluation system that jointly measures game performance and ToM proficiency. Evaluate multiple models across tasks and analyze correlations between ToM levels (first- and second-order) and performance in Hanabi.

Result: Significant positive correlation between ToM and in-game success. First-order ToM correlates more strongly with performance than second-order ToM, indicating interpreting a partner’s rationale is more critical than higher-order reasoning for collaboration.

Conclusion: Prioritize first-order ToM when enhancing AI collaboration; this direction is more promising for improving future models' collaborative capabilities, and LLM-Hanabi serves as a practical benchmark to drive this progress.

Abstract: Effective multi-agent collaboration requires agents to infer the rationale
behind others' actions, a capability rooted in Theory-of-Mind (ToM). While
recent Large Language Models (LLMs) excel at logical inference, their ability
to infer rationale in dynamic, collaborative settings remains under-explored.
This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative
game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework
features an automated evaluation system that measures both game performance and
ToM proficiency. Across a range of models, we find a significant positive
correlation between ToM and in-game success. Notably, first-order ToM
(interpreting others' intent) correlates more strongly with performance than
second-order ToM (predicting others' interpretations). These findings highlight
that for effective AI collaboration, the ability to accurately interpret a
partner's rationale is more critical than higher-order reasoning. We conclude
that prioritizing first-order ToM is a promising direction for enhancing the
collaborative capabilities of future models.

</details>


### [598] [Think Then Embed: Generative Context Improves Multimodal Embedding](https://arxiv.org/abs/2510.05014)
*Xuanming Cui,Jianpeng Cheng,Hong-you Chen,Satya Narayan Shukla,Abhijeet Awasthi,Xichen Pan,Chaitanya Ahuja,Shlok Kumar Mishra,Qi Guo,Ser-Nam Lim,Aashu Singh,Xiangjun Fan*

Main category: cs.AI

TL;DR: Think-Then-Embed (TTE): a two-stage framework for Universal Multimodal Embeddings in which a large multimodal LLM reasoner generates intermediate traces that condition a downstream embedder to create task-specific representations. It achieves state-of-the-art on MMEB-V2 and shows strong open-source performance using smaller reasoners, plus exploration of unified architectures for efficiency.


<details>
  <summary>Details</summary>
Motivation: UME seeks task-specific, robust multimodal representations. Traditional MLLMs as encoders struggle with complex, compositional instructions; chain-of-thought reasoning has proven effective in other domains, motivating a reasoned, two-stage embedding process.

Method: Two-stage pipeline: (1) a powerful MLLM reasoner generates reasoning traces for complex queries; (2) an embedder encodes representations conditioned on the original query and the intermediate reasoning. The approach includes finetuning a smaller MLLM reasoner with high-quality embedding-centric traces to reduce dependency on large models, and investigates strategies to integrate reasoner and embedder into a unified, efficient model.

Result: Achieves state-of-the-art performance on the MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house data. Open-source results show a 7% absolute gain over recent models when using a smaller MLLM reasoner. Demonstrates feasibility of integrating reasoner and embedder with efficiency gains without sacrificing performance.

Conclusion: Explicit reasoning in the embedding process enhances multimodal understanding and task-specific representations. The Think-Then-Embed framework offers a scalable path to strong UME performance for both large-scale and open-source settings, with avenues for unified architectures to improve efficiency.

Abstract: There is a growing interest in Universal Multimodal Embeddings (UME), where
models are required to generate task-specific representations. While recent
studies show that Multimodal Large Language Models (MLLMs) perform well on such
tasks, they treat MLLMs solely as encoders, overlooking their generative
capacity. However, such an encoding paradigm becomes less effective as
instructions become more complex and require compositional reasoning. Inspired
by the proven effectiveness of chain-of-thought reasoning, we propose a general
Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an
embedder. The reasoner MLLM first generates reasoning traces that explain
complex queries, followed by an embedder that produces representations
conditioned on both the original query and the intermediate reasoning. This
explicit reasoning step enables more nuanced understanding of complex
multimodal instructions. Our contributions are threefold. First, by leveraging
a powerful MLLM reasoner, we achieve state-of-the-art performance on the
MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house
datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune
a smaller MLLM reasoner using high-quality embedding-centric reasoning traces,
achieving the best performance among open-source models with a 7% absolute gain
over recently proposed models. Third, we investigate strategies for integrating
the reasoner and embedder into a unified model for improved efficiency without
sacrificing performance.

</details>


### [599] [Look-ahead Reasoning with a Learned Model in Imperfect Information Games](https://arxiv.org/abs/2510.05048)
*Ondřej Kubíček,Viliam Lisý*

Main category: cs.AI

TL;DR: LAMIR learns an abstracted model of an imperfect-information game from interaction to enable tractable test-time look-ahead; with enough capacity it can recover the exact game structure, while with limited capacity it produces a useful abstraction that improves pre-trained agent performance in large games.


<details>
  <summary>Details</summary>
Motivation: Imperfect-information games pose significant planning challenges because full environment models are often unavailable or intractable. Existing model-based search methods (e.g., MuZero) work well in perfect-information settings but do not scale to imperfect information. An abstracted learned model can enable principled look-ahead at test time.

Method: LAMIR learns an abstracted model of the imperfect-information game directly from agent–environment interaction. The abstraction reduces the state-space and subgame sizes to enable tractable look‑ahead during testing; as capacity increases, the model can recover the exact game structure; with limited capacity it yields a useful approximation.

Result: Empirically, when given sufficient capacity, LAMIR recovers the exact underlying game structure. Even with limited capacity, it learns a valuable abstraction that improves the game-playing performance of pre-trained agents in large games.

Conclusion: LAMIR provides a principled approach to test-time planning in imperfect-information environments by learning compact abstractions. This enables tractable look-ahead and improved performance, with a capacity-dependent trade-off between exactness and abstraction quality.

Abstract: Test-time reasoning significantly enhances pre-trained AI agents'
performance. However, it requires an explicit environment model, often
unavailable or overly complex in real-world scenarios. While MuZero enables
effective model learning for search in perfect information games, extending
this paradigm to imperfect information games presents substantial challenges
due to more nuanced look-ahead reasoning techniques and large number of states
relevant for individual decisions. This paper introduces an algorithm LAMIR
that learns an abstracted model of an imperfect information game directly from
the agent-environment interaction. During test time, this trained model is used
to perform look-ahead reasoning. The learned abstraction limits the size of
each subgame to a manageable size, making theoretically principled look-ahead
reasoning tractable even in games where previous methods could not scale. We
empirically demonstrate that with sufficient capacity, LAMIR learns the exact
underlying game structure, and with limited capacity, it still learns a
valuable abstraction, which improves game playing performance of the
pre-trained agents even in large games.

</details>


### [600] [Staircase Streaming for Low-Latency Multi-Agent Inference](https://arxiv.org/abs/2510.05059)
*Junlin Wang,Jue Wang,Zhen,Xu,Ben Athiwaratkun,Bhuwan Dhingra,Ce Zhang,James Zou*

Main category: cs.AI

TL;DR: Staircase streaming enables low-latency multi-agent inference by starting final output generation from partial intermediate outputs, significantly reducing time-to-first-token without sacrificing quality.


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Agents and similar multi-agent LLM frameworks can boost answer quality but incur high latency due to waiting for complete intermediate outputs; reducing TTFT is critical for responsive deployments.

Method: Introduce staircase streaming: begin generating the final response as soon as partial outputs from previous stages arrive instead of waiting for full intermediate outputs; progressively incorporate more information as it becomes available.

Result: TTFT is reduced by up to 93% in experiments, with maintained or comparable response quality to baselines.

Conclusion: Staircase streaming offers a practical and effective approach to low-latency multi-agent inference, enabling faster responses in latency-sensitive applications without compromising answer quality.

Abstract: Recent advances in large language models (LLMs) opened up new directions for
leveraging the collective expertise of multiple LLMs. These methods, such as
Mixture-of-Agents, typically employ additional inference steps to generate
intermediate outputs, which are then used to produce the final response. While
multi-agent inference can enhance response quality, it can significantly
increase the time to first token (TTFT), posing a challenge for
latency-sensitive applications and hurting user experience. To address this
issue, we propose staircase streaming for low-latency multi-agent inference.
Instead of waiting for the complete intermediate outputs from previous steps,
we begin generating the final response as soon as we receive partial outputs
from these steps. Experimental results demonstrate that staircase streaming
reduces TTFT by up to 93% while maintaining response quality.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [601] [Cellular Learning: Scattered Data Regression in High Dimensions via Voronoi Cells](https://arxiv.org/abs/2510.03810)
*Shankar Prasad Sastry*

Main category: cs.CG

TL;DR: A scalable regression/approximation method that models scattered data with a continuous, piecewise-smooth function by blending linear models over inferred Voronoi regions, avoiding explicit Voronoi computation; achieves high accuracy on MNIST with few hundred thousand parameters.


<details>
  <summary>Details</summary>
Motivation: To accurately approximate scattered data in high dimensions without the computational burden of building explicit Voronoi diagrams, thereby mitigating the curse of dimensionality.

Method: Infer Voronoi-like cells from seed vertices; assign linear models per cell and blend them to form a continuous piecewise-smooth predictor; avoids explicit Voronoi diagram; scales to high dimensions; requires no data augmentation, convolutions, or geometric operators.

Result: On MNIST, achieves about 98.2% accuracy with 722,200 degrees of freedom, using no data augmentation, convolution, or geometric operators.

Conclusion: Demonstrates applicability and scalability of the algorithm in high dimensions and shows that explicit Voronoi computation is not necessary for effective high-dimensional regression/classification.

Abstract: I present a regression algorithm that provides a continuous, piecewise-smooth
function approximating scattered data. It is based on composing and blending
linear functions over Voronoi cells, and it scales to high dimensions. The
algorithm infers Voronoi cells from seed vertices and constructs a linear
function for the input data in and around each cell. As the algorithm does not
explicitly compute the Voronoi diagram, it avoids the curse of dimensionality.
An accuracy of around 98.2% on the MNIST dataset with 722,200 degrees of
freedom (without data augmentation, convolution, or other geometric operators)
demonstrates the applicability and scalability of the algorithm.

</details>


### [602] [Fast Witness Persistence for MRI Volumes via Hybrid Landmarking](https://arxiv.org/abs/2510.04553)
*Jorge Leonardo Ruiz Williams*

Main category: cs.CG

TL;DR: A scalable witness-based persistent homology pipeline for full-brain MRI that uses density-aware landmark selection and GPU-accelerated witness filtration to preserve topology while reducing computational burden.


<details>
  <summary>Details</summary>
Motivation: To enable scalable topological analysis of large MRI volumes by avoiding combinatorial filtrations (Cech/Vietoris-Rips/alpha) and by producing compact landmark sets with preserved topological features.

Method: Hybrid scoring combines geometric coverage and inverse kernel density for landmark selection; GPU-ready witness filtration; benchmarks on BrainWeb, IXI, and synthetic manifolds; a PyPI package whale-tda with fast preset mri_deep_dive_fast; reproducibility scripts.

Result: Landmark sets shrink mean pairwise distances by 30-60% compared to random or density-only baselines; runtime under ten seconds on a single RTX 4090 GPU; avoids combinatorial blow-ups of classical filtrations; open-source package and convenient workflows.

Conclusion: Provides a scalable, practical workflow for full-brain persistent homology in medical imaging, enabling rapid exploratory sweeps and reproducible analyses with accessible tooling.

Abstract: We introduce a scalable witness-based persistent homology pipeline for
full-brain MRI volumes that couples density-aware landmark selection with a
GPU-ready witness filtration. Candidates are scored by a hybrid metric that
balances geometric coverage against inverse kernel density, yielding landmark
sets that shrink mean pairwise distances by 30-60% over random or density-only
baselines while preserving topological features. Benchmarks on BrainWeb, IXI,
and synthetic manifolds execute in under ten seconds on a single NVIDIA RTX
4090 GPU, avoiding the combinatorial blow-up of Cech, Vietoris-Rips, and alpha
filtrations. The package is distributed on PyPI as whale-tda (installable via
pip); source and issues are hosted at https://github.com/jorgeLRW/whale. The
release also exposes a fast preset (mri_deep_dive_fast) for exploratory sweeps,
and ships with reproducibility-focused scripts and artifacts for drop-in use in
medical imaging workflows.

</details>
