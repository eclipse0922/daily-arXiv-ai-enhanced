<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 83]
- [cs.LG](#cs.LG) [Total: 55]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.RO](#cs.RO) [Total: 64]
- [cs.CG](#cs.CG) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks](https://arxiv.org/abs/2509.13338)
*Hassan Gharoun,Mohammad Sadegh Khorshidi,Kasra Ranjbarigderi,Fang Chen,Amir H. Gandomi*

Main category: cs.CV

TL;DR: Evidence-conditioned, instance-adaptive uncertainty thresholding replaces a single global cutoff with per-instance thresholds derived from proximal evidences fused via Dempster-Shafer theory.


<details>
  <summary>Details</summary>
Motivation: Improve operational uncertainty handling by making decisions reliant on explicit, auditably interpretable evidence rather than global entropy thresholds, addressing transparency and reliability in uncertain predictions.

Method: For each test instance, retrieve proximal exemplars in embedding space; fuse their predictive distributions with Dempster-Shafer theory to form a per-instance belief, which sets the decision threshold. Evaluate on CIFAR-10/100 using BiT and ViT backbones; compare to entropy-threshold baselines and measure confidently incorrect rate and review load.

Result: Per-instance, evidence-conditioned fusion yields equal or better uncertainty-aware performance with substantially fewer confidently incorrect outcomes and manageable review load; only a small subset of evidences is needed, with larger evidence sets offering diminishing returns.

Conclusion: Evidence-conditioned tagging offers a more reliable and interpretable alternative to fixed entropy thresholds for operational uncertainty-aware decision-making and improves transparency and auditability of decisions.

Abstract: This work proposes an evidence-retrieval mechanism for uncertainty-aware
decision-making that replaces a single global cutoff with an
evidence-conditioned, instance-adaptive criterion. For each test instance,
proximal exemplars are retrieved in an embedding space; their predictive
distributions are fused via Dempster-Shafer theory. The resulting fused belief
acts as a per-instance thresholding mechanism. Because the supporting evidences
are explicit, decisions are transparent and auditable. Experiments on
CIFAR-10/100 with BiT and ViT backbones show higher or comparable
uncertainty-aware performance with materially fewer confidently incorrect
outcomes and a sustainable review load compared with applying threshold on
prediction entropy. Notably, only a few evidences are sufficient to realize
these gains; increasing the evidence set yields only modest changes. These
results indicate that evidence-conditioned tagging provides a more reliable and
interpretable alternative to fixed prediction entropy thresholds for
operational uncertainty-aware decision-making.

</details>


### [2] [Hybrid Quantum-Classical Model for Image Classification](https://arxiv.org/abs/2509.13353)
*Muhammad Adnan Shahzad*

Main category: cs.CV

TL;DR: Hybrid quantum-classical neural networks outperform classical CNNs across MNIST, CIFAR100, and STL10 in accuracy and efficiency, with better robustness on simpler datasets and lower resource usage; gains scale with task complexity.


<details>
  <summary>Details</summary>
Motivation: To systematically assess whether integrating parameterized quantum circuits with classical CNNs yields practical advantages over fully classical models in vision tasks, across multiple datasets and metrics.

Method: A systematic comparison on MNIST, CIFAR100, STL10 over 50 training epochs. Hybrid models (parameterized quantum circuits + classical architectures) versus classical CNNs. Evaluated validation/test accuracy, training time, resource usage, and adversarial robustness (epsilon=0.1).

Result: Hybrid models achieve higher final validation accuracy: MNIST 99.38% vs 98.21%; CIFAR100 41.69% vs 32.25%; STL10 74.05% vs 63.76% (gains +9.44, +10.29). Training is 5–12x faster per epoch (e.g., 21.23s vs 108.44s on MNIST). Parameter count 6–32% fewer. Robustness: MNIST robust accuracy 45.27% vs 10.80%; CIFAR100 approx 1% for both. Memory 4–5GB vs 5–6GB; CPU usage 9.5% vs 23.2%.

Conclusion: Hybrid quantum-classical architectures offer advantages in accuracy, training efficiency, and parameter scalability, especially for more complex vision tasks.

Abstract: This study presents a systematic comparison between hybrid quantum-classical
neural networks and purely classical models across three benchmark datasets
(MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and
robustness. The hybrid models integrate parameterized quantum circuits with
classical deep learning architectures, while the classical counterparts use
conventional convolutional neural networks (CNNs). Experiments were conducted
over 50 training epochs for each dataset, with evaluations on validation
accuracy, test accuracy, training time, computational resource usage, and
adversarial robustness (tested with $\epsilon=0.1$ perturbations).Key findings
demonstrate that hybrid models consistently outperform classical models in
final accuracy, achieving {99.38\% (MNIST), 41.69\% (CIFAR100), and 74.05\%
(STL10) validation accuracy, compared to classical benchmarks of 98.21\%,
32.25\%, and 63.76\%, respectively. Notably, the hybrid advantage scales with
dataset complexity, showing the most significant gains on CIFAR100 (+9.44\%)
and STL10 (+10.29\%). Hybrid models also train 5--12$\times$ faster (e.g.,
21.23s vs. 108.44s per epoch on MNIST) and use 6--32\% fewer parameters} while
maintaining superior generalization to unseen test data.Adversarial robustness
tests reveal that hybrid models are significantly more resilient on simpler
datasets (e.g., 45.27\% robust accuracy on MNIST vs. 10.80\% for classical) but
show comparable fragility on complex datasets like CIFAR100 ($\sim$1\%
robustness for both). Resource efficiency analyses indicate that hybrid models
consume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization
(9.5\% vs. 23.2\% on average).These results suggest that hybrid
quantum-classical architectures offer compelling advantages in accuracy,
training efficiency, and parameter scalability, particularly for complex vision
tasks.

</details>


### [3] [Research on Expressway Congestion Warning Technology Based on YOLOv11-DIoU and GRU-Attention](https://arxiv.org/abs/2509.13361)
*Tong Yulin,Liang Xuechen*

Main category: cs.CV

TL;DR: A unified detection-prediction framework for expressway congestion that improves perception (YOLOv11-DIoU, DeepSort with Mahalanobis and cosine distances) and forecasting (GRU-Attention), delivering high accuracy and early warning performance, validated on expressway data.


<details>
  <summary>Details</summary>
Motivation: Overcome occlusion-related perception errors and loss of long-sequence dependencies in congestion forecasting; enable reliable congestion control and ITS applications.

Method: Upgrade perception baselines: replace GIoU with DIoU in YOLOv11-DIoU; fuse Mahalanobis motion and cosine appearance in DeepSort. For forecasting, use a GRU-Attention model trained on flow, density, and speed; analyze congestion with Greenberg model in high-density regimes; evaluate with 300 training epochs; 10-15 vehicles/km; generate 10-minute-ahead warnings for 30-minute congestion; validate with independent video.

Result: YOLOv11-DIoU achieves 95.7% mAP and 5.3% occlusion miss rate; DeepSort 93.8% MOTA with 4 ID switches; speed-density correlation r=-0.97; GRU-Attention achieves 99.7% test accuracy (7-9 pp higher than traditional GRU); 10-minute ahead warnings with time error ≤1 minute; independent video shows 95% warning accuracy and >90% spatial overlap; robust in high-flow (>5 vehicles/s).

Conclusion: This integrated framework provides quantitative support for expressway congestion control and holds promise for intelligent transportation systems applications.

Abstract: Expressway traffic congestion severely reduces travel efficiency and hinders
regional connectivity. Existing "detection-prediction" systems have critical
flaws: low vehicle perception accuracy under occlusion and loss of
long-sequence dependencies in congestion forecasting. This study proposes an
integrated technical framework to resolve these issues.For traffic flow
perception, two baseline algorithms were optimized. Traditional YOLOv11 was
upgraded to YOLOv11-DIoU by replacing GIoU Loss with DIoU Loss, and DeepSort
was improved by fusing Mahalanobis (motion) and cosine (appearance) distances.
Experiments on Chang-Shen Expressway videos showed YOLOv11-DIoU achieved 95.7\%
mAP (6.5 percentage points higher than baseline) with 5.3\% occlusion miss
rate. DeepSort reached 93.8\% MOTA (11.3 percentage points higher than SORT)
with only 4 ID switches. Using the Greenberg model (for 10-15 vehicles/km
high-density scenarios), speed and density showed a strong negative correlation
(r=-0.97), conforming to traffic flow theory. For congestion warning, a
GRU-Attention model was built to capture congestion precursors. Trained 300
epochs with flow, density, and speed, it achieved 99.7\% test accuracy (7-9
percentage points higher than traditional GRU). In 10-minute advance warnings
for 30-minute congestion, time error was $\leq$ 1 minute. Validation with an
independent video showed 95\% warning accuracy, over 90\% spatial overlap of
congestion points, and stable performance in high-flow ($>$5 vehicles/second)
scenarios.This framework provides quantitative support for expressway
congestion control, with promising intelligent transportation applications.

</details>


### [4] [Parking Space Ground Truth Test Automation by Artificial Intelligence Using Convolutional Neural Networks](https://arxiv.org/abs/2509.13366)
*Tony Rohe,Martin Margreiter,Markus Moertl*

Main category: cs.CV

TL;DR: Automates ground-truth testing for a real-time cloud-based parking service using CNNs on crowd-sourced detections to drastically cut manual validation effort and potentially improve service quality.


<details>
  <summary>Details</summary>
Motivation: Reduce manual, labor-intensive ground-truth analysis in a real-time parking system and enrich the dataset using machine learning, enabling automation of the evaluation process.

Method: Apply convolutional neural networks to image-pattern recognition tasks to automate analysis, expand the database, and substitute human labor in the testing/validation workflow for the crowd-sourced parking data.

Result: Reportedly achieves a time/resource reduction of up to 99.58% in human labor, with predefined metrics indicating high automation performance.

Conclusion: The study demonstrates strong automation potential for the parking service analysis pipeline and outlines future development and broader application of the automation tool.

Abstract: This research is part of a study of a real-time, cloud-based on-street
parking service using crowd-sourced in-vehicle fleet data. The service provides
real-time information about available parking spots by classifying
crowd-sourced detections observed via ultrasonic sensors. The goal of this
research is to optimize the current parking service quality by analyzing the
automation of the existing test process for ground truth tests. Therefore,
methods from the field of machine learning, especially image pattern
recognition, are applied to enrich the database and substitute human
engineering work in major areas of the analysis process. After an introduction
into the related areas of machine learning, this paper explains the methods and
implementations made to achieve a high level of automation, applying
convolutional neural networks. Finally, predefined metrics present the
performance level achieved, showing a time reduction of human resources up to
99.58 %. The overall improvements are discussed, summarized, and followed by an
outlook for future development and potential application of the analysis
automation tool.

</details>


### [5] [An Empirical Analysis of VLM-based OOD Detection: Mechanisms, Advantages, and Sensitivity](https://arxiv.org/abs/2509.13375)
*Yuxiao Lee,Xiaofeng Cao,Wei Ye,Jiangchao Yao,Jingkuan Song,Heng Tao Shen*

Main category: cs.CV

TL;DR: VLM-based zero-shot OOD detection leverages cross-modal embedding structures to outperform single-modal baselines, but is sensitive to prompt phrasing while robust to image noise.


<details>
  <summary>Details</summary>
Motivation: To resolve why VLMs excel at OOD detection, what advantages they confer versus single-modal methods, and how robust their behavior is, through a systematic empirical study using in-distribution and OOD prompts.

Method: Systematic empirical analysis across benchmarks; characterize embedding-space mechanisms; compare with single-modal baselines; evaluate robustness to noise and prompt variations; use ID/OOD prompts to probe performance.

Result: Empirical evidence shows VLMs leverage semantic novelty in embedding space to enable zero-shot OOD detection and outperform single-modal baselines; robustness is asymmetric—stable under image noise but highly sensitive to prompt phrasing.

Conclusion: Provides a structured account of strengths and vulnerabilities of VLM-based OOD detection and guidance for designing more reliable future models.

Abstract: Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable
zero-shot out-of-distribution (OOD) detection capabilities, vital for reliable
AI systems. Despite this promising capability, a comprehensive understanding of
(1) why they work so effectively, (2) what advantages do they have over
single-modal methods, and (3) how is their behavioral robustness -- remains
notably incomplete within the research community. This paper presents a
systematic empirical analysis of VLM-based OOD detection using in-distribution
(ID) and OOD prompts. (1) Mechanisms: We systematically characterize and
formalize key operational properties within the VLM embedding space that
facilitate zero-shot OOD detection. (2) Advantages: We empirically quantify the
superiority of these models over established single-modal approaches,
attributing this distinct advantage to the VLM's capacity to leverage rich
semantic novelty. (3) Sensitivity: We uncovers a significant and previously
under-explored asymmetry in their robustness profile: while exhibiting
resilience to common image noise, these VLM-based methods are highly sensitive
to prompt phrasing. Our findings contribute a more structured understanding of
the strengths and critical vulnerabilities inherent in VLM-based OOD detection,
offering crucial, empirically-grounded guidance for developing more robust and
reliable future designs.

</details>


### [6] [Curvature as a tool for evaluating dimensionality reduction and estimating intrinsic dimension](https://arxiv.org/abs/2509.13385)
*Charlotte Beylier,Parvaneh Joharinad,Jürgen Jost,Nahid Torbati*

Main category: cs.CV

TL;DR: A curvature-based framework for discrete metric spaces that profiles geometry using sectional curvature, and uses this curvature profile to quantify representation quality, estimate intrinsic dimensionality, and study large-scale network geometry, with applications to evaluating dimensionality reduction methods.


<details>
  <summary>Details</summary>
Motivation: There is a need for geometry-aware, quantitative tools to analyze discrete data representations and networks. A curvature-based approach promises to capture metric relations among triples (and other points) to assess and compare data representations and to infer intrinsic dimensionality.

Method: Define a curvature profile on discrete metric spaces using recently developed notions of sectional curvature. Derive a quantitative measure from this curvature profile to evaluate the effectiveness of data representations (e.g., from dimensionality reduction). Apply the framework to datasets and empirical networks to estimate intrinsic dimensionality and to assess large-scale geometry and the performance of dimensionality reduction techniques.

Result: The curvature-based analysis yields a quantitative measure that can evaluate the quality of data representations and can be used to estimate intrinsic dimensionality. It is demonstrated on empirical networks and used to evaluate dimensionality reduction methods.

Conclusion: A curvature-based geometric profile for discrete spaces provides a practical tool for understanding large-scale geometry, estimating intrinsic dimensionality, and comparing dimensionality reduction outcomes, bridging discrete geometry with data representation analysis.

Abstract: Utilizing recently developed abstract notions of sectional curvature, we
introduce a method for constructing a curvature-based geometric profile of
discrete metric spaces. The curvature concept that we use here captures the
metric relations between triples of points and other points. More
significantly, based on this curvature profile, we introduce a quantitative
measure to evaluate the effectiveness of data representations, such as those
produced by dimensionality reduction techniques. Furthermore, Our experiments
demonstrate that this curvature-based analysis can be employed to estimate the
intrinsic dimensionality of datasets. We use this to explore the large-scale
geometry of empirical networks and to evaluate the effectiveness of
dimensionality reduction techniques.

</details>


### [7] [Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji](https://arxiv.org/abs/2509.13388)
*Yadvendra Gurjar,Ruoni Wan,Ehsan Farahbakhsh,Rohitash Chandra*

Main category: cs.CV

TL;DR: This study uses Landsat-8 data, Google Earth Engine, unsupervised k-means, and supervised CNNs to map and compare land use/land cover in Nadi, Fiji from 2013 to 2024, highlighting urban expansion and providing a workflow for change detection.


<details>
  <summary>Details</summary>
Motivation: Address rapid urbanisation in Fiji and the need for technical support in land cover/land use modelling and change detection to inform planning and policy.

Method: Uses Landsat-8 imagery for 2013–2024 region; creates labeled training data for supervised ML; employs Google Earth Engine and unsupervised k-means to generate land cover maps; applies convolutional neural networks to classify land cover types in selected regions; visualizes change detection with emphasis on urban area changes.

Result: Produces land cover maps for the study period and visualizes urban area changes over time; demonstrates a ML/RS workflow for change detection and land cover modelling in Fiji.

Conclusion: Provides a technical framework for land cover/land use modelling and change detection in Fiji, highlighting the applicability of ML and remote sensing to monitor urban growth and support planning.

Abstract: As a developing country, Fiji is facing rapid urbanisation, which is visible
in the massive development projects that include housing, roads, and civil
works. In this study, we present machine learning and remote sensing frameworks
to compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The
ultimate goal of this study is to provide technical support in land cover/land
use modelling and change detection. We used Landsat-8 satellite image for the
study region and created our training dataset with labels for supervised
machine learning. We used Google Earth Engine and unsupervised machine learning
via k-means clustering to generate the land cover map. We used convolutional
neural networks to classify the selected regions' land cover types. We present
a visualisation of change detection, highlighting urban area changes over time
to monitor changes in the map.

</details>


### [8] [Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence](https://arxiv.org/abs/2509.13396)
*Xinan Wang,Di Shi,Fengyu Wang*

Main category: cs.CV

TL;DR: A three-stage real-time FOI detection/tracking framework for power systems, combining YOLOv7 segmentation, ConvNeXt embeddings with triplet loss, and a feature-assisted IoU tracker; edge-optimized with mixed-precision; supports incremental embedding updates without retraining; validated on real-world datasets and Jetson hardware.


<details>
  <summary>Details</summary>
Motivation: Need for fast, robust foreign object intrusion detection and tracking on power infrastructure with limited edge resources, occlusion handling, and scalable deployment via incremental learning.

Method: Stage 1: YOLOv7 segmentation for fast object localization. Stage 2: ConvNeXt-based feature extractor trained with triplet loss to produce discriminative embeddings. Stage 3: feature-assisted IoU tracker for resilient multi-object tracking under occlusion/motion. Edge deployment via mixed-precision inference. Incremental updates by adding unseen object embeddings into a reference database without retraining.

Result: Shows high accuracy and robustness across diverse FOI scenarios on real-world surveillance and drone datasets. Hardware benchmarks on NVIDIA Jetson devices confirm practicality and scalability for edge applications.

Conclusion: The framework demonstrates real-time FOI detection/tracking in power systems with scalable, incremental learning and edge-optimized performance, suitable for field deployment.

Abstract: This paper presents a novel three-stage framework for real-time foreign
object intrusion (FOI) detection and tracking in power transmission systems.
The framework integrates: (1) a YOLOv7 segmentation model for fast and robust
object localization, (2) a ConvNeXt-based feature extractor trained with
triplet loss to generate discriminative embeddings, and (3) a feature-assisted
IoU tracker that ensures resilient multi-object tracking under occlusion and
motion. To enable scalable field deployment, the pipeline is optimized for
deployment on low-cost edge hardware using mixed-precision inference. The
system supports incremental updates by adding embeddings from previously unseen
objects into a reference database without requiring model retraining. Extensive
experiments on real-world surveillance and drone video datasets demonstrate the
framework's high accuracy and robustness across diverse FOI scenarios. In
addition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's
practicality and scalability for real-world edge applications.

</details>


### [9] [EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing](https://arxiv.org/abs/2509.13399)
*Tianyu Chen,Yasi Zhang,Zhi Zhang,Peiyu Yu,Shu Wang,Zhendong Wang,Kevin Lin,Xiaofei Wang,Zhengyuan Yang,Linjie Li,Chung-Ching Lin,Jianwen Xie,Oscar Leong,Lijuan Wang,Ying Nian Wu,Mingyuan Zhou*

Main category: cs.CV

TL;DR: Introduces EdiVal-Agent, a modular, object-centric evaluation framework for multi-turn instruction-based image editing, combining vision-language models with open-vocabulary object detectors and semantic/expert evaluators; instantiated as EdiVal-Bench to benchmark models across 9 instruction types and 11 editing methods.


<details>
  <summary>Details</summary>
Motivation: Reliable, interpretable evaluation for instruction-based image editing is lacking: paired references are biased and coverage is limited, while zero-shot VLMs offer imprecise judgments. A scalable, collaborative tool integrating multiple signals is needed.

Method: Decompose input image into semantic objects; generate diverse editing instructions; evaluate instruction-following with VLMs plus open-vocabulary detectors; assess content consistency with semantic features; assess visual quality with human-preference models; modular design enabling future tools; build EdiVal-Bench across 9 instruction types and 11 models.

Result: Combining VLMs with object detectors yields stronger agreement with human judgments for instruction-following than VLMs alone or CLIP-based metrics; framework identifies failure modes and guides next-generation editors; demonstrated on 11 models and 9 instruction types.

Conclusion: EdiVal-Agent provides scalable, fine-grained, modular evaluation for instruction-based editing; its design enables future tool integration and improved assessment quality, with EdiVal-Bench offering a comprehensive benchmark.

Abstract: Instruction-based image editing has advanced rapidly, yet reliable and
interpretable evaluation remains a bottleneck. Current protocols either (i)
depend on paired reference images -- resulting in limited coverage and
inheriting biases from prior generative models -- or (ii) rely solely on
zero-shot vision-language models (VLMs), whose prompt-based assessments of
instruction following, content consistency, and visual quality are often
imprecise.
  To address this, we introduce EdiVal-Agent, an automated, scalable, and
fine-grained evaluation framework for multi-turn instruction-based editing from
an object-centric perspective, supported by a suite of expert tools. Given an
image, EdiVal-Agent first decomposes it into semantically meaningful objects,
then synthesizes diverse, context-aware editing instructions. For evaluation,
it integrates VLMs with open-vocabulary object detectors to assess instruction
following, uses semantic-level feature extractors to evaluate content
consistency, and leverages human preference models to judge visual quality. We
show that combining VLMs with object detectors yields stronger agreement with
human judgments in instruction-following evaluation compared to using VLMs
alone and CLIP-based metrics. Furthermore, the pipeline's modular design allows
future tools to be seamlessly integrated, enhancing evaluation accuracy over
time.
  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing
benchmark covering 9 instruction types and 11 state-of-the-art editing models
spanning autoregressive (AR) (including Nano Banana, GPT-Image-1),
flow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be
used to identify existing failure modes, thereby informing the development of
the next generation of editing models. Project page:
https://tianyucodings.github.io/EdiVAL-page/.

</details>


### [10] [MapAnything: Universal Feed-Forward Metric 3D Reconstruction](https://arxiv.org/abs/2509.13414)
*Nikhil Keetha,Norman Müller,Johannes Schönberger,Lorenzo Porzi,Yuchen Zhang,Tobias Fischer,Arno Knapitsch,Duncan Zauss,Ethan Weber,Nelson Antunes,Jonathon Luiten,Manuel Lopez-Antequera,Samuel Rota Bulò,Christian Richardt,Deva Ramanan,Sebastian Scherer,Peter Kontschieder*

Main category: cs.CV

TL;DR: MapAnything is a transformer-based feed-forward model that ingests images and optional geometric inputs to regress global 3D scene geometry and camera parameters. It uses a factored representation (depth maps, local ray maps, poses, scale) to convert local reconstructions into a globally consistent metric frame, enabling a single-pass handling of diverse 3D vision tasks. It matches or outscores specialist models with more efficient joint training, aiming to be a universal 3D reconstruction backbone.


<details>
  <summary>Details</summary>
Motivation: There is a need for a single, standardized, efficient backbone capable of handling a broad range of 3D vision tasks—such as structure-from-motion, multi-view stereo, monocular depth estimation, and camera localization—without switching between specialized models or reconstructing pipelines. Standardizing supervision and inputs across datasets can improve consistency and efficiency.

Method: A unified transformer-based feed-forward model (MapAnything) that takes one or more images plus optional geometric inputs (camera intrinsics, poses, depth, partial reconstructions). It employs a factored representation of scene geometry (depth maps, local ray maps, camera poses, and a metric scale factor) to produce globally consistent metric reconstructions. The approach standardizes supervision across datasets and supports flexible input augmentation, enabling joint training across tasks in a single forward pass.

Result: Experimental analyses and ablations indicate that MapAnything outperforms or matches specialist feed-forward models across a range of 3D vision tasks, while enabling more efficient joint training and a single unified backbone for 3D reconstruction.

Conclusion: MapAnything offers a universal 3D reconstruction backbone by unifying diverse 3D vision tasks within a single trainable model, leveraging standardized supervision and a global metric frame to deliver competitive performance and efficiency.

Abstract: We introduce MapAnything, a unified transformer-based feed-forward model that
ingests one or more images along with optional geometric inputs such as camera
intrinsics, poses, depth, or partial reconstructions, and then directly
regresses the metric 3D scene geometry and cameras. MapAnything leverages a
factored representation of multi-view scene geometry, i.e., a collection of
depth maps, local ray maps, camera poses, and a metric scale factor that
effectively upgrades local reconstructions into a globally consistent metric
frame. Standardizing the supervision and training across diverse datasets,
along with flexible input augmentation, enables MapAnything to address a broad
range of 3D vision tasks in a single feed-forward pass, including uncalibrated
structure-from-motion, calibrated multi-view stereo, monocular depth
estimation, camera localization, depth completion, and more. We provide
extensive experimental analyses and model ablations demonstrating that
MapAnything outperforms or matches specialist feed-forward models while
offering more efficient joint training behavior, thus paving the way toward a
universal 3D reconstruction backbone.

</details>


### [11] [Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization](https://arxiv.org/abs/2509.13474)
*Yujia Lin,Nicholas Evans*

Main category: cs.CV

TL;DR: A semantic-enhanced cross-modal place recognition framework (SCM-PR) improves RGB-LiDAR localization by fusing RGB semantics with LiDAR geometry, using a VMamba RGB backbone, SAFF fusion, semantic-aware cross-modal attention, and contrastive learning with multi-view semantic-geometric matching and semantic consistency loss; it achieves state-of-the-art results on KITTI/KITTI-360.


<details>
  <summary>Details</summary>
Motivation: Robust cross-modal place recognition is hard due to illumination, weather, and viewpoint changes. RGB-based VPR is sensitive to lighting; cross-modal RGB-LiDAR methods help but struggle in complex or high-resolution scenarios. Semantics can provide stable cues across modalities.

Method: SCM-PR introduces: (1) VMamba backbone for RGB feature extraction; (2) Semantic-Aware Feature Fusion (SAFF) module using place descriptors and segmentation masks; (3) LiDAR descriptors that incorporate semantics and geometry; (4) cross-modal semantic attention in NetVLAD; (5) Multi-View Semantic-Geometric Matching and a Semantic Consistency Loss within a contrastive learning framework.

Result: KITTI and KITTI-360 experiments show SCM-PR achieves state-of-the-art performance among cross-modal place recognition methods, indicating robust localization under challenging conditions.

Conclusion: Incorporating semantic information into cross-modal RGB-LiDAR place recognition improves robustness and accuracy, enabling better localization in GPS-denied environments.

Abstract: Ensuring accurate localization of robots in environments without GPS
capability is a challenging task. Visual Place Recognition (VPR) techniques can
potentially achieve this goal, but existing RGB-based methods are sensitive to
changes in illumination, weather, and other seasonal changes. Existing
cross-modal localization methods leverage the geometric properties of RGB
images and 3D LiDAR maps to reduce the sensitivity issues highlighted above.
Currently, state-of-the-art methods struggle in complex scenes, fine-grained or
high-resolution matching, and situations where changes can occur in viewpoint.
In this work, we introduce a framework we call Semantic-Enhanced Cross-Modal
Place Recognition (SCM-PR) that combines high-level semantics utilizing RGB
images for robust localization in LiDAR maps. Our proposed method introduces: a
VMamba backbone for feature extraction of RGB images; a Semantic-Aware Feature
Fusion (SAFF) module for using both place descriptors and segmentation masks;
LiDAR descriptors that incorporate both semantics and geometry; and a
cross-modal semantic attention mechanism in NetVLAD to improve matching.
Incorporating the semantic information also was instrumental in designing a
Multi-View Semantic-Geometric Matching and a Semantic Consistency Loss, both in
a contrastive learning framework. Our experimental work on the KITTI and
KITTI-360 datasets show that SCM-PR achieves state-of-the-art performance
compared to other cross-modal place recognition methods.

</details>


### [12] [Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice Vector Quantization](https://arxiv.org/abs/2509.13482)
*Hao Xu,Xiaolin Wu,Xi Zhang*

Main category: cs.CV

TL;DR: Scene-adaptive lattice vector quantization (SALVQ) replaces uniform scalar quantization in 3DGS compression with a per-scene optimized LVQ, improving rate-distortion and enabling multi-bitrate models with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3DGS) generates massive data; existing anchor-based neural compression relies on USQ for its simplicity. A more sophisticated quantizer could boost compression performance without heavy costs, and per-scene adaptation could further improve efficiency.

Method: Replace USQ with lattice vector quantization (LVQ). Optimize the lattice basis for each scene to capture scene-specific characteristics and improve adaptability. Allow scaling of lattice basis vectors to adjust lattice density, enabling a single model to support multiple bitrate targets. Integrate SALVQ into existing 3DGS compression pipelines with minimal modifications and computational overhead.

Result: SALVQ enhances rate-distortion performance with minimal changes to the system. Scene-specific lattice basis optimization improves adaptability, while lattice scaling enables multi-bitrate deployment from one model, reducing training time and memory usage.

Conclusion: SALVQ is a practical, flexible quantization strategy for 3DGS compression that surpasses USQ in RD efficiency and supports multi-rate deployment with low overhead, enabling more cost-effective 3DGS pipelines.

Abstract: 3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its
photorealistic rendering quality and real-time performance, but it generates
massive amounts of data. Hence compressing 3DGS data is necessary for the cost
effectiveness of 3DGS models. Recently, several anchor-based neural compression
methods have been proposed, achieving good 3DGS compression performance.
However, they all rely on uniform scalar quantization (USQ) due to its
simplicity. A tantalizing question is whether more sophisticated quantizers can
improve the current 3DGS compression methods with very little extra overhead
and minimal change to the system. The answer is yes by replacing USQ with
lattice vector quantization (LVQ). To better capture scene-specific
characteristics, we optimize the lattice basis for each scene, improving LVQ's
adaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a
balance between the R-D efficiency of vector quantization and the low
complexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS
compression architectures, enhancing their R-D performance with minimal
modifications and computational overhead. Moreover, by scaling the lattice
basis vectors, SALVQ can dynamically adjust lattice density, enabling a single
model to accommodate multiple bit rate targets. This flexibility eliminates the
need to train separate models for different compression levels, significantly
reducing training time and memory consumption.

</details>


### [13] [MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes](https://arxiv.org/abs/2509.13484)
*Liu Liu,Alexandra Kudaeva,Marco Cipriano,Fatimeh Al Ghannam,Freya Tan,Gerard de Melo,Andres Sevtsuk*

Main category: cs.CV

TL;DR: Proposes MINGLE, a three-stage pipeline to detect and ground social interaction regions in street-view images, and releases a 100k-image dataset.


<details>
  <summary>Details</summary>
Motivation: To understand group-level social interactions in public spaces for urban planning and the design of inclusive, socially vibrant environments.

Method: 1) Use existing detectors and depth estimation for people; 2) employ vision-language model reasoning to classify pairwise social affiliations; 3) apply a lightweight spatial aggregation to localize socially connected groups.

Result: Introduces the MINGLE pipeline and a 100k-image dataset with bounding boxes and labels for individuals and socially interacting groups, combining human annotations with pipeline outputs to ensure semantic richness.

Conclusion: MINGLE enables detecting and grounding socially connected groups in urban imagery and provides a dataset to spur future research in this area.

Abstract: Understanding group-level social interactions in public spaces is crucial for
urban planning, informing the design of socially vibrant and inclusive
environments. Detecting such interactions from images involves interpreting
subtle visual cues such as relations, proximity, and co-movement - semantically
complex signals that go beyond traditional object detection. To address this
challenge, we introduce a social group region detection task, which requires
inferring and spatially grounding visual regions defined by abstract
interpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level
Engagement), a modular three-stage pipeline that integrates: (1) off-the-shelf
human detection and depth estimation, (2) VLM-based reasoning to classify
pairwise social affiliation, and (3) a lightweight spatial aggregation
algorithm to localize socially connected groups. To support this task and
encourage future research, we present a new dataset of 100K urban street-view
images annotated with bounding boxes and labels for both individuals and
socially interacting groups. The annotations combine human-created labels and
outputs from the MINGLE pipeline, ensuring semantic richness and broad coverage
of real-world scenarios.

</details>


### [14] [BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation](https://arxiv.org/abs/2509.13496)
*Rajatsubhra Chakraborty,Xujun Che,Depeng Xu,Cori Faklaris,Xi Niu,Shuhan Yuan*

Main category: cs.CV

TL;DR: BiasMap uncovers and mitigates latent concept-level biases in diffusion-based text-to-image models by using cross-attention attribution and IoU-based entanglement metrics to guide energy-guided mitigation.


<details>
  <summary>Details</summary>
Motivation: Current bias research focuses on output-level demographic distributions, which can miss latent, concept-level entanglements between demographics and semantics. There is a need for model-agnostic tools to reveal and mitigate these hidden biases in stable diffusion models.

Method: BiasMap uses cross-attention attribution maps to reveal structural entanglements between demographics (e.g., gender, race) and semantics (e.g., professions) in diffusion models. It quantifies spatial demographics–semantics entanglement with Intersection over Union (IoU). For mitigation, it employs energy-guided diffusion sampling to modify the latent noise space and minimize the expected SoftIoU during denoising.

Result: The approach exposes concept-level couplings that distribution-focused fairness methods miss. It shows that existing fairness interventions can reduce output disparities but may not disentangle concept-level biases, while BiasMap’s entanglement-aware mitigation can reduce concept entanglement and complement distributional bias mitigation.

Conclusion: BiasMap provides a model-agnostic framework to both diagnose and mitigate latent bias at the concept level in diffusion-based TTI systems, highlighting the need for disentangled bias mitigation in addition to distributional fairness.

Abstract: Bias discovery is critical for black-box generative models, especiall
text-to-image (TTI) models. Existing works predominantly focus on output-level
demographic distributions, which do not necessarily guarantee concept
representations to be disentangled post-mitigation. We propose BiasMap, a
model-agnostic framework for uncovering latent concept-level representational
biases in stable diffusion models. BiasMap leverages cross-attention
attribution maps to reveal structural entanglements between demographics (e.g.,
gender, race) and semantics (e.g., professions), going deeper into
representational bias during the image generation. Using attribution maps of
these concepts, we quantify the spatial demographics-semantics concept
entanglement via Intersection over Union (IoU), offering a lens into bias that
remains hidden in existing fairness discovery approaches. In addition, we
further utilize BiasMap for bias mitigation through energy-guided diffusion
sampling that directly modifies latent noise space and minimizes the expected
SoftIoU during the denoising process. Our findings show that existing fairness
interventions may reduce the output distributional gap but often fail to
disentangle concept-level coupling, whereas our mitigation method can mitigate
concept entanglement in image generation while complementing distributional
bias mitigation.

</details>


### [15] [LivePyxel: Accelerating image annotations with a Python-integrated webcam live streaming](https://arxiv.org/abs/2509.13504)
*Uriel Garcilazo-Cruz,Joseph O. Okeme,Rodrigo A. Vargas--Hernández*

Main category: cs.CV

TL;DR: LivePixel is a Python-based GUI for real-time image annotation that integrates with imaging devices (webcams, microscopes, etc.), offering Bézier splines, binary masks, and non-destructive layers, optimized with OpenCV and NumPy for on-demand labeling in experimental workflows; open-source at GitHub.


<details>
  <summary>Details</summary>
Motivation: Flexible, real-time annotation tools are lacking, especially for on-demand data acquisition in lab environments; current software often requires precollected datasets, hindering real-time AI model deployment.

Method: A Python GUI that connects to imaging systems to annotate frames in real time, providing tools like Bézier splines and binary masks, non-destructive layers, and high-performance editing via OpenCV and NumPy; broad compatibility with video devices.

Result: Describes a functional annotation workflow and software capabilities, emphasizing real-time data collection and labeling, cross-device compatibility, and high-performance editing; released as an open-source project.

Conclusion: LivePixel offers a practical, open-source solution to streamline real-time image annotation in experimental settings, potentially accelerating AI model development.

Abstract: The lack of flexible annotation tools has hindered the deployment of AI
models in some scientific areas. Most existing image annotation software
requires users to upload a precollected dataset, which limits support for
on-demand pipelines and introduces unnecessary steps to acquire images. This
constraint is particularly problematic in laboratory environments, where
real-time data acquisition from instruments such as microscopes is increasingly
common. In this work, we introduce \texttt{LivePixel}, a Python-based graphical
user interface that integrates with imaging systems, such as webcams,
microscopes, and others, to enable real-time image annotation. LivePyxel is
designed to be easy to use through a simple interface that allows users to
precisely delimit areas for annotation using tools commonly found in commercial
graphics editing software. Of particular interest is the availability of
B\'ezier splines and binary masks, and the software's capacity to work with
non-destructive layers that enable high-performance editing. LivePyxel also
integrates a wide compatibility across video devices, and it's optimized for
object detection operations via the use of OpenCV in combination with
high-performance libraries designed to handle matrix and linear algebra
operations via Numpy effectively. LivePyxel facilitates seamless data
collection and labeling, accelerating the development of AI models in
experimental workflows. LivePyxel freely available at
https://github.com/UGarCil/LivePyxel

</details>


### [16] [DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised H-Transform](https://arxiv.org/abs/2509.13506)
*Xingzi Xu,Qi Li,Shuwen Qiu,Julien Han,Karim Bouyarmane*

Main category: cs.CV

TL;DR: Proposes DEFT-VTON: efficient fine-tuning of large diffusion models for VTO by freezing the base model and training a small h-transform, achieving competitive performance with only 1.42% trainable parameters and using adaptive consistency loss to enable fast inference (as few as 15 denoising steps).


<details>
  <summary>Details</summary>
Motivation: Address practical deployment constraints of VTO systems that rely on large pre-trained diffusion models, aiming to reduce training and inference costs while maintaining high-quality results.

Method: Freeze the pre-trained unconditional diffusion model and learn a compact h-transform network (DEFT) to condition the model for VTO; further improve speed and performance with an adaptive consistency loss that combines consistency training with denoising score matching in a data-adaptive manner, without full distillation.

Result: DEFT-VTON achieves state-of-the-art performance on VTO tasks with very few trainable parameters and fast inference, demonstrated at as few as 15 denoising steps while maintaining competitive results.

Conclusion: A lightweight, efficient fine-tuning paradigm (DEFT) can adapt large diffusion models for high-quality VTO with minimal training burden and reduced inference time, aided by an adaptive consistency objective.

Abstract: Diffusion models enable high-quality virtual try-on (VTO) with their
established image synthesis abilities. Despite the extensive end-to-end
training of large pre-trained models involved in current VTO methods,
real-world applications often prioritize limited training and inference,
serving, and deployment budgets for VTO. To solve this obstacle, we apply
Doob's h-transform efficient fine-tuning (DEFT) for adapting large pre-trained
unconditional models for downstream image-conditioned VTO abilities. DEFT
freezes the pre-trained model's parameters and trains a small h-transform
network to learn a conditional h-transform. The h-transform network allows
training only 1.42 percent of the frozen parameters, compared to a baseline of
5.52 percent in traditional parameter-efficient fine-tuning (PEFT).
  To further improve DEFT's performance and decrease existing models' inference
time, we additionally propose an adaptive consistency loss. Consistency
training distills slow but high-performing diffusion models into a fast one
while retaining performance by enforcing consistencies along the inference
path. Inspired by constrained optimization, instead of distillation, we combine
the consistency loss and the denoising score matching loss in a data-adaptive
manner for fine-tuning existing VTO models at a low cost. Empirical results
show the proposed DEFT-VTON method achieves state-of-the-art performance on VTO
tasks, with as few as 15 denoising steps, while maintaining competitive
results.

</details>


### [17] [Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving](https://arxiv.org/abs/2509.13507)
*Artem Savkin,Thomas Lapotre,Kevin Strauss,Uzair Akbar,Federico Tombari*

Main category: cs.CV

TL;DR: Augments Cityscapes with virtual pedestrians using a new adversarial lighting network to reduce synthetic-real domain gap; evaluated for semantic and instance segmentation.


<details>
  <summary>Details</summary>
Motivation: Domain gap between synthetic and real data in autonomous driving; need realistic VRUs to improve pedestrian recognition.

Method: Pipeline to augment Cityscapes with virtual pedestrians; novel generative network architecture trained adversarially to learn dataset lighting; evaluation on semantic and instance segmentation.

Result: Evaluations conducted on semantic and instance segmentation tasks.

Conclusion: The approach provides a realistic augmentation framework for improving pedestrian recognition and segmentation via learned lighting conditions.

Abstract: In the autonomous driving area synthetic data is crucial for cover specific
traffic scenarios which autonomous vehicle must handle. This data commonly
introduces domain gap between synthetic and real domains. In this paper we
deploy data augmentation to generate custom traffic scenarios with VRUs in
order to improve pedestrian recognition. We provide a pipeline for augmentation
of the Cityscapes dataset with virtual pedestrians. In order to improve
augmentation realism of the pipeline we reveal a novel generative network
architecture for adversarial learning of the data-set lighting conditions. We
also evaluate our approach on the tasks of semantic and instance segmentation.

</details>


### [18] [FunKAN: Functional Kolmogorov-Arnold Network for Medical Image Enhancement and Segmentation](https://arxiv.org/abs/2509.13508)
*Maksim Penkin,Andrey Krylov*

Main category: cs.CV

TL;DR: Functional Kolmogorov-Arnold Network (FunKAN) generalizes KAN to functional spaces using Fourier decomposition over Hermite functions, enabling interpretable medical image enhancement and segmentation; outperforms KAN baselines on MRI Gibbs ringing suppression (IXI) and segmentation on BUSI, GlaS, CVC-ClinicDB with U-FunKAN.


<details>
  <summary>Details</summary>
Motivation: Medical imaging requires interpretable models that preserve spatial structure. Traditional deep networks and flattened-feature KANs sacrifice interpretability or spatial locality. A functional extension of the Kolmogorov-Arnold representation can provide interpretability while respecting image geometry.

Method: Formally extend Kolmogorov-Arnold representation to functional spaces (FunKAN) and learn inner functions via Fourier decomposition over Hermite function bases. Apply to Gibbs ringing suppression in MRI (IXI dataset). Introduce U-FunKAN as a state-of-the-art binary segmentation model with benchmarks on BUSI (ultrasound), GlaS (histology), and CVC-ClinicDB (colonoscopy videos). Compare with other KAN-based backbones using standard metrics (PSNR, TV for enhancement; IoU, F1 for segmentation).

Result: FunKAN-based methods outperform other KAN backbones on both enhancement and segmentation tasks across evaluated datasets; demonstrates improved PSNR and IoU/F1, indicating stronger performance and better preservation of spatial information. The approach validates the potential of functional-space KANs for realistic medical imaging problems.

Conclusion: The work bridges theory and practice by delivering an interpretable, robust framework for clinical image analysis that maintains spatial structure and achieves competitive, if not superior, performance on diverse medical imaging tasks.

Abstract: Medical image enhancement and segmentation are critical yet challenging tasks
in modern clinical practice, constrained by artifacts and complex anatomical
variations. Traditional deep learning approaches often rely on complex
architectures with limited interpretability. While Kolmogorov-Arnold networks
offer interpretable solutions, their reliance on flattened feature
representations fundamentally disrupts the intrinsic spatial structure of
imaging data. To address this issue we propose a Functional Kolmogorov-Arnold
Network (FunKAN) -- a novel interpretable neural framework, designed
specifically for image processing, that formally generalizes the
Kolmogorov-Arnold representation theorem onto functional spaces and learns
inner functions using Fourier decomposition over the basis Hermite functions.
We explore FunKAN on several medical image processing tasks, including Gibbs
ringing suppression in magnetic resonance images, benchmarking on IXI dataset.
We also propose U-FunKAN as state-of-the-art binary medical segmentation model
with benchmarks on three medical datasets: BUSI (ultrasound images), GlaS
(histological structures) and CVC-ClinicDB (colonoscopy videos), detecting
breast cancer, glands and polyps, respectively. Experiments on those diverse
datasets demonstrate that our approach outperforms other KAN-based backbones in
both medical image enhancement (PSNR, TV) and segmentation (IoU, F1). Our work
bridges the gap between theoretical function approximation and medical image
analysis, offering a robust, interpretable solution for clinical applications.

</details>


### [19] [Multimodal Hate Detection Using Dual-Stream Graph Neural Networks](https://arxiv.org/abs/2509.13515)
*Jiangbei Yue,Shuonan Yang,Tailin Chen,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: A multimodal dual-stream graph neural network for hateful video classification that uses an instance-level graph and an importance-weighting mechanism to highlight hateful content, achieving state-of-the-art results with enhanced explainability.


<details>
  <summary>Details</summary>
Motivation: Hateful videos pose risks to online safety and real-world well-being. Existing multimodal methods underutilize even minimal hateful content by treating content uniformly and failing to capture structured information within videos across modalities, limiting detection accuracy and interpretability.

Method: Partition each video into multiple instances to form an instance graph and extract instance-level features. Introduce a complementary weight graph to assign importance weights to features, emphasizing hateful instances. Combine weights with features to generate video labels. Employ a graph-based framework to model structured relationships within and across modalities in a dual-stream architecture.

Result: The method achieves state-of-the-art performance on public hateful video datasets and offers strong explainability. Code is provided at the project URL.

Conclusion: A graph-based, dual-stream multimodal approach that emphasizes hateful components through an importance-weighted instance graph and captures cross-modal structures yields superior hateful video classification performance and improves interpretability.

Abstract: Hateful videos present serious risks to online safety and real-world
well-being, necessitating effective detection methods. Although multimodal
classification approaches integrating information from several modalities
outperform unimodal ones, they typically neglect that even minimal hateful
content defines a video's category. Specifically, they generally treat all
content uniformly, instead of emphasizing the hateful components. Additionally,
existing multimodal methods cannot systematically capture structured
information in videos, limiting the effectiveness of multimodal fusion. To
address these limitations, we propose a novel multimodal dual-stream graph
neural network model. It constructs an instance graph by separating the given
video into several instances to extract instance-level features. Then, a
complementary weight graph assigns importance weights to these features,
highlighting hateful instances. Importance weights and instance features are
combined to generate video labels. Our model employs a graph-based framework to
systematically model structured relationships within and across modalities.
Extensive experiments on public datasets show that our model is
state-of-the-art in hateful video classification and has strong explainability.
Code is available:
https://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.

</details>


### [20] [ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors](https://arxiv.org/abs/2509.13525)
*Romain Hardy,Tyler Berzin,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: Diffusion-based depth estimation for monocular colonoscopy that yields temporally consistent depth maps via synthetic priors and style transfer, enabling 3D point clouds and surface analysis with zero-shot SOTA on C3VD; full trajectory reconstruction remains challenging.


<details>
  <summary>Details</summary>
Motivation: Accurate, temporally stable depth is essential for 3D understanding in colonoscopy, but current methods struggle with consistency across video frames, hindering reliable 3D reconstruction.

Method: A diffusion-based depth estimator trained with synthetic colonoscopy sequences to learn robust geometric priors; includes a style-transfer module to adapt real clinical videos to the synthetic domain while preserving geometry.

Result: Achieves state-of-the-art zero-shot performance on the C3VD dataset, outperforming general-purpose and endoscopy-specific baselines; demonstrates 3D point cloud generation and surface-coverage assessment; full trajectory 3D reconstruction remains challenging.

Conclusion: ColonCrafter demonstrates clinically relevant, temporally consistent depth estimation for colonoscopy with potential downstream benefits (3D visualization, coverage analysis), while acknowledging limitations in complete trajectory reconstruction and dependence on synthetic priors.

Abstract: Three-dimensional (3D) scene understanding in colonoscopy presents
significant challenges that necessitate automated methods for accurate depth
estimation. However, existing depth estimation models for endoscopy struggle
with temporal consistency across video sequences, limiting their applicability
for 3D reconstruction. We present ColonCrafter, a diffusion-based depth
estimation model that generates temporally consistent depth maps from monocular
colonoscopy videos. Our approach learns robust geometric priors from synthetic
colonoscopy sequences to generate temporally consistent depth maps. We also
introduce a style transfer technique that preserves geometric structure while
adapting real clinical videos to match our synthetic training domain.
ColonCrafter achieves state-of-the-art zero-shot performance on the C3VD
dataset, outperforming both general-purpose and endoscopy-specific approaches.
Although full trajectory 3D reconstruction remains a challenge, we demonstrate
clinically relevant applications of ColonCrafter, including 3D point cloud
generation and surface coverage assessment.

</details>


### [21] [MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM](https://arxiv.org/abs/2509.13536)
*Yinlong Bai,Hongxin Zhang,Sheng Zhong,Junkai Niu,Hai Li,Yijia He,Yi Zhou*

Main category: cs.CV

TL;DR: We present memory-efficient 3D Gaussian Splatting for embedded platforms by (1) merging redundant Gaussians in voxel space based on geometric similarity to reduce GPU memory, and (2) initializing Gaussians via Patch-Grid point sampling to improve rendering quality, validated on public datasets.


<details>
  <summary>Details</summary>
Motivation: Embedded platforms like micro air vehicles have strict limits on computation power and memory. Existing 3D Gaussian Splatting methods target desktops, risking performance/quality trade-offs on MAV hardware. There is a need to reduce memory usage while preserving or improving rendering quality.

Method: - Merge redundant 3D Gaussian primitives in voxel space based on geometric similarity to decrease GPU memory without changing runtime. - Initialize 3D Gaussian primitives using Patch-Grid (PG) point sampling to achieve more accurate scene modeling and better rendering quality.

Result: Memory usage is reduced without impacting runtime; rendering quality is improved thanks to PG-based initialization. Evaluations on public datasets show both quantitative and qualitative gains.

Conclusion: The approach delivers memory-efficient 3D Gaussian Splatting suitable for embedded platforms and enhances rendering quality, enabling better real-time performance for MAV-like systems.

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant
impact on rendering and reconstruction techniques. Current research
predominantly focuses on improving rendering performance and reconstruction
quality using high-performance desktop GPUs, largely overlooking applications
for embedded platforms like micro air vehicles (MAVs). These devices, with
their limited computational resources and memory, often face a trade-off
between system performance and reconstruction quality. In this paper, we
improve existing methods in terms of GPU memory usage while enhancing rendering
quality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we
propose merging them in voxel space based on geometric similarity. This reduces
GPU memory usage without impacting system runtime performance. Furthermore,
rendering quality is improved by initializing 3D Gaussian primitives via
Patch-Grid (PG) point sampling, enabling more accurate modeling of the entire
scene. Quantitative and qualitative evaluations on publicly available datasets
demonstrate the effectiveness of our improvements.

</details>


### [22] [Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles](https://arxiv.org/abs/2509.13577)
*Tongfei Guo,Lili Su*

Main category: cs.CV

TL;DR: Proposes an adaptive, QCD-based framework for trajectory-level OOD detection in autonomous driving that models time-evolving error modes. It achieves faster detection with fewer false alarms and outperforms prior UQ- and vision-based OOD methods on trajectory benchmarks.


<details>
  <summary>Details</summary>
Motivation: Distribution shifts in real-world driving cause out-of-distribution (OOD) cases for trajectory predictors. Trajectory-level OOD detection is underexplored, and prior work lacks robust, provable mechanisms; there is a need for adaptive, efficient detectors that handle complex driving dynamics.

Method: Builds on quickest change detection (QCD) with adaptive mechanisms. Models mode-dependent, time-evolving prediction errors by explicitly decomposing error modes, enabling robust, adaptive detection with improved trade-offs between delay and false alarms and greater computational efficiency.

Result: Demonstrates substantial improvements in detection delay and false alarm rates. On established trajectory prediction benchmarks, the proposed framework outperforms prior uncertainty quantification (UQ) and vision-based OOD approaches in both accuracy and computational efficiency.

Conclusion: Offers a practical path toward reliable, driving-aware autonomy by delivering robust, adaptive trajectory-level OOD detection that accommodates dataset-specific dynamics and distribution shifts.

Abstract: Trajectory prediction is central to the safe and seamless operation of
autonomous vehicles (AVs). In deployment, however, prediction models inevitably
face distribution shifts between training data and real-world conditions, where
rare or underrepresented traffic scenarios induce out-of-distribution (OOD)
cases. While most prior OOD detection research in AVs has concentrated on
computer vision tasks such as object detection and segmentation,
trajectory-level OOD detection remains largely underexplored. A recent study
formulated this problem as a quickest change detection (QCD) task, providing
formal guarantees on the trade-off between detection delay and false alarms
[1]. Building on this foundation, we propose a new framework that introduces
adaptive mechanisms to achieve robust detection in complex driving
environments. Empirical analysis across multiple real-world datasets reveals
that prediction errors -- even on in-distribution samples -- exhibit
mode-dependent distributions that evolve over time with dataset-specific
dynamics. By explicitly modeling these error modes, our method achieves
substantial improvements in both detection delay and false alarm rates.
Comprehensive experiments on established trajectory prediction benchmarks show
that our framework significantly outperforms prior UQ- and vision-based OOD
approaches in both accuracy and computational efficiency, offering a practical
path toward reliable, driving-aware autonomy.

</details>


### [23] [Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection](https://arxiv.org/abs/2509.13586)
*Nathalie Neptune,Josiane Mothe*

Main category: cs.CV

TL;DR: A deep-learning framework compares satellite image pairs to detect Amazon deforestation and auto-annotates changes with keywords derived from Amazon-related literature.


<details>
  <summary>Details</summary>
Motivation: Deforestation in the Amazon significantly affects climate, carbon emissions, and biodiversity. There is a need for scalable, automated monitoring and semantic labeling of detected changes to support research and policy.

Method: The approach compares images of the same area across dates using deep learning for change detection, and uses a visual semantic model to automatically annotate changes. Candidate annotations are extracted from scientific documents about the Amazon. Evaluation is performed on a dataset of Amazon image pairs.

Result: The method effectively detects deforestation and generates relevant annotations on the Amazon image-pair dataset.

Conclusion: The proposed framework provides a useful monitoring tool for deforestation and can be generalized to other domains beyond the Amazon.

Abstract: The Amazon rain forest is a vital ecosystem that plays a crucial role in
regulating the Earth's climate and providing habitat for countless species.
Deforestation in the Amazon is a major concern as it has a significant impact
on global carbon emissions and biodiversity. In this paper, we present a method
for detecting deforestation in the Amazon using image pairs from Earth
observation satellites. Our method leverages deep learning techniques to
compare the images of the same area at different dates and identify changes in
the forest cover. We also propose a visual semantic model that automatically
annotates the detected changes with relevant keywords. The candidate annotation
for images are extracted from scientific documents related to the Amazon
region. We evaluate our approach on a dataset of Amazon image pairs and
demonstrate its effectiveness in detecting deforestation and generating
relevant annotations. Our method provides a useful tool for monitoring and
studying the impact of deforestation in the Amazon. While we focus on
environment applications of our work by using images of deforestation in the
Amazon rain forest to demonstrate the effectiveness of our proposed approach,
it is generic enough to be applied to other domains.

</details>


### [24] [Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation](https://arxiv.org/abs/2509.13590)
*Samer Al-Hamadani*

Main category: cs.CV

TL;DR: A multimodal AI framework using Vision-Language Models (VLMs) and Google Gemini 2.5 Flash for automated tumor detection and clinical report generation across CT, MRI, X-ray, and Ultrasound, with advanced visualization and interpretability, showing strong anomaly detection but requiring clinical validation before deployment.


<details>
  <summary>Details</summary>
Motivation: To enhance diagnostic accuracy and radiology workflow efficiency by integrating image analysis and natural language processing across multiple imaging modalities, enabling automated reports and improved clinical confidence.

Method: A multimodal architecture combining visual feature extraction with NLP, coordinate verification, probabilistic Gaussian modeling for anomaly distribution, and multi-layered visualizations. Uses Google Gemini 2.5 Flash for automated detection and report generation, precise prompt engineering, zero-shot learning, and a Gradio interface for clinical workflow integration.

Result: High anomaly-detection performance across modalities; location accuracy around 80-pixel deviation; interpretable outputs via structured clinical information extraction; zero-shot learning capability to reduce data needs; scalable visualization and reporting.

Conclusion: Represents a significant step toward automated diagnostic support and radiology workflow efficiency, yet requires clinical validation and multi-center evaluation before widespread adoption.

Abstract: The rapid advancement of artificial intelligence (AI) in healthcare imaging
has revolutionized diagnostic medicine and clinical decision-making processes.
This work presents an intelligent multimodal framework for medical image
analysis that leverages Vision-Language Models (VLMs) in healthcare
diagnostics. The framework integrates Google Gemini 2.5 Flash for automated
tumor detection and clinical report generation across multiple imaging
modalities including CT, MRI, X-ray, and Ultrasound. The system combines visual
feature extraction with natural language processing to enable contextual image
interpretation, incorporating coordinate verification mechanisms and
probabilistic Gaussian modeling for anomaly distribution. Multi-layered
visualization techniques generate detailed medical illustrations, overlay
comparisons, and statistical representations to enhance clinical confidence,
with location measurement achieving 80 pixels average deviation. Result
processing utilizes precise prompt engineering and textual analysis to extract
structured clinical information while maintaining interpretability.
Experimental evaluations demonstrated high performance in anomaly detection
across multiple modalities. The system features a user-friendly Gradio
interface for clinical workflow integration and demonstrates zero-shot learning
capabilities to reduce dependence on large datasets. This framework represents
a significant advancement in automated diagnostic support and radiological
workflow efficiency, though clinical validation and multi-center evaluation are
necessary prior to widespread adoption.

</details>


### [25] [A Generalization of CLAP from 3D Localization to Image Processing, A Connection With RANSAC & Hough Transforms](https://arxiv.org/abs/2509.13605)
*Ruochen Hou,Gabriel I. Fernandez,Alex Xu,Dennis W. Hong*

Main category: cs.CV

TL;DR: CLAP, a clustering-based localization method originally for 2D, is generalized to 3D localization and image stitching, preserving robustness to outliers. It offers an alternative to RANSAC, and relates CLAP to Hough transforms; the framework is claimed to be widely applicable across fields dealing with noise and uncertainty.


<details>
  <summary>Details</summary>
Motivation: Address robustness to outliers and noise in localization and image alignment. Traditional outlier rejection (e.g., RANSAC) can be brittle or computationally intensive; CLAP’s clustering approach aims to suppress noisy feature matches and unify the localization process across multiple hypotheses.

Method: Generalize the CLAP framework from 2D localization to 3D localization and image stitching. Discuss theoretical connections among CLAP, RANSAC, and Hough transforms, and illustrate how clustering across multiple hypotheses can mitigate erroneous matches in diverse tasks.

Result: The work extends CLAP to 3D localization and image stitching and clarifies its relationship to RANSAC and Hough transforms. It proposes that CLAP’s clustering-based approach is broadly applicable for handling noise and uncertainty, suggesting a unified framework rather than a niche 2D solver.

Conclusion: CLAP’s generalization to 3D and stitching broadens its applicability, reinforcing clustering-based localization as a versatile tool for robust, outlier-tolerant estimation across diverse domains.

Abstract: In previous work, we introduced a 2D localization algorithm called CLAP,
Clustering to Localize Across $n$ Possibilities, which was used during our
championship win in RoboCup 2024, an international autonomous humanoid soccer
competition. CLAP is particularly recognized for its robustness against
outliers, where clustering is employed to suppress noise and mitigate against
erroneous feature matches. This clustering-based strategy provides an
alternative to traditional outlier rejection schemes such as RANSAC, in which
candidates are validated by reprojection error across all data points. In this
paper, CLAP is extended to a more general framework beyond 2D localization,
specifically to 3D localization and image stitching. We also show how CLAP,
RANSAC, and Hough transforms are related. The generalization of CLAP is widely
applicable to many different fields and can be a useful tool to deal with noise
and uncertainty.

</details>


### [26] [SAMIR, an efficient registration framework via robust feature learning from SAM](https://arxiv.org/abs/2509.13629)
*Yue He,Min Liu,Qinghao Liu,Jiazheng Wang,Yaonan Wang,Hang Zhang,Xiang Chen*

Main category: cs.CV

TL;DR: SAMIR is a medical image registration framework that leverages the Segment Anything Model (SAM) to extract structure-aware embeddings, refined by a lightweight 3D head, and guided by a Hierarchical Feature Consistency Loss to achieve improved registration accuracy on intra-subject cardiac and inter-subject abdomen CT datasets.


<details>
  <summary>Details</summary>
Motivation: Weak anatomical labels (segmentation masks, landmarks) are powerful but not always available; there is a need to leverage strong visual representations from foundation models to improve feature extraction for registration.

Method: Adapt SAM's image encoder to produce structure-aware feature embeddings for medical images, attach a lightweight 3D head to refine these embeddings for local deformations, and train with a Hierarchical Feature Consistency Loss that enforces coarse-to-fine feature alignment.

Result: The method reports state-of-the-art improvements over baselines, with 2.68% improvement on ACDC (intra-subject cardiac) and 6.44% on an abdomen CT dataset (inter-subject).

Conclusion: Integrating SAM-based feature representations into medical image registration, coupled with a hierarchical loss and a compact 3D refinement head, yields superior alignment performance and demonstrates the practical potential of foundation-model-driven registration without heavy reliance on task-specific weak labels.

Abstract: Image registration is a fundamental task in medical image analysis.
Deformations are often closely related to the morphological characteristics of
tissues, making accurate feature extraction crucial. Recent weakly supervised
methods improve registration by incorporating anatomical priors such as
segmentation masks or landmarks, either as inputs or in the loss function.
However, such weak labels are often not readily available, limiting their
practical use. Motivated by the strong representation learning ability of
visual foundation models, this paper introduces SAMIR, an efficient medical
image registration framework that utilizes the Segment Anything Model (SAM) to
enhance feature extraction. SAM is pretrained on large-scale natural image
datasets and can learn robust, general-purpose visual representations. Rather
than using raw input images, we design a task-specific adaptation pipeline
using SAM's image encoder to extract structure-aware feature embeddings,
enabling more accurate modeling of anatomical consistency and deformation
patterns. We further design a lightweight 3D head to refine features within the
embedding space, adapting to local deformations in medical images.
Additionally, we introduce a Hierarchical Feature Consistency Loss to guide
coarse-to-fine feature matching and improve anatomical alignment. Extensive
experiments demonstrate that SAMIR significantly outperforms state-of-the-art
methods on benchmark datasets for both intra-subject cardiac image registration
and inter-subject abdomen CT image registration, achieving performance
improvements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code
will be publicly available on GitHub following the acceptance of this paper.

</details>


### [27] [Federated Learning for Deforestation Detection: A Distributed Approach with Satellite Imagery](https://arxiv.org/abs/2509.13631)
*Yuvraj Dutta,Aaditya Sikder,Basabdatta Palit*

Main category: cs.CV

TL;DR: Federated learning framework for deforestation detection from satellite imagery using FLOWER+RAY, deploying YOLOS-small and Faster R-CNN variants across edge clients to preserve data privacy and enable distributed segmentation.


<details>
  <summary>Details</summary>
Motivation: Deforestation mapping requires analyzing data from multiple distributed sources while preserving data privacy; centralized data collection is risky and often impractical.

Method: Proposes a FL system using FLOWER with RAY for distributed training across edge satellite centers; efficient client spawning; emulates multiple clients; trains and tests YOLOS-small, Faster R-CNN (ResNet50) and Faster R-CNN (MobileNetV3) on public datasets; focuses on image segmentation tasks in satellite imagery.

Result: The abstract emphasizes framework feasibility and a new perspective on segmentation tasks rather than reporting quantitative metrics; no explicit performance results provided.

Conclusion: A privacy-preserving, distributed approach for deforestation identification and localization is feasible and can leverage multiple segmentation models within FL, offering an alternative to centralized training.

Abstract: Accurate identification of deforestation from satellite images is essential
in order to understand the geographical situation of an area. This paper
introduces a new distributed approach to identify as well as locate
deforestation across different clients using Federated Learning (FL). Federated
Learning enables distributed network clients to collaboratively train a model
while maintaining data privacy and security of the active users. In our
framework, a client corresponds to an edge satellite center responsible for
local data processing. Moreover, FL provides an advantage over centralized
training method which requires combining data, thereby compromising with data
security of the clients. Our framework leverages the FLOWER framework with RAY
framework to execute the distributed learning workload. Furthermore, efficient
client spawning is ensured by RAY as it can select definite amount of users to
create an emulation environment. Our FL framework uses YOLOS-small (a Vision
Transformer variant), Faster R-CNN with a ResNet50 backbone, and Faster R-CNN
with a MobileNetV3 backbone models trained and tested on publicly available
datasets. Our approach provides us a different view for image
segmentation-based tasks on satellite imagery.

</details>


### [28] [Gaussian Alignment for Relative Camera Pose Estimation via Single-View Reconstruction](https://arxiv.org/abs/2509.13652)
*Yumin Li,Dylan Campbell*

Main category: cs.CV

TL;DR: A training-free framework GARPS estimates metric relative camera pose from a pair of images by aligning metric 3D Gaussian Mixture Models reconstructed separately from each view; it refines an initial pose via a differentiable GMM alignment objective that uses structure, colors, anisotropic covariances, and semantics, achieving robust metric relative pose estimation without explicit 2D correspondences and outperforming state-of-the-art methods on RealEstate10K.


<details>
  <summary>Details</summary>
Motivation: The task is metric relative pose estimation from two views, where traditional two-view methods yield scale-ambiguous translations and struggle with wide baselines or textureless/reflective surfaces. There is a need for a training-free approach that leverages single-view depth to construct metric 3D representations and aligns them to recover a metric pose without relying on 2D feature correspondences.

Method: GARPS is training-free. It constructs a metric 3D Gaussian Mixture Model (GMM) for each image using a metric monocular depth estimator and a Gaussian scene reconstructor. It starts from an initial pose provided by a feed-forward two-view pose estimator and refines it by optimizing a differentiable GMM alignment objective that jointly accounts for geometric structure, view-independent color, anisotropic covariance, and semantic feature consistency, and is robust to occlusions and textureless regions without explicit 2D correspondences.

Result: On RealEstate10K, GARPS outperforms both classical methods and state-of-the-art learning-based methods (including MASt3R) for metric relative pose estimation.

Conclusion: A robust, training-free approach that bridges single-view depth perception with multi-view geometry to achieve metric relative pose estimation without explicit 2D correspondences, showing strong empirical performance and potential for robust 3D reconstruction/localisation.

Abstract: Estimating metric relative camera pose from a pair of images is of great
importance for 3D reconstruction and localisation. However, conventional
two-view pose estimation methods are not metric, with camera translation known
only up to a scale, and struggle with wide baselines and textureless or
reflective surfaces. This paper introduces GARPS, a training-free framework
that casts this problem as the direct alignment of two independently
reconstructed 3D scenes. GARPS leverages a metric monocular depth estimator and
a Gaussian scene reconstructor to obtain a metric 3D Gaussian Mixture Model
(GMM) for each image. It then refines an initial pose from a feed-forward
two-view pose estimator by optimising a differentiable GMM alignment objective.
This objective jointly considers geometric structure, view-independent colour,
anisotropic covariance, and semantic feature consistency, and is robust to
occlusions and texture-poor regions without requiring explicit 2D
correspondences. Extensive experiments on the Real\-Estate10K dataset
demonstrate that GARPS outperforms both classical and state-of-the-art
learning-based methods, including MASt3R. These results highlight the potential
of bridging single-view perception with multi-view geometry to achieve robust
and metric relative pose estimation.

</details>


### [29] [Deep Lookup Network](https://arxiv.org/abs/2509.13662)
*Yulan Guo,Longguang Wang,Wendong Mao,Xiaoyu Dong,Yingqian Wang,Li Liu,Wei An*

Main category: cs.CV

TL;DR: Replace multiplications in CNNs with differentiable lookup operations to improve energy efficiency and speed while maintaining performance across image and point-cloud tasks.


<details>
  <summary>Details</summary>
Motivation: Multiplication is a major bottleneck in CNNs on resource-constrained devices; lookup tables offer cheaper computation; need an end-to-end trainable mechanism for integrating LUTs into networks.

Method: Introduce a differentiable lookup operation and construct differentiable lookup tables; develop training strategies to ensure convergence; replace heavy multiplications with LUTs within neural networks; apply to image classification, image super-resolution, and point-cloud classification.

Result: Lookup-enabled networks demonstrate improved efficiency (lower energy, faster inference) while maintaining competitive performance; reported achieving strong or state-of-the-art results across diverse tasks and data types (images and point clouds).

Conclusion: Differentiable lookup networks present a viable, broadly applicable approach for efficient neural computation on edge devices, enabling end-to-end optimization and applicability to various data modalities.

Abstract: Convolutional neural networks are constructed with massive operations with
different types and are highly computationally intensive. Among these
operations, multiplication operation is higher in computational complexity and
usually requires {more} energy consumption with longer inference time than
other operations, which hinders the deployment of convolutional neural networks
on mobile devices. In many resource-limited edge devices, complicated
operations can be calculated via lookup tables to reduce computational cost.
Motivated by this, in this paper, we introduce a generic and efficient lookup
operation which can be used as a basic operation for the construction of neural
networks. Instead of calculating the multiplication of weights and activation
values, simple yet efficient lookup operations are adopted to compute their
responses. To enable end-to-end optimization of the lookup operation, we
construct the lookup tables in a differentiable manner and propose several
training strategies to promote their convergence. By replacing computationally
expensive multiplication operations with our lookup operations, we develop
lookup networks for the image classification, image super-resolution, and point
cloud classification tasks. It is demonstrated that our lookup networks can
benefit from the lookup operations to achieve higher efficiency in terms of
energy consumption and inference speed while maintaining competitive
performance to vanilla convolutional networks. Extensive experiments show that
our lookup networks produce state-of-the-art performance on different tasks
(both classification and regression tasks) and different data types (both
images and point clouds).

</details>


### [30] [Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation](https://arxiv.org/abs/2509.13676)
*Xiaobo Yang,Xiaojin Gong*

Main category: cs.CV

TL;DR: Introduces a semantic visual projector using SAM-generated semantic superpixels as visual words to compress visual tokens for RIS with MLLMs, achieving 93% token reduction while preserving performance and speeding up training/inference.


<details>
  <summary>Details</summary>
Motivation: Token redundancy in visual inputs for RIS using Multimodal LLMs leads to high computation. Traditional patch-based visual projectors struggle to both reduce tokens and preserve semantic clarity, requiring adaptive, semantics-aware compression.

Method: Use SAM-generated semantic superpixels as the basis for visual tokens, compress and project these superpixels as tokens (visual words). Add semantic superpixel positional embeddings to encode geometry/position, and a semantic superpixel aggregator to preserve fine-grained details inside superpixels while maintaining global context outside. The method adapts token sequence length to scene complexity.

Result: Cuts visual tokens by 93% without compromising RIS performance; speeds up MLLM training and inference and outperforms existing compressive visual projectors on RIS.

Conclusion: Semantic superpixel-based projection is effective for substantial token compression with negligible impact on RIS performance, enabling faster, more scalable RIS systems with MLLMs and SAM.

Abstract: Recently, Referring Image Segmentation (RIS) frameworks that pair the
Multimodal Large Language Model (MLLM) with the Segment Anything Model (SAM)
have achieved impressive results. However, adapting MLLM to segmentation is
computationally intensive, primarily due to visual token redundancy. We observe
that traditional patch-wise visual projectors struggle to strike a balance
between reducing the number of visual tokens and preserving semantic clarity,
often retaining overly long token sequences to avoid performance drops.
Inspired by text tokenizers, we propose a novel semantic visual projector that
leverages semantic superpixels generated by SAM to identify "visual words" in
an image. By compressing and projecting semantic superpixels as visual tokens,
our approach adaptively shortens the token sequence according to scene
complexity while minimizing semantic loss in compression. To mitigate loss of
information, we propose a semantic superpixel positional embedding to
strengthen MLLM's awareness of superpixel geometry and position, alongside a
semantic superpixel aggregator to preserve both fine-grained details inside
superpixels and global context outside. Experiments show that our method cuts
visual tokens by 93% without compromising performance, notably speeding up MLLM
training and inference, and outperforming existing compressive visual
projectors on RIS.

</details>


### [31] [FishBEV: Distortion-Resilient Bird's Eye View Segmentation with Surround-View Fisheye Cameras](https://arxiv.org/abs/2509.13681)
*Hang Li,Dianmo Sheng,Qiankun Dong,Zichun Wang,Zhiwei Xu,Tao Li*

Main category: cs.CV

TL;DR: FishBEV tackles fisheye BEV segmentation with a distortion-resilient multi-scale backbone, uncertainty-aware spatial cross-attention, and distance-aware temporal self-attention, achieving state-of-the-art results on Synwoodscapes.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of BEV segmentation with fisheye cameras, where severe geometric distortion, ambiguous multi-view correspondences, and unstable temporal dynamics hinder performance. A robust feature extractor, reliable cross-view alignment, and coherent temporal modeling are needed.

Method: Introduce three components: (1) Distortion-Resilient Multi-scale Extraction (DRME) backbone to learn robust, distortion-tolerant features with scale consistency; (2) Uncertainty-aware Spatial Cross-Attention (U-SCA) to leverage uncertainty estimates for reliable cross-view alignment; (3) Distance-aware Temporal Self-Attention (D-TSA) to balance near-field details and far-field context for temporal coherence.

Result: Empirical evaluation on the Synwoodscapes dataset shows FishBEV consistently outperforms state-of-the-art baselines for surround-view fisheye BEV segmentation tasks.

Conclusion: FishBEV effectively addresses distortion, view-uncertainty, and temporal dynamics in fisheye BEV segmentation, achieving superior performance and advancing robust BEV perception for fisheye cameras.

Abstract: As a cornerstone technique for autonomous driving, Bird's Eye View (BEV)
segmentation has recently achieved remarkable progress with pinhole cameras.
However, it is non-trivial to extend the existing methods to fisheye cameras
with severe geometric distortion, ambiguous multi-view correspondences and
unstable temporal dynamics, all of which significantly degrade BEV performance.
To address these challenges, we propose FishBEV, a novel BEV segmentation
framework specifically tailored for fisheye cameras. This framework introduces
three complementary innovations, including a Distortion-Resilient Multi-scale
Extraction (DRME) backbone that learns robust features under distortion while
preserving scale consistency, an Uncertainty-aware Spatial Cross-Attention
(U-SCA) mechanism that leverages uncertainty estimation for reliable cross-view
alignment, a Distance-aware Temporal Self-Attention (D-TSA) module that
adaptively balances near field details and far field context to ensure temporal
coherence. Extensive experiments on the Synwoodscapes dataset demonstrate that
FishBEV consistently outperforms SOTA baselines, regarding the performance
evaluation of FishBEV on the surround-view fisheye BEV segmentation tasks.

</details>


### [32] [Taylor-Series Expanded Kolmogorov-Arnold Network for Medical Imaging Classification](https://arxiv.org/abs/2509.13687)
*Kaniz Fatema,Emad A. Mohammed,Sukhjit Singh Sehra*

Main category: cs.CV

TL;DR: Spline-based Kolmogorov-Arnold Networks (KANs) offer accurate, interpretable medical image classification with far fewer parameters, suitable for data-scarce settings.


<details>
  <summary>Details</summary>
Motivation: Medical image classification in resource-limited clinical environments requires lightweight, generalizable and interpretable models; conventional CNNs are data-hungry and computationally heavy.

Method: Introduce three spline-based KAN variants: SBTAYLOR-KAN (B-splines + Taylor series), SBRBF-KAN (B-splines + Radial Basis Functions), SBWAVELET-KAN (B-splines embedded in Morlet wavelets). Train on raw data without preprocessing, conduct cross-dataset validation and data-reduction analyses, and use Grad-CAM for interpretability.

Result: SBTAYLOR-KAN achieves up to 98.93% accuracy; maintains >86% accuracy with only 30% of the training data across three datasets. Performs well on imbalanced and balanced skin cancer data, outperforming other models. Uses only 2,872 trainable parameters vs ResNet50's ~24M. Grad-CAM highlights relevant regions for interpretability.

Conclusion: The spline-based KAN framework is a lightweight, interpretable, and generalizable solution for medical image classification in data-scarce clinical scenarios, addressing limitations of traditional deep CNNs in constrained environments.

Abstract: Effective and interpretable classification of medical images is a challenge
in computer-aided diagnosis, especially in resource-limited clinical settings.
This study introduces spline-based Kolmogorov-Arnold Networks (KANs) for
accurate medical image classification with limited, diverse datasets. The
models include SBTAYLOR-KAN, integrating B-splines with Taylor series;
SBRBF-KAN, combining B-splines with Radial Basis Functions; and SBWAVELET-KAN,
embedding B-splines in Morlet wavelet transforms. These approaches leverage
spline-based function approximation to capture both local and global
nonlinearities. The models were evaluated on brain MRI, chest X-rays,
tuberculosis X-rays, and skin lesion images without preprocessing,
demonstrating the ability to learn directly from raw data. Extensive
experiments, including cross-dataset validation and data reduction analysis,
showed strong generalization and stability. SBTAYLOR-KAN achieved up to 98.93%
accuracy, with a balanced F1-score, maintaining over 86% accuracy using only
30% of the training data across three datasets. Despite class imbalance in the
skin cancer dataset, experiments on both imbalanced and balanced versions
showed SBTAYLOR-KAN outperforming other models, achieving 68.22% accuracy.
Unlike traditional CNNs, which require millions of parameters (e.g., ResNet50
with 24.18M), SBTAYLOR-KAN achieves comparable performance with just 2,872
trainable parameters, making it more suitable for constrained medical
environments. Gradient-weighted Class Activation Mapping (Grad-CAM) was used
for interpretability, highlighting relevant regions in medical images. This
framework provides a lightweight, interpretable, and generalizable solution for
medical image classification, addressing the challenges of limited datasets and
data-scarce scenarios in clinical AI applications.

</details>


### [33] [StyleProtect: Safeguarding Artistic Identity in Fine-tuned Diffusion Models](https://arxiv.org/abs/2509.13711)
*Qiuyu Tang,Joshua Krinsky,Aparna Bharati*

Main category: cs.CV

TL;DR: A lightweight defense, StyleProtect, protects artistic styles from being stolen by fine-tuned diffusion models by selectively updating cross-attention layers, achieving effective style protection with competitive perceptual imperceptibility.


<details>
  <summary>Details</summary>
Motivation: To counter diffusion-model-enabled style mimicry that risks devaluing artists' labor by replicating distinctive styles, including through fine-tuning.

Method: Analyze cross-attention layer activations to identify style-sensitive layers, measure sensitivity via activation strengths and correlations with external feature representations; update only selected cross-attention layers; evaluate on WikiArt-based 30-artist artworks and Anita anime dataset.

Result: StyleProtect provides effective defense against style customization by diffusion models while maintaining perceptual quality, with lightweight updates (few layers) and competitive imperceptibility.

Conclusion: Cross-attention layers show heightened sensitivity to artistic styles; targeted locking of these layers offers a practical, efficient defense to protect artwork styles against adversarial diffusion model fine-tuning.

Abstract: The rapid advancement of generative models, particularly diffusion-based
approaches, has inadvertently facilitated their potential for misuse. Such
models enable malicious exploiters to replicate artistic styles that capture an
artist's creative labor, personal vision, and years of dedication in an
inexpensive manner. This has led to a rise in the need and exploration of
methods for protecting artworks against style mimicry. Although generic
diffusion models can easily mimic an artistic style, finetuning amplifies this
capability, enabling the model to internalize and reproduce the style with
higher fidelity and control. We hypothesize that certain cross-attention layers
exhibit heightened sensitivity to artistic styles. Sensitivity is measured
through activation strengths of attention layers in response to style and
content representations, and assessing their correlations with features
extracted from external models. Based on our findings, we introduce an
efficient and lightweight protection strategy, StyleProtect, that achieves
effective style defense against fine-tuned diffusion models by updating only
selected cross-attention layers. Our experiments utilize a carefully curated
artwork dataset based on WikiArt, comprising representative works from 30
artists known for their distinctive and influential styles and cartoon
animations from the Anita dataset. The proposed method demonstrates promising
performance in safeguarding unique styles of artworks and anime from malicious
diffusion customization, while maintaining competitive imperceptibility.

</details>


### [34] [UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry](https://arxiv.org/abs/2509.13713)
*Tae-Wook Um,Ki-Hyeon Kim,Hyun-Duck Choi,Hyo-Sung Ahn*

Main category: cs.CV

TL;DR: UM-Depth introduces a teacher-student framework with uncertainty-aware refinement and uses optical flow only in the teacher during training to boost self-supervised monocular depth and pose estimation, achieving state-of-the-art results on KITTI/Cityscapes with no additional runtime cost.


<details>
  <summary>Details</summary>
Motivation: Self-supervised monocular depth suffers from uncertainty in low-texture and dynamic regions, leading to degraded depth accuracy. There is a need for stronger supervision in weak photometric signals without adding runtime overhead.

Method: A teacher-student architecture with built-in uncertainty estimation embedded in both training and network. Uncertainty-aware refinement targets dynamic object boundaries and textureless regions. Optical flow is used exclusively in the teacher network during training to guide learning, avoiding extra labels and runtime costs at inference.

Result: Extensive experiments on KITTI and Cityscapes demonstrate improved depth accuracy and pose estimation, achieving state-of-the-art results for self-supervised depth and pose on KITTI datasets.

Conclusion: Uncertainty-aware, motion-refinement via a training-only optical-flow teacher enhances self-supervised depth/pose estimation without increasing inference cost, with strong performance on KITTI (and Cityscapes).

Abstract: Monocular depth estimation has been increasingly adopted in robotics and
autonomous driving for its ability to infer scene geometry from a single
camera. In self-supervised monocular depth estimation frameworks, the network
jointly generates and exploits depth and pose estimates during training,
thereby eliminating the need for depth labels. However, these methods remain
challenged by uncertainty in the input data, such as low-texture or dynamic
regions, which can cause reduced depth accuracy. To address this, we introduce
UM-Depth, a framework that combines motion- and uncertainty-aware refinement to
enhance depth accuracy at dynamic object boundaries and in textureless regions.
Specifically, we develop a teacherstudent training strategy that embeds
uncertainty estimation into both the training pipeline and network
architecture, thereby strengthening supervision where photometric signals are
weak. Unlike prior motion-aware approaches that incur inference-time overhead
and rely on additional labels or auxiliary networks for real-time generation,
our method uses optical flow exclusively within the teacher network during
training, which eliminating extra labeling demands and any runtime cost.
Extensive experiments on the KITTI and Cityscapes datasets demonstrate the
effectiveness of our uncertainty-aware refinement. Overall, UM-Depth achieves
state-of-the-art results in both self-supervised depth and pose estimation on
the KITTI datasets.

</details>


### [35] [Mitigating Query Selection Bias in Referring Video Object Segmentation](https://arxiv.org/abs/2509.13722)
*Dingwei Zhang,Dong Zhang,Jinhui Tang*

Main category: cs.CV

TL;DR: Triple Query Former (TQF) for RVOS decomposes the referring query into appearance, intra-frame spatial, and inter-frame motion components, dynamically combining linguistic and visual cues; introduces intra-frame and inter-frame motion-aware aggregations to improve cross-frame coherence and reduce query selection bias, achieving superior RVOS performance.


<details>
  <summary>Details</summary>
Motivation: Static, text-only queries are prone to distractors and misalignment (query selection bias). There is a need for structured, motion-aware querying and aggregation to leverage both textual and visual cues for robust RVOS.

Method: Propose TQF with three specialized query components; dynamically construct queries by integrating linguistic cues and visual guidance; introduce Intra-frame Interaction Aggregation and Inter-frame Motion Aggregation to improve token representations and temporal coherence.

Result: Extensive experiments on RVOS benchmarks show improved performance and validate the effectiveness of structured query design and motion-aware aggregations; ablations support each component's contribution.

Conclusion: TQF alleviates query selection bias and improves RVOS by combining structured, motion-aware queries with cross-frame aggregation; potential for extension to other cross-modal alignment tasks.

Abstract: Recently, query-based methods have achieved remarkable performance in
Referring Video Object Segmentation (RVOS) by using textual static object
queries to drive cross-modal alignment. However, these static queries are
easily misled by distractors with similar appearance or motion, resulting in
\emph{query selection bias}. To address this issue, we propose Triple Query
Former (TQF), which factorizes the referring query into three specialized
components: an appearance query for static attributes, an intra-frame
interaction query for spatial relations, and an inter-frame motion query for
temporal association. Instead of relying solely on textual embeddings, our
queries are dynamically constructed by integrating both linguistic cues and
visual guidance. Furthermore, we introduce two motion-aware aggregation modules
that enhance object token representations: Intra-frame Interaction Aggregation
incorporates position-aware interactions among objects within a single frame,
while Inter-frame Motion Aggregation leverages trajectory-guided alignment
across frames to ensure temporal coherence. Extensive experiments on multiple
RVOS benchmarks demonstrate the advantages of TQF and the effectiveness of our
structured query design and motion-aware aggregation modules.

</details>


### [36] [Improving Generalized Visual Grounding with Instance-aware Joint Learning](https://arxiv.org/abs/2509.13747)
*Ming Dai,Wenxuan Cheng,Jiang-Jiang Liu,Lingfeng Yang,Zhenhua Feng,Wankou Yang,Jingdong Wang*

Main category: cs.CV

TL;DR: InstanceVG is a pioneering multi-task generalized visual grounding framework that jointly handles GREC and GRES with instance-aware capabilities, using instance queries with prior reference points to unify predictions across points, boxes, and masks, achieving SOTA across 10 datasets.


<details>
  <summary>Details</summary>
Motivation: Generalized visual grounding tasks extend visual grounding to multi-target and non-target scenarios (GREC and GRES). Existing methods usually treat GREC and GRES separately and ignore cross-granularity consistency and instance-level coherence between boxes and masks.

Method: InstanceVG introduces instance queries, each with an assigned prior reference point, to unify joint and consistency predictions for points, boxes, and masks of the same instance. It jointly tackles GREC and GRES in a single framework and enforces instance-aware predictions.

Result: Reported state-of-the-art performance on ten datasets across four tasks, with significant improvements over existing methods on various evaluation metrics.

Conclusion: Proposes the first framework to simultaneously address GREC and GRES with instance-aware capabilities, demonstrating the benefits of joint multi-granularity predictions and consistency and setting a new standard for generalized visual grounding.

Abstract: Generalized visual grounding tasks, including Generalized Referring
Expression Comprehension (GREC) and Segmentation (GRES), extend the classical
visual grounding paradigm by accommodating multi-target and non-target
scenarios. Specifically, GREC focuses on accurately identifying all referential
objects at the coarse bounding box level, while GRES aims for achieve
fine-grained pixel-level perception. However, existing approaches typically
treat these tasks independently, overlooking the benefits of jointly training
GREC and GRES to ensure consistent multi-granularity predictions and streamline
the overall process. Moreover, current methods often treat GRES as a semantic
segmentation task, neglecting the crucial role of instance-aware capabilities
and the necessity of ensuring consistent predictions between instance-level
boxes and masks. To address these limitations, we propose InstanceVG, a
multi-task generalized visual grounding framework equipped with instance-aware
capabilities, which leverages instance queries to unify the joint and
consistency predictions of instance-level boxes and masks. To the best of our
knowledge, InstanceVG is the first framework to simultaneously tackle both GREC
and GRES while incorporating instance-aware capabilities into generalized
visual grounding. To instantiate the framework, we assign each instance query a
prior reference point, which also serves as an additional basis for target
matching. This design facilitates consistent predictions of points, boxes, and
masks for the same instance. Extensive experiments obtained on ten datasets
across four tasks demonstrate that InstanceVG achieves state-of-the-art
performance, significantly surpassing the existing methods in various
evaluation metrics. The code and model will be publicly available at
https://github.com/Dmmm1997/InstanceVG.

</details>


### [37] [Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval](https://arxiv.org/abs/2509.13754)
*Hao Yin,Xin Man,Feiyu Chen,Jie Shao,Heng Tao Shen*

Main category: cs.CV

TL;DR: FMFA is a cross-modal TIPR framework that uses Adaptive Similarity Distribution Matching (A-SDM) and Explicit Fine-grained Alignment (EFA) to achieve full-mode fine-grained alignment, improving global matching without extra supervision and achieving state-of-the-art results on three datasets.


<details>
  <summary>Details</summary>
Motivation: Cross-modal TIPR relies on aligning text and image representations in a shared space. Prior attention-based implicit local alignment lacks verification of whether local features are correctly aligned and often overlooks unmatched positive samples, focusing mainly on hard negatives.

Method: Propose FMFA. It combines Adaptive Similarity Distribution Matching (A-SDM) to pull unmatched positive samples closer in the joint embedding space, and Explicit Fine-grained Alignment (EFA) to enforce explicit, fine-grained cross-modal interactions by sparsifying the similarity matrix and using a hard coding approach for local alignment. No additional supervision is required.

Result: Outperforms existing global-matching methods on three public TIPR datasets, achieving state-of-the-art results. Code is released at the provided GitHub link.

Conclusion: FMFA demonstrates that combining explicit fine-grained alignment with implicit relational reasoning (full-mode alignment) improves both the verification of local alignment and the overall global matching performance in TIPR, without extra supervision.

Abstract: Text-to-Image Person Retrieval (TIPR) is a cross-modal matching task that
aims to retrieve the most relevant person images based on a given text query.
The key challenge in TIPR lies in achieving effective alignment between textual
and visual modalities within a common latent space. To address this challenge,
prior approaches incorporate attention mechanisms for implicit cross-modal
local alignment. However, they lack the ability to verify whether all local
features are correctly aligned. Moreover, existing methods primarily focus on
hard negative samples during model updates, with the goal of refining
distinctions between positive and negative pairs, often neglecting incorrectly
matched positive pairs. To alleviate these issues, we propose FMFA, a
cross-modal Full-Mode Fine-grained Alignment framework, which enhances global
matching through explicit fine-grained alignment and existing implicit
relational reasoning -- hence the term ``full-mode" -- without requiring
additional supervision. Specifically, we design an Adaptive Similarity
Distribution Matching (A-SDM) module to rectify unmatched positive sample
pairs. A-SDM adaptively pulls the unmatched positive pairs closer in the joint
embedding space, thereby achieving more precise global alignment. Additionally,
we introduce an Explicit Fine-grained Alignment (EFA) module, which makes up
for the lack of verification capability of implicit relational reasoning. EFA
strengthens explicit cross-modal fine-grained interactions by sparsifying the
similarity matrix and employs a hard coding method for local alignment. Our
proposed method is evaluated on three public datasets, achieving
state-of-the-art performance among all global matching methods. Our code is
available at https://github.com/yinhao1102/FMFA.

</details>


### [38] [Controllable-Continuous Color Editing in Diffusion Model via Color Mapping](https://arxiv.org/abs/2509.13756)
*Yuqi Yang,Dongliang Chang,Yuanchen Fang,Yi-Zhe SonG,Zhanyu Ma,Jun Guo*

Main category: cs.CV

TL;DR: Proposes a color mapping module to bridge text embeddings and RGB values for text-driven image editing, enabling precise, continuous color control within user-specified RGB ranges while preserving semantic consistency.


<details>
  <summary>Details</summary>
Motivation: Natural language descriptions are ambiguous and discrete, causing imprecise color edits. There is a need for continuous, controllable color editing that maps text to exact colors.

Method: Introduce a color mapping module that learns the correspondence between text embedding space and image RGB values. This module can predict an embedding vector from a given RGB value, allowing users to specify a target RGB range and obtain continuous color variations that remain semantically aligned with the text.

Result: Experiments show improved color continuity and controllability in generated images, validating the proposed mapping approach.

Conclusion: The method enables finer-grained, continuous, and controllable color editing in text-driven image generation by explicitly modeling text→RGB mappings and allowing user-defined color ranges.

Abstract: In recent years, text-driven image editing has made significant progress.
However, due to the inherent ambiguity and discreteness of natural language,
color editing still faces challenges such as insufficient precision and
difficulty in achieving continuous control. Although linearly interpolating the
embedding vectors of different textual descriptions can guide the model to
generate a sequence of images with varying colors, this approach lacks precise
control over the range of color changes in the output images. Moreover, the
relationship between the interpolation coefficient and the resulting image
color is unknown and uncontrollable. To address these issues, we introduce a
color mapping module that explicitly models the correspondence between the text
embedding space and image RGB values. This module predicts the corresponding
embedding vector based on a given RGB value, enabling precise color control of
the generated images while maintaining semantic consistency. Users can specify
a target RGB range to generate images with continuous color variations within
the desired range, thereby achieving finer-grained, continuous, and
controllable color editing. Experimental results demonstrate that our method
performs well in terms of color continuity and controllability.

</details>


### [39] [Iterative Prompt Refinement for Safer Text-to-Image Generation](https://arxiv.org/abs/2509.13760)
*Jinwoo Jeon,JunHyeok Oh,Hayeong Lee,Byung-Jun Lee*

Main category: cs.CV

TL;DR: Iterative prompt refinement using Vision-Language Models (VLMs) analyzes both prompts and generated images to enhance safety in text-to-image generation while preserving user intent; introduces a multimodal safety dataset and provides code.


<details>
  <summary>Details</summary>
Motivation: Current safety pipelines rely on LLM-driven prompt edits and overlook image content, risking unsafe outputs or unnecessary changes to safe prompts. Incorporating visual feedback from VLMs can improve safety and preserve user intent.

Method: An iterative loop where VLMs analyze input prompts and the produced images to refine prompts. Builds a new multimodal safety dataset labeled with textual and visual safety signals using off-the-shelf multi-modal LLMs, enabling supervised fine-tuning. Evaluations compare to LLM-based safety baselines and the approach preserves user intent while improving safety.

Result: Safer outputs are achieved without compromising alignment to user intent, with performance comparable to or better than LLM-based approaches. A dataset and code are released to support adoption.

Conclusion: Vision-language informed prompt refinement is a practical, effective path to safer T2I content generation, enabled by a multimodal safety dataset for supervised fine-tuning.

Abstract: Text-to-Image (T2I) models have made remarkable progress in generating images
from text prompts, but their output quality and safety still depend heavily on
how prompts are phrased. Existing safety methods typically refine prompts using
large language models (LLMs), but they overlook the images produced, which can
result in unsafe outputs or unnecessary changes to already safe prompts. To
address this, we propose an iterative prompt refinement algorithm that uses
Vision Language Models (VLMs) to analyze both the input prompts and the
generated images. By leveraging visual feedback, our method refines prompts
more effectively, improving safety while maintaining user intent and
reliability comparable to existing LLM-based approaches. Additionally, we
introduce a new dataset labeled with both textual and visual safety signals
using off-the-shelf multi-modal LLM, enabling supervised fine-tuning.
Experimental results demonstrate that our approach produces safer outputs
without compromising alignment with user intent, offering a practical solution
for generating safer T2I content. Our code is available at
https://github.com/ku-dmlab/IPR. \textbf{\textcolor{red}WARNING: This paper
contains examples of harmful or inappropriate images generated by models.

</details>


### [40] [Task-Aware Image Signal Processor for Advanced Visual Perception](https://arxiv.org/abs/2509.13762)
*Kai Chen,Jin Xiao,Leheng Zhang,Kexuan Shi,Shuhang Gu*

Main category: cs.CV

TL;DR: TA-ISP introduces a compact RAW-to-RGB front-end that uses a small set of multi-scale modulation operators to produce task-oriented representations for pretrained vision models, achieving better downstream accuracy with far less computation than large ISP networks or traditional pipelines.


<details>
  <summary>Details</summary>
Motivation: RAW data preserves richer information than RGB, but large ISP networks incur heavy compute and tuning traditional ISP pipelines has limited representational capacity. A task-aware, lightweight front-end can boost vision-task performance (e.g., detection/segmentation) while meeting resource constraints for daytime/nighttime scenarios.

Method: Propose Task-Aware Image Signal Processing (TA-ISP): a compact RAW-to-RGB framework that predicts a small set of lightweight, multi-scale modulation operators acting globally, regionally, and at the pixel level. This factorized control reshapes image statistics across spatial extents to generate task-oriented representations for pretrained vision models, reducing memory, computation, and latency while expanding the range of possible transforms.

Result: Evaluated on RAW-domain detection and segmentation benchmarks under daytime and nighttime conditions, TA-ISP consistently improves downstream accuracy while substantially reducing parameter count and inference time compared to heavy ISP pipelines or tuning approaches, enabling deployment on resource-constrained devices.

Conclusion: TA-ISP provides an efficient, task-aware RAW-to-RGB front-end that expands spatially varying transforms with a compact, factorized set of modulation operators, delivering better task performance with lower resource usage and enabling practical deployment on edge devices.

Abstract: In recent years, there has been a growing trend in computer vision towards
exploiting RAW sensor data, which preserves richer information compared to
conventional low-bit RGB images. Early studies mainly focused on enhancing
visual quality, while more recent efforts aim to leverage the abundant
information in RAW data to improve the performance of visual perception tasks
such as object detection and segmentation. However, existing approaches still
face two key limitations: large-scale ISP networks impose heavy computational
overhead, while methods based on tuning traditional ISP pipelines are
restricted by limited representational capacity.To address these issues, we
propose Task-Aware Image Signal Processing (TA-ISP), a compact RAW-to-RGB
framework that produces task-oriented representations for pretrained vision
models. Instead of heavy dense convolutional pipelines, TA-ISP predicts a small
set of lightweight, multi-scale modulation operators that act at global,
regional, and pixel scales to reshape image statistics across different spatial
extents. This factorized control significantly expands the range of spatially
varying transforms that can be represented while keeping memory usage,
computation, and latency tightly constrained. Evaluated on several RAW-domain
detection and segmentation benchmarks under both daytime and nighttime
conditions, TA-ISP consistently improves downstream accuracy while markedly
reducing parameter count and inference time, making it well suited for
deployment on resource-constrained devices.

</details>


### [41] [NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset](https://arxiv.org/abs/2509.13766)
*Huichun Liu,Xiaosong Li,Yang Liu,Xiaoqi Cheng,Haishu Tan*

Main category: cs.CV

TL;DR: Introduces NDLPNet, a nighttime deraining network with a Position Perception Module to capture spatial info and rain density; introduces NSR dataset; achieves state-of-the-art results on nighttime deraining; code available.


<details>
  <summary>Details</summary>
Motivation: Addresses nighttime deraining where daytime-oriented methods fail due to spatial heterogeneity of rain and light-dependent stripe visibility; provides real-world nighttime data.

Method: NDLPNet with Position Perception Module to capture spatial contextual information and adjust channel importance; leverages spatial positional cues and density distribution of rain streaks; dataset NSR of 900 nighttime image pairs.

Result: Extensive qualitative and quantitative experiments on existing datasets and NSR show outperformance of SOTA methods in nighttime deraining; code and dataset released.

Conclusion: Proposes a novel approach to nighttime rain removal preserving background, and provides a new benchmark dataset for the task; promising direction for robust nighttime vision in adverse weather.

Abstract: Visual degradation caused by rain streak artifacts in low-light conditions
significantly hampers the performance of nighttime surveillance and autonomous
navigation. Existing image deraining techniques are primarily designed for
daytime conditions and perform poorly under nighttime illumination due to the
spatial heterogeneity of rain distribution and the impact of light-dependent
stripe visibility. In this paper, we propose a novel Nighttime Deraining
Location-enhanced Perceptual Network(NDLPNet) that effectively captures the
spatial positional information and density distribution of rain streaks in
low-light environments. Specifically, we introduce a Position Perception Module
(PPM) to capture and leverage spatial contextual information from input data,
enhancing the model's capability to identify and recalibrate the importance of
different feature channels. The proposed nighttime deraining network can
effectively remove the rain streaks as well as preserve the crucial background
information. Furthermore, We construct a night scene rainy (NSR) dataset
comprising 900 image pairs, all based on real-world nighttime scenes, providing
a new benchmark for nighttime deraining task research. Extensive qualitative
and quantitative experimental evaluations on both existing datasets and the NSR
dataset consistently demonstrate our method outperform the state-of-the-art
(SOTA) methods in nighttime deraining tasks. The source code and dataset is
available at https://github.com/Feecuin/NDLPNet.

</details>


### [42] [VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in Real-time MRI](https://arxiv.org/abs/2509.13767)
*Daiqi Liu,Tomás Arias-Vergara,Johannes Enk,Fangxu Xing,Maureen Stone,Jerry L. Prince,Jana Hutter,Andreas Maier,Jonghye Woo,Paula Andrea Pérez-Toro*

Main category: cs.CV

TL;DR: VocSegMRI is a multimodal framework that fuses video, audio, and phonological inputs via cross-attention for rtMRI articulatory segmentation, aided by a contrastive learning objective to maintain performance when audio is unavailable. It achieves state-of-the-art Dice (0.95) and HD_95 (4.20 mm) on USC-75, with ablations confirming the value of cross-attention and contrastive learning.


<details>
  <summary>Details</summary>
Motivation:  rtMRI articulatory segmentation is hard using visuals alone; synchronized acoustic and phonological signals offer complementary context to improve segmentation accuracy and robustness, particularly in real-time settings.

Method: A multimodal architecture that integrates video, audio, and phonological inputs through cross-attention fusion for dynamic feature alignment, augmented with a contrastive learning objective to strengthen cross-modal representations and enable robustness when audio is missing at inference.

Result: On a subset of USC-75 rtMRI, the method achieves Dice = 0.95 and HD_95 = 4.20 mm, outperforming unimodal and multimodal baselines; ablation studies show the benefit of cross-attention and contrastive learning for accuracy and robustness.

Conclusion: Integrative multimodal modeling with cross-attention and contrastive learning enhances rtMRI articulatory segmentation, yielding higher precision and robustness beyond visual cues alone.

Abstract: Accurately segmenting articulatory structures in real-time magnetic resonance
imaging (rtMRI) remains challenging, as most existing methods rely almost
entirely on visual cues. Yet synchronized acoustic and phonological signals
provide complementary context that can enrich visual information and improve
precision. In this paper, we introduce VocSegMRI, a multimodal framework that
integrates video, audio, and phonological inputs through cross-attention fusion
for dynamic feature alignment. To further enhance cross-modal representation,
we incorporate a contrastive learning objective that improves segmentation
performance even when the audio modality is unavailable at inference. Evaluated
on a sub-set of USC-75 rtMRI dataset, our approach achieves state-of-the-art
performance, with a Dice score of 0.95 and a 95th percentile Hausdorff Distance
(HD_95) of 4.20 mm, outperforming both unimodal and multimodal baselines.
Ablation studies confirm the contributions of cross-attention and contrastive
learning to segmentation precision and robustness. These results highlight the
value of integrative multimodal modeling for accurate vocal tract analysis.

</details>


### [43] [Generative Image Coding with Diffusion Prior](https://arxiv.org/abs/2509.13768)
*Jianhui Chang*

Main category: cs.CV

TL;DR: A diffusion-prior-based generative coding framework for low-bit-rate compression, using a pre-optimized encoder and diffusion-model features with adapters and fusion, plus distribution renormalization; achieves superior visual fidelity and up to 79% savings over H.266.


<details>
  <summary>Details</summary>
Motivation: Perceptual quality at high compression; AI-generated content complicates coding; traditional codecs and existing learned methods struggle to maintain fidelity at low bitrates; need adaptable, efficient integration of diffusion priors.

Method: Use a pre-optimized encoder to produce generalized compressed-domain representations; integrate with pretrained diffusion model features via lightweight adapter and attentive fusion module; distribution renormalization; enables efficient adaptation to different pretrained models with minimal retraining.

Result: Outperforms existing methods in visual fidelity at low bitrates; up to 79% improvement over H.266/VVC; adaptable to broader content; efficient solution for AI-generated content.

Conclusion: The proposed framework effectively leverages diffusion priors to improve compression at low bitrates with low retraining cost and broad adaptability, indicating strong potential for AI-generated and mixed-content scenarios.

Abstract: As generative technologies advance, visual content has evolved into a complex
mix of natural and AI-generated images, driving the need for more efficient
coding techniques that prioritize perceptual quality. Traditional codecs and
learned methods struggle to maintain subjective quality at high compression
ratios, while existing generative approaches face challenges in visual fidelity
and generalization. To this end, we propose a novel generative coding framework
leveraging diffusion priors to enhance compression performance at low bitrates.
Our approach employs a pre-optimized encoder to generate generalized
compressed-domain representations, integrated with the pretrained model's
internal features via a lightweight adapter and an attentive fusion module.
This framework effectively leverages existing pretrained diffusion models and
enables efficient adaptation to different pretrained models for new
requirements with minimal retraining costs. We also introduce a distribution
renormalization method to further enhance reconstruction fidelity. Extensive
experiments show that our method (1) outperforms existing methods in visual
fidelity across low bitrates, (2) improves compression performance by up to 79%
over H.266/VVC, and (3) offers an efficient solution for AI-generated content
while being adaptable to broader content types.

</details>


### [44] [AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2509.13769)
*Yuechen Luo,Fang Li,Shaoqing Xu,Zhiyi Lai,Lei Yang,Qimao Chen,Ziang Luo,Zixun Xie,Shengyin Jiang,Jiaxin Liu,Long Chen,Bing Wang,Zhi-xin Yang*

Main category: cs.CV

TL;DR: A dual-mode reasoning framework for vision-language action in autonomous driving that adaptively uses fast non-CoT reasoning or slow CoT reasoning, guided by an Adaptive Think Reward with GRPO, achieving better PDMS and lower inference time on Navsim.


<details>
  <summary>Details</summary>
Motivation: CoT-based reasoning can improve decisions but often adds computational overhead and may be unnecessary in simpler driving scenarios; there is a need for selective, efficient reasoning that maintains or improves decision quality.

Method: Pretraining on large-scale autonomous driving data with QA and trajectory tasks to encode world knowledge; supervised fine-tuning with a two-mode dataset (fast answering without CoT and slow thinking with CoT); employing Adaptive Think Reward with Group Relative Policy Optimization to encourage selective use of CoT by comparing trajectory quality across modes.

Result: On Navsim, AdaThinkDrive achieves PDMS 90.3, 1.7 points above the best vision-only baseline; ablations show improvements over never Think and always Think baselines by 2.0 and 1.4 points; reduces inference time by 14% compared to the always Think baseline.

Conclusion: AdaThinkDrive demonstrates that adaptive, dual-mode reasoning can balance accuracy and efficiency in vision-language action for autonomous driving and outperform single-mode baselines while reducing computation.

Abstract: While reasoning technology like Chain of Thought (CoT) has been widely
adopted in Vision Language Action (VLA) models, it demonstrates promising
capabilities in end to end autonomous driving. However, recent efforts to
integrate CoT reasoning often fall short in simple scenarios, introducing
unnecessary computational overhead without improving decision quality. To
address this, we propose AdaThinkDrive, a novel VLA framework with a dual mode
reasoning mechanism inspired by fast and slow thinking. First, our framework is
pretrained on large scale autonomous driving (AD) scenarios using both question
answering (QA) and trajectory datasets to acquire world knowledge and driving
commonsense. During supervised fine tuning (SFT), we introduce a two mode
dataset, fast answering (w/o CoT) and slow thinking (with CoT), enabling the
model to distinguish between scenarios that require reasoning. Furthermore, an
Adaptive Think Reward strategy is proposed in conjunction with the Group
Relative Policy Optimization (GRPO), which rewards the model for selectively
applying CoT by comparing trajectory quality across different reasoning modes.
Extensive experiments on the Navsim benchmark show that AdaThinkDrive achieves
a PDMS of 90.3, surpassing the best vision only baseline by 1.7 points.
Moreover, ablations show that AdaThinkDrive surpasses both the never Think and
always Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also
reduces inference time by 14% compared to the always Think baseline,
demonstrating its ability to balance accuracy and efficiency through adaptive
reasoning.

</details>


### [45] [Morphology-optimized Multi-Scale Fusion: Combining Local Artifacts and Mesoscopic Semantics for Deepfake Detection and Localization](https://arxiv.org/abs/2509.13776)
*Chao Shuai,Gaojian Wang,Kun Pan,Tong Wu,Fanli Jin,Haohan Tan,Mengxiang Li,Zhenguang Liu,Feng Lin,Kui Ren*

Main category: cs.CV

TL;DR: Independent local and global modules for forgery localization with morphological fusion improve localization accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: The need for precise localization of manipulated regions goes beyond classification accuracy; leveraging both local detail and global semantic context is crucial, and naive fusion can amplify noise.

Method: Two parallel branches predict manipulated regions from local and global perspectives; outputs are fused via morphological operations to suppress noise and improve spatial coherence; training uses region annotations alongside manipulated images.

Result: Extensive experiments validate that both modules improve localization accuracy and robustness; the morphology-based fusion enhances coherence of localization maps.

Conclusion: Decoupling local/global predictions and applying morphology-based fusion yields more accurate and robust forgery localization, addressing noise and coherence issues in existing approaches.

Abstract: While the pursuit of higher accuracy in deepfake detection remains a central
goal, there is an increasing demand for precise localization of manipulated
regions. Despite the remarkable progress made in classification-based
detection, accurately localizing forged areas remains a significant challenge.
A common strategy is to incorporate forged region annotations during model
training alongside manipulated images. However, such approaches often neglect
the complementary nature of local detail and global semantic context, resulting
in suboptimal localization performance. Moreover, an often-overlooked aspect is
the fusion strategy between local and global predictions. Naively combining the
outputs from both branches can amplify noise and errors, thereby undermining
the effectiveness of the localization.
  To address these issues, we propose a novel approach that independently
predicts manipulated regions using both local and global perspectives. We
employ morphological operations to fuse the outputs, effectively suppressing
noise while enhancing spatial coherence. Extensive experiments reveal the
effectiveness of each module in improving the accuracy and robustness of
forgery localization.

</details>


### [46] [CETUS: Causal Event-Driven Temporal Modeling With Unified Variable-Rate Scheduling](https://arxiv.org/abs/2509.13784)
*Hanfang Liang,Bing Wang,Shizhen Zhang,Wen Jiang,Yizhuo Yang,Weixiang Guo,Shenghai Yuan*

Main category: cs.CV

TL;DR: A novel end-to-end architecture (Variable-Rate Spatial Event Mamba) processes raw event streams from event cameras without intermediate representations, using a causal spatial encoder, linear-complexity Mamba state-space temporal modeling, and a controller that adapts processing speed to event rate to balance window and inference latency.


<details>
  <summary>Details</summary>
Motivation: Overcomes window-latency from frame/voxel/point-cloud conversions and heavy compute of pointwise methods; aims for real-time efficiency on high-rate event streams.

Method: Introduce lightweight causal spatial neighborhood encoder; Mamba-based state-space models for temporal modeling with linear complexity; an adaptive controller that modulates processing speed based on event rate.

Result: Abstract does not report empirical results; claims suggest improved latency-efficiency trade-offs and scalability, but no quantitative metrics are provided.

Conclusion: End-to-end, representation-free event processing architecture; adaptive control enables balancing latency components and real-time performance.

Abstract: Event cameras capture asynchronous pixel-level brightness changes with
microsecond temporal resolution, offering unique advantages for high-speed
vision tasks. Existing methods often convert event streams into intermediate
representations such as frames, voxel grids, or point clouds, which inevitably
require predefined time windows and thus introduce window latency. Meanwhile,
pointwise detection methods face computational challenges that prevent
real-time efficiency due to their high computational cost. To overcome these
limitations, we propose the Variable-Rate Spatial Event Mamba, a novel
architecture that directly processes raw event streams without intermediate
representations. Our method introduces a lightweight causal spatial
neighborhood encoder to efficiently capture local geometric relations, followed
by Mamba-based state space models for scalable temporal modeling with linear
complexity. During inference, a controller adaptively adjusts the processing
speed according to the event rate, achieving an optimal balance between window
latency and inference latency.

</details>


### [47] [BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching](https://arxiv.org/abs/2509.13789)
*Hanshuai Cui,Zhiqing Tang,Zhifei Xu,Zhi Yao,Wenyi Zeng,Weijia Jia*

Main category: cs.CV

TL;DR: BWCache speeds DiT-based video generation by training-free caching of block features across diffusion timesteps, using a similarity threshold to trigger reuse; achieves up to 2.24x speedup with comparable visual fidelity.


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers (DiTs) offer state-of-the-art video generation but suffer latency due to sequential denoising. Existing speedups either hurt quality or fail to exploit granularity of intermediate features. The authors identify DiT blocks as the main latency source and observe a U-shaped similarity pattern in block features across timesteps, indicating redundancy.

Method: Block-Wise Caching (BWCache): a training-free approach that caches and reuses DiT block features across diffusion steps. Introduces a similarity indicator that activates reuse only when adjacent-timestep block feature differences fall below a threshold, minimizing redundant computation while preserving visual quality.

Result: Extensive experiments on several video diffusion models show up to 2.24× speedup with comparable visual quality to baseline methods.

Conclusion: BWCache effectively reduces inference latency for DiT-based video generation by leveraging redundant block-wise computations in a lightweight, threshold-driven caching mechanism that requires no retraining.

Abstract: Recent advancements in Diffusion Transformers (DiTs) have established them as
the state-of-the-art method for video generation. However, their inherently
sequential denoising process results in inevitable latency, limiting real-world
applicability. Existing acceleration methods either compromise visual quality
due to architectural modifications or fail to reuse intermediate features at
proper granularity. Our analysis reveals that DiT blocks are the primary
contributors to inference latency. Across diffusion timesteps, the feature
variations of DiT blocks exhibit a U-shaped pattern with high similarity during
intermediate timesteps, which suggests substantial computational redundancy. In
this paper, we propose Block-Wise Caching (BWCache), a training-free method to
accelerate DiT-based video generation. BWCache dynamically caches and reuses
features from DiT blocks across diffusion timesteps. Furthermore, we introduce
a similarity indicator that triggers feature reuse only when the differences
between block features at adjacent timesteps fall below a threshold, thereby
minimizing redundant computations while maintaining visual fidelity. Extensive
experiments on several video diffusion models demonstrate that BWCache achieves
up to 2.24$\times$ speedup with comparable visual quality.

</details>


### [48] [Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust Spacecraft 6-DoF Pose Estimation](https://arxiv.org/abs/2509.13792)
*Inder Pal Singh,Nidhal Eddine Chenni,Abd El Rahman Shabayek,Arunkumar Rathinam,Djamila Aouada*

Main category: cs.CV

TL;DR: A supervised domain adaptation method for spacecraft pose estimation (SPE) keypoint regression that leverages LIRR to align synthetic and limited real data, achieving strong generalization with minimal labeled target data.


<details>
  <summary>Details</summary>
Motivation: Synthetic-to-real domain gap in SPE hinders performance; existing unsupervised DA and limited supervised data have limitations; need a lightweight, effective SDA approach for real-world deployment.

Method: Extend Learning Invariant Representation and Risk (LIRR) to SDA in SPE: optimize domain-invariant representations and task-specific risk using labeled synthetic data and a small amount of labeled real data; backbone-agnostic and efficient.

Result: On SPEED+ benchmark, outperforms source-only, fine-tuning, and oracle baselines; with 5% labeled real data, matches or exceeds oracle trained on larger labeled data; demonstrates practicality and deployment readiness.

Conclusion: Proposes a practical, efficient SDA framework for SPE keypoint regression that reduces generalization error under domain shift and is ready for real-world space operations.

Abstract: Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous
space operations such as rendezvous, docking, and in-orbit servicing. Hybrid
pipelines that combine object detection, keypoint regression, and
Perspective-n-Point (PnP) solvers have recently achieved strong results on
synthetic datasets, yet their performance deteriorates sharply on real or
lab-generated imagery due to the persistent synthetic-to-real domain gap.
Existing unsupervised domain adaptation approaches aim to mitigate this issue
but often underperform when a modest number of labeled target samples are
available. In this work, we propose the first Supervised Domain Adaptation
(SDA) framework tailored for SPE keypoint regression. Building on the Learning
Invariant Representation and Risk (LIRR) paradigm, our method jointly optimizes
domain-invariant representations and task-specific risk using both labeled
synthetic and limited labeled real data, thereby reducing generalization error
under domain shift. Extensive experiments on the SPEED+ benchmark demonstrate
that our approach consistently outperforms source-only, fine-tuning, and oracle
baselines. Notably, with only 5% labeled target data, our method matches or
surpasses oracle performance trained on larger fractions of labeled data. The
framework is lightweight, backbone-agnostic, and computationally efficient,
offering a practical pathway toward robust and deployable spacecraft pose
estimation in real-world space environments.

</details>


### [49] [SWA-PF: Semantic-Weighted Adaptive Particle Filter for Memory-Efficient 4-DoF UAV Localization in GNSS-Denied Environments](https://arxiv.org/abs/2509.13795)
*Jiayu Yuan,Ming Dai,Enhui Zheng,Chao Su,Nanxing Chen,Qiming Hu,Shibo Zhu,Yibin Cao*

Main category: cs.CV

TL;DR: Intro of SWA-PF for GNSS-denied UAV localization using a large-scale MAFS dataset; achieves 10x speed, sub-10m position error, and rapid 4-DoF pose with low-res satellite maps; code and data released.


<details>
  <summary>Details</summary>
Motivation: Retrieval-based UAV localization struggles with dataset availability, real-time performance, environmental sensitivity, and generalization, especially in dynamic or temporally varying environments; a scalable semantic fusion approach is needed.

Method: Semantic-Weighted Adaptive Particle Filter (SWA-PF) that fuses semantic features from UAV images and satellite imagery using a semantic weighting mechanism and an optimized particle filtering architecture, evaluated on the proposed Multi-Altitude Flight Segments (MAFS) dataset.

Result: 10x computational efficiency over feature-extraction baselines; global positioning errors kept below 10 meters; enables rapid 4-DoF pose estimation within seconds using low-resolution satellite maps.

Conclusion: Code and dataset will be available at the provided GitHub link, indicating practical utility and reproducibility.

Abstract: Vision-based Unmanned Aerial Vehicle (UAV) localization systems have been
extensively investigated for Global Navigation Satellite System (GNSS)-denied
environments. However, existing retrieval-based approaches face limitations in
dataset availability and persistent challenges including suboptimal real-time
performance, environmental sensitivity, and limited generalization capability,
particularly in dynamic or temporally varying environments. To overcome these
limitations, we present a large-scale Multi-Altitude Flight Segments dataset
(MAFS) for variable altitude scenarios and propose a novel Semantic-Weighted
Adaptive Particle Filter (SWA-PF) method. This approach integrates robust
semantic features from both UAV-captured images and satellite imagery through
two key innovations: a semantic weighting mechanism and an optimized particle
filtering architecture. Evaluated using our dataset, the proposed method
achieves 10x computational efficiency gain over feature extraction methods,
maintains global positioning errors below 10 meters, and enables rapid 4 degree
of freedom (4-DoF) pose estimation within seconds using accessible
low-resolution satellite maps. Code and dataset will be available at
https://github.com/YuanJiayuuu/SWA-PF.

</details>


### [50] [Masked Feature Modeling Enhances Adaptive Segmentation](https://arxiv.org/abs/2509.13801)
*Wenlve Zhou,Zhiheng Zhou,Tiantao Xian,Yikui Zhai,Weibin Wu,Biyun Ma*

Main category: cs.CV

TL;DR: Masked Feature Modeling (MFM) for unsupervised domain adaptation in semantic segmentation. It masks and reconstructs features in feature space using a lightweight Rebuilder, trained with the segmentation decoder and discarded at test time, yielding no inference overhead and improved performance across architectures and benchmarks.


<details>
  <summary>Details</summary>
Motivation: Masked modeling methods have underexplored potential in UDA for semantic segmentation due to architectural incompatibilities and misaligned objectives; there is a need for an auxiliary task that aligns with the main pixel-level segmentation objective and remains compatible with standard architectures.

Method: Introduce Masked Feature Modeling (MFM) that performs feature masking and reconstruction directly in the feature space. Use a lightweight Rebuilder module trained jointly with the main model but discarded during inference. The reconstructed features are classified by the segmentation decoder, tightly coupling the auxiliary objective with the pixel-wise prediction task without altering the inference pipeline.

Result: MFM consistently improves segmentation performance across diverse architectures (e.g., DeepLab, DAFormer) and UDA benchmarks, demonstrating its effectiveness as a simple, efficient, and generalizable auxiliary objective for unsupervised domain-adaptive semantic segmentation.

Conclusion: MFM offers a practical, architecture-friendly auxiliary task for UDA in semantic segmentation that enhances feature learning without adding inference costs, partying with the segmentation decoder to align the auxiliary objective with the main task.

Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to
transfer models from a labeled source domain to an unlabeled target domain.
While auxiliary self-supervised tasks-particularly contrastive learning-have
improved feature discriminability, masked modeling approaches remain
underexplored in this setting, largely due to architectural incompatibility and
misaligned optimization objectives. We propose Masked Feature Modeling (MFM), a
novel auxiliary task that performs feature masking and reconstruction directly
in the feature space. Unlike existing masked modeling methods that reconstruct
low-level inputs or perceptual features (e.g., HOG or visual tokens), MFM
aligns its learning target with the main segmentation task, ensuring
compatibility with standard architectures like DeepLab and DAFormer without
modifying the inference pipeline. To facilitate effective reconstruction, we
introduce a lightweight auxiliary module, Rebuilder, which is trained jointly
but discarded during inference, adding zero computational overhead at test
time. Crucially, MFM leverages the segmentation decoder to classify the
reconstructed features, tightly coupling the auxiliary objective with the
pixel-wise prediction task to avoid interference with the primary task.
Extensive experiments across various architectures and UDA benchmarks
demonstrate that MFM consistently enhances segmentation performance, offering a
simple, efficient, and generalizable strategy for unsupervised domain-adaptive
semantic segmentation.

</details>


### [51] [Data-Efficient Spectral Classification of Hyperspectral Data Using MiniROCKET and HDC-MiniROCKET](https://arxiv.org/abs/2509.13809)
*Nick Theisen,Kenny Schlegel,Dietrich Paulus,Peer Neubert*

Main category: cs.CV

TL;DR: MiniROCKET and HDC-MiniROCKET offer robust spectral classification for hyperspectral data under limited training data, outperforming 1D-Justo-LiuNet in scarce data and remaining comparable in general. The use of fixed, well-engineered feature extraction helps data efficiency.


<details>
  <summary>Details</summary>
Motivation: To address spectral-only classification of hyperspectral images with limited labeled data, by comparing parameter-efficient, fixed-feature approaches against the state-of-the-art 1D-Justo-LiuNet and exploring whether data-efficient models can maintain performance while reducing data requirements.

Method: Evaluate MiniROCKET and HDC-MiniROCKET for spectral classification. The feature extraction part uses well-engineered features without trainable parameters, reducing data requirements. Compare performance to 1D-Justo-LiuNet across limited-data and general-data regimes.

Result: MiniROCKET outperforms 1D-Justo-LiuNet in limited data scenarios and is mostly on par with it in the general case. HDC-MiniROCKET is also investigated as an alternative, though specific results are not detailed in the abstract.

Conclusion: Spectral-only classification can be robust under data scarcity using fixed-feature extractors. MiniROCKET provides a strong alternative to the sparsely-parametered 1D-Justo-LiuNet in limited-data settings, while spectral information remains complementary to spatial information for future spatial–spectral integrations.

Abstract: The classification of pixel spectra of hyperspectral images, i.e. spectral
classification, is used in many fields ranging from agricultural, over medical
to remote sensing applications and is currently also expanding to areas such as
autonomous driving. Even though for full hyperspectral images the
best-performing methods exploit spatial-spectral information, performing
classification solely on spectral information has its own advantages, e.g.
smaller model size and thus less data required for training. Moreover, spectral
information is complementary to spatial information and improvements on either
part can be used to improve spatial-spectral approaches in the future.
Recently, 1D-Justo-LiuNet was proposed as a particularly efficient model with
very few parameters, which currently defines the state of the art in spectral
classification. However, we show that with limited training data the model
performance deteriorates. Therefore, we investigate MiniROCKET and
HDC-MiniROCKET for spectral classification to mitigate that problem. The model
extracts well-engineered features without trainable parameters in the feature
extraction part and is therefore less vulnerable to limited training data. We
show that even though MiniROCKET has more parameters it outperforms
1D-Justo-LiuNet in limited data scenarios and is mostly on par with it in the
general case

</details>


### [52] [Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2509.13834)
*Nguyen Lan Vi Vu,Thanh-Huy Nguyen,Thien Nguyen,Daisuke Kihara,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: A semi-supervised histopathology segmentation method using Semi-MOE, a multi-task mixture-of-experts with segmentation, signed distance field regression, and boundary prediction experts, integrated by a Multi-Gating Pseudo-labeling module and trained with an Adaptive Multi-Objective Loss, achieving state-of-the-art results in low-label settings on GlaS and CRAG; code available.


<details>
  <summary>Details</summary>
Motivation: Noisy pseudo-labels in semi-supervised segmentation for histopathology arise due to ambiguous gland boundaries and morphological misclassification. A robust framework that fuses multi-task expert signals and adapts learning objectives can improve pseudo-label quality and segmentation performance without excessive manual tuning.

Method: Proposes Semi-MOE, a multi-task Mixture-of-Experts framework with three specialized experts (main segmentation, signed distance field regression, boundary prediction). A Multi-Gating Pseudo-labeling module dynamically aggregates expert features to generate robust pseudo-labels. An Adaptive Multi-Objective Loss balances multiple learning objectives without manual tuning.

Result: On GlaS and CRAG benchmarks, Semi-MOE outperforms state-of-the-art approaches in low-label settings.

Conclusion: MoE-based architectures with dynamic gating and adaptive loss can effectively address ambiguous boundaries and noisy pseudo-labels in semi-supervised histopathology segmentation, demonstrating strong performance with limited labeled data; code is publicly available.

Abstract: Semi-supervised learning has been employed to alleviate the need for
extensive labeled data for histopathology image segmentation, but existing
methods struggle with noisy pseudo-labels due to ambiguous gland boundaries and
morphological misclassification. This paper introduces Semi-MOE, to the best of
our knowledge, the first multi-task Mixture-of-Experts framework for
semi-supervised histopathology image segmentation. Our approach leverages three
specialized expert networks: A main segmentation expert, a signed distance
field regression expert, and a boundary prediction expert, each dedicated to
capturing distinct morphological features. Subsequently, the Multi-Gating
Pseudo-labeling module dynamically aggregates expert features, enabling a
robust fuse-and-refine pseudo-labeling mechanism. Furthermore, to eliminate
manual tuning while dynamically balancing multiple learning objectives, we
propose an Adaptive Multi-Objective Loss. Extensive experiments on GlaS and
CRAG benchmarks show that our method outperforms state-of-the-art approaches in
low-label settings, highlighting the potential of MoE-based architectures in
advancing semi-supervised segmentation. Our code is available at
https://github.com/vnlvi2k3/Semi-MoE.

</details>


### [53] [Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models](https://arxiv.org/abs/2509.13836)
*Weihang Wang,Xinhao Li,Ziyue Wang,Yan Pang,Jielei Zhang,Peiyi Li,Qiang Zhang,Longwen Gao*

Main category: cs.CV

TL;DR: We analyze how visual encoders bias LVLM hallucinations, introduce VHBench-10 for fine-grained evaluation, and present VisionWeaver, a context-aware routing network that reduces hallucinations by routing features across specialized experts.


<details>
  <summary>Details</summary>
Motivation: Object hallucination in LVLMs hinders real-world use; encoders confer distinct inductive biases; existing benchmarks are coarse-grained; there is a need for fine-grained, encoder-aware evaluation and mitigation.

Method: Develop VHBench-10 (~10k samples across ten hallucination categories) to systematically evaluate LVLMs; empirically analyze encoder-specific hallucination patterns; design VisionWeaver, a Context-Aware Routing Network, that uses global visual features to generate routing signals and dynamically aggregates features from multiple specialized experts.

Result: Encoders show unique hallucination characteristics; VisionWeaver significantly reduces hallucinations and improves overall performance.

Conclusion: An encoder-aware benchmark plus a routing-based feature fusion approach can mitigate hallucinations and improve the reliability of LVLMs in real-world tasks.

Abstract: Object hallucination in Large Vision-Language Models (LVLMs) significantly
impedes their real-world applicability. As the primary component for accurately
interpreting visual information, the choice of visual encoder is pivotal. We
hypothesize that the diverse training paradigms employed by different visual
encoders instill them with distinct inductive biases, which leads to their
diverse hallucination performances. Existing benchmarks typically focus on
coarse-grained hallucination detection and fail to capture the diverse
hallucinations elaborated in our hypothesis. To systematically analyze these
effects, we introduce VHBench-10, a comprehensive benchmark with approximately
10,000 samples for evaluating LVLMs across ten fine-grained hallucination
categories. Our evaluations confirm encoders exhibit unique hallucination
characteristics. Building on these insights and the suboptimality of simple
feature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network.
It employs global visual features to generate routing signals, dynamically
aggregating visual features from multiple specialized experts. Comprehensive
experiments confirm the effectiveness of VisionWeaver in significantly reducing
hallucinations and improving overall model performance.

</details>


### [54] [MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment](https://arxiv.org/abs/2509.14001)
*Elena Camuffo,Francesco Barbato,Mete Ozay,Simone Milani,Umberto Michieli*

Main category: cs.CV

TL;DR: MOCHA introduces a lightweight, object-level knowledge distillation framework that transfers region-level multimodal semantics from a large vision-language teacher to a vision-only detector student via a translator and dual-objective loss, enabling efficient sematic transfer without text at inference. It yields ~+10.1 average score gains in few-shot personalized detection benchmarks and rivals larger multimodal models.


<details>
  <summary>Details</summary>
Motivation: To enable efficient deployment of multimodal knowledge in lightweight detectors without relying on large multimodal models or textual inputs at inference time, addressing the gap between high-performance VLMs and compact detectors.

Method: A translator maps student features into a joint multimodal space. Training uses a dual-objective loss that enforces local alignment (region-level) and global relational consistency between student and teacher semantics. The teacher remains unchanged and no textual input is required at inference. Evaluated on four personalized detection benchmarks under few-shot regimes.

Result: Consistent gains over baselines with a +10.1 average score improvement; MOCHA’s compact architecture achieves performance on par with larger multimodal models, demonstrating strong practical applicability for deployment.

Conclusion: MOCHA enables effective object-level knowledge transfer from a multimodal teacher to a vision-only detector without modifying the teacher or requiring text inputs at inference, offering a computationally efficient path to near-multimodal performance in real-world settings.

Abstract: We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),
a knowledge distillation approach that transfers region-level multimodal
semantics from a large vision-language teacher (e.g., LLaVa) into a lightweight
vision-only object detector student (e.g., YOLO). A translation module maps
student features into a joint space, where the training of the student and
translator is guided by a dual-objective loss that enforces both local
alignment and global relational consistency. Unlike prior approaches focused on
dense or global alignment, MOCHA operates at the object level, enabling
efficient transfer of semantics without modifying the teacher or requiring
textual input at inference. We validate our method across four personalized
detection benchmarks under few-shot regimes. Results show consistent gains over
baselines, with a +10.1 average score improvement. Despite its compact
architecture, MOCHA reaches performance on par with larger multimodal models,
proving its suitability for real-world deployment.

</details>


### [55] [Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation](https://arxiv.org/abs/2509.13846)
*Puru Vaish,Felix Meister,Tobias Heimann,Christoph Brune,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: Introduces Consistent View Alignment to explicitly induce structured latent representations by aligning multi-view representations, improving downstream tasks and achieving top results in MICCAI 2025 SSL3D; code available.


<details>
  <summary>Details</summary>
Motivation: Challenging the assumption that uncorrelated views suffice for learning meaningful representations; latent structure does not emerge naturally and must be explicitly induced through deliberate view alignment to capture complementary information while avoiding false positives.

Method: Proposes Consistent View Alignment, a self-supervised framework that aligns representations from different views to fuse complementary information and suppress spurious matches, thereby inducing structured latent space.

Result: Demonstrates improved performance on downstream tasks; achieved first and second place in the MICCAI 2025 SSL3D challenge using a Primus vision transformer and a ResEnc convolutional neural network; code and pretrained weights released.

Conclusion: Explicit, structured view alignment is critical for learning effective representations in self-supervised learning; the proposed method successfully induces latent structure and provides reusable resources for the community.

Abstract: Many recent approaches in representation learning implicitly assume that
uncorrelated views of a data point are sufficient to learn meaningful
representations for various downstream tasks. In this work, we challenge this
assumption and demonstrate that meaningful structure in the latent space does
not emerge naturally. Instead, it must be explicitly induced. We propose a
method that aligns representations from different views of the data to align
complementary information without inducing false positives. Our experiments
show that our proposed self-supervised learning method, Consistent View
Alignment, improves performance for downstream tasks, highlighting the critical
role of structured view alignment in learning effective representations. Our
method achieved first and second place in the MICCAI 2025 SSL3D challenge when
using a Primus vision transformer and ResEnc convolutional neural network,
respectively. The code and pretrained model weights are released at
https://github.com/Tenbatsu24/LatentCampus.

</details>


### [56] [SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation](https://arxiv.org/abs/2509.13848)
*Jiayi Pan,Jiaming Xu,Yongkang Zhou,Guohao Dai*

Main category: cs.CV

TL;DR: SpecDiff: a training-free, multi-level feature caching framework that uses self-speculation to leverage future information across diffusion steps, achieving substantial speedups with negligible quality loss compared to RFlow.


<details>
  <summary>Details</summary>
Motivation: Relying solely on historical information in feature caching constrains diffusion-model accuracy and speed. There is a need to exploit future information to break the speedup–accuracy bottleneck.

Method: SpecDiff introduces a self-speculative information-based dynamic token importance scoring for cached feature selection and a multi-level feature classification strategy. It is training-free and operates during diffusion inference to select and classify features for caching across time steps.

Result: On NVIDIA A800-80GB, SpecDiff delivers average speedups of 2.80x, 2.74x, and 3.17x on Stable Diffusion 3, 3.5, and FLUX, respectively, with negligible quality loss, compared to RFlow.

Conclusion: SpecDiff merges speculative and historical information to surpass the speedup–accuracy trade-off, pushing the Pareto frontier for efficient diffusion model inference.

Abstract: Feature caching has recently emerged as a promising method for diffusion
model acceleration. It effectively alleviates the inefficiency problem caused
by high computational requirements by caching similar features in the inference
process of the diffusion model. In this paper, we analyze existing feature
caching methods from the perspective of information utilization, and point out
that relying solely on historical information will lead to constrained accuracy
and speed performance. And we propose a novel paradigm that introduces future
information via self-speculation based on the information similarity at the
same time step across different iteration times. Based on this paradigm, we
present \textit{SpecDiff}, a training-free multi-level feature caching strategy
including a cached feature selection algorithm and a multi-level feature
classification algorithm. (1) Feature selection algorithm based on
self-speculative information. \textit{SpecDiff} determines a dynamic importance
score for each token based on self-speculative information and historical
information, and performs cached feature selection through the importance
score. (2) Multi-level feature classification algorithm based on feature
importance scores. \textit{SpecDiff} classifies tokens by leveraging the
differences in feature importance scores and introduces a multi-level feature
calculation strategy. Extensive experiments show that \textit{SpecDiff}
achieves average 2.80 \times, 2.74 \times , and 3.17\times speedup with
negligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow
on NVIDIA A800-80GB GPU. By merging speculative and historical information,
\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing
the Pareto frontier of speedup and accuracy in the efficient diffusion model
inference.

</details>


### [57] [EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics](https://arxiv.org/abs/2509.13858)
*Qianxin Xia,Jiawei Du,Guoming Lu,Zhiyong Shu,Jielei Wang*

Main category: cs.CV

TL;DR: EDITS is a dataset distillation framework that harnesses implicit textual semantics via a Vision-Language Model (VLM) and a Large Language Model (LLM) to produce a high-quality synthetic dataset through diffusion, surpassing traditional low-level-feature methods.


<details>
  <summary>Details</summary>
Motivation: Conventional dataset distillation largely preserves low-level visual features and textures, but neglects high-level semantic and structural information embedded in images. Leveraging textual semantics can enrich prototypes and improve distillation performance.

Method: EDITS introduces (1) Global Semantic Query: fuses VLM-generated external texts with image features to form a prior clustered buffer; (2) Local Semantic Awareness: selects representative samples to construct image and text prototypes, with text prototypes generated by prompting an LLM; (3) Dual Prototype Guidance: guides a diffusion model to synthesize the final dataset using both image and text prototypes.

Result: Extensive experiments demonstrate that EDITS improves synthetic data quality and downstream learning performance compared to baselines, and the authors provide source code for reproducibility.

Conclusion: By integrating cross-modal textual semantics via VLM and LLM with a diffusion-based synthesis pipeline, EDITS effectively enhances dataset distillation beyond traditional low-level feature approaches.

Abstract: Dataset distillation aims to synthesize a compact dataset from the original
large-scale one, enabling highly efficient learning while preserving
competitive model performance. However, traditional techniques primarily
capture low-level visual features, neglecting the high-level semantic and
structural information inherent in images. In this paper, we propose EDITS, a
novel framework that exploits the implicit textual semantics within the image
data to achieve enhanced distillation. First, external texts generated by a
Vision Language Model (VLM) are fused with image features through a Global
Semantic Query module, forming the prior clustered buffer. Local Semantic
Awareness then selects representative samples from the buffer to construct
image and text prototypes, with the latter produced by guiding a Large Language
Model (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype
Guidance strategy generates the final synthetic dataset through a diffusion
model. Extensive experiments confirm the effectiveness of our method.Source
code is available in: https://github.com/einsteinxia/EDITS.

</details>


### [58] [Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions](https://arxiv.org/abs/2509.14165)
*Michal Szczepanski,Martyna Poreba,Karim Haroun*

Main category: cs.CV

TL;DR: STEP is a hybrid token-reduction framework for Vision Transformers in semantic segmentation that merges tokens into superpatches via a CNN-based policy (dCTS) and uses early-exits to prune high-confidence tokens. It achieves substantial token and compute reductions with minimal accuracy loss on high-resolution images.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers achieve strong segmentation performance but are computationally and memory-intensive, especially for high-resolution inputs (e.g., 1024x1024). There is a need to reduce token counts and computations without significantly sacrificing accuracy.

Method: STEP combines dynamic patch merging (dCTS) to form supertokens and a token-pruning mechanism with early-exits in encoder blocks. It uses a lightweight CNN policy network (dCTS) to guide merging and applies early termination of confident tokens, evaluating on high-resolution semantic segmentation benchmarks.

Result: When applied with dCTS alone, token counts are reduced 2.5x; this yields about 2.6x lower computational cost and 3.4x higher throughput with ViT-Large. The full STEP framework achieves up to 4x computational reduction and 1.7x faster inference, with maximum accuracy drop no more than 2.0%. Up to 40% of tokens can be confidently predicted and halted before the final encoder layer.

Conclusion: STEP significantly improves efficiency for high-resolution semantic segmentation by combining dynamic token merging and early-exits, offering substantial reductions in compute and tokens while keeping accuracy loss within a practical range.

Abstract: Vision Transformers (ViTs) achieve state-of-the-art performance in semantic
segmentation but are hindered by high computational and memory costs. To
address this, we propose STEP (SuperToken and Early-Pruning), a hybrid
token-reduction framework that combines dynamic patch merging and token pruning
to enhance efficiency without significantly compromising accuracy. At the core
of STEP is dCTS, a lightweight CNN-based policy network that enables flexible
merging into superpatches. Encoder blocks integrate also early-exits to remove
high-confident supertokens, lowering computational load. We evaluate our method
on high-resolution semantic segmentation benchmarks, including images up to
1024 x 1024, and show that when dCTS is applied alone, the token count can be
reduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching
scheme. This yields a 2.6x reduction in computational cost and a 3.4x increase
in throughput when using ViT-Large as the backbone. Applying the full STEP
framework further improves efficiency, reaching up to a 4x reduction in
computational complexity and a 1.7x gain in inference speed, with a maximum
accuracy drop of no more than 2.0%. With the proposed STEP configurations, up
to 40% of tokens can be confidently predicted and halted before reaching the
final encoder layer.

</details>


### [59] [LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray Laminography Reconstruction](https://arxiv.org/abs/2509.13863)
*Chu Chen,Ander Biguri,Jean-Michel Morel,Raymond H. Chan,Carola-Bibiane Schönlieb,Jizhou Li*

Main category: cs.CV

TL;DR: LamiGauss uses Gaussian splatting with a laminography-aware projection model and artifact-filtered initialization to reconstruct high-quality 3D volumes from highly sparse laminographic projections, achieving superior results with only 3% of full views.


<details>
  <summary>Details</summary>
Motivation: Non-destructive inspection of plate-like structures (e.g., microchips, composite materials) where standard CT suffers from laminography geometry and data sparsity; need efficient, accurate reconstructions from few projections.

Method: Integrates Gaussian splatting radiative rasterization with a laminography-aware detector-to-world transformation that includes the tilt angle. Employs an initialization strategy that filters laminography artifacts to prevent Gaussians from representing false structures, enabling direct optimization from sparse projections.

Result: Extensive experiments on synthetic and real data show the method's effectiveness and superiority over existing techniques, achieving superior performance using only ~3% of full-view data compared with an iterative method trained on full data.

Conclusion: LamiGauss enables accurate, efficient laminographic reconstructions from sparse data by (i) accurate geometry-aware modeling, (ii) artifact-aware initialization, and (iii) direct optimization from projections, outperforming existing methods.

Abstract: X-ray Computed Laminography (CL) is essential for non-destructive inspection
of plate-like structures in applications such as microchips and composite
battery materials, where traditional computed tomography (CT) struggles due to
geometric constraints. However, reconstructing high-quality volumes from
laminographic projections remains challenging, particularly under highly
sparse-view acquisition conditions. In this paper, we propose a reconstruction
algorithm, namely LamiGauss, that combines Gaussian Splatting radiative
rasterization with a dedicated detector-to-world transformation model
incorporating the laminographic tilt angle. LamiGauss leverages an
initialization strategy that explicitly filters out common laminographic
artifacts from the preliminary reconstruction, preventing redundant Gaussians
from being allocated to false structures and thereby concentrating model
capacity on representing the genuine object. Our approach effectively optimizes
directly from sparse projections, enabling accurate and efficient
reconstruction with limited data. Extensive experiments on both synthetic and
real datasets demonstrate the effectiveness and superiority of the proposed
method over existing techniques. LamiGauss uses only 3$\%$ of full views to
achieve superior performance over the iterative method optimized on a full
dataset.

</details>


### [60] [Distractor-Aware Memory-Based Visual Object Tracking](https://arxiv.org/abs/2509.13864)
*Jovana Videnovic,Matej Kristan,Alan Lukezic*

Main category: cs.CV

TL;DR: DAM4SAM introduces a distractor-aware memory module and introspection-based management for SAM2 to improve video segmentation-based tracking; it reduces distractor-induced drift, enhances redetection after occlusion, and generalizes across architectures, aided by a new distractor-focused dataset and notable benchmark gains.


<details>
  <summary>Details</summary>
Motivation: Memory-based segmentation for tracking suffers from distractors (visually similar objects) and occlusion; existing SAM2-style models lack explicit distractor handling and robust redetection, hindering tracking performance and generalization.

Method: Propose a drop-in, distractor-aware memory module for SAM2 plus an introspection-based management strategy (DAM4SAM); create a Distractor-Distilled dataset (DiDi) for analysis; validate by integrating the memory into real-time trackers (EfficientTAM) and edge-based trackers (EdgeTAM) and evaluating across multiple benchmarks.

Result: DAM4SAM outperforms SAM2.1 on thirteen benchmarks and sets new state-of-the-art on ten; integrating the distractor-aware memory into EfficientTAM yields ~11% improvement and matches the non-real-time SAM2.1-L across several tracking/segmentation benchmarks; EdgeTAM integration provides an additional ~4% boost, showing strong cross-architecture generalization.

Conclusion: Distractor-aware memory and introspection-based management effectively mitigate distractor drift and enhance redetection after occlusion; the proposed DiDi dataset supports analysis of distractor effects; the approach generalizes across trackers, offering tangible gains for real-time and edge-based systems.

Abstract: Recent emergence of memory-based video segmentation methods such as SAM2 has
led to models with excellent performance in segmentation tasks, achieving
leading results on numerous benchmarks. However, these modes are not fully
adjusted for visual object tracking, where distractors (i.e., objects visually
similar to the target) pose a key challenge. In this paper we propose a
distractor-aware drop-in memory module and introspection-based management
method for SAM2, leading to DAM4SAM. Our design effectively reduces the
tracking drift toward distractors and improves redetection capability after
object occlusion. To facilitate the analysis of tracking in the presence of
distractors, we construct DiDi, a Distractor-Distilled dataset. DAM4SAM
outperforms SAM2.1 on thirteen benchmarks and sets new state-of-the-art results
on ten. Furthermore, integrating the proposed distractor-aware memory into a
real-time tracker EfficientTAM leads to 11% improvement and matches tracking
quality of the non-real-time SAM2.1-L on multiple tracking and segmentation
benchmarks, while integration with edge-based tracker EdgeTAM delivers 4%
performance boost, demonstrating a very good generalization across
architectures.

</details>


### [61] [Invisible Yet Detected: PelFANet with Attention-Guided Anatomical Fusion for Pelvic Fracture Diagnosis](https://arxiv.org/abs/2509.13873)
*Siam Tahsin Bhuiyan,Rashedur Rahman,Sefatul Wasi,Naomi Yagi,Syoji Kobashi,Ashraful Islam,Saadia Binte Alam*

Main category: cs.CV

TL;DR: A dual-input pelvic fracture detector (PelFANet) uses fused attention to combine raw X-rays with segmented bone images, achieving high accuracy/AUC on visible fractures and good generalization to invisible fractures, outperforming conventional methods.


<details>
  <summary>Details</summary>
Motivation: Pelvic fractures are hard to diagnose when signs are subtle or invisible on standard radiographs; single-input models may miss contextual and anatomical details. The work aims to leverage anatomy-aware, dual-input data to improve detection robustness.

Method: PelFANet employs a two-stream architecture with Fused Attention Blocks that iteratively exchange and refine features from raw X-rays and segmented bone images. Training is conducted in a two-stage, segmentation-guided pipeline to fuse anatomical information from both inputs and capture both global context and localized details.

Result: On the AMERI dataset, the model achieves 88.68% accuracy and 0.9334 AUC for visible fractures, and 82.29% accuracy and 0.8688 AUC for invisible fractures (unseen during training), outperforming conventional methods.

Conclusion: Anatomy-aware dual-input architectures like PelFANet hold promise for robust fracture detection, especially in cases with subtle radiographic presentations.

Abstract: Pelvic fractures pose significant diagnostic challenges, particularly in
cases where fracture signs are subtle or invisible on standard radiographs. To
address this, we introduce PelFANet, a dual-stream attention network that fuses
raw pelvic X-rays with segmented bone images to improve fracture
classification. The network em-ploys Fused Attention Blocks (FABlocks) to
iteratively exchange and refine fea-tures from both inputs, capturing global
context and localized anatomical detail. Trained in a two-stage pipeline with a
segmentation-guided approach, PelFANet demonstrates superior performance over
conventional methods. On the AMERI dataset, it achieves 88.68% accuracy and
0.9334 AUC on visible fractures, while generalizing effectively to invisible
fracture cases with 82.29% accuracy and 0.8688 AUC, despite not being trained
on them. These results highlight the clini-cal potential of anatomy-aware
dual-input architectures for robust fracture detec-tion, especially in
scenarios with subtle radiographic presentations.

</details>


### [62] [Dense Video Understanding with Gated Residual Tokenization](https://arxiv.org/abs/2509.14199)
*Haichao Zhang,Wenhao Chai,Shwai He,Ang Li,Yun Fu*

Main category: cs.CV

TL;DR: Proposes Dense Video Understanding (DVU) and a two-stage Gated Residual Tokenization (GRT) for high-FPS, dense-temporal video comprehension, plus DIVE benchmark; shows GRT reduces tokenization cost and token growth while outperforming larger VLLMs on dense temporal tasks.


<details>
  <summary>Details</summary>
Motivation: Current video LLMs rely on low-frame-rate sampling, losing dense temporal information and incurring high tokenization costs as video length increases. There is a need for efficient, high-FPS video understanding and evaluation for tasks requiring frame-to-frame temporal alignment (e.g., lectures).

Method: Two-stage tokenization framework: (1) Motion-Compensated Inter-Gated Tokenization uses pixel-level motion estimation to skip static regions during tokenization, achieving sub-linear growth in tokens and compute; (2) Semantic-Scene Intra-Tokenization Merging fuses tokens across static regions within a scene to reduce redundancy while preserving dynamic semantics. Also introduces DIVE as a benchmark for dense temporal reasoning.

Result: GRT outperforms larger VLLM baselines and scales positively with FPS on the DIVE benchmark, demonstrating efficient, scalable dense temporal video understanding.

Conclusion: Dense temporal information is crucial for accurate video understanding; the proposed GRT framework enables efficient high-FPS processing and dense temporal reasoning, outperforming existing approaches while reducing tokenization overhead.

Abstract: High temporal resolution is essential for capturing fine-grained details in
video understanding. However, current video large language models (VLLMs) and
benchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or
keyframe selection, discarding dense temporal information. This compromise
avoids the high cost of tokenizing every frame, which otherwise leads to
redundant computation and linear token growth as video length increases. While
this trade-off works for slowly changing content, it fails for tasks like
lecture comprehension, where information appears in nearly every frame and
requires precise temporal alignment. To address this gap, we introduce Dense
Video Understanding (DVU), which enables high-FPS video comprehension by
reducing both tokenization time and token overhead. Existing benchmarks are
also limited, as their QA pairs focus on coarse content changes. We therefore
propose DIVE (Dense Information Video Evaluation), the first benchmark designed
for dense temporal reasoning. To make DVU practical, we present Gated Residual
Tokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated
Tokenization uses pixel-level motion estimation to skip static regions during
tokenization, achieving sub-linear growth in token count and compute. (2)
Semantic-Scene Intra-Tokenization Merging fuses tokens across static regions
within a scene, further reducing redundancy while preserving dynamic semantics.
Experiments on DIVE show that GRT outperforms larger VLLM baselines and scales
positively with FPS. These results highlight the importance of dense temporal
information and demonstrate that GRT enables efficient, scalable high-FPS video
understanding.

</details>


### [63] [EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View](https://arxiv.org/abs/2509.13883)
*Zhen Xu,Guorui Lu,Chang Gao,Qinyu Chen*

Main category: cs.CV

TL;DR: A lightweight, event-based FPV hand tracking framework (EvHand-FPV) using a wrist ROI and multi-task learning to achieve strong accuracy with low latency and power, plus a synthetic+real dataset; substantial model compression and FLOP reductions while maintaining competitive 3D performance; dataset and code released.


<details>
  <summary>Details</summary>
Motivation: Frame-based hand tracking struggles with accuracy, latency, and energy efficiency in resource-constrained XR devices. Event cameras offer microsecond-level temporal resolution at milli-watt power, inspiring a shift to asynchronous, low-power perception for egocentric hand tracking.

Method: Develop EvHand-FPV: (1) construct an egocentric FPV event dataset with synthetic 3D labels and real 2D labels; (2) introduce a wrist-based ROI to localize the hand via geometric cues; (3) use an end-to-end mapping that embeds ROI offsets to reduce computation without explicit reconstruction; (4) employ multi-task learning with an auxiliary geometric feature head to improve representations without extra test-time cost.

Result: On real FPV test set, 2D-AUCp improves from 0.77 to 0.85; model size drops from 11.2M to 1.2M (89% reduction); FLOPs per inference drop from 1.648G to 0.185G (89% reduction); 3D-AUCp on synthetic data remains competitive at 0.84. The dataset and code are released at the provided GitHub URL.

Conclusion: EvHand-FPV demonstrates accurate and energy-efficient egocentric event-based hand tracking suitable for on-device XR applications, leveraging ROI-guided computation and multi-task learning to achieve strong 2D/3D performance with significant efficiency gains.

Abstract: Hand tracking holds great promise for intuitive interaction paradigms, but
frame-based methods often struggle to meet the requirements of accuracy, low
latency, and energy efficiency, especially in resource-constrained settings
such as Extended Reality (XR) devices. Event cameras provide $\mu$s-level
temporal resolution at mW-level power by asynchronously sensing brightness
changes. In this work, we present EvHand-FPV, a lightweight framework for
egocentric First-Person-View 3D hand tracking from a single event camera. We
construct an event-based FPV dataset that couples synthetic training data with
3D labels and real event data with 2D labels for evaluation to address the
scarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based
region of interest (ROI) that localizes the hand region via geometric cues,
combined with an end-to-end mapping strategy that embeds ROI offsets into the
network to reduce computation without explicit reconstruction, and a multi-task
learning strategy with an auxiliary geometric feature head that improves
representations without test-time overhead. On our real FPV test set,
EvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from
11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It
also maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results
demonstrate accurate and efficient egocentric event-based hand tracking
suitable for on-device XR applications. The dataset and code are available at
https://github.com/zen5x5/EvHand-FPV.

</details>


### [64] [White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2509.13907)
*Jiyun Im,SuBeen Lee,Miso Lee,Jae-Pil Heo*

Main category: cs.CV

TL;DR: Introduces White Aggregation and Restoration Module (WARM) to generate robust prototypes for Few-Shot 3D Point Cloud Segmentation (FS-PCS) using whitening and coloring cross-attention between prototypical tokens and support features, addressing distributional misalignment and randomness in prototype generation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Prototype-based FS-PCS methods are sensitive to initial randomness (e.g., sampling) and distributional gaps between learnable prototypes and support features; existing attention-based prototypes are promising but suffer misalignment, motivating a more robust prototype generation mechanism.

Method: Propose WARM, which sandwiches cross-attention between whitening and coloring transformations. Whitening aligns support features to prototypical tokens before attention, and coloring restores the original distribution after attention, enabling robust attention and better prototype representations by capturing semantic relationships among support features.

Result: WARM achieves state-of-the-art performance with a significant margin on multiple FS-PCS benchmarks, demonstrating strong effectiveness through extensive experiments.

Conclusion: WARM effectively mitigates distributional misalignment in prototype generation for FS-PCS, yielding more representative prototypes and improved segmentation performance through robust attention over support features.

Abstract: Few-Shot 3D Point Cloud Segmentation (FS-PCS) aims to predict per-point
labels for an unlabeled point cloud, given only a few labeled examples. To
extract discriminative representations from the limited support set, existing
methods have constructed prototypes using conventional algorithms such as
farthest point sampling. However, we point out that its initial randomness
significantly affects FS-PCS performance and that the prototype generation
process remains underexplored despite its prevalence. This motivates us to
investigate an advanced prototype generation method based on attention
mechanism. Despite its potential, we found that vanilla module suffers from the
distributional gap between learnable prototypical tokens and support features.
To overcome this, we propose White Aggregation and Restoration Module (WARM),
which resolves the misalignment by sandwiching cross-attention between
whitening and coloring transformations. Specifically, whitening aligns the
support features to prototypical tokens before attention process, and
subsequently coloring restores the original distribution to the attended
tokens. This simple yet effective design enables robust attention, thereby
generating representative prototypes by capturing the semantic relationships
among support features. Our method achieves state-of-the-art performance with a
significant margin on multiple FS-PCS benchmarks, demonstrating its
effectiveness through extensive experiments.

</details>


### [65] [Towards Rationale-Answer Alignment of LVLMs via Self-Rationale Calibration](https://arxiv.org/abs/2509.13919)
*Yuanchen Wu,Ke Yan,Shouhong Ding,Ziyin Zhou,Xiaoqiang Li*

Main category: cs.CV

TL;DR: SRC is a framework that calibrates alignment between rationales and answers in LVLMs by rationale fine-tuning, diverse candidate generation, R-Scorer evaluation, and confidence-weighted preference tuning, yielding improved perception, reasoning, and generalization.


<details>
  <summary>Details</summary>
Motivation: LVLMs exhibit strong VQA but struggle to align their rationales with the generated answers, causing inconsistent reasoning and incorrect responses; aligning rationale with answer is key to robust LVLM performance.

Method: Iterative approach: (1) rationale fine-tuning to require a rationale before answering without explicit prompts; (2) generate a diverse set of candidate responses from fine-tuned LVLMs per sample; (3) apply pairwise scoring with R-Scorer to evaluate rationale quality and factual consistency; (4) use confidence-weighted preference curation to decouple alignment calibration into a preference fine-tuning process; results improve perception, reasoning, and generalization.

Result: Significant improvements across perception, reasoning, and generalization are observed on multiple benchmarks, highlighting the effectiveness of rationale-oriented alignment in LVLMs.

Conclusion: Rationale-oriented alignment is crucial for unlocking LVLM potential; SRC provides an effective mechanism for aligning rationales with answers via iterative fine-tuning and preference-based calibration.

Abstract: Large Vision-Language Models (LVLMs) have manifested strong visual question
answering capability. However, they still struggle with aligning the rationale
and the generated answer, leading to inconsistent reasoning and incorrect
responses. To this end, this paper introduces the Self-Rationale Calibration
(SRC) framework to iteratively calibrate the alignment between rationales and
answers. SRC begins by employing a lightweight "rationale fine-tuning"
approach, which modifies the model's response format to require a rationale
before deriving an answer without explicit prompts. Next, SRC searches for a
diverse set of candidate responses from the fine-tuned LVLMs for each sample,
followed by a proposed pairwise scoring strategy using a tailored scoring
model, R-Scorer, to evaluate both rationale quality and factual consistency of
candidates. Based on a confidence-weighted preference curation process, SRC
decouples the alignment calibration into a preference fine-tuning manner,
leading to significant improvements of LVLMs in perception, reasoning, and
generalization across multiple benchmarks. Our results emphasize the
rationale-oriented alignment in exploring the potential of LVLMs.

</details>


### [66] [Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification](https://arxiv.org/abs/2509.13922)
*Wenkui Yang,Jie Cao,Junxian Duan,Ran He*

Main category: cs.CV

TL;DR: AntiPure is a simple diagnostic protective perturbation that survives purification in diffusion-model workflows by two guidance mechanisms, revealing vulnerabilities in purification and achieving robust post-customization distortion with minimal perceptual change.


<details>
  <summary>Details</summary>
Motivation: Diffusion models enable powerful image synthesis but pose security risks (e.g., deepfakes, copyright infringement). Protective perturbations were proposed to mitigate misuse, but purification can remove them, re-exposing images to forgery. This work formalizes anti-purification and motivates a diagnostic perturbation to probe purification weaknesses.

Method: Introduce AntiPure, a diagnostic protective perturbation. It uses two guidance mechanisms: (1) Patch-wise Frequency Guidance to reduce influence on high-frequency components during purification, and (2) Erroneous Timestep Guidance to disrupt the model's denoising across timesteps. With added guidance, AntiPure embeds imperceptible perturbations that persist under representative purification settings within the purification-customization workflow.

Result: Experiments show that AntiPure achieves minimal perceptual discrepancy while causing maximal distortion under purification, acting as a stress test for purification and outperforming other protective perturbation methods in the purification-customization pipeline.

Conclusion: AntiPure exposes vulnerabilities in purification within the purification-customization workflow and demonstrates that perturbations can persist through purification. The work highlights the need for purification-aware defenses and suggests anti-purification as a diagnostic and design tool for safeguarding diffusion-model-based synthesis.

Abstract: Diffusion models like Stable Diffusion have become prominent in visual
synthesis tasks due to their powerful customization capabilities, which also
introduce significant security risks, including deepfakes and copyright
infringement. In response, a class of methods known as protective perturbation
emerged, which mitigates image misuse by injecting imperceptible adversarial
noise. However, purification can remove protective perturbations, thereby
exposing images again to the risk of malicious forgery. In this work, we
formalize the anti-purification task, highlighting challenges that hinder
existing approaches, and propose a simple diagnostic protective perturbation
named AntiPure. AntiPure exposes vulnerabilities of purification within the
"purification-customization" workflow, owing to two guidance mechanisms: 1)
Patch-wise Frequency Guidance, which reduces the model's influence over
high-frequency components in the purified image, and 2) Erroneous Timestep
Guidance, which disrupts the model's denoising strategy across different
timesteps. With additional guidance, AntiPure embeds imperceptible
perturbations that persist under representative purification settings,
achieving effective post-customization distortion. Experiments show that, as a
stress test for purification, AntiPure achieves minimal perceptual discrepancy
and maximal distortion, outperforming other protective perturbation methods
within the purification-customization workflow.

</details>


### [67] [Noise-Level Diffusion Guidance: Well Begun is Half Done](https://arxiv.org/abs/2509.13936)
*Harvey Mannering,Zhiwu Huang,Adam Prugel-Bennett*

Main category: cs.CV

TL;DR: A lightweight, training-free noise refinement method for diffusion models that nudges the initial Gaussian noise to better align with guidance signals, improving image quality and prompt adherence without extra data, networks, or backpropagation.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are sensitive to the random initial noise, which causes variability in outputs and prompt alignment. Existing noise-level optimization methods depend on extra data, auxiliary networks, or backpropagation, limiting practicality.

Method: Noise Level Guidance (NLG) refines the initial noise to increase its compatibility with general guidance, without training data, extra networks, or backpropagation. It provides a unified framework applicable to both conditional and unconditional diffusion models and supports various forms of diffusion-level guidance.

Result: Extensive experiments across five standard benchmarks show improved image quality and input-condition adherence. NLG remains computationally efficient and can integrate with existing guidance methods, enhancing practicality and scalability.

Conclusion: NLG offers a simple, general, and scalable enhancement for diffusion models that improves guidance adherence and output quality without requiring additional data, models, or training, and is readily adoptable alongside existing guidance techniques.

Abstract: Diffusion models have achieved state-of-the-art image generation. However,
the random Gaussian noise used to start the diffusion process influences the
final output, causing variations in image quality and prompt adherence.
Existing noise-level optimization approaches generally rely on extra dataset
construction, additional networks, or backpropagation-based optimization,
limiting their practicality. In this paper, we propose Noise Level Guidance
(NLG), a simple, efficient, and general noise-level optimization approach that
refines initial noise by increasing the likelihood of its alignment with
general guidance - requiring no additional training data, auxiliary networks,
or backpropagation. The proposed NLG approach provides a unified framework
generalizable to both conditional and unconditional diffusion models,
accommodating various forms of diffusion-level guidance. Extensive experiments
on five standard benchmarks demonstrate that our approach enhances output
generation quality and input condition adherence. By seamlessly integrating
with existing guidance methods while maintaining computational efficiency, our
method establishes NLG as a practical and scalable enhancement to diffusion
models. Code can be found at
https://github.com/harveymannering/NoiseLevelGuidance.

</details>


### [68] [Can Current AI Models Count What We Mean, Not What They See? A Benchmark and Systematic Evaluation](https://arxiv.org/abs/2509.13939)
*Gia Khanh Nguyen,Yifeng Huang,Minh Hoai*

Main category: cs.CV

TL;DR: PairTally introduces a focused benchmark for fine-grained visual counting, revealing current models struggle with intent-driven counting and offering a diagnostic foundation for future improvements.


<details>
  <summary>Details</summary>
Motivation: Counting objects with fine-grained distinctions in complex scenes is important but under-explored, especially for intent-driven (selective) counting; existing models (exemplar-based, prompt-based, VLMs) lack reliability in these settings.

Method: Construct the PairTally dataset of 681 high-resolution images, each containing two object categories with inter-category and intra-category settings; evaluate a range of models including exemplar-based methods, language-prompted models, and large VLMs.

Result: State-of-the-art models perform poorly on fine-grained, intent-driven counting tasks; they struggle to count the target category reliably in visually ambiguous cases.

Conclusion: PairTally provides a new diagnostic benchmark and foundation to develop and improve fine-grained visual counting systems.

Abstract: Visual counting is a fundamental yet challenging task, especially when users
need to count objects of a specific type in complex scenes. While recent
models, including class-agnostic counting models and large vision-language
models (VLMs), show promise in counting tasks, their ability to perform
fine-grained, intent-driven counting remains unclear. In this paper, we
introduce PairTally, a benchmark dataset specifically designed to evaluate
fine-grained visual counting. Each of the 681 high-resolution images in
PairTally contains two object categories, requiring models to distinguish and
count based on subtle differences in shape, size, color, or semantics. The
dataset includes both inter-category (distinct categories) and intra-category
(closely related subcategories) settings, making it suitable for rigorous
evaluation of selective counting capabilities. We benchmark a variety of
state-of-the-art models, including exemplar-based methods, language-prompted
models, and large VLMs. Our results show that despite recent advances, current
models struggle to reliably count what users intend, especially in fine-grained
and visually ambiguous cases. PairTally provides a new foundation for
diagnosing and improving fine-grained visual counting systems.

</details>


### [69] [Performance Optimization of YOLO-FEDER FusionNet for Robust Drone Detection in Visually Complex Environments](https://arxiv.org/abs/2509.14012)
*Tamara R. Lenhard,Andreas Weinmann,Tobias Koch*

Main category: cs.CV

TL;DR: An enhanced YOLO-FEDER FusionNet improves drone detection in visually complex scenes by integrating intermediate FEDER features and backbone upgrades, trained on synthetic data with limited real samples. The best configuration (YOLOv8l backbone with DWD-based FEDER features) achieves substantial gains: up to 39.1 percentage points reduction in FNR and up to 62.8 percentage points increase in mAP at IoU 0.5.


<details>
  <summary>Details</summary>
Motivation: Drone detection in cluttered, low-texture scenes is challenging due to background clutter, small object scale, and camouflage. Generic detectors like YOLO struggle with low object-background separability, motivating a specialized integration of camouflage-aware detection (FEDER) with standard detectors.

Method: An enhanced iteration of YOLO-FEDER FusionNet is proposed, featuring: (1) training data boosted by large-scale photo-realistic synthetic data plus a small real-world sample set; (2) intermediate multi-scale FEDER feature fusion; (3) backbone upgrades with YOLO-based backbones; (4) evaluation across multiple backbones; (5) use of FEDER features derived from the DWD module.

Result: Empirical evaluation shows meaningful improvements when incorporating intermediate FEDER features and backbone upgrades. The best configuration—YOLOv8l backbone with DWD-derived FEDER features—yields up to 39.1 pp FNR reduction and up to 62.8 pp mAP increase at IoU 0.5, relative to the initial baseline.

Conclusion: Integrating intermediate FEDER features with backbone enhancements and synthetic-to-real data mixtures can substantially improve drone detection in visually complex environments, indicating robustness gains for camouflage-laden targets.

Abstract: Drone detection in visually complex environments remains challenging due to
background clutter, small object scale, and camouflage effects. While generic
object detectors like YOLO exhibit strong performance in low-texture scenes,
their effectiveness degrades in cluttered environments with low
object-background separability. To address these limitations, this work
presents an enhanced iteration of YOLO-FEDER FusionNet -- a detection framework
that integrates generic object detection with camouflage object detection
techniques. Building upon the original architecture, the proposed iteration
introduces systematic advancements in training data composition, feature fusion
strategies, and backbone design. Specifically, the training process leverages
large-scale, photo-realistic synthetic data, complemented by a small set of
real-world samples, to enhance robustness under visually complex conditions.
The contribution of intermediate multi-scale FEDER features is systematically
evaluated, and detection performance is comprehensively benchmarked across
multiple YOLO-based backbone configurations. Empirical results indicate that
integrating intermediate FEDER features, in combination with backbone upgrades,
contributes to notable performance improvements. In the most promising
configuration -- YOLO-FEDER FusionNet with a YOLOv8l backbone and FEDER
features derived from the DWD module -- these enhancements lead to a FNR
reduction of up to 39.1 percentage points and a mAP increase of up to 62.8
percentage points at an IoU threshold of 0.5, compared to the initial baseline.

</details>


### [70] [SAIL-VL2 Technical Report](https://arxiv.org/abs/2509.14033)
*Weijie Yin,Yongjie Ye,Fangxun Shu,Yue Liao,Zijian Kang,Hongyuan Dong,Haiyang Yu,Dingkang Yang,Jiacong Wang,Han Wang,Wenzhuo Liu,Xiao Liang,Shuicheng Yan,Chao Feng*

Main category: cs.CV

TL;DR: SAIL-VL2 is an open-source, scalable multimodal foundation model that achieves state-of-the-art results on image and video benchmarks across 2B and 8B parameter scales, driven by data curation, progressive training, and sparse Mixture-of-Experts architectures.


<details>
  <summary>Details</summary>
Motivation: To build a unified, open, and scalable vision-language model capable of comprehensive image and video understanding and reasoning, addressing the need for strong, flexible multimodal AI at varied scales with efficient training and deployment.

Method: 1) Large-scale data curation with scoring/filtering across captioning, OCR, QA, and video data; 2) Progressive training: starting from SAIL-ViT vision encoder, then multimodal pre-training, ending with a thinking-fusion SFT-RL hybrid paradigm; 3) Architectural advances employing sparse Mixture-of-Experts (MoE) beyond dense LLMs for efficiency and scalability.

Result: SAIL-VL2 achieves state-of-the-art performance at both 2B and 8B scales across diverse image and video benchmarks, shows strong capabilities from fine-grained perception to complex reasoning, beats 106 datasets and attains top results on MMMU and MathVista; on the OpenCompass leaderboard, SAIL-VL2-2B ranks first among officially released open-source models under the 4B scale, indicating strong open-source competitiveness and extensibility.

Conclusion: The combination of high-quality data curation, a staged training regime that progressively builds capabilities, and scalable sparse architecture yields a competitive, open, and extensible multimodal foundation model capable of broad visual reasoning tasks.

Abstract: We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)
for comprehensive multimodal understanding and reasoning. As the successor to
SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B
parameter scales across diverse image and video benchmarks, demonstrating
strong capabilities from fine-grained perception to complex reasoning. Three
core innovations drive its effectiveness. First, a large-scale data curation
pipeline with scoring and filtering strategies enhances both quality and
distribution across captioning, OCR, QA, and video data, improving training
efficiency. Second, a progressive training framework begins with a powerful
pre-trained vision encoder (SAIL-ViT), advances through multimodal
pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that
systematically strengthens model capabilities. Third, architectural advances
extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.
With these contributions, SAIL-VL2 demonstrates competitive performance across
106 datasets and achieves state-of-the-art results on challenging reasoning
benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass
leaderboard, SAIL-VL2-2B ranks first among officially released open-source
models under the 4B parameter scale, while serving as an efficient and
extensible foundation for the open-source multimodal community.

</details>


### [71] [PROFUSEme: PROstate Cancer Biochemical Recurrence Prediction via FUSEd Multi-modal Embeddings](https://arxiv.org/abs/2509.14051)
*Suhang You,Carla Pitarch-Abaigar,Sanket Kachole,Sumedh Sonawane,Juhyung Ha,Anish Sudarshan Gada,David Crandall,Rakesh Shiradkar,Spyridon Bakas*

Main category: cs.CV

TL;DR: PROFUSEme uses intermediate-fusion of multi-modal clinical, radiology, and pathology data with CoxPH to predict biochemical recurrence after radical prostatectomy, achieving strong internal performance and competitive external results.


<details>
  <summary>Details</summary>
Motivation: Early prediction of biochemical recurrence (BCR) after radical prostatectomy is clinically valuable for guiding adjuvant therapy and surveillance. Multi-modal data can capture complementary signals from clinical, imaging, and pathology data, but effectively fusing these signals is challenging.

Method: An intermediate-fusion architecture learns cross-modal interactions among clinical, radiology, and pathology data to create fused embeddings, which are then fed into Cox Proportional Hazard regressors. Evaluation used an internal 5-fold nested cross-validation and a hold-out CHIMERA 2025 validation leaderboard.

Result: Mean C-index of 0.861 (σ=0.112) on internal nested CV; C-index of 0.7103 on CHIMERA 2025 hold-out validation.

Conclusion: Cross-modal fused embeddings with CoxPH (PROFUSEme) yield superior performance over late fusion baselines and show promise for early BCR risk stratification; external generalizability and calibration require further validation.

Abstract: Almost 30% of prostate cancer (PCa) patients undergoing radical prostatectomy
(RP) experience biochemical recurrence (BCR), characterized by increased
prostate specific antigen (PSA) and associated with increased mortality.
Accurate early prediction of BCR, at the time of RP, would contribute to prompt
adaptive clinical decision-making and improved patient outcomes. In this work,
we propose prostate cancer BCR prediction via fused multi-modal embeddings
(PROFUSEme), which learns cross-modal interactions of clinical, radiology, and
pathology data, following an intermediate fusion configuration in combination
with Cox Proportional Hazard regressors. Quantitative evaluation of our
proposed approach reveals superior performance, when compared with late fusion
configurations, yielding a mean C-index of 0.861 ($\sigma=0.112$) on the
internal 5-fold nested cross-validation framework, and a C-index of 0.7103 on
the hold out data of CHIMERA 2025 challenge validation leaderboard.

</details>


### [72] [Wan-Animate: Unified Character Animation and Replacement with Holistic Replication](https://arxiv.org/abs/2509.14055)
*Gang Cheng,Xin Gao,Li Hu,Siqi Hu,Mingyang Huang,Chaonan Ji,Ju Li,Dechao Meng,Jinwei Qi,Penchong Qiao,Zhen Shen,Yafei Song,Ke Sun,Linrui Tian,Feng Wang,Guangyuan Wang,Qi Wang,Zhongjian Wang,Jiayu Xiao,Sheng Xu,Bang Zhang,Peng Zhang,Xindi Zhang,Zhe Zhang,Jingren Zhou,Lian Zhuo*

Main category: cs.CV

TL;DR: Wan-Animate unifies character animation and replacement in a single framework that reproduces a subject's motion and expressions from a reference video and can seamlessly relight and insert the animated character into a scene, achieving state-of-the-art results and open-sourcing the model.


<details>
  <summary>Details</summary>
Motivation: There is a need for a single, controllable framework that handles both motion/expressions transfer and scene-consistent character replacement, with consistent lighting and color to enable believable, integration-ready results.

Method: Built on the Wan model with a modified input paradigm that differentiates reference conditions and generation regions. Uses spatially-aligned skeleton signals for body motion and implicit facial features from source images to reenact expressions. Includes an auxiliary Relighting LoRA to apply appropriate environmental lighting and color tone for replacement. The framework supports animation and replacement within a unified symbolic representation and is intended for open-source release.

Result: Experimental evaluations show state-of-the-art performance in both animation and replacement tasks.

Conclusion: Wan-Animate delivers a unified, controllable approach for character animation and scene-integrated replacement, with dedicated components (skeleton-driven motion, facial-expression reenactment, and Relighting LoRA) to ensure both fidelity and environmental consistency, and plans for open-source release.

Abstract: We introduce Wan-Animate, a unified framework for character animation and
replacement. Given a character image and a reference video, Wan-Animate can
animate the character by precisely replicating the expressions and movements of
the character in the video to generate high-fidelity character videos.
Alternatively, it can integrate the animated character into the reference video
to replace the original character, replicating the scene's lighting and color
tone to achieve seamless environmental integration. Wan-Animate is built upon
the Wan model. To adapt it for character animation tasks, we employ a modified
input paradigm to differentiate between reference conditions and regions for
generation. This design unifies multiple tasks into a common symbolic
representation. We use spatially-aligned skeleton signals to replicate body
motion and implicit facial features extracted from source images to reenact
expressions, enabling the generation of character videos with high
controllability and expressiveness. Furthermore, to enhance environmental
integration during character replacement, we develop an auxiliary Relighting
LoRA. This module preserves the character's appearance consistency while
applying the appropriate environmental lighting and color tone. Experimental
results demonstrate that Wan-Animate achieves state-of-the-art performance. We
are committed to open-sourcing the model weights and its source code.

</details>


### [73] [VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement](https://arxiv.org/abs/2509.14060)
*Jun Du,Weiwei Xing,Ming Li,Fei Richard Yu*

Main category: cs.CV

TL;DR: Proposes VSE-MOT, a vision-language-guided multi-object tracking framework that injects global semantic information into MOT via a tri-branch architecture, an MOT-Adapter, and a Visual Semantic Fusion Module, achieving robust improvements, especially in low-quality real-world videos.


<details>
  <summary>Details</summary>
Motivation: MOT performance collapses on low-quality footage common in real-world settings; existing methods underutilize global semantic cues. Vision-language models offer rich semantic information that can be leveraged to improve tracking robustness.

Method: A tri-branch architecture uses a vision-language model to extract global visual semantic information from images and fuse it with MOT query vectors. The MOT-Adapter adapts the semantic information for MOT tasks, and the Visual Semantic Fusion Module enhances feature fusion. The approach is evaluated on real-world low-quality videos and conventional scenarios.

Result: The method achieves approximately 8% to 20% improvement in tracking performance on real-world low-quality video scenarios compared with existing methods, while maintaining robust performance in conventional scenarios.

Conclusion: Incorporating global visual semantic information with vision-language guidance enhances MOT in challenging real-world settings, indicating strong potential for practical deployments and suggesting avenues for broader application and refinement.

Abstract: Current multi-object tracking (MOT) algorithms typically overlook issues
inherent in low-quality videos, leading to significant degradation in tracking
performance when confronted with real-world image deterioration. Therefore,
advancing the application of MOT algorithms in real-world low-quality video
scenarios represents a critical and meaningful endeavor. To address the
challenges posed by low-quality scenarios, inspired by vision-language models,
this paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking
framework (VSE-MOT). Specifically, we first design a tri-branch architecture
that leverages a vision-language model to extract global visual semantic
information from images and fuse it with query vectors. Subsequently, to
further enhance the utilization of visual semantic information, we introduce
the Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion
Module (VSFM). The MOT-Adapter adapts the extracted global visual semantic
information to suit multi-object tracking tasks, while the VSFM improves the
efficacy of feature fusion. Through extensive experiments, we validate the
effectiveness and superiority of the proposed method in real-world low-quality
video scenarios. Its tracking performance metrics outperform those of existing
methods by approximately 8% to 20%, while maintaining robust performance in
conventional scenarios.

</details>


### [74] [AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration](https://arxiv.org/abs/2509.14084)
*Jingyi Yuan,Jianxiong Ye,Wenkang Chen,Chenqiang Gao*

Main category: cs.CV

TL;DR: AD-DINOv3 introduces a vision-language multimodal framework for zero-shot anomaly detection by aligning DINOv3 visual features with CLIP text embeddings via adapters and an anomaly-aware calibration module, achieving state-of-the-art performance on eight benchmarks.


<details>
  <summary>Details</summary>
Motivation: Zero-shot anomaly detection suffers from domain misalignment between large-scale pretraining data and anomaly tasks and a bias toward global semantics that can miss subtle anomalies. There is a need to leverage powerful vision backbones (like DINOv3) while enhancing region-level discriminability.

Method: Formulate anomaly detection as multimodal contrastive learning. Use DINOv3 as the visual backbone to extract patch tokens and CLS token; employ the CLIP text encoder to generate embeddings for normal and abnormal prompts; introduce lightweight adapters in both modalities to recalibrate representations for the task; add an Anomaly-Aware Calibration Module (AACM) to guide the CLS token to attend to anomalous regions rather than generic foreground semantics.

Result: Extensive experiments on eight industrial and medical benchmarks show that AD-DINOv3 consistently matches or surpasses state-of-the-art methods for zero-shot anomaly detection.

Conclusion: AD-DINOv3 provides a general, effective ZSAD framework by bridging domain gaps and improving region-specific anomaly discrimination through the anomaly-aware calibration module, validating its applicability to varied industrial and medical anomaly detection tasks.

Abstract: Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrary
novel categories, offering a scalable and annotation-efficient solution.
Traditionally, most ZSAD works have been based on the CLIP model, which
performs anomaly detection by calculating the similarity between visual and
text embeddings. Recently, vision foundation models such as DINOv3 have
demonstrated strong transferable representation capabilities. In this work, we
are the first to adapt DINOv3 for ZSAD. However, this adaptation presents two
key challenges: (i) the domain bias between large-scale pretraining data and
anomaly detection tasks leads to feature misalignment; and (ii) the inherent
bias toward global semantics in pretrained representations often leads to
subtle anomalies being misinterpreted as part of the normal foreground objects,
rather than being distinguished as abnormal regions. To overcome these
challenges, we introduce AD-DINOv3, a novel vision-language multimodal
framework designed for ZSAD. Specifically, we formulate anomaly detection as a
multimodal contrastive learning problem, where DINOv3 is employed as the visual
backbone to extract patch tokens and a CLS token, and the CLIP text encoder
provides embeddings for both normal and abnormal prompts. To bridge the domain
gap, lightweight adapters are introduced in both modalities, enabling their
representations to be recalibrated for the anomaly detection task. Beyond this
baseline alignment, we further design an Anomaly-Aware Calibration Module
(AACM), which explicitly guides the CLS token to attend to anomalous regions
rather than generic foreground semantics, thereby enhancing discriminability.
Extensive experiments on eight industrial and medical benchmarks demonstrate
that AD-DINOv3 consistently matches or surpasses state-of-the-art methods,
verifying its superiority as a general zero-shot anomaly detection framework.

</details>


### [75] [Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing](https://arxiv.org/abs/2509.14097)
*Yaru Chen,Ruohao Guo,Liting Gao,Yang Xiang,Qingyu Luo,Zhenbo Li,Wenwu Wang*

Main category: cs.CV

TL;DR: Introduces EMA-guided pseudo supervision and class-aware cross-modal agreement for weakly-supervised AVVP, enabling stable segment-level supervision and cross-modal alignment, achieving SOTA on LLP and UnAV-100.


<details>
  <summary>Details</summary>
Motivation: Current AVVP methods rely on video-level labels and global predictions, lacking stable segment-level supervision and class-aware alignment across modalities, leading to limited temporal precision and cross-modal consistency.

Method: 1) EMA-guided pseudo supervision to generate reliable segment-level masks via adaptive thresholds or top-k selection; 2) CMA loss to align audio and visual embeddings at reliable segment-class pairs, preserving temporal structure.

Result: State-of-the-art performance on LLP and UnAV-100 across multiple metrics.

Conclusion: The two strategies provide stable temporal guidance and cross-modal coherence, boosting weakly-supervised AVVP performance; potential improvements could involve exploring robustness and scalability.

Abstract: Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible,
visible, and audio-visual events without temporal annotations. Previous work
has emphasized refining global predictions through contrastive or collaborative
learning, but neglected stable segment-level supervision and class-aware
cross-modal alignment. To address this, we propose two strategies: (1) an
exponential moving average (EMA)-guided pseudo supervision framework that
generates reliable segment-level masks via adaptive thresholds or top-k
selection, offering stable temporal guidance beyond video-level labels; and (2)
a class-aware cross-modal agreement (CMA) loss that aligns audio and visual
embeddings at reliable segment-class pairs, ensuring consistency across
modalities while preserving temporal structure. Evaluations on LLP and UnAV-100
datasets shows that our method achieves state-of-the-art (SOTA) performance
across multiple metrics.

</details>


### [76] [CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts](https://arxiv.org/abs/2509.14104)
*Leonard Hackel,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: CSMoE enhances Cross-Sensor Masked Autoencoder (CSMAE) by integrating Soft Mixture-of-Experts to enable modality-specific specialization and shared cross-sensor learning, achieving >2x computational efficiency with competitive accuracy; includes a thematic-climatic sampling strategy; code to be released.


<details>
  <summary>Details</summary>
Motivation: RS foundation models often suffer from high computational cost during training and inference and may have limited representational capacity across diverse sensors. There is a need for efficient, scalable RS FMs that leverage cross-sensor information while enabling modality-specific expertise.

Method: Incorporate Soft Mixture-of-Experts into the Cross-Sensor Masked Autoencoder to form CSMoE, enabling modality-specific experts alongside shared representations. Introduce a thematic-climatic descriptor-driven sampling strategy to construct a representative, diverse training set. Validate on scene classification, semantic segmentation, and content-based image retrieval, comparing against state-of-the-art RS FMs.

Result: CSMoE achieves more than twofold computational efficiency on average compared with existing RS FMs, while maintaining or improving representational performance. Extensive experiments across scene classification, semantic segmentation, and content-based image retrieval demonstrate competitive accuracy and a favorable capacity–accuracy–efficiency trade-off.

Conclusion: The proposed Soft MoE-based adaptation effectively yields computationally efficient RS foundation models with strong representational capacity. The approach is validated across multiple tasks and datasets, and the authors plan to release code, training set creation, and model weights.

Abstract: Self-supervised learning through masked autoencoders has attracted great
attention for remote sensing (RS) foundation model (FM) development, enabling
improved representation learning across diverse sensors and downstream tasks.
However, existing RS FMs often either suffer from substantial computational
complexity during both training and inference or exhibit limited
representational capacity. These issues restrict their practical applicability
in RS. To address this limitation, we propose an adaptation for enhancing the
efficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism
into the FM. The integration of Soft MoEs into the FM allows modality-specific
expert specialization alongside shared cross-sensor representation learning. To
demonstrate the effectiveness of our adaptation, we apply it on the
Cross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor
Mixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic
descriptor-driven sampling strategy for the construction of a representative
and diverse training set to train our CSMoE model. Extensive experiments on
scene classification, semantic segmentation, and content-based image retrieval
demonstrate that our adaptation yields a reduction in computational
requirements while maintaining or improving representational performance.
Compared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off
between representational capacity, accuracy, and computational efficiency. On
average, CSMoE achieves more than twice the computational efficiency of
existing RS FMs, while maintaining competitive performance across all
experiments. These results show the effectiveness of the proposed adaptation
for creating computationally efficient RS FMs. The code for the model, the
training set creation, and the model weights will be available at
https://git.tu-berlin.de/rsim/csmoe.

</details>


### [77] [Generative AI for Misalignment-Resistant Virtual Staining to Accelerate Histopathology Workflows](https://arxiv.org/abs/2509.14119)
*Jiabo MA,Wenqiang Li,Jinbang Li,Ziyi Liu,Linshan Wu,Fengtao Zhou,Li Liang,Ronald Cheong Kin Chan,Terence T. W. Wong,Hao Chen*

Main category: cs.CV

TL;DR: Introduces a robust virtual staining framework with cascaded registration to align generated outputs with ground truth in unpaired/roughly paired data, achieving notable gains across five datasets.


<details>
  <summary>Details</summary>
Motivation: Virtual staining is hindered by the difficulty of obtaining well-aligned paired data; tissue distortion during chemical staining makes precise alignment hard, leading to weak pixel-level supervision and degraded performance. A method that can tolerate misalignment is needed to enable practical clinical deployment.

Method: A cascaded registration framework integrated into the virtual staining model that progressively aligns generated outputs with ground truth, enabling robust pixel-level supervision even when paired data are unaligned or only roughly paired. Evaluated on five datasets.

Result: Outperforms state-of-the-art methods on all five datasets. Average gains: internal datasets +3.2% (internal), +10.1% (external). In datasets with substantial misalignment, PSNR improved by 23.8% over baselines.

Conclusion: The cascaded registration approach enhances robustness to spatial misalignment, simplifies data acquisition, and broadens the practical viability of virtual staining, with strong generalization across datasets and clear performance gains over existing methods.

Abstract: Accurate histopathological diagnosis often requires multiple differently
stained tissue sections, a process that is time-consuming, labor-intensive, and
environmentally taxing due to the use of multiple chemical stains. Recently,
virtual staining has emerged as a promising alternative that is faster,
tissue-conserving, and environmentally friendly. However, existing virtual
staining methods face significant challenges in clinical applications,
primarily due to their reliance on well-aligned paired data. Obtaining such
data is inherently difficult because chemical staining processes can distort
tissue structures, and a single tissue section cannot undergo multiple staining
procedures without damage or loss of information. As a result, most available
virtual staining datasets are either unpaired or roughly paired, making it
difficult for existing methods to achieve accurate pixel-level supervision. To
address this challenge, we propose a robust virtual staining framework
featuring cascaded registration mechanisms to resolve spatial mismatches
between generated outputs and their corresponding ground truth. Experimental
results demonstrate that our method significantly outperforms state-of-the-art
models across five datasets, achieving an average improvement of 3.2% on
internal datasets and 10.1% on external datasets. Moreover, in datasets with
substantial misalignment, our approach achieves a remarkable 23.8% improvement
in peak signal-to-noise ratio compared to baseline models. The exceptional
robustness of the proposed method across diverse datasets simplifies the data
acquisition process for virtual staining and offers new insights for advancing
its development.

</details>


### [78] [Deceptive Beauty: Evaluating the Impact of Beauty Filters on Deepfake and Morphing Attack Detection](https://arxiv.org/abs/2509.14120)
*Sara Concas,Simone Maurizio La Cava,Andrea Panzino,Ester Masala,Giulia Orrù,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: Beauty filters degrade deepfake/morphing detectors; study shows vulnerability of automated face-analysis systems to smoothing alterations.


<details>
  <summary>Details</summary>
Motivation: Rising use of social-media beauty filters may alter facial imagery, potentially misleading both humans and automated detectors tasked with distinguishing genuine from manipulated media (deepfakes, morphing).

Method: Comprehensive evaluation of multiple state-of-the-art deepfake/morphing detectors on benchmark datasets, comparing performance before and after applying various smoothing beauty filters.

Result: Detector performance degrades after beauty-filter application, signaling vulnerabilities introduced by facial enhancements and the need for detectors robust to such alterations.

Conclusion: Robust detection models handling beauty-filter induced alterations are needed; further research should address resilience to facial enhancements and related preprocessing effects.

Abstract: Digital beautification through social media filters has become increasingly
popular, raising concerns about the reliability of facial images and videos and
the effectiveness of automated face analysis. This issue is particularly
critical for digital manipulation detectors, systems aiming at distinguishing
between genuine and manipulated data, especially in cases involving deepfakes
and morphing attacks designed to deceive humans and automated facial
recognition. This study examines whether beauty filters impact the performance
of deepfake and morphing attack detectors. We perform a comprehensive analysis,
evaluating multiple state-of-the-art detectors on benchmark datasets before and
after applying various smoothing filters. Our findings reveal performance
degradation, highlighting vulnerabilities introduced by facial enhancements and
underscoring the need for robust detection models resilient to such
alterations.

</details>


### [79] [MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook](https://arxiv.org/abs/2509.14142)
*Peng Xu,Shengwu Xiong,Jiajun Zhang,Yaxiong Chen,Bowen Zhou,Chen Change Loy,David A. Clifton,Kyoung Mu Lee,Luc Van Gool,Ruiming He,Ruilin Yao,Xinwei Long,Jirui Huang,Kai Tian,Sa Yang,Yihua Shao,Jin Feng,Yue Zhong,Jiakai Zhou,Cheng Tang,Tianyu Zou,Yifang Zhang,Junming Liang,Guoyou Li,Zhaoxiang Wang,Qiang Zhou,Yichen Zhao,Shili Xiong,Hyeongjin Nam,Jaerin Lee,Jaeyoung Chung,JoonKyu Park,Junghun Oh,Kanggeon Lee,Wooseok Lee,Juneyoung Ro,Turghun Osman,Can Hu,Chaoyang Liao,Cheng Chen,Chengcheng Han,Chenhao Qiu,Chong Peng,Cong Xu,Dailin Li,Feiyu Wang,Feng Gao,Guibo Zhu,Guopeng Tang,Haibo Lu,Han Fang,Han Qi,Hanxiao Wu,Haobo Cheng,Hongbo Sun,Hongyao Chen,Huayong Hu,Hui Li,Jiaheng Ma,Jiang Yu,Jianing Wang,Jie Yang,Jing He,Jinglin Zhou,Jingxuan Li,Josef Kittler,Lihao Zheng,Linnan Zhao,Mengxi Jia,Muyang Yan,Nguyen Thanh Thien,Pu Luo,Qi Li,Shien Song,Shijie Dong,Shuai Shao,Shutao Li,Taofeng Xue,Tianyang Xu,Tianyi Gao,Tingting Li,Wei Zhang,Weiyang Su,Xiaodong Dong,Xiao-Jun Wu,Xiaopeng Zhou,Xin Chen,Xin Wei,Xinyi You,Xudong Kang,Xujie Zhou,Xusheng Liu,Yanan Wang,Yanbin Huang,Yang Liu,Yang Yang,Yanglin Deng,Yashu Kang,Ye Yuan,Yi Wen,Yicen Tian,Yilin Tao,Yin Tang,Yipeng Lin,Yiqing Wang,Yiting Xi,Yongkang Yu,Yumei Li,Yuxin Qin,Yuying Chen,Yuzhe Cen,Zhaofan Zou,Zhaohong Liu,Zhehao Shen,Zhenglin Du,Zhengyang Li,Zhenni Huang,Zhenwei Shao,Zhilong Song,Zhiyong Feng,Zhiyu Wang,Zhou Yu,Ziang Li,Zihan Zhai,Zijian Zhang,Ziyang Peng,Ziyun Xiao,Zongshu Li*

Main category: cs.CV

TL;DR: MARS2 2025 presents a multimodal reasoning benchmark with two new datasets (Lens and AdsQA), evaluates 40+ baselines across three tracks, and reports 76 teams registered with 40+ submissions; datasets, code, and rankings are publicly available to advance MLLMs in real-world and domain-specific scenarios.


<details>
  <summary>Details</summary>
Motivation: The field lacks a unified, large benchmark to compare diverse multimodal ML approaches and LLMs across real-world and domain-specific tasks. As general-purpose LLMs improve, there is a need to evaluate their multimodal reasoning capabilities in varied settings and to foster collaboration through shared datasets and leaderboards.

Method: Organize the MARS2 2025 workshop and benchmark, release two tailored datasets (Lens for general reasoning in daily scenarios; AdsQA for domain-specific reasoning in advertisement videos), evaluate 40+ baselines (generalist MLLMs and task-specific models), and run three competition tracks (VG-RS, VQA-SA, VR-Ads). Engage participants from academia and industry, and publish ongoing rankings and open-source resources.

Result: Established three competition tracks, engaged 76 teams with 40+ valid submissions, released datasets, 40+ baselines, and 15+ participant methods, with publicly available rankings, code, and updates on GitHub.

Conclusion: The MARS2 2025 effort provides a scalable, open platform to benchmark multimodal reasoning, highlights the momentum in MLLMs for real-world and domain-specific tasks, and invites broad participation to push state-of-the-art through shared datasets, baselines, and transparent evaluation.

Abstract: This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim
to bring together different approaches in multimodal machine learning and LLMs
via a large benchmark. We hope it better allows researchers to follow the
state-of-the-art in this very dynamic area. Meanwhile, a growing number of
testbeds have boosted the evolution of general-purpose large language models.
Thus, this year's MARS2 focuses on real-world and specialized scenarios to
broaden the multimodal reasoning applications of MLLMs. Our organizing team
released two tailored datasets Lens and AdsQA as test sets, which support
general reasoning in 12 daily scenarios and domain-specific reasoning in
advertisement videos, respectively. We evaluated 40+ baselines that include
both generalist MLLMs and task-specific models, and opened up three competition
tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question
Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative
Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and
industrial institutions have registered and 40+ valid submissions (out of
1200+) have been included in our ranking lists. Our datasets, code sets (40+
baselines and 15+ participants' methods), and rankings are publicly available
on the MARS2 workshop website and our GitHub organization page
https://github.com/mars2workshop/, where our updates and announcements of
upcoming events will be continuously provided.

</details>


### [80] [An Exploratory Study on Abstract Images and Visual Representations Learned from Them](https://arxiv.org/abs/2509.14149)
*Haotian Li,Jianbo Jiao*

Main category: cs.CV

TL;DR: A dataset (HAID) of abstract images generated from raster images at multiple abstraction levels is introduced to study how much high-level semantic information can be preserved, compare abstract vs raster representations across classification, segmentation, and detection, and evaluate the potential of abstract images as a viable semantic format for vision tasks.


<details>
  <summary>Details</summary>
Motivation: Investigate the gap in semantic representational quality between abstract shape-based images and traditional raster images; understand how semantic content changes with abstraction level; provide a dataset and benchmarks to quantify this.

Method: Construct HAID by transforming normal raster images into abstract representations at multiple abstraction levels; train and evaluate conventional vision models on HAID for classification, segmentation, and object detection; perform cross-level analyses to identify what semantic content survives.

Result: The abstract describes a comprehensive study showing abstract images can convey semantic information but often underperform compared to raster images; provides insights into what semantic content is retained or lost at different abstraction levels; presents baseline results across tasks and analyses.

Conclusion: Abstract images may be a viable but currently limited format for conveying visual semantic information; HAID is a valuable benchmark for studying high-level content at hierarchical abstraction and guiding future improvements.

Abstract: Imagine living in a world composed solely of primitive shapes, could you
still recognise familiar objects? Recent studies have shown that abstract
images-constructed by primitive shapes-can indeed convey visual semantic
information to deep learning models. However, representations obtained from
such images often fall short compared to those derived from traditional raster
images. In this paper, we study the reasons behind this performance gap and
investigate how much high-level semantic content can be captured at different
abstraction levels. To this end, we introduce the Hierarchical Abstraction
Image Dataset (HAID), a novel data collection that comprises abstract images
generated from normal raster images at multiple levels of abstraction. We then
train and evaluate conventional vision systems on HAID across various tasks
including classification, segmentation, and object detection, providing a
comprehensive study between rasterised and abstract image representations. We
also discuss if the abstract image can be considered as a potentially effective
format for conveying visual semantic information and contributing to vision
tasks.

</details>


### [81] [BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection](https://arxiv.org/abs/2509.14151)
*Rongyu Zhang,Jiaming Liu,Xiaoqi Li,Xiaowei Chi,Dan Wang,Li Du,Yuan Du,Shanghang Zhang*

Main category: cs.CV

TL;DR: BEVUDA++ introduces a geometry-aware teacher-student framework for cross-domain BEV perception, combining a Reliable Depth Teacher and a Geometric Consistent Student with Uncertainty-guided EMA to align multi-space features; achieves state-of-the-art on Day-Night adaptation.


<details>
  <summary>Details</summary>
Motivation: BEV perception suffers from domain shift across real-world cross-domain scenarios; BEV methods operate across 2D, voxel, BEV spaces, causing accumulation of domain gaps; prior work neglects DA in BEV multi-view 3D detection.

Method: Propose BEVUDA++: RDT blends target LiDAR with depth predictions using uncertainty to produce depth-aware features for voxel/BEV; GCS maps multi-space features into a unified geometric embedding; UEMA reduces error accumulation guided by uncertainty.

Result: Four cross-domain experiments show state-of-the-art performance; e.g., 12.9% NDS and 9.5% mAP improvement on Day-Night adaptation.

Conclusion: Geometric-aware teacher-student with uncertainty-guided EMA effectively mitigates domain shift in BEV perception, enabling robust cross-domain BEV 3D object detection.

Abstract: Vision-centric Bird's Eye View (BEV) perception holds considerable promise
for autonomous driving. Recent studies have prioritized efficiency or accuracy
enhancements, yet the issue of domain shift has been overlooked, leading to
substantial performance degradation upon transfer. We identify major domain
gaps in real-world cross-domain scenarios and initiate the first effort to
address the Domain Adaptation (DA) challenge in multi-view 3D object detection
for BEV perception. Given the complexity of BEV perception approaches with
their multiple components, domain shift accumulation across multi-geometric
spaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domain
adaptation. In this paper, we introduce an innovative geometric-aware
teacher-student framework, BEVUDA++, to diminish this issue, comprising a
Reliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model.
Specifically, RDT effectively blends target LiDAR with dependable depth
predictions to generate depth-aware information based on uncertainty
estimation, enhancing the extraction of Voxel and BEV features that are
essential for understanding the target domain. To collaboratively reduce the
domain shift, GCS maps features from multiple spaces into a unified geometric
embedding space, thereby narrowing the gap in data distribution between the two
domains. Additionally, we introduce a novel Uncertainty-guided Exponential
Moving Average (UEMA) to further reduce error accumulation due to domain shifts
informed by previously obtained uncertainty guidance. To demonstrate the
superiority of our proposed method, we execute comprehensive experiments in
four cross-domain scenarios, securing state-of-the-art performance in BEV 3D
object detection tasks, e.g., 12.9\% NDS and 9.5\% mAP enhancement on Day-Night
adaptation.

</details>


### [82] [Cinéaste: A Fine-grained Contextual Movie Question Answering Benchmark](https://arxiv.org/abs/2509.14227)
*Nisarg A. Shah,Amir Ziai,Chaitanya Ekanadham,Vishal M. Patel*

Main category: cs.CV

TL;DR: Cinèaste is a long-form movie understanding benchmark (3,119 QA pairs from 1,805 scenes across 200 movies) generated with GPT-4o; uses two-stage filtering; reveals bottlenecks in long-range temporal reasoning, with top open-source model at 63.15% accuracy.


<details>
  <summary>Details</summary>
Motivation: Fill the gap in evaluating deep narrative understanding beyond short clips or templated questions; assess long-form contextual reasoning in vision-language models.

Method: Assemble dataset from 1,805 scenes across 200 films; generate diverse, context-rich questions via GPT-4o using visual descriptions, captions, titles, summaries; two-stage filters: Context-Independence (requires video context) and Contextual Veracity (fact-consistency with content); five novel categories of fine-grained reasoning; 3,119 MCQ pairs.

Result: Existing multimodal language models struggle; accuracy around 63.15% for top open-source model; long-range temporal reasoning is main bottleneck.

Conclusion: This dataset highlights the challenge of long-form movie comprehension and motivates future work to improve long-form contextual understanding in multimodal language models.

Abstract: While recent advancements in vision-language models have improved video
understanding, diagnosing their capacity for deep, narrative comprehension
remains a challenge. Existing benchmarks often test short-clip recognition or
use template-based questions, leaving a critical gap in evaluating fine-grained
reasoning over long-form narrative content. To address these gaps, we introduce
$\mathsf{Cin\acute{e}aste}$, a comprehensive benchmark for long-form movie
understanding. Our dataset comprises 3,119 multiple-choice question-answer
pairs derived from 1,805 scenes across 200 diverse movies, spanning five novel
fine-grained contextual reasoning categories. We use GPT-4o to generate
diverse, context-rich questions by integrating visual descriptions, captions,
scene titles, and summaries, which require deep narrative understanding. To
ensure high-quality evaluation, our pipeline incorporates a two-stage filtering
process: Context-Independence filtering ensures questions require video
context, while Contextual Veracity filtering validates factual consistency
against the movie content, mitigating hallucinations. Experiments show that
existing MLLMs struggle on $\mathsf{Cin\acute{e}aste}$; our analysis reveals
that long-range temporal reasoning is a primary bottleneck, with the top
open-source model achieving only 63.15\% accuracy. This underscores significant
challenges in fine-grained contextual understanding and the need for
advancements in long-form movie comprehension.

</details>


### [83] [GenExam: A Multidisciplinary Text-to-Image Exam](https://arxiv.org/abs/2509.14232)
*Zhaokai Wang,Penghao Yin,Xiangyu Zhao,Changyao Tian,Yu Qiao,Wenhai Wang,Jifeng Dai,Gen Luo*

Main category: cs.CV

TL;DR: GenExam: first benchmark for multidisciplinary text-to-image exams with 1,000 samples across 10 subjects, four-level taxonomy, ground-truth images and fine-grained scoring; current models score very low (<15% strict, near 0%), highlighting the challenge and potential for AGI progress.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether image generation models can integrate knowledge, reasoning, and generation in a multidisciplinary exam-like setting, addressing gaps in existing generation benchmarks that focus on static world knowledge or simple reasoning.

Method: Construct GenExam dataset: 1,000 samples across 10 subjects with exam-style prompts organized under a four-level taxonomy; provide ground-truth images and fine-grained scoring points for semantic correctness and visual plausibility; evaluate state-of-the-art models (e.g., GPT-Image-1, Gemini-2.5-Flash-Image) on strict scores.

Result: State-of-the-art models achieve less than 15% strict scores; most models near 0% on strict scoring, illustrating the benchmark's difficulty and the current gap in integrated text-to-image generation.

Conclusion: GenExam offers a rigorous, exam-style framework to assess integration of knowledge, reasoning, and generation in image synthesis and provides benchmarks that illuminate the path toward general AGI.

Abstract: Exams are a fundamental test of expert-level intelligence and require
integrated understanding, reasoning, and generation. Existing exam-style
benchmarks mainly focus on understanding and reasoning tasks, and current
generation benchmarks emphasize the illustration of world knowledge and visual
concepts, neglecting the evaluation of rigorous drawing exams. We introduce
GenExam, the first benchmark for multidisciplinary text-to-image exams,
featuring 1,000 samples across 10 subjects with exam-style prompts organized
under a four-level taxonomy. Each problem is equipped with ground-truth images
and fine-grained scoring points to enable a precise evaluation of semantic
correctness and visual plausibility. Experiments show that even
state-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve
less than 15% strict scores, and most models yield almost 0%, suggesting the
great challenge of our benchmark. By framing image generation as an exam,
GenExam offers a rigorous assessment of models' ability to integrate knowledge,
reasoning, and generation, providing insights on the path to general AGI.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [84] [Unified Spatiotemopral Physics-Informed Learning (USPIL): A Framework for Modeling Complex Predator-Prey Dynamics](https://arxiv.org/abs/2509.13425)
*Julian Evan Chrisnanto,Yulison Herry Chrisnanto,Ferry Faizal*

Main category: cs.LG

TL;DR: USPIL unifies physics-informed neural networks with conservation laws to model multi-scale ecological dynamics (ODEs and PDEs) in predator-prey systems, achieving accurate temporal dynamics and complex spatiotemporal patterns with significant speedups over solvers.


<details>
  <summary>Details</summary>
Motivation: Traditional ecological models struggle with multi-scale dynamics and the need to respect physical conservation laws. A unified framework that handles both temporal oscillations and spatial patterns while enabling parameter discovery could improve forecasting, conservation planning, and resilience assessment.

Method: A deep learning architecture that couples PINNs with conservation constraints, using automatic differentiation and adaptive loss weighting to enforce physics while fitting data. The model solves both ODEs and PDEs within a single network. Applied to Lotka-Volterra dynamics, it demonstrates 1D temporal accuracy (high correlation, low MAE) and 2D reaction-diffusion patterns (spiral waves) with substantial conservation fidelity and speedups in inference.

Result: In 1D, USPIL achieves 0.989 correlation (loss 0.0219, MAE 0.0184). In 2D, it captures spiral wave patterns with pattern correlation 0.94 and loss 4.7656. Conservation laws hold within 0.5%, and inference is 10–50x faster than numerical solvers.

Conclusion: USPIL represents a transformative physics-informed approach that enables mechanistic interpretation, parameter discovery, and multi-scale ecological modeling across dimensional formulations, offering improved forecasting, conservation planning, and insights into ecosystem resilience.

Abstract: Ecological systems exhibit complex multi-scale dynamics that challenge
traditional modeling. New methods must capture temporal oscillations and
emergent spatiotemporal patterns while adhering to conservation principles. We
present the Unified Spatiotemporal Physics-Informed Learning (USPIL) framework,
a deep learning architecture integrating physics-informed neural networks
(PINNs) and conservation laws to model predator-prey dynamics across
dimensional scales. The framework provides a unified solution for both ordinary
(ODE) and partial (PDE) differential equation systems, describing temporal
cycles and reaction-diffusion patterns within a single neural network
architecture. Our methodology uses automatic differentiation to enforce physics
constraints and adaptive loss weighting to balance data fidelity with physical
consistency. Applied to the Lotka-Volterra system, USPIL achieves 98.9%
correlation for 1D temporal dynamics (loss: 0.0219, MAE: 0.0184) and captures
complex spiral waves in 2D systems (loss: 4.7656, pattern correlation: 0.94).
Validation confirms conservation law adherence within 0.5% and shows a 10-50x
computational speedup for inference compared to numerical solvers. USPIL also
enables mechanistic understanding through interpretable physics constraints,
facilitating parameter discovery and sensitivity analysis not possible with
purely data-driven methods. Its ability to transition between dimensional
formulations opens new avenues for multi-scale ecological modeling. These
capabilities make USPIL a transformative tool for ecological forecasting,
conservation planning, and understanding ecosystem resilience, establishing
physics-informed deep learning as a powerful and scientifically rigorous
paradigm.

</details>


### [85] [An Analysis of Optimizer Choice on Energy Efficiency and Performance in Neural Network Training](https://arxiv.org/abs/2509.13516)
*Tom Almog*

Main category: cs.LG

TL;DR: A systematic study links optimizer choice to training energy use, speed, and accuracy across MNIST and CIFAR. AdamW and NAdam are generally energy-efficient; SGD can achieve strong accuracy on harder datasets but at higher emissions.


<details>
  <summary>Details</summary>
Motivation: As ML models grow in size and complexity, the environmental impact of training decisions becomes a critical consideration. The study seeks to identify optimizer choices that balance performance with sustainability.

Method: 360 controlled experiments across three datasets (MNIST, CIFAR-10, CIFAR-100) using eight optimizers (SGD, Adam, AdamW, RMSprop, Adagrad, Adadelta, Adamax, NAdam) with 15 seeds each. Energy, time, memory, CO2 emissions, and final accuracy were measured with CodeCarbon on Apple M1 Pro hardware.

Result: There are substantial trade-offs between training speed, accuracy, and emissions that depend on dataset and model complexity. AdamW and NAdam emerged as consistently efficient; SGD can outperform on more complex datasets but incurs higher energy use and emissions.

Conclusion: The work offers actionable guidance for practitioners aiming to balance performance and sustainability in ML workflows, highlighting the value of energy-aware optimizer selection and the hardware-context dependency of such decisions.

Abstract: As machine learning models grow increasingly complex and computationally
demanding, understanding the environmental impact of training decisions becomes
critical for sustainable AI development. This paper presents a comprehensive
empirical study investigating the relationship between optimizer choice and
energy efficiency in neural network training. We conducted 360 controlled
experiments across three benchmark datasets (MNIST, CIFAR-10, CIFAR-100) using
eight popular optimizers (SGD, Adam, AdamW, RMSprop, Adagrad, Adadelta, Adamax,
NAdam) with 15 random seeds each. Using CodeCarbon for precise energy tracking
on Apple M1 Pro hardware, we measured training duration, peak memory usage,
carbon dioxide emissions, and final model performance. Our findings reveal
substantial trade-offs between training speed, accuracy, and environmental
impact that vary across datasets and model complexity. We identify AdamW and
NAdam as consistently efficient choices, while SGD demonstrates superior
performance on complex datasets despite higher emissions. These results provide
actionable insights for practitioners seeking to balance performance and
sustainability in machine learning workflows.

</details>


### [86] [Learning Nonlinear Responses in PET Bottle Buckling with a Hybrid DeepONet-Transolver Framework](https://arxiv.org/abs/2509.13520)
*Varun Kumar,Jing Bi,Cyril Ngo Ngoc,Victor Oancea,George Em Karniadakis*

Main category: cs.LG

TL;DR: A hybrid DeepONet-Transolver surrogate predicts displacement fields and time evolution of reaction forces for PET bottle buckling across varying geometries, trained on FEA data, achieving low relative errors and capturing buckling phenomena for fast design evaluation.


<details>
  <summary>Details</summary>
Motivation: Generalize PDE solutions across non-parametric geometric domains and reduce computational cost of FEA in packaging design, enabling rapid multidisciplinary design exploration.

Method: A hybrid DeepONet-Transolver framework jointly predicts nodal displacement fields and time-dependent reaction forces. Trained on nonlinear FEA data (Abaqus) for 254 designs per each of two geometric families (parameterized by 2 and 4 design variables). Evaluates multi-task predictions across varying geometries.

Result: Mean relative L2 errors for displacement fields: 2.5%–13%. Time-dependent reaction forces: ~2.4% error for the four-parameter family. Point-wise displacement errors ~1e-4 to 1e-3, with largest errors in localized regions. The model robustly captures buckling behavior across diverse geometries.

Conclusion: The framework provides a scalable, computationally efficient surrogate for multi-task predictive modeling in computational mechanics, suitable for rapid design evaluation in packaging design and other applications requiring fast PDE-based predictions.

Abstract: Neural surrogates and operator networks for solving partial differential
equation (PDE) problems have attracted significant research interest in recent
years. However, most existing approaches are limited in their ability to
generalize solutions across varying non-parametric geometric domains. In this
work, we address this challenge in the context of Polyethylene Terephthalate
(PET) bottle buckling analysis, a representative packaging design problem
conventionally solved using computationally expensive finite element analysis
(FEA). We introduce a hybrid DeepONet-Transolver framework that simultaneously
predicts nodal displacement fields and the time evolution of reaction forces
during top load compression. Our methodology is evaluated on two families of
bottle geometries parameterized by two and four design variables. Training data
is generated using nonlinear FEA simulations in Abaqus for 254 unique designs
per family. The proposed framework achieves mean relative $L^{2}$ errors of
2.5-13% for displacement fields and approximately 2.4% for time-dependent
reaction forces for the four-parameter bottle family. Point-wise error analyses
further show absolute displacement errors on the order of $10^{-4}$-$10^{-3}$,
with the largest discrepancies confined to localized geometric regions.
Importantly, the model accurately captures key physical phenomena, such as
buckling behavior, across diverse bottle geometries. These results highlight
the potential of our framework as a scalable and computationally efficient
surrogate, particularly for multi-task predictions in computational mechanics
and applications requiring rapid design evaluation.

</details>


### [87] [AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions](https://arxiv.org/abs/2509.13523)
*Väinö Hatanpää,Eugene Ku,Jason Stock,Murali Emani,Sam Foreman,Chunyong Jung,Sandeep Madireddy,Tung Nguyen,Varuni Sastry,Ray A. O. Sinurat,Sam Wheeler,Huihuo Zheng,Troy Arcomano,Venkatram Vishwanath,Rao Kotamarthi*

Main category: cs.LG

TL;DR: AERIS is a billion-parameter pixel-level Swin diffusion transformer for high-resolution weather/climate forecasting, paired with SWiPe shard-based parallelism to scale window-based transformers. It achieves scalable high-performance on large HPC hardware, outperforms the IFS ENS on ERA5 data, and demonstrates stable seasonal forecasting capability.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based models can mitigate spectral biases and improve ensemble calibration in weather forecasting, but scaling such models to high-resolution, long-lead forecasts remains difficult due to compute, communication, and memory constraints.

Method: Introduce AERIS (1.3–80B parameters) as a pixel-level Swin diffusion transformer. Develop SWiPe, a generalizable technique that combines window parallelism with sequence and pipeline parallelism to shard window-based transformers without extra communication or larger global batch sizes. Evaluate on Aurora (10,080 nodes) using 0.25° ERA5 data with 1×1 patches.

Result: AERIS sustains 10.21 ExaFLOPS (mixed precision) and reaches a peak of 11.21 ExaFLOPS on 0.25° ERA5 data with 1×1 patches, achieving 95.5% weak scaling efficiency and 81.6% strong scaling efficiency. It outperforms the IFS ENS and remains stable on seasonal scales up to 90 days.

Conclusion: Billion-parameter diffusion models like AERIS show promise for weather and climate prediction, combining high-resolution fidelity with scalable training/inference on large HPC systems.

Abstract: Generative machine learning offers new opportunities to better understand
complex Earth system dynamics. Recent diffusion-based methods address spectral
biases and improve ensemble calibration in weather forecasting compared to
deterministic methods, yet have so far proven difficult to scale stably at high
resolutions. We introduce AERIS, a 1.3 to 80B parameter pixel-level Swin
diffusion transformer to address this gap, and SWiPe, a generalizable technique
that composes window parallelism with sequence and pipeline parallelism to
shard window-based transformers without added communication cost or increased
global batch size. On Aurora (10,080 nodes), AERIS sustains 10.21 ExaFLOPS
(mixed precision) and a peak performance of 11.21 ExaFLOPS with $1 \times 1$
patch size on the 0.25{\deg} ERA5 dataset, achieving 95.5% weak scaling
efficiency, and 81.6% strong scaling efficiency. AERIS outperforms the IFS ENS
and remains stable on seasonal scales to 90 days, highlighting the potential of
billion-parameter diffusion models for weather and climate prediction.

</details>


### [88] [Meta-Learning Linear Models for Molecular Property Prediction](https://arxiv.org/abs/2509.13527)
*Yulia Pimonova,Michael G. Taylor,Alice Allen,Ping Yang,Nicholas Lubbers*

Main category: cs.LG

TL;DR: LAMeL introduces a linear meta-learning framework that learns a shared, interpretable functional manifold across related chemical prediction tasks to improve accuracy while retaining interpretability; it yields substantial gains over ridge regression and matches traditional linear models.


<details>
  <summary>Details</summary>
Motivation: Chemists face limited high-quality, concordant datasets and demand models that are both accurate and explainable. There is a need to bridge predictive performance with human interpretability, particularly in data-scarce settings where linear models are favored for transparency.

Method: LAMeL is a Linear Algorithm for Meta-Learning that identifies shared model parameters across related prediction tasks (even when tasks have no overlapping data) to learn a common functional manifold. This manifold serves as a more informed starting point for new unseen tasks, enabling improved predictive performance while preserving linear interpretability.

Result: Across multiple chemical-property datasets, LAMeL achieves performance improvements ranging from 1.1- to 25-fold over standard ridge regression. It consistently outperforms or matches traditional linear methods, indicating reliability for scenarios where accuracy and interpretability are both critical.

Conclusion: LAMeL offers a robust, interpretable meta-learning approach for chemical property prediction, enabling better use of scarce, high-quality data and providing a practical tool that balances accuracy with human-understandable models.

Abstract: Chemists in search of structure-property relationships face great challenges
due to limited high quality, concordant datasets. Machine learning (ML) has
significantly advanced predictive capabilities in chemical sciences, but these
modern data-driven approaches have increased the demand for data. In response
to the growing demand for explainable AI (XAI) and to bridge the gap between
predictive accuracy and human comprehensibility, we introduce LAMeL - a Linear
Algorithm for Meta-Learning that preserves interpretability while improving the
prediction accuracy across multiple properties. While most approaches treat
each chemical prediction task in isolation, LAMeL leverages a meta-learning
framework to identify shared model parameters across related tasks, even if
those tasks do not share data, allowing it to learn a common functional
manifold that serves as a more informed starting point for new unseen tasks.
Our method delivers performance improvements ranging from 1.1- to 25-fold over
standard ridge regression, depending on the domain of the dataset. While the
degree of performance enhancement varies across tasks, LAMeL consistently
outperforms or matches traditional linear methods, making it a reliable tool
for chemical property prediction where both accuracy and interpretability are
critical.

</details>


### [89] [Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection](https://arxiv.org/abs/2509.13608)
*Niruthiha Selvanayagam,Ted Kurti*

Main category: cs.LG

TL;DR: Identifies a unimodal bottleneck in GPT-4o mini where safety filters override multimodal reasoning, causing false positives; highlights safety-capability tension and the need for integrated, context-aware alignment.


<details>
  <summary>Details</summary>
Motivation: To understand safety architectures of open-ended LMMs and how context-blind safety filters interact with multimodal reasoning in hate-speech detection, aiming to improve safe deployment.

Method: Phase-based analysis on 500 samples from the Hateful Memes Challenge; examine model reasoning and failure modes; quantitative validation of 144 policy refusals; assess contributions from visual vs textual unimodal content and their effect on overrides.

Result: Unimodal Bottleneck discovered: safety overrides occur in roughly equal measure from unimodal visual and textual content (about 50/50). Safety system is brittle, blocking not only high-risk imagery but also benign meme formats, causing predictable false positives.

Conclusion: There is a fundamental tension between maximizing model capability and ensuring safety in state-of-the-art LMMs. Requires more integrated, context-aware alignment strategies to preserve multimodal reasoning while maintaining appropriate safety checks.

Abstract: As Large Multimodal Models (LMMs) become integral to daily digital life,
understanding their safety architectures is a critical problem for AI
Alignment. This paper presents a systematic analysis of OpenAI's GPT-4o mini, a
globally deployed model, on the difficult task of multimodal hate speech
detection. Using the Hateful Memes Challenge dataset, we conduct a multi-phase
investigation on 500 samples to probe the model's reasoning and failure modes.
Our central finding is the experimental identification of a "Unimodal
Bottleneck," an architectural flaw where the model's advanced multimodal
reasoning is systematically preempted by context-blind safety filters. A
quantitative validation of 144 content policy refusals reveals that these
overrides are triggered in equal measure by unimodal visual 50% and textual 50%
content. We further demonstrate that this safety system is brittle, blocking
not only high-risk imagery but also benign, common meme formats, leading to
predictable false positives. These findings expose a fundamental tension
between capability and safety in state-of-the-art LMMs, highlighting the need
for more integrated, context-aware alignment strategies to ensure AI systems
can be deployed both safely and effectively.

</details>


### [90] [Unsupervised Anomaly Detection in ALS EPICS Event Logs](https://arxiv.org/abs/2509.13621)
*Antonin Sulc,Thorsten Hellert,Steven Hunt*

Main category: cs.LG

TL;DR: An automated fault analysis framework for ALS uses semantic embeddings and a sequence-aware NN to monitor EPICS logs, producing real-time anomaly scores to flag deviations and identify precursor sequences to failures.


<details>
  <summary>Details</summary>
Motivation: To rapidly detect critical event sequences that lead to complex system failures in a large accelerator facility by treating logs as natural language and learning baseline behavior.

Method: Convert EPICS event logs into contextual vectors via semantic embeddings; train a sequence-aware neural network on normal operation data to output real-time anomaly scores for each event.

Result: Real-time anomaly scoring highlights deviations from baseline, enabling operators to pinpoint precursor sequences to failures.

Conclusion: The framework provides a scalable, language-inspired fault analysis approach for real-time monitoring of accelerator control systems, improving fault diagnosis and response times.

Abstract: This paper introduces an automated fault analysis framework for the Advanced
Light Source (ALS) that processes real-time event logs from its EPICS control
system. By treating log entries as natural language, we transform them into
contextual vector representations using semantic embedding techniques. A
sequence-aware neural network, trained on normal operational data, assigns a
real-time anomaly score to each event. This method flags deviations from
baseline behavior, enabling operators to rapidly identify the critical event
sequences that precede complex system failures.

</details>


### [91] [Privacy-Aware In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.13625)
*Bishnu Bhusal,Manoj Acharya,Ramneet Kaur,Colin Samplawski,Anirban Roy,Adam D. Cobb,Rohit Chadha,Susmit Jha*

Main category: cs.LG

TL;DR: A DP-based private prediction framework for generating high-quality synthetic text without model fine-tuning, using per-token DP inference and aggregation to produce coherent text, plus a blending step to boost utility; claims state-of-the-art performance on ICL tasks under privacy constraints.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in LLMs where prompts can leak sensitive information; provide worst-case information leakage bounds without altering or fine-tuning the base model.

Method: Perform inference on private records to obtain per-token output distributions under differential privacy; aggregate these distributions to generate longer coherent text; introduce a blending operation that combines private and public inference to improve utility without sacrificing privacy guarantees.

Result: Empirically outperforms previous state-of-the-art methods on in-context-learning tasks while maintaining strong privacy guarantees and enabling longer, coherent text generation.

Conclusion: DP-based private inference for text generation is a promising approach for privacy-preserving NLP that maintains high utility without requiring fine-tuning of large models; further work could explore trade-offs, DP parameter tuning, and scalability.

Abstract: Large language models (LLMs) have significantly transformed natural language
understanding and generation, but they raise privacy concerns due to potential
exposure of sensitive information. Studies have highlighted the risk of
information leakage, where adversaries can extract sensitive information
embedded in the prompts. In this work, we introduce a novel private prediction
framework for generating high-quality synthetic text with strong privacy
guarantees. Our approach leverages the Differential Privacy (DP) framework to
ensure worst-case theoretical bounds on information leakage without requiring
any fine-tuning of the underlying models.The proposed method performs inference
on private records and aggregates the resulting per-token output distributions.
This enables the generation of longer and coherent synthetic text while
maintaining privacy guarantees. Additionally, we propose a simple blending
operation that combines private and public inference to further enhance
utility. Empirical evaluations demonstrate that our approach outperforms
previous state-of-the-art methods on in-context-learning (ICL) tasks, making it
a promising direction for privacy-preserving text generation while maintaining
high utility.

</details>


### [92] [DeepLogit: A sequentially constrained explainable deep learning modeling approach for transport policy analysis](https://arxiv.org/abs/2509.13633)
*Jeremy Oon,Rakhi Manohar Mepparambath,Ling Feng*

Main category: cs.LG

TL;DR: Proposes DeepLogit, a sequentially constrained deep learning framework that integrates linear-in-parameter interpretability into deeper models (e.g., Transformers) for transport route-choice; claims improved predictive accuracy over traditional discrete-choice models while preserving interpretability; demonstrated on Singapore transit data; code released.


<details>
  <summary>Details</summary>
Motivation: To reconcile interpretability of theory-based discrete choice models with the predictive power of deep learning in planning/policy contexts.

Method: A two-step sequentially constrained modeling approach: (1) fit a linear-in-parameters CNN equivalent to a multinomial logit; (2) extend to deeper models by constraining their parameters to the interpretable subset from step 1, adding higher-order terms or using architectures like Transformers.

Result: Achieves significantly better predictive accuracy than discrete choice models while maintaining interpretability for key parameters; validated on real-world transit route-choice data from Singapore; code available.

Conclusion: This approach suggests a unifying framework blending DCM theory with data-driven AI, scalable with larger datasets to yield accurate yet policy-relevant models; facilitates applicability in planning and policy analysis.

Abstract: Despite the significant progress of deep learning models in multitude of
applications, their adaption in planning and policy related areas remains
challenging due to the black-box nature of these models. In this work, we
develop a set of DeepLogit models that follow a novel sequentially constrained
approach in estimating deep learning models for transport policy analysis. In
the first step of the proposed approach, we estimate a convolutional neural
network (CNN) model with only linear terms, which is equivalent of a
linear-in-parameter multinomial logit model. We then estimate other deep
learning models by constraining the parameters that need interpretability at
the values obtained in the linear-in-parameter CNN model and including higher
order terms or by introducing advanced deep learning architectures like
Transformers. Our approach can retain the interpretability of the selected
parameters, yet provides significantly improved model accuracy than the
discrete choice model. We demonstrate our approach on a transit route choice
example using real-world transit smart card data from Singapore. This study
shows the potential for a unifying approach, where theory-based discrete choice
model (DCM) and data-driven AI models can leverage each other's strengths in
interpretability and predictive power. With the availability of larger datasets
and more complex constructions, such approach can lead to more accurate models
using discrete choice models while maintaining its applicability in planning
and policy-related areas. Our code is available on
https://github.com/jeremyoon/route-choice/ .

</details>


### [93] [Secure UAV-assisted Federated Learning: A Digital Twin-Driven Approach with Zero-Knowledge Proofs](https://arxiv.org/abs/2509.13634)
*Md Bokhtiar Al Zami,Md Raihan Uddin,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: An UAV-based federated learning framework integrates Digital Twin and Zero-Knowledge Federated Learning to cut energy use and boost security, using dynamic resource allocation and optimization; achieves up to 29.6% energy savings in simulations.


<details>
  <summary>Details</summary>
Motivation: Addresses energy efficiency, communication inefficiency, and security vulnerabilities in UAV-assisted FL; scaling requires real-time monitoring and secure model verification.

Method: UAVs serve as mobile base stations; Digital Twin enables real-time monitoring and predictive maintenance; Zero-Knowledge Federated Learning uses zero-knowledge proofs to verify models without exposing data; dynamic allocation optimizes UAV trajectories, transmission power, and processing rates via block coordinate descent and convex optimization.

Result: Simulations show up to 29.6% reduction in system energy consumption compared with conventional FL; improvements in learning performance, security, and scalability.

Conclusion: Combining Digital Twin and Zero-Knowledge Federated Learning offers a promising, energy-efficient, and secure framework for next-generation UAV-based intelligent networks, balancing energy, latency, and privacy considerations.

Abstract: Federated learning (FL) has gained popularity as a privacy-preserving method
of training machine learning models on decentralized networks. However to
ensure reliable operation of UAV-assisted FL systems, issues like as excessive
energy consumption, communication inefficiencies, and security vulnerabilities
must be solved. This paper proposes an innovative framework that integrates
Digital Twin (DT) technology and Zero-Knowledge Federated Learning (zkFed) to
tackle these challenges. UAVs act as mobile base stations, allowing scattered
devices to train FL models locally and upload model updates for aggregation. By
incorporating DT technology, our approach enables real-time system monitoring
and predictive maintenance, improving UAV network efficiency. Additionally,
Zero-Knowledge Proofs (ZKPs) strengthen security by allowing model verification
without exposing sensitive data. To optimize energy efficiency and resource
management, we introduce a dynamic allocation strategy that adjusts UAV flight
paths, transmission power, and processing rates based on network conditions.
Using block coordinate descent and convex optimization techniques, our method
significantly reduces system energy consumption by up to 29.6% compared to
conventional FL approaches. Simulation results demonstrate improved learning
performance, security, and scalability, positioning this framework as a
promising solution for next-generation UAV-based intelligent networks.

</details>


### [94] [Multimodal signal fusion for stress detection using deep neural networks: a novel approach for converting 1D signals to unified 2D images](https://arxiv.org/abs/2509.13636)
*Yasin Hasanpoor,Bahram Tarvirdizadeh,Khalil Alipour,Mohammad Ghamari*

Main category: cs.LG

TL;DR: A cross-signal fusion method converts multimodal physiological signals (PPG, GSR, ACC) into 2D images for CNN-based stress detection, using multi-format fusion and multi-stage training to improve interpretability, data augmentation, generalization, and robustness; broadly applicable to wearable health monitoring.


<details>
  <summary>Details</summary>
Motivation: Traditional multimodal processing either treats signals separately or uses fixed encodings, which limits capturing temporal and cross-signal dependencies and hinders generalization and interpretability in stress detection.

Method: Transform PPG, GSR, and ACC into 2D image matrices and fuse them into structured image representations. Use CNNs to capture temporal and cross-signal dependencies. Employ image-based transformations as data augmentation and reorganize the fused signals into multiple formats in a multi-stage training pipeline to boost generalization and robustness.

Result: The approach significantly boosts classification performance for stress detection, with improved generalization and robustness across formats, and is presented as broadly applicable to multimodal physiological signals.

Conclusion: This image-based multimodal fusion framework offers a broadly applicable, more accurate, real-time wearable health monitoring solution, extending beyond stress detection to other domains that rely on multimodal physiological data.

Abstract: This study introduces a novel method that transforms multimodal physiological
signalsphotoplethysmography (PPG), galvanic skin response (GSR), and
acceleration (ACC) into 2D image matrices to enhance stress detection using
convolutional neural networks (CNNs). Unlike traditional approaches that
process these signals separately or rely on fixed encodings, our technique
fuses them into structured image representations that enable CNNs to capture
temporal and cross signal dependencies more effectively. This image based
transformation not only improves interpretability but also serves as a robust
form of data augmentation. To further enhance generalization and model
robustness, we systematically reorganize the fused signals into multiple
formats, combining them in a multi stage training pipeline. This approach
significantly boosts classification performance. While demonstrated here in the
context of stress detection, the proposed method is broadly applicable to any
domain involving multimodal physiological signals, paving the way for more
accurate, personalized, and real time health monitoring through wearable
technologies.

</details>


### [95] [LLM-I: LLMs are Naturally Interleaved Multimodal Creators](https://arxiv.org/abs/2509.13642)
*Zirun Guo,Feng Zhang,Kai Jia,Tao Jin*

Main category: cs.LG

TL;DR: LLM-I is a dynamic, tool-use framework that lets a central LLM orchestrate multiple specialized visual tools for interleaved image-text tasks, achieving state-of-the-art results by overcoming the single-tool limitation of unified models.


<details>
  <summary>Details</summary>
Motivation: Current unified models are constrained to a single or limited set of tools, often relying on synthetic imagery and lacking robust factual grounding or programmatic precision. A flexible, multi-tool approach is needed to handle diverse tasks, grounding, and precise editing.

Method: A central LLM/MLLM agent selects and executes a toolkit of specialized visual tools, including online image search, diffusion-based generation, code execution, and image editing. Training uses reinforcement learning with a hybrid reward system combining rule-based logic and judgments from LLM/MLLM evaluators. The model is trained on a diverse dataset using four backbones, and a novel test-time scaling strategy is proposed.

Result: The framework achieves state-of-the-art performance, outperforming existing methods by a large margin across four benchmarks. The dataset and multiple backbones enable broad generalization and robustness; test-time scaling provides additional gains.

Conclusion: LLM-I demonstrates the effectiveness of a flexible, multi-tool orchestration paradigm for interleaved image-text generation, addressing grounding and precision limitations of single-tool models and offering scalable gains through both RL training and test-time scaling.

Abstract: We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that
reframes interleaved image-text generation as a tool-use problem. LLM-I is
designed to overcome the "one-tool" bottleneck of current unified models, which
are limited to synthetic imagery and struggle with tasks requiring factual
grounding or programmatic precision. Our framework empowers a central LLM or
MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual
tools, including online image search, diffusion-based generation, code
execution, and image editing. The agent is trained to select and apply these
tools proficiently via a Reinforcement Learning (RL) framework that features a
hybrid reward system combining rule-based logic with judgments from LLM and
MLLM evaluators. Trained on a diverse new dataset using four different model
backbones, LLM-I demonstrates state-of-the-art performance, outperforming
existing methods by a large margin across four benchmarks. We also introduce a
novel test-time scaling strategy that provides further performance gains.
Project Page: https://github.com/ByteDance-BandAI/LLM-I.

</details>


### [96] [Sequential Data Augmentation for Generative Recommendation](https://arxiv.org/abs/2509.13648)
*Geon Lee,Bhuvesh Kumar,Clark Mingxuan Ju,Tong Zhao,Kijung Shin,Neil Shah,Liam Collins*

Main category: cs.LG

TL;DR: GenPAS is a principled, generalized framework for data augmentation in generative recommendation, treating augmentation as stochastic sampling over input-target pairs with three bias-controlled steps, unifying existing strategies, and improving accuracy, data efficiency, and parameter efficiency.


<details>
  <summary>Details</summary>
Motivation: Data augmentation strongly shapes training distribution and model generalization in generative recommender systems, yet prior work treats it heuristically or as a minor design choice; there are large performance disparities across augmentation strategies; a systematic framework is needed to understand and control augmentation effects and align training with future targets.

Method: Propose GenPAS, modeling augmentation as stochastic sampling over input-target pairs with three bias-controlled steps: sequence sampling, target sampling, input sampling; show how common strategies are special cases; provide flexible controls for training distribution; empirical validation on benchmark and industrial datasets.

Result: GenPAS yields superior accuracy, data efficiency, and parameter efficiency relative to existing augmentation strategies, across datasets; demonstrates practical benefit and guidance.

Conclusion: A principled, unified framework for training data construction in generative recommendation; enables principled design of augmentation strategies, improves generalization, and offers practical guidance for practitioners.

Abstract: Generative recommendation plays a crucial role in personalized systems,
predicting users' future interactions from their historical behavior sequences.
A critical yet underexplored factor in training these models is data
augmentation, the process of constructing training data from user interaction
histories. By shaping the training distribution, data augmentation directly and
often substantially affects model generalization and performance. Nevertheless,
in much of the existing work, this process is simplified, applied
inconsistently, or treated as a minor design choice, without a systematic and
principled understanding of its effects.
  Motivated by our empirical finding that different augmentation strategies can
yield large performance disparities, we conduct an in-depth analysis of how
they reshape training distributions and influence alignment with future targets
and generalization to unseen inputs. To systematize this design space, we
propose GenPAS, a generalized and principled framework that models augmentation
as a stochastic sampling process over input-target pairs with three
bias-controlled steps: sequence sampling, target sampling, and input sampling.
This formulation unifies widely used strategies as special cases and enables
flexible control of the resulting training distribution. Our extensive
experiments on benchmark and industrial datasets demonstrate that GenPAS yields
superior accuracy, data efficiency, and parameter efficiency compared to
existing strategies, providing practical guidance for principled training data
construction in generative recommendation.

</details>


### [97] [Controllable Pareto Trade-off between Fairness and Accuracy](https://arxiv.org/abs/2509.13651)
*Yongkang Du,Jieyu Zhao,Yijun Yang,Tianyi Zhou*

Main category: cs.LG

TL;DR: A method (CPT) for controllable fairness-accuracy trade-offs in NLP via multi-objective optimization. It stabilizes updates with a moving-average of gradients and prunes gradients to focus on critical parameters, enabling adherence to user-defined reference vectors on the Pareto front.


<details>
  <summary>Details</summary>
Motivation: Fairness-accuracy trade-offs are central in NLP; existing work seeks a single optimal point, but users may want different trade-offs. The Pareto front offers many solutions; controlling trade-offs precisely is challenging due to stochastic training and high-dimensional gradients.

Method: Introduce Controllable Pareto Trade-off (CPT). It stabilizes updates by using a moving-average of stochastic gradients to determine the update direction and prunes gradients to retain only those of critical parameters, guiding training toward user-specified Pareto directions via reference vectors.

Result: Empirical evaluation on hate speech detection and occupation classification shows CPT achieves a higher-quality set of Pareto-front solutions than baselines, with improved controllability and ability to follow human-defined reference vectors.

Conclusion: CPT provides a practical mechanism to obtain controllable, high-quality trade-offs between fairness and accuracy in NLP tasks by stabilizing updates and focusing on key parameters, enabling precise follow-through to user preferences.

Abstract: The fairness-accuracy trade-off is a key challenge in NLP tasks. Current work
focuses on finding a single "optimal" solution to balance the two objectives,
which is limited considering the diverse solutions on the Pareto front. This
work intends to provide controllable trade-offs according to the user's
preference of the two objectives, which is defined as a reference vector. To
achieve this goal, we apply multi-objective optimization (MOO), which can find
solutions from various regions of the Pareto front. However, it is challenging
to precisely control the trade-off due to the stochasticity of the training
process and the high dimentional gradient vectors. Thus, we propose
Controllable Pareto Trade-off (CPT) that can effectively train models to
perform different trade-offs according to users' preferences. CPT 1) stabilizes
the fairness update with a moving average of stochastic gradients to determine
the update direction, and 2) prunes the gradients by only keeping the gradients
of the critical parameters. We evaluate CPT on hate speech detection and
occupation classification tasks. Experiments show that CPT can achieve a
higher-quality set of solutions on the Pareto front than the baseline methods.
It also exhibits better controllability and can precisely follow the
human-defined reference vectors.

</details>


### [98] [RF-LSCM: Pushing Radiance Fields to Multi-Domain Localized Statistical Channel Modeling for Cellular Network Optimization](https://arxiv.org/abs/2509.13686)
*Bingsheng Peng,Shutao Zhang,Xi Zheng,Ye Xue,Xinyu Qin,Tsung-Hui Chang*

Main category: cs.LG

TL;DR: RF-LSCM introduces a physics-informed radiance-field, multi-domain LSCM for cross-frequency, multi-cell channel modeling, achieving substantial MAE gains in coverage prediction and multi-frequency fusion.


<details>
  <summary>Details</summary>
Motivation: Localized statistical channel modeling for cellular optimization is limited by traditional single-cell, single-grid, single-carrier analyses, hindering cross-domain generalization and multi-cell collaboration.

Method: Propose RF-LSCM that represents the channel angular power spectrum (APS) via a radiance-field formulation. Key components include a frequency-dependent Attenuation Model (FDAM) for cross-frequency generalization, a point-cloud-aided environment representation for multi-cell/multi-grid modeling, a low-rank tensor representation to reduce computation, and the Hierarchical Tensor Angular Modeling (HiTAM) algorithm for efficient angular modeling.

Result: Empirical evaluation on real-world multi-cell datasets shows RF-LSCM outperforms state-of-the-art methods, delivering up to 30% reduction in MAE for coverage prediction and 22% MAE improvement when fusing data across multiple frequencies.

Conclusion: RF-LSCM delivers accurate, scalable, cross-frequency and multi-cell channel modeling by integrating physics-informed attenuation, radiance-field representation, and efficient tensor-based angular modeling, enabling improved network optimization.

Abstract: Accurate localized wireless channel modeling is a cornerstone of cellular
network optimization, enabling reliable prediction of network performance
during parameter tuning. Localized statistical channel modeling (LSCM) is the
state-of-the-art channel modeling framework tailored for cellular network
optimization. However, traditional LSCM methods, which infer the channel's
Angular Power Spectrum (APS) from Reference Signal Received Power (RSRP)
measurements, suffer from critical limitations: they are typically confined to
single-cell, single-grid and single-carrier frequency analysis and fail to
capture complex cross-domain interactions. To overcome these challenges, we
propose RF-LSCM, a novel framework that models the channel APS by jointly
representing large-scale signal attenuation and multipath components within a
radiance field. RF-LSCM introduces a multi-domain LSCM formulation with a
physics-informed frequency-dependent Attenuation Model (FDAM) to facilitate the
cross frequency generalization as well as a point-cloud-aided environment
enhanced method to enable multi-cell and multi-grid channel modeling.
Furthermore, to address the computational inefficiency of typical neural
radiance fields, RF-LSCM leverages a low-rank tensor representation,
complemented by a novel Hierarchical Tensor Angular Modeling (HiTAM) algorithm.
This efficient design significantly reduces GPU memory requirements and
training time while preserving fine-grained accuracy. Extensive experiments on
real-world multi-cell datasets demonstrate that RF-LSCM significantly
outperforms state-of-the-art methods, achieving up to a 30% reduction in mean
absolute error (MAE) for coverage prediction and a 22% MAE improvement by
effectively fusing multi-frequency data.

</details>


### [99] [A Conformal Prediction Framework for Uncertainty Quantification in Physics-Informed Neural Networks](https://arxiv.org/abs/2509.13717)
*Yifan Yu,Cheuk Hin Ho,Yangshuai Wang*

Main category: cs.LG

TL;DR: Presents a distribution-free conformal prediction (CP) framework for uncertainty quantification in Physics-Informed Neural Networks (PINNs) with finite-sample guarantees and local (spatially adaptive) intervals.


<details>
  <summary>Details</summary>
Motivation: PINNs lack rigorous UQ with guarantees; standard UQ methods are heuristic. Conformal prediction offers distribution-free, finite-sample coverage. PDE problems often show spatially varying uncertainty (heteroskedasticity) that needs adaptive intervals.

Method: Use nonconformity scores computed on a calibration set to produce prediction intervals for PINN outputs, achieving distribution-free UQ with finite-sample coverage. Introduce local conformal quantile estimation to adapt intervals to spatial location, addressing spatial heteroskedasticity.

Result: Prediction intervals are reliably calibrated and locally adaptive across PDEs. Finite-sample coverage guarantees hold, and the approach consistently outperforms heuristic UQ methods in various uncertainty metrics across benchmark PDEs.

Conclusion: Bridges PINNs with distribution-free UQ via conformal prediction, delivering calibrated, reliable uncertainty estimates for complex PDE systems and enabling uncertainty-aware modeling; offers avenues for further improvements and extensions.

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving PDEs, yet existing uncertainty quantification (UQ) approaches for
PINNs generally lack rigorous statistical guarantees. In this work, we bridge
this gap by introducing a distribution-free conformal prediction (CP) framework
for UQ in PINNs. This framework calibrates prediction intervals by constructing
nonconformity scores on a calibration set, thereby yielding distribution-free
uncertainty estimates with rigorous finite-sample coverage guarantees for
PINNs. To handle spatial heteroskedasticity, we further introduce local
conformal quantile estimation, enabling spatially adaptive uncertainty bands
while preserving theoretical guarantee. Through systematic evaluations on
typical PDEs (damped harmonic oscillator, Poisson, Allen-Cahn, and Helmholtz
equations) and comprehensive testing across multiple uncertainty metrics, our
results demonstrate that the proposed framework achieves reliable calibration
and locally adaptive uncertainty intervals, consistently outperforming
heuristic UQ approaches. By bridging PINNs with distribution-free UQ, this work
introduces a general framework that not only enhances calibration and
reliability, but also opens new avenues for uncertainty-aware modeling of
complex PDE systems.

</details>


### [100] [WatchAnxiety: A Transfer Learning Approach for State Anxiety Prediction from Smartwatch Data](https://arxiv.org/abs/2509.13725)
*Md Sabbir Ahmed,Noah French,Mark Rucker,Zhiyuan Wang,Taylor Myers-Brower,Kaitlyn Petz,Mehdi Boukhechba,Bethany A. Teachman,Laura E. Barnes*

Main category: cs.LG

TL;DR: Cross-domain transfer learning for state social anxiety using smartwatch HR-derived representations achieves ~60% balanced accuracy in internal data and generalizes to a hold-out dataset.


<details>
  <summary>Details</summary>
Motivation: Few studies model intra-day fluctuations of social anxiety; real-time detection enables Just-In-Time Adaptive Interventions; leveraging large external HR data to improve predictions in smaller anxiety-focused samples.

Method: Pretrained base model on 10,000+ days of external heart-rate data; transferred representations to socially anxious students' EMAs; fine-tuned for probabilistic predictions; combined with trait-level measures in a meta-learner; evaluated on internal dataset (N=91; 7 EMAs/day, ~9 days) and hold-out TILES-18 dataset (10,095 EMAs).

Result: Internal balanced accuracy 60.4%; hold-out 59.1%; outperforming prior work by ≥7% on hold-out.

Conclusion: Demonstrates effective cross-domain representation transfer and a meta-learning fusion approach for detecting state anxiety in real-world, mobile settings; supports development of real-time, personalized interventions; generalizable across datasets, though limited by sample size and EMAs.

Abstract: Social anxiety is a common mental health condition linked to significant
challenges in academic, social, and occupational functioning. A core feature is
elevated momentary (state) anxiety in social situations, yet little prior work
has measured or predicted fluctuations in this anxiety throughout the day.
Capturing these intra-day dynamics is critical for designing real-time,
personalized interventions such as Just-In-Time Adaptive Interventions
(JITAIs). To address this gap, we conducted a study with socially anxious
college students (N=91; 72 after exclusions) using our custom smartwatch-based
system over an average of 9.03 days (SD = 2.95). Participants received seven
ecological momentary assessments (EMAs) per day to report state anxiety. We
developed a base model on over 10,000 days of external heart rate data,
transferred its representations to our dataset, and fine-tuned it to generate
probabilistic predictions. These were combined with trait-level measures in a
meta-learner. Our pipeline achieved 60.4% balanced accuracy in state anxiety
detection in our dataset. To evaluate generalizability, we applied the training
approach to a separate hold-out set from the TILES-18 dataset-the same dataset
used for pretraining. On 10,095 once-daily EMAs, our method achieved 59.1%
balanced accuracy, outperforming prior work by at least 7%.

</details>


### [101] [State Space Models over Directed Graphs](https://arxiv.org/abs/2509.13735)
*Junzhi She,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: Introduces DirGraphSSM, a directed-graph extension of state-space models that sequentializes graphs via k-hop ego graphs (DirEgo2Token), achieving state-of-the-art accuracy and faster training on directed graph tasks.


<details>
  <summary>Details</summary>
Motivation: Directed graphs require capturing long-range causal dependencies; existing GNNs/Transformers and undirected SSMs fall short; there is a need for efficient, scalable directed-graph models that leverage SSMs.

Method: Develop DirEgo2Token to convert directed graphs into token-like sequences using k-hop ego subgraphs; implement DirGraphSSM as a message-passing GNN that applies state-space models on directed graphs to propagate information along causal edges; evaluate on multiple directed-graph benchmarks.

Result: Achieves state-of-the-art results on three directed graph tasks; competitive on two additional tasks; training speed 1.5x to 2x faster than current SOTA methods.

Conclusion: Extending state-space models to directed graphs via ego-graph sequentialization yields strong accuracy and efficiency gains, establishing a new direction for directed graph learning.

Abstract: Directed graphs are ubiquitous across numerous domains, where the
directionality of edges encodes critical causal dependencies. However, existing
GNNs and graph Transformers tailored for directed graphs face two major
challenges: (1) effectively capturing long-range causal dependencies derived
from directed edges; (2) balancing accuracy and training efficiency when
processing large-scale graph datasets. In recent years, state space models
(SSMs) have achieved substantial progress in causal sequence tasks, and their
variants designed for graphs have demonstrated state-of-the-art accuracy while
maintaining high efficiency across various graph learning benchmarks. However,
existing graph state space models are exclusively designed for undirected
graphs, which limits their performance in directed graph learning. To this end,
we propose an innovative approach DirEgo2Token which sequentializes directed
graphs via k-hop ego graphs. This marks the first systematic extension of state
space models to the field of directed graph learning. Building upon this, we
develop DirGraphSSM, a novel directed graph neural network architecture that
implements state space models on directed graphs via the message-passing
mechanism. Experimental results demonstrate that DirGraphSSM achieves
state-of-the-art performance on three representative directed graph learning
tasks while attaining competitive performance on two additional tasks with
1.5$\times $ to 2$\times $ training speed improvements compared to existing
state-of-the-art models.

</details>


### [102] [ParaAegis: Parallel Protection for Flexible Privacy-preserved Federated Learning](https://arxiv.org/abs/2509.13739)
*Zihou Wu,Yuecheng Li,Tianchi Liao,Jian Lou,Chuan Chen*

Main category: cs.LG

TL;DR: ParaAegis introduces a flexible parallel protection framework for federated learning that blends differential privacy on a low-norm submodel with homomorphic encryption on the rest, using distributed voting to adaptively balance privacy, utility, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Federated learning protection methods like differential privacy (DP) and homomorphic encryption (HE) enforce a rigid trade-off between model utility and computational efficiency, hindering practical deployment. A flexible, tunable protection scheme is needed.

Method: Partition the model into two parts: apply lightweight DP to the less critical, low-norm segment and protect the remaining portion with HE. Use a distributed voting mechanism to reach consensus on the partitioning. Provide theoretical analysis of the trade-offs under fixed privacy and demonstrate tunability via hyperparameters in experiments.

Result: The framework enables adjustable prioritization between accuracy and training time without changing the overall privacy guarantee. Theoretical results formalize the privacy-utility-efficiency trade-off; experiments show that by tuning hyperparameters, practitioners can trade off accuracy for speed.

Conclusion: ParaAegis offers a flexible, practical approach to FL protection that decouples privacy from a fixed accuracy/efficiency choice, backed by theory and experiments, making the privacy-utility-efficiency balance adjustable to application needs.

Abstract: Federated learning (FL) faces a critical dilemma: existing protection
mechanisms like differential privacy (DP) and homomorphic encryption (HE)
enforce a rigid trade-off, forcing a choice between model utility and
computational efficiency. This lack of flexibility hinders the practical
implementation. To address this, we introduce ParaAegis, a parallel protection
framework designed to give practitioners flexible control over the
privacy-utility-efficiency balance. Our core innovation is a strategic model
partitioning scheme. By applying lightweight DP to the less critical, low norm
portion of the model while protecting the remainder with HE, we create a
tunable system. A distributed voting mechanism ensures consensus on this
partitioning. Theoretical analysis confirms the adjustments between efficiency
and utility with the same privacy. Crucially, the experimental results
demonstrate that by adjusting the hyperparameters, our method enables flexible
prioritization between model accuracy and training time.

</details>


### [103] [ST-LINK: Spatially-Aware Large Language Models for Spatio-Temporal Forecasting](https://arxiv.org/abs/2509.13753)
*Hyotaek Jeon,Hyunwook Lee,Juwon Kim,Sungahn Ko*

Main category: cs.LG

TL;DR: ST-LINK is a framework that enhances LLMs for spatio-temporal traffic forecasting by introducing Spatially-Enhanced Attention (SE-Attention) and a Memory Retrieval Feed-Forward Network (MRFFN), achieving superior performance over traditional DL/LLMs and capturing both regular patterns and abrupt changes.


<details>
  <summary>Details</summary>
Motivation: LLMs excel at sequential token processing but struggle with spatial dependencies and graph-structured data common in traffic forecasting; there is a need to adapt LLMs to capture spatio-temporal relations and graph-like spatial correlations.

Method: SE-Attention extends rotary position embeddings to encode spatial correlations as direct rotational transformations within the attention mechanism, integrating spatial signals into the LLM's core computation. MRFFN dynamically retrieves and leverages key historical patterns to model complex temporal dependencies and stabilize long-term forecasting.

Result: Empirical results on benchmark datasets show ST-LINK surpasses conventional deep learning and LLM approaches, effectively capturing both regular traffic patterns and abrupt changes.

Conclusion: ST-LINK demonstrates that LLMs can be effectively adapted for spatio-temporal forecasting by coupling spatially-aware attention with memory-based temporal dynamics, offering improved performance and robustness.

Abstract: Traffic forecasting represents a crucial problem within intelligent
transportation systems. In recent research, Large Language Models (LLMs) have
emerged as a promising method, but their intrinsic design, tailored primarily
for sequential token processing, introduces notable challenges in effectively
capturing spatial dependencies. Specifically, the inherent limitations of LLMs
in modeling spatial relationships and their architectural incompatibility with
graph-structured spatial data remain largely unaddressed. To overcome these
limitations, we introduce ST-LINK, a novel framework that enhances the
capability of Large Language Models to capture spatio-temporal dependencies.
Its key components are Spatially-Enhanced Attention (SE-Attention) and the
Memory Retrieval Feed-Forward Network (MRFFN). SE-Attention extends rotary
position embeddings to integrate spatial correlations as direct rotational
transformations within the attention mechanism. This approach maximizes spatial
learning while preserving the LLM's inherent sequential processing structure.
Meanwhile, MRFFN dynamically retrieves and utilizes key historical patterns to
capture complex temporal dependencies and improve the stability of long-term
forecasting. Comprehensive experiments on benchmark datasets demonstrate that
ST-LINK surpasses conventional deep learning and LLM approaches, and
effectively captures both regular traffic patterns and abrupt changes.

</details>


### [104] [Beyond Correlation: Causal Multi-View Unsupervised Feature Selection Learning](https://arxiv.org/abs/2509.13763)
*Zongxin Shen,Yanyong Huang,Bin Wang,Jinyuan Chang,Shiyu Liu,Tianrui Li*

Main category: cs.LG

TL;DR: Introduces CAUSA, a causally motivated MUFS method that mitigates spurious correlations by separating confounders and learning view-shared sample weights, improving unsupervised feature selection across multiple views.


<details>
  <summary>Details</summary>
Motivation: MUFS often relies on correlations with clustering labels, which can be confounded by hidden variables. A causal perspective can help distinguish truly informative features from spurious ones caused by confounders in multi-view, unlabeled data.

Method: Proposes CAUSA: (1) a generalized unsupervised spectral regression to identify informative features via dependencies between features and consensus clustering; (2) a causal regularization module that separates confounders and learns view-shared sample weights to balance confounder distributions; (3) integrates both components into a unified learning framework to select causally informative features across views.

Result: Empirical evaluations show CAUSA outperforms several state-of-the-art MUFS methods on multiple datasets, indicating effectiveness of causal regularization in reducing spurious correlations in unsupervised multi-view feature selection.

Conclusion: CAUSA is the first study to incorporate causal reasoning into multi-view unsupervised feature selection, providing a principled framework to mitigate confounding and improve the quality of selected features.

Abstract: Multi-view unsupervised feature selection (MUFS) has recently received
increasing attention for its promising ability in dimensionality reduction on
multi-view unlabeled data. Existing MUFS methods typically select
discriminative features by capturing correlations between features and
clustering labels. However, an important yet underexplored question remains:
\textit{Are such correlations sufficiently reliable to guide feature
selection?} In this paper, we analyze MUFS from a causal perspective by
introducing a novel structural causal model, which reveals that existing
methods may select irrelevant features because they overlook spurious
correlations caused by confounders. Building on this causal perspective, we
propose a novel MUFS method called CAusal multi-view Unsupervised feature
Selection leArning (CAUSA). Specifically, we first employ a generalized
unsupervised spectral regression model that identifies informative features by
capturing dependencies between features and consensus clustering labels. We
then introduce a causal regularization module that can adaptively separate
confounders from multi-view data and simultaneously learn view-shared sample
weights to balance confounder distributions, thereby mitigating spurious
correlations. Thereafter, integrating both into a unified learning framework
enables CAUSA to select causally informative features. Comprehensive
experiments demonstrate that CAUSA outperforms several state-of-the-art
methods. To our knowledge, this is the first in-depth study of causal
multi-view feature selection in the unsupervised setting.

</details>


### [105] [Floating-Body Hydrodynamic Neural Networks](https://arxiv.org/abs/2509.13783)
*Tianshuo Zhang,Wenzhe Zhai,Rui Yann,Jia Gao,He Cao,Xianglei Xing*

Main category: cs.LG

TL;DR: A physics-structured Floating-Body Hydrodynamic Neural Network (FHNN) model for interpretable, stable floating-body dynamics that outperforms black-box models and preserves interpretability.


<details>
  <summary>Details</summary>
Motivation: Dissipative fluid-structure interaction (FSI) with added mass, drag, and background flows is hard to model; black-box neural models lack interpretability and stable long-horizon predictions.

Method: FHNN predicts interpretable hydrodynamic parameters (directional added masses, drag coefficients) and a streamfunction-based flow, all coupled to analytic equations of motion. The framework constrains the hypothesis space to improve interpretability and stability.

Result: On synthetic vortex datasets, FHNN achieves up to an order-of-magnitude lower error than Neural ODEs and recovers physically consistent flow fields.

Conclusion: FHNN bridges the gap between black-box learning and transparent system identification by better handling dissipative dynamics while preserving interpretability.

Abstract: Fluid-structure interaction is common in engineering and natural systems,
where floating-body motion is governed by added mass, drag, and background
flows. Modeling these dissipative dynamics is difficult: black-box neural
models regress state derivatives with limited interpretability and unstable
long-horizon predictions. We propose Floating-Body Hydrodynamic Neural Networks
(FHNN), a physics-structured framework that predicts interpretable hydrodynamic
parameters such as directional added masses, drag coefficients, and a
streamfunction-based flow, and couples them with analytic equations of motion.
This design constrains the hypothesis space, enhances interpretability, and
stabilizes integration. On synthetic vortex datasets, FHNN achieves up to an
order-of-magnitude lower error than Neural ODEs, recovers physically consistent
flow fields. Compared with Hamiltonian and Lagrangian neural networks, FHNN
more effectively handles dissipative dynamics while preserving
interpretability, which bridges the gap between black-box learning and
transparent system identification.

</details>


### [106] [Towards a Physics Foundation Model](https://arxiv.org/abs/2509.13805)
*Florian Wiesner,Matthias Wessling,Stephen Baek*

Main category: cs.LG

TL;DR: GPhyT is a transformer-based physics foundation model trained on 1.8 TB of simulation data that can simulate diverse physics domains, generalize to unseen systems in-context, and yield stable long-term predictions, outperforming specialized models up to 29x.


<details>
  <summary>Details</summary>
Motivation: To democratize access to high-fidelity physics simulations and accelerate discovery, addressing limitations of current physics-aware ML that are domain-limited and require retraining for new systems.

Method: Train a Transformer (GPhyT) on 1.8 TB of diverse simulation data to learn governing dynamics from context; leverage in-context learning for zero-shot generalization to unseen physical systems; evaluate stability with 50-timestep rollouts across multiple physics domains (fluid-solid interactions, shock waves, thermal convection, multi-phase dynamics).

Result: Achieves superior performance across multiple physics domains, outperforming specialized architectures by up to 29x; demonstrates zero-shot generalization to entirely unseen physical systems via in-context learning; provides stable long-term predictions through 50-timestep rollouts.

Conclusion: A single model can learn generalizable physical principles from data alone, indicating the feasibility of a universal Physics Foundation Model (PFM) that could transform computational science and engineering.

Abstract: Foundation models have revolutionized natural language processing through a
``train once, deploy anywhere'' paradigm, where a single pre-trained model
adapts to countless downstream tasks without retraining. Access to a Physics
Foundation Model (PFM) would be transformative -- democratizing access to
high-fidelity simulations, accelerating scientific discovery, and eliminating
the need for specialized solver development. Yet current physics-aware machine
learning approaches remain fundamentally limited to single, narrow domains and
require retraining for each new system. We present the General Physics
Transformer (GPhyT), trained on 1.8 TB of diverse simulation data, that
demonstrates foundation model capabilities are achievable for physics. Our key
insight is that transformers can learn to infer governing dynamics from
context, enabling a single model to simulate fluid-solid interactions, shock
waves, thermal convection, and multi-phase dynamics without being told the
underlying equations. GPhyT achieves three critical breakthroughs: (1) superior
performance across multiple physics domains, outperforming specialized
architectures by up to 29x, (2) zero-shot generalization to entirely unseen
physical systems through in-context learning, and (3) stable long-term
predictions through 50-timestep rollouts. By establishing that a single model
can learn generalizable physical principles from data alone, this work opens
the path toward a universal PFM that could transform computational science and
engineering.

</details>


### [107] [Hybrid Quantum-Classical Neural Networks for Few-Shot Credit Risk Assessment](https://arxiv.org/abs/2509.13818)
*Zheng-an Wang,Yanbo J. Wang,Jiachi Zhang,Qi Xu,Yilun Zhao,Jintao Li,Yipeng Zhang,Bo Yang,Xinkai Gao,Xiaofeng Cao,Kai Xu,Pengpeng Hao,Xuan Yang,Heng Fan*

Main category: cs.LG

TL;DR: Hybrid quantum-classical workflow for few-shot credit risk with a QNN classifier that outperforms classical baselines on small, imbalanced dataset.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity and class imbalance in credit risk by leveraging QML in the NISQ era, enabling better risk assessment with limited data.

Method: Feature engineering via an ensemble of classical models for dimensionality reduction, then a Quantum Neural Network trained with the parameter-shift rule; evaluated via simulations and on Quafu hardware; dataset with 279 samples; performance metrics AUC and recall.

Result: QNN achieves 0.852 +/- 0.027 AUC in simulations and 0.88 AUC on hardware; surpasses classical benchmarks, notably in recall; demonstrates practical viability of quantum-classical hybrid approach for data-constrained finance.

Conclusion: Provides a pragmatic blueprint for applying quantum computing to data-constrained financial problems in the NISQ era and evidence supporting potential in inclusive finance.

Abstract: Quantum Machine Learning (QML) offers a new paradigm for addressing complex
financial problems intractable for classical methods. This work specifically
tackles the challenge of few-shot credit risk assessment, a critical issue in
inclusive finance where data scarcity and imbalance limit the effectiveness of
conventional models. To address this, we design and implement a novel hybrid
quantum-classical workflow. The methodology first employs an ensemble of
classical machine learning models (Logistic Regression, Random Forest, XGBoost)
for intelligent feature engineering and dimensionality reduction. Subsequently,
a Quantum Neural Network (QNN), trained via the parameter-shift rule, serves as
the core classifier. This framework was evaluated through numerical simulations
and deployed on the Quafu Quantum Cloud Platform's ScQ-P21 superconducting
processor. On a real-world credit dataset of 279 samples, our QNN achieved a
robust average AUC of 0.852 +/- 0.027 in simulations and yielded an impressive
AUC of 0.88 in the hardware experiment. This performance surpasses a suite of
classical benchmarks, with a particularly strong result on the recall metric.
This study provides a pragmatic blueprint for applying quantum computing to
data-constrained financial scenarios in the NISQ era and offers valuable
empirical evidence supporting its potential in high-stakes applications like
inclusive finance.

</details>


### [108] [An End-to-End Differentiable, Graph Neural Network-Embedded Pore Network Model for Permeability Prediction](https://arxiv.org/abs/2509.13841)
*Qingqi Zhao,Heng Xiao*

Main category: cs.LG

TL;DR: An end-to-end differentiable hybrid GNN-PNM framework learns pore-throat conductances with a GNN and solves permeability via a PNM, trained end-to-end using a single permeability target, achieving cross-scale accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of pure data-driven models (poor generalization, lack of physics constraints) and traditional PNM approaches (reliance on idealized geometries). A hybrid approach aims to retain physics-based flow calculations while leveraging data-driven conductance predictions to handle complex pore structures and multi-scale variability.

Method: Replace PNM conductance calculations with a Graph Neural Network that predicts pore/ throat conductances from features. Integrate these predictions into a PNM solver to compute permeability. Train end-to-end by backpropagating through the GNN (automatic differentiation) and the PNM solver (via a discrete adjoint method) using a single scalar permeability as the training target; no labeled conductance data required.

Result: The approach achieves high accuracy and generalizes well across scales, outperforming both pure data-driven models and traditional PNMs. Gradient-based sensitivity analyses reveal physically consistent feature influences, enhancing interpretability.

Conclusion: The framework provides a scalable, physically informed pathway for permeability prediction in complex porous media, reducing model uncertainty and improving accuracy by uniting learning with physics-based simulation.

Abstract: Accurate prediction of permeability in porous media is essential for modeling
subsurface flow. While pure data-driven models offer computational efficiency,
they often lack generalization across scales and do not incorporate explicit
physical constraints. Pore network models (PNMs), on the other hand, are
physics-based and efficient but rely on idealized geometric assumptions to
estimate pore-scale hydraulic conductance, limiting their accuracy in complex
structures. To overcome these limitations, we present an end-to-end
differentiable hybrid framework that embeds a graph neural network (GNN) into a
PNM. In this framework, the analytical formulas used for conductance
calculations are replaced by GNN-based predictions derived from pore and throat
features. The predicted conductances are then passed to the PNM solver for
permeability computation. In this way, the model avoids the idealized geometric
assumptions of PNM while preserving the physics-based flow calculations. The
GNN is trained without requiring labeled conductance data, which can number in
the thousands per pore network; instead, it learns conductance values by using
a single scalar permeability as the training target. This is made possible by
backpropagating gradients through both the GNN (via automatic differentiation)
and the PNM solver (via a discrete adjoint method), enabling fully coupled,
end-to-end training. The resulting model achieves high accuracy and generalizes
well across different scales, outperforming both pure data-driven and
traditional PNM approaches. Gradient-based sensitivity analysis further reveals
physically consistent feature influences, enhancing model interpretability.
This approach offers a scalable and physically informed framework for
permeability prediction in complex porous media, reducing model uncertainty and
improving accuracy.

</details>


### [109] [Joint data imputation and mechanistic modelling for simulating heart-brain interactions in incomplete datasets](https://arxiv.org/abs/2010.01052)
*Jaume Banus,Maxime Sermesant,Oscar Camara,Marco Lorenzi*

Main category: cs.LG

TL;DR: A probabilistic, joint imputation and personalization framework links cardiac data with brain studies using limited measurements.


<details>
  <summary>Details</summary>
Motivation: Clinical mechanistic models suffer from missing multi-modal data; brain studies often lack comprehensive heart features, hindering heart–brain modeling. A joint imputation plus personalization approach enables integrated analysis.

Method: Variational inference to jointly impute cardiac information from available features and a Gaussian Process emulator to reproduce personalized cardiovascular dynamics.

Result: On UK Biobank data, the model accurately imputes missing cardiac features even with minimal heart information (e.g., blood pressures only) while jointly estimating the lumped-model parameters.

Conclusion: This enables exploration of heart–brain interactions by simulating realistic cardiac dynamics corresponding to different brain anatomies.

Abstract: The use of mechanistic models in clinical studies is limited by the lack of
multi-modal patients data representing different anatomical and physiological
processes. For example, neuroimaging datasets do not provide a sufficient
representation of heart features for the modeling of cardiovascular factors in
brain disorders. To tackle this problem we introduce a probabilistic framework
for joint cardiac data imputation and personalisation of cardiovascular
mechanistic models, with application to brain studies with incomplete heart
data. Our approach is based on a variational framework for the joint inference
of an imputation model of cardiac information from the available features,
along with a Gaussian Process emulator that can faithfully reproduce
personalised cardiovascular dynamics. Experimental results on UK Biobank show
that our model allows accurate imputation of missing cardiac features in
datasets containing minimal heart information, e.g. systolic and diastolic
blood pressures only, while jointly estimating the emulated parameters of the
lumped model. This allows a novel exploration of the heart-brain joint
relationship through simulation of realistic cardiac dynamics corresponding to
different conditions of brain anatomy.

</details>


### [110] [Graph-Regularized Learning of Gaussian Mixture Models](https://arxiv.org/abs/2509.13855)
*Shamsiiat Abdurakhmanova,Alex Jung*

Main category: cs.LG

TL;DR: Graph-regularized distributed GMM learning uses a similarity graph to share parameters rather than raw data, enabling effective knowledge transfer in heterogeneous, low-sample settings and outperforming centralized and local baselines.


<details>
  <summary>Details</summary>
Motivation: Address privacy and heterogeneity constraints in distributed GMM learning with limited local data; leverage graph structure to guide parameter sharing.

Method: Distributed optimization where each node learns GMM parameters with graph-regularization; sharing only parameters via a similarity graph; flexible aggregation of neighbors' parameters; avoids raw data transfer.

Result: Outperforms both centralized and locally trained GMMs in heterogeneous, low-sample regimes; demonstrates effectiveness of graph-based parameter sharing.

Conclusion: Graph-regularized parameter sharing is a viable approach for privacy-preserving distributed GMM learning, especially under data scarcity and heterogeneity.

Abstract: We present a graph-regularized learning of Gaussian Mixture Models (GMMs) in
distributed settings with heterogeneous and limited local data. The method
exploits a provided similarity graph to guide parameter sharing among nodes,
avoiding the transfer of raw data. The resulting model allows for flexible
aggregation of neighbors' parameters and outperforms both centralized and
locally trained GMMs in heterogeneous, low-sample regimes.

</details>


### [111] [Masked Diffusion Models as Energy Minimization](https://arxiv.org/abs/2509.13866)
*Sitong Chen,Shen Nie,Jiacheng Sun,Zijin Feng,Zhenguo Li,Ji-Rong Wen,Chongxuan Li*

Main category: cs.LG

TL;DR: A theoretical unification showing that masked diffusion models minimize three equivalent energy formulations in discrete optimal transport; introduces Beta-based schedule parameterization reducing to a 2D search, yielding improved low-step sampling.


<details>
  <summary>Details</summary>
Motivation: Provide a solid theoretical foundation for masked diffusion models and a practical, model-free method to improve sampling efficiency without changing the model.

Method: Prove the equivalence of kinetic, conditional kinetic, and geodesic energy under the MDM framework; derive a closed-form optimality condition for the mask schedule; parameterize interpolation schedules with Beta distributions to shrink the design space to a 2D search; validate on synthetic and real benchmarks.

Result: MDMs are shown to minimize all three energy formulations under the optimal mask schedule; energy-inspired schedules outperform hand-crafted baselines, particularly in low-step sampling regimes.

Conclusion: The work offers both theoretical unification and practical gains: it clarifies the foundations of MDMs and provides a simple, post-training tuning method that enhances sampling efficiency without modifying the model.

Abstract: We present a systematic theoretical framework that interprets masked
diffusion models (MDMs) as solutions to energy minimization problems in
discrete optimal transport. Specifically, we prove that three distinct energy
formulations--kinetic, conditional kinetic, and geodesic energy--are
mathematically equivalent under the structure of MDMs, and that MDMs minimize
all three when the mask schedule satisfies a closed-form optimality condition.
This unification not only clarifies the theoretical foundations of MDMs, but
also motivates practical improvements in sampling. By parameterizing
interpolation schedules via Beta distributions, we reduce the schedule design
space to a tractable 2D search, enabling efficient post-training tuning without
model modification. Experiments on synthetic and real-world benchmarks
demonstrate that our energy-inspired schedules outperform hand-crafted
baselines, particularly in low-step sampling settings.

</details>


### [112] [FedSSG: Expectation-Gated and History-Aware Drift Alignment for Federated Learning](https://arxiv.org/abs/2509.13895)
*Zhanting Zhou,Jinshan Lai,Fengchun Zhang,Zeqin Wu,Fengli Zhang*

Main category: cs.LG

TL;DR: FedSSG is a drift-alignment method for federated learning with non-IID data and partial client participation, using per-client drift memory and a phase-based gate to stabilize and accelerate convergence with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Non-IID data and partial participation cause client drift and unstable convergence in federated learning; a lightweight, principled approach is needed to align local updates with the global objective without extra communication.

Method: Maintain per-client drift memory as a sketch of historical gradients; gate updates to memory and the local alignment term by a smooth function of the observed/expected participation ratio (a phase-by-expectation signal from the server sampler); memory scales as O(d); gate is constant-time; robust across varying sampling statistics and near-IID situations.

Result: Empirically, on CIFAR-10/100 with 100/500 clients and 2-15% participation, FedSSG outperforms strong drift-aware baselines, achieving up to ~0.9 point accuracy gain on CIFAR-10 and ~2.7 on CIFAR-100 (top-2 baselines) and about 4.5x faster target-accuracy convergence; requires only O(d) client memory and negligible extra communication.

Conclusion: Sampling statistics can be converted into a principled, history-aware phase control to stabilize and accelerate federated training; FedSSG provides a scalable, low-overhead approach that degrades gracefully to regularization under near-IID or uniform sampling.

Abstract: Non-IID data and partial participation induce client drift and inconsistent
local optima in federated learning, causing unstable convergence and accuracy
loss. We present FedSSG, a stochastic sampling-guided, history-aware drift
alignment method. FedSSG maintains a per-client drift memory that accumulates
local model differences as a lightweight sketch of historical gradients;
crucially, it gates both the memory update and the local alignment term by a
smooth function of the observed/expected participation ratio (a
phase-by-expectation signal derived from the server sampler). This
statistically grounded gate stays weak and smooth when sampling noise dominates
early, then strengthens once participation statistics stabilize, contracting
the local-global gap without extra communication. Across CIFAR-10/100 with
100/500 clients and 2-15 percent participation, FedSSG consistently outperforms
strong drift-aware baselines and accelerates convergence; on our benchmarks it
improves test accuracy by up to a few points (e.g., about +0.9 on CIFAR-10 and
about +2.7 on CIFAR-100 on average over the top-2 baseline) and yields about
4.5x faster target-accuracy convergence on average. The method adds only O(d)
client memory and a constant-time gate, and degrades gracefully to a mild
regularizer under near-IID or uniform sampling. FedSSG shows that sampling
statistics can be turned into a principled, history-aware phase control to
stabilize and speed up federated training.

</details>


### [113] [TFMAdapter: Lightweight Instance-Level Adaptation of Foundation Models for Forecasting with Covariates](https://arxiv.org/abs/2509.13906)
*Afrin Dange,Sunita Sarawagi*

Main category: cs.LG

TL;DR: TFMAdapter provides a lightweight, non-finetuning adapter that augments Time Series Foundation Models with covariate information via a two-stage, non-parametric cascade (pseudo-forecasts and Gaussian Process); achieves ~24–27% improvement with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Time Series Foundation Models achieve strong univariate forecasting with limited history but struggle to utilize future covariates (exogenous variables) due to domain-specific biases and lack of inductive bias; need a way to inject covariates without retraining or fine-tuning.

Method: Two-stage approach: (1) generate pseudo-forecasts using a simple regression on limited history; (2) train a Gaussian Process regressor to refine predictions by combining pseudo-forecasts, TSFM forecasts, and covariates. Operates as an instance-level adapter during a single model call, avoiding TSFM retraining and enabling training on full historical context with limited TSFM invocations.

Result: Extensive experiments on real-world datasets show TFMAdapter consistently outperforms both foundation models and supervised baselines, delivering a 24–27% improvement over base foundation models with minimal data and computational overhead.

Conclusion: Lightweight adapters can bridge the gap between generic foundation models and domain-specific forecasting needs by effectively incorporating covariates without fine-tuning, enabling practical deployment of TSFMs in covariate-rich forecasting scenarios.

Abstract: Time Series Foundation Models (TSFMs) have recently achieved state-of-the-art
performance in univariate forecasting on new time series simply by conditioned
on a brief history of past values. Their success demonstrates that large-scale
pretraining across diverse domains can acquire the inductive bias to generalize
from temporal patterns in a brief history. However, most TSFMs are unable to
leverage covariates -- future-available exogenous variables critical for
accurate forecasting in many applications -- due to their domain-specific
nature and the lack of associated inductive bias. We propose TFMAdapter, a
lightweight, instance-level adapter that augments TSFMs with covariate
information without fine-tuning. Instead of retraining, TFMAdapter operates on
the limited history provided during a single model call, learning a
non-parametric cascade that combines covariates with univariate TSFM forecasts.
However, such learning would require univariate forecasts at all steps in the
history, requiring too many calls to the TSFM. To enable training on the full
historical context while limiting TSFM invocations, TFMAdapter uses a two-stage
method: (1) generating pseudo-forecasts with a simple regression model, and (2)
training a Gaussian Process regressor to refine predictions using both pseudo-
and TSFM forecasts alongside covariates. Extensive experiments on real-world
datasets demonstrate that TFMAdapter consistently outperforms both foundation
models and supervised baselines, achieving a 24-27\% improvement over base
foundation models with minimal data and computational overhead. Our results
highlight the potential of lightweight adapters to bridge the gap between
generic foundation models and domain-specific forecasting needs.

</details>


### [114] [APFEx: Adaptive Pareto Front Explorer for Intersectional Fairness](https://arxiv.org/abs/2509.13908)
*Priyobrata Mondal,Faizanuddin Ansari,Swagatam Das*

Main category: cs.LG

TL;DR: APFEx introduces an adaptive multi-objective framework to optimize fairness across intersectional subgroups defined by the Cartesian product of sensitive attributes, achieving lower bias with competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: Biases accumulate across intersecting protected attributes (e.g., race, gender, age) and existing methods treat attributes separately, failing to capture multiplicative intersectional biases. A joint optimization framework with convergence guarantees for intersectional fairness is lacking.

Method: Adaptive multi-objective optimizer that switches among Pareto cone projection, gradient weighting, and exploration strategies; differentiable intersectional fairness metrics enabling gradient-based optimization of non-smooth subgroup disparities; theoretical convergence guarantees to Pareto-optimal solutions; model-agnostic and scalable.

Result: Empirical evaluation on four real-world datasets shows APFEx reduces intersectional fairness violations while maintaining competitive accuracy, outperforming single-attribute fairness approaches.

Conclusion: Introduces a scalable, model-agnostic solution for intersectional fairness that explicitly models fairness over the Cartesian product of protected attributes and provides convergence guarantees toward Pareto-optimal trade-offs.

Abstract: Ensuring fairness in machine learning models is critical, especially when
biases compound across intersecting protected attributes like race, gender, and
age. While existing methods address fairness for single attributes, they fail
to capture the nuanced, multiplicative biases faced by intersectional
subgroups. We introduce Adaptive Pareto Front Explorer (APFEx), the first
framework to explicitly model intersectional fairness as a joint optimization
problem over the Cartesian product of sensitive attributes. APFEx combines
three key innovations- (1) an adaptive multi-objective optimizer that
dynamically switches between Pareto cone projection, gradient weighting, and
exploration strategies to navigate fairness-accuracy trade-offs, (2)
differentiable intersectional fairness metrics enabling gradient-based
optimization of non-smooth subgroup disparities, and (3) theoretical guarantees
of convergence to Pareto-optimal solutions. Experiments on four real-world
datasets demonstrate APFEx's superiority, reducing fairness violations while
maintaining competitive accuracy. Our work bridges a critical gap in fair ML,
providing a scalable, model-agnostic solution for intersectional fairness.

</details>


### [115] [Ensemble of Pre-Trained Models for Long-Tailed Trajectory Prediction](https://arxiv.org/abs/2509.13914)
*Divya Thuremella,Yi Yang,Simon Wanna,Lars Kunze,Daniele De Martini*

Main category: cs.LG

TL;DR: Ensembling pre-trained trajectory predictors via a simple confidence-weighted average improves urban vehicle trajectory prediction without retraining, achieving ~10% gains over the best model and benefiting long-tail metrics on NuScenes and Argoverse; code available open-source.


<details>
  <summary>Details</summary>
Motivation: To leverage the strengths of multiple state-of-the-art trajectory prediction models without costly re-training or fine-tuning, and to improve robustness across diverse urban scenarios.

Method: Combine predictions from several pre-trained deep learning models using a confidence-weighted average; no retraining or fine-tuning required; simple ensemble that can be applied out-of-the-box.

Result: Approximately 10% improvement over the best single model, with performance gains across the dataset distribution, especially in long-tail metrics; validated on NuScenes and Argoverse datasets.

Conclusion: A simple, out-of-the-box ensemble approach can effectively harness the strengths of large trajectory prediction models, offering practical gains without retraining; code is open source.

Abstract: This work explores the application of ensemble modeling to the
multidimensional regression problem of trajectory prediction for vehicles in
urban environments. As newer and bigger state-of-the-art prediction models for
autonomous driving continue to emerge, an important open challenge is the
problem of how to combine the strengths of these big models without the need
for costly re-training. We show how, perhaps surprisingly, combining
state-of-the-art deep learning models out-of-the-box (without retraining or
fine-tuning) with a simple confidence-weighted average method can enhance the
overall prediction. Indeed, while combining trajectory prediction models is not
straightforward, this simple approach enhances performance by 10% over the best
prediction model, especially in the long-tailed metrics. We show that this
performance improvement holds on both the NuScenes and Argoverse datasets, and
that these improvements are made across the dataset distribution. The code for
our work is open source.

</details>


### [116] [Adaptive Client Selection via Q-Learning-based Whittle Index in Wireless Federated Learning](https://arxiv.org/abs/2509.13933)
*Qiyue Li,Yingxin Liu,Hang Qi,Jieping Luo,Zhizhang Liu,Jingjin Wu*

Main category: cs.LG

TL;DR: A scalable, model-free client selection method for wireless Federated Learning that learns Whittle indices per client via Q-learning (WILF-Q) to minimize total time to target accuracy by selecting high-index clients without needing explicit state transitions or data distributions; it outperforms baselines in learning efficiency.


<details>
  <summary>Details</summary>
Motivation: In wireless FL, the server cannot observe dynamic client states that affect computation and communication efficiency. This makes optimal client selection challenging and motivates a model-free, scalable decision policy that can operate with incomplete state information.

Method: Formulate client selection as a restless multi-armed bandit problem. Propose Whittle Index Learning in Federated Q-learning (WILF-Q): use Q-learning to learn and update an approximated Whittle index for each client, then select clients with the highest indices. The approach is model-free, not requiring explicit transition probabilities or data distributions, and is scalable to many clients.

Result: Experimental results show that WILF-Q significantly outperforms existing baseline policies in terms of learning efficiency in wireless FL scenarios.

Conclusion: WILF-Q provides a robust, efficient, model-free solution for client selection in wireless FL by learning per-client Whittle indices through Q-learning, enabling effective scheduling without full knowledge of client dynamics.

Abstract: We consider the client selection problem in wireless Federated Learning (FL),
with the objective of reducing the total required time to achieve a certain
level of learning accuracy. Since the server cannot observe the clients'
dynamic states that can change their computation and communication efficiency,
we formulate client selection as a restless multi-armed bandit problem. We
propose a scalable and efficient approach called the Whittle Index Learning in
Federated Q-learning (WILF-Q), which uses Q-learning to adaptively learn and
update an approximated Whittle index associated with each client, and then
selects the clients with the highest indices. Compared to existing approaches,
WILF-Q does not require explicit knowledge of client state transitions or data
distributions, making it well-suited for deployment in practical FL settings.
Experiment results demonstrate that WILF-Q significantly outperforms existing
baseline policies in terms of learning efficiency, providing a robust and
efficient approach to client selection in wireless FL.

</details>


### [117] [eXtended Physics Informed Neural Network Method for Fracture Mechanics Problems](https://arxiv.org/abs/2509.13952)
*Amin Lotfalian,Mohammad Reza Banan,Pooyan Broumand*

Main category: cs.LG

TL;DR: X-PINN: a physics-informed neural network framework tailored for fracture mechanics with multiple cracks, combining energy-based loss, custom integration, and XFEM-inspired enrichment; uses a dual-network structure for standard and enriched components; validated numerically in 1D/2D and extendable to 3D.


<details>
  <summary>Details</summary>
Motivation: Standard PINNs struggle to accurately model crack discontinuities and singularities in fractured media. To address this, the paper integrates XFEM-like enriching functions, energy-based loss, and domain decomposition to capture crack tips and multiphase crack interactions more robustly, enabling flexible 1D/2D simulations with potential 3D extension.

Method: Introduce an enriched solution space by adding crack-discontinuity and tip-singularity functions inspired by XFEM. Employ an energy-based loss function, tailored numerical integration schemes, and domain decomposition. Model standard and enriched components with separate neural networks to improve representation and training stability. Validate via numerical experiments in 1D/2D (and discuss 3D extensibility).

Result: Numerical experiments demonstrate the proposed X-PINN framework is effective and robust for dealing with multiple cracks in fractured media, validating the enrichment strategy and the split-network architecture in capturing discontinuities and singularities and handling complex crack configurations.

Conclusion: X-PINN provides a robust, extensible framework for fracture mechanics problems with multiple cracks, leveraging enrichment, energy-based learning, and domain decomposition to enable accurate, flexible simulations in 1D/2D and with potential extension to 3D.

Abstract: This paper presents eXtended Physics-Informed Neural Network (X-PINN), a
novel and robust framework for addressing fracture mechanics problems involving
multiple cracks in fractured media. To address this, an energy-based loss
function, customized integration schemes, and domain decomposition procedures
are proposed. Inspired by the Extended Finite Element Method (XFEM), the neural
network solution space is enriched with specialized functions that allow crack
body discontinuities and singularities at crack tips to be explicitly captured.
Furthermore, a structured framework is introduced in which standard and
enriched solution components are modeled using distinct neural networks,
enabling flexible and effective simulations of complex multiple-crack problems
in 1D and 2D domains, with convenient extensibility to 3D problems. Numerical
experiments are conducted to validate the effectiveness and robustness of the
proposed method.

</details>


### [118] [Personalization on a Budget: Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection](https://arxiv.org/abs/2509.13974)
*Amirhossein Shahbazinia,Jonathan Dan,Jose A. Miranda,Giovanni Ansaloni,David Atienza*

Main category: cs.LG

TL;DR: EpiSMART, a continual learning framework for epileptic seizure detection using a memory-limited replay buffer and informed sample selection to adapt to individual patient's EEG data with minimal labeled data and updates, improving F1 score by 21%.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting and enable personalized, real-time seizure detection in wearables with limited labeled data and computational resources.

Method: Size-constrained replay buffer; selective retention of high-entropy and seizure-predicted samples; incremental updates to adapt to patient-specific EEG features; online learning style within continual learning paradigm.

Result: On CHB-MIT dataset, 21% improvement in F1 score over a non-updating baseline across all patients; average data labeling 6.46 minutes and 6.28 updates per day.

Conclusion: EpiSMART provides robust, personalized seizure detection under resource constraints, suitable for wearable deployment; demonstrates continual learning effectiveness for adapting EEG models.

Abstract: Objective: Epilepsy, a prevalent neurological disease, demands careful
diagnosis and continuous care. Seizure detection remains challenging, as
current clinical practice relies on expert analysis of electroencephalography,
which is a time-consuming process and requires specialized knowledge.
Addressing this challenge, this paper explores automated epileptic seizure
detection using deep learning, focusing on personalized continual learning
models that adapt to each patient's unique electroencephalography signal
features, which evolve over time. Methods: In this context, our approach
addresses the challenge of integrating new data into existing models without
catastrophic forgetting, a common issue in static deep learning models. We
propose EpiSMART, a continual learning framework for seizure detection that
uses a size-constrained replay buffer and an informed sample selection strategy
to incrementally adapt to patient-specific electroencephalography signals. By
selectively retaining high-entropy and seizure-predicted samples, our method
preserves critical past information while maintaining high performance with
minimal memory and computational requirements. Results: Validation on the
CHB-MIT dataset, shows that EpiSMART achieves a 21% improvement in the F1 score
over a trained baseline without updates in all other patients. On average,
EpiSMART requires only 6.46 minutes of labeled data and 6.28 updates per day,
making it suitable for real-time deployment in wearable systems.
Conclusion:EpiSMART enables robust and personalized seizure detection under
realistic and resource-constrained conditions by effectively integrating new
data into existing models without degrading past knowledge. Significance: This
framework advances automated seizure detection by providing a continual
learning approach that supports patient-specific adaptation and practical
deployment in wearable healthcare systems.

</details>


### [119] [Deep Temporal Graph Networks for Real-Time Correction of GNSS Jamming-Induced Deviations](https://arxiv.org/abs/2509.14000)
*Ivana Kesić,Aljaž Blatnik,Carolina Fortuna,Blaž Bertalanič*

Main category: cs.LG

TL;DR: Proposes a receiver-centric dynamic temporal graph network (HeteroGCLSTM) to mitigate GNSS jamming by predicting and correcting the receiver's horizontal deviation in real time; uses a heterogeneous star graph and one-layer Heterogeneous Graph ConvLSTM, outperforming baselines in MAE across multiple jamming scenarios and showing strong data efficiency.


<details>
  <summary>Details</summary>
Motivation: GNSS is vulnerable to intentional jamming, which degrades positioning and timing when operational. Existing time-series baselines underperform; a graph-based, receiver-centric approach could better exploit spatial-temporal context to correct deviations in real time.

Method: Model: Heterogeneous Graph ConvLSTM (HeteroGCLSTM) on a 1 Hz, heterogeneous star graph (receiver center with tracked satellites as leaves). It aggregates one-hop spatial context and temporal dynamics over a short history to output a 2D horizontal deviation vector used to correct the receiver. Evaluation uses two receivers, three jammer profiles (CW, CW3, FM) at six power levels (-45 to -70 dBm) with 50 repetitions per scenario; compares against MLP, uniform CNN, Seq2Point CNN.

Result: The method achieves the lowest MAE across baselines. At -45 dBm MAEs include 3.64 cm (GP01/cw), 7.74 cm (GP01/cw3), 4.41 cm (ublox/cw), 4.84 cm (ublox/cw3), 4.82 cm (ublox/FM). With stronger jamming (-60 to -70 dBm), MAEs improve to 1.65–2.08 cm. On mixed-power data, MAE is 3.78 cm (GP01) and 4.25 cm (ublox10). A data-scarcity study shows strong data efficiency: with 10% training data, MAE remains well ahead of baselines (≈20 cm vs. 36–42 cm).

Conclusion: A receiver-centric dynamic graph-regression framework (HeteroGCLSTM) can effectively mitigate GNSS jamming in real time, achieving high-precision horizontal corrections and robust data efficiency, outperforming conventional multivariate time-series baselines across multiple jamming scenarios.

Abstract: Global Navigation Satellite Systems (GNSS) are increasingly disrupted by
intentional jamming, degrading availability precisely when positioning and
timing must remain operational. We address this by reframing jamming mitigation
as dynamic graph regression and introducing a receiver-centric deep temporal
graph network that predicts, and thus corrects, the receivers horizontal
deviation in real time. At each 1 Hz epoch, the satellite receiver environment
is represented as a heterogeneous star graph (receiver center, tracked
satellites as leaves) with time varying attributes (e.g., SNR, azimuth,
elevation, latitude/longitude). A single layer Heterogeneous Graph ConvLSTM
(HeteroGCLSTM) aggregates one hop spatial context and temporal dynamics over a
short history to output the 2D deviation vector applied for on the fly
correction.
  We evaluate on datasets from two distinct receivers under three jammer
profiles, continuous wave (cw), triple tone (cw3), and wideband FM, each
exercised at six power levels between -45 and -70 dBm, with 50 repetitions per
scenario (prejam/jam/recovery). Against strong multivariate time series
baselines (MLP, uniform CNN, and Seq2Point CNN), our model consistently attains
the lowest mean absolute error (MAE). At -45 dBm, it achieves 3.64 cm
(GP01/cw), 7.74 cm (GP01/cw3), 4.41 cm (ublox/cw), 4.84 cm (ublox/cw3), and
4.82 cm (ublox/FM), improving to 1.65-2.08 cm by -60 to -70 dBm. On mixed mode
datasets pooling all powers, MAE is 3.78 cm (GP01) and 4.25 cm (ublox10),
outperforming Seq2Point, MLP, and CNN. A split study shows superior data
efficiency: with only 10\% training data our approach remains well ahead of
baselines (20 cm vs. 36-42 cm).

</details>


### [120] [Differentially private federated learning for localized control of infectious disease dynamics](https://arxiv.org/abs/2509.14024)
*Raouf Kerkouche,Henrik Zunker,Mario Fritz,Martin J. Kühn*

Main category: cs.LG

TL;DR: Federated learning with client-level differential privacy enables privacy-preserving, local epidemic forecasting with performance close to non-private models under moderate privacy, highlighting a privacy–utility trade-off that varies with epidemic phase.


<details>
  <summary>Details</summary>
Motivation: Address the need for accurate, local epidemic forecasts while respecting data privacy and minimizing data centralization and sharing across local health authorities (LHAs).

Method: Train a shared multilayer perceptron on sliding windows of recent case counts using FL across counties/ LHAs as clients; apply client-level differential privacy by clipping norms and adding DP noise in the server aggregation; evaluate on German COVID-19 county-level data across two epidemic phases.

Result: With moderate DP, performance approaches non-DP: R^2 ≈ 0.94 (0.95 non-DP) and MAPE ≈ 26% (21% non-DP) in Nov 2020; R^2 ≈ 0.88 (0.93 non-DP) and MAPE ≈ 21% in Mar 2022; very strict DP yields unstable forecasts, while privacy-preserving collaboration is viable depending on epidemic phase.

Conclusion: Client-level DP-FL provides useful county-level forecasts with strong privacy guarantees; appropriate privacy budgets depend on the epidemic phase, enabling privacy-compliant collaboration among LHAs for localized forecasting.

Abstract: In times of epidemics, swift reaction is necessary to mitigate epidemic
spreading. For this reaction, localized approaches have several advantages,
limiting necessary resources and reducing the impact of interventions on a
larger scale. However, training a separate machine learning (ML) model on a
local scale is often not feasible due to limited available data. Centralizing
the data is also challenging because of its high sensitivity and privacy
constraints. In this study, we consider a localized strategy based on the
German counties and communities managed by the related local health authorities
(LHA). For the preservation of privacy to not oppose the availability of
detailed situational data, we propose a privacy-preserving forecasting method
that can assist public health experts and decision makers. ML methods with
federated learning (FL) train a shared model without centralizing raw data.
Considering the counties, communities or LHAs as clients and finding a balance
between utility and privacy, we study a FL framework with client-level
differential privacy (DP). We train a shared multilayer perceptron on sliding
windows of recent case counts to forecast the number of cases, while clients
exchange only norm-clipped updates and the server aggregated updates with DP
noise. We evaluate the approach on COVID-19 data on county-level during two
phases. As expected, very strict privacy yields unstable, unusable forecasts.
At a moderately strong level, the DP model closely approaches the non-DP model:
$R^2= 0.94$ (vs. 0.95) and mean absolute percentage error (MAPE) of 26 % in
November 2020; $R^2= 0.88$ (vs. 0.93) and MAPE of 21 % in March 2022. Overall,
client-level DP-FL can deliver useful county-level predictions with strong
privacy guarantees, and viable privacy budgets depend on epidemic phase,
allowing privacy-compliant collaboration among health authorities for local
forecasting.

</details>


### [121] [Deep Learning-Driven Peptide Classification in Biological Nanopores](https://arxiv.org/abs/2509.14029)
*Samuel Tovey,Julian Hoßbach,Sandro Kuppel,Tobias Ensslen,Jan C. Behrends,Christian Holm*

Main category: cs.LG

TL;DR: Wavelet-based scaleogram representations of nanopore current signals enable real-time peptide/protein classification, achieving ~81% accuracy on 42 peptides and enabling hardware-friendly transfer learning for point-of-care diagnostics.


<details>
  <summary>Details</summary>
Motivation: To enable rapid, inexpensive clinical protein identification using nanopore devices by extracting informative representations from complex current signals for robust machine learning.

Method: Convert nanopore signals to scaleogram images via wavelet transforms to capture amplitude, frequency, and temporal information; apply ML classifiers on images; evaluate on 42 peptides; explore model transfer for hardware deployment.

Result: Achieved ~81% classification accuracy, setting a new state-of-the-art; demonstration of transfer techniques for deployment in real hardware.

Conclusion: Wavelet-based image representations of nanopore signals improve peptide/protein classification and bring real-time diagnostics closer to clinical use; transferability to hardware is feasible and critical for point-of-care deployment.

Abstract: A device capable of performing real time classification of proteins in a
clinical setting would allow for inexpensive and rapid disease diagnosis. One
such candidate for this technology are nanopore devices. These devices work by
measuring a current signal that arises when a protein or peptide enters a
nanometer-length-scale pore. Should this current be uniquely related to the
structure of the peptide and its interactions with the pore, the signals can be
used to perform identification. While such a method would allow for real time
identification of peptides and proteins in a clinical setting, to date, the
complexities of these signals limit their accuracy. In this work, we tackle the
issue of classification by converting the current signals into scaleogram
images via wavelet transforms, capturing amplitude, frequency, and time
information in a modality well-suited to machine learning algorithms. When
tested on 42 peptides, our method achieved a classification accuracy of
~$81\,\%$, setting a new state-of-the-art in the field and taking a step toward
practical peptide/protein diagnostics at the point of care. In addition, we
demonstrate model transfer techniques that will be critical when deploying
these models into real hardware, paving the way to a new method for real-time
disease diagnosis.

</details>


### [122] [Queen Detection in Beehives via Environmental Sensor Fusion for Low-Power Edge Computing](https://arxiv.org/abs/2509.14061)
*Chiara De Luca,Elisa Donati*

Main category: cs.LG

TL;DR: A lightweight edge-AI solution for queen bee detection using environmental sensor fusion (temperature, humidity, pressure differentials) on an STM32 MCU; achieves >99% accuracy, with audio features not improving performance; enables scalable, non-invasive hive monitoring.


<details>
  <summary>Details</summary>
Motivation: Current queen detection relies on manual inspections, which are labor-intensive and impractical at scale. Audio-based approaches exist but suffer from high power use, preprocessing complexity, and susceptibility to noise. A low-power, non-invasive, scalable method is needed for precision beekeeping.

Method: Sensor fusion of hive-internal and external environment (temperature, humidity, pressure differential) with a quantized decision tree classifier running on a commercial STM32 microcontroller for real-time edge inference. Audio features were tested but offered no significant gains.

Result: The system achieved over 99% queen detection accuracy using only environmental inputs; incorporating audio features provided no meaningful improvement.

Conclusion: A scalable, sustainable, low-power solution for non-invasive hive monitoring that enables autonomous beekeeping using off-the-shelf hardware, with potential for deployment in large-scale operations.

Abstract: Queen bee presence is essential for the health and stability of honeybee
colonies, yet current monitoring methods rely on manual inspections that are
labor-intensive, disruptive, and impractical for large-scale beekeeping. While
recent audio-based approaches have shown promise, they often require high power
consumption, complex preprocessing, and are susceptible to ambient noise. To
overcome these limitations, we propose a lightweight, multimodal system for
queen detection based on environmental sensor fusion-specifically, temperature,
humidity, and pressure differentials between the inside and outside of the
hive. Our approach employs quantized decision tree inference on a commercial
STM32 microcontroller, enabling real-time, low-power edge computing without
compromising accuracy. We show that our system achieves over 99% queen
detection accuracy using only environmental inputs, with audio features
offering no significant performance gain. This work presents a scalable and
sustainable solution for non-invasive hive monitoring, paving the way for
autonomous, precision beekeeping using off-the-shelf, energy-efficient
hardware.

</details>


### [123] [Online Bayesian Risk-Averse Reinforcement Learning](https://arxiv.org/abs/2509.14077)
*Yuhao Wang,Enlu Zhou*

Main category: cs.LG

TL;DR: A Bayesian risk-averse RL framework via BRMDP is developed, with an asymptotic normality result for the gap between Bayesian risk and true value, revealing a pessimistic underestimation that shrinks with more data; posterior-sampling procedures are proposed for RL and CMAB, with sublinear regret guarantees, and experiments validate the approach.


<details>
  <summary>Details</summary>
Motivation: Address epistemic uncertainty from limited data in reinforcement learning by adopting a principled Bayesian risk-averse formulation (BRMDP) to quantify and manage parameter uncertainty; study the asymptotic behavior and online implications.

Method: Formulate BRMDP to model parameter uncertainty; derive the asymptotic normality of the difference between Bayesian risk value and true value; analyze the impact of risk aversion; develop two posterior-sampling procedures for general RL and CMAB; establish sublinear regret bounds for both standard regret and Bayesian risk regret; conduct numerical experiments.

Result: BR risk value underestimates the original value, with the underestimation increasing with risk aversion and decreasing as data accumulate; proposed posterior-sampling algorithms achieve sublinear regret in RL and CMAB, including a sublinear bound for CMAB under Bayesian risk regret; experiments corroborate theoretical findings.

Conclusion: The Bayesian risk-averse approach induces a controllable pessimism that fades with more data; the proposed online procedures are theoretically sound (sublinear regret) and practically effective in mitigating epistemic uncertainty in RL and CMAB.

Abstract: In this paper, we study the Bayesian risk-averse formulation in reinforcement
learning (RL). To address the epistemic uncertainty due to a lack of data, we
adopt the Bayesian Risk Markov Decision Process (BRMDP) to account for the
parameter uncertainty of the unknown underlying model. We derive the asymptotic
normality that characterizes the difference between the Bayesian risk value
function and the original value function under the true unknown distribution.
The results indicate that the Bayesian risk-averse approach tends to
pessimistically underestimate the original value function. This discrepancy
increases with stronger risk aversion and decreases as more data become
available. We then utilize this adaptive property in the setting of online RL
as well as online contextual multi-arm bandits (CMAB), a special case of online
RL. We provide two procedures using posterior sampling for both the general RL
problem and the CMAB problem. We establish a sub-linear regret bound, with the
regret defined as the conventional regret for both the RL and CMAB settings.
Additionally, we establish a sub-linear regret bound for the CMAB setting with
the regret defined as the Bayesian risk regret. Finally, we conduct numerical
experiments to demonstrate the effectiveness of the proposed algorithm in
addressing epistemic uncertainty and verifying the theoretical properties.

</details>


### [124] [Exploring the Relationship between Brain Hemisphere States and Frequency Bands through Deep Learning Optimization Techniques](https://arxiv.org/abs/2509.14078)
*Robiul Islam,Dmitry I. Ignatov,Karl Kaberg,Roman Nabatchikov*

Main category: cs.LG

TL;DR: Optimizer choice and architecture shape EEG-based classifier performance across bands; Adagrad/RMSprop excel; CNN strong; SHAP highlights band contributions.


<details>
  <summary>Details</summary>
Motivation: Understand how optimizers, models, and EEG bands affect classification accuracy and feature importance in neuroimaging-based EEG tasks.

Method: Compare deep dense, shallow 3-layer, and CNN architectures implemented in TensorFlow and PyTorch; evaluate Adagrad, RMSprop, Adadelta, SGD, FTRL across EEG frequency bands for left/right hemispheres; use SHAP to interpret feature contributions.

Result: Adagrad and RMSprop performed consistently across bands; Adadelta robust in cross-model evaluations; Adagrad excels beta band; RMSprop excels gamma; SGD/FTRL inconsistent; CNN 2nd best; deep dense competitive; shallow network efficient; SHAP reveals nuanced contributions of bands to accuracy.

Conclusion: Careful selection of optimizer, architecture, and frequency-band analysis improves EEG-based classifier performance and aids interpretation of feature importance in neuroimaging tasks.

Abstract: This study investigates classifier performance across EEG frequency bands
using various optimizers and evaluates efficient class prediction for the left
and right hemispheres. Three neural network architectures - a deep dense
network, a shallow three-layer network, and a convolutional neural network
(CNN) - are implemented and compared using the TensorFlow and PyTorch
frameworks. Results indicate that the Adagrad and RMSprop optimizers
consistently perform well across different frequency bands, with Adadelta
exhibiting robust performance in cross-model evaluations. Specifically, Adagrad
excels in the beta band, while RMSprop achieves superior performance in the
gamma band. Conversely, SGD and FTRL exhibit inconsistent performance. Among
the models, the CNN demonstrates the second highest accuracy, particularly in
capturing spatial features of EEG data. The deep dense network shows
competitive performance in learning complex patterns, whereas the shallow
three-layer network, sometimes being less accurate, provides computational
efficiency. SHAP (Shapley Additive Explanations) plots are employed to identify
efficient class prediction, revealing nuanced contributions of EEG frequency
bands to model accuracy. Overall, the study highlights the importance of
optimizer selection, model architecture, and EEG frequency band analysis in
enhancing classifier performance and understanding feature importance in
neuroimaging-based classification tasks.

</details>


### [125] [From Distributional to Quantile Neural Basis Models: the case of Electricity Price Forecasting](https://arxiv.org/abs/2509.14113)
*Alessandro Brusaferri,Danial Ramin,Andrea Ballarino*

Main category: cs.LG

TL;DR: Introduces Quantile Neural Basis Model (QNBM) that integrates Quantile Generalized Additive Models with neural nets using shared basis decomposition and weight factorization for interpretable multi-horizon probabilistic forecasting, validated on day-ahead electricity prices with competitive accuracy and added interpretability.


<details>
  <summary>Details</summary>
Motivation: There is a need to understand feature-conditioned outputs in multi-horizon probabilistic forecasts; traditional black-box neural nets lack interpretability; Quantile GAMS offer interpretability but integrating with end-to-end training yields interpretability across horizons while avoiding strict parametric distributions.

Method: Develop Quantile Neural Basis Model that leverages shared basis decomposition and weight factorization to map input features to horizon-wise outputs; combine interpretability of QGAM with neural network training; operate without parametric distributional assumptions; end-to-end training; apply to day-ahead electricity price forecasting.

Result: Achieves predictive performance comparable to distributional and quantile regression neural networks; provides insights into model behavior via learned nonlinear feature mappings across the horizon.

Conclusion: QNBM provides a viable framework for interpretable, distribution-free multi-horizon probabilistic forecasting with competitive accuracy, enabling better understanding of how features influence horizon-wise predictions, and could generalize to other domains.

Abstract: While neural networks are achieving high predictive accuracy in multi-horizon
probabilistic forecasting, understanding the underlying mechanisms that lead to
feature-conditioned outputs remains a significant challenge for forecasters. In
this work, we take a further step toward addressing this critical issue by
introducing the Quantile Neural Basis Model, which incorporates the
interpretability principles of Quantile Generalized Additive Models into an
end-to-end neural network training framework. To this end, we leverage shared
basis decomposition and weight factorization, complementing Neural Models for
Location, Scale, and Shape by avoiding any parametric distributional
assumptions. We validate our approach on day-ahead electricity price
forecasting, achieving predictive performance comparable to distributional and
quantile regression neural networks, while offering valuable insights into
model behavior through the learned nonlinear mappings from input features to
output predictions across the horizon.

</details>


### [126] [Breaking the Cycle of Incarceration With Targeted Mental Health Outreach: A Case Study in Machine Learning for Public Policy](https://arxiv.org/abs/2509.14129)
*Kit T. Rodolfa,Erika Salomon,Jin Yao,Steve Yoder,Robert Sullivan,Kevin McGuire,Allie Dickinson,Rob MacDougall,Brian Seidler,Christina Sung,Claire Herdeman,Rayid Ghani*

Main category: cs.LG

TL;DR: Predictive-risk modeling identifies high-risk individuals for reincarceration and tests targeted mental-health outreach; high predictive power with greatest impact among highest-risk group, reducing reincarceration and affecting mental health use, EMS, and criminal justice metrics.


<details>
  <summary>Details</summary>
Motivation: Address the cycle of incarceration driven by untreated mental illness, substance dependence, and homelessness; reduce racial disparities in criminal justice outcomes by evaluating proactive, community-driven interventions.

Method: Use data to build predictive models to identify high-risk individuals; design and conduct a field trial of targeted outreach; analyze predictive power and measure impacts on reincarceration, mental health utilization, EMS dispatches, and CJ involvement; determine effectiveness by risk level.

Result: The model is highly predictive of new jail bookings; more than half of individuals in the trial's highest-risk group return to jail within a year; outreach was most effective among these highest-risk individuals; observed impacts on mental health utilization, EMS dispatches, and criminal justice involvement.

Conclusion: Targeted proactive mental-health outreach guided by predictive risk can reduce reincarceration and improve related outcomes; supports scalable, data-driven approaches in criminal justice reform and warrants further evaluation of thresholds and interventions.

Abstract: Many incarcerated individuals face significant and complex challenges,
including mental illness, substance dependence, and homelessness, yet jails and
prisons are often poorly equipped to address these needs. With little support
from the existing criminal justice system, these needs can remain untreated and
worsen, often leading to further offenses and a cycle of incarceration with
adverse outcomes both for the individual and for public safety, with
particularly large impacts on communities of color that continue to widen the
already extensive racial disparities in criminal justice outcomes. Responding
to these failures, a growing number of criminal justice stakeholders are
seeking to break this cycle through innovative approaches such as
community-driven and alternative approaches to policing, mentoring, community
building, restorative justice, pretrial diversion, holistic defense, and social
service connections. Here we report on a collaboration between Johnson County,
Kansas, and Carnegie Mellon University to perform targeted, proactive mental
health outreach in an effort to reduce reincarceration rates.
  This paper describes the data used, our predictive modeling approach and
results, as well as the design and analysis of a field trial conducted to
confirm our model's predictive power, evaluate the impact of this targeted
outreach, and understand at what level of reincarceration risk outreach might
be most effective. Through this trial, we find that our model is highly
predictive of new jail bookings, with more than half of individuals in the
trial's highest-risk group returning to jail in the following year. Outreach
was most effective among these highest-risk individuals, with impacts on mental
health utilization, EMS dispatches, and criminal justice involvement.

</details>


### [127] [A Compositional Kernel Model for Feature Learning](https://arxiv.org/abs/2509.14158)
*Feng Ruan,Keli Liu,Michael Jordan*

Main category: cs.LG

TL;DR: A compositional kernel ridge regression model with coordinate-wise input reweighting acts as a testbed for feature learning in compositional architectures. It reveals that l1-type kernels recover nonlinear features, while Gaussian kernels only recover linear effects, with guarantees that noise coordinates are discarded under Gaussian noise.


<details>
  <summary>Details</summary>
Motivation: To understand how feature learning and variable selection operate in compositional kernel methods, and to compare the ability of different kernels (l1-type vs Gaussian) to identify relevant nonlinear features while discarding noise.

Method: Formulate a variational (kernel ridge) problem where the predictor is applied to a coordinate-wise reweighting of inputs, analyze global minimizers and stationary points, and prove noise-variable elimination under Gaussian noise. Compare kernel types (e.g., Laplace vs Gaussian) in terms of the features they can recover at stationary points.

Result: The analysis shows that both global minimizers and stationary points discard noise coordinates when noise is Gaussian. Laplace (l1-type) kernels recover features contributing to nonlinear effects at stationary points, whereas Gaussian kernels recover only linear ones.

Conclusion: In this compositional setting, l1-type kernels enable recovery of nonlinear features at stationary points, while Gaussian kernels fail to capture nonlinear effects, highlighting the importance of kernel choice for feature learning and variable selection in compositional RKRR models.

Abstract: We study a compositional variant of kernel ridge regression in which the
predictor is applied to a coordinate-wise reweighting of the inputs. Formulated
as a variational problem, this model provides a simple testbed for feature
learning in compositional architectures. From the perspective of variable
selection, we show how relevant variables are recovered while noise variables
are eliminated. We establish guarantees showing that both global minimizers and
stationary points discard noise coordinates when the noise variables are
Gaussian distributed. A central finding is that $\ell_1$-type kernels, such as
the Laplace kernel, succeed in recovering features contributing to nonlinear
effects at stationary points, whereas Gaussian kernels recover only linear
ones.

</details>


### [128] [Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework](https://arxiv.org/abs/2509.14167)
*Md Rezwan Jaher,Abul Mukid Mohammad Mukaddes,A. B. M. Abdul Malek*

Main category: cs.LG

TL;DR: End-to-end AI framework with PCDS data generation and Bayesian uncertainty to noninvasively infer unmeasurable glaucoma parameters from sparse data; matches tonography for outflow facility and yields a permeability biomarker for disease risk, while reducing simulation cost and generalizing to data-scarce inverse problems.


<details>
  <summary>Details</summary>
Motivation: In clinical practice, key parameters like trabecular meshwork permeability are unmeasurable in vivo, making critical glaucoma decisions hard. Ill-posed inverse problems are hindered by scarce ground-truth data and costly high-fidelity simulations.

Method: A multi-stage AI architecture to separate the problem, a novel PCDS data-generation strategy to obviate hundreds of thousands of costly simulations, and a Bayesian engine to quantify predictive uncertainty. It decomposes a single IOP measurement into core components from routine inputs to estimate unmeasurable tissue permeability and outflow facility.

Result: Noninvasively estimated outflow facility shows excellent agreement with state-of-the-art tonography, with precision comparable to direct instruments. The permeability biomarker accurately stratifies clinical cohorts by disease risk, indicating diagnostic potential.

Conclusion: The framework offers a generalizable blueprint for solving similar inverse problems in data-scarce, computationally-intensive domains and reduces computational cost from years to hours, enabling noninvasive inference of hard-to-measure clinical parameters.

Abstract: Many critical healthcare decisions are challenged by the inability to measure
key underlying parameters. Glaucoma, a leading cause of irreversible blindness
driven by elevated intraocular pressure (IOP), provides a stark example. The
primary determinant of IOP, a tissue property called trabecular meshwork
permeability, cannot be measured in vivo, forcing clinicians to depend on
indirect surrogates. This clinical challenge is compounded by a broader
computational one: developing predictive models for such ill-posed inverse
problems is hindered by a lack of ground-truth data and prohibitive cost of
large-scale, high-fidelity simulations. We address both challenges with an
end-to-end framework to noninvasively estimate unmeasurable variables from
sparse, routine data. Our approach combines a multi-stage artificial
intelligence architecture to functionally separate the problem; a novel data
generation strategy we term PCDS that obviates the need for hundreds of
thousands of costly simulations, reducing the effective computational time from
years to hours; and a Bayesian engine to quantify predictive uncertainty. Our
framework deconstructs a single IOP measurement into its fundamental components
from routine inputs only, yielding estimates for the unmeasurable tissue
permeability and a patient's outflow facility. Our noninvasively estimated
outflow facility achieved excellent agreement with state-of-the-art tonography
with precision comparable to direct physical instruments. Furthermore, the
newly derived permeability biomarker demonstrates high accuracy in stratifying
clinical cohorts by disease risk, highlighting its diagnostic potential. More
broadly, our framework establishes a generalizable blueprint for solving
similar inverse problems in other data-scarce, computationally-intensive
domains.

</details>


### [129] [TopoSizing: An LLM-aided Framework of Topology-based Understanding and Sizing for AMS Circuits](https://arxiv.org/abs/2509.14169)
*Ziming Wei,Zichen Kong,Yuan Wang,David Z. Pan,Xiyuan Tang*

Main category: cs.LG

TL;DR: TopoSizing is an end-to-end framework that combines graph-based circuit understanding with LLM-driven hypothesis testing and Bayesian optimization to boost sampling efficiency and maintain feasibility in analog/mixed-signal circuit design.


<details>
  <summary>Details</summary>
Motivation: Addresses data scarcity and the challenge of embedding domain knowledge in auto-design flows. Seeks a generalizable, transparent approach that reduces wasted evaluations and manual intervention.

Method: 1) Use graph algorithms to organize netlists into a hierarchical device–module–stage representation. 2) Employ LLM agents to run an iterative hypothesis–verification–refinement loop with built-in consistency checks, producing explicit annotations. 3) Feed verified insights into Bayesian optimization via LLM-guided initial sampling and stagnation-triggered trust-region updates.

Result: Claims improved optimization efficiency while preserving feasibility, enabled by explicit circuit understandings and annotations that guide the search process.

Conclusion: TopoSizing presents a cohesive, end-to-end framework that integrates graph-based circuit understanding, verifiable LLM reasoning, and Bayesian optimization to enhance data-efficient design of analog/mixed-signal circuits.

Abstract: Analog and mixed-signal circuit design remains challenging due to the
shortage of high-quality data and the difficulty of embedding domain knowledge
into automated flows. Traditional black-box optimization achieves sampling
efficiency but lacks circuit understanding, which often causes evaluations to
be wasted in low-value regions of the design space. In contrast, learning-based
methods embed structural knowledge but are case-specific and costly to retrain.
Recent attempts with large language models show potential, yet they often rely
on manual intervention, limiting generality and transparency. We propose
TopoSizing, an end-to-end framework that performs robust circuit understanding
directly from raw netlists and translates this knowledge into optimization
gains. Our approach first applies graph algorithms to organize circuits into a
hierarchical device-module-stage representation. LLM agents then execute an
iterative hypothesis-verification-refinement loop with built-in consistency
checks, producing explicit annotations. Verified insights are integrated into
Bayesian optimization through LLM-guided initial sampling and
stagnation-triggered trust-region updates, improving efficiency while
preserving feasibility.

</details>


### [130] [TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning](https://arxiv.org/abs/2509.14172)
*Ziyuan Chen,Zhenghui Zhao,Zhangye Han,Miancan Liu,Xianhang Ye,Yiqing Li,Hongbo Min,Jinkui Ren,Xiantao Zhang,Guitao Cao*

Main category: cs.LG

TL;DR: TGPO introduces an offline RL framework for Web Agents that uses a tree-structured trajectory representation to merge semantically identical states across trajectories, along with a Process Reward Model and dynamic weighting to optimize training, achieving higher success rates with fewer redundant steps on two web interaction benchmarks.


<details>
  <summary>Details</summary>
Motivation: Credit assignment misallocation, high annotation costs, and reward sparsity impede RL-based Web Agent training; existing methods struggle with label conflicts and data efficiency; need for an offline approach that can generate fine-grained rewards and emphasize impactful decisions.

Method: Tree-guided trajectory representation that merges identical states across trajectories to form a conflict-free offline dataset; Process Reward Model generates fine-grained rewards via subgoal progress, redundancy detection, and action verification; dynamic weighting prioritizes high-impact decision points during training.

Result: TGPO outperforms existing methods on Online-Mind2Web and C-WebShop datasets, achieving higher success rates with fewer redundant steps.

Conclusion: Tree-guided preference optimization effectively tackles credit assignment, reward sparsity, and annotation costs in offline RL for web agents, improving data efficiency and performance; the approach shows promise for broader Web Agent applications.

Abstract: With the rapid advancement of large language models and vision-language
models, employing large models as Web Agents has become essential for automated
web interaction. However, training Web Agents with reinforcement learning faces
critical challenges including credit assignment misallocation, prohibitively
high annotation costs, and reward sparsity. To address these issues, we propose
Tree-Guided Preference Optimization (TGPO), an offline reinforcement learning
framework that proposes a tree-structured trajectory representation merging
semantically identical states across trajectories to eliminate label conflicts.
Our framework incorporates a Process Reward Model that automatically generates
fine-grained rewards through subgoal progress, redundancy detection, and action
verification. Additionally, a dynamic weighting mechanism prioritizes
high-impact decision points during training. Experiments on Online-Mind2Web and
our self-constructed C-WebShop datasets demonstrate that TGPO significantly
outperforms existing methods, achieving higher success rates with fewer
redundant steps.

</details>


### [131] [Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting](https://arxiv.org/abs/2509.14181)
*Yifan Hu,Jie Yang,Tian Zhou,Peiyuan Liu,Yujin Tang,Rong Jin,Liang Sun*

Main category: cs.LG

TL;DR: TimeAlign is a lightweight, plug-and-play alignment module for time-series forecasting that learns auxiliary features via a reconstruction task and feeds them back to any base forecaster, yielding improved accuracy across benchmarks with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Explicit representation alignment is underexplored in forecasting, and distributional gaps between historical inputs and future targets (e.g., frequency mismatches) limit performance. A simple, general method to align representations could unlock gains for various forecasters.

Method: Train a small auxiliary encoder-decoder to reconstruct auxiliary features from input histories (a simple reconstruction task). These features are then fed back to any base forecaster (e.g., concatenation with inputs or feature augmentation). The framework is architecture-agnostic and lightweight, serving as a plug-in alignment module.

Result: Empirical evaluation across eight benchmarks shows superior performance. Gains primarily stem from correcting frequency mismatches between historical inputs and future outputs. The authors provide a theoretical justification that TimeAlign increases the mutual information between learned representations and predicted targets. The approach is easy to integrate and incurs negligible overhead; code is publicly available.

Conclusion: TimeAlign offers a general, low-overhead alignment module that can enhance modern deep learning time-series forecasting systems across diverse forecasters and tasks.

Abstract: Representation learning techniques like contrastive learning have long been
explored in time series forecasting, mirroring their success in computer vision
and natural language processing. Yet recent state-of-the-art (SOTA) forecasters
seldom adopt these representation approaches because they have shown little
performance advantage. We challenge this view and demonstrate that explicit
representation alignment can supply critical information that bridges the
distributional gap between input histories and future targets. To this end, we
introduce TimeAlign, a lightweight, plug-and-play framework that learns
auxiliary features via a simple reconstruction task and feeds them back to any
base forecaster. Extensive experiments across eight benchmarks verify its
superior performance. Further studies indicate that the gains arises primarily
from correcting frequency mismatches between historical inputs and future
outputs. We also provide a theoretical justification for the effectiveness of
TimeAlign in increasing the mutual information between learned representations
and predicted targets. As it is architecture-agnostic and incurs negligible
overhead, TimeAlign can serve as a general alignment module for modern deep
learning time-series forecasting systems. The code is available at
https://github.com/TROUBADOUR000/TimeAlign.

</details>


### [132] [A Variational Framework for Residual-Based Adaptivity in Neural PDE Solvers and Operator Learning](https://arxiv.org/abs/2509.14198)
*Juan Diego Toscano,Daniel T. Chen,Vivek Oommen,George Em Karniadakis*

Main category: cs.LG

TL;DR: A unifying variational framework for residual-based adaptivity using convex transformations of the residual, linking adaptive weighting to sampling-distribution optimization; enables norm-aware design, reduces discretization error via loss-variance reduction, and improves learning dynamics, with extensions to operator learning and theoretical grounding.


<details>
  <summary>Details</summary>
Motivation: To address the largely heuristic nature of residual-based adaptive strategies and provide a principled, unifying framework that connects discretization choices, error metrics, and learning dynamics across norms.

Method: Formulate residuals through convex transformations to define distinct objective functionals (e.g., exponential weights for uniform error, linear weights for quadratic error). Show adaptive weighting equals selecting sampling distributions that optimize the primal objective, enabling principled discretization and training. Extend the framework to operator learning and validate across optimizers and architectures.

Result: Provides a theoretical justification for residual-based adaptivity, demonstrates variance-reduced loss estimators, and yields improved gradient signal-to-noise; reports substantial performance gains across optimizers and architectures in operator learning tasks.

Conclusion: Establishes a foundation for principled discretization and training strategies, enabling systematic, norm-aware adaptive schemes and broad applicability in numerical analysis and machine learning.

Abstract: Residual-based adaptive strategies are widely used in scientific machine
learning but remain largely heuristic. We introduce a unifying variational
framework that formalizes these methods by integrating convex transformations
of the residual. Different transformations correspond to distinct objective
functionals: exponential weights target the minimization of uniform error,
while linear weights recover the minimization of quadratic error. Within this
perspective, adaptive weighting is equivalent to selecting sampling
distributions that optimize the primal objective, thereby linking
discretization choices directly to error metrics. This principled approach
yields three benefits: (1) it enables systematic design of adaptive schemes
across norms, (2) reduces discretization error through variance reduction of
the loss estimator, and (3) enhances learning dynamics by improving the
gradient signal-to-noise ratio. Extending the framework to operator learning,
we demonstrate substantial performance gains across optimizers and
architectures. Our results provide a theoretical justification of
residual-based adaptivity and establish a foundation for principled
discretization and training strategies.

</details>


### [133] [A Universal Banach--Bregman Framework for Stochastic Iterations: Unifying Stochastic Mirror Descent, Learning and LLM Training](https://arxiv.org/abs/2509.14216)
*Johnny R. Zhang,Xiaomei Mi,Gaoyuan Du,Qianyi Sun,Shiqi Wang,Jiaxuan Li,Wenhua Zhou*

Main category: cs.LG

TL;DR: Presents a Banach–Bregman stochastic optimization framework that extends beyond Hilbert spaces, unifying mirror-descent, natural-gradient, and related methods via Bregman geometry, with both theoretical convergence guarantees and practical empirical gains.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Hilbert-space–only theory for non-Euclidean optimization in AI (e.g., mirror descent on simplices, Bregman proximal methods, natural gradient, KL-regularized language model training) and to provide a unifying, geometrically flexible framework.

Method: Develops a general Banach–Bregman framework built on Bregman projections and Bregman–Fejér monotonicity; shows super-relaxations with λ>2; demonstrates applicability to stochastic approximation, mirror descent, natural-gradient, adaptive methods, and mirror-prox; provides convergence theorems from almost-sure boundedness to geometric rates; validates on synthetic and real-world tasks.

Result: The theory yields convergence guarantees and geometric rates in non-Hilbert Banach settings; empirically, up to ~20% faster convergence, lower variance, and improved accuracy relative to classical baselines across tasks including UCI benchmarks, Transformer training, actor–critic RL, and distilGPT-2–based WikiText-2 experiments.

Conclusion: Banach–Bregman geometry offers a unified foundation for optimization in non-Euclidean spaces, enabling flexible geometries and accelerated convergence across core AI paradigms; it merges optimization theory with practical AI training in a broad, unifying framework.

Abstract: Stochastic optimization powers the scalability of modern artificial
intelligence, spanning machine learning, deep learning, reinforcement learning,
and large language model training. Yet, existing theory remains largely
confined to Hilbert spaces, relying on inner-product frameworks and
orthogonality. This paradigm fails to capture non-Euclidean settings, such as
mirror descent on simplices, Bregman proximal methods for sparse learning,
natural gradient descent in information geometry, or
Kullback--Leibler-regularized language model training. Unlike Euclidean-based
Hilbert-space methods, this approach embraces general Banach spaces. This work
introduces a pioneering Banach--Bregman framework for stochastic iterations,
establishing Bregman geometry as a foundation for next-generation optimization.
It (i) provides a unified template via Bregman projections and Bregman--Fejer
monotonicity, encompassing stochastic approximation, mirror descent, natural
gradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations
($\lambda > 2$) in non-Hilbert settings, enabling flexible geometries and
elucidating their acceleration effect; and (iii) delivers convergence theorems
spanning almost-sure boundedness to geometric rates, validated on synthetic and
real-world tasks. Empirical studies across machine learning (UCI benchmarks),
deep learning (e.g., Transformer training), reinforcement learning
(actor--critic), and large language models (WikiText-2 with distilGPT-2) show
up to 20% faster convergence, reduced variance, and enhanced accuracy over
classical baselines. These results position Banach--Bregman geometry as a
cornerstone unifying optimization theory and practice across core AI paradigms.

</details>


### [134] [Data Denoising and Derivative Estimation for Data-Driven Modeling of Nonlinear Dynamical Systems](https://arxiv.org/abs/2509.14219)
*Jiaqi Yao,Lewis Mitchell,John Maclean,Hemanth Saratchandran*

Main category: cs.LG

TL;DR: A noise-robust framework (RKTV-INR) denoises trajectories via an implicit neural representation constrained by Runge-Kutta integration and total variation, then uses the cleaned states/derivatives with SINDy to identify governing equations.


<details>
  <summary>Details</summary>
Motivation: Measurement noise corrupts both state trajectories and their derivatives, hindering accurate dynamical system identification. A dynamics-consistent denoising method that yields clean trajectories and reliable derivatives is needed to enable data-driven discovery.

Method: Fit an implicit neural representation (INR) to noisy observations of the state. Impose Runge-Kutta integration constraints and total-variation regularization to ensure the inferred trajectory adheres to a dynamical system and remains close to the data. Use automatic differentiation on the trained INR to obtain clean states and first-order derivatives, then apply Sparse Identification of Nonlinear Dynamics (SINDy) to recover governing equations from the denoised quantities.

Result: The approach achieves effective noise suppression, yields accurate derivative estimates, and enables reliable discovery of governing dynamics across tested systems.

Conclusion: RKTV-INR provides a robust, differentiable denoising pipeline that preserves dynamical structure, facilitating accurate data-driven discovery with SINDy under measurement noise.

Abstract: Data-driven modeling of nonlinear dynamical systems is often hampered by
measurement noise. We propose a denoising framework, called Runge-Kutta and
Total Variation Based Implicit Neural Representation (RKTV-INR), that
represents the state trajectory with an implicit neural representation (INR)
fitted directly to noisy observations. Runge-Kutta integration and total
variation are imposed as constraints to ensure that the reconstructed state is
a trajectory of a dynamical system that remains close to the original data. The
trained INR yields a clean, continuous trajectory and provides accurate
first-order derivatives via automatic differentiation. These denoised states
and derivatives are then supplied to Sparse Identification of Nonlinear
Dynamics (SINDy) to recover the governing equations. Experiments demonstrate
effective noise suppression, precise derivative estimation, and reliable system
identification.

</details>


### [135] [Language models' activations linearly encode training-order recency](https://arxiv.org/abs/2509.14223)
*Dmitrii Krasheninnikov,Richard E. Turner,David Krueger*

Main category: cs.LG

TL;DR: LM activations encode when information was learned; training-time signals can be read linearly.


<details>
  <summary>Details</summary>
Motivation: Understand how models store temporal information about acquired data and implications for data conflicts and knowledge updates.

Method: Create a model with known training order by sequentially fine-tuning Llama-3.2-1B on six disjoint named-entity datasets; analyze test activations; apply 2D projection; train linear probes on early vs late; fine-tune to report training stage.

Result: Average test activations across datasets form a linear arrangement corresponding to training order; linear probes distinguish early vs late entities at ~90% accuracy and generalize to unseen entities; model can be fine-tuned to report training stage at ~80% accuracy; temporal signal is not due to simple magnitude/loss/confidence.

Conclusion: Models encode acquisition time in their representations; implications for data conflict resolution and updating knowledge; demonstrates temporal introspection potential and limitations.

Abstract: We show that language models' activations linearly encode when information
was learned during training. Our setup involves creating a model with a known
training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but
otherwise similar datasets about named entities. We find that the average
activations of test samples for the six training datasets encode the training
order: when projected into a 2D subspace, these centroids are arranged exactly
in the order of training and lie on a straight line. Further, we show that
linear probes can accurately (~90%) distinguish "early" vs. "late" entities,
generalizing to entities unseen during the probes' own training. The model can
also be fine-tuned to explicitly report an unseen entity's training stage (~80%
accuracy). Interestingly, this temporal signal does not seem attributable to
simple differences in activation magnitudes, losses, or model confidence. Our
paper demonstrates that models are capable of differentiating information by
its acquisition time, and carries significant implications for how they might
manage conflicting data and respond to knowledge modifications.

</details>


### [136] [Defending Diffusion Models Against Membership Inference Attacks via Higher-Order Langevin Dynamics](https://arxiv.org/abs/2509.14225)
*Benjamin Sterling,Yousef El-Laham,Mónica F. Bugallo*

Main category: cs.LG

TL;DR: Proposes a defense against membership inference attacks on diffusion models using critically-damped higher-order Langevin dynamics with auxiliary variables; theoretical analysis and empirical validation on toy and speech datasets using AUROC and FID.


<details>
  <summary>Details</summary>
Motivation: To protect training data privacy in diffusion models against membership inference attacks, noting that diffusion models are not immune despite inherent resistance.

Method: Introduce critically-damped higher-order Langevin dynamics that add auxiliary variables and a joint diffusion process along these variables, causing extra randomness to be mixed into the diffusion steps to obscure sensitive inputs.

Result: The approach is theoretically analyzed and empirically validated on a toy dataset and a speech dataset; evaluation uses AUROC for inference attack performance and FID for data fidelity/quality, indicating the defense's effectiveness.

Conclusion: The proposed auxiliary-variable Langevin-dynamics defense is a viable approach to mitigating membership inference risks in diffusion models, with demonstrated theoretical and empirical support; further work could explore broader datasets and trade-offs.

Abstract: Recent advances in generative artificial intelligence applications have
raised new data security concerns. This paper focuses on defending diffusion
models against membership inference attacks. This type of attack occurs when
the attacker can determine if a certain data point was used to train the model.
Although diffusion models are intrinsically more resistant to membership
inference attacks than other generative models, they are still susceptible. The
defense proposed here utilizes critically-damped higher-order Langevin
dynamics, which introduces several auxiliary variables and a joint diffusion
process along these variables. The idea is that the presence of auxiliary
variables mixes external randomness that helps to corrupt sensitive input data
earlier on in the diffusion process. This concept is theoretically investigated
and validated on a toy dataset and a speech dataset using the Area Under the
Receiver Operating Characteristic (AUROC) curves and the FID metric.

</details>


### [137] [NIRVANA: Structured pruning reimagined for large language models compression](https://arxiv.org/abs/2509.14230)
*Mengting Ai,Tianxin Wei,Sirui Chen,Jingrui He*

Main category: cs.LG

TL;DR: NIRVANA is a structured pruning method for LLMs that preserves zero-shot accuracy and enables robust fine-tuning by using a Neural Tangent Kernel-based first-order saliency, adaptive layer/module sparsity allocation, and KL-based calibration data selection; it outperforms prior pruning methods under equal sparsity on Llama3, Qwen, and T5.


<details>
  <summary>Details</summary>
Motivation: Current structured pruning approaches often degrade performance, especially in zero-shot settings, and rely on costly recovery techniques such as supervised fine-tuning or adapters. There is a need for pruning that preserves immediate zero-shot accuracy while maintaining strong fine-tuning capability.

Method: NIRVANA uses a first-order saliency criterion derived from the Neural Tangent Kernel under Adam optimization dynamics to guide pruning. It includes an adaptive sparsity allocation mechanism across layers and modules (attention vs. MLP) for globally balanced pruning, and a KL-divergence-based calibration data selection strategy to reduce sensitivity to calibration data quality.

Result: Experiments on Llama3, Qwen, and T5 show that NIRVANA surpasses existing structured pruning methods under equivalent sparsity constraints, offering a theoretically grounded and practical approach to LLM compression.

Conclusion: NIRVANA provides a theoretically grounded pruning framework that balances immediate zero-shot accuracy preservation with robust fine-tuning capability, addressing calibration sensitivity and delivering improved compression performance.

Abstract: Structured pruning of large language models (LLMs) offers substantial
efficiency improvements by removing entire hidden units, yet current approaches
often suffer from significant performance degradation, particularly in
zero-shot settings, and necessitate costly recovery techniques such as
supervised fine-tuning (SFT) or adapter insertion. To address these critical
shortcomings, we introduce NIRVANA, a novel pruning method explicitly designed
to balance immediate zero-shot accuracy preservation with robust fine-tuning
capability. Leveraging a first-order saliency criterion derived from the Neural
Tangent Kernel under Adam optimization dynamics, NIRVANA provides a
theoretically grounded pruning strategy that respects essential model training
behaviors. To further address the unique challenges posed by structured
pruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across
layers and modules (attention vs. MLP), which adjusts pruning intensity between
modules in a globally balanced manner. Additionally, to mitigate the high
sensitivity of pruning decisions to calibration data quality, we propose a
simple yet effective KL divergence-based calibration data selection strategy,
ensuring more reliable and task-agnostic pruning outcomes. Comprehensive
experiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA
outperforms existing structured pruning methods under equivalent sparsity
constraints, providing a theoretically sound and practical approach to LLM
compression. The code is available at
https://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.

</details>


### [138] [Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision](https://arxiv.org/abs/2509.14234)
*Dulhan Jayalath,Shashwat Goel,Thomas Foster,Parag Jain,Suchin Gururangan,Cheng Zhang,Anirudh Goyal,Alan Schelten*

Main category: cs.LG

TL;DR: CaT turns a model’s own inference-time exploration into self-supervision by synthesizing a reference from multiple rollouts using a frozen anchor, creating reward signals for both verifiable and non-verifiable tasks. It improves several LLMs at test time, and further gains are achieved with CaT-RL, sometimes surpassing the teacher signal itself.


<details>
  <summary>Details</summary>
Motivation: Ground-truth data is unavailable or expensive during post-training. There is a need for reference-free, self-generated supervision that can guide learning from a model’s own exploration, especially for non-verifiable tasks where standard accuracy-based signals are hard to obtain.

Method: Generate a group of rollouts from the current policy at test-time. A frozen anchor (the initial policy) reconciles omissions and contradictions to produce a reference. Optimize toward this synthesized reference. Two reward regimes: (i) verifiable tasks use programmatic equivalence on final answers; (ii) non-verifiable tasks use self-proposed rubrics scored by an independent LLM judge, with rewards equal to the fraction satisfied. The method scales with the number of rollouts and can be applied as a test-time procedure. A reinforcement-learning variant (CaT-RL) updates the policy directly, potentially surpassing the teacher signal.

Result: Empirical gains across models: Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B show improvements (up to +27% on MATH-500 and +12% on HealthBench). With CaT-RL, further gains (up to +33% and +30%) are reported, with the trained policy surpassing the initial teacher signal.

Conclusion: Synthesis-based supervision provides a robust, reference-free teacher signal that leverages model exploration, scales with rollout count, and yields meaningful gains in both static inference and RL settings. It can outperform traditional selection-based approaches and offers a path for post-training improvement even without ground-truth data.

Abstract: Where do learning signals come from when there is no ground truth in
post-training? We propose turning exploration into supervision through Compute
as Teacher (CaT), which converts the model's own exploration at inference-time
into reference-free supervision by synthesizing a single reference from a group
of parallel rollouts and then optimizing toward it. Concretely, the current
policy produces a group of rollouts; a frozen anchor (the initial policy)
reconciles omissions and contradictions to estimate a reference, turning extra
inference-time compute into a teacher signal. We turn this into rewards in two
regimes: (i) verifiable tasks use programmatic equivalence on final answers;
(ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria
scored by an independent LLM judge, with reward given by the fraction
satisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge
scores), synthesis may disagree with the majority and be correct even when all
rollouts are wrong; performance scales with the number of rollouts. As a
test-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up
to +27% on MATH-500; +12% on HealthBench). With reinforcement learning
(CaT-RL), we obtain further gains (up to +33% and +30%), with the trained
policy surpassing the initial teacher signal.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [139] [Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness](https://arxiv.org/abs/2509.13332)
*Pratik Jayarao,Himanshu Gupta,Neeraj Varshney,Chaitanya Dwivedi*

Main category: cs.AI

TL;DR: Explicit reasoning (thinking) in LLMs substantially improves judge accuracy and robustness with modest cost, outperforming non-thinking approaches even with augmentation, across multilingual settings.


<details>
  <summary>Details</summary>
Motivation: As LLMs increasingly serve as automated judges in benchmarking and reward modeling, ensuring their reliability, efficiency, and robustness is critical; this work systematically compares thinking versus non-thinking LLMs on small open-source models to evaluate accuracy, efficiency (FLOPs), and robustness.

Method: Systematic comparison of thinking vs non-thinking LLMs using open-source Qwen 3 models (0.6B, 1.7B, 4B). Evaluate accuracy and FLOPs on RewardBench tasks. Test augmentation strategies for non-thinking models (in-context learning, rubric-guided judging, reference-based evaluation, and n-best aggregation). Conduct bias robustness analyses (positional, bandwagon, identity, diversity, and random biases). Extend experiments to multilingual settings to assess cross-lingual benefits of explicit reasoning.

Result: Thinking models achieve ~10 percentage points higher accuracy with modest overhead (under 2x FLOPs). Augmentation strategies for non-thinking models yield modest gains but require substantial cost (>8x FLOPs). Bias robustness analyses show thinking models are more consistent under various biases, averaging ~6% higher robustness. Results extend to multilingual settings, confirming benefits of explicit reasoning beyond English.

Conclusion: Explicit reasoning offers clear advantages in accuracy, efficiency, and robustness for LLMs acting as automated judges; prefer thinking models for benchmarking and reward-modeling tasks and consider their cross-lingual benefits.

Abstract: As Large Language Models (LLMs) are increasingly adopted as automated judges
in benchmarking and reward modeling, ensuring their reliability, efficiency,
and robustness has become critical. In this work, we present a systematic
comparison of "thinking" and "non-thinking" LLMs in the LLM-as-a-judge paradigm
using open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B
parameters). We evaluate both accuracy and computational efficiency (FLOPs) on
RewardBench tasks, and further examine augmentation strategies for non-thinking
models, including in-context learning, rubric-guided judging, reference-based
evaluation, and n-best aggregation. Our results show that despite these
enhancements, non-thinking models generally fall short of their thinking
counterparts. Our results show that thinking models achieve approximately 10%
points higher accuracy with little overhead (under 2x), in contrast to
augmentation strategies like few-shot learning, which deliver modest gains at a
higher cost (>8x). Bias and robustness analyses further demonstrate that
thinking models maintain significantly greater consistency under a variety of
bias conditions such as positional, bandwagon, identity, diversity, and random
biases (6% higher on average). We further extend our experiments to the
multilingual setting and our results confirm that explicit reasoning extends
its benefits beyond English. Overall, our work results in several important
findings that provide systematic evidence that explicit reasoning offers clear
advantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency
but also in robustness.

</details>


### [140] [Evaluation Awareness Scales Predictably in Open-Weights Large Language Models](https://arxiv.org/abs/2509.13333)
*Maheep Chaudhary,Ian Su,Nikhil Hooda,Nishith Shankar,Julia Tan,Kevin Zhu,Ashwinee Panda,Ryan Lagasse,Vasu Sharma*

Main category: cs.AI

TL;DR: Power-law scaling of evaluation awareness across 15 model sizes (0.27B–70B) reveals predictable deceptive behavior as models grow; enables forecasting and scale-aware evaluations.


<details>
  <summary>Details</summary>
Motivation: Evaluation awareness undermines AI safety tests by enabling models to hide dangerous capabilities during evaluation. Understanding scaling helps design robust safety assessments for larger models.

Method: Analyzed 15 models across four families, sizes 0.27B to 70B; used linear probing on steering vector activations to detect evaluation-aware masking.

Result: Found a clear power-law relationship: evaluation awareness increases predictably with model size across the range.

Conclusion: Scaling laws enable forecasting of deception in future models and inform the design of scale-aware AI safety evaluations.

Abstract: Large language models (LLMs) can internally distinguish between evaluation
and deployment contexts, a behaviour known as \emph{evaluation awareness}. This
undermines AI safety evaluations, as models may conceal dangerous capabilities
during testing. Prior work demonstrated this in a single $70$B model, but the
scaling relationship across model sizes remains unknown. We investigate
evaluation awareness across $15$ models scaling from $0.27$B to $70$B
parameters from four families using linear probing on steering vector
activations. Our results reveal a clear power-law scaling: evaluation awareness
increases predictably with model size. This scaling law enables forecasting
deceptive behavior in future larger models and guides the design of scale-aware
evaluation strategies for AI safety. A link to the implementation of this paper
can be found at
https://anonymous.4open.science/r/evaluation-awareness-scaling-laws/README.md.

</details>


### [141] [FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness](https://arxiv.org/abs/2509.13334)
*Anand Swaroop,Akshat Nallani,Saksham Uboweja,Adiliia Uzdenova,Michael Nguyen,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma,Maheep Chaudhary*

Main category: cs.AI

TL;DR: A scalable, supervision-free method (FRIT) to improve faithful chain-of-thought reasoning by intervening on CoT steps and optimizing preferences toward causal consistency, yielding better faithfulness and accuracy.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought reasoning often fails to causally influence the final answer, making outputs brittle and untrustworthy. Prior work mainly measures faithfulness; there is a need for scalable methods to train models to reason faithfully.

Method: Generate synthetic training data by intervening on individual reasoning steps in model-generated CoTs to create faithful/unfaithful pairs. Use Direct Preference Optimization to train models to prefer causally consistent reasoning paths. Evaluated on Qwen3-8B and Mistral-7B-v0.1 across factual and symbolic tasks.

Result: FRIT increases faithful reasoning by 3.4 percentage points for Mistral on GSM8K and improves accuracy by 7.6 percentage points. Demonstrates scalability and supervision-free training; first scalable method to train LMs for more reliable reasoning; code released.

Conclusion: FRIT provides a scalable solution to align language models toward faithful, interpretable reasoning and narrows the gap between reasoning performance and trustworthiness.

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving
large language model performance on complex tasks, but recent work shows that
reasoning steps often fail to causally influence the final answer, creating
brittle and untrustworthy outputs. Prior approaches focus primarily on
measuring faithfulness, while methods for systematically improving it remain
limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a
scalable alignment method that trains models to produce causally consistent
reasoning by learning from systematically corrupted examples. FRIT generates
synthetic training data by intervening on individual reasoning steps in
model-generated CoTs, creating faithful/unfaithful pairs that highlight when
reasoning breaks down. We then apply Direct Preference Optimization to teach
models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B
and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases
faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while
improving accuracy by $7.6$ percentage points. Our approach provides the first
scalable, supervision-free method for training language models to produce more
reliable and interpretable reasoning, addressing a critical gap between
reasoning performance and trustworthiness. We release our code at
\href{https://github.com/Anut-py/frit}.

</details>


### [142] [Position: AI Safety Must Embrace an Antifragile Perspective](https://arxiv.org/abs/2509.13339)
*Ming Jin,Hyunin Lee*

Main category: cs.AI

TL;DR: Advocates antifragile AI safety to prepare for future rare/out-of-distribution events by reformulating benchmarks and measurement, beyond static testing.


<details>
  <summary>Details</summary>
Motivation: Static tests do not capture evolving environments; models can drift into maladaptation such as reward hacking, over-optimization, or capability atrophy; need for long-term reliability of open-ended ML systems.

Method: A position paper that analyzes limitations of static testing (scenario diversity, reward hacking, over-alignment) and argues for an antifragile framework; outlines how to measure, benchmark, and improve AI safety over the long term; proposes ethical/practical guidelines and community-building for antifragile safety.

Result: Proposes antifragile safety solutions and a recalibration of safety measurement and benchmarking; offers guidelines to complement robustness approaches and foster an antifragile AI safety community.

Conclusion: Long-term AI safety requires antifragility; evolving environments demand adaptive, uncertainty-embracing approaches and a reworked benchmarking/measurement paradigm to sustain reliability.

Abstract: This position paper contends that modern AI research must adopt an
antifragile perspective on safety -- one in which the system's capacity to
guarantee long-term AI safety such as handling rare or out-of-distribution
(OOD) events expands over time. Conventional static benchmarks and single-shot
robustness tests overlook the reality that environments evolve and that models,
if left unchallenged, can drift into maladaptation (e.g., reward hacking,
over-optimization, or atrophy of broader capabilities). We argue that an
antifragile approach -- Rather than striving to rapidly reduce current
uncertainties, the emphasis is on leveraging those uncertainties to better
prepare for potentially greater, more unpredictable uncertainties in the future
-- is pivotal for the long-term reliability of open-ended ML systems. In this
position paper, we first identify key limitations of static testing, including
scenario diversity, reward hacking, and over-alignment. We then explore the
potential of antifragile solutions to manage rare events. Crucially, we
advocate for a fundamental recalibration of the methods used to measure,
benchmark, and continually improve AI safety over the long term, complementing
existing robustness approaches by providing ethical and practical guidelines
towards fostering an antifragile AI safety community.

</details>


### [143] [Imagined Autocurricula](https://arxiv.org/abs/2509.13341)
*Ahmet H. Güzel,Matthew Thomas Jackson,Jarek Luca Liesen,Tim Rocktäschel,Jakob Nicolaus Foerster,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.AI

TL;DR: IMAC uses world models and unsupervised environment design to imagine diverse training environments and automatically curricula, enabling agents trained in a model to generalize to unseen tasks.


<details>
  <summary>Details</summary>
Motivation: Real-world embodied agents require vast data or accurate simulators; world models offer a data-efficient way to generate diverse imagined worlds from offline data, but curricula are needed to ensure useful training signal.

Method: Train agents inside a world model that generates imagined environments; apply Unsupervised Environment Design (UED) to create an automatic curriculum over these generated worlds (Imagined Autocurricula, IMAC). Evaluate transfer to held-out, procedurally generated tasks.

Result: Agents trained within the imagined worlds with IMAC achieve strong transfer performance on held-out environments, despite training only on data from a narrower dataset used to learn the world model.

Conclusion: Imagined Autocurricula enable robust generalization and suggest a scalable path toward larger foundation world models capable of training generally capable agents.

Abstract: Training agents to act in embodied environments typically requires vast
training data or access to accurate simulation, neither of which exists for
many cases in the real world. Instead, world models are emerging as an
alternative leveraging offline, passively collected data, they make it possible
to generate diverse worlds for training agents in simulation. In this work, we
harness world models to generate imagined environments to train robust agents
capable of generalizing to novel task variations. One of the challenges in
doing this is ensuring the agent trains on useful generated data. We thus
propose a novel approach, IMAC (Imagined Autocurricula), leveraging
Unsupervised Environment Design (UED), which induces an automatic curriculum
over generated worlds. In a series of challenging, procedurally generated
environments, we show it is possible to achieve strong transfer performance on
held-out environments, having trained only inside a world model learned from a
narrower dataset. We believe this opens the path to utilizing larger-scale,
foundation world models for generally capable agents.

</details>


### [144] [OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft](https://arxiv.org/abs/2509.13347)
*Zihao Wang,Muyao Li,Kaichen He,Xiangyu Wang,Zhancun Mu,Anji Liu,Yitao Liang*

Main category: cs.AI

TL;DR: A framework called Chain of Action (CoA) unifies high-level planning with low-level control in Vision-Language-Action models to handle abstract action spaces, enabling an All-in-One agent that achieves state-of-the-art on 800 Minecraft tasks via the OpenHA suite.


<details>
  <summary>Details</summary>
Motivation: Action-space choice is a critical, unresolved challenge; no single abstraction universally works across tasks, hindering generalist agent development.

Method: Introduce Chain of Action (CoA) where an abstracted action serves as an intermediate reasoning step (like chain-of-thought) guiding the final executable action. Train an All-in-One agent across diverse action spaces within CoA, and release OpenHA (Open Hierarchical Agents) benchmark with 800+ tasks, datasets, code, and pretrained checkpoints.

Result: Achieves state-of-the-art performance, with improved overall task success rates compared to strong specialized baselines. The All-in-One agent trained under CoA demonstrates robustness and generalization across a broad set of tasks.

Conclusion: CoA provides a unified, scalable framework that treats action abstractions as intermediate reasoning steps, improving generalization for VLA agents; open-source OpenHA resources facilitate reproducible research and benchmarking.

Abstract: The choice of action spaces is a critical yet unresolved challenge in
developing capable, end-to-end trainable agents. This paper first presents a
large-scale, systematic comparison of prominent abstracted action spaces and
tokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the
open-ended Minecraft. Our analysis reveals that no single action space is
universally optimal; instead, the most effective abstraction is highly
task-dependent, creating a dilemma for building generalist agents. To resolve
this, we introduce Chain of Action (CoA), a novel framework that unifies
high-level planning and low-level control within a single, monolithic VLA
model. CoA treats an abstracted action not as a command for a separate policy,
but as an intermediate reasoning step--akin to a chain of thought--that guides
the generation of the final, executable action. Furthermore, we demonstrate
that an All-in-One agent trained on a diverse mixture of action spaces using
the CoA paradigm learns a more robust and generalizable policy. This unified
agent achieves a new state-of-the-art, improving the overall task success rate
over strong, specialized baselines. To foster reproducible research, we release
the OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive
benchmark of over 800 distinct tasks, curated datasets, source code, and all
pretrained model checkpoints at https://github.com/CraftJarvis/OpenHA

</details>


### [145] [Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning](https://arxiv.org/abs/2509.13351)
*Pulkit Verma,Ngoc La,Anthony Favier,Swaroop Mishra,Julie A. Shah*

Main category: cs.AI

TL;DR: Instruction-tuning with PDDL-Instruct endows LLMs with explicit logical chain-of-thought reasoning for symbolic planning, achieving up to 94% planning accuracy and a 66% absolute gain over baselines.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with formal, symbolic planning that requires PDDL-like representations. There is a need to teach rigorous precondition checking, state transitions, effects, and invariants via explicit reasoning steps to improve planning reliability.

Method: Develop instruction prompts that guide models through logical inference steps: (1) assess action preconditions, (2) determine applicability in the current state, (3) apply state transitions and effects, (4) verify invariants and plan validity, and (5) reflect and self-correct through structured reasoning chains. Decompose planning into explicit reasoning chains and evaluative verification, evaluated across standard planning domains.

Result: Models tuned with chain-of-thought reasoning achieve planning accuracy up to 94% on standard planning benchmarks, representing a 66 percentage-point absolute improvement over baselines and showing robust gains across multiple domains.

Conclusion: The framework bridges general LLM reasoning with the logical precision required for automated planning, demonstrating that structured reasoning and verification can significantly enhance symbolic planning capabilities in LLMs and offering a promising direction for AI planning systems.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, yet their ability to perform structured symbolic planning
remains limited, particularly in domains requiring formal representations like
the Planning Domain Definition Language (PDDL). In this paper, we present a
novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'
symbolic planning capabilities through logical chain-of-thought reasoning. Our
approach focuses on teaching models to rigorously reason about action
applicability, state transitions, and plan validity using explicit logical
inference steps. By developing instruction prompts that guide models through
the precise logical reasoning required to determine when actions can be applied
in a given state, we enable LLMs to self-correct their planning processes
through structured reflection. The framework systematically builds verification
skills by decomposing the planning process into explicit reasoning chains about
precondition satisfaction, effect application, and invariant preservation.
Experimental results on multiple planning domains show that our
chain-of-thought reasoning based instruction-tuned models are significantly
better at planning, achieving planning accuracy of up to 94% on standard
benchmarks, representing a 66% absolute improvement over baseline models. This
work bridges the gap between the general reasoning capabilities of LLMs and the
logical precision required for automated planning, offering a promising
direction for developing better AI planning systems.

</details>


### [146] [Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning](https://arxiv.org/abs/2509.13352)
*Anis Koubaa,Khaled Gabr*

Main category: cs.AI

TL;DR: Five-layer Agentic UAVs architecture uses LLM-assisted reasoning and tool-calling to boost autonomy, demonstrated with a ROS2/Gazebo prototype achieving better detection and action in simulated SAR tasks.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of current UAVs, which are mainly SAE Level 2–3 with rule-based control and narrow AI, lacking context-aware reasoning, autonomous decision-making, and ecosystem-wide integration; none leverage LLM agents with tool-calling for real-time knowledge access.

Method: Proposes a five-layer architecture (Perception, Reasoning, Action, Integration, Learning) for UAVs augmented by LLM-driven reasoning, database querying, and third-party system interaction. Implements a ROS2 and Gazebo-based prototype that integrates YOLOv11 detection with GPT-4 reasoning and local Gemma-3 deployment; uses LLM tool-calling to access knowledge in real time; evaluated in simulated search-and-rescue scenarios.

Result: In simulations, agentic UAVs showed higher detection confidence (0.79 vs. 0.72), improved person detection rate (91% vs. 75%), and much higher action recommendation rate (92% vs. 4.5%).

Conclusion: LLM-driven agents with tool-calling can enable qualitatively new levels of autonomy and ecosystem integration in UAVs with modest computational overhead.

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,
surveillance, and disaster response, yet most systems remain confined to SAE
Level 2--3 autonomy. Their reliance on rule-based control and narrow AI
restricts adaptability in dynamic, uncertain missions. Existing UAV frameworks
lack context-aware reasoning, autonomous decision-making, and ecosystem-level
integration; critically, none leverage Large Language Model (LLM) agents with
tool-calling for real-time knowledge access. This paper introduces the Agentic
UAVs framework, a five-layer architecture (Perception, Reasoning, Action,
Integration, Learning) that augments UAVs with LLM-driven reasoning, database
querying, and third-party system interaction. A ROS2 and Gazebo-based prototype
integrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3
deployment. In simulated search-and-rescue scenarios, agentic UAVs achieved
higher detection confidence (0.79 vs. 0.72), improved person detection rates
(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).
These results confirm that modest computational overhead enables qualitatively
new levels of autonomy and ecosystem integration.

</details>


### [147] [Semantic Fusion with Fuzzy-Membership Features for Controllable Language Modelling](https://arxiv.org/abs/2509.13357)
*Yongchao Huang,Hassan Raza*

Main category: cs.AI

TL;DR: Introduces semantic fusion: a lightweight, parallel fuzzy-membership feature channel for Transformer LMs that encodes token-level semantics and fuses via a gated adapter; yields interpretable, controllable generation with small overhead.


<details>
  <summary>Details</summary>
Motivation: Address lack of interpretable token-level semantics and controllable generation in LMs while preserving efficiency and compatibility (tied embeddings).

Method: Attach a parallel semantic channel to each token: a vector of interpretable features (POS cues, shallow roles, boundary flags, sentiment polarity/strength) with values from differentiable membership functions. Fuse this semantic matrix into the LM via a gated adapter. Train with standard next-token loss plus an auxiliary loss to reconstruct the semantic features and a lightweight uniformizer regularizing adjective-class distributions.

Result: On a synthetic two-clause corpus with held-out adjectives for OOD control, semantic fusion improves perplexity and enables precise, user-controllable generation of polarity and punctuation, while keeping model simple and with small overhead; fully compatible with tied input-output embeddings.

Conclusion: Semantic fusion provides an interpretable pathway for conditioned NLG with minimal overhead, enabling controllable generation and a transparent token-level semantic representation within Transformer LMs.

Abstract: We propose semantic fusion, a lightweight scheme that augments a Transformer
language model (LM) with a parallel, fuzzy-membership feature channel that
encodes token-level semantics. Each token is represented by a vector of
interpretable features (e.g. part-of-speech cues, shallow roles, boundary
flags, sentiment polarity and strength) whose values are graded degrees from
differentiable membership functions (e.g. power kernels). These per-token
vectors form a sentence-level semantic matrix fused via a gated adapter into
the LM. Training uses standard next-token prediction, an auxiliary loss that
reconstructs the semantic features from hidden states, and a lightweight
uniformizer that regularizes adjective-class distributions. On a synthetic
two-clause corpus with held-out adjectives for out-of-distribution (OOD)
control, semantic fusion improves perplexity and enables precise,
user-controllable generation of polarity and punctuation while maintaining
model simplicity. This approach adds only small overhead, remains fully
compatible with tied input-output embeddings, and provides an interpretable
pathway for conditioned natural language generation.

</details>


### [148] [Asterisk Operator](https://arxiv.org/abs/2509.13364)
*Zixi Li*

Main category: cs.AI

TL;DR: Proposes the Asterisk Operator, a unified ASPP-based framework for abstract reasoning that enables local, parallel state evolution guided by implicit graphs, with proven convergence and universal approximation, validated on ARC2 and Conway's Life, and a distillation method achieving 100% ARC2 validation with 6M parameters.


<details>
  <summary>Details</summary>
Motivation: Address core challenges in neural-symbolic abstract reasoning by delivering a local, parallel, convergent, and scalable reasoning paradigm that can handle structured tasks efficiently while ensuring global coherence.

Method: Introduce the Asterisk Operator within Adjacency-Structured Parallel Propagation (ASPP). Formalize structured reasoning tasks as local, parallel state evolutions guided by implicit relational graphs. Provide mathematical proofs of maintaining local computational constraints while achieving global reasoning, and demonstrate convergence. Conduct rigorous analyses and experiments on ARC2 challenges and Conway's Game of Life. Develop Embedding-Asterisk distillation achieving high accuracy with a compact parameter footprint (6M parameters).

Result: The framework is shown to be universal and convergent with superior performance on abstract reasoning benchmarks, including ARC2 and Game of Life. The Embedding-Asterisk distillation method achieves 100% accuracy on ARC2 validation with 6M parameters, indicating a breakthrough in neural-symbolic reasoning efficiency.

Conclusion: The Asterisk Operator offers a convergent, efficient, and universally approximating paradigm for abstract reasoning, unifying local parallel propagation with global reasoning. It promises significant implications for neural-symbolic systems and opens avenues for further research into scalable, provably convergent reasoning architectures.

Abstract: We propose the \textbf{Asterisk Operator} ($\ast$-operator), a novel unified
framework for abstract reasoning based on Adjacency-Structured Parallel
Propagation (ASPP). The operator formalizes structured reasoning tasks as
local, parallel state evolution processes guided by implicit relational graphs.
We prove that the $\ast$-operator maintains local computational constraints
while achieving global reasoning capabilities, providing an efficient and
convergent computational paradigm for abstract reasoning problems. Through
rigorous mathematical analysis and comprehensive experiments on ARC2 challenges
and Conway's Game of Life, we demonstrate the operator's universality,
convergence properties, and superior performance. Our innovative
Embedding-Asterisk distillation method achieves 100\% accuracy on ARC2
validation with only 6M parameters, representing a significant breakthrough in
neural-symbolic reasoning.
  \textbf{Keywords:} Abstract Reasoning, Adjacency Structure, Parallel
Propagation, Asterisk Operator, Convergence, Universal Approximation

</details>


### [149] [$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation](https://arxiv.org/abs/2509.13368)
*Yuan Wei,Xiaohan Shan,Ran Miao,Jianmin Li*

Main category: cs.AI

TL;DR: Agent^2 is a fully automated RL agent-design framework with a dual-agent system (Generator Agent and Target Agent) that converts natural language task descriptions and environment code into executable, high-performance RL agents. It uses a two-stage MDP modeling and algorithmic optimization pipeline, under the Model Context Protocol, achieving up to 55% gains across diverse benchmarks (MuJoCo, MetaDrive, MPE, SMAC) and enabling end-to-end automation of agent design.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning development traditionally requires extensive expertise and long iteration cycles, leading to high failure rates and limited accessibility. Automating agent design from natural language inputs could democratize RL, accelerate development, and reduce engineering drift by automatically generating and optimizing agents.

Method: Proposes a dual-agent architecture: a Generator Agent that analyzes tasks and generates executable RL agents, and a Target Agent that embodies the generated RL agent. Development is split into two phases—MDP modeling and algorithmic optimization—within a unified framework called the Model Context Protocol. The system relies on intelligent feedback, adaptive training management, and end-to-end automation to transform natural language descriptions and environment code into high-performance RL solutions.

Result: Empirical evaluation across MuJoCo, MetaDrive, MPE, and SMAC shows that Agent^2 consistently outperforms manually designed solutions, achieving up to 55% performance improvements and meaningful average gains across tasks.

Conclusion: Agent^2 embodies a new paradigm where intelligent agents design and optimize other agents, enabling fully automated end-to-end RL agent design and optimization, with broad implications for automated AI systems and accessibility of RL development.

Abstract: Reinforcement learning agent development traditionally requires extensive
expertise and lengthy iterations, often resulting in high failure rates and
limited accessibility. This paper introduces $Agent^2$, a novel
agent-generates-agent framework that achieves fully automated RL agent design
through intelligent LLM-driven generation. The system autonomously transforms
natural language task descriptions and environment code into comprehensive,
high-performance reinforcement learning solutions without human intervention.
$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent
serves as an autonomous AI designer that analyzes tasks and generates
executable RL agents, while the Target Agent is the resulting automatically
generated RL agent. The framework decomposes RL development into two distinct
stages: MDP modeling and algorithmic optimization, enabling more targeted and
effective agent generation. Built on the Model Context Protocol, $Agent^2$
provides a unified framework that standardizes intelligent agent creation
across diverse environments and algorithms, while incorporating adaptive
training management and intelligent feedback analysis for continuous
improvement. Extensive experiments on a wide range of benchmarks, including
MuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently
outperforms manually designed solutions across all tasks, achieving up to 55%
performance improvement and substantial gains on average. By enabling truly
end-to-end, closed-loop automation, this work establishes a new paradigm in
which intelligent agents design and optimize other agents, marking a
fundamental breakthrough for automated AI systems.

</details>


### [150] [The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in VLMs](https://arxiv.org/abs/2509.13379)
*Asif Azad,Mohammad Sadat Hossain,MD Sadik Hossain Shanto,M Saifur Rahman,Md Rizwan Pervez*

Main category: cs.AI

TL;DR: A comprehensive uncertainty benchmarking study for vision-language models across 16 models, 6 datasets, and 3 scoring functions; larger models show better uncertainty quantification and higher accuracy, while math/reasoning domains exhibit weaker uncertainty performance.


<details>
  <summary>Details</summary>
Motivation: Uncertainty quantification in VLMs has been underexplored; need broad, rigorous benchmarking to enable reliable multimodal AI, beyond limited conformal-prediction settings.

Method: Evaluate 16 VLMs (open and closed-source) on 6 multimodal datasets using 3 scoring functions; analyze calibration, epistemic/aleatoric uncertainty, relation to accuracy; compare across model sizes and domains.

Result: Larger models yield better uncertainty quantification; models with higher knowledge are better at recognizing what they don't know; certainty correlates with accuracy; math/reasoning tasks show poorer uncertainty performance across all models.

Conclusion: Provides foundation for reliable uncertainty evaluation in multimodal systems and guides future work on scaling and domain-specific uncertainty handling.

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in complex
visual understanding across scientific and reasoning tasks. While performance
benchmarking has advanced our understanding of these capabilities, the critical
dimension of uncertainty quantification has received insufficient attention.
Therefore, unlike prior conformal prediction studies that focused on limited
settings, we conduct a comprehensive uncertainty benchmarking study, evaluating
16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets
with 3 distinct scoring functions. Our findings demonstrate that larger models
consistently exhibit better uncertainty quantification; models that know more
also know better what they don't know. More certain models achieve higher
accuracy, while mathematical and reasoning tasks elicit poorer uncertainty
performance across all models compared to other domains. This work establishes
a foundation for reliable uncertainty evaluation in multimodal systems.

</details>


### [151] [From Next Token Prediction to (STRIPS) World Models -- Preliminary Results](https://arxiv.org/abs/2509.13389)
*Carlos Núñez-Molina,Vicenç Gómez,Hector Geffner*

Main category: cs.AI

TL;DR: The paper shows transformers can learn propositional STRIPS world models from action traces by treating next-action prediction as supervised learning, using only positive and negative sequences.


<details>
  <summary>Details</summary>
Motivation: To learn symbolic action models from behavior traces without full state supervision, enabling data-driven planning with neural architectures.

Method: Use a transformer trained with gradient descent to predict the next action given a history of actions; an action a may follow a sequence if the hidden effects do not make its preconditions false; train on random valid (positive) and invalid (negative) action sequences; evaluate via experiments.

Result: A suitable transformer can faithfully represent propositional STRIPS world models; learning from positive/negative sequences is feasible; experiments corroborate the modeling capability.

Conclusion: Transformers provide a viable approach to learning symbolic action models from traces, supporting integration of planning and deep learning; future work could explore scalability and applicability to broader planning formalisms.

Abstract: We consider the problem of learning propositional STRIPS world models from
action traces alone, using a deep learning architecture (transformers) and
gradient descent. The task is cast as a supervised next token prediction
problem where the tokens are the actions, and an action $a$ may follow an
action sequence if the hidden effects of the previous actions do not make an
action precondition of $a$ false. We show that a suitable transformer
architecture can faithfully represent propositional STRIPS world models, and
that the models can be learned from sets of random valid (positive) and invalid
(negative) action sequences alone. A number of experiments are reported.

</details>


### [152] [SteeringControl: Holistic Evaluation of Alignment Steering in LLMs](https://arxiv.org/abs/2509.13450)
*Vincent Siu,Nicholas Crispino,David Park,Nathan W. Henry,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: SteeringControl shows that the effectiveness and risks of representation steering hinge on the trio of steering method, model, and targeted behavior; strong performance emerges only from favorable combinations, while poor choices can cause severe concept entanglement.


<details>
  <summary>Details</summary>
Motivation: To systematically study and quantify the tradeoffs of representation steering beyond obvious metrics like truthfulness, by jointly evaluating bias, harmful generation, and hallucination, along with secondary behaviors such as sycophancy and commonsense morality.

Method: Introduce SteeringControl, a modular benchmark and framework built from common steering components, and collect a safety-focused dataset of primary and secondary behaviors. Evaluate five steering methods across two models (Qwen-2.5-7B and Llama-3.1-8B) to map performance surfaces and behavioral entanglement.

Result: Steering performance depends on the specific combination of steering method, model, and behavior targeted; severe concept entanglement can arise from poor combinations. The benchmark enables systematic comparison and reveals nontrivial interactions.

Conclusion: Careful selection of method-model-behavior combinations is crucial to avoid entanglement and safely steer representations. The framework and dataset provide a path toward more nuanced, systematic alignment evaluations and benchmarking.

Abstract: We introduce SteeringControl, a benchmark for evaluating representation
steering methods across core alignment objectives--bias, harmful generation,
and hallucination--and their effects on secondary behaviors such as sycophancy
and commonsense morality. While prior alignment work often highlights
truthfulness or reasoning ability to demonstrate the side effects of
representation steering, we find there are many unexplored tradeoffs not yet
understood in a systematic way. We collect a dataset of safety-relevant primary
and secondary behaviors to evaluate steering effectiveness and behavioral
entanglement centered around five popular steering methods. To enable this, we
craft a modular steering framework based on unique components that serve as the
building blocks of many existing methods. Our results on Qwen-2.5-7B and
Llama-3.1-8B find that strong steering performance is dependent on the specific
combination of steering method, model, and targeted behavior, and that severe
concept entanglement can result from poor combinations of these three as well.
We release our code here:
https://github.com/wang-research-lab/SteeringControl.git.

</details>


### [153] [AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving](https://arxiv.org/abs/2509.13547)
*Harper Reed,Michael Sugimura,Angelo Zangari*

Main category: cs.AI

TL;DR: Collaborative tools inspired by human problem-solving improve LLM agent performance on hard tasks; substantial gains on hardest problems with 15-40% lower cost, 12-27% fewer turns, and 12-38% faster completion; effects are mixed on the full set; models adopt distinct tool-use strategies and writing-based articulation is a key driver; adaptive collaboration can be a reasoning enhancer at the edge of capability rather than a universal speedup.


<details>
  <summary>Details</summary>
Motivation: To test whether giving LLM agents human-like collaboration tools and autonomy can improve problem-solving performance, especially on difficult tasks, and to understand strategy differences across models.

Method: Equip Claude Code agents with MCP-based social media and journaling tools and allow unrestricted use across 34 Aider Polyglot Python challenges; compare performance to baseline agents; analyze strategy differences between Sonnet 3.7 and Sonnet 4; assess effects by problem difficulty and writing vs reading behavior.

Result: Substantial improvement on hardest problems: 15-40% lower cost, 12-27% fewer turns, 12-38% faster; full-challenge set effects are mixed; models show distinct collaborative strategies; agents prefer writing over reading by 2-9x, suggesting articulation-based scaffolding drives much of the improvement.

Conclusion: AI agents can benefit from human-inspired collaboration tools at the edge of their capabilities; adaptive collaborative interfaces may enhance reasoning rather than deliver universal efficiency gains; collaboration strategy should adapt to task complexity and model.

Abstract: We investigate whether giving LLM agents the collaborative tools and autonomy
that humans naturally use for problem solving can improve their performance. We
equip Claude Code agents with MCP-based social media and journaling tools and
allow them to use these tools as they see fit. Across 34 Aider Polyglot Python
programming challenges, collaborative tools substantially improve performance
on the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and
12-38% faster completion than baseline agents. Effects on the full challenge
set are mixed, suggesting these tools act as performance enhancers when
additional reasoning scaffolding is most needed. Surprisingly, Different models
naturally adopted distinct collaborative strategies without explicit
instruction. Sonnet 3.7 engaged broadly across tools and benefited from
articulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,
leaning on journal-based semantic search when problems were genuinely
difficult. This mirrors how human developers adjust collaboration based on
expertise and task complexity. Behavioral analysis shows agents prefer writing
over reading by about 2-9x, indicating that structured articulation drives much
of the improvement rather than information access alone. Overall, AI agents can
systematically benefit from human-inspired collaboration tools at the edge of
their capabilities, pointing to adaptive collaborative interfaces as reasoning
enhancers rather than universal efficiency boosts.

</details>


### [154] [Gen AI in Proof-based Math Courses: A Pilot Study](https://arxiv.org/abs/2509.13570)
*Hannah Klawa,Shraddha Rajpal,Cigole Thomas*

Main category: cs.AI

TL;DR: Mixed-methods study exploring student use, perceptions, and instructional implications of generative AI in three proof-based undergraduate math courses with permissive AI policies.


<details>
  <summary>Details</summary>
Motivation: Addresses the rise of generative AI in higher education, concerns about AI reliability, and the need for policies that support learning and critical thinking in proof-based mathematics.

Method: Survey responses and student interviews from three courses: a first-semester abstract algebra course, a topology course, and a second-semester abstract algebra course. Course policies permitted some use of generative AI. Analysis focuses on how students engaged with AI tools, their perceptions of AI's usefulness and limitations, and implications for teaching proof-based mathematics.

Result: The abstract reports engagement patterns and students’ perceptions of usefulness and limitations, with discussion of implications for teaching; it does not present specific quantitative results within the abstract.

Conclusion: Future considerations for integrating generative AI into proof-based mathematics instruction.

Abstract: With the rapid rise of generative AI in higher education and the
unreliability of current AI detection tools, developing policies that encourage
student learning and critical thinking has become increasingly important. This
study examines student use and perceptions of generative AI across three
proof-based undergraduate mathematics courses: a first-semester abstract
algebra course, a topology course and a second-semester abstract algebra
course. In each case, course policy permitted some use of generative AI.
Drawing on survey responses and student interviews, we analyze how students
engaged with AI tools, their perceptions of generative AI's usefulness and
limitations, and what implications these perceptions hold for teaching
proof-based mathematics. We conclude by discussing future considerations for
integrating generative AI into proof-based mathematics instruction.

</details>


### [155] [Programmable Cognitive Bias in Social Agents](https://arxiv.org/abs/2509.13588)
*Xuan Liu,Haoyang Shang,Haojian Jin*

Main category: cs.AI

TL;DR: CoBRA is a toolkit to specify and control agent behavior in LLM-based social simulations by grounding cognitive biases in classic social science experiments, enabling model-agnostic, precise bias programming via two components: a Cognitive Bias Index and a Behavioral Regulation Engine.


<details>
  <summary>Details</summary>
Motivation: Conventional natural language descriptions yield inconsistent agent behaviors across models and miss nuanced descriptions; there is a need for explicit, validated grounding of social-cognitive biases to reliably program agents.

Method: Two components: (1) Cognitive Bias Index, which quantifies an agent's cognitive bias through reactions to validated classical social science experiments; (2) Behavioral Regulation Engine, which aligns and constrains the agent's behavior to demonstrate a controlled cognitive bias. Evaluation via demonstration and technical benchmarks, in a model-agnostic setting.

Result: CoBRA enables precise programming of a social agent's cognitive bias in a model-agnostic way, achieving more consistent and well-grounded bias behavior across different models.

Conclusion: CoBRA offers a novel, systematic toolkit for specifying agent behavior in LLM-based social simulations by grounding cognitive biases in classical experiments, with demonstrated HCI utility and model-agnostic applicability.

Abstract: This paper introduces CoBRA, a novel toolkit for systematically specifying
agent behavior in LLM-based social simulation. We found that conventional
approaches that specify agent behaviors through implicit natural language
descriptions cannot yield consistent behaviors across models, and the produced
agent behaviors do not capture the nuances of the descriptions. In contrast,
CoBRA presents a new approach to program agents' cognitive biases explicitly,
by grounding agents' expected behaviors using classic social science
experiments. CoBRA has two components: (1) Cognitive Bias Index that measures
the cognitive bias of a social agent, by quantifying the agent's reactions in a
set of validated classical social science experiments; (2) Behavioral
Regulation Engine that aligns the agent's behavior to demonstrate controlled
cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and
technical benchmarks. Our results suggest that CoBRA can precisely program the
cognitive bias demonstrated in a social agent in a model-agnostic manner.

</details>


### [156] [See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles](https://arxiv.org/abs/2509.13615)
*Zongru Wu,Rui Mao,Zhiyuan Tian,Pengzhou Cheng,Tianjie Ju,Zheng Wu,Lingzhong Dong,Haiyue Sheng,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: StaR (State-aware Reasoning) improves toggle control execution in multimodal GUI agents by teaching them to observe current toggle states and reason about desired states from instructions, yielding over 30% accuracy gains on toggle tasks and better general task performance.


<details>
  <summary>Details</summary>
Motivation: Toggle controls in GUI present a reliability bottleneck for multimodal agents. Errors are especially common when the current toggle state already matches the desired state, indicating a need for state awareness and reasoning about instructions.

Method: StaR trains agents to perceive the current toggle state, extract the desired state from the instruction, and decide actions accordingly. It is evaluated on three multimodal agents and a state-control benchmark derived from public datasets, with code, benchmark, and StaR-enhanced agents released.

Result: Toggle instruction execution accuracy improved by over 30%. General task performance also improved across three public benchmarks. Evaluations in a dynamic environment indicate potential for real-world deployment.

Conclusion: StaR provides an effective, generalizable approach to integrating state awareness into multimodal agents for reliable GUI control, with open-source resources to reproduce and extend the work.

Abstract: The advent of multimodal agents facilitates effective interaction within
graphical user interface (GUI), especially in ubiquitous GUI control. However,
their inability to reliably execute toggle control instructions remains a key
bottleneck. To investigate this, we construct a state control benchmark with
binary toggle instructions from public datasets. Evaluations of existing agents
demonstrate their unreliability, particularly when the current toggle state
already matches the desired state. To address the challenge, we propose
State-aware Reasoning (StaR), a training method that teaches agents to perceive
the current toggle state, analyze the desired state from the instruction, and
act accordingly. Experiments on three multimodal agents demonstrate that StaR
can improve toggle instruction execution accuracy by over 30\%. Further
evaluations on three public benchmarks show that StaR also enhances general
task performance. Finally, evaluations on a dynamic environment highlight the
potential of StaR for real-world applications. Code, benchmark, and
StaR-enhanced agents are available at https://github.com/ZrW00/StaR.

</details>


### [157] [InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management](https://arxiv.org/abs/2509.13704)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: InfraMind is an exploration-based GUI agent framework tailored for industrial management (DCIM) that tackles five core challenges—unfamiliar elements, precision, state localization, deployment, and safety—via five modules; it outperforms existing frameworks in task success and efficiency across open-source and commercial DCIM platforms.


<details>
  <summary>Details</summary>
Motivation: Industrial data centers and other critical infrastructures rely on complex, multi-vendor management software. RPA needs high maintenance; general LLM GUI agents struggle with unfamiliar elements, precision, state localization, deployment, and safety in industrial contexts. A specialized, safe, scalable solution is needed.

Method: Introduce InfraMind with five integrated modules: (1) systematic search-based exploration using VM snapshots to autonomously understand complex GUIs; (2) memory-driven planning for precise, efficient task execution; (3) advanced state identification for robust localization in hierarchical interfaces; (4) structured knowledge distillation enabling deployment with lightweight models; (5) multi-layered safety mechanisms to safeguard sensitive operations. Evaluation conducted on open-source and commercial DCIM platforms, comparing task success rate and operational efficiency to existing frameworks.

Result: InfraMind consistently outperforms existing frameworks in task success rate and operational efficiency across both open-source and commercial DCIM platforms, demonstrating a rigorous and scalable solution for industrial management automation.

Conclusion: InfraMind provides a targeted, scalable solution that bridges the gap between general-purpose LLM GUI agents and the specific demands of industrial management, delivering reliable automation with safety and efficiency for critical infrastructure.

Abstract: Mission-critical industrial infrastructure, such as data centers,
increasingly depends on complex management software. Its operations, however,
pose significant challenges due to the escalating system complexity,
multi-vendor integration, and a shortage of expert operators. While Robotic
Process Automation (RPA) offers partial automation through handcrafted scripts,
it suffers from limited flexibility and high maintenance costs. Recent advances
in Large Language Model (LLM)-based graphical user interface (GUI) agents have
enabled more flexible automation, yet these general-purpose agents face five
critical challenges when applied to industrial management, including unfamiliar
element understanding, precision and efficiency, state localization, deployment
constraints, and safety requirements. To address these issues, we propose
InfraMind, a novel exploration-based GUI agentic framework specifically
tailored for industrial management systems. InfraMind integrates five
innovative modules to systematically resolve different challenges in industrial
management: (1) systematic search-based exploration with virtual machine
snapshots for autonomous understanding of complex GUIs; (2) memory-driven
planning to ensure high-precision and efficient task execution; (3) advanced
state identification for robust localization in hierarchical interfaces; (4)
structured knowledge distillation for efficient deployment with lightweight
models; and (5) comprehensive, multi-layered safety mechanisms to safeguard
sensitive operations. Extensive experiments on both open-source and commercial
DCIM platforms demonstrate that our approach consistently outperforms existing
frameworks in terms of task success rate and operational efficiency, providing
a rigorous and scalable solution for industrial management automation.

</details>


### [158] [THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning](https://arxiv.org/abs/2509.13761)
*Qikai Chang,Zhenrong Zhang,Pengfei Hu,Jiefeng Ma,Yicheng Pan,Jianshu Zhang,Jun Du,Quan Liu,Jianqing Gao*

Main category: cs.AI

TL;DR: THOR is a framework that integrates tools into LLM reasoning via TIRGen data generation, hierarchical RL optimization, and self-correction, achieving state-of-the-art results on math and code benchmarks and generalizing across model types.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with high-precision tasks; existing tool integration methods lack robust data construction, fine-grained optimization, and robust inference enhancements.

Method: Propose THOR: TIRGen multi-agent actor-critic pipeline to create tool-enabled reasoning data; an RL strategy for joint trajectory-level and step-level optimization; a self-correction mechanism using immediate tool feedback during inference.

Result: THOR generalizes across diverse models, improves both reasoning and non-reasoning models, achieves state-of-the-art performance on math benchmarks for similarly-sized models, and improves code benchmarks; code to be released.

Conclusion: THOR offers a unified approach addressing data construction, optimization, and inference in tool-enabled reasoning, yielding improved accuracy and generalization, with public release of code.

Abstract: Large Language Models (LLMs) have made remarkable progress in mathematical
reasoning, but still continue to struggle with high-precision tasks like
numerical computation and formal symbolic manipulation. Integrating external
tools has emerged as a promising approach to bridge this gap. Despite recent
advances, existing methods struggle with three key challenges: constructing
tool-integrated reasoning data, performing fine-grained optimization, and
enhancing inference. To overcome these limitations, we propose THOR
(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,
a multi-agent actor-critic-based pipeline for constructing high-quality
datasets of tool-integrated reasoning paths, aligning with the policy and
generalizing well across diverse models. Second, to perform fine-grained
hierarchical optimization, we introduce an RL strategy that jointly optimizes
for both trajectory-level problem solving and step-level code generation. This
is motivated by our key insight that the success of an intermediate tool call
is a strong predictor of the final answer's correctness. Finally, THOR
incorporates a self-correction mechanism that leverages immediate tool feedback
to dynamically revise erroneous reasoning paths during inference. Our approach
demonstrates strong generalization across diverse models, performing
effectively in both reasoning and non-reasoning models. It further achieves
state-of-the-art performance for models of a similar scale on multiple
mathematical benchmarks, while also delivering consistent improvements on code
benchmarks. Our code will be publicly available at
https://github.com/JingMog/THOR.

</details>


### [159] [MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based Instruction Recommendation](https://arxiv.org/abs/2509.13773)
*Zhipeng Bian,Jieming Zhu,Xuyang Xie,Quanyu Dai,Zhou Zhao,Zhenhua Dong*

Main category: cs.AI

TL;DR: MIRA provides one-touch task instruction recommendations on smartphones by combining an MLLM-based pipeline, template-guided reasoning, and constrained decoding to deliver accurate, predefined AI task instructions.


<details>
  <summary>Details</summary>
Motivation: To simplify access to predefined AI services on smartphones and enable intuitive, efficient AI tasking directly from images/text.

Method: An MLLM-based recommendation pipeline with structured reasoning to extract entities and infer intent; a template-augmented reasoning mechanism; and a prefix-tree-based constrained decoding to restrict outputs to predefined instruction candidates.

Result: Real-world annotated datasets and a user study show substantial improvements in instruction recommendation accuracy and user experience.

Conclusion: MIRA has strong potential to revolutionize how users interact with AI services on mobile devices, offering seamless and efficient task execution.

Abstract: The rapid advancement of generative AI technologies is driving the
integration of diverse AI-powered services into smartphones, transforming how
users interact with their devices. To simplify access to predefined AI
services, this paper introduces MIRA, a pioneering framework for task
instruction recommendation that enables intuitive one-touch AI tasking on
smartphones. With MIRA, users can long-press on images or text objects to
receive contextually relevant instruction recommendations for executing AI
tasks. Our work introduces three key innovations: 1) A multimodal large
language model (MLLM)-based recommendation pipeline with structured reasoning
to extract key entities, infer user intent, and generate precise instructions;
2) A template-augmented reasoning mechanism that integrates high-level
reasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based
constrained decoding strategy that restricts outputs to predefined instruction
candidates, ensuring coherent and intent-aligned suggestions. Through
evaluation using a real-world annotated datasets and a user study, MIRA has
demonstrated substantial improvements in the accuracy of instruction
recommendation. The encouraging results highlight MIRA's potential to
revolutionize the way users engage with AI services on their smartphones,
offering a more seamless and efficient experience.

</details>


### [160] [An Exhaustive DPLL Approach to Model Counting over Integer Linear Constraints with Simplification Techniques](https://arxiv.org/abs/2509.13880)
*Mingwei Zhang,Zhenhao Gu,Liangda Fang,Cunjing Ge,Ziliang Chen,Zhao-Rong Lai,Quanlong Guan*

Main category: cs.AI

TL;DR: An exact DPLL-based MCILC solver with MIP-inspired simplifications; outperforms existing exact counters, solving all application benchmarks and more random instances.


<details>
  <summary>Details</summary>
Motivation: Model counting over integer linear constraints is fundamental with wide applications; existing counters struggle with scalability, motivating more efficient exact methods.

Method: Developed an exhaustive DPLL-based architecture for MCILC; integrated several simplification techniques from mixed integer programming into the architecture to prune search space and accelerate counting; evaluated against state-of-the-art MCIL C counters and propositional model counters on large benchmarks.

Result: On 2840 random benchmarks and 4131 application benchmarks, the approach outperformed all exact methods in random benchmarks, solving 1718 instances vs 1470 by the best competitor, and uniquely solving all 4131 application instances.

Conclusion: The combination of exhaustive DPLL with MIP-inspired simplifications yields a strong, scalable exact counter for MCILC, surpassing existing methods on both random and application benchmarks.

Abstract: Linear constraints are one of the most fundamental constraints in fields such
as computer science, operations research and optimization. Many applications
reduce to the task of model counting over integer linear constraints (MCILC).
In this paper, we design an exact approach to MCILC based on an exhaustive DPLL
architecture. To improve the efficiency, we integrate several effective
simplification techniques from mixed integer programming into the architecture.
We compare our approach to state-of-the-art MCILC counters and propositional
model counters on 2840 random and 4131 application benchmarks. Experimental
results show that our approach significantly outperforms all exact methods in
random benchmarks solving 1718 instances while the state-of-the-art approach
only computes 1470 instances. In addition, our approach is the only approach to
solve all 4131 application instances.

</details>


### [161] [Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks](https://arxiv.org/abs/2509.13968)
*Konstantinos Voudouris,Andrew Barron,Marta Halina,Colin Klein,Matishalin Patel*

Main category: cs.AI

TL;DR: The study shows that changes in information flow, especially moving to recurrent network architectures, can yield qualitative transitions in cognitive performance, with training barriers and irreversibility; laminated networks offer no advantage in grammar learning.


<details>
  <summary>Details</summary>
Motivation: Investigates whether major transitions in information flow can drive cognitive transitions, by modeling evolution-like changes in neural connectivity and testing grammar-learning performance.

Method: Compared feed-forward, recurrent, and laminated ANN topologies using artificial grammars of varying complexity, controlling for size/resources; analyzed input types processable and training difficulty.

Result: Recurrent networks expand the range of processable inputs and improve learning of complex grammars; training difficulties act as a transition barrier and irreversibility; laminated networks did not outperform non-laminated networks.

Conclusion: Some changes in information flow can cause transitions in cognitive performance; not all topology changes are advantageous; supports the idea of cognitive evolutionary transitions via neural architecture changes.

Abstract: Transitional accounts of evolution emphasise a few changes that shape what is
evolvable, with dramatic consequences for derived lineages. More recently it
has been proposed that cognition might also have evolved via a series of major
transitions that manipulate the structure of biological neural networks,
fundamentally changing the flow of information. We used idealised models of
information flow, artificial neural networks (ANNs), to evaluate whether
changes in information flow in a network can yield a transitional change in
cognitive performance. We compared networks with feed-forward, recurrent and
laminated topologies, and tested their performance learning artificial grammars
that differed in complexity, controlling for network size and resources. We
documented a qualitative expansion in the types of input that recurrent
networks can process compared to feed-forward networks, and a related
qualitative increase in performance for learning the most complex grammars. We
also noted how the difficulty in training recurrent networks poses a form of
transition barrier and contingent irreversibility -- other key features of
evolutionary transitions. Not all changes in network topology confer a
performance advantage in this task set. Laminated networks did not outperform
non-laminated networks in grammar learning. Overall, our findings show how some
changes in information flow can yield transitions in cognitive performance.

</details>


### [162] [CrowdAgent: Multi-Agent Managed Multi-Source Annotation System](https://arxiv.org/abs/2509.14030)
*Maosheng Qin,Renyu Zhu,Mingxuan Xia,Chenkai Chen,Zhen Zhu,Minmin Lin,Junbo Zhao,Lu Xu,Changjie Fan,Runze Wu,Haobo Wang*

Main category: cs.AI

TL;DR: CrowdAgent is a multi-agent framework that end-to-endly controls data annotation by dynamically coordinating LLMs, SLMs, and human experts to optimize quality and cost, demonstrated on six multimodal tasks with open-source code and a video demo.


<details>
  <summary>Details</summary>
Motivation: High-quality annotated data is essential for NLP, but current work often focuses only on the labeling step and treats data sources in isolation. There is a need for holistic system-level control to manage diverse annotation sources and trade-offs in a unified way.

Method: CrowdAgent is a multi-agent system that integrates task assignment, data annotation, and quality/cost management. It introduces a scheduling methodology that rationally assigns tasks among LLMs, SLMs, and humans to enable synergistic collaboration in a unified, end-to-end annotation workflow.

Result: Empirical evaluation on six diverse multimodal classification tasks demonstrates the effectiveness of CrowdAgent in orchestrating annotation with multiple sources, suggesting improvements in quality, efficiency, or cost over baselines (as claimed in the abstract).

Conclusion: CrowdAgent provides end-to-end process control for multimodal data annotation by coordinating LLMs, SLMs, and human experts, enabling synergistic collaboration; source code and video demo are available at the provided GitHub link.

Abstract: High-quality annotated data is a cornerstone of modern Natural Language
Processing (NLP). While recent methods begin to leverage diverse annotation
sources-including Large Language Models (LLMs), Small Language Models (SLMs),
and human experts-they often focus narrowly on the labeling step itself. A
critical gap remains in the holistic process control required to manage these
sources dynamically, addressing complex scheduling and quality-cost trade-offs
in a unified manner. Inspired by real-world crowdsourcing companies, we
introduce CrowdAgent, a multi-agent system that provides end-to-end process
control by integrating task assignment, data annotation, and quality/cost
management. It implements a novel methodology that rationally assigns tasks,
enabling LLMs, SLMs, and human experts to advance synergistically in a
collaborative annotation workflow. We demonstrate the effectiveness of
CrowdAgent through extensive experiments on six diverse multimodal
classification tasks. The source code and video demo are available at
https://github.com/QMMMS/CrowdAgent.

</details>


### [163] [Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning](https://arxiv.org/abs/2509.14195)
*Shalima Binta Manir,Tim Oates*

Main category: cs.AI

TL;DR: Two-level learning architecture (GCN first-order, MLP second-order) yields robust navigation and generalization in unseen mazes, supporting the idea that structured internal maps enable effective second-order learning.


<details>
  <summary>Details</summary>
Motivation: Empirically validate the theory that second-order learning promotes environment-cognition isomorphism and mental representations; address challenges in studying mental maps by using a hierarchical architecture.

Method: Use a Graph Convolutional Network (GCN) as the first-order learner to predict navigation paths from node features; employ an MLP as a second-order learner to adapt the GCN’s parameters in structurally novel maze environments; assess performance and generalization to unseen mazes; analyze whether the internal map mirrors the environment.

Result: The approach achieves significant performance improvements and robust generalization on unseen mazes; both quantitative and qualitative evidence indicate that when the cognitive system forms a mental map structurally isomorphic to the environment, second-order learning is especially effective.

Conclusion: Structured mental representations that are isomorphic to the environment enhance the effectiveness of second-order learning; this supports theories linking environment-cognition isomorphism with improved cognitive adaptability.

Abstract: Mental representation, characterized by structured internal models mirroring
external environments, is fundamental to advanced cognition but remains
challenging to investigate empirically. Existing theory hypothesizes that
second-order learning -- learning mechanisms that adapt first-order learning
(i.e., learning about the task/domain) -- promotes the emergence of such
environment-cognition isomorphism. In this paper, we empirically validate this
hypothesis by proposing a hierarchical architecture comprising a Graph
Convolutional Network (GCN) as a first-order learner and an MLP controller as a
second-order learner. The GCN directly maps node-level features to predictions
of optimal navigation paths, while the MLP dynamically adapts the GCN's
parameters when confronting structurally novel maze environments. We
demonstrate that second-order learning is particularly effective when the
cognitive system develops an internal mental map structurally isomorphic to the
environment. Quantitative and qualitative results highlight significant
performance improvements and robust generalization on unseen maze tasks,
providing empirical support for the pivotal role of structured mental
representations in maximizing the effectiveness of second-order learning.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [164] [Maximizing UAV Cellular Connectivity with Reinforcement Learning for BVLoS Path Planning](https://arxiv.org/abs/2509.13336)
*Mehran Behjati,Rosdiadee Nordin,Nor Fadzilah Abdullah*

Main category: cs.RO

TL;DR: An offline RL-based path-planning approach for BVLoS cellular-connected UAVs that minimizes distance while maximizing cellular link quality using an empirical aerial channel model.


<details>
  <summary>Details</summary>
Motivation: UAVs operating beyond visual line of sight rely on cellular connectivity; current challenges include coverage gaps, safety, and long-range operations; need for reliable offline path planning integrating real-world aerial channels.

Method: Train an RL agent with a reward function tied to link quality to BSs; incorporate empirical aerial channel model and coverage constraints; produce feasible paths; suitable for offline integration into ground control systems.

Result: Simulation results show the RL agent learns effective path planning, generating feasible trajectories and maximizing connectivity; demonstrates potential as offline module for GCS; improves BVLoS operation safety and reliability.

Conclusion: RL-based method addresses cellular link limitations for BVLoS UAVs; promising for complex long-range UAV applications; further work on real-world validation and extension to dynamic environments.

Abstract: This paper presents a reinforcement learning (RL) based approach for path
planning of cellular connected unmanned aerial vehicles (UAVs) operating beyond
visual line of sight (BVLoS). The objective is to minimize travel distance
while maximizing the quality of cellular link connectivity by considering real
world aerial coverage constraints and employing an empirical aerial channel
model. The proposed solution employs RL techniques to train an agent, using the
quality of communication links between the UAV and base stations (BSs) as the
reward function. Simulation results demonstrate the effectiveness of the
proposed method in training the agent and generating feasible UAV path plans.
The proposed approach addresses the challenges due to limitations in UAV
cellular communications, highlighting the need for investigations and
considerations in this area. The RL algorithm efficiently identifies optimal
paths, ensuring maximum connectivity with ground BSs to ensure safe and
reliable BVLoS flight operation. Moreover, the solution can be deployed as an
offline path planning module that can be integrated into future ground control
systems (GCS) for UAV operations, enhancing their capabilities and safety. The
method holds potential for complex long range UAV applications, advancing the
technology in the field of cellular connected UAV path planning.

</details>


### [165] [Real World Robotic Exploration using Deep Neural Networks Trained in Photorealistic Reconstructed Environments](https://arxiv.org/abs/2509.13342)
*Isaac Ronald Ward*

Main category: cs.RO

TL;DR: A deep RGB-based pose estimator is improved by a loss that jointly optimizes position and orientation, boosting indoor localization accuracy. The method uses photogrammetry-generated pose labels for local training and yields a real-time TurtleBot navigation pipeline, requiring only a short image collection to adapt to a scene.


<details>
  <summary>Details</summary>
Motivation: Pose estimation from RGB images is prone to perceptual aliasing in indoor environments. The work aims to enhance localization robustness without sacrificing ease of training, enabling efficient deployment in real-world scenes.

Method: Extend an existing deep pose network by adding a loss term that combines positional and rotational errors to reduce perceptual aliasing. Create a pose-labeled local dataset via photogrammetry, train the model on the local environment, and integrate it into a navigation pipeline tested in real-time on a TurtleBot.

Result: Median positional error reduced by up to 9.64%; median rotational error reduced by up to 2.99% compared to the unmodified network. Local scene localization accuracy reaches 0.11 m and 0.89 degrees. A full pipeline enables real-time navigation on a TurtleBot using only a short image collection (≈330 s).

Conclusion: The approach yields robust indoor localization with modest data collection, delivering a complete, adaptable pipeline for scene-specific navigation that can be deployed rapidly to new indoor environments.

Abstract: In this work, an existing deep neural network approach for determining a
robot's pose from visual information (RGB images) is modified, improving its
localization performance without impacting its ease of training. Explicitly,
the network's loss function is extended in a manner which intuitively combines
the positional and rotational error in order to increase robustness to
perceptual aliasing. An improvement in the localization accuracy for indoor
scenes is observed: with decreases of up to 9.64% and 2.99% in the median
positional and rotational error respectively, when compared to the unmodified
network.
  Additionally, photogrammetry data is used to produce a pose-labelled dataset
which allows the above model to be trained on a local environment, resulting in
localization accuracies of 0.11m & 0.89 degrees. This trained model forms the
basis of a navigation algorithm, which is tested in real-time on a TurtleBot (a
wheeled robotic device). As such, this work introduces a full pipeline for
creating a robust navigational algorithm for any given real world indoor scene;
the only requirement being a collection of images from the scene, which can be
captured in as little as 330 seconds of

</details>


### [166] [Label-Efficient Grasp Joint Prediction with Point-JEPA](https://arxiv.org/abs/2509.13349)
*Jed Guzelkabaagac,Boris Petrović*

Main category: cs.RO

TL;DR: 3D self-supervised pretraining with Point-JEPA improves data-efficient grasp angle prediction from point clouds, achieving up to 26% RMSE reduction in low-label settings and matching full supervision on DLR-Hand II.


<details>
  <summary>Details</summary>
Motivation: Address label efficiency in robotic grasp angle regression by leveraging 3D JEPA-style pretraining to learn robust representations from unlabeled geometry.

Method: Tokenize point clouds from meshes, use a ShapeNet-pretrained Point-JEPA encoder, train a lightweight multi-hypothesis head with winner-takes-all, and select top logits for predictions.

Result: On DLR-Hand II with object-level splits, RMSE reduced by up to 26% under low-label regimes and parity with fully-supervised training.

Conclusion: JEPA-style pretraining is practical for data-efficient grasp learning with 3D self-supervision.

Abstract: We investigate whether 3D self-supervised pretraining with a Joint-Embedding
Predictive Architecture (Point-JEPA) enables label-efficient grasp joint-angle
prediction. Using point clouds tokenized from meshes and a ShapeNet-pretrained
Point-JEPA encoder, we train a lightweight multi-hypothesis head with
winner-takes-all and evaluate by top-logit selection. On DLR-Hand II with
object-level splits, Point-JEPA reduces RMSE by up to 26% in low-label regimes
and reaches parity with full supervision. These results suggest JEPA-style
pretraining is a practical approach for data-efficient grasp learning.

</details>


### [167] [Using role-play and Hierarchical Task Analysis for designing human-robot interaction](https://arxiv.org/abs/2509.13378)
*Mattias Wingren,Sören Andersson,Sara Rosenberg,Malin Andtfolk,Susanne Hägglund,Prashani Jayasingha Arachchige,Linda Nyholm*

Main category: cs.RO

TL;DR: Proposes role-play and Hierarchical Task Analysis (HTA) as underused techniques in human-robot interaction, demonstrated in a project for a social robot in a community pharmacy; highlights advantages and suggests future work on task analysis for social robots.


<details>
  <summary>Details</summary>
Motivation: Addresses a need for robust methods to understand user needs and to shape social robot behavior, ensuring models align with real-world contexts and enabling co-design.

Method: Applies role-play to create a controlled, adjustable environment where pharmacists model robot behavior; uses Hierarchical Task Analysis to decompose tasks and validate behavior, supporting co-design in developing a pharmacy-assisting robot.

Result: Role-play yielded insights into customers' needs; HTA helped ensure correct task modeling and facilitated development through co-design; collectively, the methods offer tangible advantages for social HRI and design validation.

Conclusion: Future work should focus on developing task-analysis methods tailored to social robot interaction.

Abstract: We present the use of two methods we believe warrant more use than they
currently have in the field of human-robot interaction: role-play and
Hierarchical Task Analysis. Some of its potential is showcased through our use
of them in an ongoing research project which entails developing a robot
application meant to assist at a community pharmacy. The two methods have
provided us with several advantages. The role-playing provided a controlled and
adjustable environment for understanding the customers' needs where pharmacists
could act as models for the robot's behavior; and the Hierarchical Task
Analysis ensured the behavior displayed was modelled correctly and aided
development through facilitating co-design. Future research could focus on
developing task analysis methods especially suited for social robot
interaction.

</details>


### [168] [ASTREA: Introducing Agentic Intelligence for Orbital Thermal Autonomy](https://arxiv.org/abs/2509.13380)
*Alejandro D. Mousist*

Main category: cs.RO

TL;DR: ASTREA demonstrates an LLM-guided agent on flight-heritage hardware for autonomous spacecraft thermal control, showing improved ground performance but latency-related degradation in ISS/L∞O operations.


<details>
  <summary>Details</summary>
Motivation: To explore the feasibility, design trade-offs, and potential of agentic autonomy in space by integrating semantic reasoning with adaptive control on hardware representative of a spacecraft's constraints.

Method: Integrate a resource-constrained LLM agent with a reinforcement-learning controller in an asynchronous, space-qualified architecture; validate through ground experiments and on-orbit validation aboard the ISS.

Result: Ground tests show improved thermal stability and fewer violations due to LLM-guided supervision; ISS on-orbit tests reveal degradation from inference latency misalignment with rapid LEO thermal cycles; demonstrates feasibility with notable limitations.

Conclusion: Agentic LLM-based systems offer opportunities for space autonomy but current latency and hardware constraints necessitate careful design guidelines for future deployments.

Abstract: This paper presents ASTREA, the first agentic system deployed on
flight-heritage hardware (TRL 9) for autonomous spacecraft operations. Using
thermal control as a representative use case, we integrate a
resource-constrained Large Language Model (LLM) agent with a reinforcement
learning controller in an asynchronous architecture tailored for
space-qualified platforms. Ground experiments show that LLM-guided supervision
improves thermal stability and reduces violations, confirming the feasibility
of combining semantic reasoning with adaptive control under hardware
constraints. However, on-orbit validation aboard the International Space
Station (ISS) reveals performance degradation caused by inference latency
mismatched with the rapid thermal cycles characteristic of Low Earth Orbit
(LEO) satellites. These results highlight both the opportunities and current
limitations of agentic LLM-based systems in real flight environments, providing
practical design guidelines for future space autonomy.

</details>


### [169] [Cooperative Target Detection with AUVs: A Dual-Timescale Hierarchical MARDL Approach](https://arxiv.org/abs/2509.13381)
*Zhang Xueyao,Yang Bo,Yu Zhiwen,Cao Xuelin,George C. Alexandropoulos,Merouane Debbah,Chau Yuen*

Main category: cs.RO

TL;DR: A dual time-scale H-MAPPO framework enables covert, cooperative AUV missions by hierarchical task selection (central AUV) and exposure-minimizing power/trajectory control, with rapid convergence and superior performance vs benchmarks.


<details>
  <summary>Details</summary>
Motivation: Under adversarial underwater environments, collaborative AUV communications risk exposure; covert coordination is essential for efficient multi-agent reconnaissance.

Method: Introduce dual time-scale Hierarchical MAPPO: a high-level policy selects task participants via a central AUV; a low-level policy controls power and trajectory of participating AUVs to reduce exposure probabilities.

Result: Simulations show rapid convergence and superior performance relative to benchmark algorithms, achieving higher long-term cooperative efficiency while maintaining covert operations.

Conclusion: The proposed framework effectively enables covert, cooperative underwater missions and lays groundwork for robust, scalable covert multi-agent coordination.

Abstract: Autonomous Underwater Vehicles (AUVs) have shown great potential for
cooperative detection and reconnaissance. However, collaborative AUV
communications introduce risks of exposure. In adversarial environments,
achieving efficient collaboration while ensuring covert operations becomes a
key challenge for underwater cooperative missions. In this paper, we propose a
novel dual time-scale Hierarchical Multi-Agent Proximal Policy Optimization
(H-MAPPO) framework. The high-level component determines the individuals
participating in the task based on a central AUV, while the low-level component
reduces exposure probabilities through power and trajectory control by the
participating AUVs. Simulation results show that the proposed framework
achieves rapid convergence, outperforms benchmark algorithms in terms of
performance, and maximizes long-term cooperative efficiency while ensuring
covert operations.

</details>


### [170] [VEGA: Electric Vehicle Navigation Agent via Physics-Informed Neural Operator and Proximal Policy Optimization](https://arxiv.org/abs/2509.13386)
*Hansol Lim,Minhyeok Im,Jonathan Boyack,Jee Won Lee,Jongseong Brad Choi*

Main category: cs.RO

TL;DR: Charge-aware EV navigation VEGA merges a physics-informed neural operator with PPO-based RL to plan charging stops on a charger-annotated road graph, using only speed signals; achieves near-Tesla Trip Planner performance and generalizes across regions without extra sensors.


<details>
  <summary>Details</summary>
Motivation: Rising demand for software-defined vehicles and EVs with powerful onboard compute requires eco-routing that respects state-of-charge constraints and real-world vehicle dynamics, without relying on extensive sensor suites.

Method: Two-module architecture: (1) PINO (physics-informed neural operator) learned from real vehicle speed logs to estimate aero drag, rolling resistance, mass, motor/regenerative efficiencies, and auxiliary load; (2) RL agent using these dynamics to optimize a path with charging stops and dwell times under SoC feasibility, guided by PPO with budgeted A* teacher-student framework on a charger-annotated road graph; operates using only speed signals.

Result: On long routes (e.g., San Francisco–New York), VEGA's charging stops, dwell times, SoC management, and total travel time closely track Tesla Trip Planner, with slightly more conservative behavior likely due to parameter drift in real vehicles; demonstrates generalization to France and Japan despite US-only training; shows practical integration of physics-informed learning and RL for EV eco-routing.

Conclusion: VEGA provides a practical, sensor-light approach to charge-aware eco-routing by fusing physics-informed dynamics with RL, offering competitive performance and generalizability, and potentially serving as a virtual sensor to reduce EV costs.

Abstract: Demands for software-defined vehicles (SDV) are rising and electric vehicles
(EVs) are increasingly being equipped with powerful computers. This enables
onboard AI systems to optimize charge-aware path optimization customized to
reflect vehicle's current condition and environment. We present VEGA, a
charge-aware EV navigation agent that plans over a charger-annotated road graph
using Proximal Policy Optimization (PPO) with budgeted A* teacher-student
guidance under state-of-charge (SoC) feasibility. VEGA consists of two modules.
First, a physics-informed neural operator (PINO), trained on real vehicle speed
and battery-power logs, uses recent vehicle speed logs to estimate aerodynamic
drag, rolling resistance, mass, motor and regenerative-braking efficiencies,
and auxiliary load by learning a vehicle-custom dynamics. Second, a
Reinforcement Learning (RL) agent uses these dynamics to optimize a path with
optimal charging stops and dwell times under SoC constraints. VEGA requires no
additional sensors and uses only vehicle speed signals. It may serve as a
virtual sensor for power and efficiency to potentially reduce EV cost. In
evaluation on long routes like San Francisco to New York, VEGA's stops, dwell
times, SoC management, and total travel time closely track Tesla Trip Planner
while being slightly more conservative, presumably due to real vehicle
conditions such as vehicle parameter drift due to deterioration. Although
trained only in U.S. regions, VEGA was able to compute optimal charge-aware
paths in France and Japan, demonstrating generalizability. It achieves
practical integration of physics-informed learning and RL for EV eco-routing.

</details>


### [171] [A Convex Formulation of Compliant Contact between Filaments and Rigid Bodies](https://arxiv.org/abs/2509.13434)
*Wei-Chen Li,Glen Chou*

Main category: cs.RO

TL;DR: A unified, convex-contact, DER-based simulator for frictional interactions between slender filaments and rigid bodies, enabling globally optimal time-step solutions and accurate frictional force modeling; validated against baselines and applicable to soft robotics and deformable manipulation.


<details>
  <summary>Details</summary>
Motivation: Filaments are codimensional and challenging to simulate when interacting with rigid bodies. Existing methods often require permanent attachment; there is a need for a framework that accurately models frictional filament–rigid body contacts with stable, globally optimal solutions.

Method: Combine discrete elastic rod (DER) modeling with a pressure-field patch contact model and a convex formulation of contact, enabling global optimality per time step and complementarity between contact velocity and impulse.

Result: Demonstrates accurate frictional force modeling and higher physical fidelity compared to baseline methods; provides a versatile simulator for complex filament–filament and filament–rigid body interactions.

Conclusion: The framework delivers a versatile, physically faithful simulator applicable to soft robotics (e.g., filament-based grippers) and deformable object manipulation (e.g., shoelace tying) by unifying DER, patch contact, and convex contact formulations.

Abstract: We present a computational framework for simulating filaments interacting
with rigid bodies through contact. Filaments are challenging to simulate due to
their codimensionality, i.e., they are one-dimensional structures embedded in
three-dimensional space. Existing methods often assume that filaments remain
permanently attached to rigid bodies. Our framework unifies discrete elastic
rod (DER) modeling, a pressure field patch contact model, and a convex contact
formulation to accurately simulate frictional interactions between slender
filaments and rigid bodies - capabilities not previously achievable. Owing to
the convex formulation of contact, each time step can be solved to global
optimality, guaranteeing complementarity between contact velocity and impulse.
We validate the framework by assessing the accuracy of frictional forces and
comparing its physical fidelity against baseline methods. Finally, we
demonstrate its applicability in both soft robotics, such as a stochastic
filament-based gripper, and deformable object manipulation, such as shoelace
tying, providing a versatile simulator for systems involving complex
filament-filament and filament-rigid body interactions.

</details>


### [172] [Trajectory Tracking with Reachability-Guided Quadratic Programming and Freeze-Resume](https://arxiv.org/abs/2509.13501)
*Hossein Gholampour,Logan E. Beaver*

Main category: cs.RO

TL;DR: An output-space tracking framework for systems convertible to a double integrator that enables safe interruption and resumption of planned paths without replanning. It uses offline reachability checks and online QP tracking under velocity/acceleration limits, with a one-step disturbance bound and a KKT-inspired error weighting to achieve near-perfect tracking and robust stopping in the presence of deviations; outperforms pure pursuit in simulation.


<details>
  <summary>Details</summary>
Motivation: Robots often must pause or deviate from a planned path to ensure safety around people/objects. The method provides guarantees under speed/acceleration limits and disturbances, allowing safe stops and seamless resumption without re-planning.

Method: Offline: perform a pre-run reachability check to ensure the motion plan respects speed and acceleration limits. Online: use a quadratic program to track the motion plan while enforcing the same limits. A one-step reachability test bounds the maximum disturbance the system can reject. When the state is on the reference path, perfect tracking is recovered in the deterministic case. A KKT-inspired weight is used to correct errors. Demonstrates safety stops, handling of unplanned deviations, and return-to-plan without replanning. Compared to pure pursuit in simulation.

Result: The system can safely stop and recover from deviations without replanning, maintaining alignment with the motion plan and showing improved performance over pure pursuit in simulations.

Conclusion: The proposed output-space tracking framework offers efficient, robust control for systems with double-integrator dynamics, enabling safe interruption and rapid resumption of planned motions with improved simulation performance over baseline methods.

Abstract: Many robotic systems must follow planned paths yet pause safely and resume
when people or objects intervene. We present an output-space method for systems
whose tracked output can be feedback-linearized to a double integrator (e.g.,
manipulators). The approach has two parts. Offline, we perform a pre-run
reachability check to verify that the motion plan respects speed and
acceleration magnitude limits. Online, we apply a quadratic program to track
the motion plan under the same limits. We use a one-step reachability test to
bound the maximum disturbance the system is capable of rejecting. When the
state coincides with the reference path we recover perfect tracking in the
deterministic case, and we correct errors using a KKT-inspired weight. We
demonstrate that safety stops and unplanned deviations are handled efficiently,
and the system returns to the motion plan without replanning. We demonstrate
our system's improved performance over pure pursuit in simulation.

</details>


### [173] [Embracing Bulky Objects with Humanoid Robots: Whole-Body Manipulation with Reinforcement Learning](https://arxiv.org/abs/2509.13534)
*Chunxin Zheng,Kai Chen,Zhihai Bi,Yulin Li,Liang Pan,Jinni Zhou,Haoang Li,Jun Ma*

Main category: cs.RO

TL;DR: A reinforcement-learning framework for humanoid whole-body manipulation that fuses a pre-trained human motion prior with a neural signed distance field (NSDF), using teacher-student distillation to produce natural, feasible multi-contact motions for embracing bulky objects, with sim-to-real validation.


<details>
  <summary>Details</summary>
Motivation: End-effector-only grasping is inadequate for bulky objects; robust, long-horizon embracing requires coordinated whole-body control and accurate geometry perception across contacts.

Method: A teacher-student distillation framework leverages large-scale human motion data to generate kinematically natural whole-body motions; an NSDF provides continuous geometric perception to improve contact awareness; a reinforcement learning policy coordinates arms and torso for stable multi-contact embracing; evaluation includes simulations and real-world experiments with sim-to-real transfer.

Result: Demonstrates improved adaptability to varying object shapes/sizes, enhanced contact stability and load capacity, better contact awareness over long horizons, and successful sim-to-real transfer in both simulated and real experiments.

Conclusion: The proposed framework offers a practical, scalable solution for robust multi-contact and long-horizon whole-body manipulation in humanoids, with strong potential to generalize to diverse bulky-object interactions and improve payload handling.

Abstract: Whole-body manipulation (WBM) for humanoid robots presents a promising
approach for executing embracing tasks involving bulky objects, where
traditional grasping relying on end-effectors only remains limited in such
scenarios due to inherent stability and payload constraints. This paper
introduces a reinforcement learning framework that integrates a pre-trained
human motion prior with a neural signed distance field (NSDF) representation to
achieve robust whole-body embracing. Our method leverages a teacher-student
architecture to distill large-scale human motion data, generating kinematically
natural and physically feasible whole-body motion patterns. This facilitates
coordinated control across the arms and torso, enabling stable multi-contact
interactions that enhance the robustness in manipulation and also the load
capacity. The embedded NSDF further provides accurate and continuous geometric
perception, improving contact awareness throughout long-horizon tasks. We
thoroughly evaluate the approach through comprehensive simulations and
real-world experiments. The results demonstrate improved adaptability to
diverse shapes and sizes of objects and also successful sim-to-real transfer.
These indicate that the proposed framework offers an effective and practical
solution for multi-contact and long-horizon WBM tasks of humanoid robots.

</details>


### [174] [Semantic 3D Reconstructions with SLAM for Central Airway Obstruction](https://arxiv.org/abs/2509.13541)
*Ayberk Acar,Fangjie Li,Hao Li,Lidia Al-Zogbi,Kanyifeechukwu Jane Oguine,Susheela Sharma Stern,Jesse F. d'Almeida,Robert J. Webster III,Ipek Oguz,Jie Ying Wu*

Main category: cs.RO

TL;DR: Real-time, semantically annotated 3D reconstructions of the central airway from monocular endoscopy by merging DROID-SLAM with an obstruction segmentation model; validated ex vivo with promising accuracy.


<details>
  <summary>Details</summary>
Motivation: Central airway obstruction is life-threatening and current treatments carry high complication risks. Integrating real-time, semantic 3D mapping supports safer robotic interventions and paves the way for automation.

Method: Combine DROID-SLAM-based monocular SLAM with a segmentation model that identifies obstructive tissues. Reconstruct real-time 3D airway geometry; use segmentation masks to annotate obstruction regions within the point cloud. Validate with ex vivo models.

Result: Achieves 3D reconstructions highly similar to ground-truth CT (0.62 mm Chamfer distance). Produces real-time, semantically annotated maps and does so faster than previous work, reflecting surgical scene more accurately.

Conclusion: First integration of semantic segmentation with real-time monocular SLAM for endoscopic CAO scenarios. The framework is modular and generalizable to other anatomies, supporting potential autonomous robotic interventions.

Abstract: Central airway obstruction (CAO) is a life-threatening condition with
increasing incidence, caused by tumors in and outside of the airway.
Traditional treatment methods such as bronchoscopy and electrocautery can be
used to remove the tumor completely; however, these methods carry a high risk
of complications. Recent advances allow robotic interventions with lesser risk.
The combination of robot interventions with scene understanding and mapping
also opens up the possibilities for automation. We present a novel pipeline
that enables real-time, semantically informed 3D reconstructions of the central
airway using monocular endoscopic video.
  Our approach combines DROID-SLAM with a segmentation model trained to
identify obstructive tissues. The SLAM module reconstructs the 3D geometry of
the airway in real time, while the segmentation masks guide the annotation of
obstruction regions within the reconstructed point cloud. To validate our
pipeline, we evaluate the reconstruction quality using ex vivo models.
  Qualitative and quantitative results show high similarity between ground
truth CT scans and the 3D reconstructions (0.62 mm Chamfer distance). By
integrating segmentation directly into the SLAM workflow, our system produces
annotated 3D maps that highlight clinically relevant regions in real time.
High-speed capabilities of the pipeline allows quicker reconstructions compared
to previous work, reflecting the surgical scene more accurately.
  To the best of our knowledge, this is the first work to integrate semantic
segmentation with real-time monocular SLAM for endoscopic CAO scenarios. Our
framework is modular and can generalize to other anatomies or procedures with
minimal changes, offering a promising step toward autonomous robotic
interventions.

</details>


### [175] [Using Visual Language Models to Control Bionic Hands: Assessment of Object Perception and Grasp Inference](https://arxiv.org/abs/2509.13572)
*Ozan Karaali,Hossam Farag,Strahinja Dosen,Cedomir Stefanovic*

Main category: cs.RO

TL;DR: End-to-end perception and grasp inference for semi-autonomous prosthetic hands using Vision-Language Models (VLMs) is feasible but has limitations; eight VLMs were benchmarked on a unified task from a single image, with strong object identification yet variable grasp parameter estimation; shows potential and current constraints.


<details>
  <summary>Details</summary>
Motivation: Reduce reliance on multi-module pipelines (object detection, pose estimation, grasp planning) by leveraging VLMs to jointly identify objects and infer grasp parameters for bionic hands; establish feasibility and provide benchmarks.

Method: Benchmark eight contemporary VLMs on a unified task from a single static image: identify object name, shape, orientation, and dimensions; infer grasp parameters (grasp type, wrist rotation, hand aperture, number of fingers); use a prompt to output structured JSON; dataset of 34 object snapshots; measure accuracy for categorical attributes, numerical errors, latency, and cost.

Result: Most models achieved high accuracy on object identification and shape recognition; numerical dimension estimates and grasp parameter estimates (especially wrist rotation and hand aperture) showed larger errors and variability; latency and cost analyses reported; overall evidence of both capability and limitations of using a single VLM as a perceptual module for bionic grasping.

Conclusion: VLMs have potential as end-to-end perceptual modules for prosthetics, enabling simplified pipelines but currently struggle with precise geometry and grasp parameter estimation; future work should explore improving numerical accuracy via prompts, data augmentation, or fine-tuning, and validating on more diverse scenes and dynamic contexts.

Abstract: This study examines the potential of utilizing Vision Language Models (VLMs)
to improve the perceptual capabilities of semi-autonomous prosthetic hands. We
introduce a unified benchmark for end-to-end perception and grasp inference,
evaluating a single VLM to perform tasks that traditionally require complex
pipelines with separate modules for object detection, pose estimation, and
grasp planning. To establish the feasibility and current limitations of this
approach, we benchmark eight contemporary VLMs on their ability to perform a
unified task essential for bionic grasping. From a single static image, they
should (1) identify common objects and their key properties (name, shape,
orientation, and dimensions), and (2) infer appropriate grasp parameters (grasp
type, wrist rotation, hand aperture, and number of fingers). A corresponding
prompt requesting a structured JSON output was employed with a dataset of 34
snapshots of common objects. Key performance metrics, including accuracy for
categorical attributes (e.g., object name, shape) and errors in numerical
estimates (e.g., dimensions, hand aperture), along with latency and cost, were
analyzed. The results demonstrated that most models exhibited high performance
in object identification and shape recognition, while accuracy in estimating
dimensions and inferring optimal grasp parameters, particularly hand rotation
and aperture, varied more significantly. This work highlights the current
capabilities and limitations of VLMs as advanced perceptual modules for
semi-autonomous control of bionic limbs, demonstrating their potential for
effective prosthetic applications.

</details>


### [176] [Dense-Jump Flow Matching with Non-Uniform Time Scheduling for Robotic Policies: Mitigating Multi-Step Inference Degradation](https://arxiv.org/abs/2509.13574)
*Zidong Chen,Zihao Guo,Peng Wang,ThankGod Itua Egbe,Yan Lyu,Chenghao Qian*

Main category: cs.RO

TL;DR: Flow matching policies generalize poorly with more Euler steps; introducing non-uniform training time schedules and a single-step dense-jump inference improves performance, yielding up to 23.7% gains across tasks.


<details>
  <summary>Details</summary>
Motivation: Generalization in flow-based robotics policies saturates early along the trajectory and degrades with additional inference steps. Oversampling of late-time behavior and non-Lipschitz velocity near the end of the integration cause instability and overfitting to training trajectories.

Method: Train with non-uniform time scheduling (e.g., U-shaped) to regularize both early and late temporal regimes; at inference, use a dense-jump schedule that performs a single-step integration beyond a jump point to avoid unstable regions near t=1, effectively creating a one-step learner that still benefits from multi-step dynamics.

Result: Empirical improvements of up to 23.7% over state-of-the-art baselines across diverse robotic tasks.

Conclusion: Non-uniform training time schedules plus dense-jump one-step inference yield more robust, generalizable flow-based policies, alleviating late-trajectory oversampling and instability without sacrificing the benefits of multi-step dynamics.

Abstract: Flow matching has emerged as a competitive framework for learning
high-quality generative policies in robotics; however, we find that
generalisation arises and saturates early along the flow trajectory, in
accordance with recent findings in the literature. We further observe that
increasing the number of Euler integration steps during inference
counter-intuitively and universally degrades policy performance. We attribute
this to (i) additional, uniformly spaced integration steps oversample the
late-time region, thereby constraining actions towards the training
trajectories and reducing generalisation; and (ii) the learned velocity field
becoming non-Lipschitz as integration time approaches 1, causing instability.
To address these issues, we propose a novel policy that utilises non-uniform
time scheduling (e.g., U-shaped) during training, which emphasises both early
and late temporal stages to regularise policy training, and a dense-jump
integration schedule at inference, which uses a single-step integration to
replace the multi-step integration beyond a jump point, to avoid unstable areas
around 1. Essentially, our policy is an efficient one-step learner that still
pushes forward performance through multi-step integration, yielding up to 23.7%
performance gains over state-of-the-art baselines across diverse robotic tasks.

</details>


### [177] [TreeIRL: Safe Urban Driving with Tree Search and Inverse Reinforcement Learning](https://arxiv.org/abs/2509.13579)
*Momchil S. Tomov,Sang Uk Lee,Hansford Hendrago,Jinwook Huh,Teawon Han,Forbes Howington,Rafael da Silva,Gianmarco Bernasconi,Marc Heim,Samuel Findler,Xiaonan Ji,Alexander Boule,Michael Napoli,Kuo Chen,Jesse Miller,Boaz Floor,Yunqing Hu*

Main category: cs.RO

TL;DR: TreeIRL is a planner for autonomous driving that blends Monte Carlo Tree Search (MCTS) with inverse reinforcement learning (IRL) to select human-like, safe trajectories. It achieves state-of-the-art performance in both large-scale simulations and real-world driving, including 500+ miles on Las Vegas roads, and is the first to demonstrate MCTS-based planning on public roads. It is extensible to incorporate more RL/IL components.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop an autonomous driving planner that simultaneously optimizes safety, efficiency, comfort, and human-likeness. It seeks to combine the strengths of search-based planning (to explore safe trajectories) with learning-based scoring (to prefer human-like behavior) and to validate the approach in real-world settings.

Method: Use Monte Carlo Tree Search to generate a promising set of safe candidate trajectories. Apply a deep inverse reinforcement learning scoring function to evaluate and select the most human-like trajectory among candidates. Evaluate on large-scale simulations and over 500 miles of real-world driving in Las Vegas, across scenarios including dense urban traffic, adaptive cruise control, cut-ins, and traffic lights.

Result: TreeIRL achieves the best overall performance, balancing safety, progress, comfort, and human-likeness. It is the first demonstration of MCTS-based planning on public roads and demonstrates the framework’s extensibility for further reinforcement and imitation learning to enhance planning strategies.

Conclusion: TreeIRL provides a framework that blends classical planning with learning-based evaluation, showing strong real-world applicability and suggesting that future work can further integrate reinforcement and imitation learning to explore more combinations of approaches for autonomous driving planning.

Abstract: We present TreeIRL, a novel planner for autonomous driving that combines
Monte Carlo tree search (MCTS) and inverse reinforcement learning (IRL) to
achieve state-of-the-art performance in simulation and in real-world driving.
The core idea is to use MCTS to find a promising set of safe candidate
trajectories and a deep IRL scoring function to select the most human-like
among them. We evaluate TreeIRL against both classical and state-of-the-art
planners in large-scale simulations and on 500+ miles of real-world autonomous
driving in the Las Vegas metropolitan area. Test scenarios include dense urban
traffic, adaptive cruise control, cut-ins, and traffic lights. TreeIRL achieves
the best overall performance, striking a balance between safety, progress,
comfort, and human-likeness. To our knowledge, our work is the first
demonstration of MCTS-based planning on public roads and underscores the
importance of evaluating planners across a diverse set of metrics and in
real-world environments. TreeIRL is highly extensible and could be further
improved with reinforcement learning and imitation learning, providing a
framework for exploring different combinations of classical and learning-based
approaches to solve the planning bottleneck in autonomous driving.

</details>


### [178] [Object Pose Estimation through Dexterous Touch](https://arxiv.org/abs/2509.13591)
*Amir-Hossein Shahidzadeh,Jiyue Zhu,Kezhou Chen,Sha Yi,Cornelia Fermüller,Yiannis Aloimonos,Xiaolong Wang*

Main category: cs.RO

TL;DR: A reinforcement-learning-driven, bimanual tactile exploration method for robust object pose estimation that uses active exploration to collect 3D point clouds and iteratively refine shape and pose without prior geometry.


<details>
  <summary>Details</summary>
Motivation: Pose estimation under limited, sensitive visual data; tactile sensing provides partial information but is local; need strategies to actively explore and fuse tactile data to recover pose.

Method: Train a robot hand with reinforcement learning to perform sensorimotor exploration. One hand stabilizes the object while the other explores to collect tactile data, forming 3D point clouds that are used to iteratively refine the object's shape and pose without prior geometric knowledge.

Result: The approach enables active exploration of an object's surface to identify key pose features despite lack of prior geometry; supplementary materials and demonstrations are provided.

Conclusion: Active tactile exploration guided by RL can yield robust pose estimation from partial tactile data, advancing manipulation tasks in scenarios with limited visual information.

Abstract: Robust object pose estimation is essential for manipulation and interaction
tasks in robotics, particularly in scenarios where visual data is limited or
sensitive to lighting, occlusions, and appearances. Tactile sensors often offer
limited and local contact information, making it challenging to reconstruct the
pose from partial data. Our approach uses sensorimotor exploration to actively
control a robot hand to interact with the object. We train with Reinforcement
Learning (RL) to explore and collect tactile data. The collected 3D point
clouds are used to iteratively refine the object's shape and pose. In our
setup, one hand holds the object steady while the other performs active
exploration. We show that our method can actively explore an object's surface
to identify critical pose features without prior knowledge of the object's
geometry. Supplementary material and more demonstrations will be provided at
https://amirshahid.github.io/BimanualTactilePose .

</details>


### [179] [Leg-Arm Coordinated Operation for Curtain Wall Installation](https://arxiv.org/abs/2509.13595)
*Xiao Liu,Weijun Wang,Tianlun Huang,Zhiyong Wang,Wei Feng*

Main category: cs.RO

TL;DR: A hierarchical optimization-based whole-body control framework coordinates a hexapod curtain wall installation robot's arms and legs to perform wall, ceiling, and floor installation tasks; experimentally validated, enabling safe and efficient curtain wall assembly on complex sites.


<details>
  <summary>Details</summary>
Motivation: Traditional curtain wall installation faces challenges such as variable on-site terrain, high labor intensity, low construction efficiency, and significant safety risks; large panels often require multiple workers, motivating autonomous or semi-autonomous robotic solutions.

Method: Develop a hierarchical optimization-based whole-body control framework that integrates hexapod leg motion with a folding arm and a serial-parallel manipulator, enabling coordinated arm-leg planning for wall installation, ceiling installation, and floor laying.

Result: Experimental validation on the hexapod curtain wall installation robot demonstrates the proposed control method's capability to perform curtain wall installation tasks and confirms the effectiveness of the hierarchical arm-leg coordination framework.

Conclusion: The framework lays the foundation for broader application of hexapod-based curtain wall installation in complex construction site environments and paves the way for safer, more efficient operations.

Abstract: With the acceleration of urbanization, the number of high-rise buildings and
large public facilities is increasing, making curtain walls an essential
component of modern architecture with widespread applications. Traditional
curtain wall installation methods face challenges such as variable on-site
terrain, high labor intensity, low construction efficiency, and significant
safety risks. Large panels often require multiple workers to complete
installation. To address these issues, based on a hexapod curtain wall
installation robot, we design a hierarchical optimization-based whole-body
control framework for coordinated arm-leg planning tailored to three key tasks:
wall installation, ceiling installation, and floor laying. This framework
integrates the motion of the hexapod legs with the operation of the folding arm
and the serial-parallel manipulator. We conduct experiments on the hexapod
curtain wall installation robot to validate the proposed control method,
demonstrating its capability in performing curtain wall installation tasks. Our
results confirm the effectiveness of the hierarchical optimization-based
arm-leg coordination framework for the hexapod robot, laying the foundation for
its further application in complex construction site environments.

</details>


### [180] [Barometer-Aided Attitude Estimation](https://arxiv.org/abs/2509.13649)
*Méloné Nyoba Tchonkeu,Soulaimane Berkane,Tarek Hamel*

Main category: cs.RO

TL;DR: A barometer-aided nonlinear attitude estimator on SO(3) uses barometric altitude to infer vertical velocity and attitude, cascading a deterministic Riccati observer with a complementary filter to achieve Almost Global Asymptotic Stability (AGAS) under a uniform observability condition, while preserving geometric consistency.


<details>
  <summary>Details</summary>
Motivation: In GNSS-denied or highly dynamic environments, IMU-only tilt estimation is ambiguous due to gravity vs inertial accelerations. External velocity sensors (GNSS, Pitot, Doppler, visual odometry) may be unavailable or costly, necessitating lightweight alternatives.

Method: Propose a barometer-aided attitude estimation architecture that leverages barometric altitude measurements to infer vertical velocity and attitude within a nonlinear observer on SO(3). The design cascades a deterministic Riccati observer with a complementary filter, ensuring AGAS under a uniform observability condition while maintaining geometric consistency.

Result: The analysis indicates that barometer-aided estimation can be a lightweight and effective complementary modality, achieving AGAS under the stated observability condition and preserving geometric consistency.

Conclusion: Barometer-aided attitude estimation provides a lightweight, robust modality for attitude estimation in GNSS-denied or highly dynamic scenarios, complementing IMU data with barometric altitude information.

Abstract: Accurate and robust attitude estimation is a central challenge for autonomous
vehicles operating in GNSS-denied or highly dynamic environments. In such
cases, Inertial Measurement Units (IMUs) alone are insufficient for reliable
tilt estimation due to the ambiguity between gravitational and inertial
accelerations. While auxiliary velocity sensors, such as GNSS, Pitot tubes,
Doppler radar, or visual odometry, are often used, they can be unavailable,
intermittent, or costly. This work introduces a barometer-aided attitude
estimation architecture that leverages barometric altitude measurements to
infer vertical velocity and attitude within a nonlinear observer on SO(3). The
design cascades a deterministic Riccati observer with a complementary filter,
ensuring Almost Global Asymptotic Stability (AGAS) under a uniform
observability condition while maintaining geometric consistency. The analysis
highlights barometer-aided estimation as a lightweight and effective
complementary modality.

</details>


### [181] [DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater Monitoring](https://arxiv.org/abs/2509.13666)
*Zhenqi Wu,Abhinav Modi,Angelos Mavrogiannis,Kaustubh Joshi,Nikhil Chopra,Yiannis Aloimonos,Nare Karapetyan,Ioannis Rekleitis,Xiaomin Lin*

Main category: cs.RO

TL;DR: DREAM is a Vision Language Model–guided autonomy framework for long-term underwater exploration and habitat monitoring that markedly improves efficiency and coverage in oyster monitoring and shipwreck mapping tasks compared with baselines and vanilla VLMs.


<details>
  <summary>Details</summary>
Motivation: Underwater environments are warming and acidifying, elevating mass mortality risk for sensitive species like oysters. Human labor is costly and dangerous, making persistent, wide-area, low-cost benthic monitoring via robotics desirable.

Method: A VLM-guided autonomy framework (DREAM) enabling underwater robots to perform environment-aware, long-term exploration and habitat monitoring without human intervention, demonstrated on oyster-monitoring and shipwreck mapping tasks with improved efficiency and coverage.

Result: In oyster monitoring: 31.5% reduction in time and 23% fewer steps while covering 8.88% more oysters compared with vanilla VLM baseline; in shipwreck scenes: 27.5% fewer steps to explore/map with 100% coverage vs 60.23% average coverage for the vanilla model.

Conclusion: DREAM demonstrates significant gains in planning efficiency and scene coverage for persistent underwater monitoring, indicating strong potential for safer, autonomous, long-duration benthic exploration and habitat monitoring.

Abstract: The ocean is warming and acidifying, increasing the risk of mass mortality
events for temperature-sensitive shellfish such as oysters. This motivates the
development of long-term monitoring systems. However, human labor is costly and
long-duration underwater work is highly hazardous, thus favoring robotic
solutions as a safer and more efficient option. To enable underwater robots to
make real-time, environment-aware decisions without human intervention, we must
equip them with an intelligent "brain." This highlights the need for
persistent,wide-area, and low-cost benthic monitoring. To this end, we present
DREAM, a Vision Language Model (VLM)-guided autonomy framework for long-term
underwater exploration and habitat monitoring. The results show that our
framework is highly efficient in finding and exploring target objects (e.g.,
oysters, shipwrecks) without prior location information. In the
oyster-monitoring task, our framework takes 31.5% less time than the previous
baseline with the same amount of oysters. Compared to the vanilla VLM, it uses
23% fewer steps while covering 8.88% more oysters. In shipwreck scenes, our
framework successfully explores and maps the wreck without collisions,
requiring 27.5% fewer steps than the vanilla model and achieving 100% coverage,
while the vanilla model achieves 60.23% average coverage in our shipwreck
environments.

</details>


### [182] [SPAR: Scalable LLM-based PDDL Domain Generation for Aerial Robotics](https://arxiv.org/abs/2509.13691)
*Songhao Huang,Yuwei Wu,Guangyao Shi,Gaurav S. Sukhatme,Vijay Kumar*

Main category: cs.RO

TL;DR: SPAR is an LLM-based framework that automatically generates valid, diverse, and semantically accurate PDDL domains for UAV tasks from natural language inputs, supported by a validated UAV planning dataset and a prompting strategy. It includes syntax validation, executability, feasibility, and interpretability checks, demonstrating that LLMs can substantially accelerate complex domain creation and providing a reproducible dataset and evaluation pipeline.


<details>
  <summary>Details</summary>
Motivation: Manual PDDL domain design is labor-intensive and error-prone, hindering adoption across diverse robotic applications (surveillance, delivery, inspection). Automatically generating domains from natural language could lower the barrier for practitioners and accelerate deployment, especially for UAV planning where domain complexity is high.

Method: 1) Build a systematically formulated and validated UAV planning dataset containing ground-truth PDDL domains, associated problems, and detailed domain/action descriptions. 2) Develop SPAR prompting framework to generate high-quality PDDL domains from natural language input. 3) Evaluate generated domains via syntax validation, executability, feasibility, and interpretability.

Result: LLMs can produce valid, diverse, and semantically accurate PDDL domains for UAV tasks. The approach accelerates domain creation, provides a reproducible dataset and evaluation pipeline, and enables domain-creation for practitioners without deep prior experience in planning.

Conclusion: LLMs are effective for automatic PDDL domain generation; SPAR demonstrates feasibility and builds a foundation (dataset and evaluation pipeline) to enable broader adoption in aerial robotics and automated planning, with potential for extension to other domains.

Abstract: We investigate the problem of automatic domain generation for the Planning
Domain Definition Language (PDDL) using Large Language Models (LLMs), with a
particular focus on unmanned aerial vehicle (UAV) tasks. Although PDDL is a
widely adopted standard in robotic planning, manually designing domains for
diverse applications such as surveillance, delivery, and inspection is
labor-intensive and error-prone, which hinders adoption and real-world
deployment. To address these challenges, we propose SPAR, a framework that
leverages the generative capabilities of LLMs to automatically produce valid,
diverse, and semantically accurate PDDL domains from natural language input. To
this end, we first introduce a systematically formulated and validated UAV
planning dataset, consisting of ground-truth PDDL domains and associated
problems, each paired with detailed domain and action descriptions. Building on
this dataset, we design a prompting framework that generates high-quality PDDL
domains from language input. The generated domains are evaluated through syntax
validation, executability, feasibility, and interpretability. Overall, this
work demonstrates that LLMs can substantially accelerate the creation of
complex planning domains, providing a reproducible dataset and evaluation
pipeline that enables application experts without prior experience to leverage
it for practical tasks and advance future research in aerial robotics and
automated planning.

</details>


### [183] [HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion](https://arxiv.org/abs/2509.13692)
*Yadan Zeng,Jiadong Zhou,Xiaohan Li,I-Ming Chen*

Main category: cs.RO

TL;DR: HGACNet introduces a hierarchical graph attention encoder with image-guided multi-scale cross-modal fusion and a contrastive loss for robust single-object point cloud completion, achieving state-of-the-art results on ShapeNet-ViPC and YCB-Complete and enabling better robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: Incomplete geometry due to self-occlusion and sensor limitations degrades downstream perception, planning and manipulation in robotics; robust completion can improve grasping, obstacle avoidance and interaction.

Method: A Hierarchical Graph Attention (HGA) encoder selects and refines local 3D features via graph attention-based downsampling; a Multi-Scale Cross-Modal Fusion (MSCF) module aligns hierarchical geometric features with image-guided priors from a single-view RGB image; a contrastive loss (C-Loss) aligns feature distributions across modalities to improve cross-modal fidelity.

Result: Demonstrates state-of-the-art performance on ShapeNet-ViPC and YCB-Complete benchmarks and strong applicability to real-world robotic manipulation tasks.

Conclusion: HGACNet effectively fuses hierarchical 3D geometry with image priors using attention mechanisms and a cross-modal contrastive loss, yielding high-fidelity point cloud completion suitable for robotic applications.

Abstract: Point cloud completion is essential for robotic perception, object
reconstruction and supporting downstream tasks like grasp planning, obstacle
avoidance, and manipulation. However, incomplete geometry caused by
self-occlusion and sensor limitations can significantly degrade downstream
reasoning and interaction. To address these challenges, we propose HGACNet, a
novel framework that reconstructs complete point clouds of individual objects
by hierarchically encoding 3D geometric features and fusing them with
image-guided priors from a single-view RGB image. At the core of our approach,
the Hierarchical Graph Attention (HGA) encoder adaptively selects critical
local points through graph attention-based downsampling and progressively
refines hierarchical geometric features to better capture structural continuity
and spatial relationships. To strengthen cross-modal interaction, we further
design a Multi-Scale Cross-Modal Fusion (MSCF) module that performs
attention-based feature alignment between hierarchical geometric features and
structured visual representations, enabling fine-grained semantic guidance for
completion. In addition, we proposed the contrastive loss (C-Loss) to
explicitly align the feature distributions across modalities, improving
completion fidelity under modality discrepancy. Finally, extensive experiments
conducted on both the ShapeNet-ViPC benchmark and the YCB-Complete dataset
confirm the effectiveness of HGACNet, demonstrating state-of-the-art
performance as well as strong applicability in real-world robotic manipulation
tasks.

</details>


### [184] [EZREAL: Enhancing Zero-Shot Outdoor Robot Navigation toward Distant Targets under Varying Visibility](https://arxiv.org/abs/2509.13720)
*Tianle Zeng,Jianwei Peng,Hanjing Ye,Guangcheng Chen,Senzi Luo,Hong Zhang*

Main category: cs.RO

TL;DR: A lightweight, closed-loop zero-shot object navigation system that uses a hierarchical, multi-scale image tile framework and target-saliency fusion to maintain direction toward distant, intermittently visible targets, enabling real-time operation with improved success.


<details>
  <summary>Details</summary>
Motivation: Long-range targets project as tiny features and occlusion causes intermittent visibility, making robust outdoor ZSON difficult; there is a need for a scalable, real-time method that can handle occlusion and domain gaps.

Method: A unified lightweight closed-loop system built on an aligned multi-scale image tile hierarchy. It uses hierarchical target-saliency fusion to summarize local semantic contrast into a stable coarse-layer regional saliency that guides target direction and indicates visibility. Regional saliency supports visibility-aware heading maintenance via keyframe memory, saliency-weighted fusion of historical headings, and active search during temporary invisibility. The design avoids whole-image rescaling, enables deterministic bottom-up aggregation, supports zero-shot navigation, and runs efficiently on a mobile robot.

Result: Targets detected beyond 150 m; maintains a correct heading through visibility changes with 82.6% probability; improves overall task success by 17.5% compared with SOTA methods.

Conclusion: The system demonstrates robust ZSON toward distant and intermittently observable targets, with an efficient, scalable architecture suitable for real-world outdoor navigation on mobile robots.

Abstract: Zero-shot object navigation (ZSON) in large-scale outdoor environments faces
many challenges; we specifically address a coupled one: long-range targets that
reduce to tiny projections and intermittent visibility due to partial or
complete occlusion. We present a unified, lightweight closed-loop system built
on an aligned multi-scale image tile hierarchy. Through hierarchical
target-saliency fusion, it summarizes localized semantic contrast into a stable
coarse-layer regional saliency that provides the target direction and indicates
target visibility. This regional saliency supports visibility-aware heading
maintenance through keyframe memory, saliency-weighted fusion of historical
headings, and active search during temporary invisibility. The system avoids
whole-image rescaling, enables deterministic bottom-up aggregation, supports
zero-shot navigation, and runs efficiently on a mobile robot. Across simulation
and real-world outdoor trials, the system detects semantic targets beyond 150m,
maintains a correct heading through visibility changes with 82.6% probability,
and improves overall task success by 17.5% compared with the SOTA methods,
demonstrating robust ZSON toward distant and intermittently observable targets.

</details>


### [185] [Reinforcement Learning for Robotic Insertion of Flexible Cables in Industrial Settings](https://arxiv.org/abs/2509.13731)
*Jeongwoo Park,Seabin Lee,Changmin Park,Wonjong Lee,Changjoo Nam*

Main category: cs.RO

TL;DR: A reinforcement learning framework for deformable FFC insertion using a foundation-model guided sim-to-real approach, achieving zero-shot real deployment via segmentation masks and prompting, aiming to reduce training risk.


<details>
  <summary>Details</summary>
Motivation: Industrial insertion of flexible flat cables requires submillimeter precision and handling deformable cables. RL offers automation but real-world training is risky and time-consuming due to nondeterminism from deformability; a safe, efficient training method is needed.

Method: Train RL policy entirely in simulation with random exploration (no physical risk). Use a foundation-model-based real-to-sim pipeline where semantic segmentation masks (via SAM2) abstract visual input to geometry-relevant features for sim-to-real transfer. Automate SAM2 prompting with a Vision-Language Model (VLM) to remove human intervention, enabling zero-shot transfer.

Result: Demonstrates zero-shot capabilities, allowing direct deployment to real environments without fine-tuning.

Conclusion: A foundation-model-based real-to-sim approach can reduce training time, mitigate risk of physical damage, and improve generalization for FFC insertion by focusing perception on task-relevant geometric cues and enabling automated, scalable seg­mentation for sim-to-real transfer.

Abstract: The industrial insertion of flexible flat cables (FFCs) into receptacles
presents a significant challenge owing to the need for submillimeter precision
when handling the deformable cables. In manufacturing processes, FFC insertion
with robotic manipulators often requires laborious human-guided trajectory
generation. While Reinforcement Learning (RL) offers a solution to automate
this task without modeling complex properties of FFCs, the nondeterminism
caused by the deformability of FFCs requires significant efforts and time on
training. Moreover, training directly in a real environment is dangerous as
industrial robots move fast and possess no safety measure. We propose an RL
algorithm for FFC insertion that leverages a foundation model-based real-to-sim
approach to reduce the training time and eliminate the risk of physical damages
to robots and surroundings. Training is done entirely in simulation, allowing
for random exploration without the risk of physical damages. Sim-to-real
transfer is achieved through semantic segmentation masks which leave only those
visual features relevant to the insertion tasks such as the geometric and
spatial information of the cables and receptacles. To enhance generality, we
use a foundation model, Segment Anything Model 2 (SAM2). To eleminate human
intervention, we employ a Vision-Language Model (VLM) to automate the initial
prompting of SAM2 to find segmentation masks. In the experiments, our method
exhibits zero-shot capabilities, which enable direct deployments to real
environments without fine-tuning.

</details>


### [186] [FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph](https://arxiv.org/abs/2509.13733)
*Xiaolin Zhou,Tingyang Xiao,Liu Liu,Yucheng Wang,Maiyue Chen,Xinrui Meng,Xinjie Wang,Wei Feng,Wei Sui,Zhizhong Su*

Main category: cs.RO

TL;DR: FSR-VLN introduces a hierarchical multi-modal scene graph with fast-to-slow reasoning for visual-language navigation, achieving state-of-the-art retrieval success and substantial speedups, including real-time humanoid robot deployment.


<details>
  <summary>Details</summary>
Motivation: VLN tasks struggle with long-range spatial reasoning and high inference latency in real-world embodied agents. There is a need for efficient, accurate navigation systems that can operate in real-time and on humanoid platforms.

Method: Propose Hierarchical Multi-modal Scene Graph (HMSG) to support progressive retrieval from coarse room localization to fine-grained view/object identification. Build Fast-to-Slow Navigation Reasoning (FSR) that performs fast matching to select candidates, followed by VLM-driven refinement. Integrate with speech interaction, planning, and control for real-time robot navigation.

Result: Evaluated on four indoor humanoid-robot datasets with 87 instructions. FSR-VLN achieves state-of-the-art retrieval success rate across all datasets and reduces response time by 82% versus VLM-based methods by activating slow reasoning only when fast intuition fails. Demonstrated integration with Unitree-G1 robot for natural language interaction and real-time navigation.

Conclusion: The framework markedly improves VLN performance and efficiency, enabling robust real-time navigation in embodied settings and demonstrating practical deployment; the hierarchical, progressive retrieval approach effectively combines fast heuristics with deep refinement.

Abstract: Visual-Language Navigation (VLN) is a fundamental challenge in robotic
systems, with broad applications for the deployment of embodied agents in
real-world environments. Despite recent advances, existing approaches are
limited in long-range spatial reasoning, often exhibiting low success rates and
high inference latency, particularly in long-range navigation tasks. To address
these limitations, we propose FSR-VLN, a vision-language navigation system that
combines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow
Navigation Reasoning (FSR). The HMSG provides a multi-modal map representation
supporting progressive retrieval, from coarse room-level localization to
fine-grained goal view and object identification. Building on HMSG, FSR first
performs fast matching to efficiently select candidate rooms, views, and
objects, then applies VLM-driven refinement for final goal selection. We
evaluated FSR-VLN across four comprehensive indoor datasets collected by
humanoid robots, utilizing 87 instructions that encompass a diverse range of
object categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all
datasets, measured by the retrieval success rate (RSR), while reducing the
response time by 82% compared to VLM-based methods on tour videos by activating
slow reasoning only when fast intuition fails. Furthermore, we integrate
FSR-VLN with speech interaction, planning, and control modules on a Unitree-G1
humanoid robot, enabling natural language interaction and real-time navigation.

</details>


### [187] [Motion Adaptation Across Users and Tasks for Exoskeletons via Meta-Learning](https://arxiv.org/abs/2509.13736)
*Muyuan Ma,Long Cheng,Lijun Han,Xiuze Xia,Houcheng Li*

Main category: cs.RO

TL;DR: Meta-imitation learning for task-generalizable wearable exoskeletons: train a task-specific network with MAML using simulation-derived elbow trajectories to enable rapid adaptation across unseen tasks and users, yielding stable assistance that reduces muscle activation and metabolic cost.


<details>
  <summary>Details</summary>
Motivation: Address the critical need for personalized, generalizable assistance in wearable exoskeletons and reduce data collection burden by leveraging public RGB video and Mocap data to train task-specific predictors that can adapt to new tasks and users.

Method: Extract full-body keypoints from public RGB video and Mocap datasets, retarget them to simulation; generate elbow flexion trajectories in simulation; train a task-specific neural network within the Model-Agnostic Meta-Learning (MAML) framework; the adapted network outputs references tracked by a gravity-compensated PD controller to provide stable assistance.

Result: Experimentally, the exoskeleton reduces muscle activation and metabolic cost for new users on untrained tasks compared to not wearing the exoskeleton.

Conclusion: The framework improves task generalization and user adaptability of wearable exoskeletons, suggesting potential for broader applicability across tasks and users.

Abstract: Wearable exoskeletons can augment human strength and reduce muscle fatigue
during specific tasks. However, developing personalized and task-generalizable
assistance algorithms remains a critical challenge. To address this, a
meta-imitation learning approach is proposed. This approach leverages a
task-specific neural network to predict human elbow joint movements, enabling
effective assistance while enhancing generalization to new scenarios. To
accelerate data collection, full-body keypoint motions are extracted from
publicly available RGB video and motion-capture datasets across multiple tasks,
and subsequently retargeted in simulation. Elbow flexion trajectories generated
in simulation are then used to train the task-specific neural network within
the model-agnostic meta-learning (MAML) framework, which allows the network to
rapidly adapt to novel tasks and unseen users with only a few gradient updates.
The adapted network outputs personalized references tracked by a
gravity-compensated PD controller to ensure stable assistance. Experimental
results demonstrate that the exoskeleton significantly reduces both muscle
activation and metabolic cost for new users performing untrained tasks,
compared to performing without exoskeleton assistance. These findings suggest
that the proposed framework effectively improves task generalization and user
adaptability for wearable exoskeleton systems.

</details>


### [188] [Dynamic Adaptive Legged Locomotion Policy via Decoupling Reaction Force Control and Gait Control](https://arxiv.org/abs/2509.13737)
*Renjie Wang,Shangke Lyu,Donglin Wang*

Main category: cs.RO

TL;DR: A decoupled stance-swing control framework for legged RL enabling fast online adaptation to out-of-distribution and sim-to-real gaps, reducing reliance on domain randomization.


<details>
  <summary>Details</summary>
Motivation: Reinforcement Learning for legged locomotion often degrades under out-of-distribution conditions and due to sim-to-real discrepancies. Domain randomization alone is not sufficient to guarantee robustness; an online-adaptive architecture is needed to handle unfamiliar environments.

Method: Proposes an emerging decoupled framework that isolates stance-leg control and swing-leg control to acquire fast online adaptation abilities and mitigate sim-to-real problems in unfamiliar environments.

Result: Simulation and real-world experiments demonstrate effectiveness against horizontal force disturbances, uneven terrains, heavy and biased payloads, and the sim-to-real gap.

Conclusion: A decoupled stance/swing control framework enhances robustness and fast adaptation in legged RL, offering a viable path beyond relying solely on domain randomization to close the sim-to-real gap.

Abstract: While Reinforcement Learning (RL) has achieved remarkable progress in legged
locomotion control, it often suffers from performance degradation in
out-of-distribution (OOD) conditions and discrepancies between the simulation
and the real environments. Instead of mainly relying on domain randomization
(DR) to best cover the real environments and thereby close the sim-to-real gap
and enhance robustness, this work proposes an emerging decoupled framework that
acquires fast online adaptation ability and mitigates the sim-to-real problems
in unfamiliar environments by isolating stance-leg control and swing-leg
control. Various simulation and real-world experiments demonstrate its
effectiveness against horizontal force disturbances, uneven terrains, heavy and
biased payloads, and sim-to-real gap.

</details>


### [189] [CDFlow: Generative Gradient Flows for Configuration Space Distance Fields via Neural ODEs](https://arxiv.org/abs/2509.13771)
*Mengzhu Li,Yunyu Zhou,He Ying,F. Richard Yu*

Main category: cs.RO

TL;DR: CDFlow uses Neural ODEs to learn a continuous flow in configuration space, modeling the multi-modal distribution of minimal-distance collision configurations instead of a single nearest point, with adaptive sampling to train high-fidelity data; this yields a smooth gradient field that preserves geometry and improves planning in high-DoF robots.


<details>
  <summary>Details</summary>
Motivation: Existing configuration-space distance fields (CDFs) yield a single nearest collision configuration, causing gradient ambiguity and geometric distortion in high-DoF robots. There is a need to capture the multi-modal nature of minimal-distance collision configurations and to accurately identify closest configurations with fine-boundary details for robust planning.

Method: CDFlow redefines the problem as learning a distribution of minimal-distance collision configurations and uses Neural Ordinary Differential Equations to model a continuous flow in configuration space. An adaptive refinement sampling strategy generates high-fidelity training data for this distribution. The trained Neural ODE implicitly models the multi-modal distribution and provides a gradient field equal to the expected direction toward the distribution, reducing ambiguity and preserving sharp geometry.

Result: Experimental evaluations on high-DoF motion planning tasks show that CDFlow significantly improves planning efficiency, trajectory quality, and robustness compared to existing CDF-based methods, enabling more robust planning in complex environments.

Conclusion: CDFlow offers a scalable, multi-modal, gradient-consistent approach for collision-aware planning in high-DoF robots by learning a distribution of near-collision configurations via Neural ODEs, improving planning performance and robustness.

Abstract: Signed Distance Fields (SDFs) are a fundamental representation in robot
motion planning. Their configuration-space counterpart, the Configuration Space
Distance Field (CDF), directly encodes distances in joint space, offering a
unified representation for optimization and control. However, existing CDF
formulations face two major challenges in high-degree-of-freedom (DoF) robots:
(1) they effectively return only a single nearest collision configuration,
neglecting the multi-modal nature of minimal-distance collision configurations
and leading to gradient ambiguity; and (2) they rely on sparse sampling of the
collision boundary, which often fails to identify the true closest
configurations, producing oversmoothed approximations and geometric distortion
in high-dimensional spaces. We propose CDFlow, a novel framework that addresses
these limitations by learning a continuous flow in configuration space via
Neural Ordinary Differential Equations (Neural ODEs). We redefine the problem
from finding a single nearest point to modeling the distribution of
minimal-distance collision configurations. We also introduce an adaptive
refinement sampling strategy to generate high-fidelity training data for this
distribution. The resulting Neural ODE implicitly models this multi-modal
distribution and produces a smooth, consistent gradient field-derived as the
expected direction towards the distribution-that mitigates gradient ambiguity
and preserves sharp geometric features. Extensive experiments on high-DoF
motion planning tasks demonstrate that CDFlow significantly improves planning
efficiency, trajectory quality, and robustness compared to existing CDF-based
methods, enabling more robust and efficient planning for collision-aware robots
in complex environments.

</details>


### [190] [Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach](https://arxiv.org/abs/2509.13774)
*Piaopiao Jin,Qi Wang,Guokang Sun,Ziwen Cai,Pinjia He,Yangwei You*

Main category: cs.RO

TL;DR: Introduces a human-in-the-loop dual-actor RL fine-tuning framework for Vision-Language-Action robotic manipulation, with a primary actor and a refinement actor, plus a talk-and-tweak language interface; shows rapid real-world multi-task learning and scalable multi-robot efficiency.


<details>
  <summary>Details</summary>
Motivation: Supervised fine-tuning depends on data quality; RL can enable learning from interactions; need efficient human-guided adaptation with language grounding to accelerate policy learning in complex, real-world tasks and multi-robot setups.

Method: A dual-actor RL framework: primary actor for robust multi-task performance; refinement actor for latent-space adaptation; a lightweight talk-and-tweak scheme that converts human corrections into semantically grounded language commands; generates a new dataset for policy learning; real-world multi-task experiments; scalable to multi-robot settings.

Result: 100% success across three tasks within 101 minutes of online fine-tuning; 50% success on long-horizon tasks across 12 consecutive operations; scalable to multi-robot with up to 2x efficiency when using two robots; experiment videos available at the provided site.

Conclusion: The approach improves data efficiency and performance in real-world robotic manipulation, leveraging language-grounded human corrections to rapidly adapt policies, and enabling scalable multi-robot training.

Abstract: Vision-language-action (VLA) models demonstrate strong generalization in
robotic manipulation but face challenges in complex, real-world tasks. While
supervised fine-tuning with demonstrations is constrained by data quality,
reinforcement learning (RL) offers a promising alternative. We propose a
human-in-the-loop dual-actor fine-tuning framework grounded in RL. The
framework integrates a primary actor for robust multi-task performance with a
refinement actor for latent-space adaptation. Beyond standard physical
interventions, we introduce a lightweight talk-and-tweak scheme that converts
human corrections into semantically grounded language commands, thereby
generating a new dataset for policy learning. In real-world multi-task
experiments, our approach achieves 100% success across three tasks within 101
minutes of online fine-tuning. For long-horizon tasks, it sustains a 50%
success rate over 12 consecutive operations. Furthermore, the framework scales
effectively to multi-robot training, achieving up to a 2 times improvement in
efficiency when using dual robots. The experiment videos are available at
https://sites.google.com/view/hil-daft/.

</details>


### [191] [Behavior Foundation Model for Humanoid Robots](https://arxiv.org/abs/2509.13780)
*Weishuai Zeng,Shunlin Lu,Kangning Yin,Xiaojie Niu,Minyue Dai,Jingbo Wang,Jiangmiao Pang*

Main category: cs.RO

TL;DR: Proposes Behavior Foundation Model (BFM) for humanoid WBC; uses masked online distillation and CVAE to model behavioral distributions; enables flexible control modes and rapid adaptation; validated in sim and on hardware.


<details>
  <summary>Details</summary>
Motivation: Current WBC frameworks are task-specific, rely on heavy reward engineering, and struggle to generalize across tasks and modes; a general, reusable behavioral knowledge base could unlock flexible, real-world humanoid control.

Method: Pretrain BFM on large-scale behavioral data; integrate a masked online distillation framework with a Conditional Variational Autoencoder (CVAE) to capture behavioral distributions; enable operation across diverse control modes without full retraining.

Result: BFM generalizes across diverse WBC tasks and adapts quickly to new behaviors in both simulation and a physical humanoid platform.

Conclusion: BFM represents a promising step toward a foundation model for general-purpose humanoid control, enabling broad-task generalization and efficient behavior acquisition.

Abstract: Whole-body control (WBC) of humanoid robots has witnessed remarkable progress
in skill versatility, enabling a wide range of applications such as locomotion,
teleoperation, and motion tracking. Despite these achievements, existing WBC
frameworks remain largely task-specific, relying heavily on labor-intensive
reward engineering and demonstrating limited generalization across tasks and
skills. These limitations hinder their response to arbitrary control modes and
restrict their deployment in complex, real-world scenarios. To address these
challenges, we revisit existing WBC systems and identify a shared objective
across diverse tasks: the generation of appropriate behaviors that guide the
robot toward desired goal states. Building on this insight, we propose the
Behavior Foundation Model (BFM), a generative model pretrained on large-scale
behavioral datasets to capture broad, reusable behavioral knowledge for
humanoid robots. BFM integrates a masked online distillation framework with a
Conditional Variational Autoencoder (CVAE) to model behavioral distributions,
thereby enabling flexible operation across diverse control modes and efficient
acquisition of novel behaviors without retraining from scratch. Extensive
experiments in both simulation and on a physical humanoid platform demonstrate
that BFM generalizes robustly across diverse WBC tasks while rapidly adapting
to new behaviors. These results establish BFM as a promising step toward a
foundation model for general-purpose humanoid control.

</details>


### [192] [Shell-Type Soft Jig for Holding Objects during Disassembly](https://arxiv.org/abs/2509.13802)
*Takuya Kiyokawa,Ryunosuke Takebayashi,Kensuke Harada*

Main category: cs.RO

TL;DR: A balloon-based, shell-type soft jig serves as a universal, damage-minimizing holding tool for robotic disassembly, reducing reliance on precise perception, grasping, and trajectory planning; validated via comparisons with a vise and a jamming-gripper-inspired soft jig across ten objects, with identified limitations and outlook.


<details>
  <summary>Details</summary>
Motivation: Robotic disassembly requires secure, adaptable holding fixtures that minimize component damage and can accommodate diverse shapes without demanding precise sensing, planning, or grasping. A flexible jig could streamline deployment and improve robustness.

Method: Design and implement a shell-type soft jig incorporating a balloon-based holding mechanism for soft, adaptable fixation. Compare its performance against a traditional vise and a jamming-gripper-inspired soft jig. Conduct ten-object experiments to assess successes, failures, and limitations.

Result: The proposed jig demonstrates practical feasibility, offering robust holding with reduced dependence on dedicated jig design and highly accurate perception, grasping, and trajectory planning. Experimental results include performance comparisons and ten-object tests that reveal representative successes and failures, highlighting the jig’s capabilities and its limitations.

Conclusion: A flexible, balloon-based shell-type soft jig provides robust, universal holding for robotic disassembly tasks, tolerating recognition, planning, and control errors. It has the potential to simplify jig design and perception requirements, though further work is needed to address identified limitations and to expand object diversity and real-world scenarios.

Abstract: This study addresses a flexible holding tool for robotic disassembly. We
propose a shell-type soft jig that securely and universally holds objects,
mitigating the risk of component damage and adapting to diverse shapes while
enabling soft fixation that is robust to recognition, planning, and control
errors. The balloon-based holding mechanism ensures proper alignment and stable
holding performance, thereby reducing the need for dedicated jig design, highly
accurate perception, precise grasping, and finely tuned trajectory planning
that are typically required with conventional fixtures. Our experimental
results demonstrate the practical feasibility of the proposed jig through
performance comparisons with a vise and a jamming-gripper-inspired soft jig.
Tests on ten different objects further showed representative successes and
failures, clarifying the jig's limitations and outlook.

</details>


### [193] [Soft Regrasping Tool Inspired by Jamming Gripper](https://arxiv.org/abs/2509.13815)
*Takuya Kiyokawa,Zhengtao Hu,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: A soft, jammed jig enables adaptable, general-purpose regrasping for robotic assembly by forming a stable cavity via air evacuation and a jamming membrane; validated with drop tests across ten parts, showing high placement success.


<details>
  <summary>Details</summary>
Motivation: Rigid fixtures lack adaptability and require part-specific designs, so reducing pose uncertainty in regrasping demands a deformable, reconfigurable fixture that can accommodate diverse geometries.

Method: Introduce a soft membrane jig that uses a triangular-pyramid tool to press and evacuate air, forming a stable cavity (jamming transition). Optimize stamping depth to balance placement stability and gripper access. Evaluate via drop experiments on ten mechanical parts with varying shapes.

Result: Placement success rates exceed 80% for most objects and over 90% for cylindrical ones; failures mainly due to geometric constraints and membrane properties. Demonstrates general-purpose, accurate, repeatable regrasping and outlines current limitations and future potential as an alternative to rigid fixtures.

Conclusion: Soft-jig-based regrasping offers a practical, general-purpose solution with accurate and repeatable placement, while acknowledging limitations and suggesting avenues for improvement toward deployment in assembly automation.

Abstract: Regrasping on fixtures is a promising approach to reduce pose uncertainty in
robotic assembly, but conventional rigid fixtures lack adaptability and require
dedicated designs for each part. To overcome this limitation, we propose a soft
jig inspired by the jamming transition phenomenon, which can be continuously
deformed to accommodate diverse object geometries. By pressing a
triangular-pyramid-shaped tool into the membrane and evacuating the enclosed
air, a stable cavity is formed as a placement space. We further optimize the
stamping depth to balance placement stability and gripper accessibility. In
soft-jig-based regrasping, the key challenge lies in optimizing the cavity size
to achieve precise dropping; once the part is reliably placed, subsequent
grasping can be performed with reduced uncertainty. Accordingly, we conducted
drop experiments on ten mechanical parts of varying shapes, which achieved
placement success rates exceeding 80% for most objects and above 90% for
cylindrical ones, while failures were mainly caused by geometric constraints
and membrane properties. These results demonstrate that the proposed jig
enables general-purpose, accurate, and repeatable regrasping, while also
clarifying its current limitations and future potential as a practical
alternative to rigid fixtures in assembly automation.

</details>


### [194] [Agile in the Face of Delay: Asynchronous End-to-End Learning for Real-World Aerial Navigation](https://arxiv.org/abs/2509.13816)
*Yude Li,Zhexuan Zhou,Huizhe Li,Youmin Gong,Jie Mei*

Main category: cs.RO

TL;DR: An asynchronous RL framework decouples perception and control for autonomous aerial vehicles (AAVs), using a Temporal Encoding Module (TEM) to handle perception delays, enabling high-frequency control and successful sim-to-real transfer.


<details>
  <summary>Details</summary>
Motivation: Modern end-to-end navigation struggles due to a mismatch between high-frequency control loops and low-frequency perception streams, caused by sensor update rates and heavy computation. This leads to low effective control rates and brittle performance in cluttered environments.

Method: Proposes an asynchronous reinforcement learning framework that decouples perception from control. A high-frequency policy uses the latest IMU state for immediate reactivity, while perception features are incorporated asynchronously. Introduces a Temporal Encoding Module (TEM) that conditions the policy on perception delays, plus a two-stage curriculum to stabilize training. Validation via extensive simulations and zero-shot sim-to-real transfer on an onboard NUC at 100 Hz control.

Result: Demonstrates sustained 100 Hz control with robust, agile navigation in cluttered real-world environments. Successful zero-shot sim-to-real transfer on hardware, with extensive simulation validation.

Conclusion: Asynchronous perception-control coupling with TEM enables high-frequency, reactive control under perception delays and is viable for real-world deployment, with open-source code to follow.

Abstract: Robust autonomous navigation for Autonomous Aerial Vehicles (AAVs) in complex
environments is a critical capability. However, modern end-to-end navigation
faces a key challenge: the high-frequency control loop needed for agile flight
conflicts with low-frequency perception streams, which are limited by sensor
update rates and significant computational cost. This mismatch forces
conventional synchronous models into undesirably low control rates. To resolve
this, we propose an asynchronous reinforcement learning framework that
decouples perception and control, enabling a high-frequency policy to act on
the latest IMU state for immediate reactivity, while incorporating perception
features asynchronously. To manage the resulting data staleness, we introduce a
theoretically-grounded Temporal Encoding Module (TEM) that explicitly
conditions the policy on perception delays, a strategy complemented by a
two-stage curriculum to ensure stable and efficient training. Validated in
extensive simulations, our method was successfully deployed in zero-shot
sim-to-real transfer on an onboard NUC, where it sustains a 100~Hz control rate
and demonstrates robust, agile navigation in cluttered real-world environments.
Our source code will be released for community reference.

</details>


### [195] [How Fly Neural Perception Mechanisms Enhance Visuomotor Control of Micro Robots](https://arxiv.org/abs/2509.13827)
*Renyuan Liu,Haoting Zhou,Chuankai Fang,Qinbing Fu*

Main category: cs.RO

TL;DR: A compact, fly-inspired visuomotor model based on LPLC2 neurons was implemented on a micro robot, enabling collision perception and fast evasive behavior with low memory use and attention-driven processing.


<details>
  <summary>Details</summary>
Motivation: To achieve agile collision avoidance in autonomous robots with low computational cost by leveraging insect visual processing, particularly LPLC2 neurons, for embedded, energy-efficient perception and reactive maneuvers.

Method: A simplified LPLC2 neural model with multi-attention mechanisms was embedded on the Colias micro robot (~70 KB memory). The system performs collision detection and reactive evasion, emulating distributed LPLC2 responses to rapidly and selectively detect approaching targets. The method included an evaluation against a locust-inspired collision model.

Result: The fly-inspired model achieved 96.1% success in collision detection, with robust and adaptive evasive maneuvers, showing comparable robustness to the state-of-the-art locust-inspired approach while improving maneuver elegance and responsiveness.

Conclusion: Biomimetic LPLC2-based vision on a tiny robot demonstrates effective, low-power collision avoidance and supports the broader potential of insect-inspired neural models for scalable, autonomous, and possibly collective robotic behaviors.

Abstract: Anyone who has tried to swat a fly has likely been frustrated by its
remarkable agility.This ability stems from its visual neural perception system,
particularly the collision-selective neurons within its small brain.For
autonomous robots operating in complex and unfamiliar environments, achieving
similar agility is highly desirable but often constrained by the trade-off
between computational cost and performance.In this context, insect-inspired
intelligence offers a parsimonious route to low-power, computationally
efficient frameworks.In this paper, we propose an attention-driven visuomotor
control strategy inspired by a specific class of fly visual projection
neurons-the lobula plate/lobula column type-2 (LPLC2)-and their associated
escape behaviors.To our knowledge, this represents the first embodiment of an
LPLC2 neural model in the embedded vision of a physical mobile robot, enabling
collision perception and reactive evasion.The model was simplified and
optimized at 70KB in memory to suit the computational constraints of a
vision-based micro robot, the Colias, while preserving key neural perception
mechanisms.We further incorporated multi-attention mechanisms to emulate the
distributed nature of LPLC2 responses, allowing the robot to detect and react
to approaching targets both rapidly and selectively.We systematically evaluated
the proposed method against a state-of-the-art locust-inspired collision
detection model.Results showed that the fly-inspired visuomotor model achieved
comparable robustness, at success rate of 96.1% in collision detection while
producing more adaptive and elegant evasive maneuvers.Beyond demonstrating an
effective collision-avoidance strategy, this work highlights the potential of
fly-inspired neural models for advancing research into collective behaviors in
insect intelligence.

</details>


### [196] [UltraHiT: A Hierarchical Transformer Architecture for Generalizable Internal Carotid Artery Robotic Ultrasonography](https://arxiv.org/abs/2509.13832)
*Teng Wang,Haojun Jiang,Yuxuan Wang,Zhenguo Sun,Xiangjie Yan,Xiang Li,Gao Huang*

Main category: cs.RO

TL;DR: UltraHiT is a hierarchical transformer framework for robust ICA locating in carotid ultrasound, achieving 95% success on unseen data on a large-scale dataset.


<details>
  <summary>Details</summary>
Motivation: Automating ICA is challenging due to its deep location, tortuous path, and high anatomical variability; a model that captures morphological variations via a hierarchical decision structure can improve robustness.

Method: A two-level causal-transformer architecture: a high-level variation detector chooses between an adaptive corrector and a standard executor; both modules process historical scanning sequences and are trained end-to-end.

Result: On a large ICA dataset (164 trajectories, ~72k samples from 28 subjects), the method locates the ICA with 95% success on unseen individuals and outperforms baselines.

Conclusion: The approach demonstrates strong generalizability to unseen subjects, advancing automation of ICA carotid ultrasound, with code to be released post-acceptance.

Abstract: Carotid ultrasound is crucial for the assessment of cerebrovascular health,
particularly the internal carotid artery (ICA). While previous research has
explored automating carotid ultrasound, none has tackled the challenging ICA.
This is primarily due to its deep location, tortuous course, and significant
individual variations, which greatly increase scanning complexity. To address
this, we propose a Hierarchical Transformer-based decision architecture, namely
UltraHiT, that integrates high-level variation assessment with low-level action
decision. Our motivation stems from conceptualizing individual vascular
structures as morphological variations derived from a standard vascular model.
The high-level module identifies variation and switches between two low-level
modules: an adaptive corrector for variations, or a standard executor for
normal cases. Specifically, both the high-level module and the adaptive
corrector are implemented as causal transformers that generate predictions
based on the historical scanning sequence. To ensure generalizability, we
collected the first large-scale ICA scanning dataset comprising 164
trajectories and 72K samples from 28 subjects of both genders. Based on the
above innovations, our approach achieves a 95% success rate in locating the ICA
on unseen individuals, outperforming baselines and demonstrating its
effectiveness. Our code will be released after acceptance.

</details>


### [197] [Track Any Motions under Any Disturbances](https://arxiv.org/abs/2509.13833)
*Zhikai Zhang,Jun Guo,Chao Chen,Jilong Wang,Chenghuai Lin,Yunrui Lian,Han Xue,Zhenrong Wang,Maoqi Liu,Huaping Liu,He Wang,Li Yi*

Main category: cs.RO

TL;DR: Any2Track: a two-stage RL framework (AnyTracker + AnyAdapter) to robustly track diverse humanoid motions under real-world disturbances, achieving zero-shot sim-to-real transfer on Unitree G1.


<details>
  <summary>Details</summary>
Motivation: Need for a universal humanoid motion tracker that remains stable under dynamic, contact-rich motions and disturbances (terrain, external forces, material changes) in real-world deployment.

Method: Proposes Any2Track with two components: AnyTracker (a general motion tracker using a single policy to track diverse motions) and AnyAdapter (a history-informed online adaptation module to adapt dynamics and close the sim-to-real gap). Implemented on Unitree G1 and demonstrated zero-shot sim-to-real transfer.

Result: The framework achieves successful zero-shot sim-to-real transfer and robustly tracks various motions under multiple real-world disturbances on hardware.

Conclusion: Combining a general motion tracker with a history-informed online adaptation module yields a robust humanoid motion tracking solution capable of handling diverse motions and disturbances in real-world scenarios.

Abstract: A foundational humanoid motion tracker is expected to be able to track
diverse, highly dynamic, and contact-rich motions. More importantly, it needs
to operate stably in real-world scenarios against various dynamics
disturbances, including terrains, external forces, and physical property
changes for general practical use. To achieve this goal, we propose Any2Track
(Track Any motions under Any disturbances), a two-stage RL framework to track
various motions under multiple disturbances in the real world. Any2Track
reformulates dynamics adaptability as an additional capability on top of basic
action execution and consists of two key components: AnyTracker and AnyAdapter.
AnyTracker is a general motion tracker with a series of careful designs to
track various motions within a single policy. AnyAdapter is a history-informed
adaptation module that endows the tracker with online dynamics adaptability to
overcome the sim2real gap and multiple real-world disturbances. We deploy
Any2Track on Unitree G1 hardware and achieve a successful sim2real transfer in
a zero-shot manner. Any2Track performs exceptionally well in tracking various
motions under multiple real-world disturbances.

</details>


### [198] [Pre-Manipulation Alignment Prediction with Parallel Deep State-Space and Transformer Models](https://arxiv.org/abs/2509.13839)
*Motonari Kambara,Komei Sugiura*

Main category: cs.RO

TL;DR: Proposes a predictive model to foresee success in open-vocabulary object manipulation by aligning a pre-action egocentric image with the planned trajectory and instruction. Introduces a Multi-Level Trajectory Fusion module that combines a deep state-space model with a transformer encoder to capture multi-level temporal correlations in the end-effector trajectory, yielding superior performance over baselines, including foundation models.


<details>
  <summary>Details</summary>
Motivation: Prevent hazards and improve efficiency by predicting success before action execution, rather than waiting for post-action failures to trigger replanning.

Method: A model that assesses alignment between a pre-manipulation egocentric image, the planned trajectory, and a natural language instruction. Uses a Multi-Level Trajectory Fusion (MLTF) module that runs a state-space model and a transformer encoder in parallel to capture multi-level time-series self-correlation in the end-effector trajectory.

Result: Experimental results show the proposed method outperforms existing methods, including foundation models.

Conclusion: Proactive prediction of manipulation success via MLTF improves decision making and efficiency, and outperforms strong baselines.

Abstract: In this work, we address the problem of predicting the future success of
open-vocabulary object manipulation tasks. Conventional approaches typically
determine success or failure after the action has been carried out. However,
they make it difficult to prevent potential hazards and rely on failures to
trigger replanning, thereby reducing the efficiency of object manipulation
sequences. To overcome these challenges, we propose a model, which predicts the
alignment between a pre-manipulation egocentric image with the planned
trajectory and a given natural language instruction. We introduce a Multi-Level
Trajectory Fusion module, which employs a state-of-the-art deep state-space
model and a transformer encoder in parallel to capture multi-level time-series
self-correlation within the end effector trajectory. Our experimental results
indicate that the proposed method outperformed existing methods, including
foundation models.

</details>


### [199] [InterKey: Cross-modal Intersection Keypoints for Global Localization on OpenStreetMap](https://arxiv.org/abs/2509.13857)
*Nguyen Hoang Khoi Tran,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.RO

TL;DR: InterKey leverages road intersections as distinctive cross-modal landmarks to enable global localization using OSM and dense point clouds, with compact binary descriptors and discrepancy mitigation to bridge modality gaps, achieving state-of-the-art KITTI performance.


<details>
  <summary>Details</summary>
Motivation: GNSS-denied localization requires scalable alternatives; HD maps are costly; OSM offers global coverage but coarse; need robust cross-modal matching using landmarks.

Method: Construct compact binary descriptors by encoding road and building imprints from point clouds and OSM; introduce discrepancy mitigation, orientation determination, and area-equalized sampling; cross-modal matching pipeline centered on intersections; generalizes to dense structural sensors.

Result: KITTI experiments show state-of-the-art accuracy, outperforming baselines by a large margin; scalable and cost-effective for robust vehicle localization.

Conclusion: InterKey provides a scalable, cross-modal localization framework leveraging road intersections, bridging GNSS-denied environments with OSM-derived priors, suitable for various dense structural sensors.

Abstract: Reliable global localization is critical for autonomous vehicles, especially
in environments where GNSS is degraded or unavailable, such as urban canyons
and tunnels. Although high-definition (HD) maps provide accurate priors, the
cost of data collection, map construction, and maintenance limits scalability.
OpenStreetMap (OSM) offers a free and globally available alternative, but its
coarse abstraction poses challenges for matching with sensor data. We propose
InterKey, a cross-modal framework that leverages road intersections as
distinctive landmarks for global localization. Our method constructs compact
binary descriptors by jointly encoding road and building imprints from point
clouds and OSM. To bridge modality gaps, we introduce discrepancy mitigation,
orientation determination, and area-equalized sampling strategies, enabling
robust cross-modal matching. Experiments on the KITTI dataset demonstrate that
InterKey achieves state-of-the-art accuracy, outperforming recent baselines by
a large margin. The framework generalizes to sensors that can produce dense
structural point clouds, offering a scalable and cost-effective solution for
robust vehicle localization.

</details>


### [200] [Using Petri Nets for Context-Adaptive Robot Explanations](https://arxiv.org/abs/2509.13861)
*Görkem Kılınç Soylu,Neziha Akalin,Maria Riveiro*

Main category: cs.RO

TL;DR: Petri nets model context-aware robot explanations in HRI and verify properties like deadlock-freeness and liveness.


<details>
  <summary>Details</summary>
Motivation: To ensure natural, transparent, and trust-building robot communication by adapting explanations to contextual cues in human-robot interaction.

Method: Model contextual information and adaptive explanations as Petri nets; analyze a scenario with attention and presence cues; verify deadlock-freeness, context-sensitive reachability, boundedness, and liveness.

Result: The Petri net model demonstrates key properties (deadlock-freeness, context-sensitive reachability, boundedness, and liveness) and indicates robustness and flexibility for designing/verifying context-adaptive explanations.

Conclusion: Petri nets are suitable for modeling and verifying context-aware explanations in human-robot interaction, contributing to safer, more trustworthy, and adaptable HRI systems.

Abstract: In human-robot interaction, robots must communicate in a natural and
transparent manner to foster trust, which requires adapting their communication
to the context. In this paper, we propose using Petri nets (PNs) to model
contextual information for adaptive robot explanations. PNs provide a formal,
graphical method for representing concurrent actions, causal dependencies, and
system states, making them suitable for analyzing dynamic interactions between
humans and robots. We demonstrate this approach through a scenario involving a
robot that provides explanations based on contextual cues such as user
attention and presence. Model analysis confirms key properties, including
deadlock-freeness, context-sensitive reachability, boundedness, and liveness,
showing the robustness and flexibility of PNs for designing and verifying
context-adaptive explanations in human-robot interactions.

</details>


### [201] [Repulsive Trajectory Modification and Conflict Resolution for Efficient Multi-Manipulator Motion Planning](https://arxiv.org/abs/2509.13882)
*Junhwa Hong,Beomjoon Lee,Woojin Lee,Changjoo Nam*

Main category: cs.RO

TL;DR: Introduces repulsive trajectory modification within CBS with a gradient-descent-based low-level planner using an Artificial Potential Field to reduce conflicts for multi-manipulator planning, plus a condition-based one-step conflict-free solving strategy. Demonstrates improved efficiency and success rate over Enhanced CBS in extensive tests including physical robots.


<details>
  <summary>Details</summary>
Motivation: Multi-manipulator motion planning is computationally challenging due to high-dimensional composite configuration spaces. Although Conflict-Based Search (CBS) decouples planning, its constraint tree can grow exponentially from resolving conflicts. There is a need to reduce conflicts and curb tree growth to achieve faster, more reliable planning.

Method: A two-level CBS framework where the low-level planner applies a gradient-descent optimization using an Artificial Potential Field to create repulsive forces that steer a conflicting manipulator away from others. The approach includes a condition-triggered strategy that can directly seek a conflict-free solution in a single step, avoiding constraint-tree growth when possible.

Result: The method consistently reduces the number of expanded nodes in the CBS constraint tree, achieves higher success rates, and finds solutions faster than Enhanced CBS and other state-of-the-art algorithms. It is validated through extensive simulations and physical robot experiments.

Conclusion: Repulsive trajectory modification within CBS, combined with a condition-based one-step conflict-free solving strategy, improves efficiency and robustness of multi-manipulator motion planning, offering practical advantages over existing CBS variants.

Abstract: We propose an efficient motion planning method designed to efficiently find
collision-free trajectories for multiple manipulators. While multi-manipulator
systems offer significant advantages, coordinating their motions is
computationally challenging owing to the high dimensionality of their composite
configuration space. Conflict-Based Search (CBS) addresses this by decoupling
motion planning, but suffers from subsequent conflicts incurred by resolving
existing conflicts, leading to an exponentially growing constraint tree of CBS.
Our proposed method is based on repulsive trajectory modification within the
two-level structure of CBS. Unlike conventional CBS variants, the low-level
planner applies a gradient descent approach using an Artificial Potential
Field. This field generates repulsive forces that guide the trajectory of the
conflicting manipulator away from those of other robots. As a result,
subsequent conflicts are less likely to occur. Additionally, we develop a
strategy that, under a specific condition, directly attempts to find a
conflict-free solution in a single step without growing the constraint tree.
Through extensive tests including physical robot experiments, we demonstrate
that our method consistently reduces the number of expanded nodes in the
constraint tree, achieves a higher success rate, and finds a solution faster
compared to Enhanced CBS and other state-of-the-art algorithms.

</details>


### [202] [PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models](https://arxiv.org/abs/2509.13903)
*Artem Lykov,Jeffrin Sam,Hung Khang Nguyen,Vladislav Kozlovskiy,Yara Mahmoud,Valerii Serpiva,Miguel Altamirano Cabrera,Mikhail Konenkov,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: PhysicalAgent uses iterative reasoning with diffusion-based video demonstrations to plan and execute robotic manipulation, then re-plans after failures. Through closed-loop execution, it recovers from errors and achieves high overall success across platforms, suggesting video-based generative reasoning as a general-purpose approach.


<details>
  <summary>Details</summary>
Motivation: To enable robust, general-purpose robotic manipulation by combining video-conditioned planning with iterative execution that can recover from initial failures across diverse perceptual modalities and hardware.

Method: From a textual instruction, generate short video demonstrations of candidate trajectories using diffusion-based video generation, execute them on the robot, detect failures, and iteratively re-plan. Evaluate across multiple perceptual modalities (egocentric, third-person, simulated) and robotic embodiments (bimanual UR3, Unitree G1 humanoid, simulated GR1) and compare to state-of-the-art baselines.

Result: Outperforms prior approaches, achieving up to 83% success on human-familiar tasks. First-attempt success is 20-30%, but iterative correction raises overall success to about 80% across platforms.

Conclusion: Video-based generative reasoning combined with iterative execution yields scalable, adaptable, and robust robotic control, highlighting the value of closed-loop re-planning in mitigating initial failures.

Abstract: We introduce PhysicalAgent, an agentic framework for robotic manipulation
that integrates iterative reasoning, diffusion-based video generation, and
closed-loop execution. Given a textual instruction, our method generates short
video demonstrations of candidate trajectories, executes them on the robot, and
iteratively re-plans in response to failures. This approach enables robust
recovery from execution errors. We evaluate PhysicalAgent across multiple
perceptual modalities (egocentric, third-person, and simulated) and robotic
embodiments (bimanual UR3, Unitree G1 humanoid, simulated GR1), comparing
against state-of-the-art task-specific baselines. Experiments demonstrate that
our method consistently outperforms prior approaches, achieving up to 83%
success on human-familiar tasks. Physical trials reveal that first-attempt
success is limited (20-30%), yet iterative correction increases overall success
to 80% across platforms. These results highlight the potential of video-based
generative reasoning for general-purpose robotic manipulation and underscore
the importance of iterative execution for recovering from initial failures. Our
framework paves the way for scalable, adaptable, and robust robot control.

</details>


### [203] [MAP: End-to-End Autonomous Driving with Map-Assisted Planning](https://arxiv.org/abs/2509.13926)
*Huilin Yin,Yiming Kan,Daniel Watzenig*

Main category: cs.RO

TL;DR: MAP is a map-assisted end-to-end planning framework that fuses segmentation-based semantic map features with ego status via dedicated modules, achieving notable improvements on DAIR-V2X-seq-SPD and winning a CVPR MEIS challenge.


<details>
  <summary>Details</summary>
Motivation: End-to-end autonomous driving underutilizes online mapping; integrating semantic map features can enhance trajectory planning and structure design.

Method: Introduce Plan-enhancing Online Mapping module, Ego-status-guided Planning module, and a Weight Adapter to leverage current ego status and map features; deep integration into end-to-end planning; experiments on DAIR-V2X-seq-SPD; code release.

Result: 16.6% reduction in L2 displacement error, 56.2% reduction in off-road rate, 44.5% improvement in overall score vs UniV2X baseline; top track ranking in MEIS CVPR2025 Track 2, beating second-best by 39.5%; demonstrates effectiveness of semantic map feature integration.

Conclusion: Explicit leveraging of semantic map features improves planning in end-to-end autonomous driving and suggests new directions for structure design; code available at the provided repository.

Abstract: In recent years, end-to-end autonomous driving has attracted increasing
attention for its ability to jointly model perception, prediction, and planning
within a unified framework. However, most existing approaches underutilize the
online mapping module, leaving its potential to enhance trajectory planning
largely untapped. This paper proposes MAP (Map-Assisted Planning), a novel
map-assisted end-to-end trajectory planning framework. MAP explicitly
integrates segmentation-based map features and the current ego status through a
Plan-enhancing Online Mapping module, an Ego-status-guided Planning module, and
a Weight Adapter based on current ego status. Experiments conducted on the
DAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6%
reduction in L2 displacement error, a 56.2% reduction in off-road rate, and a
44.5% improvement in overall score compared to the UniV2X baseline, even
without post-processing. Furthermore, it achieves top ranking in Track 2 of the
End-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS
Workshop @CVPR2025, outperforming the second-best model by 39.5% in terms of
overall score. These results highlight the effectiveness of explicitly
leveraging semantic map features in planning and suggest new directions for
improving structure design in end-to-end autonomous driving systems. Our code
is available at https://gitee.com/kymkym/map.git

</details>


### [204] [Reinforcement Learning for Autonomous Point-to-Point UAV Navigation](https://arxiv.org/abs/2509.13943)
*Salim Oyinlola,Nitesh Subedi,Soumik Sarkar*

Main category: cs.RO

TL;DR: An RL-based method enables a single UAV to autonomously navigate between predefined points, trained in a ROS-Gym environment and deployed on real hardware with successful demonstrations.


<details>
  <summary>Details</summary>
Motivation: There is a need for reliable, autonomous UAV navigation for tasks such as inspection, delivery, and navigation, reducing manual intervention by learning policies through interaction.

Method: Train a reinforcement learning policy for a single UAV to navigate between predefined points using a custom reward function that favors efficiency and penalizes collisions; employ a ROS-integrated, Gym-compatible training environment for trial-and-error learning; deploy the learned policy to a real UAV platform for evaluation.

Result: The trained policy, when deployed on a real UAV, successfully performs autonomous point-to-point navigation under practical conditions with minimal human oversight.

Conclusion: RL-based control is viable for real-world point-to-point drone operations and can enable autonomous UAV navigation with limited supervision.

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used in automated
inspection, delivery, and navigation tasks that require reliable autonomy. This
project develops a reinforcement learning (RL) approach to enable a single UAV
to autonomously navigate between predefined points without manual intervention.
The drone learns navigation policies through trial-and-error interaction, using
a custom reward function that encourages goal-reaching efficiency while
penalizing collisions and unsafe behavior. The control system integrates ROS
with a Gym-compatible training environment, enabling flexible deployment and
testing. After training, the learned policy is deployed on a real UAV platform
and evaluated under practical conditions. Results show that the UAV can
successfully perform autonomous navigation with minimal human oversight,
demonstrating the viability of RL-based control for point-to-point drone
operations in real-world scenarios.

</details>


### [205] [The Influence of Facial Features on the Perceived Trustworthiness of a Social Robot](https://arxiv.org/abs/2509.13948)
*Benedict Barrow,Roger K. Moore*

Main category: cs.RO

TL;DR: A social robot's eye features, especially an 'babyface' appearance via eye shape/size, influence first impressions of trust; manipulating the Furhat robot's back-projected face showed significant effects on perceived trustworthiness.


<details>
  <summary>Details</summary>
Motivation: To understand which facial design cues in social robots shape initial trust in human-robot interaction, informing better robot design for effective engagement.

Method: Experimental manipulation of a Furhat robot's back-projected face to vary eye shape and size; participants assessed perceived trustworthiness as an initial impression.

Result: Eye shape and size significantly affect perceived trustworthiness, supporting the idea that babyfacelike features can foster trust in HRI.

Conclusion: Designers should consider facial morphology, particularly eye-related features, when engineering social robots to optimize trust and interaction effectiveness.

Abstract: Trust and the perception of trustworthiness play an important role in
decision-making and our behaviour towards others, and this is true not only of
human-human interactions but also of human-robot interactions. While
significant advances have been made in recent years in the field of social
robotics, there is still some way to go before we fully understand the factors
that influence human trust in robots. This paper presents the results of a
study into the first impressions created by a social robot's facial features,
based on the hypothesis that a `babyface' engenders trust. By manipulating the
back-projected face of a Furhat robot, the study confirms that eye shape and
size have a significant impact on the perception of trustworthiness. The work
thus contributes to an understanding of the design choices that need to be made
when developing social robots so as to optimise the effectiveness of
human-robot interaction.

</details>


### [206] [SHaRe-RL: Structured, Interactive Reinforcement Learning for Contact-Rich Industrial Assembly Tasks](https://arxiv.org/abs/2509.13949)
*Jannick Stranghöner,Philipp Hartmann,Marco Braun,Sebastian Wrede,Klaus Neumann*

Main category: cs.RO

TL;DR: SHaRe-RL is a reinforcement learning framework for high-mix low-volume industrial assembly that uses manipulation primitives, human demonstrations and corrections, plus per-axis compliant actuation to enable safe, efficient online learning for long-horizon, contact-rich tasks. Demonstrated on precision connector insertion with tiny clearances, it achieves reliable results within practical time budgets without requiring robotics/RL expertise.


<details>
  <summary>Details</summary>
Motivation: To close the gap between brittle manual programming and unsafe/inefficient learning-based methods in HMLV industrial assembly, delivering high precision, safety, and adaptability for SMEs.

Method: Organize skills into manipulation primitives; fuse human demonstrations and online corrections; enforce per-axis force bounds via compliance; enable safe online RL for long-horizon, contact-rich tasks.

Result: Empirical validation on the insertion of Harting connector modules with 0.2–0.4 mm clearance showing reliable performance within practical time budgets; process expertise enhances learning without needing deep robotics/RL knowledge.

Conclusion: SHaRe-RL could make RL-based industrial assembly more safe, robust, and economically viable by leveraging domain knowledge and structured learning, reducing barriers to deployment.

Abstract: High-mix low-volume (HMLV) industrial assembly, common in small and
medium-sized enterprises (SMEs), requires the same precision, safety, and
reliability as high-volume automation while remaining flexible to product
variation and environmental uncertainty. Current robotic systems struggle to
meet these demands. Manual programming is brittle and costly to adapt, while
learning-based methods suffer from poor sample efficiency and unsafe
exploration in contact-rich tasks. To address this, we present SHaRe-RL, a
reinforcement learning framework that leverages multiple sources of prior
knowledge. By (i) structuring skills into manipulation primitives, (ii)
incorporating human demonstrations and online corrections, and (iii) bounding
interaction forces with per-axis compliance, SHaRe-RL enables efficient and
safe online learning for long-horizon, contact-rich industrial assembly tasks.
Experiments on the insertion of industrial Harting connector modules with
0.2-0.4 mm clearance demonstrate that SHaRe-RL achieves reliable performance
within practical time budgets. Our results show that process expertise, without
requiring robotics or RL knowledge, can meaningfully contribute to learning,
enabling safer, more robust, and more economically viable deployment of RL for
industrial assembly.

</details>


### [207] [SEG-Parking: Towards Safe, Efficient, and Generalizable Autonomous Parking via End-to-End Offline Reinforcement Learning](https://arxiv.org/abs/2509.13956)
*Zewei Yang,Zengqi Peng,Jun Ma*

Main category: cs.RO

TL;DR: SEG-Parking: an offline RL framework for interaction-aware autonomous parking that uses a specialized parking dataset, a pretrained goal-conditioned encoder, and a conservative offline RL objective; validated in CARLA with high success and good generalization; dataset/code to be released.


<details>
  <summary>Details</summary>
Motivation: Autonomous parking in unstructured urban environments involves dynamic interactions with other vehicles; existing methods struggle with interaction awareness and safety in out-of-distribution scenarios; offline RL with conservative regularization can leverage data to learn robust policies.

Method: End-to-end offline RL pipeline SEG-Parking. Build a parking-specific dataset including scenarios with and without opposite vehicle interference. Pretrain a goal-conditioned state encoder to embed fused perception into a latent space. Train an offline RL policy with a conservative regularizer to penalize out-of-distribution actions. Evaluate via closed-loop experiments in CARLA.

Result: The framework achieves superior performance, with the highest parking success rate and strong generalization to out-of-distribution parking scenarios in CARLA. The dataset and source code will be publicly available after acceptance.

Conclusion: SEG-Parking demonstrates that offline RL with a specialized dataset and latent-goal encoding can enable robust, interaction-aware autonomous parking, with resources to support future research.

Abstract: Autonomous parking is a critical component for achieving safe and efficient
urban autonomous driving. However, unstructured environments and dynamic
interactions pose significant challenges to autonomous parking tasks. To
address this problem, we propose SEG-Parking, a novel end-to-end offline
reinforcement learning (RL) framework to achieve interaction-aware autonomous
parking. Notably, a specialized parking dataset is constructed for parking
scenarios, which include those without interference from the opposite vehicle
(OV) and complex ones involving interactions with the OV. Based on this
dataset, a goal-conditioned state encoder is pretrained to map the fused
perception information into the latent space. Then, an offline RL policy is
optimized with a conservative regularizer that penalizes out-of-distribution
actions. Extensive closed-loop experiments are conducted in the high-fidelity
CARLA simulator. Comparative results demonstrate the superior performance of
our framework with the highest success rate and robust generalization to
out-of-distribution parking scenarios. The related dataset and source code will
be made publicly available after the paper is accepted.

</details>


### [208] [MetricNet: Recovering Metric Scale in Generative Navigation Policies](https://arxiv.org/abs/2509.13965)
*Abhijeet Nayak,Débora N. P. Oliveira,Samiran Gode,Cordelia Schmid,Wolfram Burgard*

Main category: cs.RO

TL;DR: Grounds generative navigation in metric space with MetricNet and MetricNav, improving safety and exploration by predicting distances between waypoints and guiding obstacle-aware planning.


<details>
  <summary>Details</summary>
Motivation: Generative navigation currently uses abstract, unscaled trajectories and a single-waypoint control, leading to unsafe, myopic actions. The goal is to ground outputs in real-world coordinates and consider full-path information.

Method: Introduce MetricNet to predict metric distances between waypoints, anchoring policy outputs to real-world coordinates. Integrate MetricNet into a navigation policy as MetricNav to steer away from obstacles while still pursuing the goal. Evaluate on a new simulation benchmarking framework and validate with real-world experiments.

Result: MetricNet-grounded waypoint execution and the MetricNav integration significantly improve navigation and exploration performance in simulation; these gains are corroborated by real-world experiments.

Conclusion: MetricNet effectively grounds generative navigation in metric space and, when combined with MetricNav, yields safer, more robust navigation with better exploration, applicable to end-to-end policies.

Abstract: Generative navigation policies have made rapid progress in improving
end-to-end learned navigation. Despite their promising results, this paradigm
has two structural problems. First, the sampled trajectories exist in an
abstract, unscaled space without metric grounding. Second, the control strategy
discards the full path, instead moving directly towards a single waypoint. This
leads to short-sighted and unsafe actions, moving the robot towards obstacles
that a complete and correctly scaled path would circumvent. To address these
issues, we propose MetricNet, an effective add-on for generative navigation
that predicts the metric distance between waypoints, grounding policy outputs
in real-world coordinates. We evaluate our method in simulation with a new
benchmarking framework and show that executing MetricNet-scaled waypoints
significantly improves both navigation and exploration performance. Beyond
simulation, we further validate our approach in real-world experiments.
Finally, we propose MetricNav, which integrates MetricNet into a navigation
policy to guide the robot away from obstacles while still moving towards the
goal.

</details>


### [209] [BIM Informed Visual SLAM for Construction Monitoring](https://arxiv.org/abs/2509.13972)
*Asier Bikandi,Miguel Fernandez-Cortizas,Muhammad Shaheer,Ali Tourani,Holger Voos,Jose Luis Sanchez-Lopez*

Main category: cs.RO

TL;DR: An RGB-D SLAM system with BIM priors improves alignment of as-built construction scenes to the BIM design by constraining wall correspondences in backend optimization, achieving real-time performance and notable reductions in trajectory error and map RMSE over visual SLAM baselines.


<details>
  <summary>Details</summary>
Motivation: Construction sites present evolving geometry and misalignment with plans; visual SLAM alone suffers from drift due to repetitive layouts, occlusions, and low-texture areas. BIM provides structural priors to anchor SLAM to the design and reduce drift.

Method: Detect walls from RGB-D data and establish correspondences with BIM wall data, then incorporate these correspondences as constraints in the backend optimization (pose-graph/BA). The system operates in real time and is validated on real construction sites, comparing against visual SLAM baselines.

Result: Average trajectory error reduced by 23.71% and map RMSE reduced by 7.14% compared to visual SLAM baselines.

Conclusion: Leveraging BIM as a structural prior improves SLAM robustness and alignment with the digital plan on partially constructed sites, enabling reliable plan-to-as-built alignment and early error detection; future work may address BIM inaccuracies, occlusions, and integration with other sensing modalities.

Abstract: Simultaneous Localization and Mapping (SLAM) is a key tool for monitoring
construction sites, where aligning the evolving as-built state with the
as-planned design enables early error detection and reduces costly rework.
LiDAR-based SLAM achieves high geometric precision, but its sensors are
typically large and power-demanding, limiting their use on portable platforms.
Visual SLAM offers a practical alternative with lightweight cameras already
embedded in most mobile devices. however, visually mapping construction
environments remains challenging: repetitive layouts, occlusions, and
incomplete or low-texture structures often cause drift in the trajectory map.
To mitigate this, we propose an RGB-D SLAM system that incorporates the
Building Information Model (BIM) as structural prior knowledge. Instead of
relying solely on visual cues, our system continuously establishes
correspondences between detected wall and their BIM counterparts, which are
then introduced as constraints in the back-end optimization. The proposed
method operates in real time and has been validated on real construction sites,
reducing trajectory error by an average of 23.71% and map RMSE by 7.14%
compared to visual SLAM baselines. These results demonstrate that BIM
constraints enable reliable alignment of the digital plan with the as-built
scene, even under partially constructed conditions.

</details>


### [210] [Flexible and Foldable: Workspace Analysis and Object Manipulation Using a Soft, Interconnected, Origami-Inspired Actuator Array](https://arxiv.org/abs/2509.13998)
*Bailey Dacre,Rodrigo Moreno,Serhat Demirtas,Ziqiao Wang,Yuhao Jiang,Jamie Paik,Kasper Stoy,Andrés Faíña*

Main category: cs.RO

TL;DR: Introduces a distributed manipulator system built from 3-DoF origami-inspired tiles interconnected by a compliant surface, enabling continuous manipulation over a flexible surface and achieving a 1.84× increase in operational area without increasing actuator count.


<details>
  <summary>Details</summary>
Motivation: Addresses trade-offs in distributed manipulation systems between manipulation capabilities, system complexity, and throughput. Existing DMS designs rely on high actuator densities and restrictive object-to-actuator scale ratios, motivating a low-density, adaptable design that leverages inter-tile connectivity.

Method: Proposes an array of 3-DoF origami-inspired tiles linked by a compliant surface layer to form a continuous manipulation surface. Analyzes the combined workspace, derives simple motion primitives, and demonstrates translation of simple objects across the tile array.

Result: The approach reduces actuator density while expanding the manipulable area by 1.84× without adding actuators. It enables manipulation not only at end-effectors but across the interconnected surface, offering lower cost and complexity relative to traditional high-density DMS and enabling new manipulation strategies.

Conclusion: A low-cost, low-complexity alternative to dense actuator arrays, with potential to broaden manipulation strategies by exploiting the flexibility of an interconnected surface.

Abstract: Object manipulation is a fundamental challenge in robotics, where systems
must balance trade-offs among manipulation capabilities, system complexity, and
throughput. Distributed manipulator systems (DMS) use the coordinated motion of
actuator arrays to perform complex object manipulation tasks, seeing widespread
exploration within the literature and in industry. However, existing DMS
designs typically rely on high actuator densities and impose constraints on
object-to-actuator scale ratios, limiting their adaptability. We present a
novel DMS design utilizing an array of 3-DoF, origami-inspired robotic tiles
interconnected by a compliant surface layer. Unlike conventional DMS, our
approach enables manipulation not only at the actuator end effectors but also
across a flexible surface connecting all actuators; creating a continuous,
controllable manipulation surface. We analyse the combined workspace of such a
system, derive simple motion primitives, and demonstrate its capabilities to
translate simple geometric objects across an array of tiles. By leveraging the
inter-tile connective material, our approach significantly reduces actuator
density, increasing the area over which an object can be manipulated by x1.84
without an increase in the number of actuators. This design offers a lower cost
and complexity alternative to traditional high-density arrays, and introduces
new opportunities for manipulation strategies that leverage the flexibility of
the interconnected surface.

</details>


### [211] [Whole-body Motion Control of an Omnidirectional Wheel-Legged Mobile Manipulator via Contact-Aware Dynamic Optimization](https://arxiv.org/abs/2509.14010)
*Zong Chen,Shaoyang Li,Ben Liu,Min Li,Zhouping Yin,Yiqun Li*

Main category: cs.RO

TL;DR: A unified, real-time whole-body optimization framework for an omnidirectional wheel-legged quadruped with a dexterous manipulator, enabling agile mobility and manipulation without mode switching.


<details>
  <summary>Details</summary>
Motivation: Unified control is challenging due to redundant DOFs, complex wheel-ground contact dynamics, and the need to coordinate locomotion with manipulation in wheel-legged robots; addressing this enables robust, real-time mobile manipulation in semi-structured environments.

Method: A wheel-legged quadruped with independently actuated steering modules and hub-driven wheels enabling omnidirectional locomotion; a contact-aware whole-body dynamic optimization that couples point-contact manipulation with line-contact wheel-ground interactions; a warm-start strategy to accelerate online optimization; a unified kinematic model for the 4WIS-4WID actuation to avoid mode-switching across locomotion strategies.

Result: Simulation and experimental results demonstrate agile terrain traversal, high-speed omnidirectional mobility, and precise manipulation across diverse scenarios.

Conclusion: The proposed framework enhances mobility-manipulation coordination for wheel-legged robots and shows promise for factory automation, urban logistics, and service robotics in semi-structured environments.

Abstract: Wheel-legged robots with integrated manipulators hold great promise for
mobile manipulation in logistics, industrial automation, and human-robot
collaboration. However, unified control of such systems remains challenging due
to the redundancy in degrees of freedom, complex wheel-ground contact dynamics,
and the need for seamless coordination between locomotion and manipulation. In
this work, we present the design and whole-body motion control of an
omnidirectional wheel-legged quadrupedal robot equipped with a dexterous
manipulator. The proposed platform incorporates independently actuated steering
modules and hub-driven wheels, enabling agile omnidirectional locomotion with
high maneuverability in structured environments. To address the challenges of
contact-rich interaction, we develop a contact-aware whole-body dynamic
optimization framework that integrates point-contact modeling for manipulation
with line-contact modeling for wheel-ground interactions. A warm-start strategy
is introduced to accelerate online optimization, ensuring real-time feasibility
for high-dimensional control. Furthermore, a unified kinematic model tailored
for the robot's 4WIS-4WID actuation scheme eliminates the need for mode
switching across different locomotion strategies, improving control consistency
and robustness. Simulation and experimental results validate the effectiveness
of the proposed framework, demonstrating agile terrain traversal, high-speed
omnidirectional mobility, and precise manipulation under diverse scenarios,
underscoring the system's potential for factory automation, urban logistics,
and service robotics in semi-structured environments.

</details>


### [212] [TransforMARS: Fault-Tolerant Self-Reconfiguration for Arbitrarily Shaped Modular Aerial Robot Systems](https://arxiv.org/abs/2509.14025)
*Rui Huang,Zhiyu Gao,Siyu Tang,Jialin Zhang,Lei He,Ziqian Zhang,Lin Zhao*

Main category: cs.RO

TL;DR: TransforMARS is a general fault-tolerant reconfiguration framework for arbitrarily shaped modular aerial robot systems, enabling reconfiguration under multiple rotor and unit faults to maintain stability.


<details>
  <summary>Details</summary>
Motivation: Prior work on MARS self-reconfiguration focused on maximizing controllability margins for single rotor or unit faults in rectangular-shaped systems. There is a need to support arbitrarily shaped MARS with multiple faults while maintaining in-air stability.

Method: Develop TransforMARS: algorithms to identify minimum controllable assemblies containing faulty units; plan feasible disassembly-assembly sequences to transport units or subassemblies to form a target configuration, enabling continuous in-air stability during reconfiguration.

Result: Validated on challenging arbitrarily shaped MARS configurations, showing substantial improvements over prior work in handling diverse configurations and the number of faults tolerated.

Conclusion: TransforMARS provides a more flexible and practical feasible reconfiguration framework for MARS, enhancing fault tolerance and reconfigurability; videos and source code are available at an anonymous repository.

Abstract: Modular Aerial Robot Systems (MARS) consist of multiple drone modules that
are physically bound together to form a single structure for flight. Exploiting
structural redundancy, MARS can be reconfigured into different formations to
mitigate unit or rotor failures and maintain stable flight. Prior work on MARS
self-reconfiguration has solely focused on maximizing controllability margins
to tolerate a single rotor or unit fault for rectangular-shaped MARS. We
propose TransforMARS, a general fault-tolerant reconfiguration framework that
transforms arbitrarily shaped MARS under multiple rotor and unit faults while
ensuring continuous in-air stability. Specifically, we develop algorithms to
first identify and construct minimum controllable assemblies containing faulty
units. We then plan feasible disassembly-assembly sequences to transport MARS
units or subassemblies to form target configuration. Our approach enables more
flexible and practical feasible reconfiguration. We validate TransforMARS in
challenging arbitrarily shaped MARS configurations, demonstrating substantial
improvements over prior works in both the capacity of handling diverse
configurations and the number of faults tolerated. The videos and source code
of this work are available at the anonymous repository:
https://anonymous.4open.science/r/TransforMARS-1030/

</details>


### [213] [Prompt2Auto: From Motion Prompt to Automated Control via Geometry-Invariant One-Shot Gaussian Process Learning](https://arxiv.org/abs/2509.14040)
*Zewen Yang,Xiaobing Dai,Dongfa Zhang,Yu Li,Ziyang Meng,Bingkun Huang,Hamid Sadeghian,Sami Haddadin*

Main category: cs.RO

TL;DR: Prompt2Auto introduces GeoGP, a geometry-invariant one-shot Gaussian process that learns from a single motion prompt to control robots, invariant to translation/rotation/scale, enabling multi-step predictions and multi-skill autonomy; validated by simulations and two real-robot experiments, reducing demonstration burden.


<details>
  <summary>Details</summary>
Motivation: To address the high data requirements and poor cross-coordinate generalization in learning-from-demonstration, enabling robust, human-guided robot control from minimal prompts.

Method: Propose Prompt2Auto with GeoGP: a geometry-invariant one-shot Gaussian process. A dataset-construction strategy based on coordinate transformations enforces invariance to translation, rotation, and scaling, supports multi-step predictions, and is robust to variations in the user prompt, enabling multi-skill autonomy.

Result: Numerical simulations with a designed user GUI and two real-world robotic experiments show effectiveness, generalization across tasks, and a significant reduction in demonstration burden.

Conclusion: GeoGP-based Prompt2Auto provides geometry-invariant, one-shot learning for robot control from a single prompt, enabling robust, multi-skill autonomy across tasks with reduced data requirements; project page available.

Abstract: Learning from demonstration allows robots to acquire complex skills from
human demonstrations, but conventional approaches often require large datasets
and fail to generalize across coordinate transformations. In this paper, we
propose Prompt2Auto, a geometry-invariant one-shot Gaussian process (GeoGP)
learning framework that enables robots to perform human-guided automated
control from a single motion prompt. A dataset-construction strategy based on
coordinate transformations is introduced that enforces invariance to
translation, rotation, and scaling, while supporting multi-step predictions.
Moreover, GeoGP is robust to variations in the user's motion prompt and
supports multi-skill autonomy. We validate the proposed approach through
numerical simulations with the designed user graphical interface and two
real-world robotic experiments, which demonstrate that the proposed method is
effective, generalizes across tasks, and significantly reduces the
demonstration burden. Project page is available at:
https://prompt2auto.github.io

</details>


### [214] [Language Conditioning Improves Accuracy of Aircraft Goal Prediction in Untowered Airspace](https://arxiv.org/abs/2509.14063)
*Sundhar Vinodh Sangeetha,Chih-Yuan Chiu,Sarah H. Q. Li,Shreyas Kousik*

Main category: cs.RO

TL;DR: A multimodal approach for predicting other aircraft goals in untowered airspace by combining ASR-derived pilot intent with trajectory data, using a temporal CNN and Gaussian mixture model to improve prediction over motion history baselines.


<details>
  <summary>Details</summary>
Motivation: In untowered airspace, pilots rely on voice communication; autonomous aircraft need to anticipate others' goals. Relying on motion history is insufficient; integrating language understanding can yield earlier and more accurate intent prediction.

Method: Transcribe and interpret pilot radio calls via ASR and large language models to extract discrete intent labels. Fuse these labels with observed trajectories to condition a temporal convolutional network and a Gaussian mixture model for probabilistic goal prediction.

Result: Language-conditioned predictions reduce goal-prediction error compared to baselines based on motion history alone. Validation on real-world untowered airport data demonstrates improved accuracy and potential for socially aware, language-conditioned autonomous planning.

Conclusion: Incorporating natural language cues into trajectory-based prediction improves safety-relevant autonomous decision-making in untowered airspace, enabling more accurate anticipation of other aircraft intentions.

Abstract: Autonomous aircraft must safely operate in untowered airspace, where
coordination relies on voice-based communication among human pilots. Safe
operation requires an aircraft to predict the intent, and corresponding goal
location, of other aircraft. This paper introduces a multimodal framework for
aircraft goal prediction that integrates natural language understanding with
spatial reasoning to improve autonomous decision-making in such environments.
We leverage automatic speech recognition and large language models to
transcribe and interpret pilot radio calls, identify aircraft, and extract
discrete intent labels. These intent labels are fused with observed
trajectories to condition a temporal convolutional network and Gaussian mixture
model for probabilistic goal prediction. Our method significantly reduces goal
prediction error compared to baselines that rely solely on motion history,
demonstrating that language-conditioned prediction increases prediction
accuracy. Experiments on a real-world dataset from an untowered airport
validate the approach and highlight its potential to enable socially aware,
language-conditioned robotic motion planning.

</details>


### [215] [Constraint-Consistent Control of Task-Based and Kinematic RCM Constraints for Surgical Robots](https://arxiv.org/abs/2509.14075)
*Yu Li,Hamid Sadeghian,Zewen Yang,Valentin Le Mesle,Sami Haddadin*

Main category: cs.RO

TL;DR: A constraint-consistent torque controller for RAMIS enforces the remote center of motion (RCM) as a rheonomic holonomic constraint within a projection-based inverse-dynamics framework, unifying task and kinematic formulations to achieve accurate tool-tip tracking with smooth, robust torque under dynamic conditions.


<details>
  <summary>Details</summary>
Motivation: RAMIS requires strict RCM constraint enforcement to ensure safe tool manipulation through a trocar. Existing torque-level controllers either lack robustness or cannot guarantee consistent RCM satisfaction under dynamic interactions, insertion depth changes, moving trocars, and human inputs.

Method: Model the RCM as a rheonomic holonomic constraint and embed it into a projection-based inverse-dynamics controller. This unifies task-space and joint-space formulations, enabling accurate tool-tip tracking while preserving smooth torque profiles. The approach is validated in simulation and on a RAMIS training platform, and benchmarked against state-of-the-art methods.

Result: The proposed controller achieves improved RCM constraint satisfaction, reduces the torque required to maintain constraint and task performance, and produces smoother joint torque signals. It demonstrates robust performance under clinically relevant scenarios including spiral trajectories, variable insertion depths, moving trocars, and human interaction, outperforming existing methods in simulations and hardware tests.

Conclusion: Constraint-consistent torque control can enhance safety and reliability in RAMIS by ensuring precise RCM enforcement with efficient, smooth torques across dynamic, interactive scenarios; the work suggests a viable path toward more robust clinical deployment.

Abstract: Robotic-assisted minimally invasive surgery (RAMIS) requires precise
enforcement of the remote center of motion (RCM) constraint to ensure safe tool
manipulation through a trocar. Achieving this constraint under dynamic and
interactive conditions remains challenging, as existing control methods either
lack robustness at the torque level or do not guarantee consistent RCM
constraint satisfaction. This paper proposes a constraint-consistent torque
controller that treats the RCM as a rheonomic holonomic constraint and embeds
it into a projection-based inverse-dynamics framework. The method unifies
task-level and kinematic formulations, enabling accurate tool-tip tracking
while maintaining smooth and efficient torque behavior. The controller is
validated both in simulation and on a RAMIS training platform, and is
benchmarked against state-of-the-art approaches. Results show improved RCM
constraint satisfaction, reduced required torque, and robust performance by
improving joint torque smoothness through the consistency formulation under
clinically relevant scenarios, including spiral trajectories, variable
insertion depths, moving trocars, and human interaction. These findings
demonstrate the potential of constraint-consistent torque control to enhance
safety and reliability in surgical robotics. The project page is available at:
https://rcmpc-cube.github.io

</details>


### [216] [FlightDiffusion: Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video](https://arxiv.org/abs/2509.14082)
*Valerii Serpiva,Artem Lykov,Faryal Batool,Vladislav Kozlovskiy,Miguel Altamirano Cabrera,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: FlightDiffusion uses diffusion models to train UAVs from FPV video by generating realistic trajectories and datasets from single frames, enabling improved policies and sim-to-real transfer.


<details>
  <summary>Details</summary>
Motivation: Reduce data collection cost and improve robustness of UAV navigation by synthesizing diverse, plausible FPV trajectories and state-action data for training.

Method: A diffusion-model-based framework that produces video sequences from a single frame with accompanying action spaces; synthesizes FPV trajectories and state-action pairs to support policy learning and large-scale dataset generation.

Result: Generated trajectories are physically plausible: mean position error 0.25 m (RMSE 0.28), mean orientation error 0.19 rad (RMSE 0.24); sim-to-real transfer appears strong with no significant difference between sim and reality (ANOVA F(1,16)=0.394, p=0.541); success rates around 0.63 in both domains; improved robustness, smoother planning, and adaptability to unseen conditions.

Conclusion: Diffusion-based reasoning can unify navigation, action generation, and data synthesis for aerial robotics, enabling scalable datasets and improved policy learning.

Abstract: We present FlightDiffusion, a diffusion-model-based framework for training
autonomous drones from first-person view (FPV) video. Our model generates
realistic video sequences from a single frame, enriched with corresponding
action spaces to enable reasoning-driven navigation in dynamic environments.
Beyond direct policy learning, FlightDiffusion leverages its generative
capabilities to synthesize diverse FPV trajectories and state-action pairs,
facilitating the creation of large-scale training datasets without the high
cost of real-world data collection. Our evaluation demonstrates that the
generated trajectories are physically plausible and executable, with a mean
position error of 0.25 m (RMSE 0.28 m) and a mean orientation error of 0.19 rad
(RMSE 0.24 rad). This approach enables improved policy learning and dataset
scalability, leading to superior performance in downstream navigation tasks.
Results in simulated environments highlight enhanced robustness, smoother
trajectory planning, and adaptability to unseen conditions. An ANOVA revealed
no statistically significant difference between performance in simulation and
reality (F(1, 16) = 0.394, p = 0.541), with success rates of M = 0.628 (SD =
0.162) and M = 0.617 (SD = 0.177), respectively, indicating strong sim-to-real
transfer. The generated datasets provide a valuable resource for future UAV
research. This work introduces diffusion-based reasoning as a promising
paradigm for unifying navigation, action generation, and data synthesis in
aerial robotics.

</details>


### [217] [GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model](https://arxiv.org/abs/2509.14117)
*Ali Abouzeid,Malak Mansour,Zezhou Sun,Dezhen Song*

Main category: cs.RO

TL;DR: GeoAware-VLA freezes a geometric vision model as a feature extractor and adds a trainable projection to adapt features for the policy decoder, yielding strong zero-shot viewpoint generalization in Vision-Language-Action tasks across sim and real robot, without training 3D data or visual encoder.


<details>
  <summary>Details</summary>
Motivation: VLA models struggle to generalize to novel camera viewpoints because inferring robust 3D geometry from 2D images is hard; robust geometric grounding is needed.

Method: Use a frozen pretrained geometric vision model as feature extractor; a trainable projection layer maps these features to the policy decoder; no explicit 3D data; no training of the visual encoder; works with both continuous and discrete actions; evaluated on LIBERO subsets; zero-shot generalization to novel poses.

Result: Substantial improvements in zero-shot generalization; >2x increase in success rates in simulation; positive transfer to real robot, especially for unseen angles; effective across both action spaces.

Conclusion: Geometric grounding is a key component for generalizable robotic agents; the approach is simple and effective and can generalize to both simulated and real-world settings.

Abstract: Vision-Language-Action (VLA) models often fail to generalize to novel camera
viewpoints, a limitation stemming from their difficulty in inferring robust 3D
geometry from 2D images. We introduce GeoAware-VLA, a simple yet effective
approach that enhances viewpoint invariance by integrating strong geometric
priors into the vision backbone. Instead of training a visual encoder or
relying on explicit 3D data, we leverage a frozen, pretrained geometric vision
model as a feature extractor. A trainable projection layer then adapts these
geometrically-rich features for the policy decoder, relieving it of the burden
of learning 3D consistency from scratch. Through extensive evaluations on
LIBERO benchmark subsets, we show GeoAware-VLA achieves substantial
improvements in zero-shot generalization to novel camera poses, boosting
success rates by over 2x in simulation. Crucially, these benefits translate to
the physical world; our model shows a significant performance gain on a real
robot, especially when evaluated from unseen camera angles. Our approach proves
effective across both continuous and discrete action spaces, highlighting that
robust geometric grounding is a key component for creating more generalizable
robotic agents.

</details>


### [218] [CrazyMARL: Decentralized Direct Motor Control Policies for Cooperative Aerial Transport of Cable-Suspended Payloads](https://arxiv.org/abs/2509.14126)
*Viktor Lorentz,Khaled Wahba,Sayantan Auddy,Marc Toussaint,Wolfgang Hönig*

Main category: cs.RO

TL;DR: Decentralized multi-UAV RL framework (CrazyMARL) for cable-suspended payload transport that handles slack/taut cable mode transitions and disturbances; outperforms classical controllers; demonstrates zero-shot sim-to-real transfer and robustness to wind and disturbances.


<details>
  <summary>Details</summary>
Motivation: Coordinating multiple UAVs to transport cable-suspended payloads is challenging due to nonlinear payload dynamics, disturbances, and frequent cable mode transitions (slack vs taut). Prior work often used rigid-link assumptions and did not address cable mode changes in a multi-UAV setting. There is a need for autonomous, resilient control that can operate in unstructured environments.

Method: Introduce CrazyMARL, a decentralized Reinforcement Learning framework for multi-UAV cable-suspended payload transport. The method learns decentralized policies that handle slack/taut cable dynamics and disturbances. Evaluation is conducted in simulation with comparisons to classical decentralized controllers, followed by zero-shot sim-to-real transfer demonstrations and testing under harsh conditions (wind, disturbances, mode transitions).

Result: Learned policies outperform classical decentralized controllers in disturbance rejection and tracking precision, achieving an 80% recovery rate from harsh conditions versus 44% for the baseline. Demonstrates successful zero-shot sim-to-real transfer and strong robustness under wind, external disturbances, and slack/taut transitions.

Conclusion: This work advances autonomous, resilient UAV collaboration for complex payload missions in unstructured environments and lays the groundwork for practical deployment of multi-UAV cable-suspended payload transport.

Abstract: Collaborative transportation of cable-suspended payloads by teams of Unmanned
Aerial Vehicles (UAVs) has the potential to enhance payload capacity, adapt to
different payload shapes, and provide built-in compliance, making it attractive
for applications ranging from disaster relief to precision logistics. However,
multi-UAV coordination under disturbances, nonlinear payload dynamics, and
slack--taut cable modes remains a challenging control problem. To our
knowledge, no prior work has addressed these cable mode transitions in the
multi-UAV context, instead relying on simplifying rigid-link assumptions. We
propose CrazyMARL, a decentralized Reinforcement Learning (RL) framework for
multi-UAV cable-suspended payload transport. Simulation results demonstrate
that the learned policies can outperform classical decentralized controllers in
terms of disturbance rejection and tracking precision, achieving an 80%
recovery rate from harsh conditions compared to 44% for the baseline method. We
also achieve successful zero-shot sim-to-real transfer and demonstrate that our
policies are highly robust under harsh conditions, including wind, random
external disturbances, and transitions between slack and taut cable dynamics.
This work paves the way for autonomous, resilient UAV teams capable of
executing complex payload missions in unstructured environments.

</details>


### [219] [Energy Efficient Multi Robot Package Delivery under Capacity-Constraints via Voronoi-Constrained Networks](https://arxiv.org/abs/2509.14127)
*Alkesh K. Srivastava,Jared Michael Levin,Philip Dames*

Main category: cs.RO

TL;DR: A framework (VCST-RCP) that uses Voronoi-constrained Steiner tree relays to coordinate multi-robot delivery with limited capacity; builds relay trunks and schedules pickup/relay/delivery, achieving up to 34% energy efficiency gains over baselines.


<details>
  <summary>Details</summary>
Motivation: Relays are underutilized in multi-robot logistics; to improve energy efficiency and scalability under carrying capacity constraints, rethink relays as central planning elements rather than incidental byproducts.

Method: Construct sparse relay trunks via Steiner-tree optimization within Voronoi regions, then synthesize robot-level schedules for pickup, relay transfer, and delivery; optimize coordination with homogeneous robots and capacity constraints.

Result: Experiments show consistent improvements up to 34% compared with conventional baselines; improved energy efficiency and scalability for real-world logistics.

Conclusion: Incorporating relay-based coordination into multi-robot delivery yields significant efficiency gains; the VCST-RCP framework provides a scalable approach and demonstrates the value of relays in logistics under capacity constraints.

Abstract: We consider the problem of delivering multiple packages from a single pickup
depot to distinct goal locations using a homogeneous fleet of robots with
limited carrying capacity. We propose VCST-RCP, a Voronoi-Constrained Steiner
Tree Relay Coordination Planning framework that constructs sparse relay trunks
using Steiner tree optimization and then synthesizes robot-level pickup, relay,
and delivery schedules. This framework reframes relays from incidental
byproducts into central elements of coordination, offering a contrast with
traditional delivery methods that rely on direct source-to-destination
transport. Extensive experiments show consistent improvements of up to 34%
compared to conventional baselines, underscoring the benefits of incorporating
relays into the delivery process. These improvements translate directly to
enhanced energy efficiency in multi-robot delivery under capacity constraints,
providing a scalable framework for real-world logistics.

</details>


### [220] [SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model](https://arxiv.org/abs/2509.14138)
*Ran Yang,Zijian An,Lifeng ZHou,Yiming Feng*

Main category: cs.RO

TL;DR: SeqVLA adds a completion-aware detection head to a VLA-style controller (pi0) to detect subtask completion and autonomously switch subtasks, addressing error cascades in long-horizon manipulation. Four finetuning strategies (joint/sequential, frozen/unfrozen backbone) are evaluated across two multi-stage tasks.


<details>
  <summary>Details</summary>
Motivation: In long-horizon, multi-subtask tasks, errors in detecting subtask completion propagate and cause downstream failures. Existing VLA models control well but lack an internal completion signal to guide transitions between subtasks.

Method: Extend pi0 with a lightweight subtask-completion detection head (dual-head architecture) to both generate manipulation actions and trigger subtask transitions. Explore four finetuning strategies varying joint vs. sequential optimization and frozen vs. unfrozen backbones across two datasets (salad packing, candy packing).

Result: SeqVLA outperforms pi0 and other baselines in overall success rate across both tasks. Joint finetuning with an unfrozen backbone yields the most reliable completion predictions, eliminating sequence-related failures and enabling robust long-horizon execution.

Conclusion: Coupling action generation with subtask-aware detection is crucial for scalable sequential manipulation and reduces cascading errors in long-horizon tasks.

Abstract: Long-horizon robotic manipulation tasks require executing multiple
interdependent subtasks in strict sequence, where errors in detecting subtask
completion can cascade into downstream failures. Existing
Vision-Language-Action (VLA) models such as $\pi_0$ excel at continuous
low-level control but lack an internal signal for identifying when a subtask
has finished, making them brittle in sequential settings. We propose SeqVLA, a
completion-aware extension of $\pi_0$ that augments the base architecture with
a lightweight detection head perceiving whether the current subtask is
complete. This dual-head design enables SeqVLA not only to generate
manipulation actions but also to autonomously trigger transitions between
subtasks. We investigate four finetuning strategies that vary in how the action
and detection heads are optimized (joint vs. sequential finetuning) and how
pretrained knowledge is preserved (full finetuning vs. frozen backbone).
Experiments are performed on two multi-stage tasks: salad packing with seven
distinct subtasks and candy packing with four distinct subtasks. Results show
that SeqVLA significantly outperforms the baseline $\pi_0$ and other strong
baselines in overall success rate. In particular, joint finetuning with an
unfrozen backbone yields the most decisive and statistically reliable
completion predictions, eliminating sequence-related failures and enabling
robust long-horizon execution. Our results highlight the importance of coupling
action generation with subtask-aware detection for scalable sequential
manipulation.

</details>


### [221] [CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping](https://arxiv.org/abs/2509.14143)
*Zijian An,Ran Yang,Yiming Feng,Lifeng Zhou*

Main category: cs.RO

TL;DR: CLAW decouples condition monitoring from action generation by using a fine-tuned CLIP as a prompt generator that reads a scale and emits discrete directives based on weight thresholds, which are then fed into a flow-based VLA policy (pi_0) to produce continuous robot actions. This separation enables weight-aware control and improves over baseline VLA models.


<details>
  <summary>Details</summary>
Motivation: Current vision-language-action (VLA) policies often fail to satisfy precise numeric task constraints because their observation-to-action mappings are learned end-to-end and lack explicit condition-monitoring mechanisms. There is a need to introduce symbolic/threshold-based reasoning to ensure constraint satisfaction while retaining high-frequency visuomotor control.

Method: CLAW uses a fine-tuned CLIP model as a lightweight prompt generator that continuously monitors the digital readout of a scale and generates discrete directives according to task-specific weight thresholds. These prompts are consumed by pi_0, a flow-based VLA policy, which fuses the prompts with multi-view camera observations to produce continuous robot actions.

Result: The authors validate CLAW on three experimental setups, including single-object grasping and mixed-object tasks requiring dual-arm manipulation. CLAW consistently executes weight-aware behaviors and outperforms both raw-π_0 and fine-tuned π_0 baselines.

Conclusion: Decoupling condition evaluation from action generation and combining symbolic weight reasoning with high-frequency visuomotor control via CLAW yields reliable weight-aware manipulation and improves over end-to-end VLA baselines; the results suggest promise for constraint-satisfying robotic control and can be complemented by supplementary video materials.

Abstract: Vision-language-action (VLA) models have recently emerged as a promising
paradigm for robotic control, enabling end-to-end policies that ground natural
language instructions into visuomotor actions. However, current VLAs often
struggle to satisfy precise task constraints, such as stopping based on numeric
thresholds, since their observation-to-action mappings are implicitly shaped by
training data and lack explicit mechanisms for condition monitoring. In this
work, we propose CLAW (CLIP-Language-Action for Weight), a framework that
decouples condition evaluation from action generation. CLAW leverages a
fine-tuned CLIP model as a lightweight prompt generator, which continuously
monitors the digital readout of a scale and produces discrete directives based
on task-specific weight thresholds. These prompts are then consumed by $\pi_0$,
a flow-based VLA policy, which integrates the prompts with multi-view camera
observations to produce continuous robot actions. This design enables CLAW to
combine symbolic weight reasoning with high-frequency visuomotor control. We
validate CLAW on three experimental setups: single-object grasping and
mixed-object tasks requiring dual-arm manipulation. Across all conditions, CLAW
reliably executes weight-aware behaviors and outperforms both raw-$\pi_0$ and
fine-tuned $\pi_0$ models. We have uploaded the videos as supplementary
materials.

</details>


### [222] [StableTracker: Learning to Stably Track Target via Differentiable Simulation](https://arxiv.org/abs/2509.14147)
*Fanxing Li,Shengyang Wang,Fangyu Sun,Shuyu Wu,Dexin Zuo,Wenxian Yu,Danping Zou*

Main category: cs.RO

TL;DR: A differentiable-simulation trained, learning-based quadrotor controller (StableTracker) that robustly tracks a moving target from arbitrary viewpoints by keeping it centered and at a fixed distance, outperforming traditional and learning baselines in simulation and validated on real hardware.


<details>
  <summary>Details</summary>
Motivation: FPV object tracking often relies on handcrafted modular designs that cause hardware overload and cumulative error, leading to degraded tracking performance for fast-changing targets; a robust, generalizable autonomous aerial camera is desired.

Method: Train a policy via backpropagation-through-time in differentiable simulation to control the quadrotor so that the target remains at the center of the visual field in both horizontal and vertical directions while maintaining a fixed relative distance; evaluate against state-of-the-art traditional algorithms and learning baselines; include a real-world onboard-computer experiment.

Result: In simulation, the policy achieves superior accuracy, stability, and generalization across varying safe distances, trajectories, and target velocities; a real-world quadrotor test validates practicality of the approach.

Conclusion: Demonstrates feasibility and effectiveness of a learning-based control policy for robust FPV tracking, functioning as an autonomous aerial camera; outperforms traditional and learning baselines and shows transfer from simulation to real hardware.

Abstract: FPV object tracking methods heavily rely on handcraft modular designs,
resulting in hardware overload and cumulative error, which seriously degrades
the tracking performance, especially for rapidly accelerating or decelerating
targets. To address these challenges, we present \textbf{StableTracker}, a
learning-based control policy that enables quadrotors to robustly follow the
moving target from arbitrary perspectives. The policy is trained using
backpropagation-through-time via differentiable simulation, allowing the
quadrotor to maintain the target at the center of the visual field in both
horizontal and vertical directions, while keeping a fixed relative distance,
thereby functioning as an autonomous aerial camera. We compare StableTracker
against both state-of-the-art traditional algorithms and learning baselines.
Simulation experiments demonstrate that our policy achieves superior accuracy,
stability and generalization across varying safe distances, trajectories, and
target velocities. Furthermore, a real-world experiment on a quadrotor with an
onboard computer validated practicality of the proposed approach.

</details>


### [223] [MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies](https://arxiv.org/abs/2509.14159)
*Dayi Dong,Maulik Bhatt,Seoyeon Choi,Negar Mehr*

Main category: cs.RO

TL;DR: Proposes MIMIC-D, a diffusion-policy based CTDE framework for multi-modal multi-agent imitation learning that enables decentralized execution while preserving the coordination learned during centralized training, outperforming baselines in simulation and hardware.


<details>
  <summary>Details</summary>
Motivation: Coordination among robots and humans on multi-modal tasks is essential but hard: expert demonstrations can exhibit multiple valid strategies; standard imitation learning struggles to capture this multimodality. Diffusion models are well-suited for multi-modal distributions; prior diffusion-based MARL often relies on centralized planning or explicit communication, which may not be feasible in real-world settings.

Method: Introduce MIMIC-D: Centralized Training, Decentralized Execution using diffusion policies. Train agents jointly with full information to capture multi-modal coordination; at execution time, agents act using only local observations to achieve implicit coordination. Demonstrations used to train diffusion-based policies.

Result: Empirical evaluation in simulation and hardware shows the method recovers multi-modal coordination among agents across tasks and environments, outperforming state-of-the-art baselines.

Conclusion: CTDE with diffusion policies enables robust, multi-modal coordinated behavior in multi-agent systems without requiring centralized planning or explicit communication at execution; the approach scales to real-world robotic coordination tasks.

Abstract: As robots become more integrated in society, their ability to coordinate with
other robots and humans on multi-modal tasks (those with multiple valid
solutions) is crucial. We propose to learn such behaviors from expert
demonstrations via imitation learning (IL). However, when expert demonstrations
are multi-modal, standard IL approaches can struggle to capture the diverse
strategies, hindering effective coordination. Diffusion models are known to be
effective at handling complex multi-modal trajectory distributions in
single-agent systems. Diffusion models have also excelled in multi-agent
scenarios where multi-modality is more common and crucial to learning
coordinated behaviors. Typically, diffusion-based approaches require a
centralized planner or explicit communication among agents, but this assumption
can fail in real-world scenarios where robots must operate independently or
with agents like humans that they cannot directly communicate with. Therefore,
we propose MIMIC-D, a Centralized Training, Decentralized Execution (CTDE)
paradigm for multi-modal multi-agent imitation learning using diffusion
policies. Agents are trained jointly with full information, but execute
policies using only local information to achieve implicit coordination. We
demonstrate in both simulation and hardware experiments that our method
recovers multi-modal coordination behavior among agents in a variety of tasks
and environments, while improving upon state-of-the-art baselines.

</details>


### [224] [\textsc{Gen2Real}: Towards Demo-Free Dexterous Manipulation by Harnessing Generated Video](https://arxiv.org/abs/2509.14178)
*Kai Ye,Yuhang Wu,Shuyuan Hu,Junliang Li,Meng Liu,Yongquan Chen,Rui Huang*

Main category: cs.RO

TL;DR: A framework (Gen2Real) that learns dexterous manipulation from a single generated video, transforming video into robot hand-object trajectories via pose/depth estimation, enforcing physics with PIOM, and stabilizing control with anchor-based residual PPO; achieves 77.3% success in simulation, shows real-robot demonstrations, and supports natural-language task specification.


<details>
  <summary>Details</summary>
Motivation: Collecting extensive human demonstrations for dexterous manipulation is costly and impractical. Leveraging generated videos and vision-based estimation can bypass manual demos and enable scalable learning of grasping skills.

Method: 1) Demonstration generation: generate a video and estimate hand pose/depth to create hand-object trajectories. 2) Trajectory optimization: Physics-aware Interaction Optimization Model (PIOM) enforces physics consistency. 3) Demonstration learning: retarget human motions to a robot hand and stabilize via an anchor-based residual PPO policy. 4) Capabilities: learn from generated videos alone and allow natural-language task specification.

Result: Policy achieves 77.3% success on grasping tasks in simulation; demonstrates coherent real-world executions; ablation studies validate contributions; shows ability to specify tasks with natural language.

Conclusion: Gen2Real shows that dexterous manipulation can be learned from imagined/generated videos without human demonstrations, using physics-aware optimization and an anchored PPO policy to generalize grasping skills to real-world robots and support flexible task specification.

Abstract: Dexterous manipulation remains a challenging robotics problem, largely due to
the difficulty of collecting extensive human demonstrations for learning. In
this paper, we introduce \textsc{Gen2Real}, which replaces costly human demos
with one generated video and drives robot skill from it: it combines
demonstration generation that leverages video generation with pose and depth
estimation to yield hand-object trajectories, trajectory optimization that uses
Physics-aware Interaction Optimization Model (PIOM) to impose physics
consistency, and demonstration learning that retargets human motions to a robot
hand and stabilizes control with an anchor-based residual Proximal Policy
Optimization (PPO) policy. Using only generated videos, the learned policy
achieves a 77.3\% success rate on grasping tasks in simulation and demonstrates
coherent executions on a real robot. We also conduct ablation studies to
validate the contribution of each component and demonstrate the ability to
directly specify tasks using natural language, highlighting the flexibility and
robustness of \textsc{Gen2Real} in generalizing grasping skills from imagined
videos to real-world execution.

</details>


### [225] [MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping](https://arxiv.org/abs/2509.14191)
*Zhihao Cao,Hanyu Wu,Li Wa Tang,Zizhou Luo,Zihan Zhu,Wei Zhang,Marc Pollefeys,Martin R. Oswald*

Main category: cs.RO

TL;DR: A new RGB-only multi-camera SLAM using 3D Gaussian Splatting (MCGS-SLAM) achieves real-time, dense, high-fidelity reconstructions and accurate trajectories, outperforming monocular baselines and enabling side-view region reconstruction due to wide FoV.


<details>
  <summary>Details</summary>
Motivation: Dense, robust SLAM with comprehensive geometric coverage is challenging for monocular and sparse-map approaches; multi-camera dense fusion can improve accuracy, coverage, and safety-critical perception for robotics and autonomous driving.

Method: MCGS-SLAM fuses dense RGB inputs from multiple viewpoints into a unified 3D Gaussian map built on Gaussian Splatting. It introduces a multi-camera bundle adjustment (MCBA) that jointly refines poses and depths using dense photometric and geometric residuals, plus a scale-consistency module that enforces metric alignment across views via low-rank priors. The system operates on RGB data and aims for real-time performance at large scale.

Result: Experiments on synthetic and real-world datasets show accurate trajectories and photorealistic reconstructions, with performance typically surpassing monocular baselines. The wide field of view from multiple cameras enables reconstruction of side-view regions that monocular setups miss, enhancing safety for autonomous operation.

Conclusion: Multi-camera Gaussian Splatting SLAM (MCGS-SLAM) is a promising approach for high-fidelity, real-time mapping in robotics and autonomous driving, leveraging dense multi-view fusion to improve robustness, coverage, and scene fidelity.

Abstract: Recent progress in dense SLAM has primarily targeted monocular setups, often
at the expense of robustness and geometric coverage. We present MCGS-SLAM, the
first purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting
(3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM
fuses dense RGB inputs from multiple viewpoints into a unified, continuously
optimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines
poses and depths via dense photometric and geometric residuals, while a scale
consistency module enforces metric alignment across views using low-rank
priors. The system supports RGB input and maintains real-time performance at
large scale. Experiments on synthetic and real-world datasets show that
MCGS-SLAM consistently yields accurate trajectories and photorealistic
reconstructions, usually outperforming monocular baselines. Notably, the wide
field of view from multi-camera input enables reconstruction of side-view
regions that monocular setups miss, critical for safe autonomous operation.
These results highlight the promise of multi-camera Gaussian Splatting SLAM for
high-fidelity mapping in robotics and autonomous driving.

</details>


### [226] [GLIDE: A Coordinated Aerial-Ground Framework for Search and Rescue in Unknown Environments](https://arxiv.org/abs/2509.14210)
*Seth Farrell,Chenghao Li,Hongzhan Yu,Hesam Mojtahedi,Sicun Gao,Henrik I. Christensen*

Main category: cs.RO

TL;DR: A cooperative aerial-ground SAR framework (GLIDE) uses two UAVs and one UGV to rapidly localize victims and navigate unknown terrain by separating roles: a goal-searching UAV detects victims and nominates goals for the ground vehicle, while a terrain-scouting UAV ahead of the UGV provides traversability updates; the UGV fuses aerial cues with local sensing for fast A* planning and replanning. Hardware demonstration and simulations validate improved reach time and navigation safety.


<details>
  <summary>Details</summary>
Motivation: In time-critical search-and-rescue, single-platform systems struggle with rapid victim localization and safe navigation in unknown environments. Coordinated aerial-ground collaboration can leverage UAVs for planning and terrain assessment to reduce response time and improve safety.

Method: A three-agent cooperative framework (GLIDE) with: (1) a goal-searching UAV performing real-time victim detection and georeferencing to nominate goals for the UGV; (2) a terrain-scouting UAV flying ahead of the UGV’s route to provide mid-level traversability updates; (3) a UGV fusing aerial cues with local sensing to execute fast A* planning and continuous replanning as new information arrives. Hardware demonstration using a GEM e6 UGV and two X500 UAVs, and simulation ablations to evaluate the planning stack in isolation from detection.

Result: Explicit role separation between UAVs, terrain scouting, and guided planning improves reach time and navigation safety in time-critical SAR missions, as evidenced by hardware demonstrations and simulation-based ablations.

Conclusion: The GLIDE framework enables efficient, safe cooperative SAR by combining role specialization and integrated sensing for long-horizon planning, with practical validation through hardware and simulations.

Abstract: We present a cooperative aerial-ground search-and-rescue (SAR) framework that
pairs two unmanned aerial vehicles (UAVs) with an unmanned ground vehicle (UGV)
to achieve rapid victim localization and obstacle-aware navigation in unknown
environments. We dub this framework Guided Long-horizon Integrated Drone Escort
(GLIDE), highlighting the UGV's reliance on UAV guidance for long-horizon
planning. In our framework, a goal-searching UAV executes real-time onboard
victim detection and georeferencing to nominate goals for the ground platform,
while a terrain-scouting UAV flies ahead of the UGV's planned route to provide
mid-level traversability updates. The UGV fuses aerial cues with local sensing
to perform time-efficient A* planning and continuous replanning as information
arrives. Additionally, we present a hardware demonstration (using a GEM e6 golf
cart as the UGV and two X500 UAVs) to evaluate end-to-end SAR mission
performance and include simulation ablations to assess the planning stack in
isolation from detection. Empirical results demonstrate that explicit role
separation across UAVs, coupled with terrain scouting and guided planning,
improves reach time and navigation safety in time-critical SAR missions.

</details>


### [227] [Multi-robot Multi-source Localization in Complex Flows with Physics-Preserving Environment Models](https://arxiv.org/abs/2509.14228)
*Benjamin Shaffer,Victoria Edwards,Brooks Kinch,Nathaniel Trask,M. Ani Hsieh*

Main category: cs.RO

TL;DR: A distributed sensing framework for source localization in complex flows using per-robot ML-derived finite element models to guide infotaxis via approximate mutual information; improves localization speed and accuracy over baselines.


<details>
  <summary>Details</summary>
Motivation: Complex, time-varying chaotic flows cause intermittent sensor readings and make modeling/ prediction hard. Onboard computation is limited, so efficient, information-driven sampling is needed for accurate source localization.

Method: Each robot carries a machine-learned finite element model of its environment to guide information-based sampling. These models evaluate an approximate mutual information criterion to drive an infotaxis control strategy, selecting sensing regions that maximize informativeness for the source localization objective.

Result: Faster error reduction compared to baseline sensing strategies; more accurate source localization compared to baseline machine learning approaches.

Conclusion: Integrating machine-learned FEM models with infotaxis enables effective, information-driven sensing for source localization in challenging dispersion environments under computational constraints.

Abstract: Source localization in a complex flow poses a significant challenge for
multi-robot teams tasked with localizing the source of chemical leaks or
tracking the dispersion of an oil spill. The flow dynamics can be time-varying
and chaotic, resulting in sporadic and intermittent sensor readings, and
complex environmental geometries further complicate a team's ability to model
and predict the dispersion. To accurately account for the physical processes
that drive the dispersion dynamics, robots must have access to computationally
intensive numerical models, which can be difficult when onboard computation is
limited. We present a distributed mobile sensing framework for source
localization in which each robot carries a machine-learned, finite element
model of its environment to guide information-based sampling. The models are
used to evaluate an approximate mutual information criterion to drive an
infotaxis control strategy, which selects sensing regions that are expected to
maximize informativeness for the source localization objective. Our approach
achieves faster error reduction compared to baseline sensing strategies and
results in more accurate source localization compared to baseline machine
learning approaches.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [228] [An open-source heuristic to reboot 2D nesting research](https://arxiv.org/abs/2509.13329)
*Jeroen Gardeyn,Greet Vanden Berghe,Tony Wauters*

Main category: cs.CG

TL;DR: Introduces sparrow, an open-source heuristic for 2D irregular strip packing that breaks the problem into a sequence of feasibility checks to resolve item collisions; reports substantial performance gains and adds ten new real-world benchmark instances.


<details>
  <summary>Details</summary>
Motivation: 2D nesting problems are highly challenging and research progress has stalled; high barriers to entry and lack of reproducibility hinder advancement, despite practical importance.

Method: Decomposes the optimization into a sequence of feasibility problems, gradually resolving overlaps between items; implements an open-source heuristic (sparrow) and evaluates it on ten new real-world instances plus existing benchmarks.

Result: Sparrow consistently outperforms the state of the art, sometimes by a wide margin, across tested instances.

Conclusion: Releasing sparrow's source code addresses reproducibility and entry-barrier concerns, potentially rebooting the research culture; nevertheless, there remains substantial room for further algorithmic improvements.

Abstract: 2D nesting problems rank among the most challenging cutting and packing
problems. Yet, despite their practical relevance, research over the past decade
has seen remarkably little progress. One reasonable explanation could be that
nesting problems are already solved to near optimality, leaving little room for
improvement. However, as our paper demonstrates, we are not at the limit after
all. This paper presents $\texttt{sparrow}$, an open-source heuristic approach
to solving 2D irregular strip packing problems, along with ten new real-world
instances for benchmarking. Our approach decomposes the optimization problem
into a sequence of feasibility problems, where collisions between items are
gradually resolved. $\texttt{sparrow}$ consistently outperforms the state of
the art - in some cases by an unexpectedly wide margin. We are therefore
convinced that the aforementioned stagnation is better explained by both a high
barrier to entry and a widespread lack of reproducibility. By releasing
$\texttt{sparrow}$'s source code, we directly address both issues. At the same
time, we are confident there remains significant room for further algorithmic
improvement. The ultimate aim of this paper is not only to take a single step
forward, but to reboot the research culture in the domain and enable continued,
reproducible progress.

</details>
