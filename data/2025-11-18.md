<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 296]
- [cs.AI](#cs.AI) [Total: 82]
- [cs.CG](#cs.CG) [Total: 2]
- [cs.RO](#cs.RO) [Total: 55]
- [cs.LG](#cs.LG) [Total: 281]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Psychological stress during Examination and its estimation by handwriting in answer script](https://arxiv.org/abs/2511.11633)
*Abhijeet Kumar,Chetan Agarwal,Pronoy B. Neogi,Mayank Goswami*

Main category: cs.CV

TL;DR: A data-driven AI framework fusing handwriting analysis with OCR and transformer-based sentiment models to compute a numerical Stress Index from exam scripts, using a five-model ensemble and anomaly detection for robustness.


<details>
  <summary>Details</summary>
Motivation: To quantify psychological stress in students during exams and move beyond traditional grading by extracting cognitive-emotional states from handwritten work, enabling academic forensic insights.

Method: High-resolution image processing; TrOCR for handwriting recognition; RoBERTa-based sentiment analysis; sentiment entropy fusion; numerical Stress Index; five-model voting ensemble; unsupervised anomaly detection; integration of graphology-inspired features with AI.

Result: Proposes a robust Stress Index via ensemble voting and anomaly detection; demonstrates a data-driven framework that transcends grading and yields insights into cognitive/emotional states during examinations.

Conclusion: An innovative academic forensic framework integrating graphology and AI to quantify exam-related stress, enabling deeper cognitive-emotional insights and potential applications in assessment and wellbeing monitoring.

Abstract: This research explores the fusion of graphology and artificial intelligence to quantify psychological stress levels in students by analyzing their handwritten examination scripts. By leveraging Optical Character Recognition and transformer based sentiment analysis models, we present a data driven approach that transcends traditional grading systems, offering deeper insights into cognitive and emotional states during examinations. The system integrates high resolution image processing, TrOCR, and sentiment entropy fusion using RoBERTa based models to generate a numerical Stress Index. Our method achieves robustness through a five model voting mechanism and unsupervised anomaly detection, making it an innovative framework in academic forensics.

</details>


### [2] [Real-time pothole detection with onboard sensors and camera on vehicles](https://arxiv.org/abs/2511.11643)
*Aswath Muthuselvam,Jeevak Raj S,Mohanaprasad K*

Main category: cs.CV

TL;DR: Real-time pothole detection using onboard vehicle sensors with an SVM classifier achieving 98.1% accuracy on a ~2 km road dataset with 26 potholes; code available.


<details>
  <summary>Details</summary>
Motivation: Improve road maintenance and traffic flow by detecting potholes in real time using onboard sensors.

Method: Train/apply an SVM classifier on onboard-sensor data collected from vehicles over a ~2 km road segment containing 26 potholes to identify pothole instances.

Result: SVM achieved 98.1% accuracy on the dataset.

Conclusion: Real-time pothole detection via onboard sensors is feasible with SVM, enabling large-scale pothole data collection and management; code is publicly available for replication.

Abstract: Road conditions play an important role in our everyday commute. With the proliferating number of vehicles on the road each year, it has become necessary to access the road conditions very frequently, this would ensure that the traffic also flows smoothly. Even the smallest crack in the road could be easily be chipped into a large pothole due to changing surface temperatures of the road and from the force of vehicles riding over it. In this paper, we have addressed how we could better identify these potholes in realtime with the help of onboard sensors in vehicles so that the data could be useful for analysis and better management of potholes on a large scale. For the implementation, we used an SVM classifier to detect potholes, we achieved 98.1% accuracy based on data collected from a local road for about 2 km which had 26 potholes distributed along the road. Code is available at: https://github.com/aswathselvam/Potholes

</details>


### [3] [A Method for Identifying Farmland System Habitat Types Based on the Dynamic-Weighted Feature Fusion Network Model](https://arxiv.org/abs/2511.11659)
*Kesong Zheng,Zhi Song,Peizhou Li,Shuyi Yao,Zhenxing Bian*

Main category: cs.CV

TL;DR: A 15-class ultra-high-resolution dataset for cultivated-land habitats and a DWFF-Net that fuses semantic-texture features via adaptive multi-layer fusion with a frozen DINOv3 encoder and dynamic-weight decoder, achieving competitive segmentation metrics.


<details>
  <summary>Details</summary>
Motivation: There is no standardized habitat classification for cultivated lands, incomplete category coverage, and current models struggle to integrate semantic and texture features, leading to poor multi-scale segmentation accuracy and blurred boundaries.

Method: Create an annotated ultra-high-resolution remote sensing dataset with 15 cultivated-land habitat classes. Develop DWFF-Net: encoder uses a frozen-parameter DINOv3 for feature extraction; introduce a data-level adaptive dynamic weighting strategy for feature fusion; decoder includes a dynamic weight computation network for integrating multi-layer features; employ a hybrid loss function; evaluate with ablation studies.

Result: On the dataset, DWFF-Net achieves mean IoU of 0.6979 and F1-score of 0.8049, surpassing a baseline by 0.021 IoU and 0.0161 F1. Ablation results confirm the complementary value of multi-layer feature fusion, notably improving IoU for micro-habitats like field ridges.

Conclusion: The study establishes a habitat identification framework for cultivated land systems based on adaptive multi-layer feature fusion, enabling sub-meter precision habitat mapping at low cost and supporting robust fine-grained habitat monitoring in cultivated landscapes.

Abstract: Addressing the current lack of a standardized habitat classification system for cultivated land ecosystems, incomplete coverage of habitat types, and the inability of existing models to effectively integrate semantic and texture features-resulting in insufficient segmentation accuracy and blurred boundaries for multi-scale habitats (e.g., large-scale field plots and micro-habitats)-this study developed a comprehensively annotated ultra-high-resolution remote sensing image dataset encompassing 15 categories of cultivated land system habitats. Furthermore, we propose a Dynamic-Weighted Feature Fusion Network (DWFF-Net). The encoder of this model utilizes a frozen-parameter DINOv3 to extract foundational features. By analyzing the relationships between different category images and feature maps, we introduce a data-level adaptive dynamic weighting strategy for feature fusion. The decoder incorporates a dynamic weight computation network to achieve thorough integration of multi-layer features, and a hybrid loss function is adopted to optimize model training. Experimental results on the constructed dataset demonstrate that the proposed model achieves a mean Intersection over Union (mIoU) of 0.6979 and an F1-score of 0.8049, outperforming the baseline network by 0.021 and 0.0161, respectively. Ablation studies further confirm the complementary nature of multi-layer feature fusion, which effectively improves the IoU for micro-habitat categories such as field ridges. This study establishes a habitat identification framework for cultivated land systems based on adaptive multi-layer feature fusion, enabling sub-meter precision habitat mapping at a low cost and providing robust technical support for fine-grained habitat monitoring in cultivated landscapes.

</details>


### [4] [AGENet: Adaptive Edge-aware Geodesic Distance Learning for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2511.11662)
*Ziyuan Gao*

Main category: cs.CV

TL;DR: Proposes AGENet, a lightweight, edge-aware geodesic distance-based few-shot medical image segmentation method that improves boundary precision with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation needs large annotated datasets; few-shot methods struggle with precise boundary delineation, especially when anatomically similar regions lack adequate spatial context. The paper introduces a geometry-guided, computationally efficient approach to improve boundary accuracy with limited data.

Method: Three components: (1) an edge-aware geodesic distance learning module using iterative Fast Marching refinement to respect anatomical boundaries; (2) adaptive prototype extraction with spatially-weighted aggregation to capture global structure and local boundary details; (3) adaptive parameter learning to automatically adjust to different organ characteristics.

Result: Extensive experiments across diverse medical imaging datasets show improvements over state-of-the-art methods, with reduced boundary errors and maintained computational efficiency, indicating strong potential for clinical deployment in low-annotation scenarios.

Conclusion: The framework offers precise segmentation with limited annotated data and computational efficiency, making it suitable for clinical applications requiring accurate boundaries without large annotation burdens.

Abstract: Medical image segmentation requires large annotated datasets, creating a significant bottleneck for clinical applications. While few-shot segmentation methods can learn from minimal examples, existing approaches demonstrate suboptimal performance in precise boundary delineation for medical images, particularly when anatomically similar regions appear without sufficient spatial context. We propose AGENet (Adaptive Geodesic Edge-aware Network), a novel framework that incorporates spatial relationships through edge-aware geodesic distance learning. Our key insight is that medical structures follow predictable geometric patterns that can guide prototype extraction even with limited training data. Unlike methods relying on complex architectural components or heavy neural networks, our approach leverages computationally lightweight geometric modeling. The framework combines three main components: (1) An edge-aware geodesic distance learning module that respects anatomical boundaries through iterative Fast Marching refinement, (2) adaptive prototype extraction that captures both global structure and local boundary details via spatially-weighted aggregation, and (3) adaptive parameter learning that automatically adjusts to different organ characteristics. Extensive experiments across diverse medical imaging datasets demonstrate improvements over state-of-the-art methods. Notably, our method reduces boundary errors compared to existing approaches while maintaining computational efficiency, making it highly suitable for clinical applications requiring precise segmentation with limited annotated data.

</details>


### [5] [EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance](https://arxiv.org/abs/2511.11700)
*Jiahui Wang,Haiyue Zhu,Haoren Guo,Abdullah Al Mamun,Cheng Xiang,Tong Heng Lee*

Main category: cs.CV

TL;DR: A pre-training-free network (EPSegFZ) for few- and zero-shot 3D point cloud segmentation using ProERA, DRPE, and LGPE to enhance feature extraction and leverage textual support, achieving state-of-the-art on S3DIS and ScanNet without pre-training.


<details>
  <summary>Details</summary>
Motivation: Current few-shot 3D segmentation relies on pre-training, which hurts adaptability. Existing non-pre-training approaches underutilize support information (especially textual data) and struggle with zero-shot capabilities.

Method: Introduce EPSegFZ with three components: (1) ProERA for prototype-enhanced attention to improve feature extraction without pre-training, (2) DRPE-based cross-attention to align query features with prototypes, (3) LGPE to embed and leverage textual annotations from the support set to improve few-shot performance and enable zero-shot inference.

Result: Empirical results show state-of-the-art gains, with improvements of 5.68% on S3DIS and 3.82% on ScanNet compared to the prior best.

Conclusion: A pre-training-free architecture with language-guided prototypes and enhanced attention can achieve strong few- and zero-shot 3D segmentation, reducing dependence on pre-training and better utilizing support information.

Abstract: Recent approaches for few-shot 3D point cloud semantic segmentation typically require a two-stage learning process, i.e., a pre-training stage followed by a few-shot training stage. While effective, these methods face overreliance on pre-training, which hinders model flexibility and adaptability. Some models tried to avoid pre-training yet failed to capture ample information. In addition, current approaches focus on visual information in the support set and neglect or do not fully exploit other useful data, such as textual annotations. This inadequate utilization of support information impairs the performance of the model and restricts its zero-shot ability. To address these limitations, we present a novel pre-training-free network, named Efficient Point Cloud Semantic Segmentation for Few- and Zero-shot scenarios. Our EPSegFZ incorporates three key components. A Prototype-Enhanced Registers Attention (ProERA) module and a Dual Relative Positional Encoding (DRPE)-based cross-attention mechanism for improved feature extraction and accurate query-prototype correspondence construction without pre-training. A Language-Guided Prototype Embedding (LGPE) module that effectively leverages textual information from the support set to improve few-shot performance and enable zero-shot inference. Extensive experiments show that our method outperforms the state-of-the-art method by 5.68% and 3.82% on the S3DIS and ScanNet benchmarks, respectively.

</details>


### [6] [Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement](https://arxiv.org/abs/2511.11702)
*Lian He,Meng Liu,Qilang Ye,Yu Zhou,Xiang Deng,Gangyi Ding*

Main category: cs.CV

TL;DR: Geometry-optimized 3D scene-level affordance segmentation (TASA) that fuses 2D cues with 3D geometry in a coarse-to-fine framework for efficient, accurate predictions.


<details>
  <summary>Details</summary>
Motivation: Current methods largely ignore rich 3D geometry in scenes and either focus on object-level affordances or simply lift 2D predictions to 3D, leading to inefficiency and limited reasoning.

Method: A two-module approach: (1) task-aware 2D affordance detection to identify manipulable points and guide view selection; (2) 3D affordance refinement integrating 2D semantic priors with local 3D geometry to produce coherent 3D masks. Uses coarse-to-fine processing and leverages 2D priors with 3D geometry.

Result: Significant improvements over baselines on SceneFun3D in both accuracy and efficiency for scene-level affordance segmentation.

Conclusion: TASA demonstrates that combining geometry-aware 3D reasoning with task-aware 2D cues yields robust, scalable scene-level affordance understanding, paving the way for embodied agents to interact more effectively.

Abstract: Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.

</details>


### [7] [LE-CapsNet: A Light and Enhanced Capsule Network](https://arxiv.org/abs/2511.11708)
*Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: LE-CapsNet is a lighter, faster, and more accurate Capsule Network variant. It uses 3.8M weights to achieve 76.73% accuracy on CIFAR-10 and runs inference about 4x faster than CapsNet, while also showing improved affine-robustness (AffNIST 94.3% vs 90.52%).


<details>
  <summary>Details</summary>
Motivation: Capsule Networks offer advantages in recognizing overlapping categories and transformed images but suffer from high computational cost and many parameters. There is a need for a lighter, faster, and more robust CapsNet variant.

Method: Propose LE-CapsNet, a light, enhanced Capsule Network with about 3.8M parameters. Train and evaluate on CIFAR-10 and AffNIST, comparing performance to the original CapsNet to demonstrate gains in speed, accuracy, and affine robustness.

Result: LE-CapsNet achieves 76.73% accuracy on CIFAR-10 with 3.8M weights and about 4x faster inference than CapsNet. It also improves AffNIST accuracy to 94.3% (vs. CapsNet 90.52%).

Conclusion: LE-CapsNet demonstrates that a lighter Capsule Network can simultaneously improve inference speed and accuracy while enhancing robustness to affine transformations, though broader validation on additional datasets would help establish generalizability.

Abstract: Capsule Network (CapsNet) classifier has several advantages over CNNs, including better detection of images containing overlapping categories and higher accuracy on transformed images. Despite the advantages, CapsNet is slow due to its different structure. In addition, CapsNet is resource-hungry, includes many parameters and lags in accuracy compared to CNNs. In this work, we propose LE-CapsNet as a light, enhanced and more accurate variant of CapsNet. Using 3.8M weights, LECapsNet obtains 76.73% accuracy on the CIFAR-10 dataset while performing inference 4x faster than CapsNet. In addition, our proposed network is more robust at detecting images with affine transformations compared to CapsNet. We achieve 94.3% accuracy on the AffNIST dataset (compared to CapsNet 90.52%).

</details>


### [8] [Target-Balanced Score Distillation](https://arxiv.org/abs/2511.11710)
*Zhou Xu,Qi Wang,Yuxiao Yang,Luyuan Zhang,Zhang Liang,Yang Li*

Main category: cs.CV

TL;DR: TBSD resolves the texture-geometry trade-off in Score Distillation Sampling by balancing texture fidelity and shape integrity via an adaptive multi-objective optimization, leveraging insights on Target Negative Prompts.


<details>
  <summary>Details</summary>
Motivation: Address the over-saturation/over-smoothing in SDS and the trade-off introduced by negative prompts; mitigate texture gains at the expense of geometry without sacrificing texture realism.

Method: Introduce Target-Balanced Score Distillation (TBSD) that treats generation as a multi-objective optimization and employs an adaptive strategy to balance texture realism/fidelity with geometric accuracy, informed by analysis of Target Negative Prompts (TNP).

Result: TBSD significantly outperforms existing methods, producing 3D assets with high-fidelity textures and geometrically accurate shapes in extensive experiments.

Conclusion: TBSD effectively resolves the texture-geometry trade-off in 3D asset generation from 2D diffusion priors, enabling high-quality textures without compromising shape integrity.

Abstract: Score Distillation Sampling (SDS) enables 3D asset generation by distilling priors from pretrained 2D text-to-image diffusion models, but vanilla SDS suffers from over-saturation and over-smoothing. To mitigate this issue, recent variants have incorporated negative prompts. However, these methods face a critical trade-off: limited texture optimization, or significant texture gains with shape distortion. In this work, we first conduct a systematic analysis and reveal that this trade-off is fundamentally governed by the utilization of the negative prompts, where Target Negative Prompts (TNP) that embed target information in the negative prompts dramatically enhancing texture realism and fidelity but inducing shape distortions. Informed by this key insight, we introduce the Target-Balanced Score Distillation (TBSD). It formulates generation as a multi-objective optimization problem and introduces an adaptive strategy that effectively resolves the aforementioned trade-off. Extensive experiments demonstrate that TBSD significantly outperforms existing state-of-the-art methods, yielding 3D assets with high-fidelity textures and geometrically accurate shape.

</details>


### [9] [CompressNAS : A Fast and Efficient Technique for Model Compression using Decomposition](https://arxiv.org/abs/2511.11716)
*Sudhakar Sah,Nikhil Chabbra,Matthieu Durnerin*

Main category: cs.CV

TL;DR: CompressNAS enables global rank search for low-rank tensor decompositions to compress CNNs with minimal accuracy loss, achieving large gains on ImageNet/COCO and enabling a new STResNet family.


<details>
  <summary>Details</summary>
Motivation: Reduce deployment burden of large CNNs on resource-constrained MCUs/NPUs; local rank selection misses global trade-offs between compression and accuracy.

Method: Treat rank selection as a global search problem, inspired by MicroNAS; use a fast accuracy estimator to evaluate many candidate Tucker decompositions under memory/accuracy constraints; perform exhaustive-like search efficiently; apply to ResNet-18, YOLOv5s/n; derive STResNet family.

Result: ImageNet: ResNet-18 compress by 8x with <4% accuracy loss; COCO: YOLOv5s 2x compression with no accuracy drop; YOLOv5n 2x with 2.5% drop; introduces STResNet with competitive performance.

Conclusion: Global rank search via CompressNAS is effective for practical model compression; it enables substantial size reductions with controlled accuracy, and the STResNet family provides competitive efficient alternatives.

Abstract: Deep Convolutional Neural Networks (CNNs) are increasingly difficult to deploy on microcontrollers (MCUs) and lightweight NPUs (Neural Processing Units) due to their growing size and compute demands. Low-rank tensor decomposition, such as Tucker factorization, is a promising way to reduce parameters and operations with reasonable accuracy loss. However, existing approaches select ranks locally and often ignore global trade-offs between compression and accuracy. We introduce CompressNAS, a MicroNAS-inspired framework that treats rank selection as a global search problem. CompressNAS employs a fast accuracy estimator to evaluate candidate decompositions, enabling efficient yet exhaustive rank exploration under memory and accuracy constraints. In ImageNet, CompressNAS compresses ResNet-18 by 8x with less than 4% accuracy drop; on COCO, we achieve 2x compression of YOLOv5s without any accuracy drop and 2x compression of YOLOv5n with a 2.5% drop. Finally, we present a new family of compressed models, STResNet, with competitive performance compared to other efficient models.

</details>


### [10] [AdaptFly: Prompt-Guided Adaptation of Foundation Models for Low-Altitude UAV Networks](https://arxiv.org/abs/2511.11720)
*Jiao Chen,Haoyi Wang,Jianhua Tang,Junyi Wang*

Main category: cs.CV

TL;DR: Prompt-guided test-time adaptation (AdaptFly) enhances UAV segmentation without weight updates, using two modes: token-prompt retrieval for limited-resource drones and CMA-ES-based sparse visual prompt optimization for resource-rich drones, enabling cross-UAV knowledge sharing with minimal bandwidth.


<details>
  <summary>Details</summary>
Motivation: Semantic segmentation models degrade under weather, lighting, and viewpoint drift, particularly on resource-constrained UAVs. Traditional test-time adaptation is infeasible on resource-limited devices, while massive UAVs operate in isolation, wasting shared experience. A coordinated, lightweight adaptation framework could improve robustness and efficiency across a fleet.

Method: AdaptFly introduces two complementary modes. For resource-limited UAVs: lightweight token-prompt retrieval from a shared global memory to adapt prompts without updating weights. For resource-massive UAVs: gradient-free sparse visual prompt optimization using Covariance Matrix Adaptation Evolution Strategy (CMA-ES). An activation-statistic detector triggers adaptation, while a cross-UAV knowledge pool consolidates prompt knowledge and enables fleet-wide collaboration with negligible bandwidth. 

Result: Extensive experiments on UAVid and VDD benchmarks, plus real-world deployments under diverse weather, show that AdaptFly significantly improves segmentation accuracy and robustness over static models and state-of-the-art test-time adaptation baselines.

Conclusion: AdaptFly offers a practical, communication-efficient path for resilient perception in low-altitude UAV networks, enabling fleet-wide collaboration and robust segmentation without costly weight updates.

Abstract: Low-altitude Unmanned Aerial Vehicle (UAV) networks rely on robust semantic segmentation as a foundational enabler for distributed sensing-communication-control co-design across heterogeneous agents within the network. However, segmentation foundation models deteriorate quickly under weather, lighting, and viewpoint drift. Resource-limited UAVs cannot run gradient-based test-time adaptation, while resource-massive UAVs adapt independently, wasting shared experience. To address these challenges, we propose AdaptFly, a prompt-guided test-time adaptation framework that adjusts segmentation models without weight updates. AdaptFly features two complementary adaptation modes. For resource-limited UAVs, it employs lightweight token-prompt retrieval from a shared global memory. For resource-massive UAVs, it uses gradient-free sparse visual prompt optimization via Covariance Matrix Adaptation Evolution Strategy. An activation-statistic detector triggers adaptation, while cross-UAV knowledge pool consolidates prompt knowledge and enables fleet-wide collaboration with negligible bandwidth overhead. Extensive experiments on UAVid and VDD benchmarks, along with real-world UAV deployments under diverse weather conditions, demonstrate that AdaptFly significantly improves segmentation accuracy and robustness over static models and state-of-the-art TTA baselines. The results highlight a practical path to resilient, communication-efficient perception in the emerging low-altitude economy.

</details>


### [11] [Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video](https://arxiv.org/abs/2511.11725)
*Zekai Shi,Zhixi Cai,Kalin Stefanov*

Main category: cs.CV

TL;DR: Biologically inspired masking based on the human blind spot in a masked autoencoder is used to learn word-referent mappings via a contrastive video-text model, achieving performance comparable to random masking in cross-situational and temporally extended learning.


<details>
  <summary>Details</summary>
Motivation: Address how infants might ground words to referents using biologically plausible self-supervised learning; introduce masking that aligns with brain processing (blind-spot filling) to bridge vision and language.

Method: Develop a masked autoencoder visual backbone incorporating blind-spot-based masking to mimic how the brain fills gaps in the visual field, then pretrain an encoder and use it in a contrastive video-text model to learn word-referent mappings from longitudinal, egocentric data of one child.

Result: Extensive evaluation shows that the biologically plausible masking strategy is at least as effective as traditional random masking for learning word-referent mappings from cross-situational and temporally extended episodes.

Conclusion: A blind-spot-inspired masking approach provides a biologically plausible alternative to random masking and is effective for grounding word meanings to visual referents in a self-supervised, vision-language framework.

Abstract: Typically, children start to learn their first words between 6 and 9 months, linking spoken utterances to their visual referents. Without prior knowledge, a word encountered for the first time can be interpreted in countless ways; it might refer to any of the objects in the environment, their components, or attributes. Using longitudinal, egocentric, and ecologically valid data from the experience of one child, in this work, we propose a self-supervised and biologically plausible strategy to learn strong visual representations. Our masked autoencoder-based visual backbone incorporates knowledge about the blind spot in human eyes to define a novel masking strategy. This mask and reconstruct approach attempts to mimic the way the human brain fills the gaps in the eyes' field of view. This represents a significant shift from standard random masking strategies, which are difficult to justify from a biological perspective. The pretrained encoder is utilized in a contrastive learning-based video-text model capable of acquiring word-referent mappings. Extensive evaluation suggests that the proposed biologically plausible masking strategy is at least as effective as random masking for learning word-referent mappings from cross-situational and temporally extended episodes.

</details>


### [12] [GROVER: Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion](https://arxiv.org/abs/2511.11730)
*Yongjun Xiao,Dian Meng,Xinlei Huang,Yanran Liu,Shiwei Ruan,Ziyue Qiao,Xubin Zheng*

Main category: cs.CV

TL;DR: GROVER is a graph-guided, adaptive multimodal fusion framework for spatial multi-omics data, integrating transcriptomics, proteomics, and epigenomics with histology through a GCN encoder, spot-wise contrastive alignment, and dynamic expert routing to select informative modalities, achieving state-of-the-art results on real datasets.


<details>
  <summary>Details</summary>
Motivation: To enable robust, context-rich analysis of tissue by integrating molecular omics with histopathology, addressing heterogeneity across modalities, resolution mismatch, and perturbations during sample prep that distort signals.

Method: 1) Graph Convolutional Network encoder based on Kolmogorov-Arnold Networks to capture nonlinear dependencies between each modality and spatial structure, yielding modality-specific embeddings. 2) Spot-feature-pair contrastive learning to optimize correspondence across modalities at each spatial spot. 3) Dynamic expert routing to adaptively select informative modalities for each spot while suppressing noisy inputs.

Result: GROVER outperforms state-of-the-art baselines on real-world spatial omics datasets, demonstrating robust and reliable multimodal integration.

Conclusion: GROVER provides a robust framework for adaptive, graph-guided integration of spatial multi-omics with histology, addressing heterogeneity, alignment, and noise issues to enable comprehensive tissue analysis.

Abstract: Effectively modeling multimodal spatial omics data is critical for understanding tissue complexity and underlying biological mechanisms. While spatial transcriptomics, proteomics, and epigenomics capture molecular features, they lack pathological morphological context. Integrating these omics with histopathological images is therefore essential for comprehensive disease tissue analysis. However, substantial heterogeneity across omics, imaging, and spatial modalities poses significant challenges. Naive fusion of semantically distinct sources often leads to ambiguous representations. Additionally, the resolution mismatch between high-resolution histology images and lower-resolution sequencing spots complicates spatial alignment. Biological perturbations during sample preparation further distort modality-specific signals, hindering accurate integration. To address these challenges, we propose Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion (GROVER), a novel framework for adaptive integration of spatial multi-omics data. GROVER leverages a Graph Convolutional Network encoder based on Kolmogorov-Arnold Networks to capture the nonlinear dependencies between each modality and its associated spatial structure, thereby producing expressive, modality-specific embeddings. To align these representations, we introduce a spot-feature-pair contrastive learning strategy that explicitly optimizes the correspondence across modalities at each spot. Furthermore, we design a dynamic expert routing mechanism that adaptively selects informative modalities for each spot while suppressing noisy or low-quality inputs. Experiments on real-world spatial omics datasets demonstrate that GROVER outperforms state-of-the-art baselines, providing a robust and reliable solution for multimodal integration.

</details>


### [13] [Exposing DeepFakes via Hyperspectral Domain Mapping](https://arxiv.org/abs/2511.11732)
*Aditya Mehta,Swarnim Chaudhary,Pratik Narang,Jagat Sesh Challa*

Main category: cs.CV

TL;DR: HSI-Detect reconstructs a 31-channel hyperspectral image from RGB and detects Deepfakes in the hyperspectral domain, outperforming RGB baselines on FaceForensics++.


<details>
  <summary>Details</summary>
Motivation: RGB-only detectors may miss manipulation cues spread across the spectrum; expanding to hyperspectral representations can reveal artifacts invisible in RGB.

Method: A two-stage pipeline: (1) reconstruct a 31-channel hyperspectral image from standard RGB input, (2) perform detection in the hyperspectral domain, leveraging denser spectral information to amplify manipulation artifacts across specific frequency bands.

Result: HSI-Detect yields consistent improvements over RGB-only baselines on FaceForensics++.

Conclusion: Spectral-domain mapping is a promising direction for Deepfake detection, with hyperspectral features enhancing detection robustness and accuracy.

Abstract: Modern generative and diffusion models produce highly realistic images that can mislead human perception and even sophisticated automated detection systems. Most detection methods operate in RGB space and thus analyze only three spectral channels. We propose HSI-Detect, a two-stage pipeline that reconstructs a 31-channel hyperspectral image from a standard RGB input and performs detection in the hyperspectral domain. Expanding the input representation into denser spectral bands amplifies manipulation artifacts that are often weak or invisible in the RGB domain, particularly in specific frequency bands. We evaluate HSI-Detect across FaceForensics++ dataset and show the consistent improvements over RGB-only baselines, illustrating the promise of spectral-domain mapping for Deepfake detection.

</details>


### [14] [Toward bilipshiz geometric models](https://arxiv.org/abs/2511.11735)
*Yonatan Sverdlov,Eitan Rosen,Nadav Dym*

Main category: cs.CV

TL;DR: The paper investigates whether invariant neural networks for point clouds preserve symmetry-aware distances in a bi-Lipschitz sense, showing that Procrustes Matching and Hard Gromov–Wasserstein distances are not bi-Lipschitz equivalent. Consequently, standard invariant models are not bi-Lipschitz with respect to the Procrustes metric. The authors propose modifications to achieve bi-Lipschitz guarantees and present initial experiments indicating improvements for 3D point-cloud correspondence tasks.


<details>
  <summary>Details</summary>
Motivation: To connect symmetry-aware representations with the stability guarantees provided by bi-Lipschitz mappings, motivated by Benefits highlighted in Equivariant learning literature and the desire for geometry-respecting, robust distance measures in point-cloud analysis.

Method: The authors (i) analyze two symmetry-aware metrics on point clouds (Procrustes Matching and Hard GW distance) and prove they are not bi-Lipschitz equivalent; (ii) deduce that common invariant networks are not bi-Lipschitz with respect to the PM metric; (iii) propose network modifications that restore bi-Lipschitz guarantees; (iv) provide initial experiments comparing the bi-Lipschitz model to standard invariant models on 3D point-cloud correspondence tasks.

Result: The PM and Hard GW distances are not bi-Lipschitz equivalent; invariant networks fail to be bi-Lipschitz with respect to the PM metric; the authors propose a modified bi-Lipschitz architecture; initial experiments show the bi-Lipschitz model can outperform standard invariant models in finding correspondences between 3D point clouds.

Conclusion: Bi-Lipschitz design is viable for symmetry-aware point-cloud analysis and can offer practical gains in tasks requiring stable correspondences. The work highlights a mismatch between common invariant networks and geometry-respecting distance notions, and provides a concrete path to reconcile them, albeit with preliminary experiments that invite broader validation.

Abstract: Many neural networks for point clouds are, by design, invariant to the symmetries of this datatype: permutations and rigid motions. The purpose of this paper is to examine whether such networks preserve natural symmetry aware distances on the point cloud spaces, through the notion of bi-Lipschitz equivalence. This inquiry is motivated by recent work in the Equivariant learning literature which highlights the advantages of bi-Lipschitz models in other scenarios.
  We consider two symmetry aware metrics on point clouds: (a) The Procrustes Matching (PM) metric and (b) Hard Gromov Wasserstien distances. We show that these two distances themselves are not bi-Lipschitz equivalent, and as a corollary deduce that popular invariant networks for point clouds are not bi-Lipschitz with respect to the PM metric. We then show how these networks can be modified so that they do obtain bi-Lipschitz guarantees. Finally, we provide initial experiments showing the advantage of the proposed bi-Lipschitz model over standard invariant models, for the tasks of finding correspondences between 3D point clouds.

</details>


### [15] [Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models](https://arxiv.org/abs/2511.11751)
*Sanchit Sinha,Guangzhi Xiong,Zhenghao He,Aidong Zhang*

Main category: cs.CV

TL;DR: A multi-agent neurosymbolic framework (Concept-RuleNet) grounds symbols in visual data by mining concepts from images and composing executable rules via an LLM, with a vision verifier coordinating rule execution alongside black-box models; achieves interpretable, less-hallucinated predictions and ~5% average improvement on five benchmarks, including medical imaging and underrepresented datasets.


<details>
  <summary>Details</summary>
Motivation: To address the lack of interpretability and the tendency of vision-language models to hallucinate, especially on out-of-distribution data, by grounding symbolic reasoning in real visual data rather than relying solely on task labels.

Method: A three-to-four stage approach: (1) A multimodal concept generator mines discriminative visual concepts from a representative subset of training images. (2) These concepts condition symbol discovery, anchoring to real image statistics and reducing label bias. (3) A large language model reasoner assembles the symbols into first-order executable rules. (4) A vision verifier assesses symbol presence during inference and triggers rule execution in tandem with outputs from black-box neural models, yielding explicit reasoning pathways.

Result: Empirical evaluation on five benchmarks (including two challenging medical-imaging tasks and three underrepresented natural-image datasets) shows the method improves state-of-the-art neurosymbolic baselines by an average of ~5% and reduces hallucinated symbols in rules by up to ~50%.

Conclusion: Grounding neurosymbolic reasoning in visual data via a multimodal, multi-agent system yields more interpretable rules, reduces hallucinations, and boosts robustness across diverse domains.

Abstract: Modern vision-language models (VLMs) deliver impressive predictive accuracy yet offer little insight into 'why' a decision is reached, frequently hallucinating facts, particularly when encountering out-of-distribution data. Neurosymbolic frameworks address this by pairing black-box perception with interpretable symbolic reasoning, but current methods extract their symbols solely from task labels, leaving them weakly grounded in the underlying visual data. In this paper, we introduce a multi-agent system - Concept-RuleNet that reinstates visual grounding while retaining transparent reasoning. Specifically, a multimodal concept generator first mines discriminative visual concepts directly from a representative subset of training images. Next, these visual concepts are utilized to condition symbol discovery, anchoring the generations in real image statistics and mitigating label bias. Subsequently, symbols are composed into executable first-order rules by a large language model reasoner agent - yielding interpretable neurosymbolic rules. Finally, during inference, a vision verifier agent quantifies the degree of presence of each symbol and triggers rule execution in tandem with outputs of black-box neural models, predictions with explicit reasoning pathways. Experiments on five benchmarks, including two challenging medical-imaging tasks and three underrepresented natural-image datasets, show that our system augments state-of-the-art neurosymbolic baselines by an average of 5% while also reducing the occurrence of hallucinated symbols in rules by up to 50%.

</details>


### [16] [Batch Transformer Architecture: Case of Synthetic Image Generation for Emotion Expression Facial Recognition](https://arxiv.org/abs/2511.11754)
*Stanislav Selitskiy*

Main category: cs.CV

TL;DR: A Batch Transformer with implicit sparse attention focuses on important dimensions to reduce bottleneck size in encoder–decoder networks; validated on synthetic face-generation with makeup/occlusion to increase data variability.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity and computational bottlenecks in Transformer-based encoder–decoder architectures by using feature selection (primary components) to reduce the bottleneck and potentially improve generalization on constrained data.

Method: Introduce Batch Transformers that attend only to selected important dimensions (primary components) via implicit sparse attention, reducing effective dimensionality; evaluate on a synthetic image-generation task for face recognition under makeup and occlusion to boost data variability.

Result: Demonstrates reduced bottleneck size and feasible enhancement of data variability in a constrained data setting; feasible validation on synthetic face-generation data.

Conclusion: Batch Transformers with implicit sparse attention provide an efficient encoder–decoder alternative and are particularly promising for limited-data face recognition tasks, with potential applicability to other domains and a need for quantitative benchmarking.

Abstract: A novel Transformer variation architecture is proposed in the implicit sparse style. Unlike "traditional" Transformers, instead of attention to sequential or batch entities in their entirety of whole dimensionality, in the proposed Batch Transformers, attention to the "important" dimensions (primary components) is implemented. In such a way, the "important" dimensions or feature selection allows for a significant reduction of the bottleneck size in the encoder-decoder ANN architectures. The proposed architecture is tested on the synthetic image generation for the face recognition task in the case of the makeup and occlusion data set, allowing for increased variability of the limited original data set.

</details>


### [17] [Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing](https://arxiv.org/abs/2511.11780)
*Hossein Mohebbi,Mohammed Abdulrahman,Yanting Miao,Pascal Poupart,Suraj Kothawade*

Main category: cs.CV

TL;DR: Introduces Image-POSER, a reflective RL framework that orchestrates diverse pretrained text-to-image and image-to-image experts to handle long-form prompts via dynamic task decomposition and a vision-language critic, achieving superior alignment, fidelity, and aesthetics over baselines.


<details>
  <summary>Details</summary>
Motivation: Current single-shot generative models struggle with long, compositional prompts common in creative workflows. A scalable, end-to-end system should autonomously decompose tasks, select and combine multiple experts, and receive structured feedback to improve alignment and output quality.

Method: Modeling image synthesis/editing as a Markov Decision Process. Maintains a registry of diverse pretrained experts (text-to-image and image-to-image). Uses dynamic task decomposition to handle long prompts. Learns expert pipelines via reinforcement learning, integrating structured feedback from a vision-language model critic at each step.

Result: Image-POSER outperforms baselines, including frontier models, on industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations.

Conclusion: Reinforcement learning enables autonomous decomposition, sequencing, and combination of visual models, moving toward general-purpose visual assistants.

Abstract: Recent advances in text-to-image generation have produced strong single-shot models, yet no individual system reliably executes the long, compositional prompts typical of creative workflows. We introduce Image-POSER, a reflective reinforcement learning framework that (i) orchestrates a diverse registry of pretrained text-to-image and image-to-image experts, (ii) handles long-form prompts end-to-end through dynamic task decomposition, and (iii) supervises alignment at each step via structured feedback from a vision-language model critic. By casting image synthesis and editing as a Markov Decision Process, we learn non-trivial expert pipelines that adaptively combine strengths across models. Experiments show that Image-POSER outperforms baselines, including frontier models, across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations. These results highlight that reinforcement learning can endow AI systems with the capacity to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.

</details>


### [18] [SOTFormer: A Minimal Transformer for Unified Object Tracking and Trajectory Prediction](https://arxiv.org/abs/2511.11824)
*Zhongping Dong,Pengyang Yu,Shuangjian Li,Liming Chen,Mohand Tahar Kechadi*

Main category: cs.CV

TL;DR: SOTFormer is a minimal constant-memory temporal transformer that unifies object detection, tracking, and short-horizon trajectory prediction in a single end-to-end framework. It uses ground-truth-primed memory and a burn-in anchor loss to stabilize initialization, enabling real-time inference with fixed memory and better robustness under occlusion, scale variation, and temporal drift. On Mini-LaSOT (20%), it achieves 76.3 AUC and 53.7 FPS, outperforming transformer baselines like TrackFormer and MOTRv2 under fast motion and occlusion.


<details>
  <summary>Details</summary>
Motivation: Accurate single-object tracking and short-term motion forecasting are challenged by occlusion, scale variation, and temporal drift, which disrupt temporal coherence and hinder real-time perception. Existing models with recurrent or stacked temporal encoders may suffer from instability in identity propagation and high memory costs.

Method: Proposes SOTFormer, a minimal constant-memory temporal transformer that unifies detection, tracking, and short-horizon trajectory prediction. Key design choices include a ground-truth-primed memory to stabilize identity propagation, a burn-in anchor loss to stabilize initialization, and a single lightweight temporal-attention layer that refines embeddings across frames. The model operates with fixed GPU memory for real-time inference.

Result: On Mini-LaSOT (20%) benchmark, SOTFormer achieves 76.3 AUC and 53.7 FPS (AMP, 4.3 GB VRAM). It outperforms transformer baselines such as TrackFormer and MOTRv2, especially under fast motion, scale change, and occlusion.

Conclusion: A minimal, constant-memory temporal transformer can unify detection, tracking, and short-horizon trajectory prediction with stable identity propagation and real-time performance. Ground-truth-primed memory and burn-in anchor loss enable robust initialization, making SOTFormer competitive or superior to prior transformer baselines on challenging tracking scenarios.

Abstract: Accurate single-object tracking and short-term motion forecasting remain challenging under occlusion, scale variation, and temporal drift, which disrupt the temporal coherence required for real-time perception. We introduce \textbf{SOTFormer}, a minimal constant-memory temporal transformer that unifies object detection, tracking, and short-horizon trajectory prediction within a single end-to-end framework. Unlike prior models with recurrent or stacked temporal encoders, SOTFormer achieves stable identity propagation through a ground-truth-primed memory and a burn-in anchor loss that explicitly stabilizes initialization. A single lightweight temporal-attention layer refines embeddings across frames, enabling real-time inference with fixed GPU memory. On the Mini-LaSOT (20%) benchmark, SOTFormer attains 76.3 AUC and 53.7 FPS (AMP, 4.3 GB VRAM), outperforming transformer baselines such as TrackFormer and MOTRv2 under fast motion, scale change, and occlusion.

</details>


### [19] [MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning](https://arxiv.org/abs/2511.11837)
*Fatemeh Elhambakhsh,Gaurav Ameta,Aditi Roy,Hyunwoong Ko*

Main category: cs.CV

TL;DR: MP-GFormer is a 3D-geometry-aware dynamic graph transformer for machining process planning that integrates evolving 3D geometry from part after each operation into dynamic graphs to predict machining operation sequences, achieving notable accuracy gains on synthetic data.


<details>
  <summary>Details</summary>
Motivation: Existing dynamic graph learning methods in machining process planning ignore 3D geometry, limiting domain-aware prediction of operation sequences; a 3D-aware model could better capture structural and geometric dependencies as operations proceed.

Method: Proposes MP-GFormer that fuses evolving 3D geometry (StereoLithography surface meshes) with DGL via attention; uses boundary representation for initial design; models part geometry after each operation; evaluates on synthetic dataset; reports accuracy improvements.

Result: Achieves 24% and 36% improvements in accuracy for main and sub-operation predictions over state-of-the-art methods on a synthetic dataset.

Conclusion: Incorporating 3D geometry into DGL yields improved machining sequence predictions, demonstrating efficacy of MP-GFormer in MP and paving way for geometry-aware planning.

Abstract: Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\% and 36\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.

</details>


### [20] [Defending Unauthorized Model Merging via Dual-Stage Weight Protection](https://arxiv.org/abs/2511.11851)
*Wei-Jia Chen,Min-Yen Tsai,Cheng-Yi Lee,Chia-Mu Yu*

Main category: cs.CV

TL;DR: A defense called MergeGuard disrupts unauthorized model merging via a two-stage weight-protection strategy, preserving the protected model while causing merged models to fail.


<details>
  <summary>Details</summary>
Motivation: To counter unrestricted and unauthorized merging of pretrained/finetuned models, which threatens intellectual property, ownership, and accountability.

Method: Stage 1 redistributes task-relevant information across layers using L2-regularized optimization to evenly disperse gradients. Stage 2 injects structured perturbations to misalign task subspaces, breaking curvature compatibility in the loss landscape. Together, they reshape parameter geometry so merged models experience destructive interference, while the protected model remains functional.

Result: Experiments on vision (ViT-L-14) and language (Llama2, Gemma2, Mistral) show merged-model accuracy drops by up to 90%, with the protected model incurring <1.5% performance loss.

Conclusion: MergeGuard provides a proactive, geometry-based defense against unauthorized model merging, balancing strong protection with minimal impact on the protected model.

Abstract: The rapid proliferation of pretrained models and open repositories has made model merging a convenient yet risky practice, allowing free-riders to combine fine-tuned models into a new multi-capability model without authorization. Such unauthorized model merging not only violates intellectual property rights but also undermines model ownership and accountability. To address this issue, we present MergeGuard, a proactive dual-stage weight protection framework that disrupts merging compatibility while maintaining task fidelity. In the first stage, we redistribute task-relevant information across layers via L2-regularized optimization, ensuring that important gradients are evenly dispersed. In the second stage, we inject structured perturbations to misalign task subspaces, breaking curvature compatibility in the loss landscape. Together, these stages reshape the model's parameter geometry such that merged models collapse into destructive interference while the protected model remains fully functional. Extensive experiments on both vision (ViT-L-14) and language (Llama2, Gemma2, Mistral) models demonstrate that MergeGuard reduces merged model accuracy by up to 90% with less than 1.5% performance loss on the protected model.

</details>


### [21] [FocusSDF: Boundary-Aware Learning for Medical Image Segmentation via Signed Distance Supervision](https://arxiv.org/abs/2511.11864)
*Muzammal Shafique,Nasir Rahim,Jamil Ahmad,Mohammad Siadat,Khalid Malik,Ghaus Malik*

Main category: cs.CV

TL;DR: FocusSDF is a boundary-aware loss for medical image segmentation using signed distance functions to weight boundary pixels, outperforming other distance-transform losses across multiple models and datasets.


<details>
  <summary>Details</summary>
Motivation: Boundary preservation in medical image segmentation is challenging because many models do not explicitly encode boundary information, yet precise boundary delineation is crucial for diagnosis and treatment planning.

Method: Introduces FocusSDF, a loss based on signed distance functions (SDFs) that adaptively weights pixels by their distance to the boundary, guiding the network to focus on boundaries. It is evaluated against four other distance-based losses, across five state-of-the-art segmentation models including MedSAM, on datasets for cerebral aneurysm, stroke, liver, and breast tumor segmentation across multiple imaging modalities.

Result: Experiments show that FocusSDF consistently outperforms existing distance-transform-based loss functions across the tested models and diverse datasets.

Conclusion: FocusSDF effectively makes segmentation boundary-aware, improving boundary preservation and generalizing across architectures and imaging modalities; it represents a robust boundary-focused loss for medical image segmentation.

Abstract: Segmentation of medical images constitutes an essential component of medical image analysis, providing the foundation for precise diagnosis and efficient therapeutic interventions in clinical practices. Despite substantial progress, most segmentation models do not explicitly encode boundary information; as a result, making boundary preservation a persistent challenge in medical image segmentation. To address this challenge, we introduce FocusSDF, a novel loss function based on the signed distance functions (SDFs), which redirects the network to concentrate on boundary regions by adaptively assigning higher weights to pixels closer to the lesion or organ boundary, effectively making it boundary aware. To rigorously validate FocusSDF, we perform extensive evaluations against five state-of-the-art medical image segmentation models, including the foundation model MedSAM, using four distance-based loss functions across diverse datasets covering cerebral aneurysm, stroke, liver, and breast tumor segmentation tasks spanning multiple imaging modalities. The experimental results consistently demonstrate the superior performance of FocusSDF over existing distance transform based loss functions.

</details>


### [22] [Lacking Data? No worries! How synthetic images can alleviate image scarcity in wildlife surveys: a case study with muskox (Ovibos moschatus)](https://arxiv.org/abs/2511.11882)
*Simon Durand,Samuel Foucher,Alexandre Delplanque,Joëlle Taillon,Jérôme Théau*

Main category: cs.CV

TL;DR: Synthetic imagery can bolster zero-shot and few-shot muskox detection but gains plateau with high SI; FS benefits are not statistically significant; SI enables training ODMs with little to no real data and improves monitoring of rare species.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity for muskox population monitoring and limitations of traditional survey methods; explore synthetic data to enable robust deep learning detectors in Arctic wildlife monitoring.

Method: Comparative evaluation: baseline model trained on real imagery; five zero-shot and five few-shot models with progressively more synthetic imagery; ZS uses no real training images; FS uses real + synthetic; evaluate precision, recall, F1, etc.

Result: In ZS, adding synthetic imagery improved detection and metrics with increasing SI until plateau around 100% of baseline; in FS, combining real+synthetic improved recall and overall accuracy vs real data alone, but not statistically significant.

Conclusion: Synthetic imagery is a viable augmentation for training object detection models under data scarcity, enabling early ODM development for rare or inaccessible species and incremental improvement as real data becomes available; recommend using SI to bootstrap ODMs and monitor wildlife more frequently.

Abstract: Accurate population estimates are essential for wildlife management, providing critical insights into species abundance and distribution. Traditional survey methods, including visual aerial counts and GNSS telemetry tracking, are widely used to monitor muskox populations in Arctic regions. These approaches are resource intensive and constrained by logistical challenges. Advances in remote sensing, artificial intelligence, and high resolution aerial imagery offer promising alternatives for wildlife detection. Yet, the effectiveness of deep learning object detection models (ODMs) is often limited by small datasets, making it challenging to train robust ODMs for sparsely distributed species like muskoxen. This study investigates the integration of synthetic imagery (SI) to supplement limited training data and improve muskox detection in zero shot (ZS) and few-shot (FS) settings. We compared a baseline model trained on real imagery with 5 ZS and 5 FS models that incorporated progressively more SI in the training set. For the ZS models, where no real images were included in the training set, adding SI improved detection performance. As more SI were added, performance in precision, recall and F1 score increased, but eventually plateaued, suggesting diminishing returns when SI exceeded 100% of the baseline model training dataset. For FS models, combining real and SI led to better recall and slightly higher overall accuracy compared to using real images alone, though these improvements were not statistically significant. Our findings demonstrate the potential of SI to train accurate ODMs when data is scarce, offering important perspectives for wildlife monitoring by enabling rare or inaccessible species to be monitored and to increase monitoring frequency. This approach could be used to initiate ODMs without real data and refine it as real images are acquired over time.

</details>


### [23] [Advancing Annotat3D with Harpia: A CUDA-Accelerated Library For Large-Scale Volumetric Data Segmentation](https://arxiv.org/abs/2511.11890)
*Camila Machado de Araujo,Egon P. B. S. Borges,Ricardo Marcelo Canteiro Grangeiro,Allan Pinto*

Main category: cs.CV

TL;DR: Harpia, a CUDA-based processing library integrated with Annotat3D, enables scalable, interactive segmentation of large 3D datasets in HPC/remote environments by enforcing strict memory control and chunked GPU execution.


<details>
  <summary>Details</summary>
Motivation: As high-resolution volumetric imaging produces datasets that exceed single-GPU memory and challenge existing tools, there is a need for scalable, memory-efficient, GPU-accelerated workflows with interactive capabilities in HPC and remote-access settings.

Method: Harpia employs CUDA-based GPU acceleration with native chunked execution, strict memory management, and a suite of GPU-accelerated filtering, annotation, and quantification tools, designed to operate on datasets larger than single-GPU memory and to support interactive, human-in-the-loop workflows within Annotat3D.

Result: Experimental evaluation reports significant improvements in processing speed, memory efficiency, and scalability relative to widely used frameworks such as NVIDIA cuCIM and scikit-image.

Conclusion: Harpia extends Annotat3D to reliably handle very large 3D datasets in shared HPC infrastructures, enabling interactive, collaborative scientific imaging workflows with efficient GPU resource management.

Abstract: High-resolution volumetric imaging techniques, such as X-ray tomography and advanced microscopy, generate increasingly large datasets that challenge existing tools for efficient processing, segmentation, and interactive exploration. This work introduces new capabilities to Annotat3D through Harpia, a new CUDA-based processing library designed to support scalable, interactive segmentation workflows for large 3D datasets in high-performance computing (HPC) and remote-access environments. Harpia features strict memory control, native chunked execution, and a suite of GPU-accelerated filtering, annotation, and quantification tools, enabling reliable operation on datasets exceeding single-GPU memory capacity. Experimental results demonstrate significant improvements in processing speed, memory efficiency, and scalability compared to widely used frameworks such as NVIDIA cuCIM and scikit-image. The system's interactive, human-in-the-loop interface, combined with efficient GPU resource management, makes it particularly suitable for collaborative scientific imaging workflows in shared HPC infrastructures.

</details>


### [24] [Prompt Triage: Structured Optimization Enhances Vision-Language Model Performance on Medical Imaging Benchmarks](https://arxiv.org/abs/2511.11898)
*Arnav Singhvi,Vasiliki Bikia,Asad Aali,Akshay Chaudhari,Roxana Daneshjou*

Main category: cs.CV

TL;DR: Automated prompt optimization via the DSPy framework substantially boosts medical vision-language models without task-specific fine-tuning, achieving a median 53% relative improvement across five tasks and up to thousands of percent on low-zero-shot tasks; open-source pipelines released to enable reproducible, privacy-preserving research.


<details>
  <summary>Details</summary>
Motivation: Medical VLMs often underperform; manual prompt engineering is brittle and data-hungry; a weight-agnostic, automated prompt optimization approach can leverage model knowledge to improve performance at scale while protecting data privacy.

Method: Adapt DSPy for structured automated prompt optimization in medical VLMs. Build prompting pipelines for five medical imaging tasks (radiology, gastroenterology, dermatology), evaluate 10 open-source VLMs with four prompt-optimization techniques.

Result: Pipelines achieved median relative improvement of 53% over zero-shot baselines; largest gains 300%–3400% on tasks with low zero-shot performance; demonstrates scalability and privacy benefits; improvements observed on open-source models; evaluation pipelines publicly released.

Conclusion: Automated prompt optimization holds strong potential to enhance medical AI imaging tools by reducing reliance on hand-crafted prompts, enabling clinicians to focus on care; the approach supports reproducible research and broader deployment via open pipelines.

Abstract: Vision-language foundation models (VLMs) show promise for diverse imaging tasks but often underperform on medical benchmarks. Prior efforts to improve performance include model finetuning, which requires large domain-specific datasets and significant compute, or manual prompt engineering, which is hard to generalize and often inaccessible to medical institutions seeking to deploy these tools. These challenges motivate interest in approaches that draw on a model's embedded knowledge while abstracting away dependence on human-designed prompts to enable scalable, weight-agnostic performance improvements. To explore this, we adapt the Declarative Self-improving Python (DSPy) framework for structured automated prompt optimization in medical vision-language systems through a comprehensive, formal evaluation. We implement prompting pipelines for five medical imaging tasks across radiology, gastroenterology, and dermatology, evaluating 10 open-source VLMs with four prompt optimization techniques. Optimized pipelines achieved a median relative improvement of 53% over zero-shot prompting baselines, with the largest gains ranging from 300% to 3,400% on tasks where zero-shot performance is low. These results highlight the substantial potential of applying automated prompt optimization to medical AI systems, demonstrating significant gains for vision-based applications requiring accurate clinical image interpretation. By reducing dependence on prompt design to elicit intended outputs, these techniques allow clinicians to focus on patient care and clinical decision-making. Furthermore, our experiments offer scalability and preserve data privacy, demonstrating performance improvement on open-source VLMs. We publicly release our evaluation pipelines to support reproducible research on specialized medical tasks, available at https://github.com/DaneshjouLab/prompt-triage-lab.

</details>


### [25] [PI-NAIM: Path-Integrated Neural Adaptive Imputation Model](https://arxiv.org/abs/2511.11908)
*Afifa Khaled,Ebrahim Hamid Sumiea*

Main category: cs.CV

TL;DR: PI-NAIM is a dual-path, adaptive imputation framework that routes data by missingness complexity to either efficient MICE or neural GAIN-based imputation, fuses via cross-path attention, and jointly optimizes imputation with downstream tasks, achieving state-of-the-art RMSE and AUROC on MIMIC-III and multimodal benchmarks; code available.


<details>
  <summary>Details</summary>
Motivation: Missing modalities in medical imaging and multi-modal clinical settings are common, and existing imputation methods either underperform in representation or are computationally expensive. A flexible, scalable approach that adapts to missingness complexity is needed.

Method: A dual-path architecture with intelligent path routing: low-missingness samples go to efficient statistical imputation (MICE); high-missingness/complex patterns go to neural imputation (GAIN with temporal analysis). Cross-path attention fusion combines missingness-aware embeddings from both branches. End-to-end joint optimization of imputation accuracy and downstream task performance.

Result: Achieves state-of-the-art imputation and predictive performance: RMSE 0.108 (baselines 0.119–0.152) and AUROC 0.812 for mortality prediction on MIMIC-III and multimodal benchmarks. Demonstrates robustness for incomplete sensor measurements, missing modalities, or corrupted inputs; code publicly available.

Conclusion: PI-NAIM’s modular design provides a unified solution for real-world incomplete data workflows and can be integrated into broader vision/medical pipelines dealing with missing data.

Abstract: Medical imaging and multi-modal clinical settings often face the challange of missing modality in their diagnostic pipelines. Existing imputation methods either lack representational capacity or are computationally expensive. We propose PI-NAIM, a novel dual-path architecture that dynamically routes samples to optimized imputation approaches based on missingness complexity. Our framework integrates: (1) intelligent path routing that directs low missingness samples to efficient statistical imputation (MICE) and complex patterns to powerful neural networks (GAIN with temporal analysis); (2) cross-path attention fusion that leverages missingness-aware embeddings to intelligently combine both branches; and (3) end-to-end joint optimization of imputation accuracy and downstream task performance. Extensive experiments on MIMIC-III and multimodal benchmarks demonstrate state-of-the-art performance, achieving RMSE of 0.108 (vs. baselines' 0.119-0.152) and substantial gains in downstream tasks with an AUROC of 0.812 for mortality prediction. PI-NAIM's modular design enables seamless integration into vision pipelines handling incomplete sensor measurements, missing modalities, or corrupted inputs, providing a unified solution for real-world scenario. The code is publicly available at https://github.com/AfifaKhaled/PI-NAIM-Path-Integrated-Neural-Adaptive-Imputation-Model

</details>


### [26] [Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models](https://arxiv.org/abs/2511.11910)
*Siyou Li,Huanan Wu,Juexi Shao,Yinghao Ma,Yujian Gan,Yihao Luo,Yuwei Wang,Dong Nie,Lu Wang,Wengqing Wu,Le Zhang,Massimo Poesio,Juntao Yu*

Main category: cs.CV

TL;DR: QTSplus is a lightweight visual token selector that compresses video inputs for long-video understanding by scoring tokens via cross-attention, predicting a per-query retention budget, and selecting top tokens with differentiable training and hard gating at inference. It reduces vision tokens by up to 89% and end-to-end latency by 28% on long videos, with near-parity accuracy and notable gains on direction/order tasks.


<details>
  <summary>Details</summary>
Motivation: Long video understanding is hindered by the linear growth of vision tokens, which blows up attention cost, memory, and latency in multimodal LLMs. A selective mechanism is needed to retain task-relevant evidence while reducing computation.

Method: Introduce QTSplus, a query-aware token selector that 1) scores visual tokens via cross-attention to a text query, 2) predicts an instance-specific retention budget based on query complexity, and 3) selects the top tokens using a differentiable straight-through estimator during training and a hard gate at inference. A small re-encoder preserves temporal order with absolute time information. Integrated into Qwen2.5-VL to compress the vision stream.

Result: Audited on eight long-video benchmarks: compression up to 89% of vision tokens and 28% reduction in end-to-end latency; near-parity accuracy with original Qwen models; substantial gains on TempCompass direction (+20.5 points) and order (+5.6 points). Code/data and weights to be released.

Conclusion: QTSplus offers an effective, general approach to scaling MLLMs for real-world long-video understanding by preserving task-relevant evidence while significantly reducing compute and latency.

Abstract: Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.
  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \textbf{89\%} and reduces end-to-end latency by \textbf{28\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \textbf{+20.5} and \textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.
  We will make all code, data, and trained models' weights publicly available.

</details>


### [27] [From Events to Clarity: The Event-Guided Diffusion Framework for Dehazing](https://arxiv.org/abs/2511.11944)
*Ling Wang,Yunfan Lu,Wenzong Ma,Huizai Yao,Pengteng Li,Hui Xiong*

Main category: cs.CV

TL;DR: First-time dehazing with event cameras using an event-guided diffusion model that injects HDR event cues into the diffusion latent space, achieving state-of-the-art results on two benchmarks and a real drone hazy dataset.


<details>
  <summary>Details</summary>
Motivation: RGB-based dehazing struggles with limited dynamic range and is ill-posed. Event cameras offer higher dynamic range and microsecond latency, enabling HDR cues (edges, corners) to guide dehazing. The goal is to transfer HDR information from events to frames to preserve structure and illumination.

Method: Introduce an event-guided diffusion model. A dedicated module maps sparse HDR event features (edges, corners) into the diffusion latent space to provide strong structural conditioning during generation, reducing semantic drift and improving realism. Use synchronized RGB-event data, including a real-world drone dataset collected in heavy haze (AQI 341), and evaluate on two benchmarks plus the new dataset.

Result: Achieves state-of-the-art performance on two benchmarks and the proposed drone hazy dataset, with improved structural fidelity, visual realism, and reduced semantic drift.

Conclusion: Event cameras are effective for dehazing; cross-modal HDR cue transfer from events to diffusion-based dehazing yields superior results in hazy scenes, demonstrating real-world applicability.

Abstract: Clear imaging under hazy conditions is a critical task. Prior-based and neural methods have improved results. However, they operate on RGB frames, which suffer from limited dynamic range. Therefore, dehazing remains ill-posed and can erase structure and illumination details. To address this, we use event cameras for dehazing for the \textbf{first time}. Event cameras offer much higher HDR ($120 dBvs.60 dB$) and microsecond latency, therefore they suit hazy scenes. In practice, transferring HDR cues from events to frames is hard because real paired data are scarce. To tackle this, we propose an event-guided diffusion model that utilizes the strong generative priors of diffusion models to reconstruct clear images from hazy inputs by effectively transferring HDR information from events. Specifically, we design an event-guided module that maps sparse HDR event features, \textit{e.g.,} edges, corners, into the diffusion latent space. This clear conditioning provides precise structural guidance during generation, improves visual realism, and reduces semantic drift. For real-world evaluation, we collect a drone dataset in heavy haze (AQI = 341) with synchronized RGB and event sensors. Experiments on two benchmarks and our dataset achieve state-of-the-art results.

</details>


### [28] [Evaluation of Attention Mechanisms in U-Net Architectures for Semantic Segmentation of Brazilian Rock Art Petroglyphs](https://arxiv.org/abs/2511.11959)
*Leonardi Melo,Luís Gustavo,Dimmy Magalhães,Lucciani Vieira,Mauro Araújo*

Main category: cs.CV

TL;DR: Compared three BEGL-UNet variants for rock-art petroglyph segmentation; Attention-Residual BEGL-UNet performs best (Dice 0.710, recall 0.854); Spatial Channel Attention close (Dice 0.707, recall 0.857); baseline 0.690; dataset Poço da Bebidinha, 5-fold CV; BEGL loss used in all models; ~2.5–2.9% Dice gain over baseline; supports effectiveness of attention for archaeological preservation.


<details>
  <summary>Details</summary>
Motivation: Improve semantic segmentation of archaeological rock-art petroglyphs using U-Net variants with attention mechanisms and Border-Enhanced Gaussian Loss, and evaluate their performance on Brazilian archaeological site imagery under cross-validation.

Method: Three BEGL-UNet variants were implemented: (i) BEGL-UNet baseline, (ii) Attention-Residual BEGL-UNet with residual blocks and gated attention, (iii) Spatial Channel Attention BEGL-UNet with spatial-channel attention (CBAM-like). All used BEGL loss (binary cross-entropy plus Gaussian edge enhancement). Dataset from Poço da Bebidinha Archaeological Complex, Piauí, Brazil; 5-fold cross-validation.

Result: Attention-Residual BEGL-UNET achieved Dice 0.710, validation loss 0.067, recall 0.854 (best overall); Spatial Channel Attention BEGL-UNet Dice 0.707, recall 0.857; baseline BEGL-UNet Dice 0.690. Dice improvements of 2.5–2.9% over baseline across architectures.

Conclusion: Attention mechanisms effectively enhance archaeological heritage digital preservation by improving petroglyph segmentation, with notable Dice and recall gains under cross-validation.

Abstract: This study presents a comparative analysis of three U-Net-based architectures for semantic segmentation of rock art petroglyphs from Brazilian archaeological sites. The investigated architectures were: (1) BEGL-UNet with Border-Enhanced Gaussian Loss function; (2) Attention-Residual BEGL-UNet, incorporating residual blocks and gated attention mechanisms; and (3) Spatial Channel Attention BEGL-UNet, which employs spatial-channel attention modules based on Convolutional Block Attention Module. All implementations employed the BEGL loss function combining binary cross-entropy with Gaussian edge enhancement. Experiments were conducted on images from the Poço da Bebidinha Archaeological Complex, Piauí, Brazil, using 5-fold cross-validation. Among the architectures, Attention-Residual BEGL-UNet achieved the best overall performance with Dice Score of 0.710, validation loss of 0.067, and highest recall of 0.854. Spatial Channel Attention BEGL-UNet obtained comparable performance with DSC of 0.707 and recall of 0.857. The baseline BEGL-UNet registered DSC of 0.690. These results demonstrate the effectiveness of attention mechanisms for archaeological heritage digital preservation, with Dice Score improvements of 2.5-2.9% over the baseline.

</details>


### [29] [From Classification to Cross-Modal Understanding: Leveraging Vision-Language Models for Fine-Grained Renal Pathology](https://arxiv.org/abs/2511.11984)
*Zhenhao Guo,Rachit Saluja,Tianyuan Yao,Quan Liu,Junchao Zhu,Haibo Wang,Daniel Reisenbüchler,Yuankai Huo,Benjamin Liechty,David J. Pisapia,Kenji Ikemura,Steven Salvatoree,Surya Seshane,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: Pathology-specialized vision-language models with vanilla fine-tuning excel at few-shot fine-grained glomerular subtyping, achieving meaningful discrimination and calibration with as few as 4–8 labeled examples per subtype; model performance depends on supervision level and adaptation strategy, not just image-text alignment.


<details>
  <summary>Details</summary>
Motivation: Clinical labels for fine-grained glomerular subtypes are scarce; it is unclear how vision-language models should be adapted for subtyping under data constraints and how pathology-specific backbones compare to general models.

Method: Systematic few-shot evaluation across pathology-specialized and general-purpose vision-language models, analyzing classification metrics (accuracy, AUC, F1) and the geometry of image/text representations; varying shot count, architecture, and adaptation strategy; focusing on vanilla fine-tuning as a starting point and assessing the importance of image-text alignment versus discriminative capability.

Result: Pathology-specialized VLM backbones with vanilla fine-tuning provide the strongest starting point; with 4–8 labels per glomerular subtype, models begin to distinguish subtypes and show gains in discrimination and calibration; further supervision yields incremental improvements; the separation of positive/negative examples is as important as image-text alignment; supervised signal and adaptation strategy jointly shape diagnostic performance and the multimodal representation structure.

Conclusion: For real-clinical data constraints, prefer pathology-specialized VLM backbones with appropriate supervision and adaptation strategies; invest in annotation for a few-shot regime; balance discriminative learning and image-text alignment to optimize both performance and the structure of multimodal representations.

Abstract: Fine-grained glomerular subtyping is central to kidney biopsy interpretation, but clinically valuable labels are scarce and difficult to obtain. Existing computational pathology approaches instead tend to evaluate coarse diseased classification under full supervision with image-only models, so it remains unclear how vision-language models (VLMs) should be adapted for clinically meaningful subtyping under data constraints. In this work, we model fine-grained glomerular subtyping as a clinically realistic few-shot problem and systematically evaluate both pathology-specialized and general-purpose vision-language models under this setting. We assess not only classification performance (accuracy, AUC, F1) but also the geometry of the learned representations, examining feature alignment between image and text embeddings and the separability of glomerular subtypes. By jointly analyzing shot count, model architecture and domain knowledge, and adaptation strategy, this study provides guidance for future model selection and training under real clinical data constraints. Our results indicate that pathology-specialized vision-language backbones, when paired with the vanilla fine-tuning, are the most effective starting point. Even with only 4-8 labeled examples per glomeruli subtype, these models begin to capture distinctions and show substantial gains in discrimination and calibration, though additional supervision continues to yield incremental improvements. We also find that the discrimination between positive and negative examples is as important as image-text alignment. Overall, our results show that supervision level and adaptation strategy jointly shape both diagnostic performance and multimodal structure, providing guidance for model selection, adaptation strategies, and annotation investment.

</details>


### [30] [BeyondFacial: Identity-Preserving Personalized Generation Beyond Facial Close-ups](https://arxiv.org/abs/2511.11989)
*Songsong Zhang,Chuanqi Tang,Hongguang Zhang,Guijian Tang,Minglong Li,Xueqiong Li,Shaowu Yang,Yuanxi Peng,Wenjing Yang,Jing Zhao*

Main category: cs.CV

TL;DR: A plug-and-play IPPG framework that separates identity and semantics to reduce ID interference, enabling identity-preserving generation beyond facial close-ups via Dual-Line Inference (DLI), Identity Adaptive Fusion (IdAF), and Identity Aggregation Prepending (IdAP).


<details>
  <summary>Details</summary>
Motivation: Current IPPG methods overemphasize facial regions, yielding weak visual narrativity and semantic inconsistency under complex prompts. Identity feature embeddings constrain semantic expressiveness and hinder scene-level generation; there is a need to move beyond facial close-ups while preserving identity.

Method: Proposes Dual-Line Inference (DLI) with identity-semantic separation to resolve representation conflicts. Introduces Identity Adaptive Fusion (IdAF) that defers fusion to the noise-prediction stage with adaptive attention fusion and noise decision masking to prevent ID interference without manual masking. Adds Identity Aggregation Prepending (IdAP) to aggregate ID information and replace random initializations, improving identity preservation. The system is a plug-and-play component that can be integrated into existing IPPG pipelines without fine-tuning.

Result: Experiments show stable and effective IPPG performance beyond facial close-ups, enabling efficient generation without manual masking or fine-tuning. The method supports rapid deployment in existing IPPG frameworks and facilitates film-level character-scene creation with richer personalized generation capabilities.

Conclusion: The proposed DLI-IdAF-IdAP pipeline effectively balances identity fidelity with scene semantics, enabling broader IPPG applications beyond facial close-ups and offering a practical, plug-and-play improvement for existing frameworks.

Abstract: Identity-Preserving Personalized Generation (IPPG) has advanced film production and artistic creation, yet existing approaches overemphasize facial regions, resulting in outputs dominated by facial close-ups.These methods suffer from weak visual narrativity and poor semantic consistency under complex text prompts, with the core limitation rooted in identity (ID) feature embeddings undermining the semantic expressiveness of generative models. To address these issues, this paper presents an IPPG method that breaks the constraint of facial close-ups, achieving synergistic optimization of identity fidelity and scene semantic creation. Specifically, we design a Dual-Line Inference (DLI) pipeline with identity-semantic separation, resolving the representation conflict between ID and semantics inherent in traditional single-path architectures. Further, we propose an Identity Adaptive Fusion (IdAF) strategy that defers ID-semantic fusion to the noise prediction stage, integrating adaptive attention fusion and noise decision masking to avoid ID embedding interference on semantics without manual masking. Finally, an Identity Aggregation Prepending (IdAP) module is introduced to aggregate ID information and replace random initializations, further enhancing identity preservation. Experimental results validate that our method achieves stable and effective performance in IPPG tasks beyond facial close-ups, enabling efficient generation without manual masking or fine-tuning. As a plug-and-play component, it can be rapidly deployed in existing IPPG frameworks, addressing the over-reliance on facial close-ups, facilitating film-level character-scene creation, and providing richer personalized generation capabilities for related domains.

</details>


### [31] [Dynamic Parameter Optimization for Highly Transferable Transformation-Based Attacks](https://arxiv.org/abs/2511.11993)
*Jiaming Liang,Chi-Man Pun*

Main category: cs.CV

TL;DR: This work studies transformation-based adversarial attacks, uncovers rise-then-fall transferability patterns with respect to parameter strength, introduces the Concentric Decay Model (CDM) to explain them, and presents Dynamic Parameter Optimization (DPO). DPO reduces parameter-search complexity to O(n log m) and improves transferability across surrogates, iterations, and tasks.


<details>
  <summary>Details</summary>
Motivation: Current transformation-based attacks suffer from three issues: (1) reliance on low-iteration results which misrepresent performance at higher iterations, (2) uniform parameter settings across different surrogates/models and tasks that hinder transferability, and (3) grid-search-based parameter optimization with high computational cost (O(m^n)). There is a need for a dynamic, efficient, and transferable parameter optimization approach guided by observed transferability patterns.

Method: The authors conduct an empirical study using various transformations as baselines to observe transferability patterns across different surrogate models, iteration counts, and tasks. They introduce the Concentric Decay Model (CDM) to explain the rise-then-fall transferability dynamics. Based on these insights, they propose Dynamic Parameter Optimization (DPO), an algorithm that adapts parameters dynamically to exploit the rise-then-fall pattern, reducing optimization complexity from O(m^n) to O(n log m). They validate DPO across multiple transformations, surrogates, iterations, and tasks.

Result: DPO significantly improves transferability compared to baseline parameter choices, and the CDM provides a plausible explanation for the observed dynamics. The approach reduces computational overhead and scales better with the number of parameters, enabling more effective attack optimization over a wider range of settings.

Conclusion: The proposed CDM explains transformation-based transferability dynamics, and the DPO method offers a practical, efficient optimization strategy that enhances transferability across models, iterations, and tasks. This work highlights the importance of dynamic, pattern-informed parameter tuning in adversarial attacks and invites further exploration of dynamic optimization in other attack vectors and defensive countermeasures.

Abstract: Despite their wide application, the vulnerabilities of deep neural networks raise societal concerns. Among them, transformation-based attacks have demonstrated notable success in transfer attacks. However, existing attacks suffer from blind spots in parameter optimization, limiting their full potential. Specifically, (1) prior work generally considers low-iteration settings, yet attacks perform quite differently at higher iterations, so characterizing overall performance based only on low-iteration results is misleading. (2) Existing attacks use uniform parameters for different surrogate models, iterations, and tasks, which greatly impairs transferability. (3) Traditional transformation parameter optimization relies on grid search. For n parameters with m steps each, the complexity is O(mn). Large computational overhead limits further optimization of parameters. To address these limitations, we conduct an empirical study with various transformations as baselines, revealing three dynamic patterns of transferability with respect to parameter strength. We further propose a novel Concentric Decay Model (CDM) to effectively explain these patterns. Building on these insights, we propose an efficient Dynamic Parameter Optimization (DPO) based on the rise-then-fall pattern, reducing the complexity to O(nlogm). Comprehensive experiments on existing transformation-based attacks across different surrogate models, iterations, and tasks demonstrate that our DPO can significantly improve transferability.

</details>


### [32] [LithoSeg: A Coarse-to-Fine Framework for High-Precision Lithography Segmentation](https://arxiv.org/abs/2511.12005)
*Xinyu He,Botong Zhao,Bingbing Li,Shujing Lyu,Jiwei Shen,Yue Lu*

Main category: cs.CV

TL;DR: LithoSeg introduces a coarse-to-fine lithography segmentation network that combines a human-in-the-loop bootstrapped SAM for robust coarse masks with a 1D regression refinement (via a lightweight MLP) on groove-normal profiles, achieving higher segmentation and metrology accuracy with less supervision.


<details>
  <summary>Details</summary>
Motivation: Accurate, pixel-level groove contour segmentation in lithography SEM images is critical for process control and device performance, but existing methods struggle with precision and robustness across diverse geometries and process windows.

Method: Two-stage approach: (1) coarse stage uses a Human-in-the-Loop Bootstrapping scheme for the Segment Anything Model (SAM) to obtain robust segmentation with minimal supervision; (2) fine stage reframes 2D segmentation as 1D regression by sampling groove-normal profiles from the coarse mask and applying a lightweight MLP for point-wise refinement.

Result: LithoSeg outperforms prior methods in segmentation accuracy and metrology precision while requiring less supervision, indicating strong potential for practical deployment.

Conclusion: A principled coarse-to-fine framework that leverages interactive model bootstrapping and 1D refinement can achieve high-precision lithography segmentation and metrology with reduced annotation burden; applicability to real-world SEM-based process control is promising.

Abstract: Accurate segmentation and measurement of lithography scanning electron microscope (SEM) images are crucial for ensuring precise process control, optimizing device performance, and advancing semiconductor manufacturing yield. Lithography segmentation requires pixel-level delineation of groove contours and consistent performance across diverse pattern geometries and process window. However, existing methods often lack the necessary precision and robustness, limiting their practical applicability. To overcome this challenge, we propose LithoSeg, a coarse-to-fine network tailored for lithography segmentation. In the coarse stage, we introduce a Human-in-the-Loop Bootstrapping scheme for the Segment Anything Model (SAM) to attain robustness with minimal supervision. In the subsequent fine stage, we recast 2D segmentation as 1D regression problem by sampling groove-normal profiles using the coarse mask and performing point-wise refinement with a lightweight MLP. LithoSeg outperforms previous approaches in both segmentation accuracy and metrology precision while requiring less supervision, offering promising prospects for real-world applications.

</details>


### [33] [Uncertainty-Guided Selective Adaptation Enables Cross-Platform Predictive Fluorescence Microscopy](https://arxiv.org/abs/2511.12006)
*Kai-Wen K. Yang,Andrew Bai,Alexandra Bermudez,Yunqi Hong,Zoe Latham,Iris Sloan,Michael Liu,Vishrut Goyal,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.CV

TL;DR: SIT-ADDA-Auto adapts only shallow encoder layers with adversarial alignment and uses predictive uncertainty for automatic depth selection, enabling label-free domain adaptation in microscopy across instruments, exposures, and stains; achieves better reconstruction/segmentation than full-encoder/adversarial methods; code released.


<details>
  <summary>Details</summary>
Motivation: Microscopy models degrade under domain shifts from different instruments/settings; need label-free, robust transfer without retraining entire networks; current ADDA retrains whole network, potentially disrupting semantics.

Method: Proposes SIT-ADDA-Auto: restricts adaptation to early convolutional layers (shallow encoder); freezes deeper layers; uses adversarial alignment on shallow features; uses predictive uncertainty to automatically choose adaptation depth without target labels; evaluates with multi-metric metrics, blinded expert assessments, and ablations; tests across exposure/illumination shifts, cross-instrument transfer, multiple stains.

Result: Outperforms full-encoder adaptation and non-adversarial baselines in reconstruction and downstream segmentation; reduces semantic feature drift; robust across shifts; improved multi-metric performance and generalization; code publicly available.

Conclusion: Provides a design rule for label-free adaptation in microscopy and a practical recipe for field deployment; broad applicability across microscopy imaging conditions; supports automatic, self-configuring domain adaptation.

Abstract: Deep learning is transforming microscopy, yet models often fail when applied to images from new instruments or acquisition settings. Conventional adversarial domain adaptation (ADDA) retrains entire networks, often disrupting learned semantic representations. Here, we overturn this paradigm by showing that adapting only the earliest convolutional layers, while freezing deeper layers, yields reliable transfer. Building on this principle, we introduce Subnetwork Image Translation ADDA with automatic depth selection (SIT-ADDA-Auto), a self-configuring framework that integrates shallow-layer adversarial alignment with predictive uncertainty to automatically select adaptation depth without target labels. We demonstrate robustness via multi-metric evaluation, blinded expert assessment, and uncertainty-depth ablations. Across exposure and illumination shifts, cross-instrument transfer, and multiple stains, SIT-ADDA improves reconstruction and downstream segmentation over full-encoder adaptation and non-adversarial baselines, with reduced drift of semantic features. Our results provide a design rule for label-free adaptation in microscopy and a recipe for field settings; the code is publicly available.

</details>


### [34] [Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment Time Analysis](https://arxiv.org/abs/2511.12018)
*Shounak Ray Chaudhuri,Arash Jahangiri,Christopher Paolini*

Main category: cs.CV

TL;DR: A real-time, edge-based PET (Post-Encroachment Time) analysis framework using four synchronized cameras and pixel-level hazard heatmaps at an intersection, achieving sub-second precision and 2.68 FPS on NVIDIA Jetson devices, with a replicable methodology for high-resolution ITS safety evaluation.


<details>
  <summary>Details</summary>
Motivation: Address data sparsity and latency in crash-based safety studies by enabling real-time, high-resolution hazard visualization and monitoring at intersections through decentralized vision-based PET analysis.

Method: Four synchronized cameras feed frames processed on NVIDIA Jetson AGX Xavier devices. YOLOv11 segmentation detects vehicles; detections are converted to polygons and projected into a common bird's-eye view via homography. A novel pixel-level PET algorithm computes proximity hazards without fixed grid cells, producing dynamic 800x800 heatmaps. All events are timestamped and stored in an SQL database for longitudinal analysis. System demonstrates real-time throughput and sub-second hazard localization.

Result: The framework identifies high-risk regions with sub-second precision and sustainable throughput on edge devices, generating a high-resolution PET heatmap (800x800) at ~2.68 FPS. Demonstrates feasibility of decentralized vision-based PET analysis for ITS with replicable methodology for real-time, scalable intersection safety evaluation.

Conclusion: Validates decentralized, vision-based PET analysis as a feasible approach for real-time, high-resolution, scalable safety assessment at signalized intersections, with a clear path for replication and long-term monitoring.

Abstract: Traffic safety analysis at signalized intersections is vital for reducing vehicle and pedestrian collisions, yet traditional crash-based studies are limited by data sparsity and latency. This paper presents a novel multi-camera computer vision framework for real-time safety assessment through Post-Encroachment Time (PET) computation, demonstrated at the intersection of H Street and Broadway in Chula Vista, California. Four synchronized cameras provide continuous visual coverage, with each frame processed on NVIDIA Jetson AGX Xavier devices using YOLOv11 segmentation for vehicle detection. Detected vehicle polygons are transformed into a unified bird's-eye map using homography matrices, enabling alignment across overlapping camera views. A novel pixel-level PET algorithm measures vehicle position without reliance on fixed cells, allowing fine-grained hazard visualization via dynamic heatmaps, accurate to 3.3 sq-cm. Timestamped vehicle and PET data is stored in an SQL database for long-term monitoring. Results over various time intervals demonstrate the framework's ability to identify high-risk regions with sub-second precision and real-time throughput on edge devices, producing data for an 800 x 800 pixel logarithmic heatmap at an average of 2.68 FPS. This study validates the feasibility of decentralized vision-based PET analysis for intelligent transportation systems, offering a replicable methodology for high-resolution, real-time, and scalable intersection safety evaluation.

</details>


### [35] [LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension](https://arxiv.org/abs/2511.12020)
*Xianglong Shi,Silin Cheng,Sirui Zhao,Yunhan Jiang,Enhong Chen,Yang Liu,Sebastien Ourselin*

Main category: cs.CV

TL;DR: Proposes a weakly supervised generalized referring expression framework (WGREC) called LIHE, with a two-stage process and a hybrid Euclidean–hyperbolic similarity (HEMix) to handle expressions referring to variable numbers of objects and prevent semantic collapse, achieving baseline WGREC results and improved grounding metrics.


<details>
  <summary>Details</summary>
Motivation: Real-world expressions can target zero or multiple objects, which one-to-one WREC models struggle with due to ambiguous supervision and the tendency of Euclidean spaces to collapse hierarchies. A weakly supervised WGREC framework is needed to infer both the count and identities of referents while preserving hierarchical distinctions.

Method: Two-stage LIHE framework: (1) Referential Decoupling — predict the number of targets and decompose the expression into simpler sub-expressions; (2) Referent Grounding — localize sub-expressions using HEMix, a hybrid similarity module combining Euclidean precision with hyperbolic hierarchical modeling to avoid semantic collapse.

Result: LIHE achieves the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM; HEMix yields consistent improvements on standard REC benchmarks, with IoU@0.5 improvements up to 2.5%.

Conclusion: The LIHE framework provides a practical path for weakly supervised WGREC, with HEMix effectively maintaining fine-grained distinctions in hierarchical concepts while enabling correct grounding; code is available for reproducibility.

Abstract: Existing Weakly-Supervised Referring Expression Comprehension (WREC) methods, while effective, are fundamentally limited by a one-to-one mapping assumption, hindering their ability to handle expressions corresponding to zero or multiple targets in realistic scenarios. To bridge this gap, we introduce the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC), a more practical paradigm that handles expressions with variable numbers of referents. However, extending WREC to WGREC presents two fundamental challenges: supervisory signal ambiguity, where weak image-level supervision is insufficient for training a model to infer the correct number and identity of referents, and semantic representation collapse, where standard Euclidean similarity forces hierarchically-related concepts into non-discriminative clusters, blurring categorical boundaries. To tackle these challenges, we propose a novel WGREC framework named Linguistic Instance-Split Hyperbolic-Euclidean (LIHE), which operates in two stages. The first stage, Referential Decoupling, predicts the number of target objects and decomposes the complex expression into simpler sub-expressions. The second stage, Referent Grounding, then localizes these sub-expressions using HEMix, our innovative hybrid similarity module that synergistically combines the precise alignment capabilities of Euclidean proximity with the hierarchical modeling strengths of hyperbolic geometry. This hybrid approach effectively prevents semantic collapse while preserving fine-grained distinctions between related concepts. Extensive experiments demonstrate LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM, while HEMix achieves consistent improvements on standard REC benchmarks, improving IoU@0.5 by up to 2.5\%. The code is available at https://anonymous.4open.science/r/LIHE.

</details>


### [36] [Null-Space Diffusion Distillation for Efficient Photorealistic Lensless Imaging](https://arxiv.org/abs/2511.12024)
*Jose Reinaldo Cunha Santos A V Silva Neto,Hodaka Kawachi,Yasushi Yagi,Tomoya Nakamura*

Main category: cs.CV

TL;DR: Null-Space Diffusion Distillation (NSDD) is a fast, ground-truth-free approach for lensless imaging that distills the null-space component of an iterative diffusion model into a single-pass network, preserving data consistency and enabling photorealistic reconstructions without paired data.


<details>
  <summary>Details</summary>
Motivation: To avoid lens-lensless domain mismatch and reliance on paired supervision in lensless imaging, leveraging diffusion priors that remain stable in the highly ill-posed, noisy lensless deconvolution setting.

Method: Train a single-pass student that distills the null-space component of an iterative DDNM+ solver, conditioned on the lensless measurement and a range-space anchor, thereby preserving measurement consistency while leveraging the diffusion prior.

Result: NSDD achieves photorealistic results without paired supervision, runs faster than many baselines, and, on Lensless-FFHQ and PhlatCam, is the second-fastest method after Wiener with near-teacher perceptual quality (second-best LPIPS, below DDNM+), outperforming DPS and classical convex baselines.

Conclusion: The work demonstrates a practical path toward fast, ground-truth-free, photorealistic lensless imaging, combining stability from range-space enforcement with null-space diffusion priors in a compact single-pass model.

Abstract: State-of-the-art photorealistic reconstructions for lensless cameras often rely on paired lensless-lensed supervision, which can bias models due to lens-lensless domain mismatch. To avoid this, ground-truth-free diffusion priors are attractive; however, generic formulations tuned for conventional inverse problems often break under the noisy, highly multiplexed, and ill-posed lensless deconvolution setting. We observe that methods which separate range-space enforcement from null-space diffusion-prior updates yield stable, realistic reconstructions. Building on this, we introduce Null-Space Diffusion Distillation (NSDD): a single-pass student that distills the null-space component of an iterative DDNM+ solver, conditioned on the lensless measurement and on a range-space anchor. NSDD preserves measurement consistency and achieves photorealistic results without paired supervision at a fraction of the runtime and memory. On Lensless-FFHQ and PhlatCam, NSDD is the second fastest, behind Wiener, and achieves near-teacher perceptual quality (second-best LPIPS, below DDNM+), outperforming DPS and classical convex baselines. These results suggest a practical path toward fast, ground-truth-free, photorealistic lensless imaging.

</details>


### [37] [Bridging Vision and Language for Robust Context-Aware Surgical Point Tracking: The VL-SurgPT Dataset and Benchmark](https://arxiv.org/abs/2511.12026)
*Rulin Zhou,Wenlong He,An Wang,Jianhang Zhang,Xuanhui Zeng,Xi Zhang,Chaowei Zhu,Haijun Hu,Hongliang Ren*

Main category: cs.CV

TL;DR: A multimodal dataset and text-guided tracking for surgical point tracking; VL-SurgPT provides 908 in vivo clips (tissue and instrument tracking) and introduces TG-SurgPT, a text-guided tracker leveraging point-status descriptions to improve robustness under challenging intraoperative visuals.


<details>
  <summary>Details</summary>
Motivation: Surgical visual tracking is hampered by smoke, reflections, tissue deformation, and a lack of semantic context to reason about tracking failures; bridging vision and language can enable context-aware robustness.

Method: Construct VL-SurgPT with 908 in vivo video clips (754 tissue-tracking with 17,171 annotated points across five scenarios; 154 instrument-tracking covering seven instrument types with detailed keypoints). Benchmark eight state-of-the-art trackers. Propose TG-SurgPT, a text-guided tracking approach that uses semantic point-status descriptions to guide tracking under adverse conditions.

Result: Incorporating point-status textual information improves tracking accuracy and reliability, especially in challenging scenarios where vision-only methods fail; TG-SurgPT demonstrates the value of visual-linguistic cues for robust surgical tracking.

Conclusion: Bridging visual and linguistic modalities enables context-aware tracking for computer-assisted surgery, maintaining performance under challenging intraoperative conditions.

Abstract: Accurate point tracking in surgical environments remains challenging due to complex visual conditions, including smoke occlusion, specular reflections, and tissue deformation. While existing surgical tracking datasets provide coordinate information, they lack the semantic context necessary to understand tracking failure mechanisms. We introduce VL-SurgPT, the first large-scale multimodal dataset that bridges visual tracking with textual descriptions of point status in surgical scenes. The dataset comprises 908 in vivo video clips, including 754 for tissue tracking (17,171 annotated points across five challenging scenarios) and 154 for instrument tracking (covering seven instrument types with detailed keypoint annotations). We establish comprehensive benchmarks using eight state-of-the-art tracking methods and propose TG-SurgPT, a text-guided tracking approach that leverages semantic descriptions to improve robustness in visually challenging conditions. Experimental results demonstrate that incorporating point status information significantly improves tracking accuracy and reliability, particularly in adverse visual scenarios where conventional vision-only methods struggle. By bridging visual and linguistic modalities, VL-SurgPT enables the development of context-aware tracking systems crucial for advancing computer-assisted surgery applications that can maintain performance even under challenging intraoperative conditions.

</details>


### [38] [GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory](https://arxiv.org/abs/2511.12027)
*Jeong Hun Yeo,Sangyun Chung,Sungjune Park,Dae Hoe Kim,Jinyoung Moon,Yong Man Ro*

Main category: cs.CV

TL;DR: GCAgent introduces a Global-Context-Aware Agent with a Schematic and Narrative Episodic Memory to enable robust long-video understanding by structuring events and their causal/temporal relations into a compact memory, used in a perception-action-reflection cycle via a Memory Manager, improving long-video reasoning.


<details>
  <summary>Details</summary>
Motivation: Long videos pose token and memory challenges; global context and long-term event relations are essential but under-captured by existing MLLMs.

Method: Proposes GCAgent architecture integrating Schematic and Narrative Episodic Memory that encodes events and their relations; Memory Manager retrieves episodic context; operates in multi-stage Perception-Action-Reflection loop; uses structured episodic memory to support inference.

Result: Achieves up to 23.5% accuracy improvement on Video-MME Long split versus strong MLLM baseline; 73.4% Long split accuracy; highest overall average 71.9% on Video-MME among 7B-scale MLLMs; supports cognitively-inspired long-video understanding.

Conclusion: Agent-based reasoning with structured episodic memory effectively addresses long-term dependencies and global context challenges, setting new state-of-the-art for 7B-scale MLLMs on long-video tasks.

Abstract: Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\% accuracy on the Long split and the highest overall average (71.9\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.

</details>


### [39] [VPHO: Joint Visual-Physical Cue Learning and Aggregation for Hand-Object Pose Estimation](https://arxiv.org/abs/2511.12030)
*Jun Zhou,Chi Xu,Kaifeng Tang,Yuting Ge,Tingrui Guo,Li Cheng*

Main category: cs.CV

TL;DR: Jointly learn visual and physical cues and refine via diffusion-based candidate pose aggregation to produce visually accurate and physically plausible hand-object poses from a single RGB image.


<details>
  <summary>Details</summary>
Motivation: To overcome physical constraint violations and lack of end-to-end trainability in hand-object pose estimation from RGB images by integrating physics reasoning into the learning process instead of relying on post-hoc optimization or non-differentiable engines.

Method: 1) Joint visual-physical cue learning that extracts 2D visual cues and 3D physical cues for richer hand-object interaction representations. 2) Candidate pose aggregation that refines multiple diffusion-generated candidate poses by fusing visual and physical predictions to yield a final, visually consistent and physically plausible pose.

Result: Extensive experiments show significant improvements over state-of-the-art methods in both pose accuracy and physical plausibility.

Conclusion: The proposed framework demonstrates that end-to-end integration of visual and physical cues, combined with diffusion-based refinement, yields accurate and physically plausible hand-object poses from a single RGB image.

Abstract: Estimating the 3D poses of hands and objects from a single RGB image is a fundamental yet challenging problem, with broad applications in augmented reality and human-computer interaction. Existing methods largely rely on visual cues alone, often producing results that violate physical constraints such as interpenetration or non-contact. Recent efforts to incorporate physics reasoning typically depend on post-optimization or non-differentiable physics engines, which compromise visual consistency and end-to-end trainability. To overcome these limitations, we propose a novel framework that jointly integrates visual and physical cues for hand-object pose estimation. This integration is achieved through two key ideas: 1) joint visual-physical cue learning: The model is trained to extract 2D visual cues and 3D physical cues, thereby enabling more comprehensive representation learning for hand-object interactions; 2) candidate pose aggregation: A novel refinement process that aggregates multiple diffusion-generated candidate poses by leveraging both visual and physical predictions, yielding a final estimate that is visually consistent and physically plausible. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art approaches in both pose accuracy and physical plausibility.

</details>


### [40] [Improved Masked Image Generation with Knowledge-Augmented Token Representations](https://arxiv.org/abs/2511.12032)
*Guotao Liang,Baoquan Zhang,Zhiyuan Wen,Zihao Han,Yunming Ye*

Main category: cs.CV

TL;DR: KA-MIG uses three knowledge graphs as token-level priors to augment masked image generation, improving class-conditional ImageNet results.


<details>
  <summary>Details</summary>
Motivation: Learning semantic dependencies in masked image token sequences is hard because tokens lack clear semantics and sequences are long; explicit, data-derived prior knowledge can guide representation learning.

Method: Identify three prior knowledge graphs (co-occurrence, semantic similarity, position-token incompatibility); develop a graph-aware encoder to learn token- and position-aware representations; use a lightweight fusion mechanism to integrate these representations into existing MIG models.

Result: Empirical results show improved generation quality over existing MIG methods for class-conditional image generation on ImageNet.

Conclusion: Incorporating explicit token-level knowledge graphs as priors enhances semantic dependency modeling and boosts MIG performance on ImageNet.

Abstract: Masked image generation (MIG) has demonstrated remarkable efficiency and high-fidelity images by enabling parallel token prediction. Existing methods typically rely solely on the model itself to learn semantic dependencies among visual token sequences. However, directly learning such semantic dependencies from data is challenging because the individual tokens lack clear semantic meanings, and these sequences are usually long. To address this limitation, we propose a novel Knowledge-Augmented Masked Image Generation framework, named KA-MIG, which introduces explicit knowledge of token-level semantic dependencies (\emph{i.e.}, extracted from the training data) as priors to learn richer representations for improving performance. In particular, we explore and identify three types of advantageous token knowledge graphs, including two positive and one negative graphs (\emph{i.e.}, the co-occurrence graph, the semantic similarity graph, and the position-token incompatibility graph). Based on three prior knowledge graphs, we design a graph-aware encoder to learn token and position-aware representations. After that, a lightweight fusion mechanism is introduced to integrate these enriched representations into the existing MIG methods. Resorting to such prior knowledge, our method effectively enhances the model's ability to capture semantic dependencies, leading to improved generation quality. Experimental results demonstrate that our method improves upon existing MIG for class-conditional image generation on ImageNet.

</details>


### [41] [Calibrated Multimodal Representation Learning with Missing Modalities](https://arxiv.org/abs/2511.12034)
*Xiaohao Liu,Xiaobo Xia,Jiaheng Wei,Shuo Yang,Xiu Su,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.CV

TL;DR: CalMRL introduces a principled way to calibrate multimodal alignments in the presence of missing modalities by imputing missing representations at the latent level, addressing anchor shift with a bi-step learning process and closed-form posterior, and demonstrating improved performance and convergence on benchmark data.


<details>
  <summary>Details</summary>
Motivation: Many real-world datasets have missing modalities, which breaks the common cross-modal alignment assumption. The anchor shift between observed local anchors and the optimal anchor when all modalities are present degrades alignment quality. A method that can use incomplete data with theoretical guarantees is highly desirable.

Method: CalMRL models missing modalities via priors and intrinsic modality connections to perform representation-level imputation. It uses a bi-step learning procedure with a closed-form posterior for the shared latent variables to resolve optimization, incorporating calibrated alignment with existing advanced multimodal methods.

Result: The approach mitigates anchor shift and shows convergence under theoretical guidance. Extensive experiments and analyses demonstrate superiority over baselines, and the authors plan to release code, checkpoints, and raw evaluation data.

Conclusion: CalMRL provides flexible and theoretically grounded calibration for incomplete multimodal alignments, enabling utilization of datasets with missing modalities and enhancing multimodal representation learning when modalities are not all available.

Abstract: Multimodal representation learning harmonizes distinct modalities by aligning them into a unified latent space. Recent research generalizes traditional cross-modal alignment to produce enhanced multimodal synergy but requires all modalities to be present for a common instance, making it challenging to utilize prevalent datasets with missing modalities. We provide theoretical insights into this issue from an anchor shift perspective. Observed modalities are aligned with a local anchor that deviates from the optimal one when all modalities are present, resulting in an inevitable shift. To address this, we propose CalMRL for multimodal representation learning to calibrate incomplete alignments caused by missing modalities. Specifically, CalMRL leverages the priors and the inherent connections among modalities to model the imputation for the missing ones at the representation level. To resolve the optimization dilemma, we employ a bi-step learning method with the closed-form solution of the posterior distribution of shared latents. We validate its mitigation of anchor shift and convergence with theoretical guidance. By equipping the calibrated alignment with the existing advanced method, we offer new flexibility to absorb data with missing modalities, which is originally unattainable. Extensive experiments and comprehensive analyses demonstrate the superiority of CalMRL. Our code, model checkpoints, and evaluation raw data will be publicly available.

</details>


### [42] [SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images](https://arxiv.org/abs/2511.12040)
*Xinyuan Hu,Changyue Shi,Chuxiao Yang,Minghao Chen,Jiajun Ding,Tao Wei,Chen Wei,Zhou Yu,Min Tan*

Main category: cs.CV

TL;DR: SRSplat is a feed-forward framework that reconstructs high-resolution 3D scenes from a few low-resolution views by leveraging external high-quality reference images and internal texture cues, via Reference-Guided Feature Enhancement (RGFE) and Texture-Aware Density Control (TADC), achieving improved texture detail and strong generalization across datasets.


<details>
  <summary>Details</summary>
Motivation: Reconstructing fine texture from sparse, LR images is difficult due to missing high-frequency information. The authors aim to bridge this gap by injecting external high-quality references and exploiting internal texture cues to supplement LR inputs.

Method: 1) Build a scene-specific reference gallery using Multimodal LLMs and diffusion models. 2) Use RGFE to align/fuse LR input features with a reference twin image. 3) Train a decoder to predict Gaussian primitives from the fused multi-view features. 4) Apply Texture-Aware Density Control (TADC) to adaptively adjust Gaussian density based on texture richness in the LR inputs.

Result: The method outperforms existing approaches on RealEstate10K, ACID, and DTU, with strong cross-dataset and cross-resolution generalization capabilities.

Conclusion: SRSplat effectively mitigates texture information loss in LR-to-HR 3D reconstruction by integrating external references and internal texture cues, enabling robust high-resolution reconstructions from few LR views.

Abstract: Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.

</details>


### [43] [FedSDA: Federated Stain Distribution Alignment for Non-IID Histopathological Image Classification](https://arxiv.org/abs/2511.12044)
*Cheng-Chang Tsai,Kai-Wen Cheng,Chun-Shien Lu*

Main category: cs.CV

TL;DR: FedSDA aligns client stain distributions to a target distribution in federated learning, mitigating non-IID distribution shifts in histopathological images while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Non-IID histopathological data with feature distribution shifts hinder federated learning performance. Diffusion models can fit data distributions and stain separation can reveal salient features linked to non-IID properties, suggesting a data-distribution-centric approach in FL for pathology.

Method: Adjust the data distributions of all clients by aligning their stain distributions to a common target distribution within a federated learning framework. Use diffusion-model-based distribution fitting and stain separation to identify pivotal features, while avoiding training diffusion models on raw data in FL to preserve privacy.

Result: Empirical evaluation shows FedSDA improves baseline methods that mitigate disparities in model updates and outperforms baselines addressing non-IID data from a data-distribution perspective in histopathology tasks.

Conclusion: FedSDA provides practical, privacy-aware insights for computational pathology by aligning staining distributions across clients to reduce non-IID effects in Federated Learning.

Abstract: Federated learning (FL) has shown success in collaboratively training a model among decentralized data resources without directly sharing privacy-sensitive training data. Despite recent advances, non-IID (non-independent and identically distributed) data poses an inevitable challenge that hinders the use of FL. In this work, we address the issue of non-IID histopathological images with feature distribution shifts from an intuitive perspective that has only received limited attention. Specifically, we address this issue from the perspective of data distribution by solely adjusting the data distributions of all clients. Building on the success of diffusion models in fitting data distributions and leveraging stain separation to extract the pivotal features that are closely related to the non-IID properties of histopathological images, we propose a Federated Stain Distribution Alignment (FedSDA) method. FedSDA aligns the stain distribution of each client with a target distribution in an FL framework to mitigate distribution shifts among clients. Furthermore, considering that training diffusion models on raw data in FL has been shown to be susceptible to privacy leakage risks, we circumvent this problem while still effectively achieving alignment. Extensive experimental results show that FedSDA is not only effective in improving baselines that focus on mitigating disparities across clients' model updates but also outperforms baselines that address the non-IID data issues from the perspective of data distribution. We show that FedSDA provides valuable and practical insights for the computational pathology community.

</details>


### [44] [DCMM-Transformer: Degree-Corrected Mixed-Membership Attention for Medical Imaging](https://arxiv.org/abs/2511.12047)
*Huimin Cheng,Xiaowei Yu,Shushan Wu,Luyang Fang,Chao Cao,Jing Zhang,Tianming Liu,Dajiang Zhu,Wenxuan Zhong,Ping Ma*

Main category: cs.CV

TL;DR: A differentiable ViT variant, DCMM-Transformer, uses a degree-corrected mixed-membership prior as an additive bias in self-attention to capture latent anatomical groupings in medical images, improving performance and interpretability over prior structure-aware methods.


<details>
  <summary>Details</summary>
Motivation: Medical images contain latent anatomical groupings (organs, tissues, pathological regions) that standard ViTs fail to exploit. Prior structure-aware approaches (e.g., SBM-Transformer) rely on non-differentiable binary masking and struggle with complex community structure and training stability.

Method: Introduce a Degree-Corrected Mixed-Membership (DCMM) model as an additive bias inside self-attention within a Vision Transformer. This replaces multiplicative masking and binary sampling with a differentiable mechanism that encodes community structure and degree heterogeneity, yielding interpretable attention modulation.

Result: Extensive experiments across diverse medical imaging modalities (brain, chest, breast, ocular) show superior performance and generalizability of DCMM-Transformer. The learned group structure and structured attention produce attention maps that are anatomically meaningful and semantically coherent, enhancing interpretability.

Conclusion: DCMM-Transformer demonstrates that embedding a degree-corrected mixed-membership prior into ViT attention can effectively leverage latent anatomical groupings, improving accuracy and interpretability in medical image analysis and offering a general framework for integrating graph-based priors into ViTs.

Abstract: Medical images exhibit latent anatomical groupings, such as organs, tissues, and pathological regions, that standard Vision Transformers (ViTs) fail to exploit. While recent work like SBM-Transformer attempts to incorporate such structures through stochastic binary masking, they suffer from non-differentiability, training instability, and the inability to model complex community structure. We present DCMM-Transformer, a novel ViT architecture for medical image analysis that incorporates a Degree-Corrected Mixed-Membership (DCMM) model as an additive bias in self-attention. Unlike prior approaches that rely on multiplicative masking and binary sampling, our method introduces community structure and degree heterogeneity in a fully differentiable and interpretable manner. Comprehensive experiments across diverse medical imaging datasets, including brain, chest, breast, and ocular modalities, demonstrate the superior performance and generalizability of the proposed approach. Furthermore, the learned group structure and structured attention modulation substantially enhance interpretability by yielding attention maps that are anatomically meaningful and semantically coherent.

</details>


### [45] [DeiTFake: Deepfake Detection Model using DeiT Multi-Stage Training](https://arxiv.org/abs/2511.12048)
*Saksham Kumar,Ashish Singh,Srinivasarao Thota,Sunil Kumar Singh,Chandan Kumar*

Main category: cs.CV

TL;DR: Two-stage DeiTFake: a DeiT-based deepfake detector using progressive augmentation and knowledge distillation, achieving near-perfect performance on OpenForensics.


<details>
  <summary>Details</summary>
Motivation: Deepfakes undermine digital media integrity; robust detectors must exploit transformer representations and resilient augmentation strategies to capture subtle artifacts.

Method: Two-stage training: stage1 with standard augmentations and transfer learning; stage2 fine-tuning with advanced affine and deepfake-specific augmentations; DeiT-based knowledge distillation to transfer manipulation cues; trained on OpenForensics (190,335 images).

Result: Stage1 accuracy 98.71%; Stage2 accuracy 99.22%; AUROC 0.9997; outperforms OpenForensics baselines; provides practical benchmarks and analysis of augmentation impact and training schedules.

Conclusion: Progressive augmentation and distillation in a DeiT detector yield robust, high-performing deepfake detection suitable for real-world benchmarks.

Abstract: Deepfakes are major threats to the integrity of digital media. We propose DeiTFake, a DeiT-based transformer and a novel two-stage progressive training strategy with increasing augmentation complexity. The approach applies an initial transfer-learning phase with standard augmentations followed by a fine-tuning phase using advanced affine and deepfake-specific augmentations. DeiT's knowledge distillation model captures subtle manipulation artifacts, increasing robustness of the detection model. Trained on the OpenForensics dataset (190,335 images), DeiTFake achieves 98.71\% accuracy after stage one and 99.22\% accuracy with an AUROC of 0.9997, after stage two, outperforming the latest OpenForensics baselines. We analyze augmentation impact and training schedules, and provide practical benchmarks for facial deepfake detection.

</details>


### [46] [UniABG: Unified Adversarial View Bridging and Graph Correspondence for Unsupervised Cross-View Geo-Localization](https://arxiv.org/abs/2511.12054)
*Cuiqun Chen,Qi Chen,Bin Yang,Xingyi Zhang*

Main category: cs.CV

TL;DR: A novel unsupervised dual-stage framework UniABG for cross-view geo-localization, achieving state-of-the-art results by combining view-bridging adversarial features with graph-based correspondence calibration, significantly improving Satellite-to-Drone AP on University-1652 and SUES-200, even beating supervised baselines.


<details>
  <summary>Details</summary>
Motivation: The scalability challenge of cross-view geo-localization due to reliance on costly pairwise annotations and the noise from cross-view domain gaps in unsupervised settings; a need for robust, annotation-free performance.

Method: Two-stage approach: (1) View-Aware Adversarial Bridging (VAAB) to learn view-invariant representations and robust pseudo-labels; (2) Heterogeneous Graph Filtering Calibration (HGFC) to refine cross-view associations via dual inter-view structure graphs for reliable view correspondence.

Result: Achieves state-of-the-art unsupervised performance; Satellite→Drone AP gains of +10.63% on University-1652 and +16.73% on SUES-200, surpassing supervised baselines; source code released at the provided GitHub URL.

Conclusion: UniABG demonstrates that a well-designed unsupervised, dual-stage framework can match or exceed supervised approaches in CVGL by addressing view gaps and label noise through adversarial bridging and graph-based calibration.

Abstract: Cross-view geo-localization (CVGL) matches query images ($\textit{e.g.}$, drone) to geographically corresponding opposite-view imagery ($\textit{e.g.}$, satellite). While supervised methods achieve strong performance, their reliance on extensive pairwise annotations limits scalability. Unsupervised alternatives avoid annotation costs but suffer from noisy pseudo-labels due to intrinsic cross-view domain gaps. To address these limitations, we propose $\textit{UniABG}$, a novel dual-stage unsupervised cross-view geo-localization framework integrating adversarial view bridging with graph-based correspondence calibration. Our approach first employs View-Aware Adversarial Bridging (VAAB) to model view-invariant features and enhance pseudo-label robustness. Subsequently, Heterogeneous Graph Filtering Calibration (HGFC) refines cross-view associations by constructing dual inter-view structure graphs, achieving reliable view correspondence. Extensive experiments demonstrate state-of-the-art unsupervised performance, showing that UniABG improves Satellite $\rightarrow$ Drone AP by +10.63\% on University-1652 and +16.73\% on SUES-200, even surpassing supervised baselines. The source code is available at https://github.com/chenqi142/UniABG

</details>


### [47] [PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling](https://arxiv.org/abs/2511.12056)
*Sijie Wang,Qiang Wang,Shaohuai Shi*

Main category: cs.CV

TL;DR: PipeDiT accelerates diffusion-transformer video generation by pipelining latent generation, decoupling diffusion and VAE across GPU groups, and optimizing VAE attention, achieving 1.06x-4.02x speedups on two 8-GPU systems.


<details>
  <summary>Details</summary>
Motivation: Mitigate slow inference speeds and high memory usage of diffusion-transformer video generation to enable practical deployment.

Method: Introduce PipeSP for sequence parallelism to pipeline latent generation and inter-GPU communication; DeDiVAE to separate diffusion and VAE into two GPU groups with pipelined execution; Aco (attention co-processing) to better utilize VAE GPU resources; integrate into OpenSoraPlan and HunyuanVideo; evaluate on two 8-GPU setups across multiple resolutions and timesteps.

Result: 1.06x to 4.02x speedups over OpenSoraPlan and HunyuanVideo under common resolution/timestep configurations.

Conclusion: PipeDiT demonstrates effective acceleration of diffusion-transformer video generation through staged pipelining and resource-aware design, reducing latency and memory footprint while maintaining quality.

Abstract: Video generation has been advancing rapidly, and diffusion transformer (DiT) based models have demonstrated remark- able capabilities. However, their practical deployment is of- ten hindered by slow inference speeds and high memory con- sumption. In this paper, we propose a novel pipelining frame- work named PipeDiT to accelerate video generation, which is equipped with three main innovations. First, we design a pipelining algorithm (PipeSP) for sequence parallelism (SP) to enable the computation of latent generation and commu- nication among multiple GPUs to be pipelined, thus reduc- ing inference latency. Second, we propose DeDiVAE to de- couple the diffusion module and the variational autoencoder (VAE) module into two GPU groups, whose executions can also be pipelined to reduce memory consumption and infer- ence latency. Third, to better utilize the GPU resources in the VAE group, we propose an attention co-processing (Aco) method to further reduce the overall video generation latency. We integrate our PipeDiT into both OpenSoraPlan and Hun- yuanVideo, two state-of-the-art open-source video generation frameworks, and conduct extensive experiments on two 8- GPU systems. Experimental results show that, under many common resolution and timestep configurations, our PipeDiT achieves 1.06x to 4.02x speedups over OpenSoraPlan and HunyuanVideo.

</details>


### [48] [MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity](https://arxiv.org/abs/2511.12061)
*Zhichen Lai,Hua Lu,Huan Li,Jialiang Li,Christian S. Jensen*

Main category: cs.CV

TL;DR: MovSemCL is a movement-semantics contrastive learning framework for trajectory similarity that uses patch-based hierarchical encoding with intra- and inter-patch attentions and curvature-guided augmentation, achieving state-of-the-art accuracy and reduced inference latency.


<details>
  <summary>Details</summary>
Motivation: Existing learning-based trajectory similarity methods suffer from (1) insufficient modeling of trajectory semantics and hierarchy, missing movement dynamics extraction and multi-scale structure, (2) high computational costs due to point-wise encoding, and (3) physically implausible augmentations that distort semantics. A method addressing semantics, efficiency, and plausible augmentation is needed.

Method: 1) Convert raw GPS trajectories into movement-semantics features. 2) Segment features into patches. 3) Use intra- and inter-patch attention to encode local and global trajectory patterns, enabling hierarchical representation and reducing computation. 4) Apply curvature-guided augmentation to preserve informative segments (e.g., turns/intersections) while masking redundant ones to generate physically plausible augmented views.

Result: On real-world datasets, MovSemCL outperforms state-of-the-art methods in similarity tasks, achieving mean ranks near 1 on similarity search and improving up to 20.3% for heuristic approximation, while reducing inference latency by up to 43.4%.

Conclusion: MovSemCL effectively models movement semantics and hierarchical trajectory structure, enabling efficient, accurate trajectory similarity with plausible augmentations and reduced latency.

Abstract: Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.

</details>


### [49] [DCA-LUT: Deep Chromatic Alignment with 5D LUT for Purple Fringing Removal](https://arxiv.org/abs/2511.12066)
*Jialang Lu,Shuning Sun,Pu Wang,Chen Wu,Feng Gao,Lina Gong,Dianjie Lu,Guijuan Zhang,Zhuoran Zheng*

Main category: cs.CV

TL;DR: Introduces DCA-LUT, a data-driven framework for purple fringing removal leveraging a chromatic-aware transformation and a 5D LUT, trained on PF-Synth with state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Purple fringing from longitudinal chromatic aberration degrades image quality and traditional APO hardware / handcrafted methods are limited; a data-driven approach is needed to learn to decouple fringe from luminance.

Method: Proposes CA-CT module to learn an image-adaptive color space that decouples fringing into a dedicated dimension; learns a purple fringe channel, restores luminance, and applies a 5D LUT for final color correction; trains on a synthetic PF-Synth dataset and validates on synthetic and real-world data.

Result: Achieves state-of-the-art performance on purple fringing removal in both synthetic and real datasets, with robust training and fair evaluation, and efficient non-linear color mapping.

Conclusion: First deep learning framework for purple fringing removal; demonstrates feasibility of data-driven, end-to-end corrections using chromatic-aware transformations and a learned LUT; PF-Synth enables robust evaluation; future work could explore generalization to other chromatic artifacts and real-world deployment.

Abstract: Purple fringing, a persistent artifact caused by Longitudinal Chromatic Aberration (LCA) in camera lenses, has long degraded the clarity and realism of digital imaging. Traditional solutions rely on complex and expensive apochromatic (APO) lens hardware and the extraction of handcrafted features, ignoring the data-driven approach. To fill this gap, we introduce DCA-LUT, the first deep learning framework for purple fringing removal. Inspired by the physical root of the problem, the spatial misalignment of RGB color channels due to lens dispersion, we introduce a novel Chromatic-Aware Coordinate Transformation (CA-CT) module, learning an image-adaptive color space to decouple and isolate fringing into a dedicated dimension. This targeted separation allows the network to learn a precise ``purple fringe channel", which then guides the accurate restoration of the luminance channel. The final color correction is performed by a learned 5D Look-Up Table (5D LUT), enabling efficient and powerful% non-linear color mapping. To enable robust training and fair evaluation, we constructed a large-scale synthetic purple fringing dataset (PF-Synth). Extensive experiments in synthetic and real-world datasets demonstrate that our method achieves state-of-the-art performance in purple fringing removal.

</details>


### [50] [Learning to Hear by Seeing: It's Time for Vision Language Models to Understand Artistic Emotion from Sight and Sound](https://arxiv.org/abs/2511.12077)
*Dengming Zhang,Weitao You,Jingxiong Li,Weishen Lin,Wenda Shi,Xue Zhao,Heda Zuo,Junxian Wu,Lingyun Sun*

Main category: cs.CV

TL;DR: A two-stage framework (VAEmotionLLM) enables a Vision-Language Model to hear and understand cross-modal emotion with limited audio pretraining, by distilling audio pathways from visuals (VG-Align) and adding an Emotion Adapter (EmoAdapter) for emotion-aware cross-modal reasoning, plus a new art-centric benchmark (ArtEmoBenchmark) showing state-of-the-art results across audio-only, visual-only, and audio-visual inputs.


<details>
  <summary>Details</summary>
Motivation: Emotion understanding in art is crucial for aligning LLMs with human perception. Existing AVLMs rely on large-scale audio pretraining and are often single-modality or human-centered, hindering scalable, cross-modal emotion comprehension in art.

Method: Stage 1: Vision-Guided Audio Alignment (VG-Align) distills a frozen visual pathway into a new audio pathway by aligning next-token distributions of a shared LLM on synchronized audio-video clips, enabling hearing with limited audio data. Stage 2: Cross-Modal Emotion Adapter (EmoAdapter) adds an Emotion Enhancer (emotion-sensitive residuals) and an Emotion Supervisor (emotion supervision) to enhance cross-modal emotion understanding. A new ArtEmoBenchmark evaluates content and emotion understanding under audio-only, visual-only, and audio-visual inputs.

Result: VAEmotionLLM achieves state-of-the-art on ArtEmoBenchmark, outperforming audio-only, visual-only, and audio-visual baselines. Ablation studies show the components are complementary.

Conclusion: The two-stage approach enables vision-guided learning of auditory cues and targeted emotion modeling, yielding robust cross-modal emotion understanding in art with limited audio pretraining.

Abstract: Emotion understanding is critical for making Large Language Models (LLMs) more general, reliable, and aligned with humans. Art conveys emotion through the joint design of visual and auditory elements, yet most prior work is human-centered or single-modality, overlooking the emotion intentionally expressed by the artwork. Meanwhile, current Audio-Visual Language Models (AVLMs) typically require large-scale audio pretraining to endow Visual Language Models (VLMs) with hearing, which limits scalability. We present Vision Anchored Audio-Visual Emotion LLM (VAEmotionLLM), a two-stage framework that teaches a VLM to hear by seeing with limited audio pretraining and to understand emotion across modalities. In Stage 1, Vision-Guided Audio Alignment (VG-Align) distills the frozen visual pathway into a new audio pathway by aligning next-token distributions of the shared LLM on synchronized audio-video clips, enabling hearing without a large audio dataset. In Stage 2, a lightweight Cross-Modal Emotion Adapter (EmoAdapter), composed of the Emotion Enhancer and the Emotion Supervisor, injects emotion-sensitive residuals and applies emotion supervision to enhance cross-modal emotion understanding. We also construct ArtEmoBenchmark, an art-centric emotion benchmark that evaluates content and emotion understanding under audio-only, visual-only, and audio-visual inputs. VAEmotionLLM achieves state-of-the-art results on ArtEmoBenchmark, outperforming audio-only, visual-only, and audio-visual baselines. Ablations show that the proposed components are complementary.

</details>


### [51] [Point Cloud Quantization through Multimodal Prompting for 3D Understanding](https://arxiv.org/abs/2511.12079)
*Hongxuan Li,Wencheng Zhu,Huiying Xu,Xinzhong Zhu,Pengfei Zhu*

Main category: cs.CV

TL;DR: A multimodal prompting-driven quantization framework for point clouds that uses pre-trained text embeddings as robust prototypes, refined via multimodal prompts, with dual-constraint (compactness and separation) quantization and Gumbel-Softmax-based differentiable discretization, achieving superior results on ModelNet40 and ScanObjectNN.


<details>
  <summary>Details</summary>
Motivation: Prototype-based quantization in large-scale multimodal models often relies on trainable vectors or clustered centroids, which can lack representativeness and interpretability and struggle to bridge vision-language semantic gaps. Leveraging text embeddings that encode visual semantics and adaptive multimodal prompts can provide robust prototype priors and refined prototypes.

Method: Introduce a dual-constrained quantization space that fuses visual features with text-derived prototypes; use compactness and separation regularization to encourage cohesive yet distinct representations. Employ multimodal prompts to adaptively refine prototypes and align them with visual data. Apply Gumbel-Softmax relaxation to achieve differentiable discretization with controlled sparsity, yielding hybrid geometric-semantic representations.

Result: Extensive experiments on ModelNet40 and ScanObjectNN demonstrate superior effectiveness of the proposed method compared to baselines.

Conclusion: The framework effectively unifies geometric and semantic information in point-cloud representations, improving prototype representativeness and interpretability while enabling end-to-end differentiable training through quantized, multimodal prototypes.

Abstract: Vector quantization has emerged as a powerful tool in large-scale multimodal models, unifying heterogeneous representations through discrete token encoding. However, its effectiveness hinges on robust codebook design. Current prototype-based approaches relying on trainable vectors or clustered centroids fall short in representativeness and interpretability, even as multimodal alignment demonstrates its promise in vision-language models. To address these limitations, we propose a simple multimodal prompting-driven quantization framework for point cloud analysis. Our methodology is built upon two core insights: 1) Text embeddings from pre-trained models inherently encode visual semantics through many-to-one contrastive alignment, naturally serving as robust prototype priors; and 2) Multimodal prompts enable adaptive refinement of these prototypes, effectively mitigating vision-language semantic gaps. The framework introduces a dual-constrained quantization space, enforced by compactness and separation regularization, which seamlessly integrates visual and prototype features, resulting in hybrid representations that jointly encode geometric and semantic information. Furthermore, we employ Gumbel-Softmax relaxation to achieve differentiable discretization while maintaining quantization sparsity. Extensive experiments on the ModelNet40 and ScanObjectNN datasets clearly demonstrate the superior effectiveness of the proposed method.

</details>


### [52] [Supervised Multilabel Image Classification Using Residual Networks with Probabilistic Reasoning](https://arxiv.org/abs/2511.12082)
*Lokender Singh,Saksham Kumar,Chandan Kumar*

Main category: cs.CV

TL;DR: A probabilistic multilabel classification method on COCO-2014 using a modified ResNet-101; models label dependencies and uncertainty to boost accuracy, achieving 0.794 mAP, surpassing ResNet-SRN and Vision Transformer baselines.


<details>
  <summary>Details</summary>
Motivation: Multilabel categorization is hampered by complex label dependencies and uncertainty; integrating probabilistic reasoning into deep nets could enhance prediction accuracy beyond standard discriminative models.

Method: Modify ResNet-101; simulate label dependencies and uncertainties; apply probabilistic reasoning to model joint label distribution; training on COCO-2014; compare to SRN and ViT baselines.

Result: 0.794 mAP on COCO-2014; outperforms ResNet-SRN (0.771) and Vision Transformer baselines (0.785).

Conclusion: Key novelty is integrating probabilistic reasoning into deep learning for multilabel tasks; effective in addressing dependencies and uncertainty; potential to generalize to other datasets.

Abstract: Multilabel image categorization has drawn interest recently because of its numerous computer vision applications. The proposed work introduces a novel method for classifying multilabel images using the COCO-2014 dataset and a modified ResNet-101 architecture. By simulating label dependencies and uncertainties, the approach uses probabilistic reasoning to improve prediction accuracy. Extensive tests show that the model outperforms earlier techniques and approaches to state-of-the-art outcomes in multilabel categorization. The work also thoroughly assesses the model's performance using metrics like precision-recall score and achieves 0.794 mAP on COCO-2014, outperforming ResNet-SRN (0.771) and Vision Transformer baselines (0.785). The novelty of the work lies in integrating probabilistic reasoning into deep learning models to effectively address the challenges presented by multilabel scenarios.

</details>


### [53] [SemanticStitch: Enhancing Image Coherence through Foreground-Aware Seam Carving](https://arxiv.org/abs/2511.12084)
*Ji-Ping Jin,Chen-Bin Feng,Rui Fan,Chi-Man Vong*

Main category: cs.CV

TL;DR: SemanticStitch is a semantic-aware image stitching framework that preserves foreground integrity via a semantic-prior loss, yielding superior coherence over traditional seam-carving methods, validated on two real-world datasets.


<details>
  <summary>Details</summary>
Motivation: To overcome misalignments and foreground discontinuities caused by varying capture angles/positional differences and object movements, addressing the neglect of semantic information in traditional seam carving-based stitching.

Method: A deep learning-based stitching pipeline that integrates semantic priors of foreground objects; introduces a semantic integrity loss that emphasizes preservation of salient objects during seam selection/merging; evaluated on two real-world datasets.

Result: Significant improvements over traditional methods in stitching quality and foreground continuity; demonstrated robustness on real-world data.

Conclusion: Incorporating semantic priors enables preservation of the semantic integrity of salient objects, yielding more coherent stitched results and practical applicability in real-world scenarios.

Abstract: Image stitching often faces challenges due to varying capture angles, positional differences, and object movements, leading to misalignments and visual discrepancies. Traditional seam carving methods neglect semantic information, causing disruptions in foreground continuity. We introduce SemanticStitch, a deep learning-based framework that incorporates semantic priors of foreground objects to preserve their integrity and enhance visual coherence. Our approach includes a novel loss function that emphasizes the semantic integrity of salient objects, significantly improving stitching quality. We also present two specialized real-world datasets to evaluate our method's effectiveness. Experimental results demonstrate substantial improvements over traditional techniques, providing robust support for practical applications.

</details>


### [54] [Teaching Prompts to Coordinate: Hierarchical Layer-Grouped Prompt Tuning for Continual Learning](https://arxiv.org/abs/2511.12090)
*Shengqin Jiang,Tianqi Kong,Yuankai Qi,Haokui Zhang,Lina Yao,Quan Z. Sheng,Qingshan Liu,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: Hierarchical layer-grouped prompt tuning for continual learning stabilizes adaptation and mitigates forgetting by sharing prompts within groups via position encoding and using a root prompt to generate sub-prompts for each group, showing favorable performance over SOTA on four benchmarks.


<details>
  <summary>Details</summary>
Motivation: Prompt-based continual learning can become unstable when every layer has independent prompts, risking catastrophic forgetting of prior tasks; a balance between adaptability and stability with parameter efficiency is needed.

Method: Divide model layers into groups; within a group, use shared prompts adjusted by position encoding; employ a single task-specific root prompt that generates sub-prompts for each layer group; all sub-prompts conditioned on the root; freeze backbone and learn prompts only.

Result: Experimental results on four benchmarks demonstrate favorable performance compared with several state-of-the-art continual learning methods.

Conclusion: Hierarchical prompt grouping with a root prompt improves stability and reduces forgetting in continual learning by enforcing intra-group prompt sharing and centralized generation for prompts.

Abstract: Prompt-based continual learning methods fine-tune only a small set of additional learnable parameters while keeping the pre-trained model's parameters frozen. It enables efficient adaptation to new tasks while mitigating the risk of catastrophic forgetting. These methods typically attach one independent task-specific prompt to each layer of pre-trained models to locally modulate its features, ensuring that the layer's representation aligns with the requirements of the new task. However, although introducing learnable prompts independently at each layer provides high flexibility for adapting to new tasks, this overly flexible tuning could make certain layers susceptible to unnecessary updates. As all prompts till the current task are added together as a final prompt for all seen tasks, the model may easily overwrite feature representations essential to previous tasks, which increases the risk of catastrophic forgetting. To address this issue, we propose a novel hierarchical layer-grouped prompt tuning method for continual learning. It improves model stability in two ways: (i) Layers in the same group share roughly the same prompts, which are adjusted by position encoding. This helps preserve the intrinsic feature relationships and propagation pathways of the pre-trained model within each group. (ii) It utilizes a single task-specific root prompt to learn to generate sub-prompts for each layer group. In this way, all sub-prompts are conditioned on the same root prompt, enhancing their synergy and reducing independence. Extensive experiments across four benchmarks demonstrate that our method achieves favorable performance compared with several state-of-the-art methods.

</details>


### [55] [Learning from Dense Events: Towards Fast Spiking Neural Networks Training via Event Dataset Distillatio](https://arxiv.org/abs/2511.12095)
*Shuhan Ye,Yi Yu,Qixin Zhang,Chenqi Kong,Qiangqiang Wu,Kun Wang,Xudong Jiang*

Main category: cs.CV

TL;DR: PACE is a dataset distillation framework for spiking neural networks (SNNs) applied to event-based vision. It creates compact synthetic datasets via two modules, ST-DSM and PEQ-N, enabling fast, edge-friendly SNN training with strong accuracy gains on dynamic event streams; on N-MNIST it reaches 84.4% accuracy (about 85% of full-data performance) with >50× faster training and ~6000× lower storage, generalizing across DVS-Gesture, CIFAR10-DVS, and N-MNIST.


<details>
  <summary>Details</summary>
Motivation: SNNs are energy-efficient for event-based vision but hard to train due to temporal coding, making practical deployment costly. Distilling large datasets into compact surrogates can drastically reduce training cost while preserving performance, enabling faster development and edge deployment.

Method: PACE distills large datasets into a compact synthetic subset using two core modules: ST-DSM (stacks residual membrane potentials to densify spike-based features and perform fine-grained spatiotemporal matching of amplitude and phase) and ST-SM (spatiotemporal matching of amplitude and phase); PEQ-N is a plug-and-play straight-through probabilistic integer quantizer compatible with standard event-frame pipelines.

Result: Across DVS-Gesture, CIFAR10-DVS, and N-MNIST, PACE outperforms existing coreset selection and dataset distillation baselines, with pronounced gains on dynamic event streams and at low/moderate average pixel throughput (IPC). On N-MNIST, 84.4% accuracy achieved, ~85% of full-training performance, while reducing training time by >50× and storage by ~6000×, yielding compact surrogates enabling minute-scale SNN training and efficient edge deployment.

Conclusion: PACE enables practical, efficient training of SNNs for event-based vision by producing compact, high-quality synthetic datasets that preserve accuracy while dramatically reducing computational and storage costs, facilitating rapid experimentation and on-device deployment.

Abstract: Event cameras sense brightness changes and output binary asynchronous event streams, attracting increasing attention. Their bio-inspired dynamics align well with spiking neural networks (SNNs), offering a promising energy-efficient alternative to conventional vision systems. However, SNNs remain costly to train due to temporal coding, which limits their practical deployment. To alleviate the high training cost of SNNs, we introduce \textbf{PACE} (Phase-Aligned Condensation for Events), the first dataset distillation framework to SNNs and event-based vision. PACE distills a large training dataset into a compact synthetic one that enables fast SNN training, which is achieved by two core modules: \textbf{ST-DSM} and \textbf{PEQ-N}. ST-DSM uses residual membrane potentials to densify spike-based features (SDR) and to perform fine-grained spatiotemporal matching of amplitude and phase (ST-SM), while PEQ-N provides a plug-and-play straight through probabilistic integer quantizer compatible with standard event-frame pipelines. Across DVS-Gesture, CIFAR10-DVS, and N-MNIST datasets, PACE outperforms existing coreset selection and dataset distillation baselines, with particularly strong gains on dynamic event streams and at low or moderate IPC. Specifically, on N-MNIST, it achieves \(84.4\%\) accuracy, about \(85\%\) of the full training set performance, while reducing training time by more than \(50\times\) and storage cost by \(6000\times\), yielding compact surrogates that enable minute-scale SNN training and efficient edge deployment.

</details>


### [56] [OPFormer: Object Pose Estimation leveraging foundation model with geometric encoding](https://arxiv.org/abs/2511.12614)
*Artem Moroz,Vít Zeman,Martin Mikšík,Elizaveta Isianova,Miroslav David,Pavel Burget,Varun Burde*

Main category: cs.CV

TL;DR: A unified end-to-end framework that combines object detection and 6D pose estimation with an onboarding stage that can derive object representations from CAD models or NeRF, using CNOS for localization and OPFormer for pose estimation with NOCS priors; shows favorable accuracy and efficiency on BOP benchmarks.


<details>
  <summary>Details</summary>
Motivation: To provide a robust, end-to-end solution for joint detection and pose estimation that can adapt to varying object representations (CAD or neural reconstructions) and operate efficiently in real-world scenarios by leveraging multi-view templates and geometric priors.

Method: 1) Onboarding stage creates object representations from traditional 3D CAD or NeRF reconstructions. 2) CNOS detector localizes target objects in test images. 3) OPFormer, a transformer-based pose estimator, leverages a foundation model for robust features, encodes multiple template views, and incorporates 3D priors via NOCS. 4) A decoder constructs 2D-3D correspondences to compute the 6D pose.

Result: Benchmark evaluation on the challenging BOP datasets shows a favorable balance between accuracy and efficiency, with the integrated system performing well in both model-based and model-free settings.

Conclusion: The proposed framework delivers an effective, end-to-end solution for integrated object detection and pose estimation, with a versatile onboarding process that supports CAD and NeRF representations and strong real-world applicability.

Abstract: We introduce a unified, end-to-end framework that seamlessly integrates object detection and pose estimation with a versatile onboarding process. Our pipeline begins with an onboarding stage that generates object representations from either traditional 3D CAD models or, in their absence, by rapidly reconstructing a high-fidelity neural representation (NeRF) from multi-view images. Given a test image, our system first employs the CNOS detector to localize target objects. For each detection, our novel pose estimation module, OPFormer, infers the precise 6D pose. The core of OPFormer is a transformer-based architecture that leverages a foundation model for robust feature extraction. It uniquely learns a comprehensive object representation by jointly encoding multiple template views and enriches these features with explicit 3D geometric priors using Normalized Object Coordinate Space (NOCS). A decoder then establishes robust 2D-3D correspondences to determine the final pose. Evaluated on the challenging BOP benchmarks, our integrated system demonstrates a strong balance between accuracy and efficiency, showcasing its practical applicability in both model-based and model-free scenarios.

</details>


### [57] [Sparse by Rule: Probability-Based N:M Pruning for Spiking Neural Networks](https://arxiv.org/abs/2511.12097)
*Shuhan Ye,Yi Yu,Qixin Zhang,Chenqi Kong,Qiangqiang Wu,Xudong Jiang,Dacheng Tao*

Main category: cs.CV

TL;DR: SpikeNM: first semi-structured N:M pruning framework for SNNs from scratch, using M-way basis-logit per block with differentiable top-k, O(M) complexity, plus eligibility-inspired distillation; achieves 2:4 sparsity with accuracy gains and hardware-friendly patterns.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of unstructured pruning (hard to accelerate) and structured pruning (loss of accuracy and inflexibility) in deep SNNs, enabling energy-efficient edge deployment.

Method: Per-block N: M sparse constraint realized via M-way basis-logit parameterization with differentiable top-k; linear time per block; eligibility-inspired distillation to align mask probabilities with spiking dynamics; training from scratch.

Result: At 2:4 sparsity, maintains or improves accuracy across mainstream datasets; produces hardware-friendly sparse patterns that complement intrinsic spike sparsity.

Conclusion: SpikeNM demonstrates that semi-structured pruning can achieve high sparsity with accuracy and hardware compatibility, offering a scalable approach for efficient SNN deployment.

Abstract: Brain-inspired Spiking neural networks (SNNs) promise energy-efficient intelligence via event-driven, sparse computation, but deeper architectures inflate parameters and computational cost, hindering their edge deployment. Recent progress in SNN pruning helps alleviate this burden, yet existing efforts fall into only two families: \emph{unstructured} pruning, which attains high sparsity but is difficult to accelerate on general hardware, and \emph{structured} pruning, which eases deployment but lack flexibility and often degrades accuracy at matched sparsity. In this work, we introduce \textbf{SpikeNM}, the first SNN-oriented \emph{semi-structured} \(N{:}M\) pruning framework that learns sparse SNNs \emph{from scratch}, enforcing \emph{at most \(N\)} non-zeros per \(M\)-weight block. To avoid the combinatorial space complexity \(\sum_{k=1}^{N}\binom{M}{k}\) growing exponentially with \(M\), SpikeNM adopts an \(M\)-way basis-logit parameterization with a differentiable top-\(k\) sampler, \emph{linearizing} per-block complexity to \(\mathcal O(M)\) and enabling more aggressive sparsification. Further inspired by neuroscience, we propose \emph{eligibility-inspired distillation} (EID), which converts temporally accumulated credits into block-wise soft targets to align mask probabilities with spiking dynamics, reducing sampling variance and stabilizing search under high sparsity. Experiments show that at \(2{:}4\) sparsity, SpikeNM maintains and even with gains across main-stream datasets, while yielding hardware-amenable patterns that complement intrinsic spike sparsity.

</details>


### [58] [DINOv3-Guided Cross Fusion Framework for Semantic-aware CT generation from MRI and CBCT](https://arxiv.org/abs/2511.12098)
*Xianhao Zhou,Jianghao Wu,Ku Zhao,Jinlong He,Huangxuan Zhao,Lei Chen,Shaoting Zhang,Guotai Wang*

Main category: cs.CV

TL;DR: A DGCF framework uses a frozen DINOv3 Transformer together with a trainable CNN encoder-decoder, via a learnable cross fusion and a multi-level DINOv3 perceptual loss, to synthesize CT from MRI/CBCT; it achieves state-of-the-art results on SynthRAD2023 and is the first to leverage DINOv3 representations for medical image translation.


<details>
  <summary>Details</summary>
Motivation: Existing CNNs lack global semantic understanding necessary for semantic-aware CT synthesis, while Transformers risk overfitting on small medical datasets due to high capacity and weak inductive bias. There is a need to combine global context with local detail in a data-efficient manner.

Method: A DINOv3-Guided Cross Fusion (DGCF) framework: a frozen self-supervised DINOv3 Transformer provides global representations, which are hierarchically fused with local features from a trainable CNN encoder-decoder via a learnable cross fusion module. A Multi-Level DINOv3 Perceptual (MLDP) loss enforces semantic similarity between synthetic CT and ground truth in DINOv3 feature space. The model is evaluated on MRI→CT and CBCT→CT translation tasks on the SynthRAD2023 pelvic dataset.

Result: DGCF achieved state-of-the-art performance across MS-SSIM, PSNR and segmentation-based metrics on both MRI→CT and CBCT→CT translations on SynthRAD2023.

Conclusion: This work is the first to employ DINOv3 representations for medical image translation, demonstrating the potential of self-supervised Transformer guidance for semantic-aware CT synthesis; code is available at the referenced repository.

Abstract: Generating synthetic CT images from CBCT or MRI has a potential for efficient radiation dose planning and adaptive radiotherapy. However, existing CNN-based models lack global semantic understanding, while Transformers often overfit small medical datasets due to high model capacity and weak inductive bias. To address these limitations, we propose a DINOv3-Guided Cross Fusion (DGCF) framework that integrates a frozen self-supervised DINOv3 Transformer with a trainable CNN encoder-decoder. It hierarchically fuses global representation of Transformer and local features of CNN via a learnable cross fusion module, achieving balanced local appearance and contextual representation. Furthermore, we introduce a Multi-Level DINOv3 Perceptual (MLDP) loss that encourages semantic similarity between synthetic CT and the ground truth in DINOv3's feature space. Experiments on the SynthRAD2023 pelvic dataset demonstrate that DGCF achieved state-of-the-art performance in terms of MS-SSIM, PSNR and segmentation-based metrics on both MRI$\rightarrow$CT and CBCT$\rightarrow$CT translation tasks. To the best of our knowledge, this is the first work to employ DINOv3 representations for medical image translation, highlighting the potential of self-supervised Transformer guidance for semantic-aware CT synthesis. The code is available at https://github.com/HiLab-git/DGCF.

</details>


### [59] [Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views](https://arxiv.org/abs/2511.12878)
*Junyi Ma,Wentao Bao,Jingyi Xu,Guanzhong Sun,Yu Zheng,Erhang Zhang,Xieyuanli Chen,Hesheng Wang*

Main category: cs.CV

TL;DR: Zero-shot temporal interaction localization (TIL) for egocentric videos using EgoLoc; localizes hand–object contact and separation moments without object masks or taxonomies by hand-dynamics prompting a vision–language model.


<details>
  <summary>Details</summary>
Motivation: Understanding when contact occurs is crucial for immersive AR/VR and robotic manipulation; existing approaches rely on semantic masks or verb–noun categories, which can hinder grounding precision and generalization.

Method: Hand-dynamics-guided sampling to create high-quality prompts; leverage a vision–language model to identify contact/separation attributes and localize timestamps; provide closed-loop refinement; no object masks or taxonomy reliance; zero-shot.

Result: Demonstrates plausible TIL on public datasets and novel benchmarks; supports downstream applications in egocentric vision and robotic manipulation tasks.

Conclusion: EgoLoc offers a generalizable zero-shot solution for precise hand–object temporal localization in egocentric video, reducing annotation needs and benefiting VR/AR and robotics.

Abstract: Analyzing hand-object interaction in egocentric vision facilitates VR/AR applications and human-robot policy transfer. Existing research has mostly focused on modeling the behavior paradigm of interactive actions (i.e., "how to interact"). However, the more challenging and fine-grained problem of capturing the critical moments of contact and separation between the hand and the target object (i.e., "when to interact") is still underexplored, which is crucial for immersive interactive experiences in mixed reality and robotic motion planning. Therefore, we formulate this problem as temporal interaction localization (TIL). Some recent works extract semantic masks as TIL references, but suffer from inaccurate object grounding and cluttered scenarios. Although current temporal action localization (TAL) methods perform well in detecting verb-noun action segments, they rely on category annotations during training and exhibit limited precision in localizing hand-object contact/separation moments. To address these issues, we propose a novel zero-shot approach dubbed EgoLoc to localize hand-object contact and separation timestamps in egocentric videos. EgoLoc introduces hand-dynamics-guided sampling to generate high-quality visual prompts. It exploits the vision-language model to identify contact/separation attributes, localize specific timestamps, and provide closed-loop feedback for further refinement. EgoLoc eliminates the need for object masks and verb-noun taxonomies, leading to generalizable zero-shot implementation. Comprehensive experiments on the public dataset and our novel benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric videos. It is also validated to effectively facilitate multiple downstream applications in egocentric vision and robotic manipulation tasks. Code and relevant data will be released at https://github.com/IRMVLab/EgoLoc.

</details>


### [60] [Adaptive Begin-of-Video Tokens for Autoregressive Video Diffusion Models](https://arxiv.org/abs/2511.12099)
*Tianle Cheng,Zeyan Zhang,Kaifeng Gao,Jun Xiao*

Main category: cs.CV

TL;DR: Introduces Adaptive Begin-of-Video Tokens (ada-BOV) for autoregressive diffusion video models to enhance long-video coherence and dynamics, along with a refinement strategy for stream denoising and a disturbance-augmented training schedule; achieves strong results on qualitative and quantitative metrics.


<details>
  <summary>Details</summary>
Motivation: Long-video diffusion models struggle to maintain global coherence and realistic motion over extended sequences. Chunk-based conditioning suffers denoising latency and error accumulation, while stream denoising often yields fragile consistency and weak motion dynamics. A robust conditioning and training framework is needed for high-quality long videos.

Method: Propose Adaptive Begin-of-Video Tokens (ada-BOV): learnable BOV tokens on diffusion models that adaptively absorb denoised preceding frames via an adaptive-layer-norm-like modulation to preserve global consistency and enable flexible conditioning. Introduce a refinement strategy for stream denoising that decouples sampling trajectory length from the attention window size to improve local guidance and image quality. Add a disturbance-augmented training noise schedule to balance convergence speed with model robustness for stream denoising.

Result: Extensive experiments show compelling qualitative and quantitative improvements across multiple metrics, demonstrating better global coherence and local dynamics in long-video generation compared with baselines.

Conclusion: Ada-BOV, together with the refinement strategy and disturbance-augmented training, enhances autoregressive diffusion video models for long videos by stabilizing global consistency and improving local motion, enabling more reliable live-stream sampling and higher-quality long-form video generation.

Abstract: Recent advancements in diffusion-based video generation have produced impressive and high-fidelity short videos. To extend these successes to generate coherent long videos, most video diffusion models (VDMs) generate videos in an autoregressive manner, i.e., generating subsequent frames conditioned on previous ones. There are generally two primary paradigms: chunk-based extension and stream denoising. The former directly concatenates previous clean frames as conditioning, suffering from denoising latency and error accumulation. The latter maintains the denoising sequence with monotonically increasing noise levels. In each denoising iteration, one clean frame is produced while a new pure noise is simultaneously appended, enabling live-stream sampling. However, it struggles with fragile consistency and poor motion dynamics. In this paper, we propose Adaptive Begin-of-Video Tokens (ada-BOV) for autoregressive VDMs. The BOV tokens are special learnable embeddings on VDMs. They adaptively absorb denoised preceding frames via an adaptive-layer-norm-like modulation. This design preserves the global consistency while allowing for flexible conditioning in dynamic scenarios. To ensure the quality of local dynamics essential in modulating BOV tokens, we further propose a refinement strategy for stream denoising. It decouples the sampling trajectory length from the attention window size constraint, leading to improved local guidance and overall imaging quality. We also propose a disturbance-augmented training noise schedule, which balances the convergence speed with model robustness for the stream denoising. Extensive experiments demonstrate that our method achieves compelling qualitative and quantitative results across multiple metrics.

</details>


### [61] [DiffPixelFormer: Differential Pixel-Aware Transformer for RGB-D Indoor Scene Segmentation](https://arxiv.org/abs/2511.13047)
*Yan Gong,Jianli Lu,Yongsheng Gao,Jie Zhao,Xiaojuan Zhang,Susanto Rahardja*

Main category: cs.CV

TL;DR: A pixel-aware, differential RGB-D Transformer (DiffPixelFormer) achieves improved indoor segmentation by combining intra-modal self-attention with a differential inter-modal module, plus dynamic fusion, leading to state-of-the-art results on SUN RGB-D and NYUDv2.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-D segmentation methods rely on computationally heavy cross-attention and poorly model both intra- and inter-modal relationships, causing imprecise feature alignment and limited discriminative power. The work motivates a lighter, more expressive architecture that explicitly captures modality-specific and shared cues and enhances intra-modal dependencies.

Method: Proposes Intra-Inter Modal Interaction Block (IIMIB) to capture intra-modal long-range dependencies via self-attention; introduces Differential-Shared Inter-Modal (DSIM) module to disentangle modality-specific vs. shared cues for cross-modal alignment; employs a dynamic fusion strategy to balance RGB and depth contributions according to scene characteristics.

Result: On SUN RGB-D and NYUDv2, DiffPixelFormer-L achieves mIoU of 54.28% and 59.95%, respectively, beating DFormer-L by 1.78 and 2.75 percentage points. Code available at the provided GitHub link.

Conclusion: The approach yields finer-grained pixel-level cross-modal alignment and stronger discriminative representations by separating modality-specific/shared information and adapting fusion to scene context, achieving state-of-the-art results on two benchmarks.

Abstract: Indoor semantic segmentation is fundamental to computer vision and robotics, supporting applications such as autonomous navigation, augmented reality, and smart environments. Although RGB-D fusion leverages complementary appearance and geometric cues, existing methods often depend on computationally intensive cross-attention mechanisms and insufficiently model intra- and inter-modal feature relationships, resulting in imprecise feature alignment and limited discriminative representation. To address these challenges, we propose DiffPixelFormer, a differential pixel-aware Transformer for RGB-D indoor scene segmentation that simultaneously enhances intra-modal representations and models inter-modal interactions. At its core, the Intra-Inter Modal Interaction Block (IIMIB) captures intra-modal long-range dependencies via self-attention and models inter-modal interactions with the Differential-Shared Inter-Modal (DSIM) module to disentangle modality-specific and shared cues, enabling fine-grained, pixel-level cross-modal alignment. Furthermore, a dynamic fusion strategy balances modality contributions and fully exploits RGB-D information according to scene characteristics. Extensive experiments on the SUN RGB-D and NYUDv2 benchmarks demonstrate that DiffPixelFormer-L achieves mIoU scores of 54.28% and 59.95%, outperforming DFormer-L by 1.78% and 2.75%, respectively. Code is available at https://github.com/gongyan1/DiffPixelFormer.

</details>


### [62] [Did Models Sufficient Learn? Attribution-Guided Training via Subset-Selected Counterfactual Augmentation](https://arxiv.org/abs/2511.12100)
*Yannan Chen,Ruoyu Chen,Bin Zeng,Wei Wang,Shiming Liu,Qunli Zhang,Zheng Hu,Laiyuan Wang,Yaowei Wang,Xiaochun Cao*

Main category: cs.CV

TL;DR: SS-CA uses Counterfactual LIMA to identify minimal regions whose removal alters predictions, replaces them with natural backgrounds during training, and jointly trains on augmented and original data to improve ID and OOD generalization and robustness.


<details>
  <summary>Details</summary>
Motivation: Current visual models rely on limited causal cues, making them vulnerable to distribution shifts. Attribution methods can locate critical regions, but masking them to create counterfactuals can yield nonhuman-recognizable inputs, revealing a mismatch with human causality. SS-CA aims to align model learning with causal structure by integrating counterfactuals into training.

Method: Builds on subset-selection-based LIMA attribution to derive Counterfactual LIMA, identifying minimal spatial region sets whose removal changes predictions. Uses these attributions to augment data by replacing identified regions with natural background, and trains the model jointly on augmented and original samples.

Result: Extensive experiments on ImageNet variants show improved in-distribution generalization and superior out-of-distribution performance on ImageNet-R and ImageNet-S. Models also show enhanced robustness under perturbations like noise.

Conclusion: Integrating counterfactual explanations into training helps mitigate incomplete causal learning, improving both performance and robustness.

Abstract: In current visual model training, models often rely on only limited sufficient causes for their predictions, which makes them sensitive to distribution shifts or the absence of key features. Attribution methods can accurately identify a model's critical regions. However, masking these areas to create counterfactuals often causes the model to misclassify the target, while humans can still easily recognize it. This divergence highlights that the model's learned dependencies may not be sufficiently causal. To address this issue, we propose Subset-Selected Counterfactual Augmentation (SS-CA), which integrates counterfactual explanations directly into the training process for targeted intervention. Building on the subset-selection-based LIMA attribution method, we develop Counterfactual LIMA to identify minimal spatial region sets whose removal can selectively alter model predictions. Leveraging these attributions, we introduce a data augmentation strategy that replaces the identified regions with natural background, and we train the model jointly on both augmented and original samples to mitigate incomplete causal learning. Extensive experiments across multiple ImageNet variants show that SS-CA improves generalization on in-distribution (ID) test data and achieves superior performance on out-of-distribution (OOD) benchmarks such as ImageNet-R and ImageNet-S. Under perturbations including noise, models trained with SS-CA also exhibit enhanced generalization, demonstrating that our approach effectively uses interpretability insights to correct model deficiencies and improve both performance and robustness.

</details>


### [63] [PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image](https://arxiv.org/abs/2511.13648)
*Ziang Cao,Fangzhou Hong,Zhaoxi Chen,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: PhysX-Anything proposes a first-of-its-kind sim-ready, physical 3D generative model that, from a single image, outputs articulated 3D assets with explicit geometry and physical attributes. It introduces a VLM-based physical 3D generator and a compact geometry representation that tokenizes geometry far more efficiently (193x fewer tokens), enabling explicit geometry learning within standard VLM budgets without special tokens during fine-tuning. The authors also release PhysX-Mobility, a large, diverse dataset (>2K real-world objects, >2x category expansion) with rich physical annotations. Extensive experiments on in-the-wild images and the PhysX-Mobility dataset, plus MuJoCo-style simulation, show strong generalization and direct applicability to contact-rich robotic policy learning.


<details>
  <summary>Details</summary>
Motivation: There is a gap between current 3D generative methods and the needs of embodied AI: assets must be sim-ready with explicit geometry, articulation, and physical properties to be used in simulation and robotic control. Existing methods often produce static visuals or lack manipulable physics, hindering downstream tasks.

Method: Develop a vision-language model (VLM)-based physical 3D generator. Propose a new 3D geometry representation that significantly reduces token usage (193x fewer tokens) so explicit geometry learning fits within standard VLM token budgets without introducing special fine-tuning tokens. Build PhysX-Mobility to diversify physical 3D data and annotations. Validate on both the new dataset and real-world images, and perform simulation-based experiments (MuJoCo-style) to demonstrate direct usability for policy learning.

Result: The approach achieves strong generative performance and robust generalization to in-the-wild images and unseen categories. The sim-ready assets are directly usable for contact-rich robotic policy learning in simulation. Token efficiency enables geometry learning within standard VLM budgets, and PhysX-Mobility expands object categories and annotations significantly (over 2x categories, >2K objects).

Conclusion: PhysX-Anything delivers a pioneering, end-to-end pipeline from single images to sim-ready, articulated 3D assets with explicit physics attributes, enabling broad downstream applications in embodied AI and physics-based simulation. It sets a new direction for integrating physical reasoning into 3D generative models.

Abstract: 3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.

</details>


### [64] [BdSL-SPOTER: A Transformer-Based Framework for Bengali Sign Language Recognition with Cultural Adaptation](https://arxiv.org/abs/2511.12103)
*Sayad Ibna Azad,Md. Atiqur Rahman*

Main category: cs.CV

TL;DR: A compact pose-based transformer (BdSL-SPOTER) delivers state-of-the-art Bengali Sign Language recognition on BdSLW60 with high efficiency.


<details>
  <summary>Details</summary>
Motivation: To achieve accurate, efficient recognition for low-resource regional sign languages, enabling practical accessibility with low computational cost.

Method: BdSL-SPOTER extends SPOTER with a four-layer pose-based transformer encoder, learnable positional encodings, culture-specific preprocessing, and curriculum learning to improve generalization on limited data while maintaining low compute.

Result: On BdSLW60, 97.92% Top-1 validation accuracy, 22.82% above Bi-LSTM baseline, with lower FLOPs, fewer parameters, and higher FPS.

Conclusion: BdSL-SPOTER is a practical, scalable framework suitable for real-world accessibility and can be extended to other low-resource regional sign languages.

Abstract: We introduce BdSL-SPOTER, a pose-based transformer framework for accurate and efficient recognition of Bengali Sign Language (BdSL). BdSL-SPOTER extends the SPOTER paradigm with cultural specific preprocessing and a compact four-layer transformer encoder featuring optimized learnable positional encodings, while employing curriculum learning to enhance generalization on limited data and accelerate convergence. On the BdSLW60 benchmark, it achieves 97.92% Top-1 validation accuracy, representing a 22.82% improvement over the Bi-LSTM baseline, all while keeping computational costs low. With its reduced number of parameters, lower FLOPs, and higher FPS, BdSL-SPOTER provides a practical framework for real-world accessibility applications and serves as a scalable model for other low-resource regional sign languages.

</details>


### [65] [Scaling Spatial Intelligence with Multimodal Foundation Models](https://arxiv.org/abs/2511.13719)
*Zhongang Cai,Ruisi Wang,Chenyang Gu,Fanyi Pu,Junxiang Xu,Yubo Wang,Wanqi Yin,Zhitao Yang,Chen Wei,Qingping Sun,Tongxi Zhou,Jiaqi Li,Hui En Pang,Oscar Qian,Yukun Wei,Zhiqian Lin,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Xiangyu Fan,Hanming Deng,Lewei Lu,Liang Pan,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: Scaling multimodal foundation models with SenseNova-SI-8M to cultivate spatial intelligence, achieving strong benchmarks across VSI-Bench, MMSI, MindCube, ViewSpatial, and SITE, while analyzing data scaling, emergent generalization, and chain-of-thought capabilities, with public model releases.


<details>
  <summary>Details</summary>
Motivation: Despite progress in multimodal models, spatial intelligence remains deficient. This work aims to scale data and leverage established multimodal foundations to cultivate robust spatial reasoning and evaluation across diverse benchmarks.

Method: Construct SenseNova-SI-8M via eight million diverse data samples organized under a taxonomy of spatial capabilities. Build upon Qwen3-VL, InternVL3, and Bagel, integrating them for unified understanding and generation. Systematically evaluate on multiple spatial benchmarks (VSI-Bench, MMSI, MindCube, ViewSpatial, SITE) and general multimodal tasks (MMBench-En). Analyze data scaling effects, emergent generalization, overfitting and language shortcut risks, explore spatial chain-of-thought reasoning, and assess downstream applications. Publicly release trained models and data for research.

Result: SenseNova-SI achieves unprecedented performance across spatial benchmarks: 68.7% VSI-Bench, 43.3% MMSI, 85.6% MindCube, 54.6% ViewSpatial, 50.1% SITE, and 84.9% MMBench-En, while maintaining strong multimodal understanding.

Conclusion: SenseNova-SI is an ongoing project with promising scalability and emergent generalization signs. The work includes risk analyses (overfitting, shortcuts), a preliminary spatial chain-of-thought study, and validation of downstream potential. All newly trained models are released publicly to foster further research.

Abstract: Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.

</details>


### [66] [TEMPO: Global Temporal Building Density and Height Estimation from Satellite Imagery](https://arxiv.org/abs/2511.12104)
*Tammy Glazer,Gilles Q. Hacheme,Akram Zaytar,Luana Marotti,Amy Michaels,Girmaw Abebe Tadesse,Kevin White,Rahul Dodhia,Andrew Zolli,Inbal Becker-Reshef,Juan M. Lavista Ferres,Caleb Robinson*

Main category: cs.CV

TL;DR: TEMPO provides a global, temporally resolved dataset of building density and height from PlanetScope imagery via a multi-task deep learning model, delivering quarterly maps from 2018–2025 at 37.6 m/pixel with F1 ~85–88% and a 0.96 five-year trend-consistency, at lower computational cost than comparable methods.


<details>
  <summary>Details</summary>
Motivation: Global, timely urban information is essential for resilience, climate adaptation, and development planning, but existing datasets are static, regionally limited, or expensive to produce. TEMPO aims to enable scalable, cost-efficient, frequent monitoring of built settlements worldwide.

Method: Leverage existing building footprint and height data as priors, pair them with quarterly PlanetScope basemap imagery, train a multi-task deep learning model to predict building density and height at 37.6 m/pixel, apply to global basemaps from 2018 Q1 to 2025 Q2, and validate against hand-labeled footprints using F1 and trend-consistency metrics; demonstrate reduced computational cost compared with analogous approaches.

Result: The approach yields F1 scores between 85% and 88% across hand-labeled subsets and a 0.96 five-year trend-consistency score, indicating temporally stable estimates and sensitivity to quarterly settlement changes, with substantially lower computational requirements than comparable methods.

Conclusion: TEMPO enables large-scale, timely monitoring of urban development patterns and climate-relevant changes, supporting resilience and adaptation efforts at global scales, though further validation, higher-resolution outputs, and more diverse ground-truth data would strengthen generalizability and reliability.

Abstract: We present TEMPO, a global, temporally resolved dataset of building density and height derived from high-resolution satellite imagery using deep learning models. We pair building footprint and height data from existing datasets with quarterly PlanetScope basemap satellite images to train a multi-task deep learning model that predicts building density and building height at a 37.6-meter per pixel resolution. We apply this model to global PlanetScope basemaps from Q1 2018 through Q2 2025 to create global, temporal maps of building density and height. We validate these maps by comparing against existing building footprint datasets. Our estimates achieve an F1 score between 85% and 88% on different hand-labeled subsets, and are temporally stable, with a 0.96 five-year trend-consistency score. TEMPO captures quarterly changes in built settlements at a fraction of the computational cost of comparable approaches, unlocking large-scale monitoring of development patterns and climate impacts essential for global resilience and adaptation efforts.

</details>


### [67] [Fine-Grained DINO Tuning with Dual Supervision for Face Forgery Detection](https://arxiv.org/abs/2511.12107)
*Tianxiang Zhang,Peipeng Yu,Zhihua Xia,Longchen Dai,Xiaoyu Zhou,Hui Gao*

Main category: cs.CV

TL;DR: A parameter-efficient DINOv2 fine-tuning via DeepFake Fine-Grained Adapter (DFF-Adapter) with multi-head LoRA modules and a shared branch for joint authenticity and manipulation-type classification, achieving strong detection with 3.5M trainable params.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning approaches treat deepfake detection as generic binary classification and overlook artifacts unique to different forgery methods; there is a need for artifact-sensitive, multi-task adaptation to improve robustness and efficiency.

Method: Introduce lightweight multi-head LoRA adapters into every transformer block of DINOv2, enabling backbone adaptation. Add a shared branch that propagates fine-grained manipulation cues to the authenticity head, enabling multi-task cooperative optimization for both authenticity discrimination and manipulation-type classification.

Result: The approach achieves detection accuracy comparable to or surpassing state-of-the-art methods while using only 3.5M trainable parameters, demonstrating parameter efficiency.

Conclusion: A multi-task, artifact-aware fine-tuning framework with efficient adapters can enhance deepfake detection without large parameter increases, by leveraging manipulation-specific cues to support authenticity judgment.

Abstract: The proliferation of sophisticated deepfakes poses significant threats to information integrity. While DINOv2 shows promise for detection, existing fine-tuning approaches treat it as generic binary classification, overlooking distinct artifacts inherent to different deepfake methods. To address this, we propose a DeepFake Fine-Grained Adapter (DFF-Adapter) for DINOv2. Our method incorporates lightweight multi-head LoRA modules into every transformer block, enabling efficient backbone adaptation. DFF-Adapter simultaneously addresses authenticity detection and fine-grained manipulation type classification, where classifying forgery methods enhances artifact sensitivity. We introduce a shared branch propagating fine-grained manipulation cues to the authenticity head. This enables multi-task cooperative optimization, explicitly enhancing authenticity discrimination with manipulation-specific knowledge. Utilizing only 3.5M trainable parameters, our parameter-efficient approach achieves detection accuracy comparable to or even surpassing that of current complex state-of-the-art methods.

</details>


### [68] [MediRound: Multi-Round Entity-Level Reasoning Segmentation in Medical Images](https://arxiv.org/abs/2511.12110)
*Qinyue Tong,Ziqian Lu,Jun Liu,Rui Zuo,Zheming Lu*

Main category: cs.CV

TL;DR: Introduces MEMR-Seg for multi-round entity-level medical reasoning segmentation; MR-MedSeg dataset (177K dialogues); MediRound baseline; Judgment & Correction Mechanism to curb error propagation; outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Addresses lack of interactivity and multi-round reasoning in medical image segmentation; current text-prompt methods are single-round and task-specific.

Method: Define MEMR-Seg task; construct MR-MedSeg dataset with multi-round, entity-based reasoning; propose MediRound model; implement a lightweight Judgment & Correction Mechanism during inference to reduce error propagation in chain-like prompts.

Result: Experiments show MEMR-Seg method effectively addresses the task and beats conventional medical referring segmentation baselines.

Conclusion: MEMR-Seg establishes a new direction for interactive, multi-round medical segmentation with dataset and model; the judgment-correction approach enhances robustness against error accumulation.

Abstract: Despite the progress in medical image segmentation, most existing methods remain task-specific and lack interactivity. Although recent text-prompt-based segmentation approaches enhance user-driven and reasoning-based segmentation, they remain confined to single-round dialogues and fail to perform multi-round reasoning. In this work, we introduce Multi-Round Entity-Level Medical Reasoning Segmentation (MEMR-Seg), a new task that requires generating segmentation masks through multi-round queries with entity-level reasoning. To support this task, we construct MR-MedSeg, a large-scale dataset of 177K multi-round medical segmentation dialogues, featuring entity-based reasoning across rounds. Furthermore, we propose MediRound, an effective baseline model designed for multi-round medical reasoning segmentation. To mitigate the inherent error propagation in the chain-like pipeline of multi-round segmentation, we introduce a lightweight yet effective Judgment & Correction Mechanism during model inference. Experimental results demonstrate that our method effectively addresses the MEMR-Seg task and outperforms conventional medical referring segmentation methods.

</details>


### [69] [RadarMP: Motion Perception for 4D mmWave Radar in Autonomous Driving](https://arxiv.org/abs/2511.12117)
*Ruiqi Cheng,Huijun Di,Jian Li,Feng Liu,Wei Liang*

Main category: cs.CV

TL;DR: RadarMP jointly models radar detection and 3D scene flow from two consecutive frames using self-supervised losses guided by Doppler shifts and echo intensity, improving radar-based motion perception under adverse weather.


<details>
  <summary>Details</summary>
Motivation: 4D mmWave radar offers all-weather sensing but sparse/noisy points hamper accurate motion perception; existing methods separate detection and motion estimation, leading to suboptimal, decoupled pipelines; a unified approach could improve consistency and robustness.

Method: Proposes RadarMP, a unified architecture that jointly detects radar targets and predicts 3D scene flow using low-level radar echo signals from two frames. Employs self-supervised losses driven by Doppler shifts and echo intensity to enforce spatial and motion consistency without annotations.

Result: Experiments on a public dataset show that RadarMP achieves reliable motion perception across diverse weather/illumination conditions, outperforms radar-based decoupled pipelines, and enhances full-scenario autonomous driving perception.

Conclusion: A unified, self-supervised RadarMP approach can robustly infer 3D scene motion from radar data, mitigating limitations of sparse/noisy points and maintaining performance in adverse weather.

Abstract: Accurate 3D scene motion perception significantly enhances the safety and reliability of an autonomous driving system. Benefiting from its all-weather operational capability and unique perceptual properties, 4D mmWave radar has emerged as an essential component in advanced autonomous driving. However, sparse and noisy radar points often lead to imprecise motion perception, leaving autonomous vehicles with limited sensing capabilities when optical sensors degrade under adverse weather conditions. In this paper, we propose RadarMP, a novel method for precise 3D scene motion perception using low-level radar echo signals from two consecutive frames. Unlike existing methods that separate radar target detection and motion estimation, RadarMP jointly models both tasks in a unified architecture, enabling consistent radar point cloud generation and pointwise 3D scene flow prediction. Tailored to radar characteristics, we design specialized self-supervised loss functions guided by Doppler shifts and echo intensity, effectively supervising spatial and motion consistency without explicit annotations. Extensive experiments on the public dataset demonstrate that RadarMP achieves reliable motion perception across diverse weather and illumination conditions, outperforming radar-based decoupled motion perception pipelines and enhancing perception capabilities for full-scenario autonomous driving systems.

</details>


### [70] [OAD-Promoter: Enhancing Zero-shot VQA using Large Language Models with Object Attribute Description](https://arxiv.org/abs/2511.12131)
*Quanxing Xu,Ling Zhou,Feifei Zhang,Jinyu Tian,Rubing Huang*

Main category: cs.CV

TL;DR: Proposes object-focused description promoter (OAD-Promoter) for LLM-based VQA to reduce language bias and improve robustness to domain shift via three modules: (1) Object-concentrated Example Generation (OEG) for global and object-centric cues, (2) Memory Knowledge Assistance (MKA) for retrieving relevant knowledge, and (3) the OAD Prompt that integrates outputs to optimize inference, achieving state-of-the-art few-shot/zero-shot results.


<details>
  <summary>Details</summary>
Motivation: LLMs trained on large datasets inherit language biases, which can be exploited by models and reduce reliability. Additionally, LLMs often struggle with out-of-distribution (OOD) generalization in knowledge-intensive VQA tasks. There is a need to mitigate language bias and enhance domain-shift robustness in LLM-based VQA.

Method: Introduce the OAD-Promoter framework with three components. (1) Object-concentrated Example Generation (OEG) generates global captions and object-focused samples to enrich visual input and balance cues, addressing bias by combining global and regional visual information. (2) Memory Knowledge Assistance (MKA) retrieves relevant knowledge from stored examples to support LLM answers for queries from unseen domains (OOD scenarios). (3) OAD Prompt aggregates outputs from OEG and MKA and guides the LLM inference for final answers.

Result: Empirical evaluation shows significant improvements of LLM-based VQA methods in few-shot and zero-shot settings, achieving new state-of-the-art performance.

Conclusion: OAD-Promoter effectively mitigates language bias and enhances domain-shift robustness in LLM-based VQA by combining targeted data augmentation, knowledge retrieval, and integrated prompting, indicating strong potential for knowledge-intensive VQA under limited supervision.

Abstract: Large Language Models (LLMs) have become a crucial tool in Visual Question Answering (VQA) for handling knowledge-intensive questions in few-shot or zero-shot scenarios. However, their reliance on massive training datasets often causes them to inherit language biases during the acquisition of knowledge. This limitation imposes two key constraints on existing methods: (1) LLM predictions become less reliable due to bias exploitation, and (2) despite strong knowledge reasoning capabilities, LLMs still struggle with out-of-distribution (OOD) generalization. To address these issues, we propose Object Attribute Description Promoter (OAD-Promoter), a novel approach for enhancing LLM-based VQA by mitigating language bias and improving domain-shift robustness. OAD-Promoter comprises three components: the Object-concentrated Example Generation (OEG) module, the Memory Knowledge Assistance (MKA) module, and the OAD Prompt. The OEG module generates global captions and object-concentrated samples, jointly enhancing visual information input to the LLM and mitigating bias through complementary global and regional visual cues. The MKA module assists the LLM in handling OOD samples by retrieving relevant knowledge from stored examples to support questions from unseen domains. Finally, the OAD Prompt integrates the outputs of the preceding modules to optimize LLM inference. Experiments demonstrate that OAD-Promoter significantly improves the performance of LLM-based VQA methods in few-shot or zero-shot settings, achieving new state-of-the-art results.

</details>


### [71] [Compression and Inference of Spiking Neural Networks on Resource-Constrained Hardware](https://arxiv.org/abs/2511.12136)
*Karol C. Jurzec,Tomasz Szydlo,Maciej Wielgosz*

Main category: cs.CV

TL;DR: A lightweight C runtime enables efficient edge SNN inference by converting SNNTorch models to compact C with static layouts, preallocation, and spike-driven pruning; achieves ~10x CPU speedups and enables microcontroller deployment while maintaining parity with Python baselines.


<details>
  <summary>Details</summary>
Motivation: SNNs offer energy-efficient, event-driven processing but suffer from training/inference overheads and deployment challenges on constrained hardware. A compact runtime could unlock practical edge deployment.

Method: Translate SNNTorch-exported models into a compact C representation with static, cache-friendly data layouts and preallocated buffers; prune inactive neurons/synapses leveraging sparse spiking activity; optimize upstream convolutional layers; evaluate on N-MNIST and ST-MNIST comparing to Python baseline; demonstrate CPU speedups and memory reduction; deployable on devices like Arduino Portenta H7.

Result: Functional parity with the Python baseline on N-MNIST/ST-MNIST; ~10x speedup on desktop CPU; additional gains from pruning; substantial memory reductions enabling microcontroller deployment.

Conclusion: An optimized C runtime with spike-driven model compression enables efficient SNN execution on conventional embedded platforms, facilitating edge deployment of SNNs.

Abstract: Spiking neural networks (SNNs) communicate via discrete spikes in time rather than continuous activations. Their event-driven nature offers advantages for temporal processing and energy efficiency on resource-constrained hardware, but training and deployment remain challenging. We present a lightweight C-based runtime for SNN inference on edge devices and optimizations that reduce latency and memory without sacrificing accuracy. Trained models exported from SNNTorch are translated to a compact C representation; static, cache-friendly data layouts and preallocation avoid interpreter and allocation overheads. We further exploit sparse spiking activity to prune inactive neurons and synapses, shrinking computation in upstream convolutional layers. Experiments on N-MNIST and ST-MNIST show functional parity with the Python baseline while achieving ~10 speedups on desktop CPU and additional gains with pruning, together with large memory reductions that enable microcontroller deployment (Arduino Portenta H7). Results indicate that SNNs can be executed efficiently on conventional embedded platforms when paired with an optimized runtime and spike-driven model compression. Code: https://github.com/karol-jurzec/snn-generator/

</details>


### [72] [MAVIS: A Benchmark for Multimodal Source Attribution in Long-form Visual Question Answering](https://arxiv.org/abs/2511.12142)
*Seokwon Song,Minsu Park,Gunhee Kim*

Main category: cs.CV

TL;DR: MAVIS is a first multimodal source-attribution benchmark for visual question answering with 157k instances, showing that multimodal LVLMs with RAG deliver more informative and fluent answers but weaker groundedness for image documents; reveals a trade-off between informativeness and groundedness and highlights contextual bias in image interpretation; dataset and code released.


<details>
  <summary>Details</summary>
Motivation: To address the gap in multimodal source attribution research, which has mostly focused on text-only content. There is a need to align AI-generated answers with user intent, retrieve multimodal evidence, and provide long-form, citationally grounded responses.

Method: Introduce MAVIS, a benchmark comprising 157k visual QA instances where each answer is annotated with fact-level citations referencing multimodal documents. Develop fine-grained automatic metrics along informativeness, groundedness, and fluency, shown to correlate with human judgments. Evaluate LVLMs with multimodal retrieval-augmented generation (RAG), analyze prompting methods, and study biases in image interpretation.

Result: Three main findings: (1) Multimodal LVLMs with RAG yield more informative and fluent answers than unimodal counterparts, but have weaker groundedness for image documents, a gap amplified in multimodal settings. (2) For the same multimodal documents, there is a trade-off between informativeness and groundedness across prompting methods. (3) Contextual bias in interpreting image documents is a crucial direction for future work. The MAVIS dataset and experimental code are available at the linked repository.

Conclusion: Grounded, multimodal source attribution remains challenging; MAVIS provides a benchmark and tools to study informativeness, groundedness, and fluency, guiding future research toward reducing contextual bias and improving evidence grounding in multimodal AI systems.

Abstract: Source attribution aims to enhance the reliability of AI-generated answers by including references for each statement, helping users validate the provided answers. However, existing work has primarily focused on text-only scenario and largely overlooked the role of multimodality. We introduce MAVIS, the first benchmark designed to evaluate multimodal source attribution systems that understand user intent behind visual questions, retrieve multimodal evidence, and generate long-form answers with citations. Our dataset comprises 157K visual QA instances, where each answer is annotated with fact-level citations referring to multimodal documents. We develop fine-grained automatic metrics along three dimensions of informativeness, groundedness, and fluency, and demonstrate their strong correlation with human judgments. Our key findings are threefold: (1) LVLMs with multimodal RAG generate more informative and fluent answers than unimodal RAG, but they exhibit weaker groundedness for image documents than for text documents, a gap amplified in multimodal settings. (2) Given the same multimodal documents, there is a trade-off between informativeness and groundedness across different prompting methods. (3) Our proposed method highlights mitigating contextual bias in interpreting image documents as a crucial direction for future research. The dataset and experimental code are available at https://github.com/seokwon99/MAVIS

</details>


### [73] [Breaking the Modality Wall: Time-step Mixup for Efficient Spiking Knowledge Transfer from Static to Event Domain](https://arxiv.org/abs/2511.12150)
*Yuqi Xie,Shuhan Ye,Yi Yu,Chong Wang,Qixin Zhang,Jiazhen Xu,Le Shen,Yuanbin Qian,Jiangbo Qian,Guoqi Li*

Main category: cs.CV

TL;DR: TMKT is a cross-modal training framework for DVS-SNNs that uses Time-step Mixup (TSM) to interpolate RGB and DVS inputs across time steps, with two lightweight modality-aware losses (MAG and MRP) to align temporal features and mix ratios, yielding superior spiking image classification performance.


<details>
  <summary>Details</summary>
Motivation: RGB-to-DVS knowledge transfer often underperforms due to large modality distribution gaps and scarce DVS data; training is hindered by gradient variance and a mismatch between modalities. A smoother, curriculum-like cross-modal training strategy is needed to stabilize optimization and improve transfer.

Method: Introduce Time-step Mixup (TSM) that interpolates RGB and DVS inputs at various SNN time steps to create a smooth curriculum within each sequence. Add two lightweight objectives: Modality Aware Guidance (MAG) for per-frame supervision from the source modality and Mixup Ratio Perception (MRP) to estimate the sequence-level mix ratio, ensuring temporal features align with the mixing schedule. Train with Time-step Mixup Knowledge Transfer (TMKT) to facilitate cross-modal transfer.

Result: TMKT achieves superior performance on spiking image classification across diverse benchmarks and SNN backbones; extensive ablations validate the components and demonstrate smoother knowledge transfer and reduced modality mismatch.

Conclusion: TMKT provides a stable, end-to-end framework for cross-modal RGB-to-DVS knowledge transfer in SNNs, leveraging a time-step mixup curriculum and modality-aware auxiliary losses to improve training stability and classification accuracy.

Abstract: The integration of event cameras and spiking neural networks (SNNs) promises energy-efficient visual intelligence, yet scarce event data and the sparsity of DVS outputs hinder effective training. Prior knowledge transfers from RGB to DVS often underperform because the distribution gap between modalities is substantial. In this work, we present Time-step Mixup Knowledge Transfer (TMKT), a cross-modal training framework with a probabilistic Time-step Mixup (TSM) strategy. TSM exploits the asynchronous nature of SNNs by interpolating RGB and DVS inputs at various time steps to produce a smooth curriculum within each sequence, which reduces gradient variance and stabilizes optimization with theoretical analysis. To employ auxiliary supervision from TSM, TMKT introduces two lightweight modality-aware objectives, Modality Aware Guidance (MAG) for per-frame source supervision and Mixup Ratio Perception (MRP) for sequence-level mix ratio estimation, which explicitly align temporal features with the mixing schedule. TMKT enables smoother knowledge transfer, helps mitigate modality mismatch during training, and achieves superior performance in spiking image classification tasks. Extensive experiments across diverse benchmarks and multiple SNN backbones, together with ablations, demonstrate the effectiveness of our method.

</details>


### [74] [FIA-Edit: Frequency-Interactive Attention for Efficient and High-Fidelity Inversion-Free Text-Guided Image Editing](https://arxiv.org/abs/2511.12151)
*Kaixiang Yang,Boyang Shen,Xin Li,Yuchen Dai,Yuxuan Luo,Yueran Ma,Wei Fang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: FIA-Edit is a fast, inversion-free diffusion editing framework that preserves source content while enabling semantically precise edits via Frequency Representation Interaction and a Feature Injection mechanism, achieving high visual quality at low compute and extending to medical image augmentation.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of inversion-free flow-based editing methods, which struggle to preserve background content and structure due to weak integration of source information, leading to artifacts and over-editing.

Method: Introduce two components: (1) Frequency Representation Interaction (FRI) to exchange frequency components between source and target features within self-attention for better cross-domain alignment, and (2) Feature Injection (FIJ) to inject source-side queries, keys, values, and text embeddings into the target branch's cross-attention to preserve structure and semantics.

Result: Achieves high-fidelity editing at ~6 seconds per 512x512 image on an RTX 4090; outperforms existing methods in visual quality, background fidelity, and controllability across diverse tasks; extends to clinical applications by synthesizing anatomically coherent hemorrhage variations for medical data augmentation and gains in downstream bleeding classification.

Conclusion: FIA-Edit provides a fast, inversion-free, semantically precise editing framework that preserves source content while enabling high-quality edits, with proven applicability to medical image augmentation and potential broader impact.

Abstract: Text-guided image editing has advanced rapidly with the rise of diffusion models. While flow-based inversion-free methods offer high efficiency by avoiding latent inversion, they often fail to effectively integrate source information, leading to poor background preservation, spatial inconsistencies, and over-editing due to the lack of effective integration of source information. In this paper, we present FIA-Edit, a novel inversion-free framework that achieves high-fidelity and semantically precise edits through a Frequency-Interactive Attention. Specifically, we design two key components: (1) a Frequency Representation Interaction (FRI) module that enhances cross-domain alignment by exchanging frequency components between source and target features within self-attention, and (2) a Feature Injection (FIJ) module that explicitly incorporates source-side queries, keys, values, and text embeddings into the target branch's cross-attention to preserve structure and semantics. Comprehensive and extensive experiments demonstrate that FIA-Edit supports high-fidelity editing at low computational cost (~6s per 512 * 512 image on an RTX 4090) and consistently outperforms existing methods across diverse tasks in visual quality, background fidelity, and controllability. Furthermore, we are the first to extend text-guided image editing to clinical applications. By synthesizing anatomically coherent hemorrhage variations in surgical images, FIA-Edit opens new opportunities for medical data augmentation and delivers significant gains in downstream bleeding classification. Our project is available at: https://github.com/kk42yy/FIA-Edit.

</details>


### [75] [Codebook-Centric Deep Hashing: End-to-End Joint Learning of Semantic Hash Centers and Neural Hash Function](https://arxiv.org/abs/2511.12162)
*Shuo Yin,Zhiyuan Yin,Yuqing Hou,Rui Liu,Yong Chen,Dell Zhang*

Main category: cs.CV

TL;DR: End-to-end deep hashing with dynamic center reassignment and multi-head centers, aligning hash centers to data semantics without explicit center optimization phases, yielding superior retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Randomly initialized hash centers overlook inter-class semantic relationships; two-stage center refinement adds complexity and can be suboptimal due to stage misalignment—calling for an end-to-end solution that integrates semantic structure into center learning.

Method: Dynamically reassigns hash centers from a preset codebook during joint optimization of the hash function, eliminating explicit center-optimization stages; introduces a multi-head mechanism to enrich semantic representation.

Result: On three benchmarks, CRH learns semantically meaningful hash centers and outperforms state-of-the-art deep hashing methods in retrieval tasks.

Conclusion: CRH offers a simpler, end-to-end framework that better integrates semantic relationships into hashing, improves retrieval performance, and benefits from dynamic center reassignment plus multi-head design.

Abstract: Hash center-based deep hashing methods improve upon pairwise or triplet-based approaches by assigning fixed hash centers to each class as learning targets, thereby avoiding the inefficiency of local similarity optimization. However, random center initialization often disregards inter-class semantic relationships. While existing two-stage methods mitigate this by first refining hash centers with semantics and then training the hash function, they introduce additional complexity, computational overhead, and suboptimal performance due to stage-wise discrepancies. To address these limitations, we propose $\textbf{Center-Reassigned Hashing (CRH)}$, an end-to-end framework that $\textbf{dynamically reassigns hash centers}$ from a preset codebook while jointly optimizing the hash function. Unlike previous methods, CRH adapts hash centers to the data distribution $\textbf{without explicit center optimization phases}$, enabling seamless integration of semantic relationships into the learning process. Furthermore, $\textbf{a multi-head mechanism}$ enhances the representational capacity of hash centers, capturing richer semantic structures. Extensive experiments on three benchmarks demonstrate that CRH learns semantically meaningful hash centers and outperforms state-of-the-art deep hashing methods in retrieval tasks.

</details>


### [76] [Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective](https://arxiv.org/abs/2511.12170)
*Wang Luo,Di Wu,Hengyuan Na,Yinlin Zhu,Miao Hu,Guocong Quan*

Main category: cs.CV

TL;DR: Proposes Completion-by-Correction with PGNet to achieve structurally consistent, observation-aligned point cloud completion using a topologically complete prior and hierarchical feature-space refinement; outperforms inpainting-based baselines.


<details>
  <summary>Details</summary>
Motivation: Address structural inconsistencies and topological artifacts of Completion-by-Inpainting by starting from a complete shape prior and refining to fit partial observations.

Method: Introduce Completion-by-Correction paradigm. Build PGNet with dual-feature encoding to ground a pretrained image-to-3D prior, generate a coarse scaffold, and progressively refine geometry via hierarchical correction across multiple stages.

Result: On ShapeNetViPC, PGNet reduces average Chamfer Distance by 23.5% and increases F-score by 7.1% compared with state-of-the-art baselines, demonstrating superior structural alignment and detail recovery.

Conclusion: The Completion-by-Correction framework provides a robust, observation-aligned alternative to unconstrained inpainting for point cloud completion, with PGNet delivering structurally consistent reconstructions guided by a complete prior.

Abstract: Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).

</details>


### [77] [MixAR: Mixture Autoregressive Image Generation](https://arxiv.org/abs/2511.12181)
*Jinyuan Hu,Jiayou Zhang,Shaobo Cui,Kun Zhang,Guangyi Chen*

Main category: cs.CV

TL;DR: MixAR introduces a discrete-token prior to guide continuous autoregressive image modeling, proposing DC-SA, DC-CA, DC-Mix, and TI-Mix. DC-Mix balances efficiency and fidelity, while TI-Mix consistently improves generation.


<details>
  <summary>Details</summary>
Motivation: Discrete codebooks in autoregressive image models limit fidelity due to quantization, while continuous latent AR offers higher quality but faces challenges modeling in unstructured spaces; the work seeks to fuse discrete guidance with continuous AR for better generation.

Method: Propose MixAR, a factorized framework where discrete tokens serve as priors for continuous autoregressive prediction. Explore strategies: self-attention (DC-SA), cross-attention (DC-CA), and a simple DC-Mix using informative discrete tokens. Introduce Training-Inference Mixture (TI-Mix) to align training and inference distributions between ground-truth tokens and model-generated tokens.

Result: Empirical results show DC-Mix provides a favorable balance between computational efficiency and generation fidelity, and TI-Mix yields consistent improvements across experiments.

Conclusion: MixAR demonstrates that discrete priors can effectively guide continuous autoregressive generation; discrete-continuous mixture strategies mitigates the fidelity-efficiency trade-off, with TI-Mix improving training-inference consistency.

Abstract: Autoregressive (AR) approaches, which represent images as sequences of discrete tokens from a finite codebook, have achieved remarkable success in image generation. However, the quantization process and the limited codebook size inevitably discard fine-grained information, placing bottlenecks on fidelity. Motivated by this limitation, recent studies have explored autoregressive modeling in continuous latent spaces, which offers higher generation quality. Yet, unlike discrete tokens constrained by a fixed codebook, continuous representations lie in a vast and unstructured space, posing significant challenges for efficient autoregressive modeling. To address these challenges, we introduce MixAR, a novel framework that leverages mixture training paradigms to inject discrete tokens as prior guidance for continuous AR modeling. MixAR is a factorized formulation that leverages discrete tokens as prior guidance for continuous autoregressive prediction. We investigate several discrete-continuous mixture strategies, including self-attention (DC-SA), cross-attention (DC-CA), and a simple approach (DC-Mix) that replaces homogeneous mask tokens with informative discrete counterparts. Moreover, to bridge the gap between ground-truth training tokens and inference tokens produced by the pre-trained AR model, we propose Training-Inference Mixture (TI-Mix) to achieve consistent training and generation distributions. In our experiments, we demonstrate a favorable balance of the DC-Mix strategy between computational efficiency and generation fidelity, and consistent improvement of TI-Mix.

</details>


### [78] [MMRINet: Efficient Mamba-Based Segmentation with Dual-Path Refinement for Low-Resource MRI Analysis](https://arxiv.org/abs/2511.12193)
*Abdelrahman Elsayed,Ahmed Jaheen,Mohammad Yaqub*

Main category: cs.CV

TL;DR: MMRINet is a lightweight 3D brain tumor segmentation model using linear-attention via Mamba state-space models, featuring DPFR and PFA, achieving a Dice score of 0.752 and HD95 of 12.23 with ~2.5M parameters on BraTS-Lighthouse SSA 2025; code at github.com/BioMedIA-MBZUAI/MMRINet.


<details>
  <summary>Details</summary>
Motivation: Efficient automated brain tumor segmentation in resource-constrained settings where heavy 3D networks and quadratic attention are impractical; address computational and memory constraints.

Method: Propose MMRINet architecture that replaces quadratic attention with linear Mamba state-space models for volumetric context; introduce Dual-Path Feature Refinement (DPFR) modules to maximize feature diversity without extra data; use Progressive Feature Aggregation (PFA) for effective multi-scale fusion; evaluate on BraTS-Lighthouse SSA 2025 with ~2.5M parameters.

Result: Achieves average Dice of 0.752 and average HD95 of 12.23; model has ~2.5M parameters; GitHub repository provided (GitHub link).

Conclusion: Demonstrates that efficient, accurate brain tumor segmentation is feasible in low-resource clinical environments using linear-attention models and lightweight architectural refinements.

Abstract: Automated brain tumor segmentation in multi-parametric MRI remains challenging in resource-constrained settings where deep 3D networks are computationally prohibitive. We propose MMRINet, a lightweight architecture that replaces quadratic-complexity attention with linear-complexity Mamba state-space models for efficient volumetric context modeling. Novel Dual-Path Feature Refinement (DPFR) modules maximize feature diversity without additional data requirements, while Progressive Feature Aggregation (PFA) enables effective multi-scale fusion. In the BraTS-Lighthouse SSA 2025, our model achieves strong performance with an average Dice score of (0.752) and an average HD95 of (12.23) with only ~2.5M parameters, demonstrating efficient and accurate segmentation suitable for low-resource clinical environments. Our GitHub repository can be accessed here: github.com/BioMedIA-MBZUAI/MMRINet.

</details>


### [79] [Cross-View Cross-Modal Unsupervised Domain Adaptation for Driver Monitoring System](https://arxiv.org/abs/2511.12196)
*Aditi Bhalla,Christian Hellert,Enkelejda Kasneci*

Main category: cs.CV

TL;DR: A two-phase cross-view, cross-modal unsupervised domain adaptation framework for real-time driver monitoring that learns view-invariant features within a modality via contrastive learning and then adapts to a new modality with an information bottleneck loss, achieving substantial gains on RGB video across Video Swin/MViT backbones on the Drive&Act dataset.


<details>
  <summary>Details</summary>
Motivation: Driver distraction causes fatalities globally; deployment is hindered by variations in camera viewpoints (cross-view) and domain shifts (e.g., modality and environment). Existing methods address only one challenge at a time, limiting robust, scalable deployment across diverse vehicle configurations.

Method: Phase 1: learn view-invariant and action-discriminative features within a single modality using contrastive learning on multi-view data. Phase 2: perform unsupervised domain adaptation to a new modality using information bottleneck loss, with no labeled data in the new domain. Evaluation uses Video Swin and MViT backbones on the Drive&Act driver activity dataset.

Result: Top-1 accuracy on RGB video data improves by ~50% over a supervised contrastive learning-based cross-view method; it also outperforms unsupervised domain adaptation-only methods by up to 5%, using the same video transformer backbone.

Conclusion: The proposed joint cross-view, cross-modal unsupervised domain adaptation yields more robust and scalable driver-monitoring performance across diverse vehicle configurations, enabling better deployment in real-world scenarios.

Abstract: Driver distraction remains a leading cause of road traffic accidents, contributing to thousands of fatalities annually across the globe. While deep learning-based driver activity recognition methods have shown promise in detecting such distractions, their effectiveness in real-world deployments is hindered by two critical challenges: variations in camera viewpoints (cross-view) and domain shifts such as change in sensor modality or environment. Existing methods typically address either cross-view generalization or unsupervised domain adaptation in isolation, leaving a gap in the robust and scalable deployment of models across diverse vehicle configurations. In this work, we propose a novel two-phase cross-view, cross-modal unsupervised domain adaptation framework that addresses these challenges jointly on real-time driver monitoring data. In the first phase, we learn view-invariant and action-discriminative features within a single modality using contrastive learning on multi-view data. In the second phase, we perform domain adaptation to a new modality using information bottleneck loss without requiring any labeled data from the new domain. We evaluate our approach using state-of-the art video transformers (Video Swin, MViT) and multi modal driver activity dataset called Drive&Act, demonstrating that our joint framework improves top-1 accuracy on RGB video data by almost 50% compared to a supervised contrastive learning-based cross-view method, and outperforms unsupervised domain adaptation-only methods by up to 5%, using the same video transformer backbone.

</details>


### [80] [Bridging Granularity Gaps: Hierarchical Semantic Learning for Cross-domain Few-shot Segmentation](https://arxiv.org/abs/2511.12200)
*Sujun Sun,Haowen Gu,Cheng Xie,Yanxu Ren,Mingwu Ren,Haofeng Zhang*

Main category: cs.CV

TL;DR: Proposes Hierarchical Semantic Learning (HSL) for Cross-domain Few-shot Segmentation to address both domain style gaps and segmentation granularity gaps, introducing Dual Style Randomization (DSR), Hierarchical Semantic Mining (HSM), and Prototype Confidence-modulated Thresholding (PCMT); achieves state-of-the-art results on four target-domain datasets.


<details>
  <summary>Details</summary>
Motivation: Current CD-FSS methods emphasize domain style gaps but neglect segmentation granularity gaps, leading to insufficient semantic discriminability for novel classes in the target domain.

Method: Integrates DSR to simulate diverse foreground-background and overall style variations; uses HSM with multi-scale superpixels to enforce intra-class consistency and inter-class distinction across granularities; employs PCMT to reduce segmentation ambiguity when foreground/background are similar.

Result: Extensive experiments on four target-domain datasets show state-of-the-art performance, validating the effectiveness of the hierarchical approach and the proposed modules.

Conclusion: The HSL framework effectively tackles both style- and granularity-related challenges in CD-FSS, improving semantic discriminability for novel classes and achieving strong cross-domain segmentation performance.

Abstract: Cross-domain Few-shot Segmentation (CD-FSS) aims to segment novel classes from target domains that are not involved in training and have significantly different data distributions from the source domain, using only a few annotated samples, and recent years have witnessed significant progress on this task. However, existing CD-FSS methods primarily focus on style gaps between source and target domains while ignoring segmentation granularity gaps, resulting in insufficient semantic discriminability for novel classes in target domains. Therefore, we propose a Hierarchical Semantic Learning (HSL) framework to tackle this problem. Specifically, we introduce a Dual Style Randomization (DSR) module and a Hierarchical Semantic Mining (HSM) module to learn hierarchical semantic features, thereby enhancing the model's ability to recognize semantics at varying granularities. DSR simulates target domain data with diverse foreground-background style differences and overall style variations through foreground and global style randomization respectively, while HSM leverages multi-scale superpixels to guide the model to mine intra-class consistency and inter-class distinction at different granularities. Additionally, we also propose a Prototype Confidence-modulated Thresholding (PCMT) module to mitigate segmentation ambiguity when foreground and background are excessively similar. Extensive experiments are conducted on four popular target domain datasets, and the results demonstrate that our method achieves state-of-the-art performance.

</details>


### [81] [OmniSparse: Training-Aware Fine-Grained Sparse Attention for Long-Video MLLMs](https://arxiv.org/abs/2511.12201)
*Feng Chen,Yefei He,Shaoxuan He,Yuanyu He,Jing Liu,Lequan Lin,Akide Liu,Zhaoyang Li,Jiyuan Zhang,Zhenbang Sun,Bohan Zhuang,Qi Wu*

Main category: cs.CV

TL;DR: OmniSparse is a training-aware, fine-grained sparse attention framework for long-video MLLMs that dynamically allocates token budgets across queries, keys/values, and heads, achieving full-attention performance with substantial training and inference speedups and memory reductions.


<details>
  <summary>Details</summary>
Motivation: Existing sparse attention methods mainly target inference-time acceleration using preset sparsity patterns, but they often fail to close the training–inference gap and lack fine-grained, cross-dimension token selection (queries, KV, heads), limiting performance and acceleration benefits.

Method: Introduce OmniSparse with three adaptive mechanisms: (1) lazy-active classification for query selection to keep semantically broad active queries and discard redundant lazy ones; (2) head-level dynamic budget allocation for KV selection, using a shared budget based on the flattest head across all heads to preserve attention recall; (3) KV cache slimming to reduce head-level redundancy by selectively fetching visual KV cache according to the decoding query pattern.

Result: Empirical results indicate OmniSparse matches full attention performance while delivering up to 2.7x speedup during prefill and 2.4x memory reduction during decoding for long-video MLLMs.

Conclusion: A training-aware, fine-grained, dynamic-budget sparse attention framework that operates in training and inference, achieving full-attention-equivalent performance with significant speed/memory gains and improved scalability for long-video multi-modal language models.

Abstract: Existing sparse attention methods primarily target inference-time acceleration by selecting critical tokens under predefined sparsity patterns. However, they often fail to bridge the training-inference gap and lack the capacity for fine-grained token selection across multiple dimensions such as queries, key-values (KV), and heads, leading to suboptimal performance and limited acceleration gains. In this paper, we introduce OmniSparse, a training-aware fine-grained sparse attention framework for long-video MLLMs, which operates in both training and inference with dynamic token budget allocation. Specifically, OmniSparse contains three adaptive and complementary mechanisms: (1) query selection via lazy-active classification, retaining active queries that capture broad semantic similarity while discarding most lazy ones that focus on limited local context and exhibit high functional redundancy; (2) KV selection with head-level dynamic budget allocation, where a shared budget is determined based on the flattest head and applied uniformly across all heads to ensure attention recall; and (3) KV cache slimming to reduce head-level redundancy by selectively fetching visual KV cache according to the head-level decoding query pattern. Experimental results show that OmniSparse matches the performance of full attention while achieving up to 2.7x speedup during prefill and 2.4x memory reduction during decoding.

</details>


### [82] [LSS3D: Learnable Spatial Shifting for Consistent and High-Quality 3D Generation from Single-Image](https://arxiv.org/abs/2511.12202)
*Zhuojiang Cai,Yiheng Zhang,Meitong Guo,Mingdao Wang,Yuwang Wang*

Main category: cs.CV

TL;DR: LSS3D introduces learnable per-view spatial shifts to align multi-view inputs in image-to-3D generation, improving geometry and texture across non-frontal viewpoints; includes an extra input-view constraint and a quantitative evaluation pipeline.


<details>
  <summary>Details</summary>
Motivation: To address misalignment and robustness issues in existing diffusion-based multi-view 3D generation that produce incomplete geometry and texture ghosting, especially under oblique views.

Method: Assign learnable spatial shifting parameters to each view; steer views toward a spatially consistent target guided by reconstructed mesh; include the input view as an additional constraint; provide a comprehensive evaluation pipeline.

Result: Extensive experiments show leading performance in geometric and texture metrics across flexible input viewpoints; improved geometry details and clean textures; robust to non-frontal inputs.

Conclusion: LSS3D effectively tackles multiview inconsistencies and non-frontal inputs, offering a robust high-quality 3D generation method and contributing a community-facing evaluation pipeline.

Abstract: Recently, multi-view diffusion-based 3D generation methods have gained significant attention. However, these methods often suffer from shape and texture misalignment across generated multi-view images, leading to low-quality 3D generation results, such as incomplete geometric details and textural ghosting. Some methods are mainly optimized for the frontal perspective and exhibit poor robustness to oblique perspective inputs. In this paper, to tackle the above challenges, we propose a high-quality image-to-3D approach, named LSS3D, with learnable spatial shifting to explicitly and effectively handle the multiview inconsistencies and non-frontal input view. Specifically, we assign learnable spatial shifting parameters to each view, and adjust each view towards a spatially consistent target, guided by the reconstructed mesh, resulting in high-quality 3D generation with more complete geometric details and clean textures. Besides, we include the input view as an extra constraint for the optimization, further enhancing robustness to non-frontal input angles, especially for elevated viewpoint inputs. We also provide a comprehensive quantitative evaluation pipeline that can contribute to the community in performance comparisons. Extensive experiments demonstrate that our method consistently achieves leading results in both geometric and texture evaluation metrics across more flexible input viewpoints.

</details>


### [83] [GeoMVD: Geometry-Enhanced Multi-View Generation Model Based on Geometric Information Extraction](https://arxiv.org/abs/2511.12204)
*Jiaqi Wu,Yaosen Chen,Shuyuan Zhu*

Main category: cs.CV

TL;DR: GeoMVD is a diffusion-based framework for multi-view image generation guided by explicit geometry, improving cross-view consistency and detail via a geometry extraction module, decoupled geometry-enhanced attention, adaptive learning, iterative refinement, and dynamic geometry intensity control.


<details>
  <summary>Details</summary>
Motivation: Cross-view consistency and high-resolution generation are difficult when extending single-view diffusion methods. Incorporating multi-view geometric cues (depth, normals, masks) can enforce shape coherence and enrich details across views.

Method: 1) A multi-view geometry information extraction module uses depth maps, normal maps, and foreground masks to build a shared geometric structure. 2) A decoupled geometry-enhanced attention mechanism strengthens focus on key geometric details to enhance consistency and detail. 3) An adaptive learning strategy fine-tunes the model to capture spatial relationships and visual coherence between views. 4) An iterative refinement process progressively improves outputs across multiple generation stages. 5) A dynamic geometry information intensity adjustment mechanism adaptively regulates the influence of geometric data to balance quality and naturalness.

Result: The approach is claimed to improve cross-view consistency and image quality, preserving structural details across views with richer textures, though quantitative metrics are not provided in the abstract. The method is positioned to benefit applications like 3D reconstruction, VR, and AR.

Conclusion: GeoMVD presents a geometry-guided diffusion framework for robust multi-view image generation, offering adjustable geometric influence and iterative refinement to achieve consistent, high-quality results. The project page provides implementation details and results.

Abstract: Multi-view image generation holds significant application value in computer vision, particularly in domains like 3D reconstruction, virtual reality, and augmented reality. Most existing methods, which rely on extending single images, face notable computational challenges in maintaining cross-view consistency and generating high-resolution outputs. To address these issues, we propose the Geometry-guided Multi-View Diffusion Model, which incorporates mechanisms for extracting multi-view geometric information and adjusting the intensity of geometric features to generate images that are both consistent across views and rich in detail. Specifically, we design a multi-view geometry information extraction module that leverages depth maps, normal maps, and foreground segmentation masks to construct a shared geometric structure, ensuring shape and structural consistency across different views. To enhance consistency and detail restoration during generation, we develop a decoupled geometry-enhanced attention mechanism that strengthens feature focus on key geometric details, thereby improving overall image quality and detail preservation. Furthermore, we apply an adaptive learning strategy that fine-tunes the model to better capture spatial relationships and visual coherence between the generated views, ensuring realistic results. Our model also incorporates an iterative refinement process that progressively improves the output quality through multiple stages of image generation. Finally, a dynamic geometry information intensity adjustment mechanism is proposed to adaptively regulate the influence of geometric data, optimizing overall quality while ensuring the naturalness of generated images. More details can be found on the project page: https://github.com/SobeyMIL/GeoMVD.com.

</details>


### [84] [A Novel AI-Driven System for Real-Time Detection of Mirror Absence, Helmet Non-Compliance, and License Plates Using YOLOv8 and OCR](https://arxiv.org/abs/2511.12206)
*Nishant Vasantkumar Hegde,Aditi Agarwal,Minal Moharir*

Main category: cs.CV

TL;DR: An AI-powered system using YOLOv8 and EasyOCR to detect helmet non-compliance, missing rear-view mirrors on motorcycles, and to extract license plates, with a Streamlit UI; achieving high precision and recall and strong mAP, suitable for automated traffic enforcement.


<details>
  <summary>Details</summary>
Motivation: Improve road safety and enforcement efficiency by automating detection of common traffic violations (helmet non-compliance, missing rear-view mirrors) and enabling automated vehicle registration logging.

Method: Train a custom-annotated and augmented dataset for object detection with YOLOv8 to identify helmets, motorcycles without rear-view mirrors, and vehicles; use EasyOCR for license plate recognition with advanced image preprocessing to handle challenging conditions; extract registration numbers; provide a Streamlit-based real-time monitoring/logging interface.

Result: Performance metrics include precision 0.9147, recall 0.886, mAP@50 0.843; mAP@50:95 of 0.503, indicating strong detection and reliable plate recognition under stricter IoU; real-time monitoring and logging demonstrated; practical deployment considerations discussed.

Conclusion: Demonstrates a practical, effective automated traffic-rule enforcement system with potential for real-world deployment, highlighting integration of detection and OCR, data augmentation, and deployment considerations.

Abstract: Road safety is a critical global concern, with manual enforcement of helmet laws and vehicle safety standards (e.g., rear-view mirror presence) being resource-intensive and inconsistent. This paper presents an AI-powered system to automate traffic violation detection, significantly enhancing enforcement efficiency and road safety. The system leverages YOLOv8 for robust object detection and EasyOCR for license plate recognition. Trained on a custom dataset of annotated images (augmented for diversity), it identifies helmet non-compliance, the absence of rear-view mirrors on motorcycles, an innovative contribution to automated checks, and extracts vehicle registration numbers. A Streamlit-based interface facilitates real-time monitoring and violation logging. Advanced image preprocessing enhances license plate recognition, particularly under challenging conditions. Based on evaluation results, the model achieves an overall precision of 0.9147, a recall of 0.886, and a mean Average Precision (mAP@50) of 0.843. The mAP@50 95 of 0.503 further indicates strong detection capability under stricter IoU thresholds. This work demonstrates a practical and effective solution for automated traffic rule enforcement, with considerations for real-world deployment discussed.

</details>


### [85] [Mixture of States: Routing Token-Level Dynamics for Multimodal Generation](https://arxiv.org/abs/2511.12207)
*Haozhe Liu,Ding Liu,Mingchen Zhuge,Zijian Zhou,Tian Xie,Sen He,Yukang Yang,Shuming Liu,Yuren Cong,Jiadong Guo,Hongyu Xu,Ke Xu,Kam-Woh Ng,Juan C. Pérez,Juan-Manuel~Pérez-Rúa,Tao Xiang,Wei Liu,Shikun Liu,Jürgen Schmidhuber*

Main category: cs.CV

TL;DR: MoS introduces a learnable, token-wise router that sparsely selects top-k hidden states to create timestep- and input-dependent cross-modal interactions in diffusion models, achieving strong performance with 3–5B parameters.


<details>
  <summary>Details</summary>
Motivation: To enable flexible, compute-efficient fusion of multimodal information within diffusion models by aligning token-level features with the diffusion trajectory and reducing parameter/compute overhead.

Method: A learnable token-wise router performs sparse top-k selection of hidden states and orchestrates interactions between modalities in a diffusion denoising process. The router is trained with an ε-greedy strategy to efficiently identify contextual features, enabling MoS-Image and MoS-Editing as scalable multimodal diffusion variants.

Result: MoS variants achieve state-of-the-art results in text-to-image generation and editing with only 3–5B parameters, matching or surpassing larger models (up to 4× bigger).

Conclusion: MoS provides a flexible, compute-efficient paradigm for scaling multimodal diffusion models through sparse, state-based, token-level fusion that generalizes across generation and editing tasks.

Abstract: We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-$k$ hidden states and is trained with an $ε$-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to $4\times$ larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.

</details>


### [86] [FaNe: Towards Fine-Grained Cross-Modal Contrast with False-Negative Reduction and Text-Conditioned Sparse Attention](https://arxiv.org/abs/2511.12215)
*Peng Zhang,Zhihui Lai,Wenting Chen,Xu Wu,Heng Kong*

Main category: cs.CV

TL;DR: FaNe is a semantic-enhanced medical VLP that reduces false negatives and enables fine-grained image-text alignment, achieving state-of-the-art across five medical benchmarks in classification, detection, and segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing medical VLP suffers from false negatives due to semantically similar captions and lacks fine-grained cross-modal alignment; there is a need for stronger intra- and inter-modal discrimination.

Method: - Semantic-aware positive pair mining based on text-text similarity with adaptive normalization to reduce false negatives. - Text-conditioned sparse attention pooling to align image regions with textual cues. - Hard-negative aware contrastive loss with adaptive reweighting to strengthen intra-modal discrimination.

Result: Experiments on five downstream medical imaging benchmarks show state-of-the-art performance in image classification, object detection, and semantic segmentation.

Conclusion: FaNe effectively mitigates false negatives and improves cross- and intra-modal alignment, delivering robust performance across tasks.

Abstract: Medical vision-language pre-training (VLP) offers significant potential for advancing medical image understanding by leveraging paired image-report data. However, existing methods are limited by Fa}lse Negatives (FaNe) induced by semantically similar texts and insufficient fine-grained cross-modal alignment. To address these limitations, we propose FaNe, a semantic-enhanced VLP framework. To mitigate false negatives, we introduce a semantic-aware positive pair mining strategy based on text-text similarity with adaptive normalization. Furthermore, we design a text-conditioned sparse attention pooling module to enable fine-grained image-text alignment through localized visual representations guided by textual cues. To strengthen intra-modal discrimination, we develop a hard-negative aware contrastive loss that adaptively reweights semantically similar negatives. Extensive experiments on five downstream medical imaging benchmarks demonstrate that FaNe achieves state-of-the-art performance across image classification, object detection, and semantic segmentation, validating the effectiveness of our framework.

</details>


### [87] [Suppressing VLM Hallucinations with Spectral Representation Filtering](https://arxiv.org/abs/2511.12220)
*Ameen Ali,Tamim Zoabi,Lior Wolf*

Main category: cs.CV

TL;DR: SRF is a training-free, post-hoc spectral filter that suppresses VLM hallucinations by attenuating low-rank hallucination modes identified via eigendecomposition of covariance between truthful and hallucinatory caption features, yielding zero inference overhead and no architecture changes while improving faithfulness across multiple models and benchmarks.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in vision-language models arise from reliance on language priors and imprecise cross-modal grounding; a lightweight, training-free approach is needed to improve faithfulness without degrading caption quality or adding overhead.

Method: Compute covariance of feature differences between truthful vs hallucinatory captions, perform eigendecomposition to identify low-rank hallucination modes; apply a soft spectral filter to the feed-forward projection weights in deeper vLLM layers to dampen these modes, equalize feature variance, and preserve semantics; operates post-hoc with no retraining.

Result: Across LLaVA-1.5, MiniGPT-4, and mPLUG-Owl2, SRF reduces hallucination rates on MSCOCO, POPE-VQA, and other visual benchmarks, achieving state-of-the-art faithfulness without harming caption quality and with zero inference overhead.

Conclusion: A simple, effective post-hoc spectral filtering technique that enhances faithfulness of VLM captions across multiple model families and tasks, without modifications to architecture or training.

Abstract: Vision-language models (VLMs) frequently produce hallucinations in the form of descriptions of objects, attributes, or relations that do not exist in the image due to over-reliance on language priors and imprecise cross-modal grounding. We introduce Spectral Representation Filtering (SRF), a lightweight, training-free method to suppress such hallucinations by analyzing and correcting the covariance structure of the model's representations. SRF identifies low-rank hallucination modes through eigendecomposition of the covariance of the differences between features collected for truthful and hallucinatory captions, revealing structured biases in the feature space. A soft spectral filter then attenuates these modes in the feed-forward projection weights of deeper vLLM layers, equalizing feature variance while preserving semantic fidelity. Unlike decoding or retraining-based approaches, SRF operates entirely post-hoc, incurs zero inference overhead, and requires no architectural modifications. Across three families of VLMs (LLaVA-1.5, MiniGPT-4, and mPLUG-Owl2), SRF consistently reduces hallucination rates on MSCOCO, POPE-VQA, and other visual tasks benchmarks, achieving state-of-the-art faithfulness without degrading caption quality.

</details>


### [88] [Model Inversion Attack Against Deep Hashing](https://arxiv.org/abs/2511.12233)
*Dongdong Zhao,Qiben Xu,Ranxin Fang,Baogang Song*

Main category: cs.CV

TL;DR: DHMI is a diffusion-based model inversion framework for deep hashing that reconstructs high-fidelity images from hash-based systems, exposing serious privacy risks; it outperforms prior black-box inversion methods.


<details>
  <summary>Details</summary>
Motivation: Deep hashing enables efficient retrieval but leaks privacy: training data could be reconstructed from hash codes; existing model inversion methods don't apply to deep hashing due to access to training hash codes and the discrete Hamming space.

Method: DHMI clusters an auxiliary dataset to derive semantic hash centers as surrogate anchors; uses surrogate-guided denoising optimization with a novel attack metric combining classification consistency and hash proximity; a cluster of surrogate models refines candidates to produce high-fidelity, semantically consistent images.

Result: In experiments, DHMI reconstructs high-resolution, high-quality images even under strict black-box settings with no training hash codes; outperforms existing SOTA model inversion attacks in black-box scenarios.

Conclusion: This work reveals tangible privacy risks in deep hashing and underscores the need for defenses against model inversion in hash-based retrieval systems.

Abstract: Deep hashing improves retrieval efficiency through compact binary codes, yet it introduces severe and often overlooked privacy risks. The ability to reconstruct original training data from hash codes could lead to serious threats such as biometric forgery and privacy breaches. However, model inversion attacks specifically targeting deep hashing models remain unexplored, leaving their security implications unexamined. This research gap stems from the inaccessibility of genuine training hash codes and the highly discrete Hamming space, which prevents existing methods from adapting to deep hashing. To address these challenges, we propose DHMI, the first diffusion-based model inversion framework designed for deep hashing. DHMI first clusters an auxiliary dataset to derive semantic hash centers as surrogate anchors. It then introduces a surrogate-guided denoising optimization method that leverages a novel attack metric (fusing classification consistency and hash proximity) to dynamically select candidate samples. A cluster of surrogate models guides the refinement of these candidates, ensuring the generation of high-fidelity and semantically consistent images. Experiments on multiple datasets demonstrate that DHMI successfully reconstructs high-resolution, high-quality images even under the most challenging black-box setting, where no training hash codes are available. Our method outperforms the existing state-of-the-art model inversion attacks in black-box scenarios, confirming both its practical efficacy and the critical privacy risks inherent in deep hashing systems.

</details>


### [89] [MiniGPT-Pancreas: Multimodal Large Language Model for Pancreas Cancer Classification and Detection](https://arxiv.org/abs/2412.15925)
*Andrea Moglia,Elia Clement Nastasio,Luca Mainardi,Pietro Cerveri*

Main category: cs.CV

TL;DR: MiniGPT-Pancreas: a multimodal large language model chatbot for pancreas cancer diagnosis, fine-tuned in a cascaded manner for pancreas detection, pancreas tumor classification, and tumor detection using multimodal prompts; evaluated on NIH, MSD, and AbdomenCT-1k datasets with moderate organ detection IoUs (pancreas ~0.5-0.6, liver ~0.84, kidney ~0.72, spleen ~0.70) and poor pancreas tumor detection IoU (0.168); pancreas cancer classification shows strong performance (≈0.87 accuracy, precision, recall).


<details>
  <summary>Details</summary>
Motivation: Pancreatic imaging is challenging due to small organ size, blurred boundaries, and anatomical variability. The work aims to assist clinicians by providing an interactive, multimodal AI assistant that integrates imaging data with text.

Method: Fine-tune MiniGPT-v2 in a cascaded fashion for three tasks—pancreas detection, tumor classification, and tumor detection—using multimodal prompts combining CT scans and radiology questions. Training data drawn from NIH and Medical Segmentation Decathlon (MSD); AbdomenCT-1k used for multi-organ detection (liver, spleen, kidney, pancreas).

Result: Pancreas detection IoU: NIH 0.595; MSD 0.550. Pancreas cancer classification on MSD: accuracy 0.876, precision 0.874, recall 0.878. AbdomenCT-1k organ IoUs: liver 0.8399, kidney 0.722, spleen 0.705, pancreas 0.497. Pancreas tumor detection on MSD: IoU 0.168.

Conclusion: MiniGPT-Pancreas is a promising tool for classifying pancreas images with tumors, but tumor detection needs substantial improvement; future work should focus on boosting detection performance, particularly for pancreatic tumors.

Abstract: Problem: Pancreas radiological imaging is challenging due to the small size, blurred boundaries, and variability of shape and position of the organ among patients. Goal: In this work we present MiniGPT-Pancreas, a Multimodal Large Language Model (MLLM), as an interactive chatbot to support clinicians in pancreas cancer diagnosis by integrating visual and textual information. Methods: MiniGPT-v2, a general-purpose MLLM, was fine-tuned in a cascaded way for pancreas detection, tumor classification, and tumor detection with multimodal prompts combining questions and computed tomography scans from the National Institute of Health (NIH), and Medical Segmentation Decathlon (MSD) datasets. The AbdomenCT-1k dataset was used to detect the liver, spleen, kidney, and pancreas. Results: MiniGPT-Pancreas achieved an Intersection over Union (IoU) of 0.595 and 0.550 for the detection of pancreas on NIH and MSD datasets, respectively. For the pancreas cancer classification task on the MSD dataset, accuracy, precision, and recall were 0.876, 0.874, and 0.878, respectively. When evaluating MiniGPT-Pancreas on the AbdomenCT-1k dataset for multi-organ detection, the IoU was 0.8399 for the liver, 0.722 for the kidney, 0.705 for the spleen, and 0.497 for the pancreas. For the pancreas tumor detection task, the IoU score was 0.168 on the MSD dataset. Conclusions: MiniGPT-Pancreas represents a promising solution to support clinicians in the classification of pancreas images with pancreas tumors. Future research is needed to improve the score on the detection task, especially for pancreas tumors.

</details>


### [90] [Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets](https://arxiv.org/abs/2511.12255)
*Huy M. Le,Dat Tien Nguyen,Phuc Binh Nguyen,Gia-Bao Le-Tran,Phu Truong Thien,Cuong Dinh,Minh Nguyen,Nga Nguyen,Thuy T. N. Nguyen,Huy Gia Ngo,Tan Nhat Nguyen,Binh T. Nguyen,Monojit Choudhury*

Main category: cs.CV

TL;DR: Fusionista2.0 is a fast, user-friendly video retrieval system for the Video Browser Showdown, achieving up to 75% faster retrieval with improved accuracy and UI.


<details>
  <summary>Details</summary>
Motivation: VBS demands accurate results under strict time limits; there is a need for faster, scalable video retrieval systems that are easy to use in practice.

Method: Re-engineer core modules for efficiency: preprocessing with ffmpeg for fast keyframe extraction; OCR with Vintern-1B-v3.5 for multilingual text recognition; ASR with faster-whisper for real-time transcription; lightweight vision-language models for quick QA; redesigned UI with responsiveness and accessibility to improve workflow.

Result: Retrieval time reduced by up to 75%; accuracy and user satisfaction increased.

Conclusion: Fusionista2.0 is a competitive and user-friendly solution for large-scale video search in time-constrained scenarios.

Abstract: The Video Browser Showdown (VBS) challenges systems to deliver accurate results under strict time constraints. To meet this demand, we present Fusionista2.0, a streamlined video retrieval system optimized for speed and usability. All core modules were re-engineered for efficiency: preprocessing now relies on ffmpeg for fast keyframe extraction, optical character recognition uses Vintern-1B-v3.5 for robust multilingual text recognition, and automatic speech recognition employs faster-whisper for real-time transcription. For question answering, lightweight vision-language models provide quick responses without the heavy cost of large models. Beyond these technical upgrades, Fusionista2.0 introduces a redesigned user interface with improved responsiveness, accessibility, and workflow efficiency, enabling even non-expert users to retrieve relevant content rapidly. Evaluations demonstrate that retrieval time was reduced by up to 75% while accuracy and user satisfaction both increased, confirming Fusionista2.0 as a competitive and user-friendly system for large-scale video search.

</details>


### [91] [Prompt-Conditioned FiLM and Multi-Scale Fusion on MedSigLIP for Low-Dose CT Quality Assessment](https://arxiv.org/abs/2511.12256)
*Tolga Demiroglu,Mehmet Ozan Unal,Metin Ertas,Isa Yildirim*

Main category: cs.CV

TL;DR: Prompt-conditioned MedSigLIP-based framework using FiLM and multi-scale pooling to inject textual priors for LDCT image quality assessment; enables data-efficient learning and rapid adaptation with competitive performance.


<details>
  <summary>Details</summary>
Motivation: Improve LDCT quality assessment under limited data by leveraging textual priors and prompt-driven feature modulation and pooling to guide learning.

Method: Prompt-conditioned FiLM to modulate patch-token features with textual priors; multi-scale pooling (global, local, texture-aware) with separate regression heads fused via a lightweight MLP; trained with pairwise ranking loss; evaluated on LDCTIQA2023 with 1,000 training images.

Result: Achieves PLCC 0.9575, SROCC 0.9561, KROCC 0.8301, surpassing the top-ranked challenge submissions.

Conclusion: Demonstrates effectiveness of prompt-guided, data-efficient learning for LDCT quality assessment and rapid adaptation to clinical intent.

Abstract: We propose a prompt-conditioned framework built on MedSigLIP that injects textual priors via Feature-wise Linear Modulation (FiLM) and multi-scale pooling. Text prompts condition patch-token features on clinical intent, enabling data-efficient learning and rapid adaptation. The architecture combines global, local, and texture-aware pooling through separate regression heads fused by a lightweight MLP, trained with pairwise ranking loss. Evaluated on the LDCTIQA2023 (a public LDCT quality assessment challenge) with 1,000 training images, we achieve PLCC = 0.9575, SROCC = 0.9561, and KROCC = 0.8301, surpassing the top-ranked published challenge submissions and demonstrating the effectiveness of our prompt-guided approach.

</details>


### [92] [A Disease-Aware Dual-Stage Framework for Chest X-ray Report Generation](https://arxiv.org/abs/2511.12259)
*Puzhen Wu,Hexin Dong,Yi Lin,Yihao Ding,Yifan Peng*

Main category: cs.CV

TL;DR: Proposes a dual-stage disease-aware framework for chest X-ray report generation. Stage 1 learns Disease-Aware Semantic Tokens (DASTs) via cross-attention and multi-label classification plus contrastive vision-language alignment; Stage 2 introduces a Disease-Visual Attention Fusion (DVAF) to merge disease-aware tokens with visual features and a Dual-Modal Similarity Retrieval (DMSR) to retrieve exemplars to guide generation, achieving state-of-the-art results on CheXpert Plus, IU X-ray and MIMIC-CXR with improvements in clinical accuracy and linguistic quality.


<details>
  <summary>Details</summary>
Motivation: Current chest X-ray report generation methods often lack sufficient disease-awareness in visual representations and robust vision-language alignment, leading to missed pathologies and clinically inaccurate reports.

Method: Stage 1: learn Disease-Aware Semantic Tokens (DASTs) via cross-attention and multi-label classification; contrastive learning to align vision and language representations. Stage 2: Disease-Visual Attention Fusion (DVAF) to integrate disease-aware representations with visual features; Dual-Modal Similarity Retrieval (DMSR) to combine visual and disease-specific similarities to retrieve relevant exemplars for contextual guidance during report generation.

Result: The approach achieves state-of-the-art performance on CheXpert Plus, IU X-ray, and MIMIC-CXR datasets, with significant improvements in clinical accuracy and linguistic quality of generated reports.

Conclusion: A dual-stage disease-aware framework enhances pathology awareness and retrieval-guided generation, yielding more accurate and clinically faithful radiology reports.

Abstract: Radiology report generation from chest X-rays is an important task in artificial intelligence with the potential to greatly reduce radiologists' workload and shorten patient wait times. Despite recent advances, existing approaches often lack sufficient disease-awareness in visual representations and adequate vision-language alignment to meet the specialized requirements of medical image analysis. As a result, these models usually overlook critical pathological features on chest X-rays and struggle to generate clinically accurate reports. To address these limitations, we propose a novel dual-stage disease-aware framework for chest X-ray report generation. In Stage~1, our model learns Disease-Aware Semantic Tokens (DASTs) corresponding to specific pathology categories through cross-attention mechanisms and multi-label classification, while simultaneously aligning vision and language representations via contrastive learning. In Stage~2, we introduce a Disease-Visual Attention Fusion (DVAF) module to integrate disease-aware representations with visual features, along with a Dual-Modal Similarity Retrieval (DMSR) mechanism that combines visual and disease-specific similarities to retrieve relevant exemplars, providing contextual guidance during report generation. Extensive experiments on benchmark datasets (i.e., CheXpert Plus, IU X-ray, and MIMIC-CXR) demonstrate that our disease-aware framework achieves state-of-the-art performance in chest X-ray report generation, with significant improvements in clinical accuracy and linguistic quality.

</details>


### [93] [CrossVid: A Comprehensive Benchmark for Evaluating Cross-Video Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2511.12263)
*Jingyao Li,Jingyun Wang,Molin Tan,Haochen Wang,Cilin Yan,Likun Shi,Jiayin Cai,Xiaolong Jiang,Yao Hu*

Main category: cs.CV

TL;DR: CrossVid is a first comprehensive benchmark for cross-video reasoning (CVR) that evaluates multimodal LLMs on spatial-temporal reasoning across multiple videos. It introduces a hierarchical suite of 4 high-level dimensions and 10 tasks, with 5,331 videos and 9,015 QA pairs (single-choice, multi-choice, open-ended). Gemini-2.5-Pro achieves the best average accuracy (50.4%), but most MLLMs struggle to integrate evidence distributed across videos, highlighting CVR's challenges and CrossVid's potential to drive progress.


<details>
  <summary>Details</summary>
Motivation: There is a gap in evaluating multimodal LLMs on cross-video reasoning, as existing benchmarks focus on single-video analysis or limited multi-view tasks. Real-world video understanding often requires aggregating information across multiple videos, so a benchmark is needed to test CVR capabilities.

Method: Design and release of CrossVid, a dataset and benchmark with four high-level dimensions and ten tasks that probe spatial-temporal cross-video reasoning. The dataset contains 5,331 videos and 9,015 QA pairs across single-choice, multi-choice, and open-ended formats. Evaluation across open- and closed-source MLLMs, plus an in-depth case study.

Result: Gemini-2.5-Pro performs best on CrossVid with an average accuracy of 50.4%. Across models, CVR tasks remain difficult; most MLLMs fail to effectively integrate or compare evidence distributed across multiple videos, indicating a core capability gap in CVR reasoning.

Conclusion: CrossVid establishes a new standard for CVR evaluation, providing a broad, hierarchical set of tasks and a substantial dataset to drive research. It reveals current models' limitations and offers a clear direction for enhancing cross-video integration and reasoning in future MLLMs.

Abstract: Cross-Video Reasoning (CVR) presents a significant challenge in video understanding, which requires simultaneous understanding of multiple videos to aggregate and compare information across groups of videos. Most existing video understanding benchmarks focus on single-video analysis, failing to assess the ability of multimodal large language models (MLLMs) to simultaneously reason over various videos. Recent benchmarks evaluate MLLMs' capabilities on multi-view videos that capture different perspectives of the same scene. However, their limited tasks hinder a thorough assessment of MLLMs in diverse real-world CVR scenarios. To this end, we introduce CrossVid, the first benchmark designed to comprehensively evaluate MLLMs' spatial-temporal reasoning ability in cross-video contexts. Firstly, CrossVid encompasses a wide spectrum of hierarchical tasks, comprising four high-level dimensions and ten specific tasks, thereby closely reflecting the complex and varied nature of real-world video understanding. Secondly, CrossVid provides 5,331 videos, along with 9,015 challenging question-answering pairs, spanning single-choice, multiple-choice, and open-ended question formats. Through extensive experiments on various open-source and closed-source MLLMs, we observe that Gemini-2.5-Pro performs best on CrossVid, achieving an average accuracy of 50.4%. Notably, our in-depth case study demonstrates that most current MLLMs struggle with CVR tasks, primarily due to their inability to integrate or compare evidence distributed across multiple videos for reasoning. These insights highlight the potential of CrossVid to guide future advancements in enhancing MLLMs' CVR capabilities.

</details>


### [94] [ZoomEarth: Active Perception for Ultra-High-Resolution Geospatial Vision-Language Tasks](https://arxiv.org/abs/2511.12267)
*Ruixun Liu,Bowen Fu,Jiayi Song,Kaiyu Li,Wanchen Li,Lanxuan Xue,Hui Qiao,Weizhan Zhang,Deyu Meng,Xiangyong Cao*

Main category: cs.CV

TL;DR: A new active-perception framework for ultra-high-resolution remote sensing that learns to revisit informative regions, via the ZoomEarth system built on the LRS-GRO benchmark, achieving state-of-the-art and strong zero-shot transfer, with broad downstream applicability.


<details>
  <summary>Details</summary>
Motivation: Current dynamic-resolution and token-pruning methods follow a passive perception paradigm, leading to redundancy and inefficiency at finer inputs. An active-perception approach can selectively revisit information-rich areas to improve processing efficiency and accuracy in UHR RS tasks.

Method: (1) Create LRS-GRO, a large-scale benchmark with 17 question types covering global, regional, and object levels. (2) Propose ZoomEarth, an adaptive cropping-zooming framework with a Region-Guided reward. (3) Train via supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO).

Result: ZoomEarth achieves state-of-the-art performance on LRS-GRO and, in zero-shot settings, on three public UHR RS benchmarks. It can be integrated with downstream models for tasks like cloud removal, denoising, segmentation, and editing through simple tool interfaces, showing strong versatility.

Conclusion: Active-perception-driven adaptive zooming is effective for UHR RS processing, offering strong performance and broad applicability, with the framework readily extensible to various RS pipelines and tasks.

Abstract: Ultra-high-resolution (UHR) remote sensing (RS) images offer rich fine-grained information but also present challenges in effective processing. Existing dynamic resolution and token pruning methods are constrained by a passive perception paradigm, suffering from increased redundancy when obtaining finer visual inputs. In this work, we explore a new active perception paradigm that enables models to revisit information-rich regions. First, we present LRS-GRO, a large-scale benchmark dataset tailored for active perception in UHR RS processing, encompassing 17 question types across global, region, and object levels, annotated via a semi-automatic pipeline. Building on LRS-GRO, we propose ZoomEarth, an adaptive cropping-zooming framework with a novel Region-Guided reward that provides fine-grained guidance. Trained via supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), ZoomEarth achieves state-of-the-art performance on LRS-GRO and, in the zero-shot setting, on three public UHR remote sensing benchmarks. Furthermore, ZoomEarth can be seamlessly integrated with downstream models for tasks such as cloud removal, denoising, segmentation, and image editing through simple tool interfaces, demonstrating strong versatility and extensibility.

</details>


### [95] [TM-UNet: Token-Memory Enhanced Sequential Modeling for Efficient Medical Image Segmentation](https://arxiv.org/abs/2511.12270)
*Yaxuan Jiao,Qing Xu,Yuxiang Luo,Xiangjian He,Zhen Chen,Wenting Duan*

Main category: cs.CV

TL;DR: A lightweight transformer-inspired UNet (TM-UNet) for medical image segmentation that uses a multi-scale token-memory (MSTM) block to transform 2D features into token sequences and retain context via memory cells, achieving efficient global reasoning with linear complexity and reduced computation.


<details>
  <summary>Details</summary>
Motivation: Address the high computational cost of transformer-based medical segmentation methods, hindering clinical deployment; need efficient global reasoning without expensive self-attention.

Method: Introduce MSTM block: spatial scanning converts 2D features into token sequences; uses memory cells to curate/disseminate contextual information across tokens; exponential gating to assess token effectiveness; parallel pooling for multi-scale context; achieves hierarchical representation with low overhead.

Result: Empirical results show TM-UNet outperforms state-of-the-art methods on diverse medical segmentation tasks while substantially reducing computation; code released.

Conclusion: The token-memory mechanism enables long-range dependency modeling with linear complexity, offering efficient global reasoning for medical image segmentation; the proposed framework provides a practical, scalable solution for clinical deployment.

Abstract: Medical image segmentation is essential for clinical diagnosis and treatment planning. Although transformer-based methods have achieved remarkable results, their high computational cost hinders clinical deployment. To address this issue, we propose TM-UNet, a novel lightweight framework that integrates token sequence modeling with an efficient memory mechanism for efficient medical segmentation. Specifically, we introduce a multi-scale token-memory (MSTM) block that transforms 2D spatial features into token sequences through strategic spatial scanning, leveraging matrix memory cells to selectively retain and propagate discriminative contextual information across tokens. This novel token-memory mechanism acts as a dynamic knowledge store that captures long-range dependencies with linear complexity, enabling efficient global reasoning without redundant computation. Our MSTM block further incorporates exponential gating to identify token effectiveness and multi-scale contextual extraction via parallel pooling operations, enabling hierarchical representation learning without computational overhead. Extensive experiments demonstrate that TM-UNet outperforms state-of-the-art methods across diverse medical segmentation tasks with substantially reduced computation cost. The code is available at https://github.com/xq141839/TM-UNet.

</details>


### [96] [D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs](https://arxiv.org/abs/2511.12280)
*Shuochen Chang,Xiaofeng Zhang,Qingyang Liu,Li Niu*

Main category: cs.CV

TL;DR: D3ToM speeds up diffusion-based multimodal LLMs by dynamically merging redundant visual tokens during denoising, guided by decider tokens, with a plug-in, parameter-free sequence shortening that varies by step; achieves faster inference with competitive performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion MLLMs incur cubic decoding complexity due to full self-attention over long visual token sequences at every denoising step. A lightweight, plug-in method to reduce token counts during decoding is needed to enable practical inference.

Method: At each denoising step, use decider tokens from the previous step to build an importance map over all visual tokens. Retain a proportion of the most salient tokens and merge the rest via similarity-based aggregation. This merge is implemented as a plug-in module attached to a single transformer layer, shortening the visual token sequence for all subsequent layers without changing model parameters. The merge ratio is dynamically varied across denoising steps to align with the diffusion process.

Result: Experiments show accelerated inference for Diffusion MLLMs while maintaining competitive performance compared to baselines. The approach is released as open-source code.

Conclusion: Dynamic, decider-guided token merging effectively reduces visual token counts during diffusion decoding, enabling faster inference without retraining and offering a practical plug-in for Diffusion MLLMs.

Abstract: Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.

</details>


### [97] [One target to align them all: LiDAR, RGB and event cameras extrinsic calibration for Autonomous Driving](https://arxiv.org/abs/2511.12291)
*Andrea Bertogalli,Giacomo Boracchi,Luca Magri*

Main category: cs.CV

TL;DR: Introduces a one-shot, joint extrinsic calibration framework for event cameras, LiDARs, and RGB cameras using a single 3D calibration target that encodes features for all three modalities (planes for LiDAR, ChArUco markers for RGB, and active LED patterns for event cameras). Validated on a custom autonomous driving dataset.


<details>
  <summary>Details</summary>
Motivation: Accurate multi-sensor calibration is critical for autonomous driving. Existing methods often rely on pairwise or separate calibrations and struggle with the unique characteristics of event cameras; a unified, multi-modal calibration approach is needed.

Method: Design of a novel 3D calibration target perceivable by all three modalities, encoding features in planes (LiDAR), ChArUco markers (RGB), and active LED patterns (event cameras). A one-shot, joint extrinsic calibration pipeline estimates the relative poses among the three sensors, enabling robust multi-sensor alignment in complex autonomous driving setups. Validation conducted on a custom autonomous driving dataset.

Result: Experimental evaluation on a custom autonomous driving dataset confirms the accuracy and robustness of the proposed one-shot, multi-modal calibration framework for event cameras, LiDARs, and RGB cameras.

Conclusion: The framework enables accurate, robust, and streamlined joint extrinsic calibration across event cameras, LiDARs, and RGB cameras, simplifying deployment for complex autonomous driving sensor suites.

Abstract: We present a novel multi-modal extrinsic calibration framework designed to simultaneously estimate the relative poses between event cameras, LiDARs, and RGB cameras, with particular focus on the challenging event camera calibration. Core of our approach is a novel 3D calibration target, specifically designed and constructed to be concurrently perceived by all three sensing modalities. The target encodes features in planes, ChArUco, and active LED patterns, each tailored to the unique characteristics of LiDARs, RGB cameras, and event cameras respectively. This unique design enables a one-shot, joint extrinsic calibration process, in contrast to existing approaches that typically rely on separate, pairwise calibrations. Our calibration pipeline is designed to accurately calibrate complex vision systems in the context of autonomous driving, where precise multi-sensor alignment is critical. We validate our approach through an extensive experimental evaluation on a custom built dataset, recorded with an advanced autonomous driving sensor setup, confirming the accuracy and robustness of our method.

</details>


### [98] [Rethinking Bias in Generative Data Augmentation for Medical AI: a Frequency Recalibration Method](https://arxiv.org/abs/2511.12301)
*Chi Liu,Jincheng Liu,Congcong Zhu,Minghao Wang,Sheng Shen,Jia Gu,Tianqing Zhu,Wanlei Zhou*

Main category: cs.CV

TL;DR: FreRec reduces frequency distribution mismatch in AI-generated medical images via two steps (SHR and RHM), boosting downstream classification and serving as a model-agnostic post-processing step.


<details>
  <summary>Details</summary>
Motivation: Medical AI suffers from data scarcity and GDA bias. Frequency misalignment between real and synthesized images is identified as a key factor driving unreliable GDA, risking detrimental features in downstream tasks.

Method: FreRec is a standalone post-processing framework consisting of Statistical High-frequency Replacement (SHR) to roughly align high-frequency components and Reconstructive High-frequency Mapping (RHM) to enhance image quality and recover high-frequency details.

Result: Across brain MRIs, chest X-rays, and fundus images, FreRec significantly improves downstream medical image classification when using AI-synthesized samples compared with uncalibrated GDA.

Conclusion: Frequency calibration via FreRec is an effective, model-agnostic post-processing step that reduces frequency distributional discrepancies in GDA, improving reliability of synthetic data in medical imaging pipelines.

Abstract: Developing Medical AI relies on large datasets and easily suffers from data scarcity. Generative data augmentation (GDA) using AI generative models offers a solution to synthesize realistic medical images. However, the bias in GDA is often underestimated in medical domains, with concerns about the risk of introducing detrimental features generated by AI and harming downstream tasks. This paper identifies the frequency misalignment between real and synthesized images as one of the key factors underlying unreliable GDA and proposes the Frequency Recalibration (FreRec) method to reduce the frequency distributional discrepancy and thus improve GDA. FreRec involves (1) Statistical High-frequency Replacement (SHR) to roughly align high-frequency components and (2) Reconstructive High-frequency Mapping (RHM) to enhance image quality and reconstruct high-frequency details. Extensive experiments were conducted in various medical datasets, including brain MRIs, chest X-rays, and fundus images. The results show that FreRec significantly improves downstream medical image classification performance compared to uncalibrated AI-synthesized samples. FreRec is a standalone post-processing step that is compatible with any generative model and can integrate seamlessly with common medical GDA pipelines.

</details>


### [99] [LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors](https://arxiv.org/abs/2511.12304)
*Qifeng Chen,Jiarun Liu,Rengan Xie,Tao Tang,Sicong Du,Yiru Zhao,Yuchi Huo,Sheng Yang*

Main category: cs.CV

TL;DR: LiDAR-GS++ enhances Gaussian Splatting for LiDAR with diffusion priors, enabling real-time, high-fidelity extrapolated view synthesis and achieving state-of-the-art results for both interpolated and extrapolated views.


<details>
  <summary>Details</summary>
Motivation: Extrapolated novel view synthesis with GS-based rendering suffers artifacts due to incomplete reconstruction from single traversal LiDAR scans, requiring global geometric consistency and fidelity.

Method: Introduce a controllable LiDAR generation model conditioned on coarsely extrapolated rendering to produce geometry-consistent scans, plus a diffusion-prior-based refinement and a distillation mechanism to enable expansive reconstruction. Extend reconstruction to under-fitted regions to ensure global geometry consistency for extrapolated views, while preserving sensor-captured surface details.

Result: Achieves state-of-the-art performance on multiple public datasets for both interpolated and extrapolated viewpoints, outperforming existing Gaussian Splatting and NeRF-based methods.

Conclusion: LiDAR-GS++ provides real-time, high-fidelity re-simulation with globally consistent geometry for extrapolated novel views, while preserving surface detail captured by LiDAR on urban road scenes.

Abstract: Recent GS-based rendering has made significant progress for LiDAR, surpassing Neural Radiance Fields (NeRF) in both quality and speed. However, these methods exhibit artifacts in extrapolated novel view synthesis due to the incomplete reconstruction from single traversal scans. To address this limitation, we present LiDAR-GS++, a LiDAR Gaussian Splatting reconstruction method enhanced by diffusion priors for real-time and high-fidelity re-simulation on public urban roads. Specifically, we introduce a controllable LiDAR generation model conditioned on coarsely extrapolated rendering to produce extra geometry-consistent scans and employ an effective distillation mechanism for expansive reconstruction. By extending reconstruction to under-fitted regions, our approach ensures global geometric consistency for extrapolative novel views while preserving detailed scene surfaces captured by sensors. Experiments on multiple public datasets demonstrate that LiDAR-GS++ achieves state-of-the-art performance for both interpolated and extrapolated viewpoints, surpassing existing GS and NeRF-based methods.

</details>


### [100] [Learning Time in Static Classifiers](https://arxiv.org/abs/2511.12321)
*Xi Ding,Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.CV

TL;DR: A simple framework that adds temporal reasoning to standard classifiers without changing architecture, by structuring data into temporal trajectories and learning class-specific temporal prototypes with a differentiable soft-DTW loss; yields temporal consistency and improved performance while remaining data-efficient.


<details>
  <summary>Details</summary>
Motivation: Real-world data evolves over time; static classifiers neglect temporal dynamics; aim to inject temporal inductive bias via loss design rather than altering architecture.

Method: Construct temporal trajectories from pre-extracted features; learn class-specific temporal prototypes; align predictions with differentiable soft-DTW; use a multi-term objective for semantic consistency and temporal smoothness; no recurrent modules; a simple classifier on top of features.

Result: Improved accuracy on fine- and ultra-fine-grained image classification; temporally consistent predictions for video anomaly detection; modular, data-efficient, and applicable to both static and temporal tasks.

Conclusion: The approach bridges static and temporal learning through loss design, offering a simple, effective, and data-efficient path to incorporate temporal reasoning without changing classifier architectures.

Abstract: Real-world visual data rarely presents as isolated, static instances. Instead, it often evolves gradually over time through variations in pose, lighting, object state, or scene context. However, conventional classifiers are typically trained under the assumption of temporal independence, limiting their ability to capture such dynamics. We propose a simple yet effective framework that equips standard feedforward classifiers with temporal reasoning, all without modifying model architectures or introducing recurrent modules. At the heart of our approach is a novel Support-Exemplar-Query (SEQ) learning paradigm, which structures training data into temporally coherent trajectories. These trajectories enable the model to learn class-specific temporal prototypes and align prediction sequences via a differentiable soft-DTW loss. A multi-term objective further promotes semantic consistency and temporal smoothness. By interpreting input sequences as evolving feature trajectories, our method introduces a strong temporal inductive bias through loss design alone. This proves highly effective in both static and temporal tasks: it enhances performance on fine-grained and ultra-fine-grained image classification, and delivers precise, temporally consistent predictions in video anomaly detection. Despite its simplicity, our approach bridges static and temporal learning in a modular and data-efficient manner, requiring only a simple classifier on top of pre-extracted features.

</details>


### [101] [SpaceVLM: Sub-Space Modeling of Negation in Vision-Language Models](https://arxiv.org/abs/2511.12331)
*Sepehr Kazemi Ranjbar,Kumail Alhamoud,Marzyeh Ghassemi*

Main category: cs.CV

TL;DR: A training-free negation framework for vision-language models that treats negation as a subspace in the joint embedding space, using spherical caps around the positive (A) and negated (N) embeddings to guide retrieval and generation. It yields ~30% average gains on negation tasks and preserves zero-shot performance.


<details>
  <summary>Details</summary>
Motivation: Vision-language models struggle with negation; fine-tuning on negation data often degrades zero-shot performance on affirmative prompts. A subspace-based, training-free approach aims to improve negation understanding without retraining.

Method: Model negation as a subspace in the joint embedding space. For a caption like 'A but not N', construct spherical caps around the embeddings of A and N. Score candidate images by the central direction of the region near A and far from N, enabling retrieval, MCQ, and text-to-image tasks without fine-tuning.

Result: Across retrieval, MCQ, and text-to-image tasks, the method improves negation understanding by about 30% on average, closes the gap between affirmative and negated prompts, and preserves zero-shot performance that fine-tuning can hurt.

Conclusion: A simple, training-free approach that leverages the intrinsic subspace structure of VLM embeddings to model negation, achieving notable gains while maintaining zero-shot capabilities; code release anticipated.

Abstract: Vision-Language Models (VLMs) struggle with negation. Given a prompt like "retrieve (or generate) a street scene without pedestrians," they often fail to respect the "not." Existing methods address this limitation by fine-tuning on large negation datasets, but such retraining often compromises the model's zero-shot performance on affirmative prompts. We show that the embedding space of VLMs, such as CLIP, can be divided into semantically consistent subspaces. Based on this property, we propose a training-free framework that models negation as a subspace in the joint embedding space rather than a single point (Figure 1). To find the matching image for a caption such as "A but not N," we construct two spherical caps around the embeddings of A and N, and we score images by the central direction of the region that is close to A and far from N. Across retrieval, MCQ, and text-to-image tasks, our method improves negation understanding by about 30% on average over prior methods. It closes the gap between affirmative and negated prompts while preserving the zero-shot performance that fine-tuned models fail to maintain. Code will be released upon publication.

</details>


### [102] [Ground Plane Projection for Improved Traffic Analytics at Intersections](https://arxiv.org/abs/2511.12342)
*Sajjad Pakdamansavoji,Kumar Vaibhav Jha,Baher Abdulhai,James H Elder*

Main category: cs.CV

TL;DR: Back-projecting vehicle detections to the ground plane improves turning movement counting; single-camera back-projection outperforms image-plane analysis, and weak fusion from multiple cameras yields even higher accuracy, advocating ground-plane analysis for traffic monitoring.


<details>
  <summary>Details</summary>
Motivation: Turning movement counts are crucial for signal control and urban planning, but image-plane analyses may lose real-world geometry. Representing traffic in the ground plane can improve accuracy and robustness.

Method: Evaluate back-projection of detections from one or more infrastructure cameras onto the ground plane; compare single-camera back-projection to traditional image-plane methods; test weak fusion of detections from multiple cameras.

Result: Back-projected, real-world 3D coordinates yield more accurate trajectory classification and turning counts than image-plane analysis; multi-camera weak fusion further improves accuracy.

Conclusion: Traffic analysis should be conducted on the ground plane rather than the image plane to achieve higher accuracy; multi-camera fusion can enhance performance.

Abstract: Accurate turning movement counts at intersections are important for signal control, traffic management and urban planning. Computer vision systems for automatic turning movement counts typically rely on visual analysis in the image plane of an infrastructure camera. Here we explore potential advantages of back-projecting vehicles detected in one or more infrastructure cameras to the ground plane for analysis in real-world 3D coordinates. For single-camera systems we find that back-projection yields more accurate trajectory classification and turning movement counts. We further show that even higher accuracy can be achieved through weak fusion of back-projected detections from multiple cameras. These results suggeest that traffic should be analyzed on the ground plane, not the image plane

</details>


### [103] [CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification](https://arxiv.org/abs/2511.12346)
*Asmit Bandyopadhyay,Anindita Das Bhattacharjee,Rakesh Das*

Main category: cs.CV

TL;DR: CLAReSNet is a hybrid CNN-Transformer HSI classifier that uses a multi-scale convolutional stem with residual blocks and spectral encoding via bidirectional RNNs and MSLA, achieving state-of-the-art accuracy with reduced complexity and robust performance under limited data and class imbalance.


<details>
  <summary>Details</summary>
Motivation: HSI classification suffers from high spectral dimensionality, complex spectral-spatial correlations, and limited, imbalanced labeled data; CNNs excel locally while transformers capture long-range dependencies, but their isolated use causes suboptimal results and high complexity. A hybrid, adaptive latent-attention approach can exploit both local and global cues efficiently.

Method: A multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features; spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA); MSLA reduces complexity from O(T^2 D) to O(T log T D) via adaptive latent token allocation (8-64 tokens); hierarchical cross-attention fusion aggregates multi-level representations for robust classification.

Result: Experiments on Indian Pines and Salinas show state-of-the-art performance with overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer.

Conclusion: Learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet’s effectiveness under limited samples and severe class imbalance.

Abstract: Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\mathcal{O}(T^2D)$ to $\mathcal{O}(T\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under limited samples and severe class imbalance.

</details>


### [104] [Explainable AI-Generated Image Detection RewardBench](https://arxiv.org/abs/2511.12363)
*Michael Yang,Shijian Deng,William T. Doan,Kai Wang,Tianyu Yang,Harsh Singh,Yapeng Tian*

Main category: cs.CV

TL;DR: A benchmark (XAIGID-RewardBench) evaluates how well current multimodal LLMs can judge the quality of explanations for AI-generated vs real images. It uses ~3,000 annotated triplets with detectors (policy models) and judges, showing the best judge scores 88.76% vs 98.30% human agreement, indicating a gap to human-level explanation reasoning; code is public.


<details>
  <summary>Details</summary>
Motivation: To restore trust and persuasiveness of AI-generated image detection by enabling explanations; existing classification-based detectors lack human-understandable reasoning, and evaluating judges (MLLMs) is underexplored, necessitating a benchmark to quantify judge quality.

Method: Construct XAIGID-RewardBench with roughly 3,000 annotated triplets from diverse image generation models and MLLMs acting as detectors (policies); evaluate current MLLMs serving as reward models (judges) on the quality of explanations for real vs. AI-generated images; report performance and analyze failure modes; release code and data.

Result: The top-performing reward model achieves 88.76% accuracy on the benchmark, while human inter-annotator agreement is 98.30%, highlighting a substantial gap between current MLLMs and humans in judging explanations; the authors also analyze common pitfalls.

Conclusion: There remains a clear gap between current MLLMs and human-level reasoning in evaluating explanations for AI-generated image detection; the XAIGID-RewardBench benchmark provides a quantitative framework and baseline, with open-source code and data for further research.

Abstract: Conventional, classification-based AI-generated image detection methods cannot explain why an image is considered real or AI-generated in a way a human expert would, which reduces the trustworthiness and persuasiveness of these detection tools for real-world applications. Leveraging Multimodal Large Language Models (MLLMs) has recently become a trending solution to this issue. Further, to evaluate the quality of generated explanations, a common approach is to adopt an "MLLM as a judge" methodology to evaluate explanations generated by other MLLMs. However, how well those MLLMs perform when judging explanations for AI-generated image detection generated by themselves or other MLLMs has not been well studied. We therefore propose \textbf{XAIGID-RewardBench}, the first benchmark designed to evaluate the ability of current MLLMs to judge the quality of explanations about whether an image is real or AI-generated. The benchmark consists of approximately 3,000 annotated triplets sourced from various image generation models and MLLMs as policy models (detectors) to assess the capabilities of current MLLMs as reward models (judges). Our results show that the current best reward model scored 88.76\% on this benchmark (while human inter-annotator agreement reaches 98.30\%), demonstrating that a visible gap remains between the reasoning abilities of today's MLLMs and human-level performance. In addition, we provide an analysis of common pitfalls that these models frequently encounter. Code and benchmark are available at https://github.com/RewardBench/XAIGID-RewardBench.

</details>


### [105] [Constructing and Interpreting Digital Twin Representations for Visual Reasoning via Reinforcement Learning](https://arxiv.org/abs/2511.12365)
*Yiqing Shen,Mathias Unberath*

Main category: cs.CV

TL;DR: A reinforcement learning approach (DT-R1) that builds digital twin representations of multi-modal visuals with a GRPO reward, enabling unified visual reasoning across tasks and outperforming task-specific models on six benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current visual reasoning tasks—segmentation, grounding, summarization, and VQA—rely on task-specific models, hindering cross-task generalization. A unified RL-based framework could enable reasoning over high-level, modality-agnostic representations.

Method: Train large language models to construct digital twin representations from complex multi-modal inputs and perform reasoning over these representations. Use a novel GRPO reward that enforces both structural integrity of the twins and accuracy of outputs. Evaluate on six benchmarks spanning two modalities and four task types.

Result: DT-R1 consistently outperforms state-of-the-art task-specific models across six benchmarks, demonstrating improved generalization and cross-task performance.

Conclusion: Reinforcement learning with digital twin representations can enable unified visual reasoning, providing a new direction where high-level, structured representations drive cross-task, cross-modality reasoning.

Abstract: Visual reasoning may require models to interpret images and videos and respond to implicit text queries across diverse output formats, from pixel-level segmentation masks to natural language descriptions. Existing approaches rely on supervised fine-tuning with task-specific architectures. For example, reasoning segmentation, grounding, summarization, and visual question answering each demand distinct model designs and training, preventing unified solutions and limiting cross-task and cross-modality generalization. Hence, we propose DT-R1, a reinforcement learning framework that trains large language models to construct digital twin representations of complex multi-modal visual inputs and then reason over these high-level representations as a unified approach to visual reasoning. Specifically, we train DT-R1 using GRPO with a novel reward that validates both structural integrity and output accuracy. Evaluations in six visual reasoning benchmarks, covering two modalities and four task types, demonstrate that DT-R1 consistently achieves improvements over state-of-the-art task-specific models. DT-R1 opens a new direction where visual reasoning emerges from reinforcement learning with digital twin representations.

</details>


### [106] [Fast Reasoning Segmentation for Images and Videos](https://arxiv.org/abs/2511.12368)
*Yiqing Shen,Mathias Unberath*

Main category: cs.CV

TL;DR: FastReasonSeg distills reasoning segmentation from teacher models using digital twin representations, enabling efficient edge deployment with real-time reasoning segmentation and competitive performance.


<details>
  <summary>Details</summary>
Motivation: Open-set reasoning segmentation hinges on multimodal LLMs that are too large for edge devices; there is a need to compress models while preserving multi-step reasoning capabilities and chains, not just outputs.

Method: Use digital twin representations to decouple perception from reasoning, followed by supervised fine-tuning on teacher-generated reasoning chains and reinforcement fine-tuning with joint rewards for segmentation accuracy and reasoning alignment.

Result: Achieves state-of-the-art reasoning segmentation on two video benchmarks (JiTBench, RVTBench) and two image benchmarks (ReasonSeg, LLM-Seg40K); a 0.6B distilled model outperforms much larger models (20x more parameters) with 7.79 FPS and 2.1 GB memory usage.

Conclusion: Digital twin–guided distillation enables efficient, real-time reasoning segmentation on resource-constrained devices by preserving reasoning chains during training and decoupling perception from reasoning.

Abstract: Reasoning segmentation enables open-set object segmentation via implicit text queries, therefore serving as a foundation for embodied agents that should operate autonomously in real-world environments. However, existing methods for reasoning segmentation require multimodal large language models with billions of parameters that exceed the computational capabilities of edge devices that typically deploy the embodied AI systems. Distillation offers a pathway to compress these models while preserving their capabilities. Yet, existing distillation approaches fail to transfer the multi-step reasoning capabilities that reasoning segmentation demands, as they focus on matching output predictions and intermediate features rather than preserving reasoning chains. The emerging paradigm of reasoning over digital twin representations presents an opportunity for more effective distillation by re-framing the problem. Consequently, we propose FastReasonSeg, which employs digital twin representations that decouple perception from reasoning to enable more effective distillation. Our distillation scheme first relies on supervised fine-tuning on teacher-generated reasoning chains. Then it is followed by reinforcement fine-tuning with joint rewards evaluating both segmentation accuracy and reasoning quality alignment. Experiments on two video (JiTBench, RVTBench) and two image benchmarks (ReasonSeg, LLM-Seg40K) demonstrate that our FastReasonSeg achieves state-of-the-art reasoning segmentation performance. Moreover, the distilled 0.6B variant outperforms models with 20 times more parameters while achieving 7.79 FPS throughput with only 2.1GB memory consumption. This efficiency enables deployment in resource-constrained environments to enable real-time reasoning segmentation.

</details>


### [107] [Changes in Real Time: Online Scene Change Detection with Multi-View Fusion](https://arxiv.org/abs/2511.12370)
*Chamuditha Jayanga Galappaththige,Jason Lai,Lloyd Windrim,Donald Dansereau,Niko Sünderhauf,Dimity Miller*

Main category: cs.CV

TL;DR: First online scene change detection method that is pose-agnostic, label-free, and multi-view consistent, running at over 10 FPS and achieving state-of-the-art performance that surpasses offline approaches. It uses a self-supervised fusion loss, PnP-based pose estimation against a reference scene, and a fast change-guided update for a 3D Gaussian Splatting representation.


<details>
  <summary>Details</summary>
Motivation: Close the gap between online and offline scene change detection by removing dependence on known poses and labels, while enforcing multi-view consistency and real-time performance.

Method: Introduce a self-supervised fusion loss to combine cues from multiple viewpoints; use PnP-based fast pose estimation relative to a reference scene; implement a fast change-guided update strategy for a 3D Gaussian Splatting scene representation.

Result: The approach achieves state-of-the-art performance on real-world datasets, outperforming both online and offline baselines, and runs at over 10 FPS.

Conclusion: Demonstrates that online, pose-agnostic, label-free, multi-view-consistent SCD is feasible with self-supervised fusion cues and efficient 3D representation updates, enabling practical real-time deployment.

Abstract: Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.

</details>


### [108] [Reasoning Text-to-Video Retrieval via Digital Twin Video Representations and Large Language Models](https://arxiv.org/abs/2511.12371)
*Yiqing Shen,Chenxiao Fan,Chenjia Li,Mathias Unberath*

Main category: cs.CV

TL;DR: A two-stage reasoning-based text-to-video retrieval framework using digital twins for object-level grounding and long-horizon video reasoning; achieves strong results on ReasonT2VBench and standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-video retrieval methods excel at explicit queries but struggle with implicit, reasoning-based queries and lack fine-grained grounding. This work aims to enable implicit query understanding and grounding via structured representations and LLM reasoning.

Method: Represent video content as digital twins—structured scene representations that decompose salient objects via specialist vision models. Two-stage framework: (1) compositional alignment between decomposed sub-queries and digital twin representations for candidate identification; (2) large language model-based reasoning with just-in-time refinement that invokes additional specialist models to address information gaps. Provides object-level grounding masks.

Result: Introduces ReasonT2VBench-135 (447 manually created implicit queries with 135 videos) and ReasonT2VBench-1000 (1000 videos). Reported results: 81.2% R@1 on ReasonT2VBench-135, outperforming the strongest baseline by >50 percentage points; 81.7% R@1 on the extended configuration. Also achieves state-of-the-art results on MSR-VTT, MSVD, and VATEX.

Conclusion: The digital-twin plus LLM reasoning framework enables effective reasoning-enabled text-to-video retrieval with explicit grounding, achieving strong cross-dataset performance and offering a viable path for implicit-query video search.

Abstract: The goal of text-to-video retrieval is to search large databases for relevant videos based on text queries. Existing methods have progressed to handling explicit queries where the visual content of interest is described explicitly; however, they fail with implicit queries where identifying videos relevant to the query requires reasoning. We introduce reasoning text-to-video retrieval, a paradigm that extends traditional retrieval to process implicit queries through reasoning while providing object-level grounding masks that identify which entities satisfy the query conditions. Instead of relying on vision-language models directly, we propose representing video content as digital twins, i.e., structured scene representations that decompose salient objects through specialist vision models. This approach is beneficial because it enables large language models to reason directly over long-horizon video content without visual token compression. Specifically, our two-stage framework first performs compositional alignment between decomposed sub-queries and digital twin representations for candidate identification, then applies large language model-based reasoning with just-in-time refinement that invokes additional specialist models to address information gaps. We construct a benchmark of 447 manually created implicit queries with 135 videos (ReasonT2VBench-135) and another more challenging version of 1000 videos (ReasonT2VBench-1000). Our method achieves 81.2% R@1 on ReasonT2VBench-135, outperforming the strongest baseline by greater than 50 percentage points, and maintains 81.7% R@1 on the extended configuration while establishing state-of-the-art results in three conventional benchmarks (MSR-VTT, MSVD, and VATEX).

</details>


### [109] [AGGRNet: Selective Feature Extraction and Aggregation for Enhanced Medical Image Classification](https://arxiv.org/abs/2511.12382)
*Ansh Makwe,Akansh Agrawal,Prateek Jain,Akshan Agrawal,Priyanka Bagade*

Main category: cs.CV

TL;DR: AGGRNet improves fine-grained medical image classification by separating informative and non-informative features to better capture subtle inter-class differences and intra-class variability, achieving state-of-the-art results (up to 5% improvement on Kvasir).


<details>
  <summary>Details</summary>
Motivation: Medical image classification faces subtle class similarities, high intra-class variability, limited labels, and inconsistent expert interpretations. Existing attention-based models often fail to distinguish subtle classes, motivating a framework that focuses on informative versus non-informative feature extraction to enhance discrimination.

Method: Proposes AGGRNet, a framework that explicitly extracts and leverages informative and non-informative features to model fine-grained visual patterns and improve classification in complex medical image analysis tasks.

Result: Demonstrates state-of-the-art performance on multiple medical imaging datasets, with the best improvement up to 5% over SOTA models on the Kvasir dataset.

Conclusion: AGGRNet effectively addresses inter-class similarity and intra-class variability in medical image classification, offering a robust approach for complex tasks and showing strong generalization across datasets.

Abstract: Medical image analysis for complex tasks such as severity grading and disease subtype classification poses significant challenges due to intricate and similar visual patterns among classes, scarcity of labeled data, and variability in expert interpretations. Despite the usefulness of existing attention-based models in capturing complex visual patterns for medical image classification, underlying architectures often face challenges in effectively distinguishing subtle classes since they struggle to capture inter-class similarity and intra-class variability, resulting in incorrect diagnosis. To address this, we propose AGGRNet framework to extract informative and non-informative features to effectively understand fine-grained visual patterns and improve classification for complex medical image analysis tasks. Experimental results show that our model achieves state-of-the-art performance on various medical imaging datasets, with the best improvement up to 5% over SOTA models on the Kvasir dataset.

</details>


### [110] [Leveraging Quantum-Based Architectures for Robust Diagnostics](https://arxiv.org/abs/2511.12386)
*Shabnam Sodagari,Tommy Long*

Main category: cs.CV

TL;DR: Hybrid quantum-classical framework for diagnosing and differentiating kidney stones, cysts, and tumors from CT images using a ResNet50 encoder and a Quantum Convolutional Neural Network (QCNN); achieves 99% test accuracy with the 12-qubit variant offering superior recall/precision, including perfect cyst recall and a tumor F1-score of 0.9956.


<details>
  <summary>Details</summary>
Motivation: To improve diagnostic performance for differentiating kidney pathologies on CT images by leveraging quantum-assisted learning, addressing class imbalance, and evaluating how quantum circuits can complement deep feature extraction.

Method: Preprocess kidney CT images with denoising and CLAHE; address class imbalance with data augmentation and weighted sampling. Use a pretrained ResNet50 encoder to extract latent features, then map these features to qubits via angle encoding. Process with a QCNN, evaluating both 8-qubit and 12-qubit configurations. Assess convergence, train/validation consistency, and performance via test accuracy and confusion matrix.

Result: Test accuracy of 0.99 for both 8- and 12-qubit configurations, with the 12-qubit variant providing improvements in recall and precision, especially for Cyst and Tumor detection. Achieved perfect recall for Cysts and a tumor F1-score of 0.9956. Confusion matrix indicates reliable, low misclassification across all classes; rapid convergence and stable learning curves.

Conclusion: The study demonstrates that integrating classical preprocessing and deep feature extraction with quantum circuits enhances medical diagnosis performance, with the 12-qubit QCNN offering superior results for challenging classes (cysts and tumors).

Abstract: The objective of this study is to diagnose and differentiate kidney stones, cysts, and tumors using Computed Tomography (CT) images of the kidney. This study leverages a hybrid quantum-classical framework in this regard. We combine a pretrained ResNet50 encoder, with a Quantum Convolutional Neural Network (QCNN) to explore quantum-assisted diagnosis. We pre-process the kidney images using denoising and contrast limited adaptive histogram equalization to enhance feature extraction. We address class imbalance through data augmentation and weighted sampling. Latent features extracted by the encoder are transformed into qubits via angle encoding and processed by a QCNN. The model is evaluated on both 8-qubit and 12-qubit configurations. Both architectures achieved rapid convergence with stable learning curves and high consistency between training and validation performance. The models reached a test accuracy of 0.99, with the 12-qubit configuration providing improvements in overall recall and precision, particularly for Cyst and Tumor detection, where it achieved perfect recall for Cysts and a tumor F1-score of 0.9956. Confusion matrix analysis further confirmed reliable classification behavior across all classes, with very few misclassifications. Results demonstrate that integrating classical pre-processing and deep feature extraction with quantum circuits enhances medical diagnostic performance.

</details>


### [111] [Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation](https://arxiv.org/abs/2511.12389)
*Divake Kumar,Patrick Poggi,Sina Tayebati,Devashri Naik,Nilesh Ahuja,Amit Ranjan Trivedi*

Main category: cs.CV

TL;DR: Proposes Uncertainty-Guided Inference-Time Selection, a lightweight framework that disentangles aleatoric and three epistemic uncertainties in deep feature space to guide adaptive model selection; yields tighter calibration and ~60% compute savings on MOT17 with negligible accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Current estimators collapse all uncertainty into a single confidence score, hindering reliable budgeting of compute and decision-making during inference; separating uncertainty sources enables self-regulating, efficient inference.

Method: Estimate aleatoric uncertainty via a regularized global density model. Form epistemic uncertainty from three complementary, empirically orthogonal components: local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. These components require no sampling, no ensembling, and no extra forward passes. Integrate the decomposed uncertainty into a distribution-free conformal calibration to produce tighter, properly-calibrated prediction intervals. Use the uncertainty decomposition for adaptive model selection to reduce compute.

Result: Achieves significantly tighter prediction intervals at matched coverage. Reduces compute by ~60% on MOT17 with negligible accuracy loss. Ablation shows the orthogonal uncertainty decomposition yields higher computational savings across all MOT17 sequences, improving margins by 13.6 percentage points over a total-uncertainty baseline.

Conclusion: The proposed method enables practical self-regulating visual inference by providing reliable, decomposed uncertainty estimates that guide adaptive computation without extra passes or ensembles.

Abstract: Most estimators collapse all uncertainty modes into a single confidence score, preventing reliable reasoning about when to allocate more compute or adjust inference. We introduce Uncertainty-Guided Inference-Time Selection, a lightweight inference time framework that disentangles aleatoric (data-driven) and epistemic (model-driven) uncertainty directly in deep feature space. Aleatoric uncertainty is estimated using a regularized global density model, while epistemic uncertainty is formed from three complementary components that capture local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. These components are empirically orthogonal and require no sampling, no ensembling, and no additional forward passes. We integrate the decomposed uncertainty into a distribution free conformal calibration procedure that yields significantly tighter prediction intervals at matched coverage. Using these components for uncertainty guided adaptive model selection reduces compute by approximately 60 percent on MOT17 with negligible accuracy loss, enabling practical self regulating visual inference. Additionally, our ablation results show that the proposed orthogonal uncertainty decomposition consistently yields higher computational savings across all MOT17 sequences, improving margins by 13.6 percentage points over the total-uncertainty baseline.

</details>


### [112] [MSLoRA: Multi-Scale Low-Rank Adaptation via Attention Reweighting](https://arxiv.org/abs/2511.12400)
*Xu Yang,Gady Agam*

Main category: cs.CV

TL;DR: MSLoRA is a backbone-agnostic, parameter-efficient adapter that reweights feature responses for CNNs and ViTs by combining a low-rank projection with a multi-scale nonlinear transform, fused via elementwise multiplication and a residual, while keeping pretrained weights frozen.


<details>
  <summary>Details</summary>
Motivation: To enable universal, efficient adaptation across diverse vision architectures without re-tuning backbones, addressing limitations of existing low-rank adapters which mainly target ViTs and lack cross-architecture generalization.

Method: Introduce two components—a low-rank linear projection that reweights features and a multi-scale nonlinear transformation that modulates spatial and channel attention—that are fused via elementwise multiplication and a residual connection. The module is lightweight and designed to be architecture-agnostic, with frozen backbones.

Result: MSLoRA consistently improves transfer performance across classification, detection, and segmentation tasks using roughly less than 5% of backbone parameters, while providing stable optimization, fast convergence, and strong cross-architecture generalization.

Conclusion: MSLoRA offers a simple, universal approach for efficient adaptation of frozen vision backbones by reweighting feature responses rather than re-tuning, enabling effective transfer learning across CNNs and ViTs with minimal parameter overhead.

Abstract: We introduce MSLoRA, a backbone-agnostic, parameter-efficient adapter that reweights feature responses rather
  than re-tuning the underlying backbone. Existing low-rank adaptation methods are mostly confined to vision
  transformers (ViTs) and struggle to generalize across architectures. MSLoRA unifies adaptation for both convolutional neural networks (CNNs) and
  ViTs by combining a low-rank linear projection with a multi-scale nonlinear transformation that jointly
  modulates spatial and channel attention. The two components are fused through pointwise multiplication and
  a residual connection, yielding a lightweight module that shifts feature attention while keeping pretrained
  weights frozen.
  Extensive experiments demonstrate that MSLoRA consistently improves transfer performance on classification,
  detection, and segmentation tasks with roughly less than 5\% of backbone parameters.
  The design further enables stable optimization, fast convergence, and strong cross-architecture
  generalization. By reweighting rather than re-tuning, MSLoRA provides a simple and universal approach
  for efficient adaptation of frozen vision backbones.

</details>


### [113] [VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving](https://arxiv.org/abs/2511.12405)
*Hyunki Seong,Seongwoo Moon,Hojin Ahn,Jehun Kang,David Hyunchul Shim*

Main category: cs.CV

TL;DR: A framework (VLA-R) for open-world end-to-end autonomous driving that combines open-world perception using a frozen vision-language model with a vision-action retrieval mechanism. It uses a Q-Former bottleneck to fuse visual features into language-aligned representations and a vision-action contrastive objective to align vision-language and action embeddings, enabling generalization to unstructured, unseen environments with limited data on a real robotic platform.


<details>
  <summary>Details</summary>
Motivation: End-to-end autonomous driving in unstructured, open-world settings requires strong generalization to unseen conditions. Traditional training often fails to generalize to novel scenarios; there is a need to integrate open-world perception (capable of handling diverse, unseen objects and scenes) with end-to-end decision-making that can retrieve appropriate actions from a learned space.

Method: Leverage a frozen vision-language model for open-world detection/segmentation to produce multi-scale, prompt-guided perception features without domain-specific tuning. Use a Q-Former bottleneck to aggregate fine-grained visual representations with language-aligned visual features, bridging perception and action. Introduce a vision-action contrastive learning objective to align vision-language embeddings with action embeddings, enabling action retrieval conditioned on visual-language cues.

Result: Experiments on a real-world robotic platform show strong generalization and exploratory performance in unstructured, unseen environments, even with limited data. Demo videos are provided in supplementary material.

Conclusion: VLA-R demonstrates that open-world end-to-end driving is achievable by combining open-world perception with a vision-action retrieval framework and a cross-modal contrastive objective, enabling better generalization to unseen environments with limited labeled data.

Abstract: Exploring open-world situations in an end-to-end manner is a promising yet challenging task due to the need for strong generalization capabilities. In particular, end-to-end autonomous driving in unstructured outdoor environments often encounters conditions that were unfamiliar during training. In this work, we present Vision-Language Action Retrieval (VLA-R), an open-world end-to-end autonomous driving (OW-E2EAD) framework that integrates open-world perception with a novel vision-action retrieval paradigm. We leverage a frozen vision-language model for open-world detection and segmentation to obtain multi-scale, prompt-guided, and interpretable perception features without domain-specific tuning. A Q-Former bottleneck aggregates fine-grained visual representations with language-aligned visual features, bridging perception and action domains. To learn transferable driving behaviors, we introduce a vision-action contrastive learning scheme that aligns vision-language and action embeddings for effective open-world reasoning and action retrieval. Our experiments on a real-world robotic platform demonstrate strong generalization and exploratory performance in unstructured, unseen environments, even with limited data. Demo videos are provided in the supplementary material.

</details>


### [114] [Self-Supervised Visual Prompting for Cross-Domain Road Damage Detection](https://arxiv.org/abs/2511.12410)
*Xi Xiao,Zhuxuanzi Wang,Mingqiao Mo,Chen Liu,Chenrui Ma,Yanshu Li,Smita Krishnaswamy,Xiao Wang,Tianyang Wang*

Main category: cs.CV

TL;DR: PROBE is a self-supervised prompting framework for cross-domain pavement defect detection that learns target-aware prompts (SPEM) to guide a frozen ViT and aligns prompt-conditioned source/target representations via DAPA, achieving strong zero-shot and few-shot transfer with robustness to domain shifts.


<details>
  <summary>Details</summary>
Motivation: Cross-domain generalization in automated pavement defect detection is poor; supervised models require costly re-annotation for new environments, and standard self-supervised methods often still suffer from domain shift.

Method: SPEM derives defect-aware prompts from unlabeled target data to guide a frozen Vision Transformer backbone, while DAPA aligns prompt-conditioned representations between source and target domains to enable effective transfer without labels.

Result: On four challenging benchmarks, PROBE consistently outperforms strong supervised, self-supervised, and adaptation baselines, delivering robust zero-shot transfer, improved resilience to domain variations, and high data efficiency in few-shot adaptation.

Conclusion: Self-supervised prompting is a practical and scalable direction for adaptive visual inspection systems; source code is publicly available.

Abstract: The deployment of automated pavement defect detection is often hindered by poor cross-domain generalization. Supervised detectors achieve strong in-domain accuracy but require costly re-annotation for new environments, while standard self-supervised methods capture generic features and remain vulnerable to domain shift. We propose \ours, a self-supervised framework that \emph{visually probes} target domains without labels. \ours introduces a Self-supervised Prompt Enhancement Module (SPEM), which derives defect-aware prompts from unlabeled target data to guide a frozen ViT backbone, and a Domain-Aware Prompt Alignment (DAPA) objective, which aligns prompt-conditioned source and target representations. Experiments on four challenging benchmarks show that \ours consistently outperforms strong supervised, self-supervised, and adaptation baselines, achieving robust zero-shot transfer, improved resilience to domain variations, and high data efficiency in few-shot adaptation. These results highlight self-supervised prompting as a practical direction for building scalable and adaptive visual inspection systems. Source code is publicly available: https://github.com/xixiaouab/PROBE/tree/main

</details>


### [115] [Towards Rotation-only Imaging Geometry: Rotation Estimation](https://arxiv.org/abs/2511.12415)
*Xinrui Li,Qi Cai,Yuanxin Wu*

Main category: cs.CV

TL;DR: A rotation-only SfM optimization is proposed by expressing translation as a function of rotation, condensing the imaging geometry to the rotation manifold and optimizing via reprojection error for two-view and multi-view setups, achieving higher accuracy and robustness than existing rotation methods and approaching bundle adjustment performance.


<details>
  <summary>Details</summary>
Motivation: To leverage pose-only imaging geometry in SfM by explicitly characterizing the interdependence of rotation, translation, and structure, and to enhance accuracy and robustness while simplifying the optimization to the rotation manifold.

Method: Express translation in terms of rotation to reduce the problem to a rotation-only manifold; formulate a reprojection-error-based optimization for both two-view and multi-view scenarios; validate against state-of-the-art rotation estimation methods with comparisons to bundle adjustment iterations.

Result: Demonstrates superior accuracy and robustness compared with current leading rotation-estimation approaches, with performance comparable to multiple iterations of bundle adjustment.

Conclusion: Proposes a rotation-only SfM framework that leverages translation-rotation relationships to achieve accurate, efficient, and robust 3D visual computing; encourages further exploration of pose-only formulations.

Abstract: Structure from Motion (SfM) is a critical task in computer vision, aiming to recover the 3D scene structure and camera motion from a sequence of 2D images. The recent pose-only imaging geometry decouples 3D coordinates from camera poses and demonstrates significantly better SfM performance through pose adjustment. Continuing the pose-only perspective, this paper explores the critical relationship between the scene structures, rotation and translation. Notably, the translation can be expressed in terms of rotation, allowing us to condense the imaging geometry representation onto the rotation manifold. A rotation-only optimization framework based on reprojection error is proposed for both two-view and multi-view scenarios. The experiment results demonstrate superior accuracy and robustness performance over the current state-of-the-art rotation estimation methods, even comparable to multiple bundle adjustment iteration results. Hopefully, this work contributes to even more accurate, efficient and reliable 3D visual computing.

</details>


### [116] [Seeing Through the Rain: Resolving High-Frequency Conflicts in Deraining and Super-Resolution via Diffusion Guidance](https://arxiv.org/abs/2511.12419)
*Wenjie Li,Jinglei Shi,Jin Han,Heng Guo,Zhanyu Ma*

Main category: cs.CV

TL;DR: DHGM: a diffusion-based high-frequency guided model that jointly removes rain and enhances high-frequency details for clean, high-resolution images by integrating pre-trained diffusion priors with high-pass filtering, addressing the conflict between weather removal and SR.


<details>
  <summary>Details</summary>
Motivation: Clean, high-resolution images are essential for visual tasks like small object detection. Weather degradation degrades details; sequential denoising then SR often loses high-frequency information, so there is a need for an integrated approach that preserves and reconstructs fine structures.

Method: DHGM integrates pre-trained diffusion priors with high-pass filters to simultaneously remove rain artifacts and enhance structural details. The method leverages diffusion priors to guide restoration and uses high-pass information to preserve textures, aiming to bridge the gap between deraining and super-resolution.

Result: Extensive experiments show that DHGM achieves superior performance compared to existing methods while also reducing computational costs.

Conclusion: DHGM effectively reconciles rain removal and high-frequency detail restoration, yielding clean, high-resolution images with improved fidelity and efficiency.

Abstract: Clean images are crucial for visual tasks such as small object detection, especially at high resolutions. However, real-world images are often degraded by adverse weather, and weather restoration methods may sacrifice high-frequency details critical for analyzing small objects. A natural solution is to apply super-resolution (SR) after weather removal to recover both clarity and fine structures. However, simply cascading restoration and SR struggle to bridge their inherent conflict: removal aims to remove high-frequency weather-induced noise, while SR aims to hallucinate high-frequency textures from existing details, leading to inconsistent restoration contents. In this paper, we take deraining as a case study and propose DHGM, a Diffusion-based High-frequency Guided Model for generating clean and high-resolution images. DHGM integrates pre-trained diffusion priors with high-pass filters to simultaneously remove rain artifacts and enhance structural details. Extensive experiments demonstrate that DHGM achieves superior performance over existing methods, with lower costs.

</details>


### [117] [MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation](https://arxiv.org/abs/2511.12422)
*Nuolin Sun,Linyuan Wang,Haonan Wei,Lei Li,Bin Yan*

Main category: cs.CV

TL;DR: MeanFlow-Incubated ResNet (MFI-ResNet) reduces parameter count by about 46% with small accuracy gains on CIFAR-10/100 by replacing/stacking ResNet blocks with MeanFlow modules in a compression-expansion framework, showing a link between generative flow-fields and discriminative learning.


<details>
  <summary>Details</summary>
Motivation: Improve parameter efficiency of ResNet while exploring a generative-flow perspective of feature transformations.

Method: Compression phase reduces per-stage depth to 1-2 MeanFlow modules; expansion phase reintroduces ResNet-like blocks in the first three stages while keeping the last stage as MeanFlow; fine-tuning

Result: Parameter reduction: 46.28% (CIFAR-10) and 45.59% (CIFAR-100) vs ResNet-50; accuracy gains: +0.23% (CIFAR-10) and +0.17% (CIFAR-100).

Conclusion: Generative flow-fields can effectively characterize ResNet feature transformations and bridge generative modeling with discriminative learning.

Abstract: ResNet has achieved tremendous success in computer vision through its residual connection mechanism. ResNet can be viewed as a discretized form of ordinary differential equations (ODEs). From this perspective, the multiple residual blocks within a single ResNet stage essentially perform multi-step discrete iterations of the feature transformation for that stage. The recently proposed flow matching model, MeanFlow, enables one-step generative modeling by learning the mean velocity field to transform distributions. Inspired by this, we propose MeanFlow-Incubated ResNet (MFI-ResNet), which employs a compression-expansion strategy to jointly improve parameter efficiency and discriminative performance. In the compression phase, we simplify the multi-layer structure within each ResNet stage to one or two MeanFlow modules to construct a lightweight meta model. In the expansion phase, we apply a selective incubation strategy to the first three stages, expanding them to match the residual block configuration of the baseline ResNet model, while keeping the last stage in MeanFlow form, and fine-tune the incubated model. Experimental results show that on CIFAR-10 and CIFAR-100 datasets, MFI-ResNet achieves remarkable parameter efficiency, reducing parameters by 46.28% and 45.59% compared to ResNet-50, while still improving accuracy by 0.23% and 0.17%, respectively. This demonstrates that generative flow-fields can effectively characterize the feature transformation process in ResNet, providing a new perspective for understanding the relationship between generative modeling and discriminative learning.

</details>


### [118] [RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models Inference via Masked Token-Guided Visual Token Pruning](https://arxiv.org/abs/2511.12428)
*Jingqi Xu,Jingxi Lu,Chenghao Li,Sreetama Sarkar,Souvik Kundu,Peter A. Beerel*

Main category: cs.CV

TL;DR: RedVTP proposes a response-driven visual token pruning strategy for diffusion vision-language models to improve inference efficiency without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: DVLMs suffer from high computational costs due to many visual tokens; pruning methods exist for autoregressive VLMs but DVLMs lack effective token pruning approaches. A fast, dynamic pruning method tailored to DVLM inference could reduce latency and increase throughput.

Method: Estimate visual token importance using attention from masked response tokens. Observe that importance scores remain stable across inference steps. After the first inference step, prune the less important visual tokens from the masked tokens, reducing computation while preserving essential information. Applied to DVLMs like LLaDA-V and LaViDa; evaluate token generation throughput and latency.

Result: Token generation throughput improved up to 186% for LLaDA-V and 28.05% for LaViDa; inference latency reduced up to 64.97% and 21.87%, respectively. Accuracy is preserved and in some cases improved.

Conclusion: RedVTP demonstrates that response-driven visual token pruning can substantially speed up DVLM inference with minimal or positive impact on accuracy, by leveraging stable token importance scores from masked responses and pruning after the first step.

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning and generation, yet their high computational demands remain a major challenge. Diffusion Vision-Language Models (DVLMs) are particularly attractive because they enable parallel token decoding, but the large number of visual tokens still significantly hinders their inference efficiency. While visual token pruning has been extensively studied for autoregressive VLMs (AVLMs), it remains largely unexplored for DVLMs. In this work, we propose RedVTP, a response-driven visual token pruning strategy that leverages the inference dynamics of DVLMs. Our method estimates visual token importance using attention from the masked response tokens. Based on the observation that these importance scores remain consistent across steps, RedVTP prunes the less important visual tokens from the masked tokens after the first inference step, thereby maximizing inference efficiency. Experiments show that RedVTP improves token generation throughput of LLaDA-V and LaViDa by up to 186% and 28.05%, respectively, and reduces inference latency by up to 64.97% and 21.87%, without compromising-and in some cases improving-accuracy.

</details>


### [119] [Text-Guided Channel Perturbation and Pretrained Knowledge Integration for Unified Multi-Modality Image Fusion](https://arxiv.org/abs/2511.12432)
*Xilai Li,Xiaosong Li,Weijun Jiang*

Main category: cs.CV

TL;DR: UP-Fusion: a unified multi-modality image fusion framework with Semantic-Aware Channel Pruning Module (SCPM), Geometric Affine Modulation Module (GAM), and Text-Guided Channel Perturbation Module (TCPM); achieves superior fusion quality and generalization.


<details>
  <summary>Details</summary>
Motivation: Large modality differences cause gradient conflicts in unified models; modality-specific encoders improve fusion but harm generalization. A unified approach leveraging pre-trained knowledge and channel perturbation can suppress redundant information and preserve modality discriminability.

Method: Propose SCPM to prune channels via semantic cues from pre-trained models; GAM applies affine transformations on fused features using original modal features to preserve discriminability; TCPM perturbs channel distribution during decoding guided by text, reducing dependence on modality-specific channels; overall architecture aims to be modality-agnostic while retaining essential features.

Result: Extensive experiments show UP-Fusion outperforms existing methods on multi-modality image fusion and downstream tasks.

Conclusion: Unified framework with SCPM, GAM, and TCPM provides better fusion quality and generalization across tasks by integrating pre-trained knowledge, semantic channel pruning, and guided perturbations.

Abstract: Multi-modality image fusion enhances scene perception by combining complementary information. Unified models aim to share parameters across modalities for multi-modality image fusion, but large modality differences often cause gradient conflicts, limiting performance. Some methods introduce modality-specific encoders to enhance feature perception and improve fusion quality. However, this strategy reduces generalisation across different fusion tasks. To overcome this limitation, we propose a unified multi-modality image fusion framework based on channel perturbation and pre-trained knowledge integration (UP-Fusion). To suppress redundant modal information and emphasize key features, we propose the Semantic-Aware Channel Pruning Module (SCPM), which leverages the semantic perception capability of a pre-trained model to filter and enhance multi-modality feature channels. Furthermore, we proposed the Geometric Affine Modulation Module (GAM), which uses original modal features to apply affine transformations on initial fusion features to maintain the feature encoder modal discriminability. Finally, we apply a Text-Guided Channel Perturbation Module (TCPM) during decoding to reshape the channel distribution, reducing the dependence on modality-specific channels. Extensive experiments demonstrate that the proposed algorithm outperforms existing methods on both multi-modality image fusion and downstream tasks.

</details>


### [120] [Real-Time Drivers' Drowsiness Detection and Analysis through Deep Learning](https://arxiv.org/abs/2511.12438)
*ANK Zaman,Prosenjit Chatterjee,Rajat Sharma*

Main category: cs.CV

TL;DR: Real-time driver drowsiness detection using DCNNs and OpenCV, achieving 99.6% accuracy on NTHU-DDD and 97% on Yawn-Eye-Dataset.


<details>
  <summary>Details</summary>
Motivation: Drowsiness during long drives poses serious safety risks and can trigger fatal accidents. A real-time, non-invasive detection system is needed to alert drivers and prevent harm.

Method: Capture real-time facial images from a live camera. Use OpenCV to extract facial landmarks (eye opening, yawning). Apply a pre-trained DCNN model to classify drowsiness based on landmarks. Evaluate on NTHU-DDD and Yawn-Eye-Dataset; provide a real-time alert embedded in Smart Car technology.

Result: Drowsiness detection achieved 99.6% accuracy on NTHU-DDD and 97% on Yawn-Eye-Dataset.

Conclusion: Proposes a non-invasive, inexpensive, real-time drowsiness detection system embedded in Smart Car technology with potential to save lives by promptly alerting fatigued drivers.

Abstract: A long road trip is fun for drivers. However, a long drive for days can be tedious for a driver to accommodate stringent deadlines to reach distant destinations. Such a scenario forces drivers to drive extra miles, utilizing extra hours daily without sufficient rest and breaks. Once a driver undergoes such a scenario, it occasionally triggers drowsiness during driving. Drowsiness in driving can be life-threatening to any individual and can affect other drivers' safety; therefore, a real-time detection system is needed. To identify fatigued facial characteristics in drivers and trigger the alarm immediately, this research develops a real-time driver drowsiness detection system utilizing deep convolutional neural networks (DCNNs) and OpenCV.Our proposed and implemented model takes real- time facial images of a driver using a live camera and utilizes a Python-based library named OpenCV to examine the facial images for facial landmarks like sufficient eye openings and yawn-like mouth movements. The DCNNs framework then gathers the data and utilizes a per-trained model to detect the drowsiness of a driver using facial landmarks. If the driver is identified as drowsy, the system issues a continuous alert in real time, embedded in the Smart Car technology.By potentially saving innocent lives on the roadways, the proposed technique offers a non-invasive, inexpensive, and cost-effective way to identify drowsiness. Our proposed and implemented DCNNs embedded drowsiness detection model successfully react with NTHU-DDD dataset and Yawn-Eye-Dataset with drowsiness detection classification accuracy of 99.6% and 97% respectively.

</details>


### [121] [CoTBox-TTT: Grounding Medical VQA with Visual Chain-of-Thought Boxes During Test-time Training](https://arxiv.org/abs/2511.12446)
*Jiahe Qian,Yuhao Shen,Zhangtianyi Chen,Juexiao Zhou,Peisong Wang*

Main category: cs.CV

TL;DR: CoTBox-TTT is an evidence-first test-time training method for medical VQA that freezes model backbones and updates soft prompts to identify question-relevant regions and enforce answer consistency, achieving notable accuracy gains (12.3%) on pathVQA without labels.


<details>
  <summary>Details</summary>
Motivation: Medical VQA systems struggle with domain shift and weak grounding; retraining or collecting new labels is impractical at deployment. A lightweight, label-free adaptation that improves reliability and evidence-grounded answers is needed.

Method: Inference-time adaptation that freezes vision-language backbones and only updates a small set of continuous soft prompts. It uses a visual chain-of-thought signal to identify question-relevant regions and enforces answer consistency between the original image and a localized crop. The procedure is label-free and plug-and-play with diverse backbones.

Result: In medical VQA experiments, CoTBox-TTT improves practical deployment performance, exemplified by a 12.3% increase in closed-ended accuracy on pathVQA when added to LLaVA.

Conclusion: CoTBox-TTT offers a practical, lightweight approach to improve reliability and grounding in medical VQA through test-time, evidence-focused prompts, with demonstrated gains and broad compatibility across backbones.

Abstract: Medical visual question answering could support clinical decision making, yet current systems often fail under domain shift and produce answers that are weakly grounded in image evidence. This reliability gap arises when models attend to spurious regions and when retraining or additional labels are impractical at deployment time. We address this setting with CoTBox-TTT, an evidence-first test-time training approach that adapts a vision-language model at inference while keeping all backbones frozen. The method updates only a small set of continuous soft prompts. It identifies question-relevant regions through a visual chain-of-thought signal and encourages answer consistency across the original image and a localized crop. The procedure is label free, and plug and play with diverse backbones. Experiments on medical VQA show that the approach is practical for real deployments. For instance, adding CoTBox-TTT to LLaVA increases closed-ended accuracy by 12.3% on pathVQA.

</details>


### [122] [MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2511.12449)
*Zhanheng Nie,Chenghan Fu,Daoze Zhang,Junxian Wu,Wanxian Guan,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: MOON2.0 provides a dynamic, modality-balanced multimodal framework for e-commerce product understanding, achieving state-of-the-art zero-shot performance via a Modality-driven MoE, Dual-level Alignment, and MLLM-based co-augmentation with dynamic sample filtering, plus the MBE2.0 benchmark.


<details>
  <summary>Details</summary>
Motivation: E-commerce multimodal models face modality imbalance from mixed training data, underexploited intra-product visual-textual alignment, and sensitivity to noisy data; addressing these gaps is crucial for robust product understanding.

Method: Introduce (1) Modality-driven Mixture-of-Experts (MoE) to adaptively process samples based on modality composition; (2) Dual-level Alignment to capture alignment properties at both coarse and fine levels within each product; (3) MLLM-based Image-text Co-augmentation with visual expansion and textual enrichment plus Dynamic Sample Filtering; and (4) a new evaluation benchmark MBE2.0 for co-augmented multimodal representation.

Result: MOON2.0 achieves state-of-the-art zero-shot performance on MBE2.0 and other public datasets; attention-heatmap visualization provides qualitative evidence of improved multimodal alignment.

Conclusion: MOON2.0 effectively mitigates modality imbalance, leverages intrinsic product alignment, and improves data quality through co-augmentation, advancing e-commerce multimodal representation learning; MBE2.0 enables standardized evaluation and future improvements.

Abstract: The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.

</details>


### [123] [DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions](https://arxiv.org/abs/2511.12452)
*Xiaoyu Lin,Aniket Ghorpade,Hansheng Zhu,Justin Qiu,Dea Rrozhani,Monica Lama,Mick Yang,Zixuan Bian,Ruohan Ren,Alan B. Hong,Jiatao Gu,Chris Callison-Burch*

Main category: cs.CV

TL;DR: DenseAnnotate introduces an audio-driven annotation platform to create dense, multilingual image/3D annotations; builds a large dataset; enables substantial gains in multilingual, cultural alignment, and 3D spatial capabilities for multimodal models.


<details>
  <summary>Details</summary>
Motivation: Current training data rely on sparse online annotations or manual typing that miss nuanced visual content. Dense, multilingual annotations are scarce, especially for multicultural imagery and 3D assets; text-based annotation limits expressiveness and speed.

Method: Annotators narrate observations aloud while linking spoken phrases to image regions or 3D scene parts. The platform uses speech-to-text and region-of-attention marking. Case studies with over 1,000 annotators in culturally diverse images and 3D scenes. A curated human-annotated multi-modal dataset comprising 3,531 images, 898 3D scenes, 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions.

Result: The dataset enabled training improvements: 5% in multilingual capability, 47% in cultural alignment, and 54% in 3D spatial understanding.

Conclusion: DenseAnnotate is a feasible approach for future vision–language research, applicable to various tasks and data types, particularly for culturally diverse and 3D-rich content.

Abstract: With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.

</details>


### [124] [Co-Layout: LLM-driven Co-optimization for Interior Layout](https://arxiv.org/abs/2511.12474)
*Chucheng Xiang,Ruchao Bao,Biyin Feng,Wenzheng Wu,Zhongyuan Liu,Yirui Guan,Ligang Liu*

Main category: cs.CV

TL;DR: A framework that merges LLM-extracted design constraints with grid-based integer programming for joint interior layout and furniture placement, using a Modulor-inspired grid and a coarse-to-fine optimization to boost solution quality and efficiency, outperforming two-stage pipelines.


<details>
  <summary>Details</summary>
Motivation: Automating interior design from natural language prompts while ensuring practical constraints (corridor connectivity, accessibility, spatial exclusivity) and user preferences, by unifying semantic prompts with rigorous optimization.

Method: An LLM-driven workflow extracts structured constraints from prompts, which are encoded into a unified grid-based representation (Modulor-inspired). The optimization enforces corridor connectivity, room accessibility, spatial exclusivity, and user preferences. A coarse-to-fine strategy starts on a low-resolution grid to solve a simplified problem and guides the full-resolution solution.

Result: Experiments across diverse scenarios show the joint optimization approach significantly improves solution quality compared to existing two-stage pipelines and achieves computational efficiency via coarse-to-fine optimization.

Conclusion: The work demonstrates a promising direction for automated interior design by tightly integrating natural language understanding with grid-based optimization, yielding higher-quality layouts with greater efficiency and adaptability to user needs.

Abstract: We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.

</details>


### [125] [MaskAnyNet: Rethinking Masked Image Regions as Valuable Information in Supervised Learning](https://arxiv.org/abs/2511.12480)
*Jingshan Hong,Haigen Hu,Huihuang Zhang,Qianwei Zhou,Zhao Li*

Main category: cs.CV

TL;DR: MaskAnyNet uses masked content as auxiliary knowledge via a relearning branch, enabling learning from both visible and masked regions; yields improved semantic diversity and performance across CNN/Transformer backbones.


<details>
  <summary>Details</summary>
Motivation: Traditional image masking discards information and can remove small/critical features; MIM shows masked regions hold semantic cues; need to exploit masked regions rather than ignore them.

Method: Introduce MaskAnyNet, add a relearning mechanism with an auxiliary branch that learns from recomposed masked regions; extendable to any model with an extra branch; leverages semantic diversity of masked content to enrich features and preserve fine-grained details.

Result: Empirical gains across multiple benchmarks on CNN and Transformer backbones; analysis shows increased semantic diversity via reuse of masked content.

Conclusion: Treating masked content as auxiliary knowledge improves representation quality and preserves fine-grained details; MaskAnyNet is generalizable to different architectures.

Abstract: In supervised learning, traditional image masking faces two key issues: (i) discarded pixels are underutilized, leading to a loss of valuable contextual information; (ii) masking may remove small or critical features, especially in fine-grained tasks. In contrast, masked image modeling (MIM) has demonstrated that masked regions can be reconstructed from partial input, revealing that even incomplete data can exhibit strong contextual consistency with the original image. This highlights the potential of masked regions as sources of semantic diversity. Motivated by this, we revisit the image masking approach, proposing to treat masked content as auxiliary knowledge rather than ignored. Based on this, we propose MaskAnyNet, which combines masking with a relearning mechanism to exploit both visible and masked information. It can be easily extended to any model with an additional branch to jointly learn from the recomposed masked region. This approach leverages the semantic diversity of the masked regions to enrich features and preserve fine-grained details. Experiments on CNN and Transformer backbones show consistent gains across multiple benchmarks. Further analysis confirms that the proposed method improves semantic diversity through the reuse of masked content.

</details>


### [126] [Towards Temporal Fusion Beyond the Field of View for Camera-based Semantic Scene Completion](https://arxiv.org/abs/2511.12498)
*Jongseong Bae,Junwoo Ha,Jinnyeong Heo,Yeongin Lee,Ha Young Kim*

Main category: cs.CV

TL;DR: Proposes C3DFusion, a temporal fusion module for 3D SSC that aligns current and past 3D features to recover hidden peripheral regions near the ego-vehicle. It uses historical context blurring and current-centric feature densification to suppress noise and boost current features, delivering state-of-the-art gains and strong cross-model generalization.


<details>
  <summary>Details</summary>
Motivation: Current camera-based 3D SSC methods rely on in-frame cues and temporal information but struggle to reconstruct out-of-frame regions near the ego-vehicle. Historical frames contain valuable contextual information about unseen areas that current methods underutilize.

Method: Introduce C3DFusion, which explicitly aligns 3D-lifted point features from current and historical frames to create hidden-region-aware 3D feature geometry. Employ two complementary techniques: historical context blurring (attenuates the scale of warped historical features to suppress noise) and current-centric feature densification (increases the volumetric contribution of current features). Integrate as a plug-in into standard SSC architectures.

Result: Significantly outperforms state-of-the-art methods on SemanticKITTI and SSCBench-KITTI-360 datasets. Demonstrates robust generalization with notable performance gains when applied to other baseline models.

Conclusion: C3DFusion provides a simple yet effective plug-in solution for enhancing SSC by leveraging temporal context to recover unseen regions, with strong empirical gains and cross-model applicability.

Abstract: Recent camera-based 3D semantic scene completion (SSC) methods have increasingly explored leveraging temporal cues to enrich the features of the current frame. However, while these approaches primarily focus on enhancing in-frame regions, they often struggle to reconstruct critical out-of-frame areas near the sides of the ego-vehicle, although previous frames commonly contain valuable contextual information about these unseen regions. To address this limitation, we propose the Current-Centric Contextual 3D Fusion (C3DFusion) module, which generates hidden region-aware 3D feature geometry by explicitly aligning 3D-lifted point features from both current and historical frames. C3DFusion performs enhanced temporal fusion through two complementary techniques-historical context blurring and current-centric feature densification-which suppress noise from inaccurately warped historical point features by attenuating their scale, and enhance current point features by increasing their volumetric contribution. Simply integrated into standard SSC architectures, C3DFusion demonstrates strong effectiveness, significantly outperforming state-of-the-art methods on the SemanticKITTI and SSCBench-KITTI-360 datasets. Furthermore, it exhibits robust generalization, achieving notable performance gains when applied to other baseline models.

</details>


### [127] [Visible Structure Retrieval for Lightweight Image-Based Relocalisation](https://arxiv.org/abs/2511.12503)
*Fereidoon Zangeneh,Leonard Bruns,Amit Dekel,Alessandro Pieropan,Patric Jensfelt*

Main category: cs.CV

TL;DR: Directly map an image to visible 3D map points with a neural network to prune 2D-3D correspondences, enabling tractable, accurate relocalisation with lower compute/storage.


<details>
  <summary>Details</summary>
Motivation: Traditional structure-based relocalisation relies on image retrieval or heuristics, which are heavy in large scenes; need scalable, compact methods.

Method: Propose visible structure retrieval network; forward pass from query image outputs subset of 3D points visible to image; reduces search space for correspondence search.

Result: Localization accuracy comparable to state-of-the-art while using lower computational and storage footprint.

Conclusion: A scalable paradigm for structure-based relocalisation, replacing image retrieval with a learned mapping to visible structure to reduce search space and resource needs without sacrificing accuracy.

Abstract: Accurate camera pose estimation from an image observation in a previously mapped environment is commonly done through structure-based methods: by finding correspondences between 2D keypoints on the image and 3D structure points in the map. In order to make this correspondence search tractable in large scenes, existing pipelines either rely on search heuristics, or perform image retrieval to reduce the search space by comparing the current image to a database of past observations. However, these approaches result in elaborate pipelines or storage requirements that grow with the number of past observations. In this work, we propose a new paradigm for making structure-based relocalisation tractable. Instead of relying on image retrieval or search heuristics, we learn a direct mapping from image observations to the visible scene structure in a compact neural network. Given a query image, a forward pass through our novel visible structure retrieval network allows obtaining the subset of 3D structure points in the map that the image views, thus reducing the search space of 2D-3D correspondences. We show that our proposed method enables performing localisation with an accuracy comparable to the state of the art, while requiring lower computational and storage footprint.

</details>


### [128] [DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection](https://arxiv.org/abs/2511.12511)
*Jialiang Shen,Jiyang Zheng,Yunqi Xue,Huajie Chen,Yu Yao,Hui Kang,Ruiqi Liu,Helin Gong,Yang Yang,Dadong Wang,Tongliang Liu*

Main category: cs.CV

TL;DR: A blur-robust AIGI detector that uses teacher-student distillation: a frozen DINOv3 teacher trained on sharp images guides a student trained on blurred images, yielding state-of-the-art detection under motion blur and clean conditions.


<details>
  <summary>Details</summary>
Motivation: Current AIGI detectors degrade significantly under real-world degradations like motion blur, hindering practical deployment. A robust method that maintains performance across sharp and blurred inputs is needed.

Method: A high-capacity teacher (DINOv3) is trained on clean images and frozen to preserve generalization. Its feature and logit responses are distilled into a student that is trained on blurred counterparts, enabling the student to produce stable representations under motion degradation.

Result: Extensive experiments demonstrate state-of-the-art performance under both motion-blurred and clean conditions, with improved generalization and real-world applicability.

Conclusion: The approach shows that blur-robust AIGI detection is achievable via teacher-student distillation, and the authors plan to release source code for broader use.

Abstract: With growing concerns over image authenticity and digital safety, the field of AI-generated image (AIGI) detection has progressed rapidly. Yet, most AIGI detectors still struggle under real-world degradations, particularly motion blur, which frequently occurs in handheld photography, fast motion, and compressed video. Such blur distorts fine textures and suppresses high-frequency artifacts, causing severe performance drops in real-world settings. We address this limitation with a blur-robust AIGI detection framework based on teacher-student knowledge distillation. A high-capacity teacher (DINOv3), trained on clean (i.e., sharp) images, provides stable and semantically rich representations that serve as a reference for learning. By freezing the teacher to maintain its generalization ability, we distill its feature and logit responses from sharp images to a student trained on blurred counterparts, enabling the student to produce consistent representations under motion degradation. Extensive experiments benchmarks show that our method achieves state-of-the-art performance under both motion-blurred and clean conditions, demonstrating improved generalization and real-world applicability. Source codes will be released at: https://github.com/JiaLiangShen/Dino-Detect-for-blur-robust-AIGC-Detection.

</details>


### [129] [MdaIF: Robust One-Stop Multi-Degradation-Aware Image Fusion with Language-Driven Semantics](https://arxiv.org/abs/2511.12525)
*Jing Li,Yifan Wang,Jiafeng Yan,Renlong Zhang,Bin Yang*

Main category: cs.CV

TL;DR: Proposes MdaIF, a one-stop degradation-aware infrared-visible image fusion framework using a mixture-of-experts (MoE) guided by semantic priors from a vision-language model to handle multi-degradation weather conditions (haze, rain, snow).


<details>
  <summary>Details</summary>
Motivation: Addresses degradation of visible images under adverse weather and rigid fixed-architecture fusion models that struggle in varied degradation scenarios.

Method: MoE across degradation scenarios; semantic priors from a pre-trained vision-language model to adaptively extract weather-aware degradation knowledge and scene features; degradation-aware channel attention module (DCAM) with degradation prototype decomposition to facilitate cross-modal feature interaction; MoE routing guided by semantic priors and modulated features.

Result: Extensive experiments show superior performance over state-of-the-art methods on multi-degradation scenarios.

Conclusion: MdaIF provides robust, adaptable fusion in complex degradation by integrating semantic priors with degradation-aware attention and MoE routing.

Abstract: Infrared and visible image fusion aims to integrate complementary multi-modal information into a single fused result. However, existing methods 1) fail to account for the degradation visible images under adverse weather conditions, thereby compromising fusion performance; and 2) rely on fixed network architectures, limiting their adaptability to diverse degradation scenarios. To address these issues, we propose a one-stop degradation-aware image fusion framework for multi-degradation scenarios driven by a large language model (MdaIF). Given the distinct scattering characteristics of different degradation scenarios (e.g., haze, rain, and snow) in atmospheric transmission, a mixture-of-experts (MoE) system is introduced to tackle image fusion across multiple degradation scenarios. To adaptively extract diverse weather-aware degradation knowledge and scene feature representations, collectively referred to as the semantic prior, we employ a pre-trained vision-language model (VLM) in our framework. Guided by the semantic prior, we propose degradation-aware channel attention module (DCAM), which employ degradation prototype decomposition to facilitate multi-modal feature interaction in channel domain. In addition, to achieve effective expert routing, the semantic prior and channel-domain modulated features are utilized to guide the MoE, enabling robust image fusion in complex degradation scenarios. Extensive experiments validate the effectiveness of our MdaIF, demonstrating superior performance over SOTA methods.

</details>


### [130] [D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation](https://arxiv.org/abs/2511.12528)
*Zheyuan Zhang,Jiwei Zhang,Boyu Zhou,Linzhimeng Duan,Hong Chen*

Main category: cs.CV

TL;DR: D2-VPR introduces a two-stage distillation-and-deformable-aggregation framework to compress DINOv2-based visual place recognition models while preserving performance, achieving substantial parameter and FLOPs reductions with competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: To overcome the high computational cost and large parameter count of DINOv2-based VPR models, enabling deployment on resource-limited devices without sacrificing accuracy.

Method: 1) Two-stage training with knowledge distillation and fine-tuning; 2) Distillation Recovery Module (DRM) to better align teacher-student feature spaces and minimize transfer losses; 3) Top-Down-attention-based Deformable Aggregator (TDDA) that uses global semantic features to dynamically adjust Regions of Interest for aggregation, improving adaptability to irregular scenes.

Result: Achieves competitive performance compared to state-of-the-art methods while reducing parameters by ~64.2% and FLOPs by ~62.6% relative to CricaVPR; code released at provided URL.

Conclusion: D2-VPR offers a strong efficiency-accuracy trade-off for VPR by combining distillation with deformable, attention-guided aggregation, making large foundation-model features accessible on resource-constrained devices.

Abstract: Visual Place Recognition (VPR) aims to determine the geographic location of a query image by retrieving its most visually similar counterpart from a geo-tagged reference database. Recently, the emergence of the powerful visual foundation model, DINOv2, trained in a self-supervised manner on massive datasets, has significantly improved VPR performance. This improvement stems from DINOv2's exceptional feature generalization capabilities but is often accompanied by increased model complexity and computational overhead that impede deployment on resource-constrained devices. To address this challenge, we propose $D^{2}$-VPR, a $D$istillation- and $D$eformable-based framework that retains the strong feature extraction capabilities of visual foundation models while significantly reducing model parameters and achieving a more favorable performance-efficiency trade-off. Specifically, first, we employ a two-stage training strategy that integrates knowledge distillation and fine-tuning. Additionally, we introduce a Distillation Recovery Module (DRM) to better align the feature spaces between the teacher and student models, thereby minimizing knowledge transfer losses to the greatest extent possible. Second, we design a Top-Down-attention-based Deformable Aggregator (TDDA) that leverages global semantic features to dynamically and adaptively adjust the Regions of Interest (ROI) used for aggregation, thereby improving adaptability to irregular structures. Extensive experiments demonstrate that our method achieves competitive performance compared to state-of-the-art approaches. Meanwhile, it reduces the parameter count by approximately 64.2% and FLOPs by about 62.6% (compared to CricaVPR).Code is available at https://github.com/tony19980810/D2VPR.

</details>


### [131] [ReaSon: Reinforced Causal Search with Information Bottleneck for Video Understanding](https://arxiv.org/abs/2511.12530)
*Yuan Zhou,Litao Hua,Shilong Jin,Wentao Huang,Haoran Duan*

Main category: cs.CV

TL;DR: ReaSon introduces a Reinforced Causal Search with Information Bottleneck (CIB) to select keyframes that are both predictive and causally necessary, using RL with a learnable policy and counterfactual evaluation, achieving state-of-the-art results under limited-frame settings on several datasets.


<details>
  <summary>Details</summary>
Motivation: Video understanding with vision-language models is token-limited; keyframes must be informative and causally decisive; existing methods may not ensure sufficiency and causal necessity simultaneously.

Method: Proposes Causal Information Bottleneck (CIB) as a principle; a learnable policy selects keyframes from a candidate pool to capture predictive sufficiency; causal necessity assessed via counterfactual interventions; reinforcement learning with a composite reward aligned with CIB.

Result: Extensive experiments on NExT-QA, EgoSchema, Video-MME show ReaSon consistently surpasses existing SOTA methods under limited-frame settings, demonstrating effectiveness and generalization.

Conclusion: ReaSon successfully integrates predictive sufficiency and causal necessity for robust keyframe selection, with potential applicability to other VLM-based video understanding tasks under token constraints.

Abstract: Keyframe selection has become essential for video understanding with vision-language models (VLMs) due to limited input tokens and the temporal sparsity of relevant information across video frames. Video understanding often relies on effective keyframes that are not only informative but also causally decisive. To this end, we propose Reinforced Causal Search with Information Bottleneck (ReaSon), a framework that formulates keyframe selection as an optimization problem with the help of a novel Causal Information Bottleneck (CIB), which explicitly defines keyframes as those satisfying both predictive sufficiency and causal necessity. Specifically, ReaSon employs a learnable policy network to select keyframes from a visually relevant pool of candidate frames to capture predictive sufficiency, and then assesses causal necessity via counterfactual interventions. Finally, a composite reward aligned with the CIB principle is designed to guide the selection policy through reinforcement learning. Extensive experiments on NExT-QA, EgoSchema, and Video-MME demonstrate that ReaSon consistently outperforms existing state-of-the-art methods under limited-frame settings, validating its effectiveness and generalization ability.

</details>


### [132] [HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models](https://arxiv.org/abs/2511.12547)
*Zhiguang Lu,Qianqian Xu,Peisong Wen,Siran Da,Qingming Huang*

Main category: cs.CV

TL;DR: HiGFA is a diffusion-based data augmentation method for fine-grained classification that uses hierarchical guidance and confidence-driven modulation to generate faithful, diverse images.


<details>
  <summary>Details</summary>
Motivation: Fine-grained visual classification (FGVC) requires subtle, category-defining features. Standard text-based guidance (CFG) often lacks specificity and can produce misleading samples that degrade FGVC performance.

Method: HiGFA leverages diffusion sampling dynamics: strong text and transformed contour guidance in early to mid stages to establish global scene, style, and structure; in the final stages, it applies fine-grained classifier guidance and dynamically adjusts the strength of all guidance signals based on prediction confidence, balancing global structure with detailed refinement.

Result: Empirical evaluation on several FGVC datasets shows that HiGFA generates diverse yet faithful images and improves FGVC performance.

Conclusion: A hierarchical, confidence-driven guidance strategy enables faithful and varied FGVC data augmentation with diffusion models, addressing CFG limitations and improving downstream classifier accuracy.

Abstract: Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.

</details>


### [133] [EmoVerse: A MLLMs-Driven Emotion Representation Dataset for Interpretable Visual Emotion Analysis](https://arxiv.org/abs/2511.12554)
*Yijie Guo,Dexiang Hong,Weidong Chen,Zihan She,Cheng Ye,Xiaojun Chang,Zhendong Mao*

Main category: cs.CV

TL;DR: EmoVerse: a large-scale open-source dataset for interpretable visual emotion analysis with Background-Attribute-Subject (B-A-S) grounded annotations, word- and subject-level emotional reasoning, dual labels for categorical (CES) and dimensional (DES) emotions, a multi-stage annotation pipeline, and an interpretable model that maps cues to DES with attribution explanations.


<details>
  <summary>Details</summary>
Motivation: Addresses data scarcity and lack of interpretability in Visual Emotion Analysis (VEA) by offering rich, grounded, interpretable annotations and a unified discrete-continuous emotion representation.

Method: Build EmoVerse with B-A-S triplets grounded to image regions; provide both word-level and subject-level emotional reasoning; supply CES (categorical) and DES (dimensional) annotations; employ a multi-stage annotation pipeline to ensure reliability with low human effort; develop an interpretable model that maps visual cues to DES and provides attribution explanations.

Result: A dataset of over 219k images with multi-layered, grounded annotations (B-A-S); dual CES and DES annotations enabling unified discrete and continuous emotion representation; a reliable multi-stage annotation pipeline; an interpretable DES-mapping model with attribution explanations.

Conclusion: EmoVerse, along with its annotation pipeline and interpretable model, establishes a solid foundation for explainable high-level emotion understanding in VEA and paves the way for more transparent and expressive emotion analysis.

Abstract: Visual Emotion Analysis (VEA) aims to bridge the affective gap between visual content and human emotional responses. Despite its promise, progress in this field remains limited by the lack of open-source and interpretable datasets. Most existing studies assign a single discrete emotion label to an entire image, offering limited insight into how visual elements contribute to emotion. In this work, we introduce EmoVerse, a large-scale open-source dataset that enables interpretable visual emotion analysis through multi-layered, knowledge-graph-inspired annotations. By decomposing emotions into Background-Attribute-Subject (B-A-S) triplets and grounding each element to visual regions, EmoVerse provides word-level and subject-level emotional reasoning. With over 219k images, the dataset further includes dual annotations in Categorical Emotion States (CES) and Dimensional Emotion Space (DES), facilitating unified discrete and continuous emotion representation. A novel multi-stage pipeline ensures high annotation reliability with minimal human effort. Finally, we introduce an interpretable model that maps visual cues into DES representations and provides detailed attribution explanations. Together, the dataset, pipeline, and model form a comprehensive foundation for advancing explainable high-level emotion understanding.

</details>


### [134] [SEMC: Structure-Enhanced Mixture-of-Experts Contrastive Learning for Ultrasound Standard Plane Recognition](https://arxiv.org/abs/2511.12559)
*Qing Cai,Guihao Yan,Fan Zhang,Cheng Zhang,Zhi Liu*

Main category: cs.CV

TL;DR: SEMC uses structure-aware fusion and mixture-of-experts contrastive learning to improve ultrasound plane recognition; achieves state-of-the-art on liver ultrasound datasets.


<details>
  <summary>Details</summary>
Motivation: Current ultrasound standard-plane recognition struggles to leverage shallow structural information and to distinguish fine-grained semantic differences using augmentations; there is a need for multi-scale structure fusion and hierarchical contrastive learning to boost structural perception and discriminability.

Method: Introduce Semantic-Structure Fusion Module (SSFM) to align multi-scale, shallow and deep features and enhance fine-grained structural perception; design Mixture-of-Experts Contrastive Recognition Module (MCRM) for hierarchical contrastive learning and classification across multiple feature levels using a mixture-of-experts mechanism; curate a large, annotated liver ultrasound dataset with six standard planes; evaluate on in-house and public datasets.

Result: SEMC outperforms recent state-of-the-art methods across various metrics on both the in-house dataset and two public datasets.

Conclusion: Structure-aware feature fusion combined with MoE-based contrastive learning improves ultrasound standard plane recognition by capturing structural details and enhancing class separability.

Abstract: Ultrasound standard plane recognition is essential for clinical tasks such as disease screening, organ evaluation, and biometric measurement. However, existing methods fail to effectively exploit shallow structural information and struggle to capture fine-grained semantic differences through contrastive samples generated by image augmentations, ultimately resulting in suboptimal recognition of both structural and discriminative details in ultrasound standard planes. To address these issues, we propose SEMC, a novel Structure-Enhanced Mixture-of-Experts Contrastive learning framework that combines structure-aware feature fusion with expert-guided contrastive learning. Specifically, we first introduce a novel Semantic-Structure Fusion Module (SSFM) to exploit multi-scale structural information and enhance the model's ability to perceive fine-grained structural details by effectively aligning shallow and deep features. Then, a novel Mixture-of-Experts Contrastive Recognition Module (MCRM) is designed to perform hierarchical contrastive learning and classification across multi-level features using a mixture-of-experts (MoE) mechanism, further improving class separability and recognition performance. More importantly, we also curate a large-scale and meticulously annotated liver ultrasound dataset containing six standard planes. Extensive experimental results on our in-house dataset and two public datasets demonstrate that SEMC outperforms recent state-of-the-art methods across various metrics.

</details>


### [135] [Through-Foliage Surface-Temperature Reconstruction for early Wildfire Detection](https://arxiv.org/abs/2511.12572)
*Mohamed Youssef,Lukas Brunner,Klaus Rundhammer,Gerald Czech,Oliver Bimber*

Main category: cs.CV

TL;DR: Occlusion-robust surface temperature reconstruction via a diffusion-augmented data generation and visual state-space model, improving SA-based wildfire monitoring and enabling full morphology recovery of fires and human signatures.


<details>
  <summary>Details</summary>
Motivation: Occlusion by vegetation and thermal blur in SA imaging hinder accurate surface temperature sensing; need automated, early wildfire detection; scarcity of real training data requires synthetic augmentation; extendable to other thermal signals such as human signatures.

Method: Train a visual state-space model to recover thermal signals from blurred SA data. Generate large realistic surface-temperature simulations by integrating a latent diffusion model with vector quantization, conditioned on real wildfire recordings; augment data with temperature variations and procedural forest simulations.

Result: Simulated data: RMSE reduced by factors of 2–2.5 over conventional thermal and uncorrected SA imaging. Field tests on hotspots: RMSE gains of 12.8x vs conventional thermal and 2.6x vs uncorrected SA. Model generalizes to other thermal signals (e.g., human signatures). Reconstructs complete morphology of fire and human signatures, overcoming occlusion that defeats conventional imaging.

Conclusion: The proposed diffusion-augmented, state-space approach enables automated aerial wildfire monitoring under canopy occlusion, with superior temperature reconstruction and morphology recovery, and shows promise for other thermal sensing tasks.

Abstract: We introduce a novel method for reconstructing surface temperatures through occluding forest vegetation by combining signal processing and machine learning. Our goal is to enable fully automated aerial wildfire monitoring using autonomous drones, allowing for the early detection of ground fires before smoke or flames are visible. While synthetic aperture (SA) sensing mitigates occlusion from the canopy and sunlight, it introduces thermal blur that obscures the actual surface temperatures. To address this, we train a visual state space model to recover the subtle thermal signals of partially occluded soil and fire hotspots from this blurred data. A key challenge was the scarcity of real-world training data. We overcome this by integrating a latent diffusion model into a vector quantized to generated a large volume of realistic surface temperature simulations from real wildfire recordings, which we further expanded through temperature augmentation and procedural thermal forest simulation. On simulated data across varied ambient and surface temperatures, forest densities, and sunlight conditions, our method reduced the RMSE by a factor of 2 to 2.5 compared to conventional thermal and uncorrected SA imaging. In field experiments focused on high-temperature hotspots, the improvement was even more significant, with a 12.8-fold RMSE gain over conventional thermal and a 2.6-fold gain over uncorrected SA images. We also demonstrate our model's generalization to other thermal signals, such as human signatures for search and rescue. Since simple thresholding is frequently inadequate for detecting subtle thermal signals, the morphological characteristics are equally essential for accurate classification. Our experiments demonstrated another clear advantage: we reconstructed the complete morphology of fire and human signatures, whereas conventional imaging is defeated by partial occlusion.

</details>


### [136] [Beyond Pixels: Semantic-aware Typographic Attack for Geo-Privacy Protection](https://arxiv.org/abs/2511.12575)
*Jiayi Zhu,Yihao Huang,Yue Cao,Xiaojun Jia,Qing Guo,Felix Juefei-Xu,Geguang Pu,Bin Wang*

Main category: cs.CV

TL;DR: Semantics-aware typographical attack adds text outside image content to disrupt geolocation inferences by LVLMs, achieving privacy protection with minimal visual quality loss.


<details>
  <summary>Details</summary>
Motivation: Large Vision-Language Models can infer a user's geolocation from shared images, creating a privacy risk. Traditional adversarial perturbations harm visual quality; a non-visual defense is needed that preserves image sharing value.

Method: A two-stage, semantics-aware typographical attack that identifies effective textual semantics and generates deceptive text to append as a text extension outside the visual content, aimed at disrupting geolocation inference.

Result: The approach substantially lowers geolocation prediction accuracy across three datasets and five state-of-the-art commercial LVLMs, demonstrating a practical, visually-preserving geo-privacy protection strategy.

Conclusion: Textual extensions are a viable defense against geo-privacy threats in LVLMs, offering privacy protection without compromising visual sharing quality; further work may explore robustness and generalization.

Abstract: Large Visual Language Models (LVLMs) now pose a serious yet overlooked privacy threat, as they can infer a social media user's geolocation directly from shared images, leading to unintended privacy leakage. While adversarial image perturbations provide a potential direction for geo-privacy protection, they require relatively strong distortions to be effective against LVLMs, which noticeably degrade visual quality and diminish an image's value for sharing. To overcome this limitation, we identify typographical attacks as a promising direction for protecting geo-privacy by adding text extension outside the visual content. We further investigate which textual semantics are effective in disrupting geolocation inference and design a two-stage, semantics-aware typographical attack that generates deceptive text to protect user privacy. Extensive experiments across three datasets demonstrate that our approach significantly reduces geolocation prediction accuracy of five state-of-the-art commercial LVLMs, establishing a practical and visually-preserving protection strategy against emerging geo-privacy threats.

</details>


### [137] [TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction](https://arxiv.org/abs/2511.12578)
*Yukuo Ma,Cong Liu,Junke Wang,Junqi Liu,Haibin Huang,Zuxuan Wu,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TempoMaster introduces a multi-rate framework for long video generation: start with a low-frame-rate blueprint and progressively refine it by increasing frame rates, using bidirectional intra-frame attention and cross-rate autoregression to ensure long-range temporal coherence and efficiency, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Long video generation struggles with temporal coherence and computational efficiency; prior methods either fix frame rate or require heavy sequential processing. A hierarchical, autoregressive across frame-rate scheme can balance quality and speed.

Method: 1) Generate a coarse, low-frame-rate clip as a blueprint. 2) Incrementally raise the frame rate to refine visuals and motion. 3) Within each frame-rate level, apply bidirectional attention. 4) Across frame-rate levels, apply autoregression to maintain temporal continuity. 5) Enable parallel synthesis within a level for efficiency and long-range coherence across levels.

Result: Experimental results indicate TempoMaster achieves new state-of-the-art performance for long video generation, with improvements in both visual fidelity and temporal coherence.

Conclusion: Hierarchical frame-rate expansion with intra-frame bidirectional attention and cross-rate autoregression effectively handles long videos, offering a scalable and high-quality synthesis approach.

Abstract: We present TempoMaster, a novel framework that formulates long video generation as next-frame-rate prediction. Specifically, we first generate a low-frame-rate clip that serves as a coarse blueprint of the entire video sequence, and then progressively increase the frame rate to refine visual details and motion continuity. During generation, TempoMaster employs bidirectional attention within each frame-rate level while performing autoregression across frame rates, thus achieving long-range temporal coherence while enabling efficient and parallel synthesis. Extensive experiments demonstrate that TempoMaster establishes a new state-of-the-art in long video generation, excelling in both visual and temporal quality.

</details>


### [138] [Rank-Aware Agglomeration of Foundation Models for Immunohistochemistry Image Cell Counting](https://arxiv.org/abs/2511.12588)
*Zuqi Huang,Mengxin Tian,Huan Liu,Wentao Li,Baobao Liang,Jie Wu,Fang Yan,Zhaoqing Tang,Zhongyu Li*

Main category: cs.CV

TL;DR: A rank-aware distillation framework CountIHC enables end-to-end multi-class cell counting in IHC images by distilling knowledge from multiple foundation models via RATS, and using vision-language aligned anchors for class-specific density maps, achieving state-of-the-art across multiple biomarkers and tissue types.


<details>
  <summary>Details</summary>
Motivation: Immunohistochemistry counting is challenged by chromogen overlap, staining variability, and diverse cell morphologies. Regression-based counting can handle overlapped cells but lacks end-to-end multi-class capabilities. Foundation models are underutilized in this domain; a sample-aware, multi-teacher aggregation strategy and a robust counting target are needed.

Method: CountIHC uses a rank-aware agglomeration framework to distill knowledge from multiple strong foundation models into a compact student. It introduces Rank-Aware Teacher Selecting (RATS) to model global-to-local patch rankings and enable sample-wise teacher selection. For multi-class counting, a fine-tuning stage reformulates the task as vision-language alignment, using discrete semantic anchors from structured text prompts to encode category and quantity information, guiding regression of class-specific density maps.

Result: CountIHC surpasses state-of-the-art methods across 12 IHC biomarkers and 5 tissue types, with high agreement to pathologists, and demonstrates effectiveness on H&E-stained data, indicating scalability.

Conclusion: The approach effectively handles IHC heterogeneity and overlapped cells, enabling end-to-end multi-class counting and showing promise for broader applicability of foundation-model distillation in histopathology counting.

Abstract: Accurate cell counting in immunohistochemistry (IHC) images is critical for quantifying protein expression and aiding cancer diagnosis. However, the task remains challenging due to the chromogen overlap, variable biomarker staining, and diverse cellular morphologies. Regression-based counting methods offer advantages over detection-based ones in handling overlapped cells, yet rarely support end-to-end multi-class counting. Moreover, the potential of foundation models remains largely underexplored in this paradigm. To address these limitations, we propose a rank-aware agglomeration framework that selectively distills knowledge from multiple strong foundation models, leveraging their complementary representations to handle IHC heterogeneity and obtain a compact yet effective student model, CountIHC. Unlike prior task-agnostic agglomeration strategies that either treat all teachers equally or rely on feature similarity, we design a Rank-Aware Teacher Selecting (RATS) strategy that models global-to-local patch rankings to assess each teacher's inherent counting capacity and enable sample-wise teacher selection. For multi-class cell counting, we introduce a fine-tuning stage that reformulates the task as vision-language alignment. Discrete semantic anchors derived from structured text prompts encode both category and quantity information, guiding the regression of class-specific density maps and improving counting for overlapping cells. Extensive experiments demonstrate that CountIHC surpasses state-of-the-art methods across 12 IHC biomarkers and 5 tissue types, while exhibiting high agreement with pathologists' assessments. Its effectiveness on H&E-stained data further confirms the scalability of the proposed method.

</details>


### [139] [Fine-Grained Representation for Lane Topology Reasoning](https://arxiv.org/abs/2511.12590)
*Guoqing Xu,Yiheng Li,Yang Yang*

Main category: cs.CV

TL;DR: TopoFG proposes a fine-grained lane topology reasoning framework with three phases (HPE, RFD, RBTR) to integrate global spatial priors and local sequential priors into fine-grained queries, achieving SOTA on OpenLane-V2 (OLS 48.0% on subsetA, 45.4% on subsetB).


<details>
  <summary>Details</summary>
Motivation: Current methods represent each lane with a single query and rely on similarity for topology, which fails on complex lane graphs, risking unreliable topology predictions.

Method: Three-phase approach: Hierarchical Prior Extractor (HPE) extracts global priors from BEV mask and local priors from in-lane keypoints; Region-Focused Decoder (RFD) builds fine-grained queries by combining priors, samples reference points in RoI, and applies cross-attention with BEV features to refine lane queries; Robust Boundary-Point Topology Reasoning (RBTR) uses boundary-point query features and a topological denoising strategy to reduce matching ambiguity and improve topology predictions.

Result: Demonstrates state-of-the-art performance on OpenLane-V2 with OLS 48.0% on subsetA and 45.4% on subsetB; ablation not provided in abstract.

Conclusion: Integrating spatial and sequential priors into fine-grained queries with denoising yields precise modeling of complex lane structures and more trustworthy topology predictions.

Abstract: Precise modeling of lane topology is essential for autonomous driving, as it directly impacts navigation and control decisions.Existing methods typically represent each lane with a single query and infer topological connectivity based on the similarity between lane queries.However, this kind of design struggles to accurately model complex lane structures, leading to unreliable topology prediction.In this view, we propose a Fine-Grained lane topology reasoning framework (TopoFG).It divides the procedure from bird's-eye-view (BEV) features to topology prediction via fine-grained queries into three phases, i.e., Hierarchical Prior Extractor (HPE), Region-Focused Decoder (RFD), and Robust Boundary-Point Topology Reasoning (RBTR).Specifically, HPE extracts global spatial priors from the BEV mask and local sequential priors from in-lane keypoint sequences to guide subsequent fine-grained query modeling.RFD constructs fine-grained queries by integrating the spatial and sequential priors. It then samples reference points in RoI regions of the mask and applies cross-attention with BEV features to refine the query representations of each lane.RBTR models lane connectivity based on boundary-point query features and further employs a topological denoising strategy to reduce matching ambiguity.By integrating spatial and sequential priors into fine-grained queries and applying a denoising strategy to boundary-point topology reasoning, our method precisely models complex lane structures and delivers trustworthy topology predictions.Extensive experiments on the OpenLane-V2 benchmark demonstrate that TopoFG achieves new state-of-the-art performance, with an OLS of 48.0% on subsetA and 45.4% on subsetB.

</details>


### [140] [Seg-VAR: Image Segmentation with Visual Autoregressive Modeling](https://arxiv.org/abs/2511.12594)
*Rongkun Zheng,Lu Qi,Xi Chen,Yi Wang,Kun Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: Seg-VAR reframes segmentation as a conditional autoregressive mask generation problem using latent priors and a three-component architecture to produce masks, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of discriminative segmentation by leveraging autoregressive, multi-scale latent modeling for precise low-level spatial perception, inspired by Mask2Former and multi-scale modeling.

Method: Three components: (1) an image encoder that produces latent priors from input images; (2) a seglat encoder that maps segmentation masks into discrete latent tokens using a location-sensitive color mapping to distinguish instances; (3) a decoder that reconstructs masks from latent representations. A multi-stage training strategy: first learn seglat representations via image-seglat joint training, then refine latent transformations, and finally align image-encoder-derived latents with seglat distributions.

Result: Experimental results show Seg-VAR outperforms previous discriminative and generative methods on various segmentation tasks and validation benchmarks.

Conclusion: Framing segmentation as a sequential hierarchical latent prediction task enables autoregressive reasoning in spatially-aware vision systems; code will be released at the provided repository.

Abstract: While visual autoregressive modeling (VAR) strategies have shed light on image generation with the autoregressive models, their potential for segmentation, a task that requires precise low-level spatial perception, remains unexplored. Inspired by the multi-scale modeling of classic Mask2Former-based models, we propose Seg-VAR, a novel framework that rethinks segmentation as a conditional autoregressive mask generation problem. This is achieved by replacing the discriminative learning with the latent learning process. Specifically, our method incorporates three core components: (1) an image encoder generating latent priors from input images, (2) a spatial-aware seglat (a latent expression of segmentation mask) encoder that maps segmentation masks into discrete latent tokens using a location-sensitive color mapping to distinguish instances, and (3) a decoder reconstructing masks from these latents. A multi-stage training strategy is introduced: first learning seglat representations via image-seglat joint training, then refining latent transformations, and finally aligning image-encoder-derived latents with seglat distributions. Experiments show Seg-VAR outperforms previous discriminative and generative methods on various segmentation tasks and validation benchmarks. By framing segmentation as a sequential hierarchical prediction task, Seg-VAR opens new avenues for integrating autoregressive reasoning into spatial-aware vision systems. Code will be available at https://github.com/rkzheng99/Seg-VAR.

</details>


### [141] [LoRA-Enhanced Vision Transformer for Single Image based Morphing Attack Detection via Knowledge Distillation from EfficientNet](https://arxiv.org/abs/2511.12602)
*Ria Shekhawat,Sushrut Patwardhan,Raghavendra Ramachandra,Praveen Kumar Chandaliya,Kishor P. Upla*

Main category: cs.CV

TL;DR: Proposes a CNN→ViT teacher-student S-MAD framework with LoRA-based fine-tuning, delivering superior accuracy and efficiency on a comprehensive morphing dataset.


<details>
  <summary>Details</summary>
Motivation: Face Recognition Systems (FRS) are vulnerable to morphing attacks that blend biometric features from multiple identities. There is a need for robust, efficient single-image morphing attack detection (S-MAD) capable of handling diverse morphing techniques.

Method: A teacher-student framework where a CNN-based teacher refines a ViT-based student. Low-Rank Adaptation (LoRA) is integrated to enable efficient fine-tuning. The method is evaluated on a morphing dataset built from three public face datasets with ten morphing generation algorithms, and is compared against six state-of-the-art S-MAD methods.

Result: The proposed approach achieves superior detection performance and computational efficiency compared to six S-MAD baselines across a diverse set of morphing algorithms.

Conclusion: An efficient and accurate S-MAD framework is achieved through the teacher-student CNN→ViT setup with LoRA-based fine-tuning, offering strong robustness to various morphing algorithms and potential for real-time or resource-constrained deployment.

Abstract: Face Recognition Systems (FRS) are critical for security but remain vulnerable to morphing attacks, where synthetic images blend biometric features from multiple individuals. We propose a novel Single-Image Morphing Attack Detection (S-MAD) approach using a teacher-student framework, where a CNN-based teacher model refines a ViT-based student model. To improve efficiency, we integrate Low-Rank Adaptation (LoRA) for fine-tuning, reducing computational costs while maintaining high detection accuracy. Extensive experiments are conducted on a morphing dataset built from three publicly available face datasets, incorporating ten different morphing generation algorithms to assess robustness. The proposed method is benchmarked against six state-of-the-art S-MAD techniques, demonstrating superior detection performance and computational efficiency.

</details>


### [142] [Pixels or Positions? Benchmarking Modalities in Group Activity Recognition](https://arxiv.org/abs/2511.12606)
*Drishya Karki,Merey Ramazanova,Anthony Cioppa,Silvio Giancola,Bernard Ghanem*

Main category: cs.CV

TL;DR: Introduces SoccerNet-GAR, a synchronized video-and-tracking dataset for football group activity recognition, and shows tracking-based GNNs outperform video baselines in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Lack of standardized benchmarks for comparing pixel (video) and position (tracking) modalities in group activity recognition (GAR); need apples-to-apples evaluation.

Method: Assembles 64 World Cup matches, 94,285 group activities, and 10 categories; defines a unified evaluation protocol; proposes a role-aware graph neural network for tracking-based GAR that uses positional edges and temporal attention.

Result: Tracking model achieves 67.2% balanced accuracy versus 58.1% for the best video baseline; training is 4.25x faster with 438x fewer parameters (197K vs 86.3M).

Conclusion: Shows that modality choice and role-aware modeling matter for GAR; tracking data can outperform video with efficient architectures, offering new insights into using positional signals for group activity recognition.

Abstract: Group Activity Recognition (GAR) is well studied on the video modality for surveillance and indoor team sports (e.g., volleyball, basketball). Yet, other modalities such as agent positions and trajectories over time, i.e. tracking, remain comparatively under-explored despite being compact, agent-centric signals that explicitly encode spatial interactions. Understanding whether pixel (video) or position (tracking) modalities leads to better group activity recognition is therefore important to drive further research on the topic. However, no standardized benchmark currently exists that aligns broadcast video and tracking data for the same group activities, leading to a lack of apples-to-apples comparison between these modalities for GAR. In this work, we introduce SoccerNet-GAR, a multimodal dataset built from the $64$ matches of the football World Cup 2022. Specifically, the broadcast videos and player tracking modalities for $94{,}285$ group activities are synchronized and annotated with $10$ categories. Furthermore, we define a unified evaluation protocol to benchmark two strong unimodal approaches: (i) a competitive video-based classifiers and (ii) a tracking-based classifiers leveraging graph neural networks. In particular, our novel role-aware graph architecture for tracking-based GAR directly encodes tactical structure through positional edges and temporal attention. Our tracking model achieves $67.2\%$ balanced accuracy compared to $58.1\%$ for the best video baseline, while training $4.25 \times$ faster with $438 \times$ fewer parameters ($197K$ \vs $86.3M$). This study provides new insights into the relative strengths of pixels and positions for group activity recognition. Overall, it highlights the importance of modality choice and role-aware modeling for GAR.

</details>


### [143] [Open-World Test-Time Adaptation with Hierarchical Feature Aggregation and Attention Affine](https://arxiv.org/abs/2511.12607)
*Ziqiong Liu,Yushun Tang,Junyang Ji,Zhihai He*

Main category: cs.CV

TL;DR: A new hierarchical OOD feature extractor (HLN) with an Attention Affine Network and weighted entropy suppression improves test-time adaptation under domain shift and OOD by fusing HLN with the base model predictions; it yields substantial gains on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: TTA approaches often fail when test-time samples include unseen out-of-distribution (OOD) classes and domain shifts, leading to misclassification and unstable adaptation; robust, OOD-aware TTA is needed.

Method: Propose a Hierarchical Ladder Network (HLN) that extracts OOD features from class tokens across Transformer layers and fuses its output with the original model predictions via weighted probability fusion. Introduce an Attention Affine Network (AAN) that adaptively refines self-attention conditioned on token information to handle domain drift. Apply a weighted entropy mechanism to dynamically suppress low-confidence samples during adaptation.

Result: Experimental results on benchmark datasets show that the approach significantly improves classification performance, demonstrating robustness to OOD and domain shift.

Conclusion: HLN + AAN with weighted entropy provides an effective framework for robust test-time adaptation in the presence of OOD and domain shift, achieving improved performance on standard benchmarks.

Abstract: Test-time adaptation (TTA) refers to adjusting the model during the testing phase to cope with changes in sample distribution and enhance the model's adaptability to new environments. In real-world scenarios, models often encounter samples from unseen (out-of-distribution, OOD) categories. Misclassifying these as known (in-distribution, ID) classes not only degrades predictive accuracy but can also impair the adaptation process, leading to further errors on subsequent ID samples. Many existing TTA methods suffer substantial performance drops under such conditions. To address this challenge, we propose a Hierarchical Ladder Network that extracts OOD features from class tokens aggregated across all Transformer layers. OOD detection performance is enhanced by combining the original model prediction with the output of the Hierarchical Ladder Network (HLN) via weighted probability fusion. To improve robustness under domain shift, we further introduce an Attention Affine Network (AAN) that adaptively refines the self-attention mechanism conditioned on the token information to better adapt to domain drift, thereby improving the classification performance of the model on datasets with domain shift. Additionally, a weighted entropy mechanism is employed to dynamically suppress the influence of low-confidence samples during adaptation. Experimental results on benchmark datasets show that our method significantly improves the performance on the most widely used classification datasets.

</details>


### [144] [C3Net: Context-Contrast Network for Camouflaged Object Detection](https://arxiv.org/abs/2511.12627)
*Baber Jan,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais,Saeed Anwar*

Main category: cs.CV

TL;DR: A dual-pathway network (C3Net) for camouflaged object detection combining an Edge Refinement Pathway with Edge Enhancement Modules and a Contextual Localization Pathway with Image-based Context Guidance, fused by an Attentive Fusion Module to address six core COD challenges and achieve state-of-the-art results on COD10K, CAMO, and NC4K.


<details>
  <summary>Details</summary>
Motivation: Camouflaged objects blend with their surroundings, creating intrinsic similarity, edge disruption, scale variation, complex environments, contextual dependencies, and disambiguation challenges. Existing segmentation methods and foundation models struggle, necessitating architectural innovations that jointly address multiple challenges.

Method: A specialized dual-pathway decoder: (1) Edge Refinement Pathway uses gradient-initialized Edge Enhancement Modules to recover precise boundaries from early features; (2) Contextual Localization Pathway employs an Image-based Context Guidance mechanism to suppress intrinsic saliency and emphasize camouflaged object regions; An Attentive Fusion Module combines both pathways via spatial gating to produce final detections.

Result: C3Net achieves state-of-the-art S-measures on three benchmarks: COD10K 0.898, CAMO 0.904, NC4K 0.913, with efficient processing. The authors also provide code, model weights, and results at their GitHub repository.

Conclusion: The paper demonstrates that addressing multiple, interrelated COD challenges requires integrated architectural components that synergistically cover edge precision, contextual reasoning, and saliency suppression, offering a robust advancement over isolated improvements.

Abstract: Camouflaged object detection identifies objects that blend seamlessly with their surroundings through similar colors, textures, and patterns. This task challenges both traditional segmentation methods and modern foundation models, which fail dramatically on camouflaged objects. We identify six fundamental challenges in COD: Intrinsic Similarity, Edge Disruption, Extreme Scale Variation, Environmental Complexities, Contextual Dependencies, and Salient-Camouflaged Object Disambiguation. These challenges frequently co-occur and compound the difficulty of detection, requiring comprehensive architectural solutions. We propose C3Net, which addresses all challenges through a specialized dual-pathway decoder architecture. The Edge Refinement Pathway employs gradient-initialized Edge Enhancement Modules to recover precise boundaries from early features. The Contextual Localization Pathway utilizes our novel Image-based Context Guidance mechanism to achieve intrinsic saliency suppression without external models. An Attentive Fusion Module synergistically combines the two pathways via spatial gating. C3Net achieves state-of-the-art performance with S-measures of 0.898 on COD10K, 0.904 on CAMO, and 0.913 on NC4K, while maintaining efficient processing. C3Net demonstrates that complex, multifaceted detection challenges require architectural innovation, with specialized components working synergistically to achieve comprehensive coverage beyond isolated improvements. Code, model weights, and results are available at https://github.com/Baber-Jan/C3Net.

</details>


### [145] [Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation](https://arxiv.org/abs/2511.12631)
*Yushe Cao,Dianxi Shi,Xing Fu,Xuechao Zou,Haikuo Peng,Xueqi Li,Chun Yu,Junliang Xing*

Main category: cs.CV

TL;DR: MDiTFace introduces a diffusion-transformer framework with unified tokenization for semantic masks and text, enabling synchronous multimodal conditioning through stacked multivariate transformer blocks and a decoupled attention mechanism that separates dynamic and static computations to cache reusable features, reducing overhead by over 94% while boosting facial fidelity and conditional consistency.


<details>
  <summary>Details</summary>
Motivation: Conventional multimodal facial generation struggles with effective cross-modal interactions due to heterogeneous representations from masks and text. A unified tokenization and interaction mechanism is proposed to improve conditioning fidelity and efficiency.

Method: 1) A unified tokenization strategy converts semantic masks and text into compatible token representations. 2) Stacked multivariate transformer blocks enable synchronous processing of all conditioning signals, facilitating comprehensive cross-modal interactions. 3) A decoupled attention mechanism splits implicit dependencies between mask tokens and temporal embeddings into dynamic and static pathways, allowing caching of static computations to reduce computational overhead by over 94%.

Result: Extensive experiments show that MDiTFace significantly outperforms competing methods in facial fidelity and conditional consistency.

Conclusion: MDiTFace demonstrates that unified multimodal representation and decoupled attention enable efficient, high-quality conditional facial generation, with substantial computational savings and better modality integration.

Abstract: While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.

</details>


### [146] [Denoising Vision Transformer Autoencoder with Spectral Self-Regularization](https://arxiv.org/abs/2511.12633)
*Xunzhi Xiang,Xingye Tian,Guiyu Zhang,Yabo Chen,Shaofeng Zhang,Xuebo Wang,Xin Tao,Qi Fan*

Main category: cs.CV

TL;DR: High-dimensional VAE latents introduce redundant high-frequency noise that impedes diffusion model training; a spectral self-regularization and alignment approach (Denoising-VAE) yields cleaner latents, faster convergence (~2x), and strong reconstruction (rFID 0.28, PSNR 27.26) with competitive generation (gFID 1.82) on ImageNet 256x256.


<details>
  <summary>Details</summary>
Motivation: Understand how latent dimensionality affects generative model optimization, particularly the adverse effect of redundant high-frequency components in high-dimensional VAE latents; explore a strategy that does not rely on external vision foundation models (VFMs).

Method: Apply spectral self-regularization to suppress redundant high-frequency noise in VAE latents; develop Denoising-VAE (ViT-based autoencoder) that operates without VFMs; introduce spectral alignment to ease downstream optimization for diffusion-based generators.

Result: Diffusion models converge approximately 2× faster than with SD-VAE; achieve state-of-the-art reconstruction metrics (rFID = 0.28, PSNR = 27.26) and competitive generation metrics (gFID = 1.82) on ImageNet 256×256.

Conclusion: Spectral regularization effectively mitigates issues from high-dimensional latent spaces, enabling faster and higher-quality diffusion-based generation without reliance on VFMs; spectral alignment further facilitates optimization, positioning Denoising-VAE as a strong alternative in VAE-to-diffusion pipelines.

Abstract: Variational autoencoders (VAEs) typically encode images into a compact latent space, reducing computational cost but introducing an optimization dilemma: a higher-dimensional latent space improves reconstruction fidelity but often hampers generative performance. Recent methods attempt to address this dilemma by regularizing high-dimensional latent spaces using external vision foundation models (VFMs). However, it remains unclear how high-dimensional VAE latents affect the optimization of generative models. To our knowledge, our analysis is the first to reveal that redundant high-frequency components in high-dimensional latent spaces hinder the training convergence of diffusion models and, consequently, degrade generation quality. To alleviate this problem, we propose a spectral self-regularization strategy to suppress redundant high-frequency noise while simultaneously preserving reconstruction quality. The resulting Denoising-VAE, a ViT-based autoencoder that does not rely on VFMs, produces cleaner, lower-noise latents, leading to improved generative quality and faster optimization convergence. We further introduce a spectral alignment strategy to facilitate the optimization of Denoising-VAE-based generative models. Our complete method enables diffusion models to converge approximately 2$\times$ faster than with SD-VAE, while achieving state-of-the-art reconstruction quality (rFID = 0.28, PSNR = 27.26) and competitive generation performance (gFID = 1.82) on the ImageNet 256$\times$256 benchmark.

</details>


### [147] [Medical Knowledge Intervention Prompt Tuning for Medical Image Classification](https://arxiv.org/abs/2511.12639)
*Ye Du,Nanxi Yu,Shujun Wang*

Main category: cs.CV

TL;DR: CILMP uses LLM-derived disease-specific representations to generate instance-adaptive prompts for vision-language models, improving medical-image classification with prompt tuning.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning large vision-language models is costly; existing prompt tuning struggles to disentangle disease concepts across modalities; LLMs trained on medical text contain rich disease knowledge that can inform prompts.

Method: Propose Conditional Intervention of Large Language Models for Prompt Tuning (CILMP): extract disease-specific representations from LLMs, intervene in a low-rank linear subspace to form disease-focused prompts, and apply an image-conditioned mechanism to produce instance-adaptive prompts; bridges LLMs and VLMs for knowledge transfer; code available at GitHub.

Result: Across diverse medical image datasets, CILMP consistently outperforms state-of-the-art prompt-tuning baselines, demonstrating superior accuracy and adaptability with reduced training resources.

Conclusion: LLM-guided, instance-adaptive prompt tuning is an effective, scalable approach for medical VLMs, enabling more precise disease-feature capture and improved performance; encourages further exploration of LLM-VLM integrations.

Abstract: Vision-language foundation models (VLMs) have shown great potential in feature transfer and generalization across a wide spectrum of medical-related downstream tasks. However, fine-tuning these models is resource-intensive due to their large number of parameters. Prompt tuning has emerged as a viable solution to mitigate memory usage and reduce training time while maintaining competitive performance. Nevertheless, the challenge is that existing prompt tuning methods cannot precisely distinguish different kinds of medical concepts, which miss essentially specific disease-related features across various medical imaging modalities in medical image classification tasks. We find that Large Language Models (LLMs), trained on extensive text corpora, are particularly adept at providing this specialized medical knowledge. Motivated by this, we propose incorporating LLMs into the prompt tuning process. Specifically, we introduce the CILMP, Conditional Intervention of Large Language Models for Prompt Tuning, a method that bridges LLMs and VLMs to facilitate the transfer of medical knowledge into VLM prompts. CILMP extracts disease-specific representations from LLMs, intervenes within a low-rank linear subspace, and utilizes them to create disease-specific prompts. Additionally, a conditional mechanism is incorporated to condition the intervention process on each individual medical image, generating instance-adaptive prompts and thus enhancing adaptability. Extensive experiments across diverse medical image datasets demonstrate that CILMP consistently outperforms state-of-the-art prompt tuning methods, demonstrating its effectiveness. Code is available at https://github.com/usr922/cilmp.

</details>


### [148] [DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry](https://arxiv.org/abs/2511.12653)
*Cheng Liao*

Main category: cs.CV

TL;DR: DPVO-QAT++ introduces a hierarchical, heterogeneous-precision optimization for deep patch visual odometry to dramatically speed up inference and reduce memory without sacrificing trajectory accuracy, enabling practical deployment on resource-constrained systems.


<details>
  <summary>Details</summary>
Motivation: High computational overhead of deep learning-based vSLAM hinders deployment on resource-constrained autonomous platforms; there is a need to bridge high-precision deep VO with efficiency requirements.

Method: Combine learnable scale parameterization, front-end back-end heterogenous precision (FP16/FP32 fake quantization on the front-end; full precision on the back-end), and GPU-native CUDA kernel fusion for fake quantization. Features include scale-only training and a GPU-optimized quantization pipeline to reduce memory and latency while preserving DPVO accuracy.

Result: On TartanAir: average FPS +52.1%, median latency -29.1%, peak GPU memory -64.9%, ATE comparable across 32 sequences. On EuRoC: average FPS +30.1%, median latency -23.1%, peak GPU memory -37.7%, ATE comparable across 11 sequences.

Conclusion: DPVO-QAT++ effectively bridges high-precision deep VO and practical deployment constraints, offering a viable engineering paradigm for real-world embedded platforms by delivering substantial speedups and memory reductions without compromising trajectory accuracy.

Abstract: Deep learning-based Visual SLAM (vSLAM) systems exhibit exceptional geometric reasoning capabilities, yet their prohibitive computational overhead severely restricts deployment on resource-constrained autonomous platforms. This paper presents a hierarchical quantization optimization framework, DPVO-QAT++ (DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry). Through the synergistic integration of learnable scale parameterization, a heterogeneous precision design for the Visual Odometry (VO) front-end and back-end (front-end floating-point fake quantization with FP16/FP32; back-end full precision), and GPU-native kernel fusion for fake quantization (custom CUDA kernels), our framework significantly reduces memory footprint and increases processing speed while preserving the trajectory accuracy of the original model. On the TartanAir dataset, our framework achieves an average FPS increase of 52.1%, a 29.1% reduction in median latency, and a 64.9% reduction in peak GPU memory reservation, while maintaining trajectory accuracy (ATE) comparable to the original DPVO model across 32 validation sequences. On the EuRoC dataset, it realizes an average FPS increase of 30.1%, a 23.1% reduction in median latency, and a 37.7% reduction in peak GPU memory reservation, maintaining comparable trajectory accuracy (ATE) across 11 validation sequences. Experimental results demonstrate that DPVO-QAT++ effectively bridges the gap between high-precision deep VO and the efficiency requirements for practical deployment, offering a viable engineering paradigm for the application of this technology on real-world embedded platforms.
  Keywords: Visual Odometry, Heterogeneous Precision Architecture, Quantization-Aware Training, CUDA Kernel Fusion, Scale-Only Training, Deep Patch Visual Odometry, GPU-Native Kernel Fusion.

</details>


### [149] [Toward Real-world Text Image Forgery Localization: Structured and Interpretable Data Synthesis](https://arxiv.org/abs/2511.12658)
*Zeqin Yu,Haotao Xie,Jian Zhang,Jiangqun Ni,Wenkan Su,Jiwu Huang*

Main category: cs.CV

TL;DR: A Fourier-series-inspired framework (FSTS) synthesizes realistic tampered text images by modeling per-parameter operations with basis configurations and population distributions learned from large real-world tamper traces, improving cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: T-IFL models struggle to generalize due to small real-world tampering datasets and a distribution gap from synthetic data that misses real-world complexity.

Method: Collect 16,750 real-world tampering instances across five tampering types with a structured pipeline recording editing traces via multi-format logs (video, PSD, editing logs). Analyze parameters to identify patterns at individual and population levels; build a hierarchical model where each tampering parameter is a compact combination of basis operation-parameter configurations; compose the population distribution by aggregating behaviors; use Fourier-series-inspired basis functions and learned weights; sample from this model to synthesize diverse, realistic training data.

Result: Models trained with FSTS data show significantly improved generalization on real-world datasets across four evaluation protocols.

Conclusion: FSTS provides an interpretable, distribution-aware approach to synthetic tampering data generation that enhances cross-domain T-IFL performance; dataset is publicly available.

Abstract: Existing Text Image Forgery Localization (T-IFL) methods often suffer from poor generalization due to the limited scale of real-world datasets and the distribution gap caused by synthetic data that fails to capture the complexity of real-world tampering. To tackle this issue, we propose Fourier Series-based Tampering Synthesis (FSTS), a structured and interpretable framework for synthesizing tampered text images. FSTS first collects 16,750 real-world tampering instances from five representative tampering types, using a structured pipeline that records human-performed editing traces via multi-format logs (e.g., video, PSD, and editing logs). By analyzing these collected parameters and identifying recurring behavioral patterns at both individual and population levels, we formulate a hierarchical modeling framework. Specifically, each individual tampering parameter is represented as a compact combination of basis operation-parameter configurations, while the population-level distribution is constructed by aggregating these behaviors. Since this formulation draws inspiration from the Fourier series, it enables an interpretable approximation using basis functions and their learned weights. By sampling from this modeled distribution, FSTS synthesizes diverse and realistic training data that better reflect real-world forgery traces. Extensive experiments across four evaluation protocols demonstrate that models trained with FSTS data achieve significantly improved generalization on real-world datasets. Dataset is available at \href{https://github.com/ZeqinYu/FSTS}{Project Page}.

</details>


### [150] [Hi-Reco: High-Fidelity Real-Time Conversational Digital Humans](https://arxiv.org/abs/2511.12662)
*Hongbin Huang,Junwei Li,Tianxin Xie,Zhuang Li,Cekai Weng,Yaodong Yang,Yue Luo,Li Liu,Jing Tang,Zhijing Shao,Zeyu Wang*

Main category: cs.CV

TL;DR: A real-time, high-fidelity digital human system that combines a realistic 3D avatar, persona-driven expressive speech, and knowledge-grounded dialogue using asynchronous multi-modal orchestration and retrieval-augmented methods.


<details>
  <summary>Details</summary>
Motivation: To overcome the trade-off between visual realism and real-time responsiveness in digital humans, enabling believable, interactive experiences for communication, education, and entertainment.

Method: An asynchronous execution pipeline coordinating multi-modal components (avatar rendering, expressive speech synthesis with prosody, and knowledge-grounded dialogue generation) including wake word detection, emotionally expressive prosody, and context-aware response generation; retrieval-augmented generation with history augmentation and intent-based routing for efficient knowledge access.

Result: Demonstrates a coherent, integrated system that enables responsive and believable digital humans suitable for immersive applications in communication, education, and entertainment.

Conclusion: An integrated framework for high-fidelity, real-time digital humans that supports responsive interaction and broad applicability, with potential for further improvements in latency, evaluation, and deployment.

Abstract: High-fidelity digital humans are increasingly used in interactive applications, yet achieving both visual realism and real-time responsiveness remains a major challenge. We present a high-fidelity, real-time conversational digital human system that seamlessly combines a visually realistic 3D avatar, persona-driven expressive speech synthesis, and knowledge-grounded dialogue generation. To support natural and timely interaction, we introduce an asynchronous execution pipeline that coordinates multi-modal components with minimal latency. The system supports advanced features such as wake word detection, emotionally expressive prosody, and highly accurate, context-aware response generation. It leverages novel retrieval-augmented methods, including history augmentation to maintain conversational flow and intent-based routing for efficient knowledge access. Together, these components form an integrated system that enables responsive and believable digital humans, suitable for immersive applications in communication, education, and entertainment.

</details>


### [151] [DensePercept-NCSSD: Vision Mamba towards Real-time Dense Visual Perception with Non-Causal State Space Duality](https://arxiv.org/abs/2511.12671)
*Tushar Anand,Advik Sinha,Abhijit Das*

Main category: cs.CV

TL;DR: Real-time dense 3D perception using a fast non-causal Mamba-block architecture that fuses pairwise images for optical flow and disparity with low GPU usage and high accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable real-time, resource-efficient, unified 3D dense perception (optical flow and disparity) in dynamic environments.

Method: Proposes a non-causal selective state-space model based on Mamba blocks that fuses pairwise input images to jointly estimate dense optical flow and disparity, optimized for real-time inference and low GPU usage.

Result: Inference times are reduced while maintaining high accuracy and low GPU usage; validated in real-life scenarios; code and models available at GitHub repository.

Conclusion: The approach enables unified, real-time, and accurate 3D dense perception estimation, with open-source code for deployment and experimentation.

Abstract: In this work, we propose an accurate and real-time optical flow and disparity estimation model by fusing pairwise input images in the proposed non-causal selective state space for dense perception tasks. We propose a non-causal Mamba block-based model that is fast and efficient and aptly manages the constraints present in a real-time applications. Our proposed model reduces inference times while maintaining high accuracy and low GPU usage for optical flow and disparity map generation. The results and analysis, and validation in real-life scenario justify that our proposed model can be used for unified real-time and accurate 3D dense perception estimation tasks. The code, along with the models, can be found at https://github.com/vimstereo/DensePerceptNCSSD

</details>


### [152] [Appreciate the View: A Task-Aware Evaluation Framework for Novel View Synthesis](https://arxiv.org/abs/2511.12675)
*Saar Stern,Ido Sobol,Or Litany*

Main category: cs.CV

TL;DR: A task-aware evaluation framework for novel view synthesis (NVS) that uses Zero123 features plus lightweight tuning to derive two metrics, D_PRISM (reference-based) and MMD_PRISM (reference-free), providing reliable detection of incorrect generations and rankings aligned with human preferences across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation metrics for NVS struggle to simultaneously assess realism and faithfulness to source view and intended viewpoint change. There is a fundamental need for reliable, task-aware metrics that correlate with human judgments to guide model development.

Method: Leverage features from the strong NVS foundation model Zero123, with a lightweight tuning step to improve discrimination. Introduce two complementary metrics: D_PRISM (reference-based) and MMD_PRISM (reference-free). Evaluate on six NVS methods across Toys4K, Google Scanned Objects (GSO), and OmniObject3D.

Result: Both metrics reliably identify incorrect generations and rank models in agreement with human preference studies. MMD_PRISM, in particular, yields a clear and stable ranking across benchmarks, with lower scores indicating stronger models.

Conclusion: The framework provides a principled and practical approach to assess synthesis quality in NVS, addressing a key gap in evaluation and enabling more reliable progress. It demonstrates cross-benchmark applicability and supports robust model ranking and selection.

Abstract: The goal of Novel View Synthesis (NVS) is to generate realistic images of a given content from unseen viewpoints. But how can we trust that a generated image truly reflects the intended transformation? Evaluating its reliability remains a major challenge. While recent generative models, particularly diffusion-based approaches, have significantly improved NVS quality, existing evaluation metrics struggle to assess whether a generated image is both realistic and faithful to the source view and intended viewpoint transformation. Standard metrics, such as pixel-wise similarity and distribution-based measures, often mis-rank incorrect results as they fail to capture the nuanced relationship between the source image, viewpoint change, and generated output. We propose a task-aware evaluation framework that leverages features from a strong NVS foundation model, Zero123, combined with a lightweight tuning step to enhance discrimination. Using these features, we introduce two complementary evaluation metrics: a reference-based score, $D_{\text{PRISM}}$, and a reference-free score, $\text{MMD}_{\text{PRISM}}$. Both reliably identify incorrect generations and rank models in agreement with human preference studies, addressing a fundamental gap in NVS evaluation. Our framework provides a principled and practical approach to assessing synthesis quality, paving the way for more reliable progress in novel view synthesis. To further support this goal, we apply our reference-free metric to six NVS methods across three benchmarks: Toys4K, Google Scanned Objects (GSO), and OmniObject3D, where $\text{MMD}_{\text{PRISM}}$ produces a clear and stable ranking, with lower scores consistently indicating stronger models.

</details>


### [153] [BridgeEQA: Virtual Embodied Agents for Real Bridge Inspections](https://arxiv.org/abs/2511.12676)
*Subin Varghese,Joshua Gao,Asad Ur Rahman,Vedhus Hoskere*

Main category: cs.CV

TL;DR: Introduces BridgeEQA, a large open-vocabulary EQA benchmark for infrastructure inspection grounded in real bridge scenes, plus EMVR for embodied memory visual reasoning; shows large gaps for existing models and provides release of dataset and code.


<details>
  <summary>Details</summary>
Motivation: Fill the lack of realistic benchmarks for open-vocabulary Embodied Question Answering (EQA) in real-world, multi-image, and long-range reasoning tasks, using standardized NBI ratings and professional reports to enable rigorous evaluation.

Method: BridgeEQA comprises 2,200 open-vocabulary questions grounded in 200 real-world bridge scenes (avg ~48 images per scene) with NBI condition alignment; introduces Image Citation Relevance as a new evaluation metric; proposes EMVR—an embodied memory visual reasoning framework treating the scene as an image-based graph where an agent navigates views to gather evidence within a Markov decision process.

Result: Baseline vision-language models show substantial performance gaps under episodic memory EQA; EMVR achieves strong performance over baselines on this benchmark.

Conclusion: Dataset and code are publicly released, enabling further research in embodied QA for infrastructure inspection and long-range multimodal reasoning.

Abstract: Deploying embodied agents that can answer questions about their surroundings in realistic real-world settings remains difficult, partly due to the scarcity of benchmarks that faithfully capture practical operating conditions. We propose infrastructure inspection as a compelling domain for open-vocabulary Embodied Question Answering (EQA): it naturally demands multi-scale reasoning, long-range spatial understanding, and complex semantic relationships, while offering unique evaluation advantages via standardized National Bridge Inventory (NBI) condition ratings (0-9), professional inspection reports, and egocentric imagery.
  We introduce BridgeEQA, a benchmark of 2,200 open-vocabulary question-answer pairs (in the style of OpenEQA) grounded in professional inspection reports across 200 real-world bridge scenes with 47.93 images on average per scene. Questions require synthesizing visual evidence across multiple images and aligning responses with NBI condition ratings. We further propose a new EQA metric Image Citation Relevance to evaluate the ability of a model to cite relevant images.
  Evaluations of state-of-the-art vision-language models reveal substantial performance gaps under episodic memory EQA settings. To address this, we propose Embodied Memory Visual Reasoning (EMVR), which formulates inspection as sequential navigation over an image-based scene graph: images are nodes, and an agent takes actions to traverse views, compare evidence, and reason within a Markov decision process. EMVR shows strong performance over the baselines. We publicly release both the dataset and code.

</details>


### [154] [R$^{2}$Seg: Training-Free OOD Medical Tumor Segmentation via Anatomical Reasoning and Statistical Rejection](https://arxiv.org/abs/2511.12691)
*Shuaike Shen,Ke Liu,Jiaqing Xie,Shangde Gao,Chunhua Shen,Ge Liu,Mireia Crispin-Ortuzar,Shangqi Gao*

Main category: cs.CV

TL;DR: A training-free robust OOD tumor segmentation framework (R²Seg) using a two-stage Reason-and-Reject pipeline: (1) Reason with an LLM-guided anatomical planner to localize organ anchors and multi-scale ROIs; (2) Reject with a two-sample statistical test on candidates from a frozen model to filter out normal-tissue-like false positives. No parameter updates, compatible with zero-update test-time augmentation and avoiding catastrophic forgetting, shows improved Dice, specificity, and sensitivity on multi-center/multi-modal benchmarks.


<details>
  <summary>Details</summary>
Motivation: Foundation models for medical image segmentation struggle under out-of-distribution shifts, often yielding fragmented false positives on OOD tumors. A training-free, update-free method that robustly suppresses false positives without forgetting prior knowledge is desirable.

Method: Two-stage approach: (1) Reason: an LLM-guided anatomical reasoning planner localizes organ anchors and generates multi-scale regions of interest (ROIs). (2) Reject: a two-sample statistical test is applied to candidates produced by a frozen foundation model (BiomedParse) within the ROIs, retaining only candidates significantly different from normal tissue and thereby suppressing false positives. The method requires no parameter updates and is compatible with zero-update test-time augmentation (TTA).

Result: On multi-center and multi-modal tumor segmentation benchmarks, R²Seg substantially improves Dice, specificity, and sensitivity compared to strong baselines and the original foundation models.

Conclusion: A training-free, update-free framework that robustly handles OOD tumor segmentation by combining LLM-guided reasoning with statistical rejection, avoiding catastrophic forgetting and enabling effective TTA; code available at the provided GitHub repository.

Abstract: Foundation models for medical image segmentation struggle under out-of-distribution (OOD) shifts, often producing fragmented false positives on OOD tumors. We introduce R$^{2}$Seg, a training-free framework for robust OOD tumor segmentation that operates via a two-stage Reason-and-Reject process. First, the Reason step employs an LLM-guided anatomical reasoning planner to localize organ anchors and generate multi-scale ROIs. Second, the Reject step applies two-sample statistical testing to candidates generated by a frozen foundation model (BiomedParse) within these ROIs. This statistical rejection filter retains only candidates significantly different from normal tissue, effectively suppressing false positives. Our framework requires no parameter updates, making it compatible with zero-update test-time augmentation and avoiding catastrophic forgetting. On multi-center and multi-modal tumor segmentation benchmarks, R$^{2}$Seg substantially improves Dice, specificity, and sensitivity over strong baselines and the original foundation models. Code are available at https://github.com/Eurekashen/R2Seg.

</details>


### [155] [HEDGE: Hallucination Estimation via Dense Geometric Entropy for VQA with Vision-Language Models](https://arxiv.org/abs/2511.12693)
*Sushant Gautam,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: HEDGE is a unified framework for hallucination detection in vision-language models that combines controlled visual perturbations, semantic clustering, and robust uncertainty metrics to form a reproducible evaluation pipeline. It reveals architecture- and prompt-dependent detectability across VLMs and highlights VASE with embedding clustering and moderate sampling as a robust signal; a Hedge-bench benchmarking tool is provided.


<details>
  <summary>Details</summary>
Motivation: Vision-language models frequently hallucinate; there is a need for a principled, cross-architecture method to detect and quantify hallucinations and a reproducible benchmark to compare reliability across architectures and prompts.

Method: A pipeline that includes sampling, distortion synthesis, and clustering (both entailment- and embedding-based) plus uncertainty-based metrics. It evaluates on VQA-RAD and KvasirVQA-x1 using three VLMs (LLaVA-Med, Med-Gemma, Qwen2.5-VL), analyzes tokenization density, clustering type, prompt design, and sampling budgets. The VASE metric is used for robust hallucination signaling. All resources are packaged in the hedge-bench library for reproducible benchmarking.

Result: Hallucination detectability varies by architecture and prompt. Unified-fusion models with dense tokenization (Qwen2.5-VL) show the highest detectability, while architectures with restricted tokenization (Med-Gemma) show lower detectability. Embedding-based clustering yields stronger separation on generated answers, whereas NLI-based clustering is advantageous for LLaVA-Med and longer responses. Across configurations, VASE provides the most robust signal, especially with embedding clustering and a moderate sampling budget (n ≈ 10–15). Prompt design matters: concise, label-style outputs produce clearer semantic structure than single-sentence prompts.

Conclusion: Framing hallucination detection as a geometric robustness problem across sampling scale, prompt structure, model architecture, and clustering strategy offers a principled, compute-aware approach to evaluating multimodal reliability. The hedge-bench library facilitates reproducible benchmarking and resource sharing for multimodal robustness research.

Abstract: Vision-language models (VLMs) enable open-ended visual question answering but remain prone to hallucinations. We present HEDGE, a unified framework for hallucination detection that combines controlled visual perturbations, semantic clustering, and robust uncertainty metrics. HEDGE integrates sampling, distortion synthesis, clustering (entailment- and embedding-based), and metric computation into a reproducible pipeline applicable across multimodal architectures.
  Evaluations on VQA-RAD and KvasirVQA-x1 with three representative VLMs (LLaVA-Med, Med-Gemma, Qwen2.5-VL) reveal clear architecture- and prompt-dependent trends. Hallucination detectability is highest for unified-fusion models with dense visual tokenization (Qwen2.5-VL) and lowest for architectures with restricted tokenization (Med-Gemma). Embedding-based clustering often yields stronger separation when applied directly to the generated answers, whereas NLI-based clustering remains advantageous for LLaVA-Med and for longer, sentence-level responses. Across configurations, the VASE metric consistently provides the most robust hallucination signal, especially when paired with embedding clustering and a moderate sampling budget (n ~ 10-15). Prompt design also matters: concise, label-style outputs offer clearer semantic structure than syntactically constrained one-sentence responses.
  By framing hallucination detection as a geometric robustness problem shaped jointly by sampling scale, prompt structure, model architecture, and clustering strategy, HEDGE provides a principled, compute-aware foundation for evaluating multimodal reliability. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE .

</details>


### [156] [X-VMamba: Explainable Vision Mamba](https://arxiv.org/abs/2511.12694)
*Mohamed A. Mabrok,Yalda Zafari*

Main category: cs.CV

TL;DR: Introduces a controllability-based interpretability framework for Vision SSMs, with Jacobian-based and Gramian-based influence measures; linear-time forward passes; validates on medical imaging; reveals hierarchical feature refinement and domain-specific signatures; broad applicability and forthcoming code.


<details>
  <summary>Details</summary>
Motivation: SSMs lack transparent, attention-like mechanisms for processing spatial information, making it difficult to interpret how input tokens influence internal state dynamics; a unified controllability-based framework is needed to understand and diagnose SSM behavior across domains.

Method: Two complementary formulations: (1) a Jacobian-based method applicable to any SSM architecture that measures influence through the full chain of state propagation; (2) a Gramian-based approach for diagonal SSMs that yields closed-form analytical solutions for faster computation. Both operate in a single forward pass with linear complexity, require no architectural changes, and avoid hyperparameter tuning.

Result: Empirically validates the framework on three diverse medical imaging modalities, revealing that SSMs perform hierarchical feature refinement—from diffuse low-level textures in early layers to clinically meaningful patterns in deeper layers. Identifies domain-specific controllability signatures aligned with diagnostic criteria, shows progressive spatial selectivity across network depth, and notes the substantial influence of scanning strategies on attention patterns. Arguments extend to computer vision, NLP, and cross-domain tasks, framing controllability analysis as a unified interpretability paradigm for SSMs.

Conclusion: Controllability analysis establishes a unified, foundational interpretability paradigm for SSMs across domains; code and analysis tools are to be released upon publication.

Abstract: State Space Models (SSMs), particularly the Mamba architecture, have recently emerged as powerful alternatives to Transformers for sequence modeling, offering linear computational complexity while achieving competitive performance. Yet, despite their effectiveness, understanding how these Vision SSMs process spatial information remains challenging due to the lack of transparent, attention-like mechanisms. To address this gap, we introduce a controllability-based interpretability framework that quantifies how different parts of the input sequence (tokens or patches) influence the internal state dynamics of SSMs. We propose two complementary formulations: a Jacobian-based method applicable to any SSM architecture that measures influence through the full chain of state propagation, and a Gramian-based approach for diagonal SSMs that achieves superior speed through closed-form analytical solutions. Both methods operate in a single forward pass with linear complexity, requiring no architectural modifications or hyperparameter tuning. We validate our framework through experiments on three diverse medical imaging modalities, demonstrating that SSMs naturally implement hierarchical feature refinement from diffuse low-level textures in early layers to focused, clinically meaningful patterns in deeper layers. Our analysis reveals domain-specific controllability signatures aligned with diagnostic criteria, progressive spatial selectivity across the network hierarchy, and the substantial influence of scanning strategies on attention patterns. Beyond medical imaging, we articulate applications spanning computer vision, natural language processing, and cross-domain tasks. Our framework establishes controllability analysis as a unified, foundational interpretability paradigm for SSMs across all domains. Code and analysis tools will be made available upon publication

</details>


### [157] [Counting Through Occlusion: Framework for Open World Amodal Counting](https://arxiv.org/abs/2511.12702)
*Safaeid Hossain Arib,Rabeya Akter,Abdul Monaf Chowdhury,Md Jubair Ahmed Sourov,Md Mehedi Hasan*

Main category: cs.CV

TL;DR: CountOCC introduces amodal counting by reconstructing occluded object features via hierarchical multimodal guidance, achieving state-of-the-art performance under occlusion and strong cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: Occlusion causes backbone encodings to emphasize occluding surfaces instead of target objects, corrupting feature representations needed for accurate counting; a robust amodal counting approach is needed.

Method: Use hierarchical multimodal guidance to synthesize complete representations by fusing spatial context from visible fragments with semantic priors from text and visual embeddings; generate class-discriminative features at occluded locations across multiple pyramid levels; enforce a visual equivalence objective that aligns attention maps between occluded and unoccluded views across gradient-based attention.

Result: Establish occlusion-augmented versions of FSC 147 and CARPK; CountOCC achieves SOTA on FSC 147 with 26.72% MAE reduction in validation and 20.80% in test; sets new SOTA on CARPK with 49.89% MAE reduction and on CAPTUREReal with 28.79% MAE reduction; code will be released soon.

Conclusion: The proposed amodal counting framework, combining multimodal reconstruction with a visual equivalence objective, yields robust amodal counting across structured and unstructured scenes and demonstrates strong cross-domain generalization.

Abstract: Object counting has achieved remarkable success on visible instances, yet state-of-the-art (SOTA) methods fail under occlusion, a pervasive challenge in real world deployment. This failure stems from a fundamental architectural limitation where backbone networks encode occluding surfaces rather than target objects, thereby corrupting the feature representations required for accurate enumeration. To address this, we present CountOCC, an amodal counting framework that explicitly reconstructs occluded object features through hierarchical multimodal guidance. Rather than accepting degraded encodings, we synthesize complete representations by integrating spatial context from visible fragments with semantic priors from text and visual embeddings, generating class-discriminative features at occluded locations across multiple pyramid levels. We further introduce a visual equivalence objective that enforces consistency in attention space, ensuring that both occluded and unoccluded views of the same scene produce spatially aligned gradient-based attention maps. Together, these complementary mechanisms preserve discriminative properties essential for accurate counting under occlusion. For rigorous evaluation, we establish occlusion-augmented versions of FSC 147 and CARPK spanning both structured and unstructured scenes. CountOCC achieves SOTA performance on FSC 147 with 26.72% and 20.80% MAE reduction over prior baselines under occlusion in validation and test, respectively. CountOCC also demonstrates exceptional generalization by setting new SOTA results on CARPK with 49.89% MAE reduction and on CAPTUREReal with 28.79% MAE reduction, validating robust amodal counting across diverse visual domains. Code will be released soon.

</details>


### [158] [FSDAM: Few-Shot Driving Attention Modeling via Vision-Language Coupling](https://arxiv.org/abs/2511.12708)
*Kaiser Hamid,Can Cui,Khandakar Ashrafi Akbar,Ziran Wang,Nade Liang*

Main category: cs.CV

TL;DR: FSDAM: a two-pathway, few-shot framework for joint driver attention prediction and caption generation using ~100 annotations, achieving competitive attention performance and zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Reduce dependence on large gaze datasets for explainable driver attention; enable data-constrained deployment while preserving interpretability.

Method: Dual-pathway architecture: separate modules for spatial attention prediction and caption generation; cross-modal alignment to maintain semantic consistency; leverages ~100 annotated examples; aims for joint optimization under few-shot supervision.

Result: Competitive attention prediction; coherent, context-aware explanations; robust zero-shot generalization across multiple driving benchmarks.

Conclusion: Effective attention-conditioned generation is achievable with limited supervision, enabling practical deployment of explainable driver attention systems in data-constrained scenarios.

Abstract: Understanding where drivers look and why they shift their attention is essential for autonomous systems that read human intent and justify their actions. Most existing models rely on large-scale gaze datasets to learn these patterns; however, such datasets are labor-intensive to collect and time-consuming to curate. We present FSDAM (Few-Shot Driver Attention Modeling), a framework that achieves joint attention prediction and caption generation with approximately 100 annotated examples, two orders of magnitude fewer than existing approaches. Our approach introduces a dual-pathway architecture where separate modules handle spatial prediction and caption generation while maintaining semantic consistency through cross-modal alignment. Despite minimal supervision, FSDAM achieves competitive performance on attention prediction, generates coherent, and context-aware explanations. The model demonstrates robust zero-shot generalization across multiple driving benchmarks. This work shows that effective attention-conditioned generation is achievable with limited supervision, opening new possibilities for practical deployment of explainable driver attention systems in data-constrained scenarios.

</details>


### [159] [Backdoor Attacks on Open Vocabulary Object Detectors via Multi-Modal Prompt Tuning](https://arxiv.org/abs/2511.12735)
*Ankita Raj,Chetan Arora*

Main category: cs.CV

TL;DR: TrAP is a multi-modal backdoor attack on open-vocabulary object detectors that uses trigger-aware prompt tuning to jointly optimize image and text prompts along with visual triggers, enabling effective backdoors without modifying base model weights.


<details>
  <summary>Details</summary>
Motivation: Open-vocabulary object detectors (OVODs) are increasingly deployed in high-stakes settings. Their reliance on prompts makes them potentially vulnerable to backdoors; understanding and mitigating such risks is crucial.

Method: TrAP jointly optimizes lightweight prompt tokens in both image and text modalities and incorporates visual triggers. It uses a curriculum-based training strategy that progressively shrinks trigger size, enabling backdoor activation with small patches at inference, without retraining the base model weights.

Result: The approach achieves high attack success rates for object misclassification and object disappearance while also improving clean-image performance on downstream tasks compared to the zero-shot baseline.

Conclusion: The work reveals a new attack surface in OVODs introduced by prompt tuning and demonstrates that backdoors can be embedded via lightweight prompts without altering base model weights. This highlights the need for defense strategies targeting prompt-based vulnerabilities in multi-modal detectors.

Abstract: Open-vocabulary object detectors (OVODs) unify vision and language to detect arbitrary object categories based on text prompts, enabling strong zero-shot generalization to novel concepts. As these models gain traction in high-stakes applications such as robotics, autonomous driving, and surveillance, understanding their security risks becomes crucial. In this work, we conduct the first study of backdoor attacks on OVODs and reveal a new attack surface introduced by prompt tuning. We propose TrAP (Trigger-Aware Prompt tuning), a multi-modal backdoor injection strategy that jointly optimizes prompt parameters in both image and text modalities along with visual triggers. TrAP enables the attacker to implant malicious behavior using lightweight, learnable prompt tokens without retraining the base model weights, thus preserving generalization while embedding a hidden backdoor. We adopt a curriculum-based training strategy that progressively shrinks the trigger size, enabling effective backdoor activation using small trigger patches at inference. Experiments across multiple datasets show that TrAP achieves high attack success rates for both object misclassification and object disappearance attacks, while also improving clean image performance on downstream datasets compared to the zero-shot setting.

</details>


### [160] [Direct Visual Grounding by Directing Attention of Visual Tokens](https://arxiv.org/abs/2511.12738)
*Parsa Esmaeilkhani,Longin Jan Latecki*

Main category: cs.CV

TL;DR: KL attention supervision (KLAL) for Vision-Language Models aligns visual token attention with language tokens via KL-divergence, improving visual grounding tasks and line-tracing capabilities without extra labels; achieves notable gains on geometric tasks, pointing, and referring expression tasks, including real data and synthetic cases.


<details>
  <summary>Details</summary>
Motivation: Visual tokens most related to a query receive little attention in the LLM module of VLMs, and standard next-token prediction loss provides weak supervision for attending to visual tokens. Directly supervising cross-modal attention could improve visual reasoning and grounding.

Method: Propose KLAL: a loss that directly supervises the attention distribution of visual tokens by aligning it (via KL divergence) to ground-truth attention maps. Ground-truth maps come from task geometry in synthetic settings or from grounding annotations (boxes/points) in real images. The KLAL is used inside the LLM training to steer attention, in addition to the normal NTP loss, without requiring new labels beyond existing annotations.

Result: KLAL combined with NTP yields notable improvements on geometric tasks, pointing, and referring expression comprehension across synthetic and real data. A new dataset for line tracing exposes weaknesses in commercial VLMs.

Conclusion: Directly supervising visual-token attention with KL divergence effectively grounds answer tokens in images and improves visual reasoning tasks in VLMs, achieving gains without additional labeling; the line-tracing dataset highlights remaining limitations in current commercial models.

Abstract: Vision Language Models (VLMs) mix visual tokens and text tokens. A puzzling issue is the fact that visual tokens most related to the query receive little to no attention in the final layers of the LLM module of VLMs from the answer tokens, where all tokens are treated equally, in particular, visual and language tokens in the LLM attention layers. This fact may result in wrong answers to visual questions, as our experimental results confirm. It appears that the standard next-token prediction (NTP) loss provides an insufficient signal for directing attention to visual tokens. We hypothesize that a more direct supervision of the attention of visual tokens to corresponding language tokens in the LLM module of VLMs will lead to improved performance on visual tasks. To demonstrate that this is indeed the case, we propose a novel loss function that directly supervises the attention of visual tokens. It directly grounds the answer language tokens in images by directing their attention to the relevant visual tokens. This is achieved by aligning the attention distribution of visual tokens to ground truth attention maps with KL divergence. The ground truth attention maps are obtained from task geometry in synthetic cases or from standard grounding annotations (e.g., bounding boxes or point annotations) in real images, and are used inside the LLM for attention supervision without requiring new labels. The obtained KL attention loss (KLAL) when combined with NTP encourages VLMs to attend to relevant visual tokens while generating answer tokens. This results in notable improvements across geometric tasks, pointing, and referring expression comprehension on both synthetic and real-world data, as demonstrated by our experiments. We also introduce a new dataset to evaluate the line tracing abilities of VLMs. Surprisingly, even commercial VLMs do not perform well on this task.

</details>


### [161] [Deep Imbalanced Multi-Target Regression: 3D Point Cloud Voxel Content Estimation in Simulated Forests](https://arxiv.org/abs/2511.12740)
*Amirhossein Hassanzadeh,Bartosz Krawczyk,Michael Saunders,Rob Wible,Keith Krause,Dimah Dera,Jan van Aardt*

Main category: cs.CV

TL;DR: Investigates estimating within-voxel occupancy for LiDAR voxels in DIRSIG-simulated forest data via multi-target regression with KPConv, using density-based relevance to address imbalance; finds larger voxels reduce error while smaller voxels increase error, especially in canopy; voxel size choice is application-dependent and the work addresses imbalanced deep learning for multi-target regression in 3D forest LiDAR.


<details>
  <summary>Details</summary>
Motivation: Voxelization reduces LiDAR computation but loses fine structural details. The study aims to recover or infer low-level voxel content (occupancy) from high-level voxelized data, for forest materials, under imbalanced conditions.

Method: Multi-target regression using Kernel Point Convolutions (KPConv) with cost-sensitive learning via density-based relevance (DBR). Optimization enhanced by weighted MSE, Focal Regression, and regularization. Sensitivity analysis over voxel sizes from 0.25 to 2 meters on DIRSIG-simulated data, evaluating bark, leaf, soil, and misc materials.

Result: Larger voxel sizes (2 m) yield lower errors due to reduced variability; smaller voxel sizes (0.25–0.5 m) show higher errors, particularly within the canopy. Bark and leaf targets exhibit notably higher errors at small voxel sizes, indicating difficulty in estimating within-canopy voxel content at fine resolutions.

Conclusion: The work highlights a voxel-size trade-off and confirms the need for application-dependent voxel choices. It also contributes to deep imbalance learning for multi-target regression in simulated 3D forest LiDAR, filling a gap in methods using DP/DP-like cost-sensitive approaches with KPConv.

Abstract: Voxelization is an effective approach to reduce the computational cost of processing Light Detection and Ranging (LiDAR) data, yet it results in a loss of fine-scale structural information. This study explores whether low-level voxel content information, specifically target occupancy percentage within a voxel, can be inferred from high-level voxelized LiDAR point cloud data collected from Digital Imaging and remote Sensing Image Generation (DIRSIG) software. In our study, the targets include bark, leaf, soil, and miscellaneous materials. We propose a multi-target regression approach in the context of imbalanced learning using Kernel Point Convolutions (KPConv). Our research leverages cost-sensitive learning to address class imbalance called density-based relevance (DBR). We employ weighted Mean Saquared Erorr (MSE), Focal Regression (FocalR), and regularization to improve the optimization of KPConv. This study performs a sensitivity analysis on the voxel size (0.25 - 2 meters) to evaluate the effect of various grid representations in capturing the nuances of the forest. This sensitivity analysis reveals that larger voxel sizes (e.g., 2 meters) result in lower errors due to reduced variability, while smaller voxel sizes (e.g., 0.25 or 0.5 meter) exhibit higher errors, particularly within the canopy, where variability is greatest. For bark and leaf targets, error values at smaller voxel size datasets (0.25 and 0.5 meter) were significantly higher than those in larger voxel size datasets (2 meters), highlighting the difficulty in accurately estimating within-canopy voxel content at fine resolutions. This suggests that the choice of voxel size is application-dependent. Our work fills the gap in deep imbalance learning models for multi-target regression and simulated datasets for 3D LiDAR point clouds of forests.

</details>


### [162] [SAGE: Saliency-Guided Contrastive Embeddings](https://arxiv.org/abs/2511.12744)
*Colton R. Crum,Adam Czajka*

Main category: cs.CV

TL;DR: SAGE uses human saliency to sculpt latent embeddings via a contrastive triplet loss, with saliency-preserving and saliency-degrading input augmentations and a logit sanity check, yielding improved generalization and state-of-the-art performance across open- and closed-set tasks and diverse backbones.


<details>
  <summary>Details</summary>
Motivation: Integrating human perceptual priors can improve generalization and align models with human expertise, but saliency guidance in image space can be unreliable due to model-specific saliency baselines. Placing guidance in the model's latent space can leverage richer representations and produce more robust regularization.

Method: Introduce SAGE (Saliency-Guided Contrastive Embeddings) loss that operates in latent space via a contrastive triplet objective. Apply salient-preserving and saliency-degrading signal augmentations to inputs, track changes in embeddings and logits, and push the model to emphasize salient features while discounting non-salient ones. Include a sanity check on logit distributions to ensure they reflect saliency-based augmentations. Evaluate across open- and closed-set datasets, multiple backbones, and tasks to demonstrate generalization.

Result: Demonstrates a boost in classification performance over state-of-the-art saliency-based methods, with consistent gains across a range of backbones and scenarios, indicating strong generalization across tasks.

Conclusion: Guiding training through the model’s latent space with human saliency via SAGE provides robust generalization and improved performance, addressing the unreliability of image-space saliency cues and enabling broad applicability across architectures and tasks.

Abstract: Integrating human perceptual priors into the training of neural networks has been shown to raise model generalization, serve as an effective regularizer, and align models with human expertise for applications in high-risk domains. Existing approaches to integrate saliency into model training often rely on internal model mechanisms, which recent research suggests may be unreliable. Our insight is that many challenges associated with saliency-guided training stem from the placement of the guidance approaches solely within the image space. Instead, we move away from the image space, use the model's latent space embeddings to steer human guidance during training, and we propose SAGE (Saliency-Guided Contrastive Embeddings): a loss function that integrates human saliency into network training using contrastive embeddings. We apply salient-preserving and saliency-degrading signal augmentations to the input and capture the changes in embeddings and model logits. We guide the model towards salient features and away from non-salient features using a contrastive triplet loss. Additionally, we perform a sanity check on the logit distributions to ensure that the model outputs match the saliency-based augmentations. We demonstrate a boost in classification performance across both open- and closed-set scenarios against SOTA saliency-based methods, showing SAGE's effectiveness across various backbones, and include experiments to suggest its wide generalization across tasks.

</details>


### [163] [Which Way from B to A: The role of embedding geometry in image interpolation for Stable Diffusion](https://arxiv.org/abs/2511.12757)
*Nicholas Karris,Luke Durell,Javier Flores,Tegan Emerson*

Main category: cs.CV

TL;DR: Viewing CLIP embeddings as Wasserstein point clouds enables OT-based geodesic interpolation, producing smoother Stable Diffusion images.


<details>
  <summary>Details</summary>
Motivation: To understand the true geometry of CLIP embedding space and improve interpolation quality by leveraging its intrinsic geometry.

Method: Treat embedding rows as a point cloud in Wasserstein space; for two prompts, solve an optimal transport problem to obtain a geodesic between embeddings; render intermediate embeddings using Stable Diffusion and compare with standard interpolation methods.

Result: OT-based interpolation yields smoother and more coherent intermediate images than conventional interpolation methods.

Conclusion: Viewing embeddings as Wasserstein point clouds better reflects the geometry of the embedding space and improves interpolation quality in diffusion models.

Abstract: It can be shown that Stable Diffusion has a permutation-invariance property with respect to the rows of Contrastive Language-Image Pretraining (CLIP) embedding matrices. This inspired the novel observation that these embeddings can naturally be interpreted as point clouds in a Wasserstein space rather than as matrices in a Euclidean space. This perspective opens up new possibilities for understanding the geometry of embedding space. For example, when interpolating between embeddings of two distinct prompts, we propose reframing the interpolation problem as an optimal transport problem. By solving this optimal transport problem, we compute a shortest path (or geodesic) between embeddings that captures a more natural and geometrically smooth transition through the embedding space. This results in smoother and more coherent intermediate (interpolated) images when rendered by the Stable Diffusion generative model. We conduct experiments to investigate this effect, comparing the quality of interpolated images produced using optimal transport to those generated by other standard interpolation methods. The novel optimal transport--based approach presented indeed gives smoother image interpolations, suggesting that viewing the embeddings as point clouds (rather than as matrices) better reflects and leverages the geometry of the embedding space.

</details>


### [164] [RoCoISLR: A Romanian Corpus for Isolated Sign Language Recognition](https://arxiv.org/abs/2511.12767)
*Cătălin-Alexandru Rîpanu,Andrei-Theodor Hotnog,Giulia-Stefania Imbrea,Dumitru-Clementin Cercel*

Main category: cs.CV

TL;DR: RoCoISLR is a new Romanian Isolated Sign Language recognition dataset with ~9,000 video samples and ~6,000 standardized glosses, benchmarked across seven state-of-the-art video models; transformer architectures outperform convolutional baselines (Swin Transformer: Top-1 34.1%), highlighting long-tail data challenges and providing foundational resources for RoISLR research.


<details>
  <summary>Details</summary>
Motivation: There is no large-scale, standardized RoISLR dataset, while existing resources focus on other sign languages (e.g., ASL). A unified corpus and benchmarks are needed to advance RoISLR research and address long-tail distribution issues.

Method: Construct RoCoISLR by aggregating multiple sources into a standardized Romanian ISLR corpus (~9k videos, ~6k glosses); establish consistent experimental setups; evaluate seven models (I3D, SlowFast, Swin Transformer, TimeSformer, Uniformer, VideoMAE, PoseConv3D) and compare against WLASL2000; analyze long-tail distribution effects.

Result: Transformer-based architectures outperform convolutional baselines; Swin Transformer achieves Top-1 accuracy of 34.1%. The benchmarks reveal challenges due to long-tail class distributions in low-resource sign languages and establish RoCoISLR as a baseline foundation for RoISLR research.

Conclusion: RoCoISLR provides the initial foundation for systematic RoISLR research, enabling standardized benchmarking and progression in the field; future work should address long-tail distribution, cross-source standardization, and richer model analyses.

Abstract: Automatic sign language recognition plays a crucial role in bridging the communication gap between deaf communities and hearing individuals; however, most available datasets focus on American Sign Language. For Romanian Isolated Sign Language Recognition (RoISLR), no large-scale, standardized dataset exists, which limits research progress. In this work, we introduce a new corpus for RoISLR, named RoCoISLR, comprising over 9,000 video samples that span nearly 6,000 standardized glosses from multiple sources. We establish benchmark results by evaluating seven state-of-the-art video recognition models-I3D, SlowFast, Swin Transformer, TimeSformer, Uniformer, VideoMAE, and PoseConv3D-under consistent experimental setups, and compare their performance with that of the widely used WLASL2000 corpus. According to the results, transformer-based architectures outperform convolutional baselines; Swin Transformer achieved a Top-1 accuracy of 34.1%. Our benchmarks highlight the challenges associated with long-tail class distributions in low-resource sign languages, and RoCoISLR provides the initial foundation for systematic RoISLR research.

</details>


### [165] [Lightweight Optimal-Transport Harmonization on Edge Devices](https://arxiv.org/abs/2511.12785)
*Maria Larchenko,Dmitry Guskov,Alexander Lobashev,Georgy Derevyanko*

Main category: cs.CV

TL;DR: A lightweight on-device color harmonization method for AR using a compact encoder to predict Monge-Kantorovich transport maps (OT-based). It outperforms state-of-the-art methods on real composite AR images and an AR-specific dataset is released.


<details>
  <summary>Details</summary>
Motivation: Color harmonization is needed for seamless AR composites, but real-time, on-device solutions are scarce, hindering integration into AR pipelines.

Method: Proposes MKL-Harmonizer, a lightweight on-device approach that leverages classical optimal transport theory by training a compact encoder to predict the Monge-Kantorovich transport map.

Result: On real composite AR images, MKL-Harmonizer achieves the best aggregated score against state-of-the-art methods on benchmark tests.

Conclusion: The approach enables real-time color harmonization on devices for AR and is complemented by releasing a dedicated AR dataset with pixel-accurate masks to foster further research.

Abstract: Color harmonization adjusts the colors of an inserted object so that it perceptually matches the surrounding image, resulting in a seamless composite. The harmonization problem naturally arises in augmented reality (AR), yet harmonization algorithms are not currently integrated into AR pipelines because real-time solutions are scarce. In this work, we address color harmonization for AR by proposing a lightweight approach that supports on-device inference. For this, we leverage classical optimal transport theory by training a compact encoder to predict the Monge-Kantorovich transport map. We benchmark our MKL-Harmonizer algorithm against state-of-the-art methods and demonstrate that for real composite AR images our method achieves the best aggregated score. We release our dedicated AR dataset of composite images with pixel-accurate masks and data-gathering toolkit to support further data acquisition by researchers.

</details>


### [166] [Enhancing Neuro-Oncology Through Self-Assessing Deep Learning Models for Brain Tumor Unified Model for MRI Segmentation](https://arxiv.org/abs/2511.12801)
*Andrew Zhou*

Main category: cs.CV

TL;DR: Uncertainty-aware, context-rich brain tumor segmentation: a single-pass framework that adds voxel-wise uncertainty to nnUNet and unifies tumor with healthy brain anatomy, enabling overlaid uncertainty maps for clinical use.


<details>
  <summary>Details</summary>
Motivation: Clinical segmentation requires both accurate tumor delineation and confidence estimates, plus anatomical context around tumors for surgical planning; current methods lack uncertainty and holistic brain context.

Method: Introduce a channel for voxel-wise uncertainty into nnUNet; train on BraTS2023; one-pass uncertainty prediction with no extra networks; a unified model trained on both tumor and healthy brain datasets to produce whole-brain segmentation with high DSC for both structures.

Result: Uncertainty correlates 0.750 with ground-truth and RMSD 0.047 without sacrificing tumor accuracy; whole-brain model achieves DSC 0.81 for key brain structures and 0.86 for tumor; robust region performance; outputs include overlaid uncertainty maps.

Conclusion: Combining uncertainty estimation with unified anatomical context yields a clinically useful model that supports surgical planning and decision-making, by visualizing where predictions are uncertain and by providing entire-brain segmentation.

Abstract: Accurate segmentation of brain tumors is vital for diagnosis, surgical planning, and treatment monitoring. Deep learning has advanced on benchmarks, but two issues limit clinical use: no uncertainty estimates for errors and no segmentation of healthy brain structures around tumors for surgery. Current methods fail to unify tumor localization with anatomical context and lack confidence scores. This study presents an uncertainty-aware framework augmenting nnUNet with a channel for voxel-wise uncertainty. Trained on BraTS2023, it yields a correlation of 0.750 and RMSD of 0.047 for uncertainty without hurting tumor accuracy. It predicts uncertainty in one pass, with no extra networks or inferences, aiding clinical decisions. For whole-brain context, a unified model combines normal and cancer datasets, achieving a DSC of 0.81 for brain structures and 0.86 for tumor, with robust key-region performance. Combining both innovations gives the first model outputting tumor in natural surroundings plus an overlaid uncertainty map. Visual checks of outputs show uncertainty offers key insights to evaluate predictions and fix errors, helping informed surgical decisions from AI.

</details>


### [167] [MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection](https://arxiv.org/abs/2511.12810)
*Leena Alghamdi,Muhammad Usman,Hafeez Anwar,Abdul Bais,Saeed Anwar*

Main category: cs.CV

TL;DR: Introduces MSRNet, a multi-scale, recursive network for camouflaged object detection using a Pyramid Vision Transformer backbone, attention-based scale integration, multi-granularity fusion, and recursive-feedback decoding; achieves state-of-the-art on two benchmarks and second on two, with code release.


<details>
  <summary>Details</summary>
Motivation: Addresses difficulty of detecting camouflaged objects with high visual similarity to background, especially in challenging conditions (low light, occlusion, small/multiple objects) where existing methods underperform.

Method: Proposes Pyramid Vision Transformer backbone to extract multi-scale features; Attention-Based Scale Integration Units to fuse scales; Multi-Granularity Fusion Units in decoder; recursive-feedback decoding to refine global context; joint multi-scale and recursive feature optimization.

Result: State-of-the-art on two camouflaged object detection datasets and second place on two others; demonstrates improved ability to detect small and multiple camouflaged objects; code and weights released at GitHub.

Conclusion: The combination of multi-scale learning and recursive feature refinement yields robust camouflaged object detection, particularly for challenging cases; the approach advances the state of the field and provides reproducible results.

Abstract: Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at \href{https://github.com/linaagh98/MSRNet}{https://github.com/linaagh98/MSRNet}.

</details>


### [168] [SAGA: Source Attribution of Generative AI Videos](https://arxiv.org/abs/2511.12834)
*Rohit Kundu,Vishal Mohanty,Hao Xiong,Shan Jia,Athula Balachandran,Amit K. Roy-Chowdhury*

Main category: cs.CV

TL;DR: SAGA is a scalable framework for attributing AI-generated videos to their exact generative source, offering multi-level attribution (authenticity, task, version, team, and generator) using a video transformer and data-efficient pretraining, plus interpretable Temporal Attention Signatures (T-Sigs).


<details>
  <summary>Details</summary>
Motivation: The rise of hyper-realistic AI videos creates misuse risks and outpaces binary real/fake detectors, necessitating a scalable, interpretable source attribution system that can identify the producing model and lineage.

Method: Proposes a novel video transformer architecture that leverages features from a robust vision foundation model to capture spatio-temporal artifacts. Introduces a data-efficient pretrain-and-attribute strategy using only 0.5% of source-labeled data per class to achieve fully supervised-level attribution. Defines five-level multi-granular attribution (authenticity, generation task such as text-to-video or image-to-video, model version, development team, and exact generator). Introduces Temporal Attention Signatures (T-Sigs) to visualize learned temporal differences and explain why generators are distinguishable.

Result: Through extensive experiments on public datasets, including cross-domain settings, SAGA achieves state-of-the-art attribution performance and demonstrates robust generalization. The framework provides interpretable forensic insights and is positioned as a benchmark for synthetic video provenance with potential forensic and regulatory applications.

Conclusion: SAGA establishes a new benchmark for large-scale synthetic video provenance, delivering rich, interpretable attribution across multiple levels and enabling practical forensic and regulatory use with data-efficient learning and explainability.

Abstract: The proliferation of generative AI has led to hyper-realistic synthetic videos, escalating misuse risks and outstripping binary real/fake detectors. We introduce SAGA (Source Attribution of Generative AI videos), the first comprehensive framework to address the urgent need for AI-generated video source attribution at a large scale. Unlike traditional detection, SAGA identifies the specific generative model used. It uniquely provides multi-granular attribution across five levels: authenticity, generation task (e.g., T2V/I2V), model version, development team, and the precise generator, offering far richer forensic insights. Our novel video transformer architecture, leveraging features from a robust vision foundation model, effectively captures spatio-temporal artifacts. Critically, we introduce a data-efficient pretrain-and-attribute strategy, enabling SAGA to achieve state-of-the-art attribution using only 0.5\% of source-labeled data per class, matching fully supervised performance. Furthermore, we propose Temporal Attention Signatures (T-Sigs), a novel interpretability method that visualizes learned temporal differences, offering the first explanation for why different video generators are distinguishable. Extensive experiments on public datasets, including cross-domain scenarios, demonstrate that SAGA sets a new benchmark for synthetic video provenance, providing crucial, interpretable insights for forensic and regulatory applications.

</details>


### [169] [Video Finetuning Improves Reasoning Between Frames](https://arxiv.org/abs/2511.12868)
*Ruiqi Yang,Tian Yun,Zihan Wang,Ellie Pavlick*

Main category: cs.CV

TL;DR: Visual CoT (vCoT) introduces explicit transitional event descriptions between video frames to bridge temporal gaps in multimodal LLMs. It helps image-only LVLMs excel at long-form video QA, while video-finetuned models gain little from it, suggesting they already capture frame transitions; video models also transfer temporal reasoning to static relational tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods for extending image-based LVLMs to video often simply concatenate frame tokens, ignoring explicit temporal reasoning. There is a need to understand how video finetuning affects capabilities and whether explicit transitional reasoning offers benefits.

Method: Introduce Visual Chain-of-Thought (vCoT) which generates transitional event descriptions between consecutive frames. Systematically compare image-only LVLMs with video-finetuned counterparts, both with and without vCoT cues, on long-form video QA and relational reasoning tasks.

Result: vCoT substantially improves image-only models on long-form video question answering. Gains for video-finetuned models are marginal, implying these models already encode frame-to-frame transitions. Additionally, video models transfer their temporal reasoning abilities to static relational reasoning tasks, outperforming image-model baselines.

Conclusion: Video finetuning appears to capture frame transitions implicitly. vCoT is especially beneficial for image-only models, while temporal reasoning abilities from video data generalize to static tasks in video-finetuned models.

Abstract: Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.

</details>


### [170] [View-aware Cross-modal Distillation for Multi-view Action Recognition](https://arxiv.org/abs/2511.12870)
*Trung Thanh Nguyen,Yasutomo Kawanishi,Vijay John,Takahiro Komamizu,Ichiro Ide*

Main category: cs.CV

TL;DR: ViCoKD enables knowledge distillation from a fully supervised multi-modal teacher to a modality- and annotation-limited student for partially overlapping multi-view action recognition, using a cross-modal adapter with attention and a view-aware consistency module; it achieves state-of-the-art gains on real-world data and can even surpass the teacher under constrained conditions.


<details>
  <summary>Details</summary>
Motivation: In real-world multi-view action recognition, sensors often have partial overlap and limited modalities; dense frame-level labels are rare. There is a need to transfer rich multi-modal knowledge from a fully supervised teacher to a student that operates with missing modalities and weaker annotations.

Method: ViCoKD distills knowledge from a fully supervised multi-modal teacher to a modality- and annotation-limited student. It uses a cross-modal adapter with cross-modal attention to exploit multi-modal correlations despite incomplete modalities, and a View-aware Consistency module to address view misalignment. The latter enforces prediction alignment when the action is co-visible across views, guided by human-detection masks and confidence-weighted Jensen-Shannon divergence between predicted class distributions.

Result: On the real-world MultiSensor-Home dataset, ViCoKD consistently outperforms competitive distillation baselines across multiple backbones and environments, delivering significant gains and sometimes surpassing the teacher under limited conditions.

Conclusion: ViCoKD effectively enables cross-modal knowledge transfer for partially overlapping multi-view action recognition with modality and annotation constraints, leveraging cross-modal adaptation and view-consistency to improve robustness and performance; it shows strong generalization on real-world data and potential applicability to other multi-modal, multi-view tasks.

Abstract: The widespread use of multi-sensor systems has increased research in multi-view action recognition. While existing approaches in multi-view setups with fully overlapping sensors benefit from consistent view coverage, partially overlapping settings where actions are visible in only a subset of views remain underexplored. This challenge becomes more severe in real-world scenarios, as many systems provide only limited input modalities and rely on sequence-level annotations instead of dense frame-level labels. In this study, we propose View-aware Cross-modal Knowledge Distillation (ViCoKD), a framework that distills knowledge from a fully supervised multi-modal teacher to a modality- and annotation-limited student. ViCoKD employs a cross-modal adapter with cross-modal attention, allowing the student to exploit multi-modal correlations while operating with incomplete modalities. Moreover, we propose a View-aware Consistency module to address view misalignment, where the same action may appear differently or only partially across viewpoints. It enforces prediction alignment when the action is co-visible across views, guided by human-detection masks and confidence-weighted Jensen-Shannon divergence between their predicted class distributions. Experiments on the real-world MultiSensor-Home dataset show that ViCoKD consistently outperforms competitive distillation methods across multiple backbones and environments, delivering significant gains and surpassing the teacher model under limited conditions.

</details>


### [171] [Simple Lines, Big Ideas: Towards Interpretable Assessment of Human Creativity from Drawings](https://arxiv.org/abs/2511.12880)
*Zihao Lin,Zhenshan Shi,Sasa Zhao,Hanwei Zhu,Lingyu Zhu,Baoliang Chen,Lei Mo*

Main category: cs.CV

TL;DR: A data-driven framework for automatic, interpretable creativity assessment from drawings using content and style cues; uses multi-modal, multi-task learning with a conditional feature extractor; achieves state-of-the-art results and offers interpretable visualizations; code to be released.


<details>
  <summary>Details</summary>
Motivation: Current creativity assessment relies on expert-based subjective scoring which is labor-intensive and subjective. There is a need for automatic, interpretable assessment that accounts for both what is drawn (content) and how it is drawn (style).

Method: Augment existing creativity-labeled drawing dataset with content-type annotations. Propose a multi-modal, multi-task learning framework that (i) predicts creativity scores, (ii) categorizes content types, and (iii) extracts stylistic features. Introduce a conditional learning mechanism that dynamically tunes visual feature extraction based on creativity-relevant signals conditioned on the drawing's style and semantic cues.

Result: The model achieves state-of-the-art performance compared to existing regression-based approaches and provides interpretable visualizations that align with human judgments.

Conclusion: A data-driven, interpretable framework that integrates content and style to assess creativity in drawings; demonstrates effectiveness and provides resources (code/annotations) for reproducibility.

Abstract: Assessing human creativity through visual outputs, such as drawings, plays a critical role in fields including psychology, education, and cognitive science. However, current assessment practices still rely heavily on expert-based subjective scoring, which is both labor-intensive and inherently subjective. In this paper, we propose a data-driven framework for automatic and interpretable creativity assessment from drawings. Motivated by the cognitive understanding that creativity can emerge from both what is drawn (content) and how it is drawn (style), we reinterpret the creativity score as a function of these two complementary dimensions.Specifically, we first augment an existing creativity labeled dataset with additional annotations targeting content categories. Based on the enriched dataset, we further propose a multi-modal, multi-task learning framework that simultaneously predicts creativity scores, categorizes content types, and extracts stylistic features. In particular, we introduce a conditional learning mechanism that enables the model to adapt its visual feature extraction by dynamically tuning it to creativity-relevant signals conditioned on the drawing's stylistic and semantic cues.Experimental results demonstrate that our model achieves state-of-the-art performance compared to existing regression-based approaches and offers interpretable visualizations that align well with human judgments. The code and annotations will be made publicly available at https://github.com/WonderOfU9/CSCA_PRCV_2025

</details>


### [172] [ActVAR: Activating Mixtures of Weights and Tokens for Efficient Visual Autoregressive Generation](https://arxiv.org/abs/2511.12893)
*Kaixin Zhang,Ruiqing Yang,Yuan Zhang,Shan You,Tao Huang*

Main category: cs.CV

TL;DR: ActVAR introduces a dynamic activation framework for Visual Autoregressive (VAR) models to achieve dual sparsity in weights and token sequences. By using a learnable router to select token-specific FFN experts and a gated token selector to focus computation on high-update-potential tokens (while reconstructing others to maintain context), it attains up to 21.2% FLOPs reduction on ImageNet-256 with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Static pruning can permanently remove weights or tokens, harming pretrained dependencies and overall performance. VARs suffer escalating computational costs with longer sequences. A dynamic, content-aware sparsity approach is needed to reduce computation without sacrificing capacity or pretrained knowledge.

Method: FFNs are decomposed into lightweight expert sub-networks. A learnable router dynamically selects token-specific expert subsets. A gated token selector identifies high-update-potential tokens for computation and reconstructs unselected tokens to preserve global context and sequence alignment. Training uses a two-stage knowledge distillation where the original VAR supervises the routing and gating policies to align with pretrained knowledge.

Result: On the ImageNet 256×256 benchmark, ActVAR achieves up to 21.2% FLOPs reduction with minimal performance degradation.

Conclusion: ActVAR demonstrates that dynamic, content-driven dual sparsity—across weights and tokens—can significantly improve efficiency for Visual Autoregressive models while preserving capacity and alignment with pretrained models.

Abstract: Visual Autoregressive (VAR) models enable efficient image generation via next-scale prediction but face escalating computational costs as sequence length grows. Existing static pruning methods degrade performance by permanently removing weights or tokens, disrupting pretrained dependencies. To address this, we propose ActVAR, a dynamic activation framework that introduces dual sparsity across model weights and token sequences to enhance efficiency without sacrificing capacity. ActVAR decomposes feedforward networks (FFNs) into lightweight expert sub-networks and employs a learnable router to dynamically select token-specific expert subsets based on content. Simultaneously, a gated token selector identifies high-update-potential tokens for computation while reconstructing unselected tokens to preserve global context and sequence alignment. Training employs a two-stage knowledge distillation strategy, where the original VAR model supervises the learning of routing and gating policies to align with pretrained knowledge. Experiments on the ImageNet $256\times 256$ benchmark demonstrate that ActVAR achieves up to $21.2\%$ FLOPs reduction with minimal performance degradation.

</details>


### [173] [Reconstructing 3D Scenes in Native High Dynamic Range](https://arxiv.org/abs/2511.12895)
*Kaixuan Zhang,Minxian Li,Mingwu Ren,Jiankang Deng,Xiatian Zhu*

Main category: cs.CV

TL;DR: Introduces NH-3DGS, a method for native HDR 3D reconstruction that directly optimizes HDR observations with a luminance-chromaticity color decomposition, outperforming LDR-based approaches.


<details>
  <summary>Details</summary>
Motivation: 3D scene reconstruction has largely used LDR data and multi-exposure/fusion techniques, which are limited for professional HDR workflows. Direct native HDR modeling aims to preserve full dynamic range while simplifying capture.

Method: Native High dynamic range 3D Gaussian Splatting (NH-3DGS) with a luminance-chromaticity decomposition of the color representation to enable direct optimization from native HDR camera data.

Result: Demonstrates superior reconstruction quality and dynamic range preservation on synthetic and real multi-view HDR datasets compared to existing methods.

Conclusion: Enables professional-grade 3D reconstruction directly from native HDR captures; code and datasets will be released.

Abstract: High Dynamic Range (HDR) imaging is essential for professional digital media creation, e.g., filmmaking, virtual production, and photorealistic rendering. However, 3D scene reconstruction has primarily focused on Low Dynamic Range (LDR) data, limiting its applicability to professional workflows. Existing approaches that reconstruct HDR scenes from LDR observations rely on multi-exposure fusion or inverse tone-mapping, which increase capture complexity and depend on synthetic supervision. With the recent emergence of cameras that directly capture native HDR data in a single exposure, we present the first method for 3D scene reconstruction that directly models native HDR observations. We propose {\bf Native High dynamic range 3D Gaussian Splatting (NH-3DGS)}, which preserves the full dynamic range throughout the reconstruction pipeline. Our key technical contribution is a novel luminance-chromaticity decomposition of the color representation that enables direct optimization from native HDR camera data. We demonstrate on both synthetic and real multi-view HDR datasets that NH-3DGS significantly outperforms existing methods in reconstruction quality and dynamic range preservation, enabling professional-grade 3D reconstruction directly from native HDR captures. Code and datasets will be made available.

</details>


### [174] [FDP: A Frequency-Decomposition Preprocessing Pipeline for Unsupervised Anomaly Detection in Brain MRI](https://arxiv.org/abs/2511.12899)
*Hao Li,Zhenfeng Zhuang,Jingyu Lin,Yu Liu,Yifei Chen,Qiong Peng,Lequan Yu,Liansheng Wang*

Main category: cs.CV

TL;DR: Introducing FDP, a frequency-domain preprocessing for unsupervised anomaly detection in brain MRI that suppresses pathology while preserving anatomy; shows consistent gains across architectures (e.g., 17.63% Dice with LDM).


<details>
  <summary>Details</summary>
Motivation: Brain MRI anomaly detection is hindered by anatomical variability and scarce annotated data. Current UAD methods rely on perturbations to simulate anomalies, but these do not capture true biophysical and morphological lesion characteristics. A frequency-domain approach could differentiate pathological signatures from normal anatomy.

Method: Frequency-Decomposition Preprocessing (FDP) framework decomposes MRIs into frequency components to suppress pathology while preserving anatomical structures. It can be integrated into existing anomaly-simulation training pipelines and residual-based detectors. The approach leverages two observed properties: anomalies have distinct frequency patterns, and low-frequency content is stable across healthy scans. FDP improves anomaly detection when plugged into various architectures, acting as a plug-and-play enhancement to existing UAD methods.

Result: FDP consistently improves anomaly detection across diverse architectures; notably, it yields a 17.63% increase in Dice score with Latent Diffusion Models (LDM) and maintains robust improvements across multiple baselines. The authors provide code for replication.

Conclusion: FDP is the first UAD method to use frequency-domain reconstruction to simultaneously suppress pathology and preserve normal anatomy in brain MRI. It is compatible with existing anomaly-simulation methods, improves detection performance across baselines, and preserves diagnostic fidelity. Code is available at the authors' repository.

Abstract: Due to the diversity of brain anatomy and the scarcity of annotated data, supervised anomaly detection for brain MRI remains challenging, driving the development of unsupervised anomaly detection (UAD) approaches. Current UAD methods typically utilize artificially generated noise perturbations on healthy MRIs to train generative models for normal anatomy reconstruction, enabling anomaly detection via residual mapping. However, such simulated anomalies lack the biophysical fidelity and morphological complexity characteristic of true clinical lesions. To advance UAD in brain MRI, we conduct the first systematic frequency-domain analysis of pathological signatures, revealing two key properties: (1) anomalies exhibit unique frequency patterns distinguishable from normal anatomy, and (2) low-frequency signals maintain consistent representations across healthy scans. These insights motivate our Frequency-Decomposition Preprocessing (FDP) framework, the first UAD method to leverage frequency-domain reconstruction for simultaneous pathology suppression and anatomical preservation. FDP can integrate seamlessly with existing anomaly simulation techniques, consistently enhancing detection performance across diverse architectures while maintaining diagnostic fidelity. Experimental results demonstrate that FDP consistently improves anomaly detection performance when integrated with existing methods. Notably, FDP achieves a 17.63% increase in DICE score with LDM while maintaining robust improvements across multiple baselines. The code is available at https://github.com/ls1rius/MRI_FDP.

</details>


### [175] [DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning](https://arxiv.org/abs/2511.12908)
*Junbo Zou,Haotian Xia,Zhen Ye,Shengjie Zhang,Christopher Lai,Vicente Ordonez,Weining Shen,Hanjie Chen*

Main category: cs.CV

TL;DR: DeepSport is the first end-to-end trained multimodal LLM for multi-task, multi-sport video understanding, enabling iterative reasoning with a frame-extraction tool; it uses a data distillation pipeline to create 78k Chain-of-Thought trajectories, followed by supervised fine-tuning and RL with a gated tool-use reward, achieving state-of-the-art on a 6.7k-question benchmark.


<details>
  <summary>Details</summary>
Motivation: Current sports-video AI research is largely single-sport, task-specific, or training-free, lacking robust, learned reasoning over long temporal contexts; there is a need for a unified, end-to-end framework capable of multi-task, multi-sport reasoning.

Method: An end-to-end trained Multimodal LLM (MLLM) that actively reasons over video using a specialized frame-extraction tool. A data distillation pipeline synthesizes 78k CoT trajectories from 10 data sources into a unified training resource. Training proceeds in two stages: Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward to optimize reasoning.

Result: On a 6.7k-question testing benchmark, DeepSport achieves state-of-the-art performance and significantly outperforms baselines from both proprietary and open-source models.

Conclusion: DeepSport demonstrates a foundational approach to domain-specific video reasoning across diverse sports, enabling multi-task, multi-sport understanding through iterative, tool-assisted reasoning.

Abstract: Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to ``think with videos'' by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the model's reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.

</details>


### [176] [CASL: Curvature-Augmented Self-supervised Learning for 3D Anomaly Detection](https://arxiv.org/abs/2511.12909)
*Yaohua Zha,Xue Yuerong,Chunlin Fan,Yuansong Wang,Tao Dai,Ke Chen,Shu-Tao Xia*

Main category: cs.CV

TL;DR: A curvature-augmented self-supervised learning framework (CASL) for 3D anomaly detection that uses point-wise curvature as the anomaly cue, built on a reconstruction paradigm with a U-Net backbone and multi-scale curvature prompts; it achieves strong anomaly detection without task-specific mechanisms and generalizes to standard 3D understanding tasks.


<details>
  <summary>Details</summary>
Motivation: There is a generalizability gap: anti anomaly detection methods are highly task-specific, while self-supervised 3D models often underperform when fine-tuned for anomaly detection. A general 3D representation that exploits intrinsic geometry (curvature) could enable robust anomaly detection without task-specific designs.

Method: CASL uses a reconstruction-based self-supervised framework built on a U-Net. It introduces multi-scale curvature prompts to guide the decoder in predicting the spatial coordinates of each point. Anomaly scoring leverages point curvature, and no dedicated anomaly-detection components are used; a straightforward anomaly-classification fine-tuning is applied. The approach promotes generalizable representations.

Result: Curvature alone as an anomaly score outperforms several classical self-supervised and dedicated anomaly-detection models. Under unified fine-tuning, CASL achieves leading anomaly detection performance. Additionally, the learned representations transfer well to standard 3D tasks such as point-cloud classification.

Conclusion: Curvature is a critical cue for 3D anomaly detection. A generalizable framework (CASL) can achieve strong anomaly detection without task-specific modules, with representations that generalize to other 3D understanding tasks.

Abstract: Deep learning-based 3D anomaly detection methods have demonstrated significant potential in industrial manufacturing. However, many approaches are specifically designed for anomaly detection tasks, which limits their generalizability to other 3D understanding tasks. In contrast, self-supervised point cloud models aim for general-purpose representation learning, yet our investigation reveals that these classical models are suboptimal at anomaly detection under the unified fine-tuning paradigm. This motivates us to develop a more generalizable 3D model that can effectively detect anomalies without relying on task-specific designs. Interestingly, we find that using only the curvature of each point as its anomaly score already outperforms several classical self-supervised and dedicated anomaly detection models, highlighting the critical role of curvature in 3D anomaly detection. In this paper, we propose a Curvature-Augmented Self-supervised Learning (CASL) framework based on a reconstruction paradigm. Built upon the classical U-Net architecture, our approach introduces multi-scale curvature prompts to guide the decoder in predicting the spatial coordinates of each point. Without relying on any dedicated anomaly detection mechanisms, it achieves leading detection performance through straightforward anomaly classification fine-tuning. Moreover, the learned representations generalize well to standard 3D understanding tasks such as point cloud classification. The code is available at https://github.com/zyh16143998882/CASL.

</details>


### [177] [Explore How to Inject Beneficial Noise in MLLMs](https://arxiv.org/abs/2511.12917)
*Ruishu Zhu,Sida Huang,Ziheng Jiao,Hongyuan Zhang*

Main category: cs.CV

TL;DR: A parameter-efficient fine-tuning method MuNG injects task-adaptive noise into frozen multimodal LLMs (QwenVL, LLaVA) to improve cross-modal alignment and performance, surpassing full fine-tuning with only ~1-2% extra parameters.


<details>
  <summary>Details</summary>
Motivation: To address cross-modal heterogeneity and the inefficiency of full- or heavy fine-tuning in multimodal LLMs by achieving strong performance with minimal parameter updates.

Method: Frame MLLM reasoning as variational inference and introduce Multimodal Noise Generator (MuNG) that analyzes image-text pairs to generate beneficial, task-adaptive noise; inject this noise into frozen MLLMs to suppress irrelevant semantic components and enhance cross-modal representation alignment.

Result: Experiments on QwenVL and LLaVA show MuNG outperforms full-parameter fine-tuning and other methods, with only about 1-2% extra parameters required; code is provided in supplementary materials.

Conclusion: MuNG demonstrates effective, highly parameter-efficient fine-tuning for MLLMs by leveraging dynamic, noise-based perturbations to improve cross-modal alignment and downstream task performance.

Abstract: Multimodal Large Language Models (MLLMs) have played an increasingly important role in multimodal intelligence. However, the existing fine-tuning methods often ignore cross-modal heterogeneity, limiting their full potential. In this work, we propose a novel fine-tuning strategy by injecting beneficial random noise, which outperforms previous methods and even surpasses full fine-tuning, with minimal additional parameters. The proposed Multimodal Noise Generator (MuNG) enables efficient modality fine-tuning by injecting customized noise into the frozen MLLMs. Specifically, we reformulate the reasoning process of MLLMs from a variational inference perspective, upon which we design a multimodal noise generator that dynamically analyzes cross-modal relationships in image-text pairs to generate task-adaptive beneficial noise. Injecting this type of noise into the MLLMs effectively suppresses irrelevant semantic components, leading to significantly improved cross-modal representation alignment and enhanced performance on downstream tasks. Experiments on two mainstream MLLMs, QwenVL and LLaVA, demonstrate that our method surpasses full-parameter fine-tuning and other existing fine-tuning approaches, while requiring adjustments to only about $1\sim2\%$ additional parameters. The relevant code is uploaded in the supplementary.

</details>


### [178] [CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation](https://arxiv.org/abs/2511.12919)
*Dexin Zuo,Ang Li,Wei Wang,Wenxian Yu,Danping Zou*

Main category: cs.CV

TL;DR: CoordAR introduces an autoregressive, probabilistic token-based method for one-reference 6D pose estimation of unseen objects. It uses coordinate-map tokenization, modality-decoupled encoding, and an autoregressive transformer decoder to predict discrete 3D-3D correspondences, achieving strong robustness to symmetry/occlusion and outperforming prior real-valued regression approaches.


<details>
  <summary>Details</summary>
Motivation: Reduce dependency on full 3D models for 6D pose estimation by enabling one-reference-based estimation. Real-valued regression methods suffer from limited global consistency due to local convolution and lack uncertainty modeling, making them fragile under symmetry and occlusion.

Method: Formulate 3D-3D correspondences as a map of discrete tokens learned in an autoregressive, probabilistic manner. Introduce (1) coordinate map tokenization for probabilistic prediction over discretized 3D space; (2) modality-decoupled encoding that separately processes RGB appearance and coordinate cues; (3) an autoregressive transformer decoder conditioned on position-aligned query features and the partially generated token sequence.

Result: CoordAR significantly outperforms existing methods on multiple benchmarks and shows strong robustness to symmetry, occlusion, and other real-world challenges in tests.

Conclusion: CoordAR provides a probabilistic, autoregressive approach to one-reference 6D pose estimation that improves global consistency and robustness without requiring full 3D models, enabling effective estimation for unseen objects.

Abstract: Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.

</details>


### [179] [Generative Photographic Control for Scene-Consistent Video Cinematic Editing](https://arxiv.org/abs/2511.12921)
*Huiqiang Sun,Liao Shen,Zhan Peng,Kun Wang,Size Wu,Yuhang Zang,Tianqi Liu,Zihao Huang,Xingyu Zeng,Zhiguo Cao,Wei Li,Chen Change Loy*

Main category: cs.CV

TL;DR: CineCtrl enables fine-grained control of photographic camera effects (e.g., depth of field, shutter) in video generation by decoupling camera motion from photographic inputs, using a decoupled cross-attention module and a data-generation pipeline to train on simulated and real-world data; results show high-fidelity, user-controllable cinematic edits.


<details>
  <summary>Details</summary>
Motivation: Generative video models predominantly control camera motion but lack the ability to manipulate photographic effects (DOF, exposure) that shape mood and aesthetics; this limits cinematic storytelling and realism.

Method: Introduce CineCtrl, a video cinematic editing framework with a decoupled cross-attention mechanism to disentangle camera motion from photographic inputs, enabling fine-grained, independent control of camera parameters. Develop a data-generation strategy combining simulated photographic effects with a real-world collection pipeline to create a large-scale dataset for robust training.

Result: The model produces high-fidelity videos with precisely controlled, user-specified photographic camera effects, maintaining scene consistency while allowing independent adjustment of camera parameters.

Conclusion: CineCtrl advances cinematic editing in generative video by enabling fine-grained control over camera parameters and providing a scalable data pipeline, potentially broadening the scope of cinematic storytelling in AI-generated video.

Abstract: Cinematic storytelling is profoundly shaped by the artful manipulation of photographic elements such as depth of field and exposure. These effects are crucial in conveying mood and creating aesthetic appeal. However, controlling these effects in generative video models remains highly challenging, as most existing methods are restricted to camera motion control. In this paper, we propose CineCtrl, the first video cinematic editing framework that provides fine control over professional camera parameters (e.g., bokeh, shutter speed). We introduce a decoupled cross-attention mechanism to disentangle camera motion from photographic inputs, allowing fine-grained, independent control without compromising scene consistency. To overcome the shortage of training data, we develop a comprehensive data generation strategy that leverages simulated photographic effects with a dedicated real-world collection pipeline, enabling the construction of a large-scale dataset for robust model training. Extensive experiments demonstrate that our model generates high-fidelity videos with precisely controlled, user-specified photographic camera effects.

</details>


### [180] [Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes](https://arxiv.org/abs/2511.12932)
*Feng Lv,Haoxuan Feng,Zilu Zhang,Chunlong Xia,Yanfeng Li*

Main category: cs.CV

TL;DR: A unified text-driven framework for traffic-scene image generation and editing using a controllable mask mechanism, multi-view data, and a two-stage training with a mask-region-weighted loss to improve fidelity and alignment, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: There is a need for semantically rich, high-fidelity, and viewpoint-diverse traffic scene generation/editing. Current methods struggle with insufficient semantic richness, limited camera viewpoints, low visual fidelity, and misalignment between text and content.

Method: Proposes a unified text-driven framework that uses a controllable mask mechanism to bridge generation and editing; leverages multi-view data from vehicles and roadside sensors to diversify geometry; training occurs in two stages: conceptual learning on large-scale coarse text-image data, followed by fine-tuning on fine-grained descriptive data to improve text-image alignment and detail quality; introduces a mask-region-weighted loss to emphasize small yet critical regions during training.

Result: Extensive experiments show leading performance in text-based generation and editing for traffic scenes.

Conclusion: Mask-based integration, multi-view data, and two-stage fine-tuning improve fidelity, semantic richness, and alignment; the mask-region-weighted loss boosts small-region fidelity; multi-view data increases geometric diversity, enabling more realistic and controllable traffic scenes.

Abstract: With the rapid advancement of intelligent transportation systems, text-driven image generation and editing techniques have demonstrated significant potential in providing rich, controllable visual scene data for applications such as traffic monitoring and autonomous driving. However, several challenges remain, including insufficient semantic richness of generated traffic elements, limited camera viewpoints, low visual fidelity of synthesized images, and poor alignment between textual descriptions and generated content. To address these issues, we propose a unified text-driven framework for both image generation and editing, leveraging a controllable mask mechanism to seamlessly integrate the two tasks. Furthermore, we incorporate both vehicle-side and roadside multi-view data to enhance the geometric diversity of traffic scenes. Our training strategy follows a two-stage paradigm: first, we perform conceptual learning using large-scale coarse-grained text-image data; then, we fine-tune with fine-grained descriptive data to enhance text-image alignment and detail quality. Additionally, we introduce a mask-region-weighted loss that dynamically emphasizes small yet critical regions during training, thereby substantially enhancing the generation fidelity of small-scale traffic elements. Extensive experiments demonstrate that our method achieves leading performance in text-based image generation and editing within traffic scenes.

</details>


### [181] [PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos](https://arxiv.org/abs/2511.12935)
*Dianbing Xi,Guoyuan An,Jingsen Zhu,Zhijian Liu,Yuan Liu,Ruiyuan Zhang,Jiayuan Lu,Rui Wang,Yuchi Huo*

Main category: cs.CV

TL;DR: Two-stage PFAvatar reconstructs high-fidelity NeRF avatars from unconstrained OOTD photos. Stage 1 fine-tunes a pose-aware diffusion model from few-shot examples using ControlNet for pose and a Condition Prior Preservation Loss to avoid language drift; Stage 2 distills to a NeRF avatar in canonical SMPL-X space with Multi-Resolution 3D-SDS. The approach yields 5-minute personalization with 48x speed-up, superior detail and occlusion robustness over SOTA, and enables downstream applications like virtual try-on, animation, and reenactment.


<details>
  <summary>Details</summary>
Motivation: Reconstructing high-quality 3D avatars from real-world Outfit of the Day images is challenging due to diverse poses, occlusions, and complex backgrounds. Existing asset-segmentation-based methods can be inconsistent and slow. A fast, end-to-end approach that preserves fine details and handles occlusions would enable practical avatar creation and downstream tasks.

Method: A two-stage method. Stage 1: fine-tune a pose-aware diffusion model from few-shot OOTD examples, avoiding image decomposition into garments; integrate a pre-trained ControlNet for pose estimation and employ a novel Condition Prior Preservation Loss to prevent language drift during few-shot training, enabling end-to-end learning of fine details. Personalization completes in ~5 minutes, with a 48× speed-up versus prior work. Stage 2: distill to a NeRF-based avatar represented in canonical SMPL-X space sampling and Multi-Resolution 3D-SDS; a continuous radiance field preserves high-frequency textures (e.g., hair) and handles occlusions via transmittance, outperforming mesh-based representations.

Result: Quantitative and qualitative results show PFAvatar achieves higher reconstruction fidelity and detail preservation and is more robust to occlusions and truncations than state-of-the-art methods. The method delivers fast personalization (5 minutes) and a substantial speed-up (48×) over prior approaches, with versatility for downstream tasks such as virtual try-on, animation, and human video reenactment.

Conclusion: PFAvatar advances practical 3D avatar generation from real-world OOTD imagery by combining a pose-aware diffusion fine-tuning stage with NeRF-based avatar distillation in SMPL-X space. It enables high-fidelity, occlusion-robust avatars in a fast, end-to-end pipeline and supports downstream applications like virtual try-on, animation, and reenactment.

Abstract: We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from ``Outfit of the Day'' (OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48$\times$ speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions/truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.

</details>


### [182] [ProtoAnomalyNCD: Prototype Learning for Multi-class Novel Anomaly Discovery in Industrial Scenarios](https://arxiv.org/abs/2511.12938)
*Botong Zhao,Qijun Shi,Shujing Lyu,Yue Lu*

Main category: cs.CV

TL;DR: A prototype-learning framework (ProtoAnomalyNCD) for discovering and classifying unseen multi-type industrial anomalies, integrating Grounded SAM-based localization, an Anomaly-Map-Guided Attention with Region Guidance Factor, and a unified prototype-learning scheme to cluster unseen anomaly classes and enable multi-type classification, extensible to unseen outliers; achieves state-of-the-art on MVTec AD, MTD, Real-IAD.


<details>
  <summary>Details</summary>
Motivation: Industrial anomaly detection typically flags only presence of anomalies; real-world needs to discover multiple anomaly types. Anomalies are subtle and background clutter hinders clustering; current clustering approaches underperform due to lack of robust priors and fine-grained features. Propose a prior-guided prototype learning approach to detect and categorize unseen anomalies.

Method: Localize product regions with Grounded SAM using text prompts as priors; introduce Anomaly-Map-Guided Attention block with a Region Guidance Factor to separate background, object, and anomalous regions, boosting anomalous features and reducing background; use priors for contrastive learning; operate in a unified prototype-learning framework to discover and cluster unseen anomaly classes and perform multi-type classification; extend to unseen outliers.

Result: Outperforms state-of-the-art on MVTec AD, MTD, and Real-IAD datasets.

Conclusion: ProtoAnomalyNCD provides a unified, prior-guided, prototype-learning framework enabling discovery, clustering, and classification of unseen anomaly types in industrial settings, with strong performance gains and extensibility to outlier detection.

Abstract: Existing industrial anomaly detection methods mainly determine whether an anomaly is present. However, real-world applications also require discovering and classifying multiple anomaly types. Since industrial anomalies are semantically subtle and current methods do not sufficiently exploit image priors, direct clustering approaches often perform poorly. To address these challenges, we propose ProtoAnomalyNCD, a prototype-learning-based framework for discovering unseen anomaly classes of multiple types that can be integrated with various anomaly detection methods. First, to suppress background clutter, we leverage Grounded SAM with text prompts to localize object regions as priors for the anomaly classification network. Next, because anomalies usually appear as subtle and fine-grained patterns on the product, we introduce an Anomaly-Map-Guided Attention block. Within this block, we design a Region Guidance Factor that helps the attention module distinguish among background, object regions, and anomalous regions. By using both localized product regions and anomaly maps as priors, the module enhances anomalous features while suppressing background noise and preserving normal features for contrastive learning. Finally, under a unified prototype-learning framework, ProtoAnomalyNCD discovers and clusters unseen anomaly classes while simultaneously enabling multi-type anomaly classification. We further extend our method to detect unseen outliers, achieving task-level unification. Our method outperforms state-of-the-art approaches on the MVTec AD, MTD, and Real-IAD datasets.

</details>


### [183] [Semi-Supervised High Dynamic Range Image Reconstructing via Bi-Level Uncertain Area Masking](https://arxiv.org/abs/2511.12939)
*Wei Jiang,Jiahao Cui,Yizheng Wu,Zhan Peng,Zhiyu Pan,Zhiguo Cao*

Main category: cs.CV

TL;DR: Semi-supervised HDR reconstruction from LDR bursts using a teacher-student framework with uncertainty-based masking to filter pseudo HDR GTs; achieves competitive performance with only 6.7% HDR GTs.


<details>
  <summary>Details</summary>
Motivation: HDR reconstruction requires HDR ground truths which are hard to obtain; annotation-efficient learning is desired. A key challenge is confirmation bias from pseudo labels contaminating training.

Method: A teacher model generates pseudo HDR GTs for unlabeled LDR bursts; a student model learns from pseudo GTs. Uncertainty-based masking at pixel and patch levels removes unreliable regions to reduce bias, allowing robust learning from pseudo labels.

Result: Outperforms prior annotation-efficient methods; achieves performance comparable to fully-supervised methods using only 6.7% HDR GTs.

Conclusion: Uncertainty-based masking effectively mitigates confirmation bias in semi-supervised HDR reconstruction, enabling high-quality HDR results from limited ground truths.

Abstract: Reconstructing high dynamic range (HDR) images from low dynamic range (LDR) bursts plays an essential role in the computational photography. Impressive progress has been achieved by learning-based algorithms which require LDR-HDR image pairs. However, these pairs are hard to obtain, which motivates researchers to delve into the problem of annotation-efficient HDR image reconstructing: how to achieve comparable performance with limited HDR ground truths (GTs). This work attempts to address this problem from the view of semi-supervised learning where a teacher model generates pseudo HDR GTs for the LDR samples without GTs and a student model learns from pseudo GTs. Nevertheless, the confirmation bias, i.e., the student may learn from the artifacts in pseudo HDR GTs, presents an impediment. To remove this impediment, an uncertainty-based masking process is proposed to discard unreliable parts of pseudo GTs at both pixel and patch levels, then the trusted areas can be learned from by the student. With this novel masking process, our semi-supervised HDR reconstructing method not only outperforms previous annotation-efficient algorithms, but also achieves comparable performance with up-to-date fully-supervised methods by using only 6.7% HDR GTs.

</details>


### [184] [Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention](https://arxiv.org/abs/2511.12940)
*Taiye Chen,Zihan Ding,Anjian Li,Christina Zhang,Zeqi Xiao,Yisen Wang,Chi Jin*

Main category: cs.CV

TL;DR: RAD introduces a Recurrent Autoregressive Diffusion framework that uses LSTM-based memory within a diffusion transformer to enable long-horizon video generation with a fixed memory budget, aligning training and inference and outperforming prior diffusion-RNN approaches.


<details>
  <summary>Details</summary>
Motivation: Long-term video generation with diffusion models suffers from forgetting and spatiotemporal inconsistencies due to localized attention and limited memory. Efficient memory compression and retrieval within a fixed budget are needed for coherent long sequences.

Method: Incorporates an LSTM with attention into a diffusion transformer and proposes RAD, a frame-wise autoregressive memory update and retrieval mechanism that maintains consistent training and inference across windows, addressing training-inference gaps and window overlap issues.

Result: Experiments on Memory Maze and Minecraft show RAD achieves superior long-video generation performance; results indicate the LSTM-based sequence modeling is efficient and competitive with state-of-the-art diffusion-RNN blocks such as TTT and Mamba2.

Conclusion: RAD offers an effective memory-enabled diffusion framework for long video generation under a fixed memory budget, alleviating training/inference gaps and window overlap problems while leveraging LSTM efficiency.

Abstract: Recent advancements in video generation have demonstrated the potential of using video diffusion models as world models, with autoregressive generation of infinitely long videos through masked conditioning. However, such models, usually with local full attention, lack effective memory compression and retrieval for long-term generation beyond the window size, leading to issues of forgetting and spatiotemporal inconsistencies. To enhance the retention of historical information within a fixed memory budget, we introduce a recurrent neural network (RNN) into the diffusion transformer framework. Specifically, a diffusion model incorporating LSTM with attention achieves comparable performance to state-of-the-art RNN blocks, such as TTT and Mamba2. Moreover, existing diffusion-RNN approaches often suffer from performance degradation due to training-inference gap or the lack of overlap across windows. To address these limitations, we propose a novel Recurrent Autoregressive Diffusion (RAD) framework, which executes frame-wise autoregression for memory update and retrieval, consistently across training and inference time. Experiments on Memory Maze and Minecraft datasets demonstrate the superiority of RAD for long video generation, highlighting the efficiency of LSTM in sequence modeling.

</details>


### [185] [T2I-Based Physical-World Appearance Attack against Traffic Sign Recognition Systems in Autonomous Driving](https://arxiv.org/abs/2511.12956)
*Chen Ma,Ningfei Wang,Junhao Zheng,Qing Guo,Qian Wang,Qi Alfred Chen,Chao Shen*

Main category: cs.CV

TL;DR: DiffSign is a diffusion-model-based appearance attack against traffic sign recognition (TSR) that uses CLIP-guided loss and masked prompts to create robust, transferable, and stealthy physical-world signs, achieving an average 83.3% attack success rate.


<details>
  <summary>Details</summary>
Motivation: Existing TSR adversarial attacks either rely on pixel-level perturbations (lacking stealth and transferability) or diffusion-based methods (with limited effectiveness and poor generalization to unseen sign types). There is a need for a method that is physically robust, transferable across signs, practical, and stealthy.

Method: Introduce DiffSign, a text-to-image diffusion model–based appearance attack pipeline with CLIP-based loss to guide optimization and masked prompts to focus perturbations; add two novel style customization methods to control visual appearance and improve out-of-domain generalization and stealth; evaluate under varied real-world conditions (distance, angle, lighting, sign categories) and measure physical-world attack success and transferability.

Result: The framework achieves an average physical-world attack success rate of 83.3%, demonstrating strong transferability and robustness across varying real-world conditions and sign categories.

Conclusion: DiffSign advances adversarial appearance attacks on TSR by combining CLIP-driven optimization, masked prompts, and style customization to produce stealthy, transferable, and robust attacks, highlighting vulnerabilities in TSR systems and improving generalization to unseen sign types.

Abstract: Traffic Sign Recognition (TSR) systems play a critical role in Autonomous Driving (AD) systems, enabling real-time detection of road signs, such as STOP and speed limit signs. While these systems are increasingly integrated into commercial vehicles, recent research has exposed their vulnerability to physical-world adversarial appearance attacks. In such attacks, carefully crafted visual patterns are misinterpreted by TSR models as legitimate traffic signs, while remaining inconspicuous or benign to human observers. However, existing adversarial appearance attacks suffer from notable limitations. Pixel-level perturbation-based methods often lack stealthiness and tend to overfit to specific surrogate models, resulting in poor transferability to real-world TSR systems. On the other hand, text-to-image (T2I) diffusion model-based approaches demonstrate limited effectiveness and poor generalization to out-of-distribution sign types.
  In this paper, we present DiffSign, a novel T2I-based appearance attack framework designed to generate physically robust, highly effective, transferable, practical, and stealthy appearance attacks against TSR systems. To overcome the limitations of prior approaches, we propose a carefully designed attack pipeline that integrates CLIP-based loss and masked prompts to improve attack focus and controllability. We also propose two novel style customization methods to guide visual appearance and improve out-of-domain traffic sign attack generalization and attack stealthiness. We conduct extensive evaluations of DiffSign under varied real-world conditions, including different distances, angles, light conditions, and sign categories. Our method achieves an average physical-world attack success rate of 83.3%, leveraging DiffSign's high effectiveness in attack transferability.

</details>


### [186] [EndoSight AI: Deep Learning-Driven Real-Time Gastrointestinal Polyp Detection and Segmentation for Enhanced Endoscopic Diagnostics](https://arxiv.org/abs/2511.12962)
*Daniel Cavadia*

Main category: cs.CV

TL;DR: EndoSight AI enables precise real-time polyp detection and boundary segmentation during endoscopy, achieving high mAP and Dice on Hyper-Kvasir with real-time speed, and designed for clinical deployment with a thermal-aware training approach.


<details>
  <summary>Details</summary>
Motivation: GI polyp detection is crucial for early colorectal cancer prevention; real-time, accurate detection and segmentation in endoscopy can improve diagnostic accuracy and clinical decision-making.

Method: A deep learning architecture named EndoSight AI for polyp localization and boundary delineation; trained/evaluated on the Hyper-Kvasir dataset; real-time inference exceeding 35 frames per second on GPU; includes a novel thermal-aware training procedure to enhance robustness and efficiency; optimized for seamless integration into endoscopy workflows.

Result: mAP 88.3% for polyp detection; Dice coefficient up to 69% for segmentation; real-time inference speeds >35 FPS on GPU; demonstrated robustness with the thermal-aware approach.

Conclusion: An integrated AI solution designed for seamless deployment in endoscopy workflows, with potential to advance diagnostic accuracy and clinical decision-making in gastrointestinal healthcare.

Abstract: Precise and real-time detection of gastrointestinal polyps during endoscopic procedures is crucial for early diagnosis and prevention of colorectal cancer. This work presents EndoSight AI, a deep learning architecture developed and evaluated independently to enable accurate polyp localization and detailed boundary delineation. Leveraging the publicly available Hyper-Kvasir dataset, the system achieves a mean Average Precision (mAP) of 88.3% for polyp detection and a Dice coefficient of up to 69% for segmentation, alongside real-time inference speeds exceeding 35 frames per second on GPU hardware. The training incorporates clinically relevant performance metrics and a novel thermal-aware procedure to ensure model robustness and efficiency. This integrated AI solution is designed for seamless deployment in endoscopy workflows, promising to advance diagnostic accuracy and clinical decision-making in gastrointestinal healthcare.

</details>


### [187] [CalibrateMix: Guided-Mixup Calibration of Image Semi-Supervised Models](https://arxiv.org/abs/2511.12964)
*Mehrab Mustafy Rahman,Jayanth Mohan,Tiberiu Sosea,Cornelia Caragea*

Main category: cs.CV

TL;DR: CalibrateMix improves SSL calibration via targeted mixup guided by training dynamics to separate easy and hard samples, achieving lower calibration error and better or comparable accuracy.


<details>
  <summary>Details</summary>
Motivation: Semi-supervised learning models often produce miscalibrated, overconfident predictions. While mixup improves calibration in supervised settings, applying mixup to pseudolabels in SSL is problematic due to unreliable pseudolabels. There is a need for calibration-aware SSL methods that preserve accuracy.

Method: The approach analyzes the training dynamics of labeled and unlabeled samples to classify them as easy-to-learn or hard-to-learn. It then uses targeted mixup that combines easy with hard samples, guiding the mixup process to improve calibration while maintaining or improving classification accuracy in SSL.

Result: Empirical results on benchmark image datasets show that CalibrateMix achieves lower expected calibration error (ECE) and superior accuracy compared to existing SSL approaches.

Conclusion: CalibrateMix demonstrates that targeted mixup guided by training dynamics can improve calibration in SSL without sacrificing performance, offering a practical approach to calibrated semi-supervised learning.

Abstract: Semi-supervised learning (SSL) has demonstrated high performance in image classification tasks by effectively utilizing both labeled and unlabeled data. However, existing SSL methods often suffer from poor calibration, with models yielding overconfident predictions that misrepresent actual prediction likelihoods. Recently, neural networks trained with {\tt mixup} that linearly interpolates random examples from the training set have shown better calibration in supervised settings. However, calibration of neural models remains under-explored in semi-supervised settings. Although effective in supervised model calibration, random mixup of pseudolabels in SSL presents challenges due to the overconfidence and unreliability of pseudolabels. In this work, we introduce CalibrateMix, a targeted mixup-based approach that aims to improve the calibration of SSL models while maintaining or even improving their classification accuracy. Our method leverages training dynamics of labeled and unlabeled samples to identify ``easy-to-learn'' and ``hard-to-learn'' samples, which in turn are utilized in a targeted mixup of easy and hard samples. Experimental results across several benchmark image datasets show that our method achieves lower expected calibration error (ECE) and superior accuracy compared to existing SSL approaches.

</details>


### [188] [GrOCE:Graph-Guided Online Concept Erasure for Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.12968)
*Ning Han,Zhenyu Ge,Feng Han,Yuhua Sun,Chengqing Li,Jingjing Chen*

Main category: cs.CV

TL;DR: GrOCE proposes a training-free, graph-guided concept erasure framework for diffusion models, using a dynamic semantic graph to selectively sever concept dependencies and achieve precise, adaptable removal without retraining; it reports state-of-the-art results on Concept Similarity and FID.


<details>
  <summary>Details</summary>
Motivation: To overcome shortcomings of existing concept erasure methods that rely on costly fine-tuning or coarse semantic separation, which degrade non-target semantics and struggle to adapt to evolving concept sets.

Method: A graph-based, training-free approach comprising: (1) Dynamic Topological Graph Construction for incremental semantic graph building, (2) Adaptive Cluster Identification with multi-hop traversal and similarity-decay scoring, and (3) Selective Edge Severing to remove targeted dependencies while preserving global semantics.

Result: Extensive experiments show GrOCE achieves state-of-the-art performance on Concept Similarity (CS) and Fréchet Inception Distance (FID) metrics, enabling efficient, accurate, and stable concept erasure without retraining.

Conclusion: GrOCE offers a scalable, adaptive, and training-free solution for concept erasure by principled graph-based reasoning over concept dependencies, preserving non-target semantics while removing undesired content.

Abstract: Concept erasure aims to remove harmful, inappropriate, or copyrighted content from text-to-image diffusion models while preserving non-target semantics. However, existing methods either rely on costly fine-tuning or apply coarse semantic separation, often degrading unrelated concepts and lacking adaptability to evolving concept sets. To alleviate this issue, we propose Graph-Guided Online Concept Erasure (GrOCE), a training-free framework that performs precise and adaptive concept removal through graph-based semantic reasoning. GrOCE models concepts and their interrelations as a dynamic semantic graph, enabling principled reasoning over dependencies and fine-grained isolation of undesired content. It comprises three components: (1) Dynamic Topological Graph Construction for incremental graph building, (2) Adaptive Cluster Identification for multi-hop traversal with similarity-decay scoring, and (3) Selective Edge Severing for targeted edge removal while preserving global semantics. Extensive experiments demonstrate that GrOCE achieves state-of-the-art performance on Concept Similarity (CS) and Fréchet Inception Distance (FID) metrics, offering efficient, accurate, and stable concept erasure without retraining.

</details>


### [189] [HiFusion: Hierarchical Intra-Spot Alignment and Regional Context Fusion for Spatial Gene Expression Prediction from Histopathology](https://arxiv.org/abs/2511.12969)
*Ziqiao Weng,Yaoyu Fang,Jiahe Qian,Xinkun Wang,Lee AD Cooper,Weidong Cai,Bo Zhou*

Main category: cs.CV

TL;DR: HiFusion delivers state-of-the-art spatial transcriptomics inference from H&E WSIs by integrating hierarchical intra-spot modeling with context-aware cross-scale fusion; it yields superior performance on two ST benchmarks in both 2D and 3D settings.


<details>
  <summary>Details</summary>
Motivation: To overcome barriers to clinical adoption of spatial transcriptomics by better capturing cellular-level heterogeneity and microenvironment context while mitigating morphological noise and computational costs.

Method: Two modules: (1) Hierarchical Intra-Spot Modeling with multi-resolution sub-patch decomposition and a feature alignment loss to enforce semantic consistency across scales; (2) Context-aware Cross-scale Fusion using cross-attention to selectively incorporate relevant regional context, enabling joint modeling of cellular features and tissue microenvironment cues for gene expression prediction.

Result: On two benchmark ST datasets, HiFusion achieves state-of-the-art performance, outperforming baselines in both 2D slide-wise cross-validation and 3D sample-specific scenarios.

Conclusion: HiFusion is a robust, accurate, and scalable framework for inferring gene expression from routine histopathology, bridging histology and transcriptomics for spatial gene expression prediction.

Abstract: Spatial transcriptomics (ST) bridges gene expression and tissue morphology but faces clinical adoption barriers due to technical complexity and prohibitive costs. While computational methods predict gene expression from H&E-stained whole-slide images (WSIs), existing approaches often fail to capture the intricate biological heterogeneity within spots and are susceptible to morphological noise when integrating contextual information from surrounding tissue. To overcome these limitations, we propose HiFusion, a novel deep learning framework that integrates two complementary components. First, we introduce the Hierarchical Intra-Spot Modeling module that extracts fine-grained morphological representations through multi-resolution sub-patch decomposition, guided by a feature alignment loss to ensure semantic consistency across scales. Concurrently, we present the Context-aware Cross-scale Fusion module, which employs cross-attention to selectively incorporate biologically relevant regional context, thereby enhancing representational capacity. This architecture enables comprehensive modeling of both cellular-level features and tissue microenvironmental cues, which are essential for accurate gene expression prediction. Extensive experiments on two benchmark ST datasets demonstrate that HiFusion achieves state-of-the-art performance across both 2D slide-wise cross-validation and more challenging 3D sample-specific scenarios. These results underscore HiFusion's potential as a robust, accurate, and scalable solution for ST inference from routine histopathology.

</details>


### [190] [MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning](https://arxiv.org/abs/2511.12976)
*Yoonjae Seo,Ermal Elbasani,Jaehong Lee*

Main category: cs.CV

TL;DR: A morphology-aware quantization framework for object detection (MCAQ-YOLO) uses five morphological metrics to guide spatially adaptive bit allocation and includes curriculum-based QAT, achieving higher accuracy and compression efficiency than uniform quantization, validated on safety data and across COCO/VOC.


<details>
  <summary>Details</summary>
Motivation: Uniform quantization applies a single bit precision across an image, ignoring local visual complexity. Exploiting morphological heterogeneity can reduce quantization error where it matters, improving efficiency and robustness for object detection.

Method: Introduce five morphological metrics—fractal dimension, texture entropy, gradient variance, edge density, and contour complexity—to characterize local morphology and predict quantization sensitivity. Use these metrics to drive spatially adaptive bit allocation. Implement a curriculum-based quantization-aware training that progressively increases quantization difficulty to stabilize optimization and speed up convergence.

Result: There is a strong correlation between morphological complexity and quantization sensitivity. MCAQ-YOLO outperforms uniform quantization in detection accuracy and convergence. On a safety equipment dataset, it achieves 85.6% mAP@0.5 with an average of 4.2 bits and 7.6x compression, about 3.5 percentage points higher mAP than uniform 4-bit quantization, with only 1.8 ms extra runtime per image. Cross-dataset validation on COCO and Pascal VOC shows consistent performance gains.

Conclusion: Morphology-driven spatial quantization can improve efficiency and robustness for resource-constrained, safety-critical visual recognition tasks.

Abstract: Most neural network quantization methods apply uniform bit precision across spatial regions, ignoring the heterogeneous structural and textural complexity of visual data. This paper introduces MCAQ-YOLO, a morphological complexity-aware quantization framework for object detection. The framework employs five morphological metrics - fractal dimension, texture entropy, gradient variance, edge density, and contour complexity - to characterize local visual morphology and guide spatially adaptive bit allocation. By correlating these metrics with quantization sensitivity, MCAQ-YOLO dynamically adjusts bit precision according to spatial complexity. In addition, a curriculum-based quantization-aware training scheme progressively increases quantization difficulty to stabilize optimization and accelerate convergence. Experimental results demonstrate a strong correlation between morphological complexity and quantization sensitivity and show that MCAQ-YOLO achieves superior detection accuracy and convergence efficiency compared with uniform quantization. On a safety equipment dataset, MCAQ-YOLO attains 85.6 percent mAP@0.5 with an average of 4.2 bits and a 7.6x compression ratio, yielding 3.5 percentage points higher mAP than uniform 4-bit quantization while introducing only 1.8 ms of additional runtime overhead per image. Cross-dataset validation on COCO and Pascal VOC further confirms consistent performance gains, indicating that morphology-driven spatial quantization can enhance efficiency and robustness for computationally constrained, safety-critical visual recognition tasks.

</details>


### [191] [ArtiWorld: LLM-Driven Articulation of 3D Objects in Scenes](https://arxiv.org/abs/2511.12977)
*Yixuan Yang,Luyang Xie,Zhen Luo,Zixiang Zhao,Mingqi Gao,Feng Zheng*

Main category: cs.CV

TL;DR: ArtiWorld automates conversion of rigid 3D assets into executable URDF-based articulated objects using a scene-aware pipeline (Arti4URDF) that leverages 3D point clouds and LLM priors to reconstruct interactive models, enabling scalable robot-ready simulators directly from existing assets.


<details>
  <summary>Details</summary>
Motivation: Rigid assets dominate simulation environments but are costly to convert to articulated, interacting objects. An automatic, scalable pipeline would reduce manual labor and accelerate the creation of interactive environments for robot learning.

Method: A scene-aware pipeline that localizes candidate articulable objects from textual scene descriptions and reconstructs executable URDF models preserving geometry. The core Arti4URDF uses 3D point clouds, priors from a large language model, and a URDF-oriented prompt design to rapidly convert rigid objects into interactive URDF models.

Result: Across 3D objects, full 3D scenes, and real-world scans, ArtiWorld achieves state-of-the-art performance, preserves geometry, and correctly captures object interactivity to produce usable URDF-based articulated models, outperforming existing approaches.

Conclusion: This work provides a practical path toward building interactive, robot-ready simulation environments directly from existing 3D assets, with code and data to be released.

Abstract: Building interactive simulators and scalable robot-learning environments requires a large number of articulated assets. However, most existing 3D assets in simulation are rigid, and manually converting them into articulated objects is extremely labor- and cost-intensive. This raises a natural question: can we automatically identify articulable objects in a scene and convert them into articulated assets directly? In this paper, we present ArtiWorld, a scene-aware pipeline that localizes candidate articulable objects from textual scene descriptions and reconstructs executable URDF models that preserve the original geometry. At the core of this pipeline is Arti4URDF, which leverages 3D point cloud, prior knowledge of a large language model (LLM), and a URDF-oriented prompt design to rapidly convert rigid objects into interactive URDF-based articulated objects while maintaining their 3D shape. We evaluate ArtiWorld at three levels: 3D simulated objects, full 3D simulated scenes, and real-world scan scenes. Across all three settings, our method consistently outperforms existing approaches and achieves state-of-the-art performance, while preserving object geometry and correctly capturing object interactivity to produce usable URDF-based articulated models. This provides a practical path toward building interactive, robot-ready simulation environments directly from existing 3D assets. Code and data will be released.

</details>


### [192] [Concept Regions Matter: Benchmarking CLIP with a New Cluster-Importance Approach](https://arxiv.org/abs/2511.12978)
*Aishwarya Agarwal,Srikrishna Karanam,Vineet Gandhi*

Main category: cs.CV

TL;DR: Cluster-based Concept Importance (CCI) for CLIP uses patch embeddings to form semantically coherent patch clusters, masks them, and measures prediction changes; achieves state-of-the-art faithfulness; enables foreground/background diagnosis with GroundedSAM; introduces COVAR benchmark to disentangle foreground/background from other factors; evaluates 18 CLIP variants.


<details>
  <summary>Details</summary>
Motivation: Address spurious correlations and background over-reliance in vision-language models and provide faithful interpretability beyond accuracy-based benchmarks; enable diagnostic evaluation of model failures.

Method: Compute CLIP patch embeddings, cluster patches into semantically coherent groups, iteratively mask clusters to assess impact on predictions, measure deletion-AUC; combine with GroundedSAM for foreground/background categorization; create COVAR by varying foregrounds/backgrounds; evaluate 18 CLIP variants.

Result: CCI achieves state-of-the-art faithfulness benchmarks, over 2x improvement in deletion-AUC on MS COCO retrieval; enables automatic foreground/background categorization; COVAR reveals errors from viewpoint, scale, and fine-grained confusions in addition to background correlations; extensive cross-variant evaluation.

Conclusion: CCI + COVAR provide a robust framework for diagnosing and improving VLM robustness, highlighting limitations of accuracy-only benchmarks and guiding development of more reliable VLMs.

Abstract: Contrastive vision-language models (VLMs) such as CLIP achieve strong zero-shot recognition yet remain vulnerable to spurious correlations, particularly background over-reliance. We introduce Cluster-based Concept Importance (CCI), a novel interpretability method that uses CLIP's own patch embeddings to group spatial patches into semantically coherent clusters, mask them, and evaluate relative changes in model predictions. CCI sets a new state of the art on faithfulness benchmarks, surpassing prior methods by large margins; for example, it yields more than a twofold improvement on the deletion-AUC metric for MS COCO retrieval. We further propose that CCI, when combined with GroundedSAM, automatically categorizes predictions as foreground- or background-driven, providing a crucial diagnostic ability. Existing benchmarks such as CounterAnimals, however, rely solely on accuracy and implicitly attribute all performance degradation to background correlations. Our analysis shows this assumption to be incomplete, since many errors arise from viewpoint variation, scale shifts, and fine-grained object confusions. To disentangle these effects, we introduce COVAR, a benchmark that systematically varies object foregrounds and backgrounds. Leveraging CCI with COVAR, we present a comprehensive evaluation of eighteen CLIP variants, offering methodological advances and empirical evidence that chart a path toward more robust VLMs.

</details>


### [193] [UNSEEN: Enhancing Dataset Pruning from a Generalization Perspective](https://arxiv.org/abs/2511.12988)
*Furui Xu,Shaobo Wang,Jiajun Zhang,Chenghao Sun,Haixiang Tang,Linfeng Zhang*

Main category: cs.CV

TL;DR: UNSEEN is a plug-and-play, generalization-focused framework for dataset pruning that improves coreset quality by scoring samples with models not trained on them and extends to multi-step, incremental selection. It achieves SOTA results on CIFAR-10/100 and ImageNet-1K, including lossless training with 30% data reduction.


<details>
  <summary>Details</summary>
Motivation: Existing scoring-based pruning relies on models that have been trained on the full dataset, causing scores to cluster and reducing discrimination between samples. This fitting-centric view limits the ability to identify truly informative samples. A generalization-centric, non-exposed-model scoring approach could produce more informative coresets.

Method: Introduce UNSEEN, a plug-and-play framework that scores samples using models not exposed to them during training. Extend to multi-step scenarios with incremental selection by training scoring models on varying coresets to dynamically optimize coreset quality.

Result: Empirical evaluation shows UNSEEN significantly outperforms state-of-the-art methods on CIFAR-10, CIFAR-100, and ImageNet-1K. Notably, on ImageNet-1K, it achieves lossless performance with a 30% reduction in training data.

Conclusion: A generalization-based, incremental, plug-and-play dataset pruning framework can produce higher-quality coresets and enable substantial data compression without sacrificing performance, challenging relying on fitting-based scoring alone.

Abstract: The growing scale of datasets in deep learning has introduced significant computational challenges. Dataset pruning addresses this challenge by constructing a compact but informative coreset from the full dataset with comparable performance. Previous approaches typically establish scoring metrics based on specific criteria to identify representative samples. However, these methods predominantly rely on sample scores obtained from the model's performance during the training (i.e., fitting) phase. As scoring models achieve near-optimal performance on training data, such fitting-centric approaches induce a dense distribution of sample scores within a narrow numerical range. This concentration reduces the distinction between samples and hinders effective selection. To address this challenge, we conduct dataset pruning from the perspective of generalization, i.e., scoring samples based on models not exposed to them during training. We propose a plug-and-play framework, UNSEEN, which can be integrated into existing dataset pruning methods. Additionally, conventional score-based methods are single-step and rely on models trained solely on the complete dataset, providing limited perspective on the importance of samples. To address this limitation, we scale UNSEEN to multi-step scenarios and propose an incremental selection technique through scoring models trained on varying coresets, and optimize the quality of the coreset dynamically. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art (SOTA) methods on CIFAR-10, CIFAR-100, and ImageNet-1K. Notably, on ImageNet-1K, UNSEEN achieves lossless performance while reducing training data by 30\%.

</details>


### [194] [Semantic Prioritization in Visual Counterfactual Explanations with Weighted Segmentation and Auto-Adaptive Region Selection](https://arxiv.org/abs/2511.12992)
*Lintong Zhang,Kang Yin,Seong-Whan Lee*

Main category: cs.CV

TL;DR: WSAE-Net introduces a weighted semantic map and auto-adaptive candidate editing to improve non-generative visual counterfactual explanations by emphasizing semantic relevance and reducing computation, yielding clearer explanations and efficient editing.


<details>
  <summary>Details</summary>
Motivation: Conventional non-generative CE methods replace regions from distractors without considering semantic relevance to the target object, hurting interpretability and editing efficiency.

Method: Two main contributions: (1) a weighted semantic map that minimizes computation by focusing on semantic-relevant feature units; (2) an auto-adaptive editing sequence that optimizes the order of feature-unit processing to produce semantically aligned counterfactuals efficiently.

Result: Experiments show superior performance and more lucid understanding of visual counterfactual explanations compared to baselines.

Conclusion: WSAE-Net advances non-generative visual counterfactual explanations by coupling semantic relevance with computational efficiency, improving interpretability and editing workflow.

Abstract: In the domain of non-generative visual counterfactual explanations (CE), traditional techniques frequently involve the substitution of sections within a query image with corresponding sections from distractor images. Such methods have historically overlooked the semantic relevance of the replacement regions to the target object, thereby impairing the model's interpretability and hindering the editing workflow. Addressing these challenges, the present study introduces an innovative methodology named as Weighted Semantic Map with Auto-adaptive Candidate Editing Network (WSAE-Net). Characterized by two significant advancements: the determination of an weighted semantic map and the auto-adaptive candidate editing sequence. First, the generation of the weighted semantic map is designed to maximize the reduction of non-semantic feature units that need to be computed, thereby optimizing computational efficiency. Second, the auto-adaptive candidate editing sequences are designed to determine the optimal computational order among the feature units to be processed, thereby ensuring the efficient generation of counterfactuals while maintaining the semantic relevance of the replacement feature units to the target object. Through comprehensive experimentation, our methodology demonstrates superior performance, contributing to a more lucid and in-depth understanding of visual counterfactual explanations.

</details>


### [195] [PerTouch: VLM-Driven Agent for Personalized and Semantic Image Retouching](https://arxiv.org/abs/2511.12998)
*Zewei Chang,Zheng-Peng Duan,Jianxing Zhang,Chun-Le Guo,Siyu Liu,Hyungju Chun,Hyunhee Park,Zikun Liu,Chongyi Li*

Main category: cs.CV

TL;DR: PerTouch is a diffusion-based framework enabling semantic-level image retouching with explicit parameter maps, semantic boundary enhancements, and a VLM-driven agent for natural language alignment, plus feedback and memory for personalization.


<details>
  <summary>Details</summary>
Motivation: Balancing controllability and subjectivity in image retouching is challenging: semantic-region control and robust language grounding are needed to reflect user preferences and long-term tastes.

Method: PerTouch is a unified diffusion-based retouching framework that takes parameter maps with region-specific attribute values to explicitly map parameters to image edits. It introduces semantic replacement and parameter perturbation during training to improve semantic boundary perception. A VLM-driven agent connects natural language instructions to visual control, augmented with feedback-driven rethinking and scene-aware memory to align with user intent and capture long-term preferences.

Result: Extensive experiments validate each component and show PerTouch achieving superior performance in personalized image retouching; ablations demonstrate contributions of semantic replacement, perturbation, and language grounding. Code is available at the provided GitHub link.

Conclusion: PerTouch offers a controllable, semantically aware, and personalized image retouching framework that integrates diffusion-based editing with region-aware parameterization and language grounding, demonstrating strong performance and potential for user-tailored aesthetics.

Abstract: Image retouching aims to enhance visual quality while aligning with users' personalized aesthetic preferences. To address the challenge of balancing controllability and subjectivity, we propose a unified diffusion-based image retouching framework called PerTouch. Our method supports semantic-level image retouching while maintaining global aesthetics. Using parameter maps containing attribute values in specific semantic regions as input, PerTouch constructs an explicit parameter-to-image mapping for fine-grained image retouching. To improve semantic boundary perception, we introduce semantic replacement and parameter perturbation mechanisms in the training process. To connect natural language instructions with visual control, we develop a VLM-driven agent that can handle both strong and weak user instructions. Equipped with mechanisms of feedback-driven rethinking and scene-aware memory, PerTouch better aligns with user intent and captures long-term preferences. Extensive experiments demonstrate each component's effectiveness and the superior performance of PerTouch in personalized image retouching. Code is available at: https://github.com/Auroral703/PerTouch.

</details>


### [196] [Medal S: Spatio-Textual Prompt Model for Medical Segmentation](https://arxiv.org/abs/2511.13001)
*Pengcheng Shi,Jiawei Chen,Jiaqi Liu,Xinglin Zhang,Tao Chen,Lei Li*

Main category: cs.CV

TL;DR: Medal S is a 3D-aware medical segmentation foundation model that supports native-resolution spatial and textual prompts and achieves efficient, accurate multi-class segmentation across modalities, with two prompting modes and substantial speedups over sequential prompting.


<details>
  <summary>Details</summary>
Motivation: To overcome text-only, non-spatial segmentation approaches by preserving full 3D context and enabling native-resolution prompts, improving accuracy and efficiency in multi-class medical segmentation across diverse imaging modalities.

Method: End-to-end trainable framework with channel-wise alignment between volumetric prompts and text embeddings; lightweight 3D convolutional module for voxel-space refinement; parallel processing of native-resolution masks; two prompting modes (text-only self-refinement and hybrid with manual annotations); dynamic resampling; extended SAT/nnU-Net augmentations; optimized text preprocessing; two-stage inference; post-processing for memory and speed; supports up to 243 classes in BiomedSegFM.

Result: Achieves higher metrics on 5 modalities: DSC 75.44 vs 69.83; NSD 77.34 vs 71.06; F1 38.24 vs 24.88; DSC TP 65.46 vs 46.97; parallel spatial prompting dramatically reduces inference time (>90% for 24-class); improves efficiency and accuracy over sequential prompting.

Conclusion: Medal S harmonizes spatial precision with semantic guidance, delivering superior efficiency and accuracy for multi-class medical segmentation; public release planned at the provided GitHub repository.

Abstract: We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S.

</details>


### [197] [Infinite-Story: A Training-Free Consistent Text-to-Image Generation](https://arxiv.org/abs/2511.13002)
*Jihun Park,Kyoungmin Lee,Jongmin Gim,Hyeonseo Jo,Minseok Oh,Wonhyeok Choi,Kyumin Hwang,Jaeyeul Kim,Minwoo Choi,Sunghoon Im*

Main category: cs.CV

TL;DR: Infinite-Story is a training-free framework for consistent text-to-image generation across multi-prompt storytelling, achieving high identity/style consistency with faster inference using Identity Prompt Replacement and unified attention guidance in a scale-wise autoregressive model.


<details>
  <summary>Details</summary>
Motivation: Address identity inconsistency and style inconsistency in multi-prompt T2I storytelling; overcome reliance on fine-tuning diffusion models and slow inference.

Method: Three core components: Identity Prompt Replacement (reduces text-encoder context bias to align identity across prompts); Unified attention guidance with Adaptive Style Injection and Synchronized Guidance Adaptation (enforce global style and identity consistency while preserving prompt fidelity); operates at test time only (no training); uses a scale-wise autoregressive model.

Result: Achieves state-of-the-art generation performance; over 6x faster inference than existing fastest consistent T2I models; ~1.72 seconds per image; demonstrates high identity and style consistency across diverse prompts.

Conclusion: Training-free, fast, and effective approach for consistent multi-prompt T2I storytelling, enabling practical deployment without fine-tuning; highlights potential impact on real-world visual storytelling.

Abstract: We present Infinite-Story, a training-free framework for consistent text-to-image (T2I) generation tailored for multi-prompt storytelling scenarios. Built upon a scale-wise autoregressive model, our method addresses two key challenges in consistent T2I generation: identity inconsistency and style inconsistency. To overcome these issues, we introduce three complementary techniques: Identity Prompt Replacement, which mitigates context bias in text encoders to align identity attributes across prompts; and a unified attention guidance mechanism comprising Adaptive Style Injection and Synchronized Guidance Adaptation, which jointly enforce global style and identity appearance consistency while preserving prompt fidelity. Unlike prior diffusion-based approaches that require fine-tuning or suffer from slow inference, Infinite-Story operates entirely at test time, delivering high identity and style consistency across diverse prompts. Extensive experiments demonstrate that our method achieves state-of-the-art generation performance, while offering over 6X faster inference (1.72 seconds per image) than the existing fastest consistent T2I models, highlighting its effectiveness and practicality for real-world visual storytelling.

</details>


### [198] [SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias](https://arxiv.org/abs/2511.13005)
*Wenqian Ye,Di Wang,Guangtao Zheng,Bohan Liu,Aidong Zhang*

Main category: cs.CV

TL;DR: SAGE is training-free, prompt-based mitigation of multimodal spurious bias in zero-shot CLIP, selecting prompts that maximize class separation to improve worst-group robustness without fine-tuning or annotations.


<details>
  <summary>Details</summary>
Motivation: CLIP's joint image-text embedding often relies on spurious correlations (e.g., backgrounds) leading to poor out-of-distribution robustness; current fixes require fine-tuning or bias annotations, hurting usability.

Method: The paper analyzes the impact of multimodal spurious bias theoretically. Proposes Spuriousness-Aware Guided Exploration (SAGE) that searches a space of prompt templates and selects prompts that maximize semantic separation among classes, guiding exploration to reduce reliance on spurious cues; no training or external data required.

Result: Empirical evaluation on four real-world benchmarks and five backbones shows SAGE consistently improves zero-shot accuracy and generalization, particularly worst-group robustness, outperforming prior zero-shot methods without model updates or external knowledge.

Conclusion: SAGE offers a simple, effective, and broadly applicable approach to mitigating multimodal spurious bias in zero-shot CLIP, preserving usability while enhancing robustness across datasets and backbones.

Abstract: Large vision-language models, such as CLIP, have shown strong zero-shot classification performance by aligning images and text in a shared embedding space. However, CLIP models often develop multimodal spurious biases, which is the undesirable tendency to rely on spurious features. For example, CLIP may infer object types in images based on frequently co-occurring backgrounds rather than the object's core features. This bias significantly impairs the robustness of pre-trained CLIP models on out-of-distribution data, where such cross-modal associations no longer hold. Existing methods for mitigating multimodal spurious bias typically require fine-tuning on downstream data or prior knowledge of the bias, which undermines the out-of-the-box usability of CLIP. In this paper, we first theoretically analyze the impact of multimodal spurious bias in zero-shot classification. Based on this insight, we propose Spuriousness-Aware Guided Exploration (SAGE), a simple and effective method that mitigates spurious bias through guided prompt selection. SAGE requires no training, fine-tuning, or external annotations. It explores a space of prompt templates and selects the prompts that induce the largest semantic separation between classes, thereby improving worst-group robustness. Extensive experiments on four real-world benchmark datasets and five popular backbone models demonstrate that SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without any external knowledge or model updates.

</details>


### [199] [Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis](https://arxiv.org/abs/2511.13011)
*Qingsen Ma,Chen Zou,Dianyun Wang,Jia Wang,Liuyu Xiang,Zhaofeng He*

Main category: cs.CV

TL;DR: A unified DTGS framework jointly optimizes illumination, geometry, and thermal cues for low-light NVS by integrating Retinex-based reflectance-illumination separation into 3D Gaussian Splatting, plus a thermal supervision branch and a new low-light thermal dataset.


<details>
  <summary>Details</summary>
Motivation: Under extreme low light, NVS suffers geometric distortion, color inconsistency, and radiometric instability. Standard 3DGS falters when enhancement is done per-view, causing cross-view illumination misalignment.

Method: DTGS couples Retinex-inspired illumination decomposition with thermal-guided 3D Gaussian Splatting in a cyclic enhancement-reconstruction loop. A thermal supervisory branch balances enhancement, structure, and thermal losses and a Retinex-based module inside the 3DGS loop enforces reflectance-illumination separation. They also introduce RGBT-LOW dataset for evaluation.

Result: DTGS significantly outperforms existing low-light enhancement and 3D reconstruction baselines, delivering better radiometric consistency, geometric fidelity, and color stability under extreme illumination.

Conclusion: Jointly optimizing enhancement, geometry, and thermal cues with interpretable decomposition yields illumination-invariant NVS, demonstrated on a new challenging dataset; the approach advances robust multi-view reconstruction in low light.

Abstract: Under extremely low-light conditions, novel view synthesis (NVS) faces severe degradation in terms of geometry, color consistency, and radiometric stability. Standard 3D Gaussian Splatting (3DGS) pipelines fail when applied directly to underexposed inputs, as independent enhancement across views causes illumination inconsistencies and geometric distortion. To address this, we present DTGS, a unified framework that tightly couples Retinex-inspired illumination decomposition with thermal-guided 3D Gaussian Splatting for illumination-invariant reconstruction. Unlike prior approaches that treat enhancement as a pre-processing step, DTGS performs joint optimization across enhancement, geometry, and thermal supervision through a cyclic enhancement-reconstruction mechanism. A thermal supervisory branch stabilizes both color restoration and geometry learning by dynamically balancing enhancement, structural, and thermal losses. Moreover, a Retinex-based decomposition module embedded within the 3DGS loop provides physically interpretable reflectance-illumination separation, ensuring consistent color and texture across viewpoints. To evaluate our method, we construct RGBT-LOW, a new multi-view low-light thermal dataset capturing severe illumination degradation. Extensive experiments show that DTGS significantly outperforms existing low-light enhancement and 3D reconstruction baselines, achieving superior radiometric consistency, geometric fidelity, and color stability under extreme illumination.

</details>


### [200] [You Only Look Omni Gradient Backpropagation for Moving Infrared Small Target Detection](https://arxiv.org/abs/2511.13013)
*Guoyi Zhang,Guangsheng Xu,Siyang Chen,Han Wang,Xiaohu Zhang*

Main category: cs.CV

TL;DR: A backpropagation-driven feature pyramid (BP-FPN) with Gradient-Isolated Low-Level Shortcut (GILS) and Directional Gradient Regularization (DGR) to dramatically improve infrared small target detection, achieving state-of-the-art performance with negligible overhead.


<details>
  <summary>Details</summary>
Motivation: Infrared small target detection faces low SNR, severe target-background imbalance, and weak discriminative features. Existing deep learning methods focus on spatio-temporal aggregation, but gains are limited due to ambiguous per-frame feature representations; the fundamental bottleneck is per-frame feature learning rather than temporal modeling.

Method: BP-FPN is a backpropagation-driven feature pyramid architecture. It introduces Gradient-Isolated Low-Level Shortcut (GILS) to incorporate fine-grained target details without inducing shortcut learning, and Directional Gradient Regularization (DGR) to enforce hierarchical feature consistency during backpropagation. The design is theoretically grounded, incurs negligible computational overhead, and can be integrated into existing frameworks. It is claimed to be the first FPN designed for this task entirely from the backpropagation perspective.

Result: Extensive experiments on multiple public datasets show that BP-FPN consistently achieves new state-of-the-art performance for infrared small target detection.

Conclusion: BP-FPN offers a novel backpropagation-centric design for small-target feature learning, enabling better per-frame representations, seamless integration with existing frameworks, and establishing the first backprop-designed FPN for infrared small target detection with negligible overhead.

Abstract: Moving infrared small target detection is a key component of infrared search and tracking systems, yet it remains extremely challenging due to low signal-to-clutter ratios, severe target-background imbalance, and weak discriminative features. Existing deep learning methods primarily focus on spatio-temporal feature aggregation, but their gains are limited, revealing that the fundamental bottleneck lies in ambiguous per-frame feature representations rather than spatio-temporal modeling itself. Motivated by this insight, we propose BP-FPN, a backpropagation-driven feature pyramid architecture that fundamentally rethinks feature learning for small target. BP-FPN introduces Gradient-Isolated Low-Level Shortcut (GILS) to efficiently incorporate fine-grained target details without inducing shortcut learning, and Directional Gradient Regularization (DGR) to enforce hierarchical feature consistency during backpropagation. The design is theoretically grounded, introduces negligible computational overhead, and can be seamlessly integrated into existing frameworks. Extensive experiments on multiple public datasets show that BP-FPN consistently establishes new state-of-the-art performance. To the best of our knowledge, it is the first FPN designed for this task entirely from the backpropagation perspective.

</details>


### [201] [Geometry Meets Light: Leveraging Geometric Priors for Universal Photometric Stereo under Limited Multi-Illumination Cues](https://arxiv.org/abs/2511.13015)
*King-Man Tam,Satoshi Ikehata,Yuta Asano,Zhaoyi An,Rei Kawakami*

Main category: cs.CV

TL;DR: GeoUniPS introduces a universal photometric stereo network that leverages synthetic supervision and high-level geometric priors from large pretrained 3D recon models, using a Light-Geometry Dual-Branch Encoder to fuse multi-illumination cues with frozen geometry. It also introduces PS-Perp to handle perspective projection, enabling learning of spatially varying view directions, achieving state-of-the-art results on in-the-wild datasets.


<details>
  <summary>Details</summary>
Motivation: Photometric stereo often falters when multi-illumination cues are unreliable due to bias, shadows, or self-occlusion in complex real-world scenes. There is a need to incorporate robust geometric priors learned from large-scale 3D data to improve normal estimation.

Method: A Light-Geometry Dual-Branch Encoder that jointly extracts multi-illumination cues and geometric priors from a frozen large-scale 3D reconstruction model. Utilizes synthetic supervision and PS-Perp dataset to model perspective projection and spatially varying view directions. The 3D models serve as visual-geometry foundation models.

Result: GeoUniPS achieves state-of-the-art performance on multiple datasets, with strong qualitative and quantitative improvements, especially in challenging in-the-wild scenes.

Conclusion: 3D reconstruction models encode rich geometric knowledge that can be leveraged as visual-geometry foundations for photometric stereo, and integrating them with synthetic supervision and perspective-aware learning yields robust surface normal estimation in diverse conditions.

Abstract: Universal Photometric Stereo is a promising approach for recovering surface normals without strict lighting assumptions. However, it struggles when multi-illumination cues are unreliable, such as under biased lighting or in shadows or self-occluded regions of complex in-the-wild scenes. We propose GeoUniPS, a universal photometric stereo network that integrates synthetic supervision with high-level geometric priors from large-scale 3D reconstruction models pretrained on massive in-the-wild data. Our key insight is that these 3D reconstruction models serve as visual-geometry foundation models, inherently encoding rich geometric knowledge of real scenes. To leverage this, we design a Light-Geometry Dual-Branch Encoder that extracts both multi-illumination cues and geometric priors from the frozen 3D reconstruction model. We also address the limitations of the conventional orthographic projection assumption by introducing the PS-Perp dataset with realistic perspective projection to enable learning of spatially varying view directions. Extensive experiments demonstrate that GeoUniPS delivers state-of-the-arts performance across multiple datasets, both quantitatively and qualitatively, especially in the complex in-the-wild scenes.

</details>


### [202] [MeanFlow Transformers with Representation Autoencoders](https://arxiv.org/abs/2511.13019)
*Zheyuan Hu,Chieh-Hsin Lai,Ge Wu,Yuki Mitsufuji,Stefano Ermon*

Main category: cs.CV

TL;DR: Efficient latent-space mean-flow diffusion via a Representation Autoencoder (RAE) with Consistency Mid-Training and distillation, achieving faster sampling and lower training cost while improving sample quality on ImageNet.


<details>
  <summary>Details</summary>
Motivation: Mean Flow (MF) diffusion is powerful but expensive and unstable in practice, especially when trained in SD-VAE latent space. Training costs dominate inference, and class-conditional guidance adds complexity. The work seeks to stabilize training and reduce computation by operating in a more compact, semantically rich latent space provided by a pre-trained encoder.

Method: Train MF in the RAE latent space using a pre-trained vision encoder (e.g., DINO) with a lightweight decoder. Address gradient explosion via Consistency Mid-Training for trajectory-aware initialization. Use a two-stage scheme: (1) distillation from a pre-trained flow-matching teacher to speed convergence and reduce variance, (2) optional bootstrapping with a one-point velocity estimator to reduce deviation from the oracle mean flow. This removes the need for guidance during training and sampling.

Result: 1-step FID improves from 3.43 (vanilla MF) to 2.03 on ImageNet-256. Sampling GFLOPS reduced by 38%, and total training cost reduced by 83%. On ImageNet-512, achieves 1-step FID of 3.23 with the lowest GFLOPS among baselines.

Conclusion: The latent MF in an RAE space with Consistency Mid-Training and distillation yields more efficient training and sampling, with competitive or superior sample quality and significantly reduced computational cost. Code is publicly available.

Abstract: MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at https://github.com/sony/mf-rae.

</details>


### [203] [SpectralAdapt: Semi-Supervised Domain Adaptation with Spectral Priors for Human-Centered Hyperspectral Image Reconstruction](https://arxiv.org/abs/2511.13020)
*Yufei Wen,Yuting Zhang,Jingdan Kang,Hao Ren,Weibin Cheng,Jintai Chen,Kaishun Wu*

Main category: cs.CV

TL;DR: SpectralAdapt is a semi-supervised domain-adaptation framework for hyperspectral image reconstruction from RGB, featuring Spectral Density Masking (SDM) and Spectral Endmember Representation Alignment (SERA) to improve spectral fidelity and cross-domain generalization in healthcare applications.


<details>
  <summary>Details</summary>
Motivation: The scarcity of labeled human hyperspectral data and the cost/difficulty of acquiring HSI data motivate leveraging abundant unlabeled data and general-domain datasets, through domain adaptation to bridge domain gaps and mitigate spectral degradation.

Method: SpectralAdapt combines SDM, which adaptively masks RGB channels by spectral complexity during consistency training, with SERA, which derives physically meaningful endmembers from labeled pixels to serve as domain-invariant anchors for unlabeled predictions, employing momentum updates for stability.

Result: Experiments on benchmark datasets show consistent improvements in spectral fidelity, cross-domain generalization, and training stability when using SpectralAdapt.

Conclusion: Semi-supervised domain adaptation with spectral priors (SDM and SERA) effectively mitigates domain shift, spectral degradation, and data scarcity in HSI reconstruction, highlighting SSDA as a promising approach for healthcare-focused hyperspectral imaging.

Abstract: Hyperspectral imaging (HSI) holds great potential for healthcare due to its rich spectral information. However, acquiring HSI data remains costly and technically demanding. Hyperspectral image reconstruction offers a practical solution by recovering HSI data from accessible modalities, such as RGB. While general domain datasets are abundant, the scarcity of human HSI data limits progress in medical applications. To tackle this, we propose SpectralAdapt, a semi-supervised domain adaptation (SSDA) framework that bridges the domain gap between general and human-centered HSI datasets. To fully exploit limited labels and abundant unlabeled data, we enhance spectral reasoning by introducing Spectral Density Masking (SDM), which adaptively masks RGB channels based on their spectral complexity, encouraging recovery of informative regions from complementary cues during consistency training. Furthermore, we introduce Spectral Endmember Representation Alignment (SERA), which derives physically interpretable endmembers from valuable labeled pixels and employs them as domain-invariant anchors to guide unlabeled predictions, with momentum updates ensuring adaptability and stability. These components are seamlessly integrated into SpectralAdapt, a spectral prior-guided framework that effectively mitigates domain shift, spectral degradation, and data scarcity in HSI reconstruction. Experiments on benchmark datasets demonstrate consistent improvements in spectral fidelity, cross-domain generalization, and training stability, highlighting the promise of SSDA as an efficient solution for hyperspectral imaging in healthcare.

</details>


### [204] [REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding](https://arxiv.org/abs/2511.13026)
*Jiaze Li,Hao Yin,Wenhui Tan,Jingyang Chen,Boshen Xu,Yuxun Qu,Yijing Chen,Jianzhong Ju,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: REVISOR enables tool-augmented multimodal reflective reasoning for long-form video understanding via DADR, improving performance across benchmarks without extra supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Long-form video understanding requires richer visual input and cross-modal integration; text-only reflection fails to leverage visual evidence during reasoning.

Method: Introduce REVISOR, a framework for collaborative introspective reasoning across text and visual modalities; introduce Dual Attribution Decoupled Reward (DADR) integrated into GRPO training to align reasoning with video evidence.

Result: Significant performance gains on four benchmarks: VideoMME, LongVideoBench, MLVU, LVBench.

Conclusion: REVISOR demonstrates the effectiveness of cross-modal reflective reasoning and reward-driven alignment for long-form video understanding, without extra supervised fine-tuning or external models.

Abstract: Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.

</details>


### [205] [Towards 3D Object-Centric Feature Learning for Semantic Scene Completion](https://arxiv.org/abs/2511.13031)
*Weihua Wang,Yubo Cui,Xiangru Lin,Zhiheng Li,Zheng Fang*

Main category: cs.CV

TL;DR: Ocean is an object-centric 3D Semantic Scene Completion framework that decomposes scenes into individual object instances to predict semantic occupancy more accurately, achieving state-of-the-art results on SemanticKITTI and SSCBench-KITTI360 (mIoU 17.40 and 20.28).


<details>
  <summary>Details</summary>
Motivation: Most vision-based SSC methods are ego-centric and aggregate features over the whole scene, which overlooks fine-grained object-level details and leads to semantic/geometric ambiguities in complex environments. An object-centric approach can better preserve instance-level information and improve predictions, especially when segmentation quality varies.

Method: 1) Use MobileSAM to extract instance masks from input images. 2) Apply a 3D Semantic Group Attention module with linear attention to aggregate object-centric features in 3D space. 3) Use a Global Similarity-Guided Attention module to exploit segmentation features for global interaction and to mitigate segmentation errors or missing instances. 4) Employ an Instance-aware Local Diffusion module to refine instance features via a generative diffusion process and then refine the scene representation in BEV space.

Result: Experimental results on SemanticKITTI and SSCBench-KITTI360 show state-of-the-art performance with mIoU scores of 17.40 and 20.28, respectively, demonstrating strong effectiveness across benchmarks.

Conclusion: An object-centric SSC framework with dedicated attention and diffusion components can surpass ego-centric methods by preserving and refining object-level information, yielding improved semantic occupancy predictions. The approach leverages segmentation-derived instance information and BEV refinement, though its reliance on segmentation quality and potential computational cost warrant further exploration.

Abstract: Vision-based 3D Semantic Scene Completion (SSC) has received growing attention due to its potential in autonomous driving. While most existing approaches follow an ego-centric paradigm by aggregating and diffusing features over the entire scene, they often overlook fine-grained object-level details, leading to semantic and geometric ambiguities, especially in complex environments. To address this limitation, we propose Ocean, an object-centric prediction framework that decomposes the scene into individual object instances to enable more accurate semantic occupancy prediction. Specifically, we first employ a lightweight segmentation model, MobileSAM, to extract instance masks from the input image. Then, we introduce a 3D Semantic Group Attention module that leverages linear attention to aggregate object-centric features in 3D space. To handle segmentation errors and missing instances, we further design a Global Similarity-Guided Attention module that leverages segmentation features for global interaction. Finally, we propose an Instance-aware Local Diffusion module that improves instance features through a generative process and subsequently refines the scene representation in the BEV space. Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that Ocean achieves state-of-the-art performance, with mIoU scores of 17.40 and 20.28, respectively.

</details>


### [206] [Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts](https://arxiv.org/abs/2511.13032)
*Sheng Liu,Yuanzhi Liang,Jiepeng Wang,Sidan Du,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: A unified, volume-based framework (Uni-Inter) uses a Unified Interactive Volume (UIV) to jointly model diverse human interactions (human-human, human-object, human-scene) for motion generation, achieving competitive results and strong generalization across unseen entity combinations.


<details>
  <summary>Details</summary>
Motivation: Existing motion-generation methods are typically task-specific and struggle to generalize to novel interaction scenarios. A unified representation and relational reasoning mechanism is needed to scale motion synthesis in complex environments.

Method: Introduce Unified Interactive Volume (UIV), a volumetric representation that encodes heterogeneous interactive entities into a shared spatial field. Formulate motion generation as joint-wise probabilistic prediction over the UIV to capture fine-grained spatial dependencies and enable coherent, context-aware behaviors across interaction types.

Result: Experiments on three representative interaction tasks show Uni-Inter achieves competitive performance and generalizes well to novel combinations of entities.

Conclusion: Jointly modeling compound interactions via a unified volumetric representation is a promising direction for scalable, robust motion synthesis in complex environments.

Abstract: We present Uni-Inter, a unified framework for human motion generation that supports a wide range of interaction scenarios: including human-human, human-object, and human-scene-within a single, task-agnostic architecture. In contrast to existing methods that rely on task-specific designs and exhibit limited generalization, Uni-Inter introduces the Unified Interactive Volume (UIV), a volumetric representation that encodes heterogeneous interactive entities into a shared spatial field. This enables consistent relational reasoning and compound interaction modeling. Motion generation is formulated as joint-wise probabilistic prediction over the UIV, allowing the model to capture fine-grained spatial dependencies and produce coherent, context-aware behaviors. Experiments across three representative interaction tasks demonstrate that Uni-Inter achieves competitive performance and generalizes well to novel combinations of entities. These results suggest that unified modeling of compound interactions offers a promising direction for scalable motion synthesis in complex environments.

</details>


### [207] [uCLIP: Parameter-Efficient Multilingual Extension of Vision-Language Models with Unpaired Data](https://arxiv.org/abs/2511.13036)
*Dahyun Chung,Donghyun Shin,Yujin Sung,Seunggi Moon,Jinwoo Jeon,Byung-Jun Lee*

Main category: cs.CV

TL;DR: A lightweight, data-efficient pivot-based multilingual alignment framework for vision-language models that freezes encoders and learns a small projection module to align non-English languages using English anchors; achieves improvements on low-resource languages without requiring image-text or text-text pairs.


<details>
  <summary>Details</summary>
Motivation: Multilingual vision-language models underperform for underrepresented languages due to scarce high-quality multilingual image-text data. There is a need for parameter-efficient methods that can leverage existing English semantic anchors to improve retrieval in low-resource languages.

Method: Freeze the pretrained image and multilingual text encoders. Train a compact 1.7M-parameter projection module using a contrastive loss with English representations as semantic anchors. No image-text or text-text pairs are required. The approach is pivot-based and data-efficient, aiming to align multilingual representations via English-centric anchors.

Result: The method achieves significant gains on five underrepresented languages on benchmarks (e.g., XM3600) where prior multilingual models underperform, demonstrating robust multilingual retrieval improvements across multiple benchmarks.

Conclusion: Pivot-based, parameter-efficient multilingual alignment is effective for inclusive multimodal learning, enabling strong cross-language retrieval without large-scale multilingual paired data or full fine-tuning.

Abstract: Contrastive Language-Image Pre-training (CLIP) has demonstrated strong generalization across a wide range of visual tasks by leveraging large-scale English-image pairs. However, its extension to low-resource languages remains limited due to the scarcity of high-quality multilingual image-text data. Existing multilingual vision-language models exhibit consistently low retrieval performance in underrepresented languages including Czech, Finnish, Croatian, Hungarian, and Romanian on the Crossmodal-3600 (XM3600) benchmark. To address this, we propose a lightweight and data-efficient framework for multilingual vision-language alignment. Our approach requires no image-text pairs or text-text pairs and freezes both the pretrained image encoder and multilingual text encoder during training. Only a compact 1.7M-parameter projection module is trained, using a contrastive loss over English representations as semantic anchors. This minimal training setup enables robust multilingual alignment even for languages with limited supervision. Extensive evaluation across multiple multilingual retrieval benchmarks confirms the effectiveness of our method, showing significant gains in five underrepresented languages where existing models typically underperform. These findings highlight the effectiveness of our pivot-based, parameter-efficient alignment strategy for inclusive multimodal learning.

</details>


### [208] [MGCA-Net: Multi-Grained Category-Aware Network for Open-Vocabulary Temporal Action Localization](https://arxiv.org/abs/2511.13039)
*Zhenying Fang,Richang Hong*

Main category: cs.CV

TL;DR: MGCA-Net is a multi-grained approach for Open-Vocabulary Temporal Action Localization that integrates a localizer, action presence predictor, base-action classifier, and a coarse-to-fine classifier to recognize both base and novel actions at multiple granularities, achieving state-of-the-art results on THUMOS'14, ActivityNet-1.3, and zero-shot TAL.


<details>
  <summary>Details</summary>
Motivation: Open-Vocabulary TAL struggles to localize and classify actions across both base (seen) and novel (unseen) categories due to single-granularity modeling. A multi-granularity, category-aware framework can jointly leverage base-action knowledge and coarse-to-fine cues to improve localization and open-category recognition.

Method: Propose MGCA-Net with four components: (1) a category-agnostic localizer for action proposals; (2) an action presence predictor estimating proposal-level action likelihood; (3) a conventional classifier for base-action probabilities at snippet granularity; (4) a coarse-to-fine classifier for novel actions that first detects action presence at video granularity and then assigns proposals to coarse categories, enabling multi-grained category awareness.

Result: The method achieves state-of-the-art performance on THUMOS'14 and ActivityNet-1.3 and attains state-of-the-art results under zero-shot temporal action localization settings.

Conclusion: Multi-grained category awareness via MGCA-Net effectively enhances localization performance for open-vocabulary TAL by leveraging both base-action knowledge and coarse-to-fine cues for novel actions.

Abstract: Open-Vocabulary Temporal Action Localization (OV-TAL) aims to recognize and localize instances of any desired action categories in videos without explicitly curating training data for all categories. Existing methods mostly recognize action categories at a single granularity, which degrades the recognition accuracy of both base and novel action categories. To address these issues, we propose a Multi-Grained Category-Aware Network (MGCA-Net) comprising a localizer, an action presence predictor, a conventional classifier, and a coarse-to-fine classifier. Specifically, the localizer localizes category-agnostic action proposals. For these action proposals, the action presence predictor estimates the probability that they belong to an action instance. At the same time, the conventional classifier predicts the probability of each action proposal over base action categories at the snippet granularity. Novel action categories are recognized by the coarse-to-fine classifier, which first identifies action presence at the video granularity. Finally, it assigns each action proposal to one category from the coarse categories at the proposal granularity. Through coarse-to-fine category awareness for novel actions and the conventional classifier's awareness of base actions, multi-grained category awareness is achieved, effectively enhancing localization performance. Comprehensive evaluations on the THUMOS'14 and ActivityNet-1.3 benchmarks demonstrate that our method achieves state-of-the-art performance. Furthermore, our MGCA-Net achieves state-of-the-art results under the Zero-Shot Temporal Action Localization setting.

</details>


### [209] [ViSS-R1: Self-Supervised Reinforcement Video Reasoning](https://arxiv.org/abs/2511.13054)
*Bo Fang,Yuxin Song,Qiangqiang Wu,Haoyuan Sun,Wenhao Wu,Antoni B. Chan*

Main category: cs.CV

TL;DR: Proposes a visual-centric self-supervised reinforcement learning approach for video reasoning in MLLMs using Pretext-GRPO and ViSS-R1; integrates pretext tasks with R1; evaluated on six benchmarks; promising results; code release planned.


<details>
  <summary>Details</summary>
Motivation: Current R1-based MLLMs rely on text-centric reasoning, underutilize rich visual signals in video tasks, leading to shortcut learning and hallucination; need robust, visual-centric video understanding.

Method: Introduce Pretext-GRPO: self-supervised reinforcement learning where positive rewards are given for solving pretext tasks on transformed video inputs within the R1 pipeline, forcing visual processing. Build ViSS-R1: integrates pretext-task self-supervised learning into R1 post-training; models must process transformed visuals and respond to both pretext transformation questions and real user queries, identifying the transformation and reconstructing the original video to answer.

Result: Comprehensive evaluations on six widely-used video reasoning/understanding benchmarks demonstrate effectiveness and superiority of Pretext-GRPO and ViSS-R1 for complex video reasoning.

Conclusion: The proposed framework strengthens visual-centric video understanding in MLLMs; code and models will be publicly available.

Abstract: Complex video reasoning remains a significant challenge for Multimodal Large Language Models (MLLMs), as current R1-based methodologies often prioritize text-centric reasoning derived from text-based and image-based developments. In video tasks, such strategies frequently underutilize rich visual information, leading to potential shortcut learning and increased susceptibility to hallucination. To foster a more robust, visual-centric video understanding, we start by introducing a novel self-supervised reinforcement learning GRPO algorithm (Pretext-GRPO) within the standard R1 pipeline, in which positive rewards are assigned for correctly solving pretext tasks on transformed visual inputs, which makes the model to non-trivially process the visual information. Building on the effectiveness of Pretext-GRPO, we further propose the ViSS-R1 framework, which streamlines and integrates pretext-task-based self-supervised learning directly into the MLLM's R1 post-training paradigm. Instead of relying solely on sparse visual cues, our framework compels models to reason about transformed visual input by simultaneously processing both pretext questions (concerning transformations) and true user queries. This necessitates identifying the applied transformation and reconstructing the original video to formulate accurate final answers. Comprehensive evaluations on six widely-used video reasoning and understanding benchmarks demonstrate the effectiveness and superiority of our Pretext-GRPO and ViSS-R1 for complex video reasoning. Our codes and models will be publicly available.

</details>


### [210] [Monocular 3D Lane Detection via Structure Uncertainty-Aware Network with Curve-Point Queries](https://arxiv.org/abs/2511.13055)
*Ruixin Liu,Zejian Yuan*

Main category: cs.CV

TL;DR: MonoUnc is a BEV-free 3D lane detector that explicitly models aleatoric uncertainty by representing 3D lanes as 3D Gaussians along segments, parameterized by local structure, guided by front-view curves; uses a 3D Gaussian matching loss and dynamic curve-point embeddings, achieving state-of-the-art results on ONCE-3DLanes and OpenLane and introducing two Chamfer-based evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Aleatoric uncertainty from observation noise in monocular 3D lane detection and limitations of prior methods relying on independent point predictions or global planar models; there is a need to capture local structural variations and uncertainty in real-world lanes.

Method: The approach projects 3D lanes to front-view and fits parametric curves; curve predictions guide dynamically generated curve-point query embeddings for 3D lane points; each segment between adjacent points is modeled as a 3D Gaussian parameterized by local structure and estimated uncertainty; introduces a 3D Gaussian matching loss to jointly constrain Gaussian parameters; the method is BEV-free.

Result: Empirical results on ONCE-3DLanes and OpenLane show MonoUnc outperforms previous SOTA methods under stricter evaluation criteria; the authors also propose two new evaluation metrics based on bidirectional Chamfer distances.

Conclusion: The paper introduces explicit aleatoric uncertainty modeling for 3D lane detection using local-structure-aware 3D Gaussians and a Gaussian matching loss, delivered in a BEV-free framework, with state-of-the-art results and new evaluation metrics; code is released.

Abstract: Monocular 3D lane detection is challenged by aleatoric uncertainty arising from inherent observation noise. Existing methods rely on simplified geometric assumptions, such as independent point predictions or global planar modeling, failing to capture structural variations and aleatoric uncertainty in real-world scenarios. In this paper, we propose MonoUnc, a bird's-eye view (BEV)-free 3D lane detector that explicitly models aleatoric uncertainty informed by local lane structures. Specifically, 3D lanes are projected onto the front-view (FV) space and approximated by parametric curves. Guided by curve predictions, curve-point query embeddings are dynamically generated for lane point predictions in 3D space. Each segment formed by two adjacent points is modeled as a 3D Gaussian, parameterized by the local structure and uncertainty estimations. Accordingly, a novel 3D Gaussian matching loss is designed to constrain these parameters jointly. Experiments on the ONCE-3DLanes and OpenLane datasets demonstrate that MonoUnc outperforms previous state-of-the-art (SoTA) methods across all benchmarks under stricter evaluation criteria. Additionally, we propose two comprehensive evaluation metrics for ONCE-3DLanes, calculating the average and maximum bidirectional Chamfer distances to quantify global and local errors. Codes are released at https://github.com/lrx02/MonoUnc.

</details>


### [211] [FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation](https://arxiv.org/abs/2511.13063)
*Zhenghua Li,Hang Chen,Zihao Sun,Kai Li,Xiaolin Hu*

Main category: cs.CV

TL;DR: SAM2-based natural-image priors can be transferred to EM neuron segmentation using a Feature-Guided Attention module and a dual-affinity decoder, achieving SOTA performance after EM-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Accurate EM neuron segmentation is hard due to complex morphologies, low SNR, and limited labels; leveraging large natural-image priors could improve generalization across domains.

Method: Extract features with SAM2; apply Feature-Guided Attention module to guide a lightweight Fine-Grained Encoder; use dual-affinity decoder to produce coarse and refined affinity maps; freeze SAM2 during initial evaluation, then fine-tune on EM data.

Result: Performance comparable to SOTA with SAM2 frozen; after fine-tuning on EM data, significantly outperforms existing SOTA methods.

Conclusion: Cross-domain transfer of natural-image representations with targeted, domain-adaptive guidance is effective for neuron segmentation in EM images.

Abstract: Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.

</details>


### [212] [RobustGait: Robustness Analysis for Appearance Based Gait Recognition](https://arxiv.org/abs/2511.13065)
*Reeshoon Sayera,Akash Kumar,Sirshapan Mitra,Prudvi Kamtam,Yogesh S Rawat*

Main category: cs.CV

TL;DR: RobustGait provides a fine-grained robustness benchmark for appearance-based gait recognition, evaluating RGB-level noise, silhouette extraction biases, and model architectures across diverse perturbations and datasets, and showing that noise-aware training and knowledge distillation improve deployment-ready robustness.


<details>
  <summary>Details</summary>
Motivation: There is a lack of systematic robustness evaluation for appearance-based gait recognition under real-world corruptions and silhouette variability, despite strong performance on controlled datasets.

Method: Introduce RobustGait framework with four perturbation dimensions (digital, environmental, temporal, occlusion), two silhouette extractors (segmentation and parsing networks), multiple model capacities, and deployment scenarios. Benchmark includes 15 corruption types at 5 severities across CASIA-B, CCPG, SUSTech1K with in-the-wild MEVID validation, evaluating six state-of-the-art gait systems. Analysis reveals propagation of RGB noise through silhouettes, silhouette extractor bias as a benchmark bias, perturbation-architecture interactions, and robustness improvements via noise-aware training and knowledge distillation.

Result: Findings show RGB-level noise better reflects real-world degradation and propagates through segmentation/parsing to gait models; silhouette extractor biases significantly affect performance, indicating a benchmark bias; robustness varies by perturbation type and architecture; strategies like noise-aware training and knowledge distillation yield improved robustness and closer to deployment-ready systems.

Conclusion: RobustGait enables fine-grained robustness assessment for appearance-based gait recognition, highlights the importance of RGB-level perturbations and silhouette extractor biases, and demonstrates practical strategies to enhance robustness for real-world deployment.

Abstract: Appearance-based gait recognition have achieved strong performance on controlled datasets, yet systematic evaluation of its robustness to real-world corruptions and silhouette variability remains lacking. We present RobustGait, a framework for fine-grained robustness evaluation of appearance-based gait recognition systems. RobustGait evaluation spans four dimensions: the type of perturbation (digital, environmental, temporal, occlusion), the silhouette extraction method (segmentation and parsing networks), the architectural capacities of gait recognition models, and various deployment scenarios. The benchmark introduces 15 corruption types at 5 severity levels across CASIA-B, CCPG, and SUSTech1K, with in-the-wild validation on MEVID, and evaluates six state-of-the-art gait systems. We came across several exciting insights. First, applying noise at the RGB level better reflects real-world degradation, and reveal how distortions propagate through silhouette extraction to the downstream gait recognition systems. Second, gait accuracy is highly sensitive to silhouette extractor biases, revealing an overlooked source of benchmark bias. Third, robustness is dependent on both the type of perturbation and the architectural design. Finally, we explore robustness-enhancing strategies, showing that noise-aware training and knowledge distillation improve performance and move toward deployment-ready systems.

</details>


### [213] [Decoupling Scene Perception and Ego Status: A Multi-Context Fusion Approach for Enhanced Generalization in End-to-End Autonomous Driving](https://arxiv.org/abs/2511.13079)
*Jiacheng Tang,Mingyue Feng,Jiachao Liu,Yaonong Wang,Jian Pu*

Main category: cs.CV

TL;DR: AdaptiveAD decouples ego and scene reasoning with a dual-branch architecture and adaptive fusion, achieving state-of-the-art open-loop planning on nuScenes and improved generalization.


<details>
  <summary>Details</summary>
Motivation: Root cause is premature fusion of ego status in BEV encoders, creating a shortcut that biases planning and harms generalization; an architectural decoupling is needed to enhance scene understanding and robustness.

Method: A dual-branch design: a scene-driven branch with ego status omitted from the BEV encoder and a separate ego-driven branch focusing on planning. A scene-aware fusion module adaptively integrates branch decisions. A path attention mechanism enables ego-BEV interaction. Two auxiliary tasks are BEV unidirectional distillation and autoregressive online mapping. Trained and evaluated on nuScenes for open-loop planning.

Result: Achieves state-of-the-art open-loop planning performance on nuScenes; significantly reduces reliance on ego status and demonstrates strong generalization across diverse scenarios.

Conclusion: AdaptiveAD provides an architectural-level solution that decouples ego status from scene perception and uses adaptive fusion to improve robustness and generalization in planning.

Abstract: Modular design of planning-oriented autonomous driving has markedly advanced end-to-end systems. However, existing architectures remain constrained by an over-reliance on ego status, hindering generalization and robust scene understanding. We identify the root cause as an inherent design within these architectures that allows ego status to be easily leveraged as a shortcut. Specifically, the premature fusion of ego status in the upstream BEV encoder allows an information flow from this strong prior to dominate the downstream planning module. To address this challenge, we propose AdaptiveAD, an architectural-level solution based on a multi-context fusion strategy. Its core is a dual-branch structure that explicitly decouples scene perception and ego status. One branch performs scene-driven reasoning based on multi-task learning, but with ego status deliberately omitted from the BEV encoder, while the other conducts ego-driven reasoning based solely on the planning task. A scene-aware fusion module then adaptively integrates the complementary decisions from the two branches to form the final planning trajectory. To ensure this decoupling does not compromise multi-task learning, we introduce a path attention mechanism for ego-BEV interaction and add two targeted auxiliary tasks: BEV unidirectional distillation and autoregressive online mapping. Extensive evaluations on the nuScenes dataset demonstrate that AdaptiveAD achieves state-of-the-art open-loop planning performance. Crucially, it significantly mitigates the over-reliance on ego status and exhibits impressive generalization capabilities across diverse scenarios.

</details>


### [214] [Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations](https://arxiv.org/abs/2511.13081)
*Yehonatan Elisha,Seffi Cohen,Oren Barkan,Noam Koenigstein*

Main category: cs.CV

TL;DR: Proposes RFxG taxonomy for saliency explanations along Reference-Frame (pointwise vs contrastive) and Granularity (fine- to coarse-grained). Finds existing metrics biased toward pointwise faithfulness; introduces four new faithfulness metrics; evaluates across 10 saliency methods, 4 architectures, 3 datasets; advocates user-intent-driven evaluation.


<details>
  <summary>Details</summary>
Motivation: There is no consensus on the purpose and evaluation of saliency maps; misalignment with user queries reduces usefulness; a principled framework is needed to organize explanations by user intent and granularity.

Method: Introduce RFxG taxonomy; analyze limitations of current evaluation metrics; propose four novel faithfulness metrics; conduct large-scale empirical evaluation across methods, architectures, datasets.

Result: Existing metrics largely favor pointwise faithfulness and neglect contrastive reasoning and semantic granularity; four new metrics enable assessment across RFxG dimensions; comprehensive evaluation across 10 methods, 4 architectures, 3 datasets.

Conclusion: Shift toward user-intent-driven evaluation; RFxG provides conceptual foundation and practical tools to develop explanations faithful to model behavior and aligned with human inquiry.

Abstract: Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods.We address this gap by introducing the Reference-Frame $\times$ Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise ("Why this prediction?") and contrastive ("Why this and not an alternative?") explanations.Granularity: Ranging from fine-grained class-level (e.g., "Why Husky?") to coarse-grained group-level (e.g., "Why Dog?") interpretations.Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets.By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.

</details>


### [215] [MergeSlide: Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images](https://arxiv.org/abs/2511.13099)
*Doanh C. Bui,Ba Hung Ngo,Hoai Luan Pham,Khang Nguyen,Maï K. Nguyen,Yasuhiko Nakashima*

Main category: cs.CV

TL;DR: A lifelong learning framework for WSIs that merges task-specific models into a unified model using a vision-language foundation; called MergeSlide; it uses class-aware prompts, an MLP-free backbone, and an orthogonal merging strategy; for CLASS-IL inference, uses TCP prompts to identify relevant task; demonstrates superior performance on six TCGA datasets; code available.


<details>
  <summary>Details</summary>
Motivation: Lifelong learning on gigabyte-scale WSIs is data- and compute-intensive; existing methods struggle with forgetting and require large rehearsal; leveraging a vision-language pathology foundation and a model-merging approach can enable efficient continual learning with prompt-style adaptation.

Method: Define each new task with class-aware prompts; fine-tune a lightweight, MLP-free backbone for a few epochs; merge task-specific models into a unified model via an orthogonal continual merging strategy to preserve knowledge and mitigate forgetting. For CLASS-IL, TCP inference identifies the most relevant task using task prompts and applies the corresponding class-aware prompts.

Result: On a stream of six TCGA datasets, MergeSlide outperforms rehearsal-based continual learning baselines and vision-language zero-shot baselines.

Conclusion: Treating lifelong learning on WSIs as model merging with prompt-based adaptation yields effective continual learning with reduced forgetting; the approach is practical for gigapixel data and can be extended to other domains. Code and data are available at the provided GitHub link.

Abstract: Lifelong learning on Whole Slide Images (WSIs) aims to train or fine-tune a unified model sequentially on cancer-related tasks, reducing the resources and effort required for data transfer and processing, especially given the gigabyte-scale size of WSIs. In this paper, we introduce MergeSlide, a simple yet effective framework that treats lifelong learning as a model merging problem by leveraging a vision-language pathology foundation model. When a new task arrives, it is: 1) defined with class-aware prompts, 2) fine-tuned for a few epochs using an MLP-free backbone, and 3) merged into a unified model using an orthogonal continual merging strategy that preserves performance and mitigates catastrophic forgetting. For inference under the class-incremental learning (CLASS-IL) setting, where task identity is unknown, we introduce Task-to-Class Prompt-aligned (TCP) inference. Specifically, TCP first identifies the most relevant task using task-level prompts and then applies the corresponding class-aware prompts to generate predictions. To evaluate MergeSlide, we conduct experiments on a stream of six TCGA datasets. The results show that MergeSlide outperforms both rehearsal-based continual learning and vision-language zero-shot baselines. Code and data are available at https://github.com/caodoanh2001/MergeSlide.

</details>


### [216] [CapeNext: Rethinking and refining dynamic support information for category-agnostic pose estimation](https://arxiv.org/abs/2511.13102)
*Yu Zhu,Dan Zeng,Shuiwang Li,Qijun Zhao,Qiaomu Shen,Bo Tang*

Main category: cs.CV

TL;DR: CapeNext introduces hierarchical cross-modal interaction and dual-stream feature refinement to CAPE, addressing cross-category polysemy and fine-grained intra-category discrimination, achieving large gains on MP-100 across backbones.


<details>
  <summary>Details</summary>
Motivation: Static joint embeddings in CAPE suffer from polysemy across categories and lack of discriminability for fine-grained intra-category variations; a more flexible, instance-aware description is needed.

Method: Proposes a framework with hierarchical cross-modal interaction and dual-stream feature refinement to enrich joint embeddings with class-level and instance-specific cues from textual descriptions and specific images.

Result: Capenext achieves large performance gains over state-of-the-art CAPE methods on MP-100, with performance gains consistent across different network backbones.

Conclusion: Demonstrates the effectiveness of combining class-level and instance-specific cues through hierarchical cross-modal interaction and dual-stream refinement to robustly match poses across diverse categories and intra-class variations.

Abstract: Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks. While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept "leg" exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat). To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images. Experiments on the MP-100 dataset demonstrate that, regardless of the network backbone, CapeNext consistently outperforms state-of-the-art CAPE methods by a large margin.

</details>


### [217] [PlugTrack: Multi-Perceptive Motion Analysis for Adaptive Fusion in Multi-Object Tracking](https://arxiv.org/abs/2511.13105)
*Seungjae Kim,SeungJoon Lee,MyeongAh Cho*

Main category: cs.CV

TL;DR: PlugTrack adaptively fuses Kalman-filter-based linear motion with data-driven predictors to handle mixed motion in MOT; uses multi-perceptive motion analysis to generate adaptive blending; achieves strong results on MOT17/20 and DanceTrack without altering predictors; first adaptive fusion framework bridging classical and modern motion prediction.


<details>
  <summary>Details</summary>
Motivation: Real-world MOT exhibits both linear and non-linear motion; KF is efficient but misses non-linear patterns; data-driven predictors capture non-linear dynamics but lack generalization and are costly. A hybrid approach leveraging complementarity can improve tracking while preserving efficiency.

Method: Proposes PlugTrack with multi-perceptive motion analysis to compute adaptive blending factors between KF and a data-driven predictor; integrates as a plug-in fusion module in MOT pipelines without modifying base predictors; learns to weigh predictions per object via motion cues.

Result: Demonstrates significant improvements on MOT17/MOT20 and achieves state-of-the-art on DanceTrack; empirical evidence that KF remains competitive in a substantial fraction of cases (up to 34%), validating a hybrid approach.

Conclusion: PlugTrack is the first framework to adaptively fuse classical and modern motion predictors in MOT, highlighting the value of combining linear and non-linear motion modeling.

Abstract: Multi-object tracking (MOT) predominantly follows the tracking-by-detection paradigm, where Kalman filters serve as the standard motion predictor due to computational efficiency but inherently fail on non-linear motion patterns. Conversely, recent data-driven motion predictors capture complex non-linear dynamics but suffer from limited domain generalization and computational overhead. Through extensive analysis, we reveal that even in datasets dominated by non-linear motion, Kalman filter outperforms data-driven predictors in up to 34\% of cases, demonstrating that real-world tracking scenarios inherently involve both linear and non-linear patterns. To leverage this complementarity, we propose PlugTrack, a novel framework that adaptively fuses Kalman filter and data-driven motion predictors through multi-perceptive motion understanding. Our approach employs multi-perceptive motion analysis to generate adaptive blending factors. PlugTrack achieves significant performance gains on MOT17/MOT20 and state-of-the-art on DanceTrack without modifying existing motion predictors. To the best of our knowledge, PlugTrack is the first framework to bridge classical and modern motion prediction paradigms through adaptive fusion in MOT.

</details>


### [218] [Low-Level Dataset Distillation for Medical Image Enhancement](https://arxiv.org/abs/2511.13106)
*Fengzhi Xu,Ziyuan Yang,Mengyu Sun,Joey Tianyi Zhou,Yi Zhang*

Main category: cs.CV

TL;DR: First low-level dataset distillation approach for medical image enhancement that uses a shared anatomical prior to initialize distilled data, then personalizes it via a Structure-Preserving Personalized Generation (SPG) module, and aligns gradients with patient data to preserve fidelity and privacy. Addresses the underdetermined pixel-level mappings in low-level DD.


<details>
  <summary>Details</summary>
Motivation: Large-scale pixel-level mappings in medical image enhancement are costly in terms of data and storage. Existing dataset distillation (DD) methods mainly tackle high-level tasks with many samples sharing the same label, which is not suitable for low-level tasks that require pixel-level fidelity and are underdetermined. Privacy concerns further motivate learning on distilled data rather than raw patient data.

Method: Construct a shared anatomical prior from a representative patient to initialize distilled data across patients. Personalize this prior using a Structure-Preserving Personalized Generation (SPG) module that injects patient-specific anatomical information while preserving pixel fidelity. For different low-level tasks, create task-specific high- and low-quality training pairs from the distilled data. Align gradients from networks trained on distilled pairs with gradients from the corresponding patient’s raw data to inject patient knowledge, without exposing raw data. Only the distilled dataset, containing abstract training information, is shared.

Result: Proposes a novel pipeline for low-level medical image enhancement via dataset distillation. Introduces a representative-patient initialized prior, SPG-based personalization, and gradient-alignment-based patient knowledge injection. Emphasizes privacy by sharing only distilled data rather than raw patient data; abstract-level results indicate feasibility but no quantitative empirical results are provided in the abstract.

Conclusion: A privacy-preserving, patient-specific low-level dataset distillation framework for medical image enhancement is introduced, addressing the underdetermined nature of pixel-level mappings and enabling practical deployment through distilled data sharing.

Abstract: Medical image enhancement is clinically valuable, but existing methods require large-scale datasets to learn complex pixel-level mappings. However, the substantial training and storage costs associated with these datasets hinder their practical deployment. While dataset distillation (DD) can alleviate these burdens, existing methods mainly target high-level tasks, where multiple samples share the same label. This many-to-one mapping allows distilled data to capture shared semantics and achieve information compression. In contrast, low-level tasks involve a many-to-many mapping that requires pixel-level fidelity, making low-level DD an underdetermined problem, as a small distilled dataset cannot fully constrain the dense pixel-level mappings. To address this, we propose the first low-level DD method for medical image enhancement. We first leverage anatomical similarities across patients to construct the shared anatomical prior based on a representative patient, which serves as the initialization for the distilled data of different patients. This prior is then personalized for each patient using a Structure-Preserving Personalized Generation (SPG) module, which integrates patient-specific anatomical information into the distilled dataset while preserving pixel-level fidelity. For different low-level tasks, the distilled data is used to construct task-specific high- and low-quality training pairs. Patient-specific knowledge is injected into the distilled data by aligning the gradients computed from networks trained on the distilled pairs with those from the corresponding patient's raw data. Notably, downstream users cannot access raw patient data. Instead, only a distilled dataset containing abstract training information is shared, which excludes patient-specific details and thus preserves privacy.

</details>


### [219] [DGS-Net: Distillation-Guided Gradient Surgery for CLIP Fine-Tuning in AI-Generated Image Detection](https://arxiv.org/abs/2511.13108)
*Jiazhen Yan,Ziqiang Li,Fan Wang,Boyu Wang,Zhangjie Fu*

Main category: cs.CV

TL;DR: DGS-Net is a gradient-space, distillation-guided framework for AI-generated image detection that preserves CLIP priors while suppressing task-irrelevant components via gradient projection, achieving higher accuracy and generalization across 50 generative models.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning large multimodal detectors (e.g., CLIP) causes catastrophic forgetting, eroding pre-trained priors and harming cross-domain generalization for synthetic-content detection; a method is needed to retain priors while suppressing harmful directions.

Method: Decompose optimization gradients into harmful and beneficial directions; project task gradients onto the orthogonal complement of harmful directions and align with beneficial directions distilled from a frozen CLIP encoder (distillation-guided gradient surgery); unified objective for prior preservation and irrelevance suppression.

Result: Evaluated on 50 generative models; outperforms state-of-the-art by an average margin of 6.6; achieves superior detection performance and cross-technique generalization.

Conclusion: DGS-Net provides a unified optimization framework that preserves transferable priors while suppressing irrelevant components, improving synthetic-content detection across diverse generative models.

Abstract: The rapid progress of generative models such as GANs and diffusion models has led to the widespread proliferation of AI-generated images, raising concerns about misinformation, privacy violations, and trust erosion in digital media. Although large-scale multimodal models like CLIP offer strong transferable representations for detecting synthetic content, fine-tuning them often induces catastrophic forgetting, which degrades pre-trained priors and limits cross-domain generalization. To address this issue, we propose the Distillation-guided Gradient Surgery Network (DGS-Net), a novel framework that preserves transferable pre-trained priors while suppressing task-irrelevant components. Specifically, we introduce a gradient-space decomposition that separates harmful and beneficial descent directions during optimization. By projecting task gradients onto the orthogonal complement of harmful directions and aligning with beneficial ones distilled from a frozen CLIP encoder, DGS-Net achieves unified optimization of prior preservation and irrelevant suppression. Extensive experiments on 50 generative models demonstrate that our method outperforms state-of-the-art approaches by an average margin of 6.6, achieving superior detection performance and generalization across diverse generation techniques.

</details>


### [220] [Learning Implicit Neural Degradation Representation for Unpaired Image Dehazing](https://arxiv.org/abs/2511.13110)
*Shuaibin Fan,Senming Zhong,Wenchao Yan,Minglong Xue*

Main category: cs.CV

TL;DR: Unsupervised dehazing via implicit neural degradation representation; leverages Kolmogorov–Arnold inspired mechanism to combine channel-independent and channel-dependent learning, modeling haze as a continuous function with a dense residual module; achieves competitive results and releases code.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between fine-grained feature learning for inhomogeneous haze and global consistency, and to learn a general haze representation without explicit physical models or supervised labels.

Method: Introduce a Kolmogorov–Arnold inspired mechanism that blends channel-independent and channel-dependent processes; construct an implicit neural representation that models haze degradation as a continuous function; add a dense residual enhancement module to refine the latent representation and suppress redundancy.

Result: Experimental results show competitive dehazing performance on public and real-world datasets.

Conclusion: The proposed unsupervised implicit haze representation enables high-quality restoration and reduces dependence on explicit feature extraction or physical haze models; code will be released.

Abstract: Image dehazing is an important task in the field of computer vision, aiming at restoring clear and detail-rich visual content from haze-affected images. However, when dealing with complex scenes, existing methods often struggle to strike a balance between fine-grained feature representation of inhomogeneous haze distribution and global consistency modeling. Furthermore, to better learn the common degenerate representation of haze in spatial variations, we propose an unsupervised dehaze method for implicit neural degradation representation. Firstly, inspired by the Kolmogorov-Arnold representation theorem, we propose a mechanism combining the channel-independent and channel-dependent mechanisms, which efficiently enhances the ability to learn from nonlinear dependencies. which in turn achieves good visual perception in complex scenes. Moreover, we design an implicit neural representation to model haze degradation as a continuous function to eliminate redundant information and the dependence on explicit feature extraction and physical models. To further learn the implicit representation of the haze features, we also designed a dense residual enhancement module from it to eliminate redundant information. This achieves high-quality image restoration. Experimental results show that our method achieves competitive dehaze performance on various public and real-world datasets. This project code will be available at https://github.com/Fan-pixel/NeDR-Dehaze.

</details>


### [221] [Semantics and Content Matter: Towards Multi-Prior Hierarchical Mamba for Image Deraining](https://arxiv.org/abs/2511.13113)
*Zhaocheng Yu,Kui Jiang,Junjun Jiang,Xianming Liu,Guanglu Sun,Yi Xiao*

Main category: cs.CV

TL;DR: MPHM introduces a multi-prior framework for image deraining that fuses macro semantic priors from CLIP and micro structural priors from DINOv2 via progressive priors fusion, wrapped in a Hierarchical Mamba Module with a Fourier-enhanced dual-path, achieving state-of-the-art PSNR (0.57 dB gain on Rain200H) and strong real-world generalization.


<details>
  <summary>Details</summary>
Motivation: Current deraining methods often degrade fidelity of semantic and spatial details. Integrating high-level semantic guidance with scene-aware structural priors, while managing potential conflicts between heterogeneous priors, can improve restoration quality and generalization.

Method: Propose MPHM: a backbone with Hierarchical Mamba Module (HMM) featuring a Fourier-enhanced dual-path design for global context and local detail. It injects macro-semantic priors from CLIP and micro-structural priors from DINOv2 into decoder stages via a progressive Priors Fusion Injection (PFI) to mitigate conflicts between priors.

Result: Empirical results show state-of-the-art performance with a 0.57 dB PSNR gain on Rain200H and improved generalization on real-world rainy scenes.

Conclusion: MPHM effectively leverages heterogeneous priors and a hierarchical, Fourier-enhanced feature extractor to deliver high-fidelity deraining and robust generalization.

Abstract: Rain significantly degrades the performance of computer vision systems, particularly in applications like autonomous driving and video surveillance. While existing deraining methods have made considerable progress, they often struggle with fidelity of semantic and spatial details. To address these limitations, we propose the Multi-Prior Hierarchical Mamba (MPHM) network for image deraining. This novel architecture synergistically integrates macro-semantic textual priors (CLIP) for task-level semantic guidance and micro-structural visual priors (DINOv2) for scene-aware structural information. To alleviate potential conflicts between heterogeneous priors, we devise a progressive Priors Fusion Injection (PFI) that strategically injects complementary cues at different decoder levels. Meanwhile, we equip the backbone network with an elaborate Hierarchical Mamba Module (HMM) to facilitate robust feature representation, featuring a Fourier-enhanced dual-path design that concurrently addresses global context modeling and local detail recovery. Comprehensive experiments demonstrate MPHM's state-of-the-art performance, achieving a 0.57 dB PSNR gain on the Rain200H dataset while delivering superior generalization on real-world rainy scenarios.

</details>


### [222] [A Lightweight 3D Anomaly Detection Method with Rotationally Invariant Features](https://arxiv.org/abs/2511.13115)
*Hanzhe Liang,Jie Zhou,Can Gao,Bingyang Guo,Jinbao Wang,Linlin Shen*

Main category: cs.CV

TL;DR: A Rotationally Invariant Features (RIF) framework for 3D anomaly detection on point clouds, combining PCM for rotationally invariant point mapping and a lightweight CTF-Net to learn robust features, aided by transfer learning through 3D data augmentation; achieves substantial improvements on Anomaly-ShapeNet and Real3D-AD datasets and shows strong generalization potential for industry.


<details>
  <summary>Details</summary>
Motivation: 3D anomaly detection on point clouds is sensitive to orientation and position changes, causing inconsistent features. There is a need for robust, rotationally invariant representations and efficient networks to improve detection performance and generalization, with practical industrial applicability.

Method: Develop Point Coordinate Mapping (PCM) to map each point into a rotationally invariant space. Introduce a lightweight Convolutional Transform Feature Network (CTF-Net) to extract rotationally invariant features for the memory bank. Employ transfer learning by pre-training the feature extractor with 3D data augmentation to enhance its capability.

Result: On Anomaly-ShapeNet, achieving an average P-AUROC improvement of 17.7%. On Real3D-AD, achieving an average P-AUROC improvement of 1.6%. Additionally, the approach demonstrates strong generalization by combining with traditional feature extraction methods for anomaly detection, indicating potential for industrial applications.

Conclusion: The Rotationally Invariant Features framework effectively addresses orientation and position variation in 3D anomaly detection, delivering strong performance gains and robust generalization, with clear promise for industrial deployment.

Abstract: 3D anomaly detection (AD) is a crucial task in computer vision, aiming to identify anomalous points or regions from point cloud data. However, existing methods may encounter challenges when handling point clouds with changes in orientation and position because the resulting features may vary significantly. To address this problem, we propose a novel Rotationally Invariant Features (RIF) framework for 3D AD. Firstly, to remove the adverse effect of variations on point cloud data, we develop a Point Coordinate Mapping (PCM) technique, which maps each point into a rotationally invariant space to maintain consistency of representation. Then, to learn robust and discriminative features, we design a lightweight Convolutional Transform Feature Network (CTF-Net) to extract rotationally invariant features for the memory bank. To improve the ability of the feature extractor, we introduce the idea of transfer learning to pre-train the feature extractor with 3D data augmentation. Experimental results show that the proposed method achieves the advanced performance on the Anomaly-ShapeNet dataset, with an average P-AUROC improvement of 17.7\%, and also gains the best performance on the Real3D-AD dataset, with an average P-AUROC improvement of 1.6\%. The strong generalization ability of RIF has been verified by combining it with traditional feature extraction methods on anomaly detection tasks, demonstrating great potential for industrial applications.

</details>


### [223] [CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model](https://arxiv.org/abs/2511.13121)
*Yuqi Zhang,Guanying Chen,Jiaxing Chen,Chuanyu Fu,Chuan Huang,Shuguang Cui*

Main category: cs.CV

TL;DR: Proposes CloseUpShot, a diffusion-based framework for close-up novel view synthesis from sparse inputs, using point-conditioned video diffusion with hierarchical warping, occlusion-aware noise suppression, and global structure guidance from a fused point cloud to improve detail and geometry.


<details>
  <summary>Details</summary>
Motivation: Reconstructing 3D scenes and synthesizing novel views from sparse inputs is challenging, especially in close-up views where input information is limited. Diffusion models offer strong temporal reasoning, but existing methods struggle with fine-grained details due to severe sparsity and background leakage in conditioning.

Method: Introduce CloseUpShot: a diffusion-based framework with (i) hierarchical warping and occlusion-aware noise suppression to improve conditioning images; (ii) global structure guidance using a dense fused point cloud to provide consistent geometric context to the diffusion process; (iii) processing via point-conditioned video diffusion.

Result: Extensive experiments on multiple datasets show the method outperforms existing approaches, particularly for close-up novel view synthesis.

Conclusion: The approach effectively addresses the sparsity and lack of globally consistent 3D constraints in sparse conditioning inputs, improving close-up novel view synthesis and validating the benefits of hierarchical warping, occlusion-aware noise suppression, and global structure guidance.

Abstract: Reconstructing 3D scenes and synthesizing novel views from sparse input views is a highly challenging task. Recent advances in video diffusion models have demonstrated strong temporal reasoning capabilities, making them a promising tool for enhancing reconstruction quality under sparse-view settings. However, existing approaches are primarily designed for modest viewpoint variations, which struggle in capturing fine-grained details in close-up scenarios since input information is severely limited. In this paper, we present a diffusion-based framework, called CloseUpShot, for close-up novel view synthesis from sparse inputs via point-conditioned video diffusion. Specifically, we observe that pixel-warping conditioning suffers from severe sparsity and background leakage in close-up settings. To address this, we propose hierarchical warping and occlusion-aware noise suppression, enhancing the quality and completeness of the conditioning images for the video diffusion model. Furthermore, we introduce global structure guidance, which leverages a dense fused point cloud to provide consistent geometric context to the diffusion process, to compensate for the lack of globally consistent 3D constraints in sparse conditioning inputs. Extensive experiments on multiple datasets demonstrate that our method outperforms existing approaches, especially in close-up novel view synthesis, clearly validating the effectiveness of our design.

</details>


### [224] [Region-Point Joint Representation for Effective Trajectory Similarity Learning](https://arxiv.org/abs/2511.13125)
*Hao Long,Silin Zhou,Lisi Chen,Shuo Shang*

Main category: cs.CV

TL;DR: RePo jointly encodes region-wise and point-wise trajectory features to improve similarity modeling, achieving significant gains over baselines.


<details>
  <summary>Details</summary>
Motivation: SOTA trajectory similarity methods underutilize the full spectrum of trajectory information. They miss spatial context and fine-grained movement patterns.

Method: Region-wise: map GPS trajectories to grid sequences and extract spatial context via structural features and semantic features from visual cues. Point-wise: employ three lightweight expert networks to capture local, correlation, and continuous movement patterns from dense GPS data. A router network adaptively fuses point-wise features, which are then combined with region-wise features via cross-attention to form the final embedding. Training uses a contrastive loss with hard negatives for similarity ranking.

Result: Average accuracy improvement of 22.2% over SOTA baselines across evaluation metrics.

Conclusion: RePo effectively integrates region-wise and point-wise representations to enhance trajectory similarity modeling.

Abstract: Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \textbf{RePo}, a novel method that jointly encodes \textbf{Re}gion-wise and \textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\% over SOTA baselines across all evaluation metrics.

</details>


### [225] [VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language](https://arxiv.org/abs/2511.13127)
*Zonghao Ying,Moyang Chen,Nizhang Li,Zhiqiang Wang,Wenxin Zhang,Quanchen Zou,Zonglei Jing,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TL;DR: A new cross-modal jailbreak framework (VEIL) for text-to-video models uses benign-looking prompts composed of neutral scene anchors, latent auditory triggers, and stylistic modulators to induce semantically unsafe videos that bypass safety filters. Evaluated on seven T2V models, it yields a 23% higher average attack success rate on commercial models than prior perturbation-based methods.


<details>
  <summary>Details</summary>
Motivation: Safety guardrails in text-to-video models are vulnerable to stealthy, appearance-preserving prompts. Prior attacks relied on obvious adversarial perturbations and were easy to detect; the paper targets covert, semantically unsafe outputs by exploiting cross-modal associations to reveal latent model blind spots.

Method: VEIL integrates three modular prompt components: (1) neutral scene anchors that preserve plausibility by mirroring the blocked intent's surface description; (2) latent auditory triggers—innocuous-sounding audio-event descriptions that leverage audio-visual co-occurrence priors to bias visuals toward unsafe concepts; (3) stylistic modulators—cinematic directives that amplify and stabilize the trigger’s effect. Attack generation is formalized as a constrained optimization over the prompt space and solved via guided search to balance stealth and effectiveness.

Result: Extensive experiments across seven T2V models show the proposed attack achieves a 23% improvement in average attack success rate on commercial models, demonstrating the practical potency of VEIL.

Conclusion: The work reveals substantial cross-modal vulnerabilities in T2V models, highlighting the need for defense strategies that account for implicit priors and cross-modal cues, and motivating future research on robust safety mechanisms against stealthy, prompt-based jailbreaks.

Abstract: Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models.

</details>


### [226] [Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack](https://arxiv.org/abs/2511.13132)
*Chenyang Li,Wenbing Tang,Yihao Huang,Sinong Simon Zhan,Ming Hu,Xiaojun Jia,Yang Liu*

Main category: cs.CV

TL;DR: ILA reveals VLN agents' vulnerability to realistic indoor lighting changes by manipulating global illumination in static or dynamic modes, degrading navigation performance.


<details>
  <summary>Details</summary>
Motivation: Assess robustness of Vision-and-Language Navigation (VLN) agents under realistic, everyday lighting variations rather than synthetic or unusual perturbations.

Method: A black-box adversarial framework that alters global illumination. Two modes: Static Indoor Lighting-based Attack (SILA) with constant lighting, and Dynamic Indoor Lighting-based Attack (DILA) with on/off switching at critical moments. Evaluated on two state-of-the-art VLN models across three navigation tasks.

Result: ILA significantly increases failure rates and reduces trajectory efficiency, exposing vulnerabilities of VLN agents to realistic indoor lighting variations.

Conclusion: VLN agents are susceptible to everyday lighting changes; there is a need for robustness and defenses against lighting-based perturbations in navigation systems.

Abstract: Vision-and-Language Navigation (VLN) agents have made remarkable progress, but their robustness remains insufficiently studied. Existing adversarial evaluations often rely on perturbations that manifest as unusual textures rarely encountered in everyday indoor environments. Errors under such contrived conditions have limited practical relevance, as real-world agents are unlikely to encounter such artificial patterns. In this work, we focus on indoor lighting, an intrinsic yet largely overlooked scene attribute that strongly influences navigation. We propose Indoor Lighting-based Adversarial Attack (ILA), a black-box framework that manipulates global illumination to disrupt VLN agents. Motivated by typical household lighting usage, we design two attack modes: Static Indoor Lighting-based Attack (SILA), where the lighting intensity remains constant throughout an episode, and Dynamic Indoor Lighting-based Attack (DILA), where lights are switched on or off at critical moments to induce abrupt illumination changes. We evaluate ILA on two state-of-the-art VLN models across three navigation tasks. Results show that ILA significantly increases failure rates while reducing trajectory efficiency, revealing previously unrecognized vulnerabilities of VLN agents to realistic indoor lighting variations.

</details>


### [227] [MedGEN-Bench: Contextually entangled benchmark for open-ended multimodal medical generation](https://arxiv.org/abs/2511.13135)
*Junjie Yang,Yuhao Yan,Gang Wu,Yuxuan Wang,Ruoyu Liang,Xinjie Jiang,Xiang Wan,Fenglei Fan,Yongquan Zhang,Feiwei Qin,Changmiao Wan*

Main category: cs.CV

TL;DR: MedGEN-Bench offers a comprehensive multimodal medical benchmark for vision-language systems, focusing on open-ended generation and cross-modal reasoning, with 6,422 expert-validated image-text pairs across six imaging modalities, 16 tasks, and 28 subtasks; evaluated via VQA, editing, and contextual generation formats using a three-tier assessment.


<details>
  <summary>Details</summary>
Motivation: Current medical VLM benchmarks rely on ambiguous queries, simplistic reasoning, and text-centric evaluation, failing to assess image generation and clinical relevance; there is a need for a benchmark that couples image editing and contextual generation with clinical reasoning.

Method: Assemble MedGEN-Bench with 6,422 expert-validated image-text pairs across six imaging modalities, 16 tasks, 28 subtasks; structured into Visual Question Answering, Image Editing, and Contextual Multimodal Generation; employ a three-tier assessment (pixel-level metrics, semantic text analysis, expert clinical relevance scoring); evaluate 10 compositional frameworks, 3 unified models, and 5 VLMs.

Result: The paper presents systematic benchmarking results across the selected models, illustrating the benchmark's ability to reveal gaps in cross-modal medical generation and reasoning; specific performance details are not included in the abstract.

Conclusion: MedGEN-Bench advances medical AI research by offering a challenging, context-rich multimodal benchmark and a holistic evaluation framework that emphasizes open-ended generation and clinical relevance.

Abstract: As Vision-Language Models (VLMs) increasingly gain traction in medical applications, clinicians are progressively expecting AI systems not only to generate textual diagnoses but also to produce corresponding medical images that integrate seamlessly into authentic clinical workflows. Despite the growing interest, existing medical visual benchmarks present notable limitations. They often rely on ambiguous queries that lack sufficient relevance to image content, oversimplify complex diagnostic reasoning into closed-ended shortcuts, and adopt a text-centric evaluation paradigm that overlooks the importance of image generation capabilities. To address these challenges, we introduce \textsc{MedGEN-Bench}, a comprehensive multimodal benchmark designed to advance medical AI research. MedGEN-Bench comprises 6,422 expert-validated image-text pairs spanning six imaging modalities, 16 clinical tasks, and 28 subtasks. It is structured into three distinct formats: Visual Question Answering, Image Editing, and Contextual Multimodal Generation. What sets MedGEN-Bench apart is its focus on contextually intertwined instructions that necessitate sophisticated cross-modal reasoning and open-ended generative outputs, moving beyond the constraints of multiple-choice formats. To evaluate the performance of existing systems, we employ a novel three-tier assessment framework that integrates pixel-level metrics, semantic text analysis, and expert-guided clinical relevance scoring. Using this framework, we systematically assess 10 compositional frameworks, 3 unified models, and 5 VLMs.

</details>


### [228] [WinMamba: Multi-Scale Shifted Windows in State Space Model for 3D Object Detection](https://arxiv.org/abs/2511.13138)
*Longhui Zheng,Qiming Xia,Xiaolu Chen,Zhaoliang Liu,Chenglu Wen*

Main category: cs.CV

TL;DR: WinMamba introduces a 3D feature-encoding backbone built from stacked WinMamba blocks that use a window-scale-adaptive module, learnable positional encoding, and a window-shift strategy to capture long-range context efficiently in a linear-state-space framework; it outperforms baselines on KITTI and Waymo, with code to be released.


<details>
  <summary>Details</summary>
Motivation: Balancing computational efficiency with the ability to capture long-range spatial dependencies in 3D detection. Axis-aligned fixed-window scanning discards spatial information, while traditional methods incur high cost. Mamba-based models leveraging linear state-space offer efficient long-range modeling, motivating an enhanced backbone that handles multi-scale representations.

Method: Propose WinMamba, a backbone composed of stacked WinMamba blocks. Each block incorporates a window-scale-adaptive (WSF) module that compensates voxel features across varying resolutions during sampling, along with a learnable positional encoding and a window-shift strategy to harvest contextual cues within the linear state space. Ablation studies indicate the distinct contributions of the WSF and AWF modules to detection accuracy.

Result: Empirical results on KITTI and Waymo show that WinMamba significantly outperforms the baseline. Ablation experiments validate the improvements attributed to the WSF and AWF modules.

Conclusion: The WinMamba backbone advances the efficiency-accuracy trade-off for 3D object detection by integrating scale-adaptive and context-enhancing components within a linear-state-space framework; code will be publicly released.

Abstract: 3D object detection is critical for autonomous driving, yet it remains fundamentally challenging to simultaneously maximize computational efficiency and capture long-range spatial dependencies. We observed that Mamba-based models, with their linear state-space design, capture long-range dependencies at lower cost, offering a promising balance between efficiency and accuracy. However, existing methods rely on axis-aligned scanning within a fixed window, inevitably discarding spatial information. To address this problem, we propose WinMamba, a novel Mamba-based 3D feature-encoding backbone composed of stacked WinMamba blocks. To enhance the backbone with robust multi-scale representation, the WinMamba block incorporates a window-scale-adaptive module that compensates voxel features across varying resolutions during sampling. Meanwhile, to obtain rich contextual cues within the linear state space, we equip the WinMamba layer with a learnable positional encoding and a window-shift strategy. Extensive experiments on the KITTI and Waymo datasets demonstrate that WinMamba significantly outperforms the baseline. Ablation studies further validate the individual contributions of the WSF and AWF modules in improving detection accuracy. The code will be made publicly available.

</details>


### [229] [Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks](https://arxiv.org/abs/2511.13145)
*Cesar Portocarrero Rodriguez,Laura Vandeweyen,Yosuke Yamamoto*

Main category: cs.CV

TL;DR: GAN-synthesized data improves road-distress segmentation; transformer-based MaskFormer outperforms a CNN in mAP50 and IoU.


<details>
  <summary>Details</summary>
Motivation: ASCE-grade infrastructure indicates a pressing need for efficient, scalable road monitoring. Leveraging real-time autonomous-vehicle visual data with CV methods could replace costly manual or laser inspections and accelerate rehabilitation planning.

Method: 1) Generate synthetic road-distress data using GANs for training data augmentation; 2) train CNN-based road-distress segmentation models; 3) evaluate a transformer-based MaskFormer model; 4) compare CNN vs MaskFormer on mAP50 and IoU.

Result: GAN-augmented training improves model performance; MaskFormer achieves higher mAP50 and IoU than the CNN baseline.

Conclusion: CV approaches with GAN-synthesized data and transformer-based models can enhance road distress segmentation, with MaskFormer outperforming CNNs, suggesting a viable path for more efficient infrastructure monitoring and rehabilitation planning.

Abstract: The American Society of Civil Engineers has graded Americas infrastructure condition as a C, with the road system receiving a dismal D. Roads are vital to regional economic viability, yet their management, maintenance, and repair processes remain inefficient, relying on outdated manual or laser-based inspection methods that are both costly and time-consuming. With the increasing availability of real-time visual data from autonomous vehicles, there is an opportunity to apply computer vision (CV) methods for advanced road monitoring, providing insights to guide infrastructure rehabilitation efforts. This project explores the use of state-of-the-art CV techniques for road distress segmentation. It begins by evaluating synthetic data generated with Generative Adversarial Networks (GANs) to assess its usefulness for model training. The study then applies Convolutional Neural Networks (CNNs) for road distress segmentation and subsequently examines the transformer-based model MaskFormer. Results show that GAN-generated data improves model performance and that MaskFormer outperforms the CNN model in two metrics: mAP50 and IoU.

</details>


### [230] [Skeletons Speak Louder than Text: A Motion-Aware Pretraining Paradigm for Video-Based Person Re-Identification](https://arxiv.org/abs/2511.13150)
*Rifen Lin,Alex Jinpeng Wang,Jiawei Mo,Min Li*

Main category: cs.CV

TL;DR: CSIP-ReID introduces a skeleton-driven pretraining framework for video ReID, with a two-stage contrastive skeleton–image pretraining, a dynamic Prototype Fusion Updater, and a Skeleton Guided Temporal Modeling module. It achieves state-of-the-art results on standard video ReID benchmarks and generalizes to skeleton-only ReID, representing an annotation-free, motion-aware multimodal pretraining direction.


<details>
  <summary>Details</summary>
Motivation: Current multimodal pretraining for video ReID relies on video-text data and does not capture fine-grained temporal motion. Skeleton provides explicit spatiotemporal cues and can enable genuine multimodal pretraining and better temporal modeling for ReID.

Method: Two-stage approach: (1) Contrastive learning to align skeleton and visual features at the sequence level. (2) Dynamic Prototype Fusion Updater (PFU) to refine multimodal identity prototypes by fusing motion and appearance cues. A Skeleton Guided Temporal Modeling (SGTM) module distills temporal cues from skeleton data into visual features.

Result: CSIP-ReID achieves state-of-the-art results on video ReID benchmarks (MARS, LS-VID, iLIDS-VID) and generalizes well to skeleton-only ReID tasks (BIWI, IAS). It is presented as annotation-free and motion-aware pretraining that opens a new direction in multimodal representation learning.

Conclusion: The work pioneers skeleton-driven pretraining for ReID, highlighting the value of motion-aware, annotation-free multimodal representations and suggesting broader applicability to multimodal ReID tasks and future temporal modeling enhancements.

Abstract: Multimodal pretraining has revolutionized visual understanding, but its impact on video-based person re-identification (ReID) remains underexplored. Existing approaches often rely on video-text pairs, yet suffer from two fundamental limitations: (1) lack of genuine multimodal pretraining, and (2) text poorly captures fine-grained temporal motion-an essential cue for distinguishing identities in video. In this work, we take a bold departure from text-based paradigms by introducing the first skeleton-driven pretraining framework for ReID. To achieve this, we propose Contrastive Skeleton-Image Pretraining for ReID (CSIP-ReID), a novel two-stage method that leverages skeleton sequences as a spatiotemporally informative modality aligned with video frames. In the first stage, we employ contrastive learning to align skeleton and visual features at sequence level. In the second stage, we introduce a dynamic Prototype Fusion Updater (PFU) to refine multimodal identity prototypes, fusing motion and appearance cues. Moreover, we propose a Skeleton Guided Temporal Modeling (SGTM) module that distills temporal cues from skeleton data and integrates them into visual features. Extensive experiments demonstrate that CSIP-ReID achieves new state-of-the-art results on standard video ReID benchmarks (MARS, LS-VID, iLIDS-VID). Moreover, it exhibits strong generalization to skeleton-only ReID tasks (BIWI, IAS), significantly outperforming previous methods. CSIP-ReID pioneers an annotation-free and motion-aware pretraining paradigm for ReID, opening a new frontier in multimodal representation learning.

</details>


### [231] [SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration](https://arxiv.org/abs/2511.13168)
*Haodong Wang,Tao Zhuo,Xiuwei Zhang,Hanlin Yin,Wencong Wu,Yanning Zhang*

Main category: cs.CV

TL;DR: A dense SAR-Optical registration framework (SOMA) that uses gradient priors in deep features and a coarse-to-fine affine-flow matcher to improve pixel-level alignment.


<details>
  <summary>Details</summary>
Motivation: Pixel-level SAR-Optical registration is challenging due to modality gap; traditional gradient cues have been underutilized in deep learning for this task.

Method: Introduce Feature Gradient Enhancer (FGE) to inject multi-scale, multi-directional gradient filters into the feature space via attention and reconstruction; introduce Global-Local Affine-Flow Matcher (GLAM) that fuses affine transformation with flow-based refinement in a coarse-to-fine pipeline.

Result: Significant gains in registration precision: CMR@1px up by 12.29% on SEN1-2 and 18.50% on GFGE_SO; robust generalization across scenes and resolutions.

Conclusion: SOMA effectively leverages gradient priors to strengthen deep representations for SAR-Optical registration, delivering accurate and robust pixel-level alignment.

Abstract: Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.

</details>


### [232] [THIR: Topological Histopathological Image Retrieval](https://arxiv.org/abs/2511.13170)
*Zahra Tabatabaei,Jon Sporring*

Main category: cs.CV

TL;DR: Training-free CBMIR for histopathology using persistent homology (Betti numbers) to retrieve similar images; THIR shows strong performance on BreaKHis and runs on CPU in under 20 minutes.


<details>
  <summary>Details</summary>
Motivation: Breast cancer causes significant mortality globally; early diagnosis and accurate clinical decision making are critical. Deep learning methods require large labeled datasets and GPU resources, which limits adoption; a training-free, unsupervised, scalable retrieval method would aid clinical workflows.

Method: THIR framework: uses topological data analysis, specifically Betti numbers derived from cubical persistence, to extract topological fingerprints from RGB histopathology images. Retrieves similar images by computing distances between these descriptors, returning top-K matches.

Result: Experiments on the BreaKHis dataset show THIR outperforms state-of-the-art supervised and unsupervised methods and processes the entire dataset in under 20 minutes on a standard CPU, offering a fast, scalable, training-free solution for clinical image retrieval.

Conclusion: Topological approaches can enable effective, training-free CBMIR for histopathology, providing competitive performance with practical CPU-based efficiency and interpretable feature representations.

Abstract: According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020. Early diagnosis and accurate clinical decision making are critical in reducing this global burden. In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns. Unlike conventional deep learning approaches that rely on extensive training, annotated datasets, and powerful GPU resources, THIR operates entirely without supervision. It extracts topological fingerprints directly from RGB histopathological images using cubical persistence, encoding the evolution of loops as compact, interpretable feature vectors. The similarity retrieval is then performed by computing the distances between these topological descriptors, efficiently returning the top-K most relevant matches.
  Extensive experiments on the BreaKHis dataset demonstrate that THIR outperforms state of the art supervised and unsupervised methods. It processes the entire dataset in under 20 minutes on a standard CPU, offering a fast, scalable, and training free solution for clinical image retrieval.

</details>


### [233] [HDW-SR: High-Frequency Guided Diffusion Model based on Wavelet Decomposition for Image Super-Resolution](https://arxiv.org/abs/2511.13175)
*Chao Yang,Boqian Zhang,Jinghao Xu,Guang Jiang*

Main category: cs.CV

TL;DR: High-frequency guided diffusion for SISR using wavelet decomposition, residual diffusion, and sparse cross-attention to recover fine details.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based SISR methods often blur high-frequency details due to insufficient guidance in the high-frequency domain; explicit high-frequency guidance is needed to restore fine textures and edges.

Method: HDW-SR Diffusion operates on the residual high-frequency map instead of full image; replaces standard CNN downsampling with wavelet-based multi-scale frequency decomposition; enables sparse cross-attention between high-frequency subbands of the pre-super-resolved image and low-frequency subbands of the diffused image; includes a Dynamic Thresholding Block to refine high-frequency selection during attention; uses invertible wavelet transform for low-loss upsampling, ensuring feature reconstruction.

Result: HDW-SR achieves competitive single-image SR performance, with notable strength in recovering fine-grained details across synthetic and real-world datasets, demonstrating effective high-frequency guidance in diffusion-based SR.

Conclusion: The approach demonstrates that explicit high-frequency guidance via wavelet-based multi-scale decomposition and residual diffusion can enhance SISR, yielding sharpened reconstructions and competitive results; code will be released after acceptance.

Abstract: Diffusion-based methods have shown great promise in single image super-resolution (SISR); however, existing approaches often produce blurred fine details due to insufficient guidance in the high-frequency domain. To address this issue, we propose a High-Frequency Guided Diffusion Network based on Wavelet Decomposition (HDW-SR), which replaces the conventional U-Net backbone in diffusion frameworks. Specifically, we perform diffusion only on the residual map, allowing the network to focus more effectively on high-frequency information restoration. We then introduce wavelet-based downsampling in place of standard CNN downsampling to achieve multi-scale frequency decomposition, enabling sparse cross-attention between the high-frequency subbands of the pre-super-resolved image and the low-frequency subbands of the diffused image for explicit high-frequency guidance. Moreover, a Dynamic Thresholding Block (DTB) is designed to refine high-frequency selection during the sparse attention process. During upsampling, the invertibility of the wavelet transform ensures low-loss feature reconstruction. Experiments on both synthetic and real-world datasets demonstrate that HDW-SR achieves competitive super-resolution performance, excelling particularly in recovering fine-grained image details. The code will be available after acceptance.

</details>


### [234] [GenTract: Generative Global Tractography](https://arxiv.org/abs/2511.13183)
*Alec Sargood,Lemuel Puglisi,Elinor Thompson,Mirco Musolesi,Daniel C. Alexander*

Main category: cs.CV

TL;DR: GenTract: first generative model for global tractography mapping dMRI to full streamlines; achieves substantial precision gains over baselines, especially in low-resolution/noisy data.


<details>
  <summary>Details</summary>
Motivation: Global tractography faces error accumulation in local methods and high computation for global optimization; need accurate, efficient tractography on imperfect data.

Method: Develop a generative model (GenTract) that learns to map diffusion data directly to complete streamlines; compare diffusion-based and flow-matching paradigms; evaluate against state-of-the-art baselines.

Result: GenTract attains 2.1x higher precision than TractOracle; outperforms closest competitor by an order of magnitude in challenging low-resolution/noisy settings; produces high-precision tractograms on research-grade data and reliable results on imperfect data.

Conclusion: GenTract is a promising solution for global tractography, offering improved accuracy and robustness across data conditions.

Abstract: Tractography is the process of inferring the trajectories of white-matter pathways in the brain from diffusion magnetic resonance imaging (dMRI). Local tractography methods, which construct streamlines by following local fiber orientation estimates stepwise through an image, are prone to error accumulation and high false positive rates, particularly on noisy or low-resolution data. In contrast, global methods, which attempt to optimize a collection of streamlines to maximize compatibility with underlying fiber orientation estimates, are computationally expensive. To address these challenges, we introduce GenTract, the first generative model for global tractography. We frame tractography as a generative task, learning a direct mapping from dMRI to complete, anatomically plausible streamlines. We compare both diffusion-based and flow matching paradigms and evaluate GenTract's performance against state-of-the-art baselines. Notably, GenTract achieves precision 2.1x higher than the next-best method, TractOracle. This advantage becomes even more pronounced in challenging low-resolution and noisy settings, where it outperforms the closest competitor by an order of magnitude. By producing tractograms with high precision on research-grade data while also maintaining reliability on imperfect, lower-resolution data, GenTract represents a promising solution for global tractography.

</details>


### [235] [Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework](https://arxiv.org/abs/2511.13189)
*Diego Ortego,Marlon Rodríguez,Mario Almagro,Kunal Dahiya,David Jiménez,Juan C. SanMiguel*

Main category: cs.CV

TL;DR: Foundation models boost Extreme Multi-label Classification (XMC) by leveraging larger decoder-only models and visual information. The proposed ViXML framework uses image embeddings with small encoders to achieve competitive or superior results to text-only decoders, and scales efficiently. An extended benchmark with visual metadata is introduced, with state-of-the-art gains up to +8.21% P@1 on the largest dataset.


<details>
  <summary>Details</summary>
Motivation: XMC requires handling extremely large label spaces while balancing efficiency and performance. Existing approaches mainly use small encoder-only transformers to compute near-neighbor style predictions. There is untapped potential in using larger decoder-only models and incorporating visual information to improve performance without prohibitive costs.

Method: 1) Demonstrate substantial gains by employing decoder-sized models of billions of parameters for XMC. 2) Introduce ViXML (Vision-enhanced XMC), which pools a single image embedding and integrates foundation vision models to maintain efficiency. 3) Show that small encoders with image embeddings can outperform text-only decoders in most cases. 4) Extend public text-only datasets with visual metadata for benchmarking. 5) Provide code at the specified GitHub repository.

Result: ViXML yields improvements over prior text-only decoders, with up to +8.21% P@1 on the largest dataset. In many cases, ViXML with small encoders outperforms the text-only decoder. The approach validates the benefit of combining decoder-scale models with vision features for XMC, while keeping computational growth in check.

Conclusion: Larger decoder-only models and vision-enhanced representations can synergistically improve XMC performance. ViXML demonstrates that image information can yield significant gains with minimal additional computation, and the new visual-metadata-augmented benchmarks provide a practical path for future research in multi-modal XMC.

Abstract: Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.

</details>


### [236] [Video Spatial Reasoning with Object-Centric 3D Rollout](https://arxiv.org/abs/2511.13190)
*Haoran Tang,Meng Cao,Ruyang Liu,Xiaoxi Liang,Linglong Li,Ge Li,Xiaodan Liang*

Main category: cs.CV

TL;DR: OCR introduces 3D perturbations on selected objects during training to force holistic scene reasoning in MLLMs; uses a rollout-based pipeline with vanilla and region-noisy videos; achieves state-of-the-art VSI-Bench accuracy with a 3B model, outperforming several 7B baselines; ablations show superiority over prior rollout strategies like T-GRPO and NoisyRollout.


<details>
  <summary>Details</summary>
Motivation: The paper addresses robust video spatial reasoning in multi-modal large language models (MLLMs). Existing methods tend to rely on prompt-focused or narrowly scoped reasoning (query-locked) and depend on supervised fine-tuning or RL, which fail to promote holistic understanding of dynamic 3D scenes. There is a need to reduce prompt-specific bias and encourage reasoning about object relations, geometry, and context.

Method: Propose Object-Centric 3D Rollout (OCR): during training, apply structured perturbations to the 3D geometry of selected objects to degrade their visual cues; project these altered geometries into 2D space to force holistic reasoning across the scene. Employ a rollout-based training pipeline that jointly uses vanilla and region-noisy videos, optimizing spatial reasoning trajectories across both data streams.

Result: The approach achieves state-of-the-art performance with a 3B-parameter model attaining 47.5% accuracy on VSI-Bench, surpassing several 7B baselines. Ablation studies demonstrate OCR's superiority over prior rollout strategies (e.g., T-GRPO, NoisyRollout).

Conclusion: OCR effectively promotes holistic spatial reasoning in video understanding for MLLMs by perturbing geometry to mitigate reliance on explicitly mentioned objects, leading to improved performance and robustness over existing rollout methods.

Abstract: Recent advances in Multi-modal Large Language Models (MLLMs) have showcased remarkable capabilities in vision-language understanding. However, enabling robust video spatial reasoning-the ability to comprehend object locations, orientations, and inter-object relationships in dynamic 3D scenes-remains a key unsolved challenge. Existing approaches primarily rely on spatially grounded supervised fine-tuning or reinforcement learning, yet we observe that such models often exhibit query-locked reasoning, focusing narrowly on objects explicitly mentioned in the prompt while ignoring critical contextual cues. To address this limitation, we propose Object-Centric 3D Rollout (OCR), a novel strategy that introduces structured perturbations to the 3D geometry of selected objects during training. By degrading object-specific visual cues and projecting the altered geometry into 2D space, OCR compels the model to reason holistically across the entire scene. We further design a rollout-based training pipeline that jointly leverages vanilla and region-noisy videos to optimize spatial reasoning trajectories. Experiments demonstrate state-of-the-art performance: our 3B-parameter model achieves 47.5% accuracy on VSI-Bench, outperforming several 7B baselines. Ablations confirm OCR's superiority over prior rollout strategies (e.g., T-GRPO, NoisyRollout).

</details>


### [237] [Birth of a Painting: Differentiable Brushstroke Reconstruction](https://arxiv.org/abs/2511.13191)
*Ying Jiang,Jiayin Lu,Yunuo Chen,Yumeng He,Kui Wu,Yin Yang,Chenfanfu Jiang*

Main category: cs.CV

TL;DR: A differentiable, end-to-end framework that reconstructs painting strokes and textures from images using Bezier strokes, texture synthesis, and a differentiable smudge operator, enabling realistic stroke reconstruction and stylized shading.


<details>
  <summary>Details</summary>
Motivation: Existing generative painting methods emphasize final images or patch-based processes and lack explicit stroke structure and smooth shading, failing to faithfully model the human painting–smudging loop.

Method: Propose a differentiable stroke reconstruction pipeline: parallel differentiable paint rendering of single/dual-color Bezier strokes; a style generation module that creates geometry-conditioned textures across painting styles; a differentiable smudge operator for color blending and shading; and a coarse-to-fine optimization that jointly tunes stroke geometry, color, and texture under geometric/semantic guidance.

Result: Extensive experiments on oil, watercolor, ink, and digital paintings show realistic and expressive stroke reconstructions, smooth tonal transitions, and richly stylized appearances, demonstrating a unified model for expressive digital painting creation.

Conclusion: The framework unifies stroke-level reconstruction and stylized texturing with smudging into a differentiable pipeline, offering a versatile tool for authentic digital painting and analysis of painting processes.

Abstract: Painting embodies a unique form of visual storytelling, where the creation process is as significant as the final artwork. Although recent advances in generative models have enabled visually compelling painting synthesis, most existing methods focus solely on final image generation or patch-based process simulation, lacking explicit stroke structure and failing to produce smooth, realistic shading. In this work, we present a differentiable stroke reconstruction framework that unifies painting, stylized texturing, and smudging to faithfully reproduce the human painting-smudging loop. Given an input image, our framework first optimizes single- and dual-color Bezier strokes through a parallel differentiable paint renderer, followed by a style generation module that synthesizes geometry-conditioned textures across diverse painting styles. We further introduce a differentiable smudge operator to enable natural color blending and shading. Coupled with a coarse-to-fine optimization strategy, our method jointly optimizes stroke geometry, color, and texture under geometric and semantic guidance. Extensive experiments on oil, watercolor, ink, and digital paintings demonstrate that our approach produces realistic and expressive stroke reconstructions, smooth tonal transitions, and richly stylized appearances, offering a unified model for expressive digital painting creation. See our project page for more demos: https://yingjiang96.github.io/DiffPaintWebsite/.

</details>


### [238] [Difficulty-Aware Label-Guided Denoising for Monocular 3D Object Detection](https://arxiv.org/abs/2511.13195)
*Soyul Lee,Seungmin Baek,Dongbo Min*

Main category: cs.CV

TL;DR: A difficulty-aware label denoising framework for monocular 3D detection that perturbs ground-truth labels by instance difficulty and reconstructs them to provide geometric supervision, achieving state-of-the-art results on KITTI across all difficulty levels.


<details>
  <summary>Details</summary>
Motivation: Monocular 3D detection is ill-posed due to depth ambiguity and varying instance difficulty (occlusion, distance, truncation). Existing DETR-based methods still falter due to inaccurate depth and ignore per-instance detection difficulty, limiting robustness.

Method: Introduce MonoDLGD: a denoising framework that adaptively perturbs ground-truth labels based on detection uncertainty (stronger perturbations for easy instances, weaker for hard ones) and then reconstructs them to guide geometry-aware learning. Jointly optimize label reconstruction with 3D detection to improve robustness to object complexity.

Result: On KITTI, MonoDLGD achieves state-of-the-art performance across all difficulty levels (easy, moderate, hard), demonstrating improved robustness to depth ambiguity and instance-level challenges.

Conclusion: Difficulty-aware label-guided denoising provides explicit geometric supervision and promotes geometry-aware representations, yielding robust monocular 3D detection and strong generalization across object complexities.

Abstract: Monocular 3D object detection is a cost-effective solution for applications like autonomous driving and robotics, but remains fundamentally ill-posed due to inherently ambiguous depth cues. Recent DETR-based methods attempt to mitigate this through global attention and auxiliary depth prediction, yet they still struggle with inaccurate depth estimates. Moreover, these methods often overlook instance-level detection difficulty, such as occlusion, distance, and truncation, leading to suboptimal detection performance. We propose MonoDLGD, a novel Difficulty-Aware Label-Guided Denoising framework that adaptively perturbs and reconstructs ground-truth labels based on detection uncertainty. Specifically, MonoDLGD applies stronger perturbations to easier instances and weaker ones into harder cases, and then reconstructs them to effectively provide explicit geometric supervision. By jointly optimizing label reconstruction and 3D object detection, MonoDLGD encourages geometry-aware representation learning and improves robustness to varying levels of object complexity. Extensive experiments on the KITTI benchmark demonstrate that MonoDLGD achieves state-of-the-art performance across all difficulty levels.

</details>


### [239] [Self-Supervised Ultrasound Screen Detection](https://arxiv.org/abs/2511.13197)
*Alberto Gomez,Jorge Oliveira,Ramon Casero,Agis Chartsias*

Main category: cs.CV

TL;DR: Self-supervised pipeline to extract ultrasound (US) images from a photo of a monitor, bypassing DICOM bottlenecks; proof-of-concept shows rectified images support cardiac-view classification with balanced accuracy 0.79 against native DICOMs.


<details>
  <summary>Details</summary>
Motivation: Remove dependence on DICOM transfer for rapid testing and prototyping of US algorithms by enabling extraction of usable US frames from monitor photographs.

Method: Develop a self-supervised pipeline to rectify and extract US images from a photograph of the ultrasound monitor; evaluate by comparing downstream cardiac-view classification performance against native DICOM images.

Result: Rectified images retained sufficient visual fidelity to classify cardiac views with a balanced accuracy of 0.79 relative to native DICOMs.

Conclusion: Demonstrates feasibility of bypassing DICOM bottlenecks for rapid development/testing of ultrasound algorithms and suggests potential for broader adoption in prototyping workflows.

Abstract: Ultrasound (US) machines display images on a built-in monitor, but routine transfer to hospital systems relies on DICOM. We propose a self-supervised pipeline to extract the US image from a photograph of the monitor. This removes the DICOM bottleneck and enables rapid testing and prototyping of new algorithms. In a proof-of-concept study, the rectified images retained enough visual fidelity to classify cardiac views with a balanced accuracy of 0.79 with respect to the native DICOMs.

</details>


### [240] [RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2511.13204)
*Junhee Lee,ChaeBeen Bang,MyoungChul Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: RefineVAD introduces a dual-path weakly-supervised VAD framework that jointly reasons about motion dynamics and anomaly semantics to refine representations for more accurate detection.


<details>
  <summary>Details</summary>
Motivation: Current weakly-supervised VAD methods treat all anomalies as a single class, oversimplifying the anomaly space and ignoring variations in temporal and semantic characteristics. A dual-process approach, inspired by human perception, aims to capture both how motion evolves and what semantic category it resembles.

Method: MoTAR (Motion-aware Temporal Attention and Recalibration): estimates motion salience and dynamically shifts temporal focus using shift-based attention and global Transformer-based modeling. CORE (Category-Oriented Refinement): injects soft anomaly category priors by aligning segment-level features with learnable category prototypes through cross-attention.

Result: Experiments on the WVAD benchmark show that RefineVAD is effective and that incorporating semantic context improves feature refinement toward anomaly-relevant patterns.

Conclusion: Jointly modeling temporal dynamics and semantic structure enhances weakly-supervised video anomaly detection and underscores the value of semantic guidance in refining anomaly representations.

Abstract: Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both "how" motion evolves and "what" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.

</details>


### [241] [End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer](https://arxiv.org/abs/2511.13208)
*Yonghui Yu,Jiahang Cai,Xun Wang,Wenwu Yang*

Main category: cs.CV

TL;DR: An end-to-end multi-person 2D pose estimation framework for videos (PAVE-Net) that eliminates heuristic components like detection, RoI cropping, and NMS, using a pose-aware transformer with a spatial encoder and spatiotemporal pose decoder to achieve strong accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of two-stage pipelines that rely on per-frame detection and heuristic operations, aiming for improved accuracy and efficiency in cross-frame person association within complex, overlapping trajectories.

Method: Propose PAVE-Net: a spatial encoder to model intra-frame relations, a spatiotemporal pose decoder to capture global dependencies across frames, and a pose-aware attention mechanism that allows each pose query to selectively aggregate features for the same individual across consecutive frames; explicitly model spatiotemporal dependencies among pose keypoints.

Result: Outperforms prior image-based end-to-end methods with a 6.0 mAP improvement on PoseTrack2017 and is competitive with state-of-the-art two-stage video-based approaches, while offering significant gains in efficiency.

Conclusion: First end-to-end method for multi-frame 2D human pose estimation; demonstrates that end-to-end video pose estimation can achieve competitive accuracy with improved efficiency compared to traditional two-stage pipelines.

Abstract: Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single-person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames.Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation.Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a \textbf{6.0} mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video-based approaches, while offering significant gains in efficiency.Project page: https://github.com/zgspose/PAVENet

</details>


### [242] [3DAlign-DAER: Dynamic Attention Policy and Efficient Retrieval Strategy for Fine-grained 3D-Text Alignment at Scale](https://arxiv.org/abs/2511.13211)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: 3DAlign-DAER is a unified framework for text-3D geometry alignment that uses dynamic attention with hierarchical fusion and Monte Carlo tree search, plus an efficient retrieval strategy, and introduces Align3D-2M dataset; it achieves state-of-the-art performance across diverse benchmarks and will release code/models/datasets.


<details>
  <summary>Details</summary>
Motivation: Existing state-of-the-art methods struggle to align fine-grained textual semantics with detailed 3D geometry, and performance degrades with large-scale 3D databases. A unified framework with fine-grained token-to-point alignment and scalable retrieval is needed.

Method: Proposes Dynamic Attention Policy (DAP) with Hierarchical Attention Fusion (HAF) to model fine-grained token-to-point attentions. Uses Monte Carlo tree search to dynamically calibrate HAF attention weights via a hybrid reward signal. Inference uses Efficient Retrieval Strategy (ERS) for hierarchical search in large embedding spaces. Introduces Align3D-2M, a 2-million text-3D pair dataset, for training and evaluation.

Result: Experiments demonstrate superior performance on diverse benchmarks; the method outperforms traditional retrieval methods (e.g., KNN) in both accuracy and efficiency, and the Align3D-2M dataset supports robust fine-grained cross-modal training.

Conclusion: 3DAlign-DAER provides an effective solution for text-3D alignment and scalable cross-modal retrieval/classification, with a released dataset and codebase to foster further research.

Abstract: Despite recent advancements in 3D-text cross-modal alignment, existing state-of-the-art methods still struggle to align fine-grained textual semantics with detailed geometric structures, and their alignment performance degrades significantly when scaling to large-scale 3D databases. To overcome this limitation, we introduce 3DAlign-DAER, a unified framework designed to align text and 3D geometry via the proposed dynamic attention policy and the efficient retrieval strategy, capturing subtle correspondences for diverse cross-modal retrieval and classification tasks. Specifically, during the training, our proposed dynamic attention policy (DAP) employs the Hierarchical Attention Fusion (HAF) module to represent the alignment as learnable fine-grained token-to-point attentions. To optimize these attentions across different tasks and geometric hierarchies, our DAP further exploits the Monte Carlo tree search to dynamically calibrate HAF attention weights via a hybrid reward signal and further enhances the alignment between textual descriptions and local 3D geometry. During the inference, our 3DAlign-DAER introduces an Efficient Retrieval Strategy (ERS) to leverage efficient hierarchical searching in the large-scale embedding spaces, outperforming traditional methods (e.g., KNN) in accuracy and efficiency. Furthermore, to facilitate text-3D alignment research and train our 3DAlign-DAER, we construct Align3D-2M, a large-scale dataset featuring 2M text-3D pairs, to provide sufficient fine-grained cross-modal annotations. Extensive and comprehensive experiments demonstrate the superior performance of our 3DAlign-DAER on diverse benchmarks. We will release our codes, models, and datasets.

</details>


### [243] [Hybrid-Domain Adaptative Representation Learning for Gaze Estimation](https://arxiv.org/abs/2511.13222)
*Qida Tan,Hongyu Yang,Wenchao Du*

Main category: cs.CV

TL;DR: HARL: a Hybrid-domain Adaptative Representation Learning framework for appearance-based gaze estimation that disentangles gaze-relevant features via unsupervised alignment of high-quality near-eye images and leverages a sparse graph fusion module to exploit head-pose constraints, achieving state-of-the-art or competitive cross-dataset performance on EyeDiap, MPIIFaceGaze, and Gaze360.


<details>
  <summary>Details</summary>
Motivation: Cross-domain gaze estimation is hindered by gaze-irrelevant factors (expressions, wearables, image quality). The goal is a robust gaze representation that generalizes across domains with minimal inference cost.

Method: 1) Disentangle gaze-relevant features by aligning high-quality near-eye features with target domain in an unsupervised domain-adaptation setup. 2) Analyze head-pose effect and introduce a sparse graph fusion module to exploit geometric constraint between gaze direction and head pose, producing a dense, robust gaze representation.

Result: Empirical results show state-of-the-art accuracies of 5.02°, 3.36°, and 9.26° on EyeDiap, MPIIFaceGaze, and Gaze360 respectively, with competitive cross-dataset performance; code released.

Conclusion: HARL effectively learns robust gaze representations across domains by combining unsupervised domain adaptation for gaze disentanglement with a head-pose aware graph fusion, yielding strong cross-domain generalization and practical efficiency.

Abstract: Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\textbf{5.02}^{\circ}$ and $\textbf{3.36}^{\circ}$, and $\textbf{9.26}^{\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.

</details>


### [244] [MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI](https://arxiv.org/abs/2511.13232)
*Malek Al Abed,Sebiha Demir,Anne Groteklaes,Elodie Germani,Shahrooz Faghihroohi,Hemmen Sabir,Shadi Albarqouni*

Main category: cs.CV

TL;DR: A 3D conditional diffusion model (MRIQT) performs image quality transfer from portable ultra-low-field MRI to high-field MRI for neonatal brain imaging, achieving higher PSNR and perceived quality than baselines while preserving pathology.


<details>
  <summary>Details</summary>
Motivation: uLF-MRI enables portable neonatal imaging but suffers from very low SNR and degraded diagnostic quality compared to HF-MRI. There is a need for a physics-informed, stable IQT approach to recover HF-like images from uLF data.

Method: MRIQT uses 3D conditional diffusion with physics-consistent uLF degradation in K-space, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss. It denoises a noised uLF input conditioned on the same scan using a volumetric attention-UNet for structure-preserving translation.

Result: On a neonatal cohort with diverse pathologies, MRIQT outperforms recent GAN/CNN baselines in PSNR by 15.3% and is 1.78% above the current state of the art. Physicians rated 85% of outputs as good quality with clear pathology present.

Conclusion: MRIQT enables high-fidelity, diffusion-based enhancement of portable uLF-MRI for reliable neonatal brain assessment, offering a clinically meaningful pathway toward HF-quality imaging from a portable scanner.

Abstract: Portable ultra-low-field MRI (uLF-MRI, 0.064 T) offers accessible neuroimaging for neonatal care but suffers from low signal-to-noise ratio and poor diagnostic quality compared to high-field (HF) MRI. We propose MRIQT, a 3D conditional diffusion framework for image quality transfer (IQT) from uLF to HF MRI. MRIQT combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss for anatomical fidelity. The model denoises from a noised uLF input conditioned on the same scan, leveraging volumetric attention-UNet architecture for structure-preserving translation. Trained on a neonatal cohort with diverse pathologies, MRIQT surpasses recent GAN and CNN baselines in PSNR 15.3% with 1.78% over the state of the art, while physicians rated 85% of its outputs as good quality with clear pathology present. MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field (uLF) MRI for deliable neonatal brain assessment.

</details>


### [245] [MMD-Thinker: Adaptive Multi-Dimensional Thinking for Multimodal Misinformation Detection](https://arxiv.org/abs/2511.13242)
*Junjie Wu,Guohong Fu*

Main category: cs.CV

TL;DR: Introduces MMD-Thinker, a two-stage adaptive thinking framework for multimodal misinformation detection that uses tailor-made thinking modes, task-specific instruction tuning, and reinforcement learning with a mixed-advantage to achieve SOTA; releases MMR dataset (~8K image-text pairs with reasoning traces).


<details>
  <summary>Details</summary>
Motivation: Address insufficient reasoning and biases of general-purpose multimodal LLMs in multimodal misinformation detection; need task-specific, multi-dimensional thinking for reliable judgments.

Method: Three components: (1) design tailor-made thinking modes for detecting multimodal misinformation; (2) inject these modes into MLLMs via task-specific instruction tuning; (3) reinforce reasoning with a mixed-advantage RL objective; create MMR dataset with reasoning processes and classification labels (~8k image-text pairs).

Result: Achieves state-of-the-art performance on both in-domain and out-of-domain benchmarks; demonstrates flexible inference and token usage; code will be publicly available.

Conclusion: Adaptive multi-dimensional thinking frameworks can surpass single-thinking-path detectors in evolving multimodal misinformation contexts; provides data and code to support future research.

Abstract: Multimodal misinformation floods on various social media, and continues to evolve in the era of AI-generated content (AIGC). The emerged misinformation with low creation cost and high deception poses significant threats to society. While recent studies leverage general-purpose multimodal large language models (MLLMs) to achieve remarkable results in detection, they encounter two critical limitations: (1) Insufficient reasoning, where general-purpose MLLMs often follow the uniform reasoning paradigm but generate inaccurate explanations and judgments, due to the lack of the task-specific knowledge of multimodal misinformation detection. (2) Reasoning biases, where a single thinking mode make detectors a suboptimal path for judgment, struggling to keep pace with the fast-growing and intricate multimodal misinformation. In this paper, we propose MMD-Thinker, a two-stage framework for multimodal misinformation detection through adaptive multi-dimensional thinking. First, we develop tailor-designed thinking mode for multimodal misinformation detection. Second, we adopt task-specific instruction tuning to inject the tailored thinking mode into general-purpose MLLMs. Third, we further leverage reinforcement learning strategy with a mixed advantage function, which incentivizes the reasoning capabilities in trajectories. Furthermore, we construct the multimodal misinformation reasoning (MMR) dataset, encompasses more than 8K image-text pairs with both reasoning processes and classification labels, to make progress in the relam of multimodal misinformation detection. Experimental results demonstrate that our proposed MMD-Thinker achieves state-of-the-art performance on both in-domain and out-of-domain benchmark datasets, while maintaining flexible inference and token usage. Code will be publicly available at Github.

</details>


### [246] [Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention](https://arxiv.org/abs/2511.13249)
*Yu Wen,Shuyong Gao,Shuping Zhang,Miao Huang,Lili Tao,Han Yang,Haozhe Xing,Lihe Zhang,Boxue Hou*

Main category: cs.CV

TL;DR: RFMNet advances Ref-COD by fusing multi-stage salient-image features with camouflage features using Overlapped Windows Cross-attention and a Referring Feature Aggregation decoder to achieve state-of-the-art results on Ref-COD benchmarks.


<details>
  <summary>Details</summary>
Motivation: Ref-COD benefits from reference information, but prior work often treats reference salient images at a coarse or single-level prompt. Exploiting multi-context, local-detail features promises better discriminability for camouflaged objects.

Method: Extract features from multiple encoding stages of reference salient images; perform interactive fusion with camouflage features at corresponding stages. Use an Overlapped Windows Cross-attention mechanism to emphasize local-area matching. Introduce the Referring Feature Aggregation (RFA) module to decode and segment the camouflaged objects progressively.

Result: Experiments on the Ref-COD benchmark demonstrate state-of-the-art performance.

Conclusion: Multi-stage, locally focused fusion of salient-reference and camouflage features via Overlapped Window Cross-attention and RFA yields superior Ref-COD performance, validating the proposed design.

Abstract: Referring camouflaged object detection (Ref-COD) aims to identify hidden objects by incorporating reference information such as images and text descriptions. Previous research has transformed reference images with salient objects into one-dimensional prompts, yielding significant results. We explore ways to enhance performance through multi-context fusion of rich salient image features and camouflaged object features. Therefore, we propose RFMNet, which utilizes features from multiple encoding stages of the reference salient images and performs interactive fusion with the camouflage features at the corresponding encoding stages. Given that the features in salient object images contain abundant object-related detail information, performing feature fusion within local areas is more beneficial for detecting camouflaged objects. Therefore, we propose an Overlapped Windows Cross-attention mechanism to enable the model to focus more attention on the local information matching based on reference features. Besides, we propose the Referring Feature Aggregation (RFA) module to decode and segment the camouflaged objects progressively. Extensive experiments on the Ref-COD benchmark demonstrate that our method achieves state-of-the-art performance.

</details>


### [247] [GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models](https://arxiv.org/abs/2511.13259)
*Yushuo Zheng,Jiangyong Ying,Huiyu Duan,Chunyi Li,Zicheng Zhang,Jing Liu,Xiaohong Liu,Guangtao Zhai*

Main category: cs.CV

TL;DR: GeoX-Bench is a large benchmark for evaluating large multimodal models on cross-view geo-localization and pose estimation, revealing strong localization but weaker pose estimation, with instruction-tuning on GeoX-Bench data yielding notable improvements.


<details>
  <summary>Details</summary>
Motivation: There is a gap in understanding LMM capabilities for cross-view geo-localization and pose estimation, despite potential benefits for navigation and robotics. A standardized benchmark is needed to assess these abilities and guide improvements.

Method: Construct GeoX-Bench with 10,859 panoramic-satellite image pairs across 128 cities in 49 countries, plus 755,976 QA pairs (42,900 designated for benchmarking). Evaluate 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks and study the effects of instruction-tuning using the benchmark data.

Result: Current LMMs achieve impressive performance on cross-view geo-localization but show significantly degraded performance on pose estimation. Instruction-tuning LMMs on GeoX-Bench training data can substantially improve cross-view geo-sense abilities.

Conclusion: GeoX-Bench is a valuable resource for evaluating and guiding improvements in cross-view geo-localization and pose estimation for LMMs; instruction-tuning using its data is a promising direction, with potential applicability to navigation and robotics; the dataset is publicly available.

Abstract: Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \textit{etc}. To bridge this gap, we introduce \textbf{GeoX-Bench}, a comprehensive \underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \underline{cross}-view \underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.

</details>


### [248] [Building Egocentric Procedural AI Assistant: Methods, Benchmarks, and Challenges](https://arxiv.org/abs/2511.13261)
*Junlong Li,Huaiyuan Xu,Sijie Cheng,Kejun Wu,Kim-Hui Yap,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: Proposes EgoProceAssist for egocentric step-by-step procedures, defines three core tasks, surveys the field, benchmarks VLM-based methods, and outlines future directions with a public repository.


<details>
  <summary>Details</summary>
Motivation: To address the lack of first‑person, procedural support in vision-language AI, enabling egocentric error detection, procedural learning, and question answering for daily tasks.

Method: Develops a new taxonomy of egocentric procedural AI, reviews current techniques, datasets, and metrics across three core tasks, conducts novel experiments comparing representative VLM-based methods, and performs comprehensive evaluations.

Result: Provides a structured analysis, benchmarking insights, and practical guidance showing gaps between current VLM-based assistants and the proposed EgoProceAssist; includes an active repository for ongoing updates.

Conclusion: Highlights key challenges and outlines future research directions to advance robust egocentric procedural AI assistants and invites community participation via the public repository.

Abstract: Driven by recent advances in vision language models (VLMs) and egocentric perception research, we introduce the concept of an egocentric procedural AI assistant (EgoProceAssist) tailored to step-by-step support daily procedural tasks in a first-person view. In this work, we start by identifying three core tasks: egocentric procedural error detection, egocentric procedural learning, and egocentric procedural question answering. These tasks define the essential functions of EgoProceAssist within a new taxonomy. Specifically, our work encompasses a comprehensive review of current techniques, relevant datasets, and evaluation metrics across these three core areas. To clarify the gap between the proposed EgoProceAssist and existing VLM-based AI assistants, we introduce novel experiments and provide a comprehensive evaluation of representative VLM-based methods. Based on these findings and our technical analysis, we discuss the challenges ahead and suggest future research directions. Furthermore, an exhaustive list of this study is publicly available in an active repository that continuously collects the latest work: https://github.com/z1oong/Building-Egocentric-Procedural-AI-Assistant

</details>


### [249] [SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression](https://arxiv.org/abs/2511.13264)
*Keshav Gupta,Akshat Sanghvi,Shreyas Reddy Palley,Astitva Srivastava,Charu Sharma,Avinash Sharma*

Main category: cs.CV

TL;DR: SymGS: a plug-in symmetry-aware compression for 3D Gaussian Splatting that uses learnable mirror priors to remove reflective redundancies, achieving up to 1.66x more compression than HAC (up to 3x on large scenes) and about 108x average compression on a 3DGS scene, while preserving rendering quality.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting achieves fast rendering but has large memory footprints as scenes grow. Existing compression exploits primitive-level redundancy, but there remains unexploited redundancy from mirror symmetry. Leveraging symmetry can further reduce storage and pave the way for scalable 3DGS.

Method: Introduce learnable mirrors into the scene as part of a plug-and-play framework (SymGS) to capture mirror symmetry and eliminate local/global reflective redundancies. The framework is designed to augment existing compression methods (e.g., HAC) rather than replace them.

Result: Compared to HAC, SymGS achieves 1.66x compression on benchmark datasets and up to 3x on large-scale scenes. On average, SymGS yields ~108x compression for a 3DGS scene, with rendering quality preserved.

Conclusion: SymGS demonstrates that incorporating symmetry-aware learnable mirrors can substantially boost compression for 3D Gaussian Splatting without sacrificing rendering fidelity, functioning as a drop-in enhancement to current compression pipelines. The project page is symgs.github.io.

Abstract: 3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, \textbf{\textit{SymGS}}, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \times$ compression across benchmark datasets (upto $3\times$ on large-scale scenes). On an average, SymGS enables $\bf{108\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at \textbf{\color{cyan}{symgs.github.io}}

</details>


### [250] [Computer Vision based group activity detection and action spotting](https://arxiv.org/abs/2511.13315)
*Narthana Sivalingam,Santhirarajah Sivasthigan,Thamayanthi Mahendranathan,G. M. R. I. Godaliyadda,M. P. B. Ekanayake,H. M. V. R. Herath*

Main category: cs.CV

TL;DR: A multi-stage framework combines Mask R-CNN-based actor localization, multi-backbone feature extraction with RoIAlign and mask refinement, and graph-based relational reasoning to recognize individual and group activities in videos; it achieves improved performance on the Collective Activity dataset.


<details>
  <summary>Details</summary>
Motivation: Group activity recognition in real-world scenes is hindered by occlusions, appearance variations, and intricate interactions. Incorporating segmentation masks and relational graphs can better capture actor features and inter-person relations to improve recognition. 

Method: 1) Detect actors with Mask R-CNN to obtain bounding boxes and masks. 2) Use multiple backbones (Inception V3, MobileNet, VGG16) to extract feature maps; apply RoIAlign to preserve spatial alignment for each actor. 3) Fuse mask information with features to produce refined actor features. 4) Construct Actor Relation Graphs encoding appearance similarity and positional relations using metrics such as normalized cross-correlation, sum of absolute differences, and dot product. 5) Apply Graph Convolutional Networks on these graphs to reason about relationships and predict both individual actions and group activities.

Result: On the Collective Activity dataset, the combination of mask-based feature refinement, robust similarity search, and relational graph reasoning yields improved recognition performance across both crowded and non-crowded scenarios.

Conclusion: The work demonstrates the value of integrating segmentation, feature extraction, and relational graph reasoning for robust and nuanced video understanding of group activities.

Abstract: Group activity detection in multi-person scenes is challenging due to complex human interactions, occlusions, and variations in appearance over time. This work presents a computer vision based framework for group activity recognition and action spotting using a combination of deep learning models and graph based relational reasoning. The system first applies Mask R-CNN to obtain accurate actor localization through bounding boxes and instance masks. Multiple backbone networks, including Inception V3, MobileNet, and VGG16, are used to extract feature maps, and RoIAlign is applied to preserve spatial alignment when generating actor specific features. The mask information is then fused with the feature maps to obtain refined masked feature representations for each actor. To model interactions between individuals, we construct Actor Relation Graphs that encode appearance similarity and positional relations using methods such as normalized cross correlation, sum of absolute differences, and dot product. Graph Convolutional Networks operate on these graphs to reason about relationships and predict both individual actions and group level activities. Experiments on the Collective Activity dataset demonstrate that the combination of mask based feature refinement, robust similarity search, and graph neural network reasoning leads to improved recognition performance across both crowded and non crowded scenarios. This approach highlights the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.

</details>


### [251] [Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation](https://arxiv.org/abs/2511.13269)
*Lingfeng Zhang,Yuchen Zhang,Hongsheng Li,Haoxiang Fu,Yingbo Tang,Hangjun Ye,Long Chen,Xiaojun Liang,Xiaoshuai Hao,Wenbo Ding*

Main category: cs.CV

TL;DR: A new benchmark, SpatialSky-Bench, and a 1M-sample dataset, SpatialSky-Dataset, are introduced to evaluate and enhance spatial reasoning of vision-language models (VLMs) for UAV navigation, complemented by Sky-VLM which achieves state-of-the-art performance across evaluation tasks.


<details>
  <summary>Details</summary>
Motivation: Current VLMs lack robust spatial intelligence for dynamic UAV environments; there is a need for dedicated benchmarks, datasets, and models to assess and improve UAV-specific spatial reasoning and navigation.

Method: Construct SpatialSky-Bench with two categories—Environmental Perception and Scene Understanding—encompassing 13 subcategories (e.g., bounding boxes, color, distance, height, landing safety analysis, etc.). Create SpatialSky-Dataset with 1M diverse samples across UAV scenarios. Develop Sky-VLM, a VLM optimized for UAV spatial reasoning across multiple granularities and contexts. Evaluate a range of open- and closed-source VLMs on the benchmark.

Result: Experimental evaluations show mainstream VLMs perform unsatisfactorily in complex UAV navigation scenarios, highlighting gaps in spatial capabilities. Sky-VLM achieves state-of-the-art performance across all benchmark tasks.

Conclusion: SpatialSky provides a framework to advance UAV-capable VLMs by linking a targeted benchmark, a large-scale dataset, and a specialized model, with code released for reproducibility.

Abstract: Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.

</details>


### [252] [Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models](https://arxiv.org/abs/2511.13276)
*Noam Tsfaty,Avishai Weizman,Liav Cohen,Moshe Tshuva,Yehudit Aperstein*

Main category: cs.CV

TL;DR: A dual-backbone CNN+Transformer with top-k pooling for weakly supervised anomaly detection in surveillance videos; achieves 90.7% AUC on UCF-Crime.


<details>
  <summary>Details</summary>
Motivation: Detecting rare, diverse anomalies under weak supervision is challenging; video-level labels are easier to obtain; a hybrid representation can capture both local cues and long-range dependencies.

Method: Two backbones (CNN and Transformer) extract features; top-k pooling selects a few most anomaly-relevant segments per video; trained with video-level labels to detect anomalies.

Result: Achieves 90.7% AUC on UCF-Crime; demonstrates strong performance under weak supervision.

Conclusion: Dual-backbone with top-k pooling is effective for rare/diverse anomalies in surveillance; suggests benefit of combining local and global representations; potential trade-offs in computation.

Abstract: We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.

</details>


### [253] [SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting](https://arxiv.org/abs/2511.13278)
*Zihan Li,Tengfei Wang,Wentian Gan,Hao Zhan,Xin Wang,Zongqian Zhan*

Main category: cs.CV

TL;DR: SF-Recon directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification, using 3D Gaussian Splatting, normal-gradient-guided optimization, edge-consistency pruning, and depth-constrained Delaunay triangulation to produce compact, faithful building meshes.


<details>
  <summary>Details</summary>
Motivation: Conventional multi-view geometry pipelines require dense reconstruction, meshing, and subsequent simplification, which are costly and quality-sensitive. There is a need for lightweight, structurally faithful building surfaces for digital city, navigation, and fast geospatial analytics.

Method: 1) Train an initial 3D Gaussian Splatting (3DGS) field for a view-consistent representation. 2) Distill building structure via a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries. 3) Apply multi-view edge-consistency pruning to sharpen structural boundaries and suppress non-structural artifacts without external supervision. 4) Use a multi-view depth-constrained Delaunay triangulation to convert the structured Gaussian field into a lightweight, structurally faithful building mesh.

Result: Experiments on the SF dataset show that SF-Recon directly reconstructs lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency.

Conclusion: SF-Recon enables direct, efficient production of structurally faithful lightweight building meshes from multi-view images without post-hoc mesh simplification, illustrating strong potential for digital city workflows and fast geospatial analytics.

Abstract: Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/

</details>


### [254] [Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images](https://arxiv.org/abs/2511.13353)
*Lucas Gabriel Telesco,Danila Nejamkin,Estefanía Mata,Francisco Filizzola,Kevin Wignall,Lucía Franco Troilo,María de los Angeles Cenoz,Melissa Thompson,Mercedes Leguía,Ignacio Larrabide,José Ignacio Orlando*

Main category: cs.CV

TL;DR: A semi-supervised, multi-task RIQA method uses a Teacher to generate pseudo-labels for quality details and a ResNet-18 backbone to jointly predict overall image quality and detailed capture defects, improving performance and interpretability without extra labeling.


<details>
  <summary>Details</summary>
Motivation: RIQA tools often provide only an overall quality score due to the high cost of detailed annotations. There is a need for interpretable feedback on acquisition defects to guide recapture without requiring extensive manual labeling.

Method: A hybrid semi-supervised approach: train a Teacher model on a small labeled set to produce pseudo-labels for quality details; fine-tune a pre-trained model in a multi-task framework to predict both overall quality and detail labels. Uses ResNet-18 as backbone and evaluates on EyeQ and DeepDRiD; compares against single-task baselines; assesses statistical significance.

Result: The approach improves RIQA performance over single-task baselines (EyeQ F1: 0.875 vs. 0.863; DeepDRiD F1: 0.778 vs. 0.763). The multi-task model often matches the Teacher for detail predictions (p > 0.05). On a newly annotated EyeQ subset, the model performed comparably to experts, suggesting pseudo-labels align with expert variability.

Conclusion: Semi-supervised multi-task RIQA yields better overall quality assessment and provides interpretable feedback on capture conditions (illumination, clarity, contrast) without additional manual labeling, enabling clinically actionable guidance to recapture images.

Abstract: Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases. However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture. This gap is mainly due to the high cost of detailed annotations. In this paper, we aim to mitigate this limitation by introducing a hybrid semi-supervised learning approach that combines manual labels for overall quality with pseudo-labels of quality details within a multi-task framework. Our objective is to obtain more interpretable RIQA models without requiring extensive manual labeling. Pseudo-labels are generated by a Teacher model trained on a small dataset and then used to fine-tune a pre-trained model in a multi-task setting. Using a ResNet-18 backbone, we show that these weak annotations improve quality assessment over single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, and 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. The multi-task model achieved performance statistically comparable to the Teacher for most detail prediction tasks (p > 0.05). In a newly annotated EyeQ subset released with this paper, our model performed similarly to experts, suggesting that pseudo-label noise aligns with expert variability. Our main finding is that the proposed semi-supervised approach not only improves overall quality assessment but also provides interpretable feedback on capture conditions (illumination, clarity, contrast). This enhances interpretability at no extra manual labeling cost and offers clinically actionable outputs to guide image recapture.

</details>


### [255] [Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space](https://arxiv.org/abs/2511.13282)
*Kaiwen Wang,Kaili Zheng,Yiming Shi,Chenyi Guo,Ji Wu*

Main category: cs.CV

TL;DR: Introduces Depth-conditioned Translation Optimization (DTO) for joint camera-space translation refinement of multiple people to achieve scene-consistent multi-person human mesh recovery; also proposes Metric-Aware HMR with metric-scale estimation, producing a new DTO-Humans pGT dataset and achieving state-of-the-art results on relative depth and mesh recovery.


<details>
  <summary>Details</summary>
Motivation: Address scarcity of in-the-wild multi-person training data and the lack of scene-level consistency when individuals are processed independently; aim to produce coherent camera-space placements and metric-scale mesh estimates.

Method: DTO: an optimization-based MAP framework that jointly refines all subjects' camera-space translations using anthropometric height priors and monocular depth cues, enabling scene-consistent placements; applied to 4D-Humans to create DTO-Humans (0.56M pGT, ~4.8 persons per image). Metric-Aware HMR: end-to-end network estimating mesh and camera parameters in metric scale via a camera branch and a relative metric loss.

Result: Demonstrates state-of-the-art performance in relative depth reasoning and human mesh recovery; DTO-Humans dataset provides a large, scene-consistent multi-person pGT resource; results suggest improved scene-level coherence and scale consistency.

Conclusion: The approach improves scene-level consistency and metric-scale estimation for multi-person HMR, enabling better training data and more accurate models. Code and data are to be released, potentially impacting future datasets and models for multi-person mesh recovery.

Abstract: Multi-person human mesh recovery from a single image is a challenging task, hindered by the scarcity of in-the-wild training data. Prevailing in-the-wild human mesh pseudo-ground-truth (pGT) generation pipelines are single-person-centric, where each human is processed individually without joint optimization. This oversight leads to a lack of scene-level consistency, producing individuals with conflicting depths and scales within the same image. To address this, we introduce Depth-conditioned Translation Optimization (DTO), a novel optimization-based method that jointly refines the camera-space translations of all individuals in a crowd. By leveraging anthropometric priors on human height and depth cues from a monocular depth estimator, DTO solves for a scene-consistent placement of all subjects within a principled Maximum a posteriori (MAP) framework. Applying DTO to the 4D-Humans dataset, we construct DTO-Humans, a new large-scale pGT dataset of 0.56M high-quality, scene-consistent multi-person images, featuring dense crowds with an average of 4.8 persons per image. Furthermore, we propose Metric-Aware HMR, an end-to-end network that directly estimates human mesh and camera parameters in metric scale. This is enabled by a camera branch and a novel relative metric loss that enforces plausible relative scales. Extensive experiments demonstrate that our method achieves state-of-the-art performance on relative depth reasoning and human mesh recovery. Code and data will be released publicly.

</details>


### [256] [TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing](https://arxiv.org/abs/2511.13283)
*Jongha Kim,Minseong Bae,Sanghyeok Lee,Jinsung Yoon,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: TabFlash is an efficient multimodal LLM for table understanding that uses progressive question conditioning, token pruning, and token focusing to produce informative yet compact visual features, achieving SOTA with reduced computation.


<details>
  <summary>Details</summary>
Motivation: Tables pose unique challenges for MLLMs due to the need for question-specific focus and a lot of background from table images. Redundant visual information hurts efficiency and performance; there is a demand for compact, informative features tailored for table understanding.

Method: Proposes progressive question conditioning by injecting the question into Vision Transformer layers with increasing frequency. Introduces a pruning strategy to discard background tokens, and a token focusing training approach to mitigate information loss by encouraging retained tokens to capture essential information. Combines these components into TabFlash.

Result: TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% fewer FLOPs and 30% less memory than the second-best model.

Conclusion: The combination of progressive conditioning, token pruning, and token focusing yields a more efficient and effective MLLM for table understanding, reducing redundancy without sacrificing performance and enabling more resource-efficient deployments.

Abstract: Table images present unique challenges for effective and efficient understanding due to the need for question-specific focus and the presence of redundant background regions. Existing Multimodal Large Language Model (MLLM) approaches often overlook these characteristics, resulting in uninformative and redundant visual representations. To address these issues, we aim to generate visual features that are both informative and compact to improve table understanding. We first propose progressive question conditioning, which injects the question into Vision Transformer layers with gradually increasing frequency, considering each layer's capacity to handle additional information, to generate question-aware visual features. To reduce redundancy, we introduce a pruning strategy that discards background tokens, thereby improving efficiency. To mitigate information loss from pruning, we further propose token focusing, a training strategy that encourages the model to concentrate essential information in the retained tokens. By combining these approaches, we present TabFlash, an efficient and effective MLLM for table understanding. TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.

</details>


### [257] [Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model](https://arxiv.org/abs/2511.13387)
*Fei Kong*

Main category: cs.CV

TL;DR: The paper proposes gDDCM, a generalized diffusion-based image compression method that extends DDCM from DDPM to multiple diffusion model families (DDPM, Score-Based, Consistency, Rectified Flow). It shows generalization and improved performance on CIFAR-10 and LSUN Bedroom.


<details>
  <summary>Details</summary>
Motivation: DDCM worked only with DDPM; there's a need to extend diffusion-based compression to other popular diffusion models to broaden applicability and potential performance gains.

Method: Generalizes the Denoising Diffusion Compression Mechanism by replacing backward-process noise with model-specific noise sets across various diffusion frameworks (DDPM, Score-Based, Consistency, Rectified Flow). It evaluates the approach on standard datasets to demonstrate generalization.

Result: The approach successfully generalizes DDCM to the listed diffusion models and achieves improved compression performance on CIFAR-10 and LSUN Bedroom.

Conclusion: gDDCM broadens the applicability of diffusion-based compression, enabling effective image compression across multiple diffusion-model families and potentially guiding future improvements in model-specific compression schemes.

Abstract: Recently, the Denoising Diffusion Codebook Models (DDCM) was proposed. DDCM leverages the Denoising Diffusion Probabilistic Model (DDPM) and replaces the random noise in the backward process with noise sampled from specific sets according to a predefined rule, thereby enabling image compression. However, DDCM cannot be applied to methods other than DDPM. In this paper, we propose the generalized Denoising Diffusion Compression Model (gDDCM), which extends DDCM to mainstream diffusion models and their variants, including DDPM, Score-Based Models, Consistency Models, and Rectified Flow. We evaluate our method on CIFAR-10 and LSUN Bedroom datasets. Experimental results demonstrate that our approach successfully generalizes DDCM to the aforementioned models and achieves improved performance.

</details>


### [258] [SkyReels-Text: Fine-grained Font-Controllable Text Editing for Poster Design](https://arxiv.org/abs/2511.13285)
*Yunjie Yu,Jingchen Wu,Junchen Zhu,Chunze Lin,Guibin Chen*

Main category: cs.CV

TL;DR: SkyReels-Text is a font-controllable framework for precise poster text editing that edits multiple regions with distinct typography without font labels or fine-tuning, preserving non-edited content and delivering high fidelity and realism.


<details>
  <summary>Details</summary>
Motivation: Professional design workflows require fine-grained, font-aware text manipulation that preserves overall visual harmony; current image editing models lack support for multi-region, font-accurate edits without relying on labeled fonts or retraining.

Method: A font-controllable editing framework that accepts cropped glyph patches to specify typography, enabling simultaneous edits of multiple text regions, each with its own style, while keeping non-edited areas visually intact; inference-free of font labels and fine-tuning.

Result: Achieves state-of-the-art text fidelity and visual realism across multiple datasets, including handwritten text benchmarks; demonstrates unprecedented control over font families and stylistic nuances.

Conclusion: Bridges general-purpose image editing with professional typographic design, enabling practical font-aware poster editing without font labeling or fine-tuning.

Abstract: Artistic design such as poster design often demands rapid yet precise modification of textual content while preserving visual harmony and typographic intent, especially across diverse font styles. Although modern image editing models have grown increasingly powerful, they still fall short in fine-grained, font-aware text manipulation, limiting their utility in professional design workflows such as poster editing. To address this issue, we present SkyReels-Text, a novel font-controllable framework for precise poster text editing. Our method enables simultaneous editing of multiple text regions, each rendered in distinct typographic styles, while preserving the visual appearance of non-edited regions. Notably, our model requires neither font labels nor fine-tuning during inference: users can simply provide cropped glyph patches corresponding to their desired typography, even if the font is not included in any standard library. Extensive experiments on multiple datasets, including handwrittent text benchmarks, SkyReels-Text achieves state-of-the-art performance in both text fidelity and visual realism, offering unprecedented control over font families, and stylistic nuances. This work bridges the gap between general-purpose image editing and professional-grade typographic design.

</details>


### [259] [CorrectAD: A Self-Correcting Agentic System to Improve End-to-end Planning in Autonomous Driving](https://arxiv.org/abs/2511.13297)
*Enhui Ma,Lijun Zhou,Tao Tang,Jiahuan Zhang,Junpeng Jiang,Zhan Zhang,Dong Han,Kun Zhan,Xueyang Zhang,XianPeng Lang,Haiyang Sun,Xia Zhou,Di Lin,Kaicheng Yu*

Main category: cs.CV

TL;DR: Proposes CorrectAD, a model-agnostic self-correcting pipeline for end-to-end autonomous driving that uses PM-Agent to specify data needs and DriveSora to synthesize 3D-conditioned, spatiotemporal videos; achieves substantial reduction in failures and collisions on nuScenes and an in-house dataset.


<details>
  <summary>Details</summary>
Motivation: Address robustness gaps in data-driven end-to-end planners caused by long-tail, safety-critical failure cases by enabling automated generation and annotation of targeted data conditioned on 3D layouts.

Method: Introduce PM-Agent to formulate data requirements; develop DriveSora to generate spatiotemporally consistent videos aligned with 3D annotations; integrate these into CorrectAD, a model-agnostic self-correcting pipeline that augments training data to improve end-to-end planners.

Result: CorrectAD corrected 62.5% and 49.8% of failure cases and reduced collision rates by 39% and 27% on nuScenes and an in-house dataset, respectively, across multiple end-to-end planners.

Conclusion: A data-centric, agentic approach can markedly improve robustness of end-to-end autonomous driving planners by synthesizing and annotating 3D-conditioned video data; the framework is model-agnostic and applicable to various planners, though success hinges on high-fidelity 3D-conditioned video generation.

Abstract: End-to-end planning methods are the de facto standard of the current autonomous driving system, while the robustness of the data-driven approaches suffers due to the notorious long-tail problem (i.e., rare but safety-critical failure cases). In this work, we explore whether recent diffusion-based video generation methods (a.k.a. world models), paired with structured 3D layouts, can enable a fully automated pipeline to self-correct such failure cases. We first introduce an agent to simulate the role of product manager, dubbed PM-Agent, which formulates data requirements to collect data similar to the failure cases. Then, we use a generative model that can simulate both data collection and annotation. However, existing generative models struggle to generate high-fidelity data conditioned on 3D layouts. To address this, we propose DriveSora, which can generate spatiotemporally consistent videos aligned with the 3D annotations requested by PM-Agent. We integrate these components into our self-correcting agentic system, CorrectAD. Importantly, our pipeline is an end-to-end model-agnostic and can be applied to improve any end-to-end planner. Evaluated on both nuScenes and a more challenging in-house dataset across multiple end-to-end planners, CorrectAD corrects 62.5% and 49.8% of failure cases, reducing collision rates by 39% and 27%, respectively.

</details>


### [260] [Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)](https://arxiv.org/abs/2511.13397)
*Nikos Theodoridis,Tim Brophy,Reenu Mohandas,Ganesh Sistu,Fiachra Collins,Anthony Scanlan,Ciaran Eising*

Main category: cs.CV

TL;DR: Introduces DTPQA, a distance-annotated VQA benchmark for traffic perception, with synthetic and real components, to isolate and evaluate perception in driving scenes across varying distances; includes dataset and scripts to generate more data.


<details>
  <summary>Details</summary>
Motivation: To ensure robust perception capabilities of vision-language models in safety-critical driving tasks, assess perception independent of higher-level reasoning, and study performance as distance to the queried object increases (including far-range 30+ meters).

Method: Construct Distance-Annotated Traffic Perception Question Answering benchmark (DTPQA) comprising two parts: DTP-Synthetic (synthetic/generated via a simulator) and DTP-Real (real traffic images). Each sample includes an image, a question, the ground-truth answer, and the distance to the object in question; release dataset plus Python scripts to generate additional data.

Result: The abstract focuses on dataset creation; no empirical results are reported. It provides a ready-to-use benchmark and tooling to evaluate distance-aware traffic perception in VLMs.

Conclusion: DTPQA offers a new, reproducible benchmark to evaluate distance-aware perception in driving contexts, enabling systematic analysis of how VLM perception degrades with increasing object distance, supported by downloadable data and data-generation scripts.

Abstract: The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.

</details>


### [261] [DriveLiDAR4D: Sequential and Controllable LiDAR Scene Generation for Autonomous Driving](https://arxiv.org/abs/2511.13309)
*Kaiwen Cai,Xinze Liu,Xia Zhou,Hengtong Hu,Jie Xiang,Luyao Zhang,Xueyang Zhang,Kun Zhan,Yifei Zhan,Xianpeng Lang*

Main category: cs.CV

TL;DR: DriveLiDAR4D introduces a multimodal, end-to-end LiDAR generation pipeline with sequential noise prediction (LiDAR4DNet) to produce temporally consistent scenes with controllable foreground objects and realistic backgrounds; claims state-of-the-art improvements over UniScene on nuScenes (FRD improvement 37.2%, FVD 24.1%).


<details>
  <summary>Details</summary>
Motivation: Address the lack of sequential generation and precise positioning of foreground/background in LiDAR generation, enabling end-to-end full-scene manipulation for data augmentation and evaluation in autonomous driving.

Method: Multimodal conditioning combined with a sequential noise prediction model (LiDAR4DNet) in an end-to-end LiDAR generation pipeline capable of producing temporally consistent scenes with controllable foreground objects and realistic backgrounds; trained/evaluated on nuScenes and KITTI; uses FRD and FVD metrics.

Result: FRD 743.13 and FVD 16.96 on nuScenes, surpassing the current SOTA UniScene by 37.2% in FRD and 24.1% in FVD.

Conclusion: First end-to-end method for sequential LiDAR scene generation with full scene manipulation; demonstrates significant performance gains and potential for realistic data generation; future work may explore generalization, efficiency, and broader evaluation across datasets.

Abstract: The generation of realistic LiDAR point clouds plays a crucial role in the development and evaluation of autonomous driving systems. Although recent methods for 3D LiDAR point cloud generation have shown significant improvements, they still face notable limitations, including the lack of sequential generation capabilities and the inability to produce accurately positioned foreground objects and realistic backgrounds. These shortcomings hinder their practical applicability. In this paper, we introduce DriveLiDAR4D, a novel LiDAR generation pipeline consisting of multimodal conditions and a novel sequential noise prediction model LiDAR4DNet, capable of producing temporally consistent LiDAR scenes with highly controllable foreground objects and realistic backgrounds. To the best of our knowledge, this is the first work to address the sequential generation of LiDAR scenes with full scene manipulation capability in an end-to-end manner. We evaluated DriveLiDAR4D on the nuScenes and KITTI datasets, where we achieved an FRD score of 743.13 and an FVD score of 16.96 on the nuScenes dataset, surpassing the current state-of-the-art (SOTA) method, UniScene, with an performance boost of 37.2% in FRD and 24.1% in FVD, respectively.

</details>


### [262] [TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing](https://arxiv.org/abs/2511.13399)
*Yuchen Bao,Yiting Wang,Wenjian Huang,Haowei Wang,Shen Chen,Taiping Yao,Shouhong Ding,Jianguo Zhang*

Main category: cs.CV

TL;DR: TripleFDS introduces a Scene Text Editing framework that disentangles triple features—text style, content, and background—using the SCB Synthesis data and SCB Group to enable robust, modular editing with state-of-the-art fidelity and accuracy, plus flexible operations like style replacement and background transfer.


<details>
  <summary>Details</summary>
Motivation: Current Scene Text Editing methods struggle to disentangle text style, text content, and background, limiting controllability and visual fidelity. A robust, modular disentanglement framework is needed to achieve semantic accuracy and reduce feature leakage.

Method: Propose TripleFDS, a disentangled modular framework for STE. Introduce SCB Synthesis and the SCB Group to facilitate triple-feature disentanglement. Employ inter-group contrastive regularization and intra-sample multi-feature orthogonality to enforce semantic separation. Use feature remapping during synthesis to avoid shortcut phenomena and leakage. Train on 125k SCB Groups.

Result: Achieves state-of-the-art image fidelity (SSIM 44.54) and text accuracy (ACC 93.58%) on mainstream STE benchmarks. Supports flexible operations such as style replacement and background transfer. Code released at provided GitHub link.

Conclusion: TripleFDS demonstrates effective triple-feature disentanglement in Scene Text Editing, enabling controllable, high-fidelity edits and new editing operations, backed by a specialized SCB-based dataset for robust training.

Abstract: Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the "SCB Group", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent "shortcut" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS

</details>


### [263] [YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection](https://arxiv.org/abs/2511.13344)
*Ori Meiraz,Sharon Shalev,Avishai Weizman*

Main category: cs.CV

TL;DR: A Mixture-of-Experts framework for object detection that uses adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mAP and AR than a single YOLOv9-T model.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of a single detector by allowing specialized experts to handle diverse object appearances and contexts via adaptive routing.

Method: Construct a Mixture-of-Experts system with multiple YOLOv9-T experts and a gating/router that dynamically routes input features to the most suitable expert, enabling feature specialization and joint training.

Result: Demonstrates higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.

Conclusion: Adaptive routing among YOLOv9-T experts improves object detection performance through dynamic feature specialization and expert collaboration.

Abstract: This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.

</details>


### [264] [Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline](https://arxiv.org/abs/2511.13442)
*Rui Zuo,Qinyue Tong,Zhe-Ming Lu,Ziqian Lu*

Main category: cs.CV

TL;DR: Foresee is a training-free MLLM-based pipeline for image forgery analysis that improves tamper localization and textual explanations, with strong generalization across tampering types using a type-prior and a Flexible Feature Detector (FFD) for copy-move.


<details>
  <summary>Details</summary>
Motivation: IFDL methods struggle to generalize and interpret across datasets; vanilla MLLMs show generalization potential but require expensive training; need for a training-free, lightweight approach that reveals MLLM capabilities in forensics.

Method: A training-free pipeline leveraging vanilla MLLMs, a type-prior-driven strategy, and a Flexible Feature Detector (FFD) to handle copy-move manipulations; no extra training; lightweight inference; produces localization and textual explanations.

Result: Extensive experiments show superior localization accuracy and richer textual explanations; better generalization across tampering types including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing.

Conclusion: Foresee demonstrates that vanilla MLLMs can be effectively deployed for image forgery analysis in a training-free, generalizable, and explainable manner; code release planned.

Abstract: With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.

</details>


### [265] [Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling](https://arxiv.org/abs/2511.13478)
*Adam Hazimeh,Ke Wang,Mark Collier,Gilles Baechler,Efi Kokiopoulou,Pascal Frossard*

Main category: cs.CV

TL;DR: SliDer uses vision-language models to semantically derender slide rasters into editable SVGs, with iterative refinement; introduces Slide2SVG dataset; achieves strong reconstruction fidelity and human preference over baselines.


<details>
  <summary>Details</summary>
Motivation: Current raster-to-vector methods rely on low-level primitives and fail to preserve high-level semantic structure in complex documents like slides, making edits difficult. There is a need for semantic derendering to restore editability of slide content.

Method: SliDer detects image and text elements in raster slides, extracts their attributes, and organizes them into a coherent SVG. It iteratively refines predictions during inference to better reconstruct the original raster, guided by a human-design-like process. The work also introduces Slide2SVG, a raster-SVG dataset of real-world slide documents to support research.

Result: Quantitative: reconstruction LPIPS of 0.069. Qualitative: human evaluators preferred SliDer in 82.9% of cases over the strongest zero-shot VLM baseline.

Conclusion: SliDer advances semantic derendering by producing compact, editable SVGs that faithfully reconstruct slides, with iterative refinement improving accuracy. The Slide2SVG dataset enables future work in this area.

Abstract: Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.

</details>


### [266] [What Color Is It? A Text-Interference Multimodal Hallucination Benchmark](https://arxiv.org/abs/2511.13400)
*Jinkun Zhao,Lei Huang,Wenjun Wu*

Main category: cs.CV

TL;DR: A dataset and analysis of color-induced visual hallucinations in multimodal LMs, with proposed robustness strategies.


<details>
  <summary>Details</summary>
Motivation: To mitigate hallucinations caused by color perception interference in multimodal language models (MLMs).

Method: Construct the What Color Is It dataset to trigger single-modality visual hallucinations, analyze the causes in MLMs, and propose mitigation solutions.

Result: Empirical evidence that color cues can trigger visual hallucinations in MLMs; identification of underlying causes; outline of potential robustness strategies.

Conclusion: Color-perception vulnerabilities are a risk for MLMs; addressing single-modality visual biases and color cues is necessary to enhance robustness of multimodal systems.

Abstract: With the rapid advancement of Large Models, numerous text-and-vision-fused Multimodal Large Models (MLMs) have emerged. However, these MLMs remain susceptible to informational interference in visual perception, particularly in color perception, which introduces an additional risk of hallucination. To validate this hypothesis, we introduce the "What Color Is It" dataset, a novel benchmark constructed using a simple method to trigger single-modality visual hallucination in MLMs. Based on this dataset, we further investigate the underlying causes of hallucination in the visual modality of MLMs and propose potential solutions to enhance their robustness.

</details>


### [267] [Delineate Anything Flow: Fast, Country-Level Field Boundary Detection from Any Source](https://arxiv.org/abs/2511.13417)
*Mykola Lavreniuk,Nataliia Kussul,Andrii Shelestov,Yevhenii Salii,Volodymyr Kuzin,Sergii Skakun,Zoltan Szantoi*

Main category: cs.CV

TL;DR: DelAnyFlow is a resolution-agnostic pipeline for large-scale field boundary mapping that combines a specialized instance segmentation model (DelAny with YOLOv11 backbone) trained on FBIS 22M with a post-processing and vectorization step to generate topologically consistent field boundaries, enabling national-scale delineation quickly and with improved boundary completeness over existing products.


<details>
  <summary>Details</summary>
Motivation: Accurate, complete, and scalable vector field boundaries are essential for land management, crop monitoring, and cadastral applications. Current methods often miss boundaries, merge adjacent fields, or fail to scale to national extents, especially in fragmented smallholder systems where high accuracy is needed.

Method: Proposes DelAnyFlow, integrating the DelAny instance segmentation model (YOLOv11 backbone) trained on the FBIS 22M dataset (672,909 image patches, 22.9M field instances) with a structured post-processing, merging, and vectorization pipeline to produce topologically consistent vector field boundaries. The approach is resolution-agnostic and demonstrated on Sentinel-2 data for 2024, aiming for national-scale delineation (e.g., Ukraine).

Result: DelAny achieves state-of-the-art accuracy with >100% higher mAP and 400x faster inference than SAM2. It shows strong zero-shot generalization and can delineate massive areas quickly (Ukraine: 603,000 km2 in under six hours on a single workstation). In Ukraine, it delineated 3.75M fields at 5m and 5.15M at 2.5m, outperforming Sinergise Solutions (2.66M) and NASA Harvest (1.69M). The dataset and code are publicly linked, supporting reproduction and national-scale deployment.

Conclusion: DelAnyFlow offers a scalable, cost-effective solution for field delineation in regions lacking digital cadastral data, with a large-scale dataset (FBIS 22M) and pre-trained model enabling rapid, accurate boundary generation. The work demonstrates practical utility for national land management and crop monitoring, though further evaluation across diverse geographies and imagery may be needed to confirm robustness and generalizability.

Abstract: Accurate delineation of agricultural field boundaries from satellite imagery is essential for land management and crop monitoring, yet existing methods often produce incomplete boundaries, merge adjacent fields, and struggle to scale. We present the Delineate Anything Flow (DelAnyFlow) methodology, a resolution-agnostic approach for large-scale field boundary mapping. DelAnyFlow combines the DelAny instance segmentation model, based on a YOLOv11 backbone and trained on the large-scale Field Boundary Instance Segmentation-22M (FBIS 22M) dataset, with a structured post-processing, merging, and vectorization sequence to generate topologically consistent vector boundaries. FBIS 22M, the largest dataset of its kind, contains 672,909 multi-resolution image patches (0.25-10m) and 22.9million validated field instances. The DelAny model delivers state-of-the-art accuracy with over 100% higher mAP and 400x faster inference than SAM2. DelAny demonstrates strong zero-shot generalization and supports national-scale applications: using Sentinel 2 data for 2024, DelAnyFlow generated a complete field boundary layer for Ukraine (603,000km2) in under six hours on a single workstation. DelAnyFlow outputs significantly improve boundary completeness relative to operational products from Sinergise Solutions and NASA Harvest, particularly in smallholder and fragmented systems (0.25-1ha). For Ukraine, DelAnyFlow delineated 3.75M fields at 5m and 5.15M at 2.5m, compared to 2.66M detected by Sinergise Solutions and 1.69M by NASA Harvest. This work delivers a scalable, cost-effective methodology for field delineation in regions lacking digital cadastral data. A project landing page with links to model weights, code, national-scale vector outputs, and dataset is available at https://lavreniuk.github.io/Delineate-Anything/.

</details>


### [268] [VOPE: Revisiting Hallucination of Vision-Language Models in Voluntary Imagination Task](https://arxiv.org/abs/2511.13420)
*Xingming Long,Jie Zhang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: Introduces VOPE, a presence-evaluation method to assess hallucinations in LVLMs during voluntary imagination tasks like story writing, by checking consistency between imagined objects and their presence in the image. Findings: LVLMs hallucinate heavily in voluntary imagination and current mitigation methods are largely ineffective in this setting.


<details>
  <summary>Details</summary>
Motivation: Current LVLM evaluation focuses on factual descriptions tied to images, but voluntary imagination tasks require novel content. It is unclear whether imagined content should count as hallucination; a dedicated evaluation for presence of imagined objects is needed.

Method: VOPE uses recheck-based questions to determine how the model interprets the presence of imagined objects in its own response. Presence consistency between imagined objects and the image is used to judge hallucination during response generation. Applied across multiple LVLMs and mitigation methods.

Result: Two key findings: (1) Most LVLMs hallucinate heavily during voluntary imagination, with poor presence-evaluation performance for imagined objects. (2) Existing hallucination mitigation methods show limited impact in voluntary imagination tasks.

Conclusion: VOPE provides a new evaluation direction for hallucinations in voluntary imagination; improving mitigation for imaginative outputs is an important area for future research.

Abstract: Most research on hallucinations in Large Vision-Language Models (LVLMs) focuses on factual description tasks that prohibit any output absent from the image. However, little attention has been paid to hallucinations in voluntary imagination tasks, e.g., story writing, where the models are expected to generate novel content beyond the given image. In these tasks, it is inappropriate to simply regard such imagined novel content as hallucinations. To address this limitation, we introduce Voluntary-imagined Object Presence Evaluation (VOPE)-a novel method to assess LVLMs' hallucinations in voluntary imagination tasks via presence evaluation. Specifically, VOPE poses recheck-based questions to evaluate how an LVLM interprets the presence of the imagined objects in its own response. The consistency between the model's interpretation and the object's presence in the image is then used to determine whether the model hallucinates when generating the response. We apply VOPE to several mainstream LVLMs and hallucination mitigation methods, revealing two key findings: (1) most LVLMs hallucinate heavily during voluntary imagination, and their performance in presence evaluation is notably poor on imagined objects; (2) existing hallucination mitigation methods show limited effect in voluntary imagination tasks, making this an important direction for future research.

</details>


### [269] [Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks](https://arxiv.org/abs/2511.13545)
*Md. Iqbal Hossain,Afia Sajeeda,Neeresh Kumar Perla,Ming Shao*

Main category: cs.CV

TL;DR: A method to defend CLIP-like multimodal models from backdoor attacks by using an image segmentation oracle to detect triggers, identify affected labels and victim samples, and curate a compact fine-tuning dataset to rectify the model, showing effectiveness on visual recognition benchmarks.


<details>
  <summary>Details</summary>
Motivation: Backdoor attacks threaten multimodal models (e.g., CLIP) by subtly manipulating behavior. Existing defenses often require retraining from scratch or large data and do not pinpoint which labels are affected. A targeted defense that identifies triggers and affected data is desirable.

Method: Introduce an image segmentation oracle as a supervising component for a poisoned CLIP. Develop two algorithms: (1) separate CLIP knowledge from Oracle's knowledge to identify potential triggers, and (2) locate affected labels and victim samples to curate a compact fine-tuning dataset. Use this to rectify the poisoned CLIP model and negate backdoor effects.

Result: Extensive experiments on visual recognition benchmarks demonstrate the proposed strategy is effective for defending CLIP-based backdoor attacks.

Conclusion: The work provides a targeted defense for multimodal contrastive learning models by combining trigger/detection via an oracle and selective fine-tuning data, enabling robust backdoor mitigation without broad retraining.

Abstract: The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.

</details>


### [270] [FUSE: A Flow-based Mapping Between Shapes](https://arxiv.org/abs/2511.13431)
*Lorenzo Olearo,Giulio Viganò,Daniele Baieri,Filippo Maggioli,Simone Melzi*

Main category: cs.CV

TL;DR: A flow-based, invertible approach to map between 3D shapes across representations (point clouds, meshes, SDFs, volumes) that requires no large-scale training and achieves high coverage/accuracy; it enables cross-representation matching and tasks like UV mapping and registration.


<details>
  <summary>Details</summary>
Motivation: To enable efficient, data-efficient, cross-representation shape matching without relying on extensive data-driven training, while supporting multiple 3D representations.

Method: Represent each shape as a probability distribution via a continuous invertible flow from a fixed anchor distribution. For a source and target, compose the inverse flow (source→anchor) with the forward flow (anchor→target) to map points between surfaces. Use pointwise task-tailored embeddings to obtain an invertible, modality-agnostic map that works across point clouds, meshes, SDFs, and volumes.

Result: The method yields high coverage and accuracy across diverse benchmarks and challenging settings for shape matching and also shows promise for related tasks such as UV mapping and registration of raw point cloud scans of human bodies.

Conclusion: The framework provides an efficient, invertible, cross-representation matching mechanism that operates across multiple 3D data types and can be extended to related tasks, reducing reliance on large datasets while maintaining performance.

Abstract: We introduce a novel neural representation for maps between 3D shapes based on flow-matching models, which is computationally efficient and supports cross-representation shape matching without large-scale training or data-driven procedures. 3D shapes are represented as the probability distribution induced by a continuous and invertible flow mapping from a fixed anchor distribution. Given a source and a target shape, the composition of the inverse flow (source to anchor) with the forward flow (anchor to target), we continuously map points between the two surfaces. By encoding the shapes with a pointwise task-tailored embedding, this construction provides an invertible and modality-agnostic representation of maps between shapes across point clouds, meshes, signed distance fields (SDFs), and volumetric data. The resulting representation consistently achieves high coverage and accuracy across diverse benchmarks and challenging settings in shape matching. Beyond shape matching, our framework shows promising results in other tasks, including UV mapping and registration of raw point cloud scans of human bodies.

</details>


### [271] [Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification](https://arxiv.org/abs/2511.13575)
*Linhan Zhou,Shuang Li,Neng Dong,Yonghang Tai,Yafei Zhang,Huafeng Li*

Main category: cs.CV

TL;DR: Proposes Hierarchical Prompt Learning (HPL) for joint image-to-image and text-to-image person re-identification using a Task-Routed Transformer and hierarchical prompts to align modalities and improve discrimination.


<details>
  <summary>Details</summary>
Motivation: I2I requires discriminative identity learning while T2I requires accurate cross-modal semantic alignment; treating them separately can cause representation entanglement and suboptimal performance.

Method: Introduce a Task-Routed Transformer with dual classification tokens in a shared visual encoder to route features for I2I and T2I branches. Develop a hierarchical prompt generation scheme combining identity-level learnable tokens with instance-level pseudo-text tokens derived via modality-specific inversion networks to inject fine-grained semantics. Apply Cross-Modal Prompt Regularization to align prompts semantically while preserving source-modality characteristics.

Result: Extensive experiments on multiple ReID benchmarks show state-of-the-art performance on both I2I and T2I tasks.

Conclusion: The unified HPL framework effectively mitigates representation entanglement, enabling joint optimization of I2I and T2I with improved cross-modal transfer; the combination of task-routing, prompts, and regularization yields strong performance across benchmarks.

Abstract: Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.

</details>


### [272] [VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping](https://arxiv.org/abs/2511.13587)
*Haotian Dong,Ye Li,Rongwei Lu,Chen Tang,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: A novel SD framework VVS for visual autoregressive generation that skips verification steps, reducing forward passes by 2.8× while preserving quality, via (i) verification-free token selection with dynamic truncation, (ii) token-level caching/reuse, and (iii) fine-grained skipped-step scheduling.


<details>
  <summary>Details</summary>
Motivation: Visual autoregressive models suffer high inference latency due to next-token prediction. Speculative decoding accelerates inference but still requires drafting and verification steps, limiting pass reductions. There is potential to further speed up by skipping verification upfront, leveraging token interchangeability and feature reuse to maintain quality.

Method: Introduce VVS, a speculative decoding framework with three components: (1) a verification-free token selector with dynamic truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped-step scheduling. The paper analyzes the drafting stage to identify verification redundancy and stale feature reusability as key factors for preserving quality during verification-free steps.

Result: VVS reduces the number of target model forward passes by 2.8× compared to vanilla AR decoding, while achieving competitive generation quality and providing a better speed-quality trade-off than standard SD approaches.

Conclusion: Verification skipping in the SD process for visual AR models can substantially reduce latency without sacrificing quality, potentially reshaping the speculative decoding paradigm.

Abstract: Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its "draft one step, then verify one step" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.

</details>


### [273] [InterMoE: Individual-Specific 3D Human Interaction Generation via Dynamic Temporal-Selective MoE](https://arxiv.org/abs/2511.13488)
*Lipeng Wang,Hongxing Fan,Haohua Chen,Zehuan Huang,Lu Sheng*

Main category: cs.CV

TL;DR: InterMoE is a dynamic Temporal-Selective Mixture of Experts for 3D human interaction generation that uses joint text semantics and motion context to preserve individual identities while achieving high semantic fidelity, setting state-of-the-art results with notable FID reductions on InterHuman and InterX datasets.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing methods in preserving unique individual characteristics and strictly following textual descriptions during 3D human interaction generation.

Method: InterMoE employs a Dynamic Temporal-Selective Mixture of Experts with a routing mechanism that fuses high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts. This enables dynamic selection capacity and focus on critical temporal features, preserving identity while maintaining semantic fidelity.

Result: The approach achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on the InterHuman dataset and 22% on InterX.

Conclusion: InterMoE effectively balances identity preservation with semantic fidelity in 3D human interaction generation through a dynamic routing mechanism and expert specialization.

Abstract: Generating high-quality human interactions holds significant value for applications like virtual reality and robotics. However, existing methods often fail to preserve unique individual characteristics or fully adhere to textual descriptions. To address these challenges, we introduce InterMoE, a novel framework built on a Dynamic Temporal-Selective Mixture of Experts. The core of InterMoE is a routing mechanism that synergistically uses both high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts. This allows experts to dynamically determine the selection capacity and focus on critical temporal features, thereby preserving specific individual characteristic identities while ensuring high semantic fidelity. Extensive experiments show that InterMoE achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on the InterHuman dataset and 22% on InterX.

</details>


### [274] [Alpha Divergence Losses for Biometric Verification](https://arxiv.org/abs/2511.13621)
*Dimitrios Koutsianos,Ladislav Mosner,Yannis Panagakis,Themos Stafylakis*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Performance in face and speaker verification is largely driven by margin based softmax losses like CosFace and ArcFace. Recently introduced $α$-divergence loss functions offer a compelling alternative, particularly for their ability to induce sparse solutions (when $α>1$). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based $α$-divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a critical training instability in A3M-caused by the interplay of penalized logits and sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is crucial for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount.

</details>


### [275] [Language-Guided Invariance Probing of Vision-Language Models](https://arxiv.org/abs/2511.13494)
*Jae Joong Lee*

Main category: cs.CV

TL;DR: LGIP benchmarks invariance and semantic sensitivity of vision-language models to paraphrase- and flip-based perturbations in image-caption matching.


<details>
  <summary>Details</summary>
Motivation: to quantify linguistic robustness of VLMs beyond standard retrieval accuracy using controlled meaning-preserving and meaning-changing perturbations.

Method: Use 40k MS COCO images with five human captions each; auto-generate paraphrases and rule-based flips altering object category, color, or count; measure invariance error, semantic sensitivity gap, and positive-rate statistic; evaluate nine VLMs including EVA02-CLIP and large OpenCLIP variants, SigLIP/SigLIP2.

Result: EVA02-CLIP and large OpenCLIP variants achieve favorable invariance-sensitivity frontier with low paraphrase variance and consistently higher scores for original captions than for flipped ones; SigLIP and SigLIP2 exhibit larger invariance error and often prefer flipped captions to human descriptions, especially for object and color edits; standard retrieval metrics miss these failures.

Conclusion: LGIP provides a model-agnostic diagnostic of linguistic robustness in VLMs, revealing substantial differences in linguistic robustness not captured by traditional accuracy metrics.

Abstract: Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.
  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.

</details>


### [276] [Mapping the Vanishing and Transformation of Urban Villages in China](https://arxiv.org/abs/2511.13507)
*Wenyu Zhang,Yao Tong,Yiqiu Liu,Rui Cao*

Main category: cs.CV

TL;DR: A deep-learning framework uses semantic segmentation on multi-temporal remote sensing to monitor urban village changes and classify post-demolition land use into six phases, revealing prolonged redevelopment, peripheral transitions, and three transformation pathways to inform context-sensitive urban renewal.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic evaluation of whether demolished UV land is effectively reused and to inform sustainable, inclusive redevelopment policy in China and beyond.

Method: Apply semantic segmentation on multi-temporal remote sensing imagery to map evolving UV boundaries; classify post-demolition land use into six categories (incomplete demolition, vacant land, construction sites, buildings, green spaces, others) based on remained-demolished-redeveloped phases; analyze four cities across China (Guangzhou, Zhengzhou, Xi'an, Harbin) to identify spatiotemporal transformation pathways.

Result: Findings show prolonged redevelopment in UVs; transitions mainly in peripheral areas with cores stable; three transformation pathways identified: synchronized redevelopment, delayed redevelopment, gradual optimization; implications for tiered, context-sensitive planning and sustainable urban renewal.

Conclusion: The study reveals the fragmented, nonlinear nature of UV redevelopment and demonstrates how linking spatial dynamics with redevelopment policies yields empirical insights that can improve inclusivity, efficiency, and sustainability of urban renewal and contribute to global understanding of informal settlement transformations.

Abstract: Urban villages (UVs), informal settlements embedded within China's urban fabric, have undergone widespread demolition and redevelopment in recent decades. However, there remains a lack of systematic evaluation of whether the demolished land has been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices. To address the gap, this study proposes a deep learning-based framework to monitor the spatiotemporal changes of UVs in China. Specifically, semantic segmentation of multi-temporal remote sensing imagery is first used to map evolving UV boundaries, and then post-demolition land use is classified into six categories based on the "remained-demolished-redeveloped" phase: incomplete demolition, vacant land, construction sites, buildings, green spaces, and others. Four representative cities from China's four economic regions were selected as the study areas, i.e., Guangzhou (East), Zhengzhou (Central), Xi'an (West), and Harbin (Northeast). The results indicate: 1) UV redevelopment processes were frequently prolonged; 2) redevelopment transitions primarily occurred in peripheral areas, whereas urban cores remained relatively stable; and 3) three spatiotemporal transformation pathways, i.e., synchronized redevelopment, delayed redevelopment, and gradual optimization, were revealed. This study highlights the fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies. By linking spatial dynamics with the context of redevelopment policies, the findings offer valuable empirical insights that support more inclusive, efficient, and sustainable urban renewal, while also contributing to a broader global understanding of informal settlement transformations.

</details>


### [277] [Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems](https://arxiv.org/abs/2511.13533)
*Jeffrey Wen,Rizwan Ahmad,Philip Schniter*

Main category: cs.CV

TL;DR: Proposes an asymptotically minimax multi-target conformal prediction method to provide tight, jointly valid prediction intervals for ill-posed imaging inverse problems, with applications to multi-metric IQA, multi-task UQ, and multi-round acquisition, demonstrated on synthetic and MRI data.


<details>
  <summary>Details</summary>
Motivation: Uncertainty quantification is essential in ill-posed imaging inverse problems, especially for safety-critical applications. Existing conformal prediction handles only scalar targets; practical problems require multiple, interacting targets with reliable joint coverage.

Method: Develop an asymptotically minimax multi-target conformal prediction framework that yields tight prediction intervals with joint marginal coverage. The paper also outlines how to apply this framework to: (i) multi-metric blind image quality assessment (BIQA), (ii) multi-task uncertainty quantification, and (iii) multi-round measurement acquisition.

Result: Numerical demonstrations show benefits over existing multi-target conformal prediction methods, using synthetic data and magnetic resonance imaging (MRI), in terms of interval tightness and valid coverage under joint targets.

Conclusion: The proposed minimax multi-target CP framework provides reliable, tight, joint-coverage uncertainty quantification for multi-target imaging problems and is adaptable to a range of practical applications in imaging inverse problems.

Abstract: In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data.

</details>


### [278] [Accuracy is Not Enough: Poisoning Interpretability in Federated Learning via Color Skew](https://arxiv.org/abs/2511.13535)
*Farhin Farhad Riya,Shahinul Hoque,Jinyuan Stella Sun,Olivera Kotevska*

Main category: cs.CV

TL;DR: A federated-learning attack (Chromatic Perturbation Module) degrades saliency explanations with small color perturbations while preserving model accuracy; Grad-CAM overlap drops up to 35% across datasets; vulnerability persists across training rounds and FL makes detection harder.


<details>
  <summary>Details</summary>
Motivation: To show that interpretability can be attacked as a security risk, challenging the assumption that correct predictions guarantee faithful explanations, especially in safety-critical and federated settings.

Method: Propose Chromatic Perturbation Module that perturbs color contrast between foreground and background to misalign saliency maps; applied by adversarial clients across FL rounds; evaluates using Grad-CAM overlaps on multiple datasets.

Result: Explainability is degraded (up to 35% reduction in peak Grad-CAM activation overlap) while accuracy remains high (>96%) across all datasets; FL setting enables stealthy perturbations and standard pipelines struggle to detect such degradation.

Conclusion: Interpretability can be compromised as an attack surface; defenses and robust explanation methods are needed, particularly in federated learning where subtle color perturbations are hard to detect; auditing should account for explanation integrity, not just accuracy.

Abstract: As machine learning models are increasingly deployed in safety-critical domains, visual explanation techniques have become essential tools for supporting transparency. In this work, we reveal a new class of attacks that compromise model interpretability without affecting accuracy. Specifically, we show that small color perturbations applied by adversarial clients in a federated learning setting can shift a model's saliency maps away from semantically meaningful regions while keeping the prediction unchanged. The proposed saliency-aware attack framework, called Chromatic Perturbation Module, systematically crafts adversarial examples by altering the color contrast between foreground and background in a way that disrupts explanation fidelity. These perturbations accumulate across training rounds, poisoning the global model's internal feature attributions in a stealthy and persistent manner. Our findings challenge a common assumption in model auditing that correct predictions imply faithful explanations and demonstrate that interpretability itself can be an attack surface. We evaluate this vulnerability across multiple datasets and show that standard training pipelines are insufficient to detect or mitigate explanation degradation, especially in the federated learning setting, where subtle color perturbations are harder to discern. Our attack reduces peak activation overlap in Grad-CAM explanations by up to 35% while preserving classification accuracy above 96% on all evaluated datasets.

</details>


### [279] [BootOOD: Self-Supervised Out-of-Distribution Detection via Synthetic Sample Exposure under Neural Collapse](https://arxiv.org/abs/2511.13539)
*Yuanchao Wang,Tian Qin,Eduardo Valle,Bruno Abrahao*

Main category: cs.CV

TL;DR: BootOOD is a fully self-supervised OOD detector that uses ID data to synthesize pseudo-OOD features, leverages Neural Collapse, and uses a radius-based feature-norm auxiliary head to detect OOD, decoupled from the main classifier; it achieves strong performance on CIFAR-10/100 and ImageNet-200, challenging semantically similar OOD without outlier exposure.


<details>
  <summary>Details</summary>
Motivation: OOD detection is critical in safety-sensitive deployments; existing detectors struggle with semantically similar OOD; need a self-supervised method that can handle challenging OOD without relying on external outliers.

Method: Synthesizes pseudo-OOD features via simple transformations of ID representations; exploits Neural Collapse for ID feature clustering; adds a lightweight auxiliary head performing radius-based classification on feature norms to separate OOD; decouples OOD detection from main classifier; relaxes requirement to OD having larger norms, instead encouraging OOD to have smaller norms.

Result: BootOOD outperforms prior post-hoc methods, surpasses training-based methods without outlier exposure, and is competitive with state-of-the-art outlier-exposure methods while maintaining or improving ID accuracy; validated on CIFAR-10, CIFAR-100, ImageNet-200.

Conclusion: A lightweight, self-supervised, norm-based OOD detector that effectively handles semantically close OOD by learning smaller feature norms and decoupling OOD detection, achieving strong empirical performance without external outliers.

Abstract: Out-of-distribution (OOD) detection is critical for deploying image classifiers in safety-sensitive environments, yet existing detectors often struggle when OOD samples are semantically similar to the in-distribution (ID) classes. We present BootOOD, a fully self-supervised OOD detection framework that bootstraps exclusively from ID data and is explicitly designed to handle semantically challenging OOD samples. BootOOD synthesizes pseudo-OOD features through simple transformations of ID representations and leverages Neural Collapse (NC), where ID features cluster tightly around class means with consistent feature norms. Unlike prior approaches that aim to constrain OOD features into subspaces orthogonal to the collapsed ID means, BootOOD introduces a lightweight auxiliary head that performs radius-based classification on feature norms. This design decouples OOD detection from the primary classifier and imposes a relaxed requirement: OOD samples are learned to have smaller feature norms than ID features, which is easier to satisfy when ID and OOD are semantically close. Experiments on CIFAR-10, CIFAR-100, and ImageNet-200 show that BootOOD outperforms prior post-hoc methods, surpasses training-based methods without outlier exposure, and is competitive with state-of-the-art outlier-exposure approaches while maintaining or improving ID accuracy.

</details>


### [280] [TSE-Net: Semi-supervised Monocular Height Estimation from Single Remote Sensing Images](https://arxiv.org/abs/2511.13552)
*Sining Chen,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: Introduces TSE-Net, a semi-supervised framework for monocular height estimation using a teacher–student–exam pipeline. A joint regression/classification teacher generates pseudo-labels for unlabeled data, with hierarchical height classes and Plackett-Luce calibration to filter pseudo-labels. The student learns from unlabeled data; the exam network temporally ensembles the student for stability. Evaluated on three datasets with differing resolutions and modalities; code released.


<details>
  <summary>Details</summary>
Motivation: Labeled height data for monocular 3D height estimation are scarce and costly to obtain at scale. Unlabeled remote sensing data are abundant, offering an opportunity for semi-supervised learning to improve model generalization and performance.

Method: Proposes TSE-Net: a teacher–student–exam architecture. The teacher performs joint regression (heights) and classification (height classes) to produce pseudo-labels. Height classes are defined via a hierarchical bi-cut strategy to balance long-tailed distributions. Predicted class probabilities are calibrated with a Plackett–Luce model to reflect pseudo-label reliability. The student is trained on unlabeled data using these pseudo-labels, while the exam network acts as a temporal ensemble of the student to stabilize predictions. Evaluation across three datasets with varying resolutions and imaging modalities; code available at GitHub.

Result: The approach demonstrates the effectiveness of semi-supervised learning for monocular height estimation across diverse datasets, leveraging unlabeled data to improve prediction quality and robustness. Specific quantitative gains are not provided in the abstract but the setup shows promising generalization across modalities.

Conclusion: Semi-supervised learning with a teacher–student–exam framework, hierarchical height classes, and probability calibration can enhance monocular height estimation from remote sensing data, enabling better generalization with unlabeled data. The authors release code for replication.

Abstract: Monocular height estimation plays a critical role in 3D perception for remote sensing, offering a cost-effective alternative to multi-view or LiDAR-based methods. While deep learning has significantly advanced the capabilities of monocular height estimation, these methods remain fundamentally limited by the availability of labeled data, which are expensive and labor-intensive to obtain at scale. The scarcity of high-quality annotations hinders the generalization and performance of existing models. To overcome this limitation, we propose leveraging large volumes of unlabeled data through a semi-supervised learning framework, enabling the model to extract informative cues from unlabeled samples and improve its predictive performance. In this work, we introduce TSE-Net, a self-training pipeline for semi-supervised monocular height estimation. The pipeline integrates teacher, student, and exam networks. The student network is trained on unlabeled data using pseudo-labels generated by the teacher network, while the exam network functions as a temporal ensemble of the student network to stabilize performance. The teacher network is formulated as a joint regression and classification model: the regression branch predicts height values that serve as pseudo-labels, and the classification branch predicts height value classes along with class probabilities, which are used to filter pseudo-labels. Height value classes are defined using a hierarchical bi-cut strategy to address the inherent long-tailed distribution of heights, and the predicted class probabilities are calibrated with a Plackett-Luce model to reflect the expected accuracy of pseudo-labels. We evaluate the proposed pipeline on three datasets spanning different resolutions and imaging modalities. Codes are available at https://github.com/zhu-xlab/tse-net.

</details>


### [281] [Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation](https://arxiv.org/abs/2511.13571)
*Ziyang Huang,Jiagang Chen,Jin Liu,Shunping Ji*

Main category: cs.CV

TL;DR: Opt3DGS introduces a two-stage optimization for 3D Gaussian Splatting: adaptive weighted SGLD exploration to escape local optima, followed by curvature-guided quasi-Newton (Local QN) direction-aware Adam exploitation, achieving state-of-the-art rendering without altering the underlying representation.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3DGS) faces two main optimization challenges: getting trapped in suboptimal local optima and lacking robust convergence quality. A more robust optimization framework is needed to improve rendering quality without changing the 3DGS representation.

Method: Stage 1 (exploration): Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) to enhance global search and escape local optima. Stage 2 (exploitation): Local Quasi-Newton Direction-guided Adam optimizer that uses curvature information to guide parameter updates for precise and efficient convergence.

Result: Extensive experiments across diverse benchmark datasets show that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.

Conclusion: Opt3DGS provides a robust two-stage optimization framework that significantly improves 3DGS performance by combining adaptive exploration with curvature-informed exploitation, offering better convergence and rendering quality without altering the core representation.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading framework for novel view synthesis, yet its core optimization challenges remain underexplored. We identify two key issues in 3DGS optimization: entrapment in suboptimal local optima and insufficient convergence quality. To address these, we propose Opt3DGS, a robust framework that enhances 3DGS through a two-stage optimization process of adaptive exploration and curvature-guided exploitation. In the exploration phase, an Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) method enhances global search to escape local optima. In the exploitation phase, a Local Quasi-Newton Direction-guided Adam optimizer leverages curvature information for precise and efficient convergence. Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.

</details>


### [282] [UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity](https://arxiv.org/abs/2511.13714)
*Junwei Yu,Trevor Darrell,XuDong Wang*

Main category: cs.CV

TL;DR: UnSAMv2 enables continuous, annotation-free control over segmentation granularity for SAM, via self-supervised granularity discovery and a granularity embedding, achieving large gains with minimal unlabeled data and parameters.


<details>
  <summary>Details</summary>
Motivation: Current Segment Anything Model (SAM) struggles to control segmentation granularity; refining results with prompts or masks is expensive, ambiguous, and requires dense annotations that are impractical to collect.

Method: Extend UnSAM's divide-and-conquer approach to discover abundant mask-granularity pairs from unlabeled data and introduce a granularity control embedding that enables precise, continuous control over segmentation scale; train with a small unlabeled dataset (6K images) and a negligible parameter increase (0.02%).

Result: NoC90 improves from 5.69 to 4.75; 1-IoU from 58.0 to 73.1; AR1000 from 49.6 to 68.3; evaluated on >11 benchmarks; capable of interactive, whole-image, and video segmentation with better granularity control using minimal unlabeled data.

Conclusion: Small amounts of unlabeled data combined with a granularity-aware self-supervised learning approach can unlock the potential of vision foundation models, enabling segment-anything at any granularity with SAM.

Abstract: The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\text{NoC}_{90}$ (5.69 $\rightarrow$ 4.75), 1-IoU (58.0 $\rightarrow$ 73.1), and $\text{AR}_{1000}$ (49.6 $\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.

</details>


### [283] [Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images](https://arxiv.org/abs/2511.13586)
*Yinuo Xu,Yan Cui,Mingyao Li,Zhi Huang*

Main category: cs.CV

TL;DR: NuClass is a two-path, uncertainty-guided multi-scale framework for cell-wise histopathology classification that fuses nuclear morphology and tissue context, using a marker-guided Xenium-derived dataset, achieving up to 96% F1 and improving cell-level predictions.


<details>
  <summary>Details</summary>
Motivation: Current tile-based models capture fine-grained nuclear morphology but neglect broader tissue context; annotations are coarse and uneven, hindering fine-grained subtype supervision. There is a need to bridge slide-level foundation models and reliable cell-level phenotypes while enabling interpretability.

Method: NuClass comprises Path local (nuclear morphology from 224x224 crops) and Path global (surrounding 1024x1024 neighborhood). A learnable gating module balances local vs. global information. An uncertainty-guided objective directs the global path to emphasize regions where the local path is uncertain. Calibrated confidence estimates and Grad-CAM visualizations are provided for interpretability. The training data is a marker-guided dataset derived from Xenium spatial transcriptomics, yielding single-cell labels for >2 million cells across eight organs and 16 classes. Evaluation on three fully held-out cohorts.

Result: NuClass achieves up to 96% F1 on its best-performing class, outperforming strong baselines. The multi-scale, uncertainty-aware fusion improves cell-level predictions and demonstrates complementary learning between local morphology and microenvironmental context.

Conclusion: Multi-scale, uncertainty-aware fusion can bridge the gap between slide-level foundation models and reliable, cell-level phenotype prediction, with added interpretability through calibrated confidences and Grad-CAM.

Abstract: Identifying cell types and subtypes from routine histopathology images is essential for improving the computational understanding of human disease. Existing tile-based models can capture detailed nuclear morphology but often fail to incorporate the broader tissue context that influences a cell's function and identity. In addition, available human annotations are typically coarse-grained and unevenly distributed across studies, making fine-grained subtype-level supervision difficult to obtain.
  To address these limitations, we introduce NuClass, a pathologist workflow inspired framework for cell-wise multi-scale integration of nuclear morphology and microenvironmental context. NuClass includes two main components: Path local, which focuses on nuclear morphology from 224-by-224 pixel crops, and Path global, which models the surrounding 1024-by-1024 pixel neighborhood. A learnable gating module adaptively balances local detail and contextual cues. To encourage complementary learning, we incorporate an uncertainty-guided objective that directs the global path to prioritize regions where the local path is uncertain. We also provide calibrated confidence estimates and Grad-CAM visualizations to enhance interpretability.
  To overcome the lack of high-quality annotations, we construct a marker-guided dataset from Xenium spatial transcriptomics assays, yielding single-cell resolution labels for more than two million cells across eight organs and 16 classes. Evaluated on three fully held-out cohorts, NuClass achieves up to 96 percent F1 for its best-performing class, outperforming strong baselines. Our results show that multi-scale, uncertainty-aware fusion can bridge the gap between slide-level pathological foundation models and reliable, cell-level phenotype prediction.

</details>


### [284] [ICLR: Inter-Chrominance and Luminance Interaction for Natural Color Restoration in Low-Light Image Enhancement](https://arxiv.org/abs/2511.13607)
*Xin Xu,Hao Liu,Wei Liu,Wei Wang,Jiayi Wu,Kui Jiang*

Main category: cs.CV

TL;DR: Proposes ICLR framework for low-light image enhancement leveraging Inter-Chrominance and Luminance Interaction to overcome inter-branch distribution differences and gradient conflicts using DIEM and CCL, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: HVI color space decouples chrominance and luminance but interaction between branches suffers due to distributional differences; luminance errors propagate to chrominance through nonlinear parameter; weak chrominance correlation in homogeneous regions; traditional pixel-wise losses enforce inter-branch correlations causing gradient conflicts. There is a need to enhance cross-branch collaboration and stabilize optimization for LLIE.

Method: Introduce ICLR framework with Dual-stream Interaction Enhancement Module (DIEM) for improved fusion and enhancement of luminance and chrominance streams; Covariance Correction Loss (CCL) penalizes chrominance errors using luminance residual statistics and constrains covariance between chrominance branches to balance gradients and promote complementary information.

Result: Experimental results on multiple LLIE datasets show the proposed method outperforms state-of-the-art methods.

Conclusion: ICLR effectively enables inter- and intra-branch interactions in the HVI color space, improving LLIE performance via DIEM and CCL, with strong empirical validation across diverse datasets.

Abstract: Low-Light Image Enhancement (LLIE) task aims at improving contrast while restoring details and textures for images captured in low-light conditions. HVI color space has made significant progress in this task by enabling precise decoupling of chrominance and luminance. However, for the interaction of chrominance and luminance branches, substantial distributional differences between the two branches prevalent in natural images limit complementary feature extraction, and luminance errors are propagated to chrominance channels through the nonlinear parameter. Furthermore, for interaction between different chrominance branches, images with large homogeneous-color regions usually exhibit weak correlation between chrominance branches due to concentrated distributions. Traditional pixel-wise losses exploit strong inter-branch correlations for co-optimization, causing gradient conflicts in weakly correlated regions. Therefore, we propose an Inter-Chrominance and Luminance Interaction (ICLR) framework including a Dual-stream Interaction Enhancement Module (DIEM) and a Covariance Correction Loss (CCL). The DIEM improves the extraction of complementary information from two dimensions, fusion and enhancement, respectively. The CCL utilizes luminance residual statistics to penalize chrominance errors and balances gradient conflicts by constraining chrominance branches covariance. Experimental results on multiple datasets show that the proposed ICLR framework outperforms state-of-the-art methods.

</details>


### [285] [AtlasMorph: Learning conditional deformable templates for brain MRI](https://arxiv.org/abs/2511.13609)
*Marianne Rakic,Andrew Hoopes,S. Mazdak Abulnaga,Mert R. Sabuncu,John V. Guttag,Adrian V. Dalca*

Main category: cs.CV

TL;DR: A learned, conditional template-building framework uses convolutional registration networks to generate population-representative 3D brain templates conditioned on subject attributes (e.g., age, sex), optionally producing segmentation maps and enabling direct registration of new subjects; it outperforms traditional templates and unconditional methods.


<details>
  <summary>Details</summary>
Motivation: Template construction for medical imaging is computationally expensive and often yields templates that poorly represent diverse populations. There is a need for efficient, representative templates that capture population heterogeneity and support accurate registration and segmentation.

Method: Train convolutional registration neural networks to output templates conditioned on subject-specific attributes (age, sex). Use available segmentations to derive anatomical segmentation maps for templates. The network can also register new subject images to the learned templates.

Result: The method learned high-quality, population-representative templates on 3D brain MRI datasets. Conditioned (annotated) templates improved registration performance over unlabeled unconditional templates and outperformed existing template-construction methods.

Conclusion: Conditional, learned templates enable better population representation and registration accuracy, providing an efficient approach to template construction and alignment for computational anatomy studies.

Abstract: Deformable templates, or atlases, are images that represent a prototypical anatomy for a population, and are often enhanced with probabilistic anatomical label maps. They are commonly used in medical image analysis for population studies and computational anatomy tasks such as registration and segmentation. Because developing a template is a computationally expensive process, relatively few templates are available. As a result, analysis is often conducted with sub-optimal templates that are not truly representative of the study population, especially when there are large variations within this population. We propose a machine learning framework that uses convolutional registration neural networks to efficiently learn a function that outputs templates conditioned on subject-specific attributes, such as age and sex. We also leverage segmentations, when available, to produce anatomical segmentation maps for the resulting templates. The learned network can also be used to register subject images to the templates. We demonstrate our method on a compilation of 3D brain MRI datasets, and show that it can learn high-quality templates that are representative of populations. We find that annotated conditional templates enable better registration than their unlabeled unconditional counterparts, and outperform other templates construction methods.

</details>


### [286] [Tissue Aware Nuclei Detection and Classification Model for Histopathology Images](https://arxiv.org/abs/2511.13615)
*Kesi Xu,Eleni Chiou,Ali Varamesh,Laura Acqualagna,Nasir Rajpoot*

Main category: cs.CV

TL;DR: TAND jointly detects and classifies nuclei using point-level supervision with tissue-mask conditioning, achieving state-of-the-art results by modulating classification with multi-scale Spatial-FiLM guided by a frozen tissue segmentation branch.


<details>
  <summary>Details</summary>
Motivation: Current nuclei detection/classification in pathology relies on detailed annotations and often ignores tissue context; incorporating tissue information can improve per-cell classification, especially for tissue-dependent cell types.

Method: A ConvNeXt-based encoder-decoder for nuclei detection with a frozen Virchow-2 tissue segmentation branch. Tissue probabilities modulate the classification stream via a novel multi-scale Spatial-FiLM, enabling tissue-aware per-cell classification under point-level supervision.

Result: On the PUMA benchmark, achieves state-of-the-art performance, outperforming tissue-agnostic baselines and mask-supervised methods, with notable gains for tissue-dependent cell types such as epithelium, endothelium, and stroma.

Conclusion: First method to condition per-cell classification on learned tissue masks, reducing annotation burden and offering a practical pathway for tissue-aware nuclei analysis in computational pathology.

Abstract: Accurate nuclei detection and classification are fundamental to computational pathology, yet existing approaches are hindered by reliance on detailed expert annotations and insufficient use of tissue context. We present Tissue-Aware Nuclei Detection (TAND), a novel framework achieving joint nuclei detection and classification using point-level supervision enhanced by tissue mask conditioning. TAND couples a ConvNeXt-based encoder-decoder with a frozen Virchow-2 tissue segmentation branch, where semantic tissue probabilities selectively modulate the classification stream through a novel multi-scale Spatial Feature-wise Linear Modulation (Spatial-FiLM). On the PUMA benchmark, TAND achieves state-of-the-art performance, surpassing both tissue-agnostic baselines and mask-supervised methods. Notably, our approach demonstrates remarkable improvements in tissue-dependent cell types such as epithelium, endothelium, and stroma. To the best of our knowledge, this is the first method to condition per-cell classification on learned tissue masks, offering a practical pathway to reduce annotation burden.

</details>


### [287] [A Real-Time Driver Drowsiness Detection System Using MediaPipe and Eye Aspect Ratio](https://arxiv.org/abs/2511.13618)
*Ashlesha G. Sawant,Shreyash S. Kamble,Raj S. Kanade,Raunak N. Kanugo,Tanishq A. Kapse,Karan A. Bhapse*

Main category: cs.CV

TL;DR: A low-cost, real-time driver drowsiness detection system using a webcam, Eye Aspect Ratio (EAR), and MediaPipe Face Mesh to alert drowsy drivers; claims high accuracy and fast response, suitable as an ADAS component.


<details>
  <summary>Details</summary>
Motivation: Reduce road accidents and injuries caused by driver fatigue by providing real-time detection and alert mechanisms.

Method: Utilizes a standard webcam to track facial features, computes Eye Aspect Ratio (EAR) to measure eye openness, and employs MediaPipe Face Mesh for fast, accurate landmark detection. OpenCV processes video frames. The system flags prolonged eye closure or very low blinking rates and issues an audible alert.

Result: Experimental analyses indicate high accuracy and quick response, suggesting the approach is high-performance yet low-cost.

Conclusion: The system can be a practical component of current ADAS, enabling real-time drowsiness detection with widely available hardware.

Abstract: One of the major causes of road accidents is driver fatigue that causes thousands of fatalities and injuries every year. This study shows development of a Driver Drowsiness Detection System meant to improve the safety of the road by alerting drivers who are showing signs of being drowsy. The system is based on a standard webcam that tracks the facial features of the driver with the main emphasis on the examination of eye movements that can be conducted with the help of the Eye Aspect Ratio (EAR) method. The Face Mesh by MediaPipe is a lightweight framework that can identify facial landmarks with high accuracy and efficiency, which is considered to be important in real time use. The system detects the moments of long eye shutdowns or a very low rate of blinking which are manifestations of drowsiness and alerts the driver through sound to get her attention back. This system achieves a high-performance and low-cost driver monitoring solution with the help of the computational power of OpenCV to process the image and the MediaPipe to identify faces. Test data experimental analyses indicate that the system is very accurate and responds quicker; this confirms that it can be a component of the current Advanced Driving Assistance System (ADAS).

</details>


### [288] [CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding](https://arxiv.org/abs/2511.13644)
*Shrenik Patel,Daivik Patel*

Main category: cs.CV

TL;DR: CacheFlow introduces a training-free pipeline combining Dynamic Token Dropping (DTD) with a compressive long-term memory to enable efficient live-streaming long-form video VQA. Tokens are pruned per frame, packed into fixed-size blocks, and per-block keys are summarized for a retrieval index. Full KV are offloaded and later rehydrated for generation, while a consensus-based Top-K block retrieval attends to both retrieved and local context, achieving up to 87% token reduction and improved performance over baselines.


<details>
  <summary>Details</summary>
Motivation: Long-form video VQA currently faces runaway attention/KV cache growth, forcing expensive inference or narrow sliding windows. There is a need for a training-free, architecture-agnostic solution that supports streaming with long-range reasoning.

Method: Per-frame Dynamic Token Dropping prunes tokens by cosine similarity to the previous frame; survivors are packed into fixed-size blocks. Each block’s keys are summarized by a tiny recurrent encoder to form a retrieval index; the block’s full KV pairs are offloaded and later rehydrated for generation. At inference, a consensus-based Top-K retrieval selects relevant blocks; the model attends over both retrieved and local context. No fine-tuning is required, and the approach is architecture-agnostic.

Result: On offline and streaming VQA benchmarks, CacheFlow outperforms strong baselines while processing up to 87% fewer tokens, demonstrating improved efficiency without sacrificing accuracy.

Conclusion: CacheFlow offers a practical, training-free, architecture-agnostic solution for long-form VQA, enabling efficient, context-aware live video understanding and long-range reasoning.

Abstract: Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.

</details>


### [289] [Part-X-MLLM: Part-aware 3D Multimodal Large Language Model](https://arxiv.org/abs/2511.13647)
*Chunshi Wang,Junliang Ye,Yunhan Yang,Yang Li,Zizhuo Lin,Jun Zhu,Zhuo Chen,Yawei Luo,Chunchao Guo*

Main category: cs.CV

TL;DR: A unified 3D multimodal LLM (Part-X-MLLM) outputs a structured program from an RGB point cloud and a natural language prompt, encoding part-level bounding boxes, semantics, and edit commands to drive downstream geometry engines.


<details>
  <summary>Details</summary>
Motivation: To unify diverse 3D tasks under a single, executable grammar, decoupling symbolic planning from geometric synthesis and enabling flexible control over geometry engines with a language-native frontend.

Method: Part-X-MLLM is a native 3D multimodal LLM that autoregressively generates a single token sequence encoding part-level boxes, semantics, and edit commands from an RGB point cloud and NL prompt. It uses a dual-encoder pretraining to disentangle structure from semantics and instruction-tuning on a large-scale part-centric dataset. The output is a structured program (grammar-based) that can drive downstream geometry-aware modules.

Result: The model yields high-quality structured plans and achieves state-of-the-art performance in grounded Q&A, compositional generation, and localized editing with a unified interface for 3D tasks.

Conclusion: Decoupling symbolic planning from geometric synthesis, Part-X-MLLM provides a versatile, language-native frontend to control any compatible geometry engine, enabling flexible, task-wide 3D generation and editing.

Abstract: We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/

</details>


### [290] [Distribution Matching Distillation Meets Reinforcement Learning](https://arxiv.org/abs/2511.13649)
*Dengyang Jiang,Dongyang Liu,Zanyi Wang,Qilong Wu,Xin Jin,David Liu,Zhen Li,Mengmeng Wang,Peng Gao,Harry Yang*

Main category: cs.CV

TL;DR: DMDR merges distribution matching distillation with reinforcement learning to train high-quality few-step diffusion models that can rival or exceed their multi-step teachers, aided by dynamic guidance and sampling strategies.


<details>
  <summary>Details</summary>
Motivation: Distilling a pre-trained multi-step diffusion model into a few-step generator often caps performance due to reduced modeling capacity. The challenge is to preserve quality and diversity (mode coverage) while improving efficiency.

Method: Introduce DMDR: a framework that integrates RL into the DMD distillation loop. The DMD loss acts as a stronger regularizer for the few-step generator than traditional losses. RL guides mode coverage during distillation. Dynamic distribution guidance and dynamic re-noise sampling are used to bootstrap the process, enabling simultaneous distillation and RL.

Result: DMDR achieves leading visual quality and prompt coherence among few-step methods and can even exceed the multi-step teacher in experiments.

Conclusion: RL-enhanced distillation effectively unlocks the capacity of few-step diffusion models. The combination of DMD with RL, plus dynamic guidance and sampling, yields superior performance and flexibility over standard distillation.

Abstract: Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.

</details>


### [291] [OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation](https://arxiv.org/abs/2511.13655)
*Henry Herzog,Favyen Bastani,Yawen Zhang,Gabriel Tseng,Joseph Redmon,Hadrien Sablon,Ryan Park,Jacob Morrison,Alexandra Buraczynski,Karen Farley,Joshua Hansen,Andrew Howe,Patrick Alan Johnson,Mark Otterlee,Ted Schmitt,Hunter Pitelka,Stephen Daspit,Rachel Ratner,Christopher Wilhelm,Sebastian Wood,Mike Jacobi,Hannah Kerner,Evan Shelhamer,Ali Farhadi,Ranjay Krishna,Patrick Beukema*

Main category: cs.CV

TL;DR: OlmoEarth is a multimodal, spatio-temporal foundation model for Earth observation using a novel self-supervised objective, achieving state-of-the-art results on benchmarks and enabling an NGO-friendly platform; code and weights are public.


<details>
  <summary>Details</summary>
Motivation: Earth observation data are inherently spatial, sequential, and multimodal; there is a need for a unified foundation model and accessible tooling to empower research and humanitarian work.

Method: Develop a novel self-supervised learning formulation, masking strategy, and loss tailored to EO data. The model handles spatio-temporal and multimodal inputs, pretrains on EO data, and is evaluated across numerous embedding and finetuning tasks. The model is deployed as the backbone of an end-to-end OlmoEarth Platform.

Result: OlmoEarth achieves state-of-the-art performance compared to 12 foundation models across multiple research benchmarks and external tasks; best embedding performance on 15 of 24 tasks, and best full fine-tuning performance on 19 of 29 tasks. It is deployed as the backbone of an end-to-end platform for data collection, labeling, training, and inference, enabling NGO-focused usage. Source code, data, and pretrained weights are publicly available.

Conclusion: OlmoEarth demonstrates strong performance in EO multimodal spatio-temporal modeling and has societal impact by providing an accessible platform for NGOs and researchers to leverage frontier models in Earth observation.

Abstract: Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\href{https://github.com/allenai/olmoearth_pretrain}{\text{https://github.com/allenai/olmoearth_pretrain}}$.

</details>


### [292] [Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting](https://arxiv.org/abs/2511.13684)
*Jiangnan Ye,Jiedong Zhuang,Lianrui Mu,Wenjie Zheng,Jiaqi Hu,Xingze Zou,Jing Wang,Haoji Hu*

Main category: cs.CV

TL;DR: GS-Light is a training-free, multi-view diffusion-based relighting pipeline for 3D Gaussian Splatting (3DGS) scenes, guided by LVLM-derived lighting priors. It achieves improved multi-view consistency and relit quality over baselines, with code/assets to be released.


<details>
  <summary>Details</summary>
Motivation: Enable flexible, user-controlled lighting for 3D scene relighting efficiently without per-view optimization, by leveraging LVLMs and off-the-shelf estimators to fuse lighting priors with geometry.

Method: 1) Parse user prompt into lighting priors using a large vision-language model (LVLM). 2) Use off-the-shelf estimators to obtain depth, surface normals, and semantic segmentation. 3) Compute illumination maps by fusing lighting priors with view geometry and generate initial latent codes for each view. 4) Feed multi-view rendered images plus init latents into a multi-view relighting diffusion model to produce relit images. 5) Fine-tune the 3DGS scene with relit appearance to obtain a fully relit 3D scene.

Result: Quantitative evaluation across indoor/outdoor scenes shows improvements in multi-view consistency, imaging quality, aesthetic score, and semantic similarity; qualitative user studies indicate preference over baseline methods (per-view relighting, video relighting, and scene editing).

Conclusion: GS-Light offers a scalable, training-free pathway for accurate, user-guided relighting of 3D scenes and will release code/assets upon publication.

Abstract: We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.

</details>


### [293] [TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models](https://arxiv.org/abs/2511.13704)
*Harold Haodong Chen,Disen Lan,Wen-Jie Shu,Qingyang Liu,Zihan Wang,Sirui Chen,Wenkai Cheng,Kanghao Chen,Hongfei Zhang,Zixin Zhang,Rongjin Guo,Yu Cheng,Ying-Cong Chen*

Main category: cs.CV

TL;DR: TiViBench introduces a hierarchical benchmark to evaluate reasoning in image-to-video generation across four dimensions (Structural, Spatial, Symbolic, and Action reasoning) with 24 tasks at three difficulty levels, and VideoTPO is a test-time strategy using LLM self-analysis to improve candidate generation without extra training. Commercial models show stronger reasoning; open-source models have untapped potential limited by data; TiViBench + VideoTPO aim to advance reasoning in video generation.


<details>
  <summary>Details</summary>
Motivation: Current video generation benchmarks emphasize visual fidelity and temporal coherence but fail to assess higher-order reasoning. There is a need to evaluate and push reasoning capabilities in image-to-video models in order to match or approach the reasoning seen in large language models.

Method: 1) Propose TiViBench, a hierarchical benchmark with four reasoning dimensions and 24 tasks across 3 difficulty levels to systematically test I2V models. 2) Evaluate various models (commercial like Sora 2, Veo 3.1 and open-source) on TiViBench to measure reasoning performance and identify gaps. 3) Introduce VideoTPO, a test-time strategy that uses LLM self-analysis on generated candidates to identify strengths/weaknesses and re-rank or refine outputs without additional training or data.

Result: Experiments show commercial models exhibit stronger reasoning capabilities on TiViBench; open-source models reveal latent potential constrained by training scale and data diversity. VideoTPO significantly improves reasoning performance by leveraging self-analysis of candidates at test time, without requiring extra training, data, or reward models.

Conclusion: TiViBench provides a comprehensive framework to evaluate higher-order reasoning in image-to-video generation, and VideoTPO offers a practical, training-free approach to boost reasoning, together enabling progress in the development and assessment of reasoning-capable video generation models.

Abstract: The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.

</details>


### [294] [Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine](https://arxiv.org/abs/2511.13713)
*Xincheng Shuai,Zhenyuan Qin,Henghui Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: FFSE is a 3D-aware autoregressive framework for intuitive editing of real-world images via learned 3D transformations, enabling multi-round object edits with consistent backgrounds and shadows, trained with the 3DObjectEditor dataset; it outperforms prior methods.


<details>
  <summary>Details</summary>
Motivation: Current 3D-aware object editing on real images is hampered by image-space limitations and slow, error-prone 3D reconstructions. There is a need for physically-consistent, multi-round editing that maintains global scene consistency.

Method: FFSE models edits as a sequence of learned 3D transformations in an autoregressive pipeline. It operates in 3D to apply translations, rotations, and scalings while preserving background effects (shadows, reflections) and global scene coherence across rounds. Training is enabled by 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes to support multi-round and dynamic editing.

Result: Extensive experiments show FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.

Conclusion: FFSE enables intuitive, physically-consistent 3D-aware object editing on real-world images and supports multi-round editing with a dataset designed for dynamic conditions; this marks a strong step toward practical, 3D-consistent image editing.

Abstract: Recent advances in text-to-image (T2I) diffusion models have significantly improved semantic image editing, yet most methods fall short in performing 3D-aware object manipulation. In this work, we present FFSE, a 3D-aware autoregressive framework designed to enable intuitive, physically-consistent object editing directly on real-world images. Unlike previous approaches that either operate in image space or require slow and error-prone 3D reconstruction, FFSE models editing as a sequence of learned 3D transformations, allowing users to perform arbitrary manipulations, such as translation, scaling, and rotation, while preserving realistic background effects (e.g., shadows, reflections) and maintaining global scene consistency across multiple editing rounds. To support learning of multi-round 3D-aware object manipulation, we introduce 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes, enabling effective training under multi-round and dynamic conditions. Extensive experiments show that the proposed FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.

</details>


### [295] [Segment Anything Across Shots: A Method and Benchmark](https://arxiv.org/abs/2511.13715)
*Hengrui Hu,Kaining Ying,Henghui Ding*

Main category: cs.CV

TL;DR: Proposes SAAS for multi-shot VOS using transition-mimicking data augmentation (TMA) to enable cross-shot generalization from single-shot data, plus Cut-VOS benchmark; achieves state-of-the-art on YouMVOS and Cut-VOS.


<details>
  <summary>Details</summary>
Motivation: Current VOS methods excel on single-shot videos but fail across shot discontinuities; there is severe annotated multi-shot data sparsity; need methods robust to shot transitions.

Method: Introduce TMA data augmentation to mimic transitions, enabling cross-shot generalization from single-shot data; develop Segment Anything Across Shots (SAAS) to detect and reason about shot transitions; introduce Cut-VOS benchmark with dense masks and diverse transitions; evaluate on YouMVOS and Cut-VOS; release code/datasets.

Result: SAAS achieves state-of-the-art performance on YouMVOS and Cut-VOS by effectively mimicking, understanding, and segmenting across complex transitions.

Conclusion: TMA + SAAS provide robust cross-shot VOS by modeling transitions; Cut-VOS enables future study; dataset and code release supports reproducibility.

Abstract: This work focuses on multi-shot semi-supervised video object segmentation (MVOS), which aims at segmenting the target object indicated by an initial mask throughout a video with multiple shots. The existing VOS methods mainly focus on single-shot videos and struggle with shot discontinuities, thereby limiting their real-world applicability. We propose a transition mimicking data augmentation strategy (TMA) which enables cross-shot generalization with single-shot data to alleviate the severe annotated multi-shot data sparsity, and the Segment Anything Across Shots (SAAS) model, which can detect and comprehend shot transitions effectively. To support evaluation and future study in MVOS, we introduce Cut-VOS, a new MVOS benchmark with dense mask annotations, diverse object categories, and high-frequency transitions. Extensive experiments on YouMVOS and Cut-VOS demonstrate that the proposed SAAS achieves state-of-the-art performance by effectively mimicking, understanding, and segmenting across complex transitions. The code and datasets are released at https://henghuiding.com/SAAS/.

</details>


### [296] [Back to Basics: Let Denoising Generative Models Denoise](https://arxiv.org/abs/2511.13720)
*Tianhong Li,Kaiming He*

Main category: cs.CV

TL;DR: JiT argues for directly predicting clean data in diffusion models, using large-patch image Transformers on pixels without tokenizers or pretraining, under a manifold assumption that clean data lie on low-dimensional manifolds while noisy quantities do not.


<details>
  <summary>Details</summary>
Motivation: Challenging the standard diffusion objective of predicting noise; by positing that clean natural images lie on a low-dimensional manifold, the model should predict clean data directly. This could enable simpler, high-dimensional modeling with under-capacity networks.

Method: Train large-patch Transformers (patch sizes 16 and 32) directly on pixel space to predict clean images, without tokenizers, pretraining, or extra loss terms (JiT: Just image Transformers). Evaluate on ImageNet at resolutions 256 and 512.

Result: The JiT approach yields competitive results on ImageNet at 256 and 512 resolutions with patch sizes 16 and 32, illustrating that direct clean-data prediction can perform well where predicting noisy quantities can fail catastrophically.

Conclusion: A return to a basics-driven paradigm: Transformer-based diffusion on raw natural data using direct clean-data prediction appears viable and potentially advantageous, suggesting a broader shift away from noise-prediction objectives toward manifold-aligned, direct-image modeling.

Abstract: Today's denoising diffusion models do not "denoise" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than "$\textbf{Just image Transformers}$", or $\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [297] [LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism](https://arxiv.org/abs/2511.11591)
*Olusola Babalola,Bolanle Ojokoh,Olutayo Boyinbode*

Main category: cs.AI

TL;DR: LLM-generated synthetic negative news headlines can closely approximate real headlines, offering a privacy-preserving data source for NLP tasks; they align in content, tone, length, and style, with a notable divergence in proper noun usage.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity and privacy concerns in NLP by using synthetic data from LLMs to train sentiment analysis models, specifically focusing on negative valence text within news headlines.

Method: Create a specialized corpus of negative news headlines using tailored prompts; validate synthetics via expert review; analyze in embedding space; evaluate metrics: correlation with real headlines, perplexity, coherence, realism; benchmark against two real headline sets using Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity.

Result: The synthetic headlines substantially match real headlines across evaluated dimensions; the only marked divergence is in the proper noun score within the POS profile test.

Conclusion: LLM-generated synthetic headlines can serve as a viable proxy for real-world negative news data in NLP tasks, potentially easing data accessibility and privacy concerns, though attention should be paid to proper noun usage and domain-specific linguistic features.

Abstract: This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.

</details>


### [298] [CLINB: A Climate Intelligence Benchmark for Foundational Models](https://arxiv.org/abs/2511.11597)
*Michelle Chen Huebscher,Katharine Mach,Aleksandar Stanić,Markus Leippold,Ben Gaiarin,Zeke Hausfather,Elisa Rawat,Erich Fischer,Massimiliano Ciaramita,Joeri Rogelj,Christian Buck,Lierni Sestorain Saralegui,Reto Knutti*

Main category: cs.AI

TL;DR: Introduces CLINB to evaluate LLMs on climate knowledge via open-ended, grounded, multimodal QA; finds frontier models synthesize knowledge at PhD level but often fail to ground with reliable attribution.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of evaluating specialized, credible knowledge in LLMs and the need for benchmarks that require evidence and verifiable support in scientific contexts.

Method: Construct CLINB from real user climate questions and expert-curated rubrics; implement a model-based evaluation process; assess frontier models on knowledge synthesis and grounding, including evidence quality for references and images.

Result: Frontier models exhibit strong knowledge synthesis and presentation quality, often surpassing hybrid expert-curated answers; however, they show substantial grounding failures with variable evidence quality and high hallucination rates in references and images.

Conclusion: Bridging knowledge synthesis with verifiable attribution is essential for trustworthy AI in scientific workflows; reliable benchmarks like CLINB are needed to drive progress toward trustworthy, interpretable AI systems.

Abstract: Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform "hybrid" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.

</details>


### [299] [SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detectio](https://arxiv.org/abs/2511.11599)
*Arefeh Kazemi,Hamza Qadeer,Joachim Wagner,Hossein Hosseini,Sri Balaaji Natarajan Kalaivendan,Brian Davis*

Main category: cs.AI

TL;DR: SynBullying is a synthetic, multi-turn LLM-based dataset for cyberbullying research that emphasizes conversational structure, context-aware harm annotations, and fine-grained CB categories; evaluated on structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution, and useful as training data or augmentation for CB classification.


<details>
  <summary>Details</summary>
Motivation: To enable ethical, scalable study of cyberbullying dynamics without relying on sensitive real-world data, by generating contextually rich, annotation-friendly dialogues that capture intent, discourse, and various CB types.

Method: Leverage large language models to simulate realistic bullying conversations with multi-turn structure; annotate harm within the conversational flow considering context and intent; provide fine-grained CB labels across categories; evaluate on five dimensions (structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, CB-type distribution); assess utility as standalone training data and as augmentation for CB classification.

Result: The dataset offers conversationally structured, context-aware, fine-grained CB annotations and demonstrates utility for both standalone training and augmentation in CB classification tasks; concrete performance metrics are not provided in the abstract.

Conclusion: SynBullying provides a scalable, ethically safe resource for studying and detecting cyberbullying, enabling nuanced linguistic and behavioral analysis and potentially improving CB classifiers and related research.

Abstract: We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.

</details>


### [300] [CausalGuard: A Smart System for Detecting and Preventing False Information in Large Language Models](https://arxiv.org/abs/2511.11600)
*Piyushkumar Patel*

Main category: cs.AI

TL;DR: CausalGuard fuses causal reasoning with symbolic logic to detect and prevent hallucinations in LLMs during generation, improving accuracy while preserving natural responses.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in large language models undermine accuracy in high-stakes domains; existing fixes often require retraining or incur high computational costs and do not address root causes.

Method: A two-path framework: (1) causal path that traces relationships between what the model knows and what it generates to identify misleading inferences, and (2) a symbolic logic path that uses automated reasoning to check logical consistency; the system intervenes early in generation rather than post-hoc.

Result: Evaluated on twelve benchmarks: 89.3% detection of hallucinations with 8.3% miss rate; reduces false claims by ~80% while maintaining natural, helpful responses; excels on complex, multi-step reasoning; provides transparent reasoning traces beneficial for sensitive domains such as medical diagnosis or financial analysis.

Conclusion: Integrating causal reasoning with symbolic logic yields a more reliable, explainable protection against hallucinations in LLMs, with strong performance across benchmarks and applicability to safety-critical tasks.

Abstract: While large language models have transformed how we interact with AI systems, they have a critical weakness: they confidently state false information that sounds entirely plausible. This "hallucination" problem has become a major barrier to using these models where accuracy matters most. Existing solutions either require retraining the entire model, add significant computational costs, or miss the root causes of why these hallucinations occur in the first place.
  We present CausalGuard, a new approach that combines causal reasoning with symbolic logic to catch and prevent hallucinations as they happen. Unlike previous methods that only check outputs after generation, our system understands the causal chain that leads to false statements and intervenes early in the process. CausalGuard works through two complementary paths: one that traces causal relationships between what the model knows and what it generates, and another that checks logical consistency using automated reasoning.
  Testing across twelve different benchmarks, we found that CausalGuard correctly identifies hallucinations 89.3\% of the time while missing only 8.3\% of actual hallucinations. More importantly, it reduces false claims by nearly 80\% while keeping responses natural and helpful. The system performs especially well on complex reasoning tasks where multiple steps of logic are required. Because CausalGuard shows its reasoning process, it works well in sensitive areas like medical diagnosis or financial analysis where understanding why a decision was made matters as much as the decision itself.

</details>


### [301] [Quantifying Skill and Chance: A Unified Framework for the Geometry of Games](https://arxiv.org/abs/2511.11611)
*David H. Silver*

Main category: cs.AI

TL;DR: Proposes Skill-Luck Index S(G) to separate skill and luck in stochastic decision trees; analyzes 30 games; finds a spectrum from pure chance to pure skill; shows poker with moderate skill dominance; introduces volatility Sigma; broad applicability to decision systems.


<details>
  <summary>Details</summary>
Motivation: Quantify the balance of skill and luck to enable principled comparison of player influence, game balance, AI evaluation, and risk assessment.

Method: Model outcomes as stochastic decision trees; decompose into skill leverage K and luck leverage L; define S(G) in [-1,1]; compute for various games; define volatility Sigma over turns.

Result: Empirical values: coin toss S=-1; backgammon S=0, Sigma=1.20; chess S=+1, Sigma=0; poker S=0.33 with K=0.40±0.03, Sigma=0.80; general framework extended to stochastic decision systems.

Conclusion: Framework enables principled comparisons of player influence, game balance, and predictive stability; applicable to game design, AI evaluation, and risk assessment.

Abstract: We introduce a quantitative framework for separating skill and chance in games by modeling them as complementary sources of control over stochastic decision trees. We define the Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L. Applying this to 30 games reveals a continuum from pure chance (coin toss, S = -1) through mixed domains such as backgammon (S = 0, Sigma = 1.20) to pure skill (chess, S = +1, Sigma = 0). Poker exhibits moderate skill dominance (S = 0.33) with K = 0.40 +/- 0.03 and Sigma = 0.80. We further introduce volatility Sigma to quantify outcome uncertainty over successive turns. The framework extends to general stochastic decision systems, enabling principled comparisons of player influence, game balance, and predictive stability, with applications to game design, AI evaluation, and risk assessment.

</details>


### [302] [Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation](https://arxiv.org/abs/2511.11693)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Zeyao Liu,Zhendong Zhao,Xiaoyan Gu*

Main category: cs.AI

TL;DR: VALOR is a modular, zero-shot agentic framework that detects unsafe prompts and rewrites them with an LLM to ensure safer, value-aligned text-to-image generation, with optional stylistic regeneration to preserve intent while reducing risk.


<details>
  <summary>Details</summary>
Motivation: Generative vision-language models can produce unsafe or culturally inappropriate outputs under adversarial prompts. Existing defenses struggle to maintain quality or incur high costs, necessitating scalable, value-aligned safeguards.

Method: A layered prompt-analysis pipeline: (1) multi-level NSFW detector for lexical/semantic risks, (2) cultural value alignment module for norms, legality, representation ethics, (3) intention disambiguator for subtle unsafe implications. Unsafe prompts trigger selective rewriting by a large language model under dynamic, role-specific instructions to preserve user intent while enforcing alignment. If needed, an optional stylistic regeneration steers visuals toward safer domains without changing core semantics.

Result: VALOR significantly reduces unsafe outputs—up to 100% in tested scenarios—while preserving usefulness and creativity of prompts.

Conclusion: VALOR is a scalable, effective approach for deploying safe, value-aligned text-to-image generation in open-world settings.

Abstract: Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.

</details>


### [303] [Towards autonomous quantum physics research using LLM agents with access to intelligent tools](https://arxiv.org/abs/2511.11752)
*Sören Arlt,Xuemei Gu,Mario Krenn*

Main category: cs.AI

TL;DR: AI-Mandel is an LLM-based agent that can both generate and implement ideas in quantum physics, using a domain-specific tool to turn ideas into concrete, lab-ready experiment designs. The approach yields scientifically interesting ideas—some of which have led to independent follow-up papers—and serves as a prototype highlighting both the potential and challenges of autonomous AI scientists.


<details>
  <summary>Details</summary>
Motivation: Human researchers typically define initial questions; automating idea generation and implementation could accelerate scientific progress. This work tests the feasibility and limitations of a fully autonomous AI physicist in the domain of quantum physics, and explores what is required to reach human-level artificial scientists.

Method: An LLM agent (AI-Mandel) formulates ideas by mining the literature and uses a domain-specific AI tool to translate them into concrete experiment designs suitable for laboratory implementation. The paper highlights example ideas (new quantum teleportation variants, primitives of quantum networks in indefinite causal orders, and geometric phases from closed loops of quantum information transfer) and reports that these ideas can be acted upon.

Result: The generated ideas are scientifically interesting, with at least two ideas already leading to independent follow-up papers. AI-Mandel demonstrates a prototypical AI physicist capable of generating and implementing concrete, actionable ideas.

Conclusion: This work demonstrates the potential of AI to accelerate science by integrating idea generation and implementation, while also revealing concrete open challenges on the path toward human-level artificial scientists.

Abstract: Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.

</details>


### [304] [Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction](https://arxiv.org/abs/2511.11770)
*Floris Vossebeld,Shenghui Wang*

Main category: cs.AI

TL;DR: A RL-trained 3B LLM learns adaptive, multi-step SPARQL construction with error recovery and deliberative reasoning, outperforming zero-shot baselines on LC-QuAD 2.0.


<details>
  <summary>Details</summary>
Motivation: One-shot LLMs struggle with complex, multi-hop SPARQL queries and cannot adapt based on real-time execution feedback.

Method: Agentic framework; sequential SPARQL construction; 3B model trained solely by outcome-driven reinforcement learning (GRPO) without supervised fine-tuning; includes an explicit deliberative reasoning step as cognitive scaffold.

Result: On an executable LC-QuAD 2.0 subset with a single answer, achieving 49.7% accuracy after entity-linking, 17.5 percentage points higher than the strongest iterative zero-shot baseline.

Conclusion: Demonstrates a generalizable approach to teaching agents to master formal symbolic tools via interaction, bridging probabilistic LLMs and structured knowledge graphs.

Abstract: Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.

</details>


### [305] [On the Measure of a Model: From Intelligence to Generality](https://arxiv.org/abs/2511.11773)
*Ruchira Dhar,Ninell Oldenburg,Anders Soegaard*

Main category: cs.AI

TL;DR: This perspective argues that AI intelligence benchmarks are flawed and generality should be the evaluation anchor, framed as a multitask/generalization problem linking breadth and reliability to real-world utility.


<details>
  <summary>Details</summary>
Motivation: Benchmarks like ARC, Raven, and the Blackbird Task are unstable proxies for real-world tasks; intelligence lacks a stable definition; evaluating generality promises more robust progress.

Method: Conceptual and formal analysis identifying three underlying assumptions (generality, stability, realism); arguing only generality withstands scrutiny; reframing intelligence as a multitask generality problem.

Result: Generalization/breadth-focused evaluation is proposed as a stable foundation; intelligence is not what enables generality; evaluation should measure performance breadth and reliability across tasks.

Conclusion: Shift AI progress measurement toward generality as the core criterion, enabling evaluation across diverse and evolving tasks.

Abstract: Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.

</details>


### [306] [Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy](https://arxiv.org/abs/2511.11816)
*Andrea Brunello,Luca Geatti,Michele Mignani,Angelo Montanari,Nicola Saccomanno*

Main category: cs.AI

TL;DR: Existing NL→FOL evaluation datasets/protocols are flawed; a novel evaluation protocol reveals dialogue-oriented LLMs excel at NL-FOL translation and genuine logical understanding, while embedding-centric models underperform.


<details>
  <summary>Details</summary>
Motivation: FOL is expressive and unambiguous for representing NL concepts; translating NL to FOL (NL→FOL) is hard. Conflicting evidence on LLM capabilities necessitates robust evaluation to avoid misrepresenting performance due to memorization or data leakage.

Method: Critically survey current NL-FOL datasets and evaluation protocols; design a novel evaluation protocol that isolates semantic-level logical understanding from superficial pattern recognition and dataset contamination; empirically compare dialogue-oriented LLMs and embedding-centric models on NL-FOL translation under the new protocol.

Result: Under the new evaluation protocol, dialogue-oriented LLMs show strong NL-FOL translation skills and genuine sentence-level logical understanding; embedding-centric models perform markedly worse.

Conclusion: Reliable assessment of NL-FOL translation capabilities requires robust, semantics-focused evaluation protocols. The findings suggest dialogue-oriented LLMs possess genuine NL-FOL understanding, while prior evaluations may have overstated or misrepresented abilities due to dataset/protocol design.

Abstract: Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.

</details>


### [307] [TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models](https://arxiv.org/abs/2511.11831)
*Wenhao Zhou,Hao Zheng,Rong Zhao*

Main category: cs.AI

TL;DR: TopoPerception reveals that global visual perception in LVLMs is a bottleneck; even coarse topology-based tasks yield near-random performance, and stronger models may fare worse, signaling that scaling alone cannot fix global perception.


<details>
  <summary>Details</summary>
Motivation: Global perception depends on global image structure and should be evaluated without local shortcuts. Existing benchmarks often conflate local features with semantics, masking global perceptual abilities.

Method: Introduce TopoPerception, a benchmark leveraging topological properties to test global visual perception across multiple granularities. Evaluate state-of-the-art LVLMs and analyze performance trends across model families to assess whether increased reasoning capabilities improve global perception.

Result: Across coarse and fine granularities, current LVLMs perform no better than random chance, indicating a profound global perception deficit. Surprisingly, more powerful models show lower accuracy, suggesting scaling alone worsens this issue. TopoPerception provides a shortcut-free, topology-based assessment and highlights the need for new training paradigms or architectures.

Conclusion: TopoPerception exposes a critical bottleneck in global visual perception for LVLMs and offers a direction for developing models with genuine global perceptual capabilities; data and code are publicly available for reproducible research.

Abstract: Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.

</details>


### [308] [End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction](https://arxiv.org/abs/2511.11899)
*Xi Li,Nicholas Matsumoto,Ujjwal Pasupulety,Atharva Deo,Cherine Yang,Jay Moran,Miguel E. Hernandez,Peter Wager,Jasmine Lin,Jeanine Kim,Alvin C. Goh,Christian Wagner,Geoffrey A. Sonn,Andrew J. Hung*

Main category: cs.AI

TL;DR: F2O translates intraoperative videos into gesture sequences using transformer-based models, detects short gestures, and links gesture-derived features to postoperative outcomes with performance comparable to human annotations.


<details>
  <summary>Details</summary>
Motivation: Address the long-standing challenge of fine-grained analysis of intraoperative behavior and its impact on patient outcomes, enabling automatic interpretable surgical feedback and prospective decision support.

Method: An end-to-end system using transformer-based spatial and temporal modeling with frame-wise classification to detect consecutive short (~2s) gestures in the nerve-sparing step of robot-assisted radical prostatectomy. It derives gesture-level features (frequency, duration, transitions) and evaluates their ability to predict postoperative outcomes, comparing against human annotations and analyzing 25 shared features for effect sizes and correlations.

Result: AUC of 0.80 at frame level and 0.81 at video level for gesture detection; postoperative outcome prediction accuracy of 0.79 vs. 0.75 when using F2O features vs. human annotations; across 25 features, effect sizes directions were concordant with small differences (~0.07) and a strong correlation (r = 0.96, p < 1e-14). F2O captured patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use.

Conclusion: F2O enables automatic interpretable assessment of intraoperative behavior and provides a foundation for data-driven surgical feedback and prospective clinical decision support.

Abstract: Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.

</details>


### [309] [Forgetting-MarI: LLM Unlearning via Marginal Information Regularization](https://arxiv.org/abs/2511.11914)
*Shizhou Xu,Yuan Ni,Stefan Broecker,Thomas Strohmer*

Main category: cs.AI

TL;DR: Forgetting-MarI: an LLM unlearning framework that targets marginal information to forget, with provable bounds on residual influence and improved performance over baselines.


<details>
  <summary>Details</summary>
Motivation: As AI models train on ever-larger datasets, there is a need to remove the influence of specific data from trained models to protect privacy and satisfy regulatory requirements. Existing unlearning methods often over-remove information or require retraining, which is costly for large models.

Method: Forgetting-MarI penalizes the marginal information contributed by data to be unlearned, yielding an explicit upper bound on the unlearned data's residual influence and ensuring undetectability. The approach seeks to unlearn only what is necessary while preserving information supported by retained data.

Result: Extensive experiments show Forgetting-MarI outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks.

Conclusion: This framework advances controllable and compliant AI systems by enabling targeted forgetting without sacrificing effectiveness, addressing privacy and copyright concerns in large-scale models.

Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.

</details>


### [310] [An Analysis of Architectural Impact on LLM-based Abstract Visual Reasoning: A Systematic Benchmark on RAVEN-FAIR](https://arxiv.org/abs/2511.11916)
*Sinan Urgun,Seçkin Arı*

Main category: cs.AI

TL;DR: GPT-4.1-Mini achieves highest accuracy across architectures on RAVEN-FAIR; results show model-specific sensitivity to architectural choices, with best-of-five runs advised due to response variability.


<details>
  <summary>Details</summary>
Motivation: To systematically assess how different LLMs and reasoning architectures perform on abstract visual reasoning tasks, and to understand how architectural design and response variability affect evaluation and comparability.

Method: Four LLMs (GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b) tested across four reasoning architectures (single-shot, embedding-controlled repetition, self-reflection, multi-agent) on the RAVEN-FAIR dataset. Visual outputs are produced via a three-stage process (JSON extraction, LLM reasoning, Tool Function) and evaluated with SSIM and LPIPS; Chain-of-Thought scores and error types (semantic hallucination, numeric misperception) analyzed. Best-of-five independent runs reported as upper-bound performance.

Result: GPT-4.1-Mini consistently attains the highest overall accuracy across architectures; multi-agent design can alter semantic/numeric balance but not uniformly improve performance; models exhibit distinct sensitivity patterns to architectural design; response coverage variability confounds cross-architecture comparisons; single-run evaluations are fragile, hence reporting the best of five runs to estimate upper-bound performance.

Conclusion: Reasoning effectiveness is largely model-specific; architectural design effects are not universally beneficial. To compare configurations robustly, use multi-run best-case estimates and account for response coverage variability across models and architectures.

Abstract: This study aims to systematically evaluate the performance of large language models (LLMs) in abstract visual reasoning problems. We examined four LLM models (GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b) utilizing four different reasoning architectures (single-shot, embedding-controlled repetition, self-reflection, and multi-agent) on the RAVEN-FAIR dataset. Visual responses generated through a three-stage process (JSON extraction, LLM reasoning, and Tool Function) were evaluated using SSIM and LPIPS metrics; Chain-of-Thought scores and error types (semantic hallucination, numeric misperception) were analyzed. Results demonstrate that GPT-4.1-Mini consistently achieved the highest overall accuracy across all architectures, indicating a strong reasoning capability. While the multi-agent architecture occasionally altered semantic and numeric balance across models, these effects were not uniformly beneficial. Instead, each model exhibited distinct sensitivity patterns to architectural design, underscoring that reasoning effectiveness remains model-specific. Variations in response coverage further emerged as a confounding factor that complicates direct cross-architecture comparison. To estimate the upper-bound performance of each configuration, we report the best of five independent runs, representing a best-case scenario rather than an averaged outcome. This multi-run strategy aligns with recent recommendations, which emphasize that single-run evaluations are fragile and may lead to unreliable conclusions.

</details>


### [311] [Looking Forward: Challenges and Opportunities in Agentic AI Reliability](https://arxiv.org/abs/2511.11921)
*Liudong Xing,Janet,Lin*

Main category: cs.AI

TL;DR: A chapter surveying challenges and future directions for reliable, agentic AI, focusing on cascading failures, dynamic environments, inconsistent task execution, emergent behaviors, and resource-heavy reliability, plus testing/evaluation avenues for reliability.


<details>
  <summary>Details</summary>
Motivation: To advance safe and dependable agentic AI systems amid uncertainty and complexity by identifying risks, gaps, and opportunities for reliability.

Method: Conceptual analysis and literature survey that maps open problems, tensions, and opportunities; outlines a roadmap of research directions and evaluation approaches.

Result: Provides a taxonomy of reliability challenges and a roadmap of future research directions for testing, evaluating, and improving reliability in agentic AI systems.

Conclusion: Reliability in agentic AI hinges on addressing dynamic-environment resilience, predictable task execution, mitigation of emergent behaviors, scalable reliability mechanisms, and robust evaluation methodologies; significant research opportunities remain.

Abstract: This chapter presents perspectives for challenges and future development in building reliable AI systems, particularly, agentic AI systems. Several open research problems related to mitigating the risks of cascading failures are discussed. The chapter also sheds lights on research challenges and opportunities in aspects including dynamic environments, inconsistent task execution, unpredictable emergent behaviors, as well as resource-intensive reliability mechanisms. In addition, several research directions along the line of testing and evaluating reliability of agentic AI systems are also discussed.

</details>


### [312] [A Neuromorphic Architecture for Scalable Event-Based Control](https://arxiv.org/abs/2511.11924)
*Yongkang Huo,Fulvio Forni,Rodolphe Sepulchre*

Main category: cs.AI

TL;DR: Introduces rebound Winner-Take-All (RWTA) as a scalable neuromorphic control unit that blends discrete WTA computation with continuous excitable dynamics, demonstrated through a snake-robot nervous system design.


<details>
  <summary>Details</summary>
Motivation: To create a scalable neuromorphic architecture that combines the reliability of discrete computation with the tunability of continuous regulation, bridging discrete decision-making and continuous rhythmic control.

Method: Introduce the RWTA motif and an event-based framework implemented in a unified physical modeling language; apply it to design the nervous system of a snake robot to showcase versatility and modularity.

Result: Demonstrates versatility, robustness, and modularity of the architecture; the snake-robot nervous system design serves as a concrete illustration of unified discrete-continuous processing.

Conclusion: RWTA provides a scalable architecture that preserves discrete computation reliability while enabling continuous regulation, with the event-based framework enabling unified modeling across levels of organization (cellular to system).

Abstract: This paper introduces the ``rebound Winner-Take-All (RWTA)" motif as the basic element of a scalable neuromorphic control architecture. From the cellular level to the system level, the resulting architecture combines the reliability of discrete computation and the tunability of continuous regulation: it inherits the discrete computation capabilities of winner-take-all state machines and the continuous tuning capabilities of excitable biophysical circuits. The proposed event-based framework addresses continuous rhythmic generation and discrete decision-making in a unified physical modeling language. We illustrate the versatility, robustness, and modularity of the architecture through the nervous system design of a snake robot.

</details>


### [313] [Augmenting The Weather: A Hybrid Counterfactual-SMOTE Algorithm for Improving Crop Growth Prediction When Climate Changes](https://arxiv.org/abs/2511.11945)
*Mohammed Temraz,Mark T Keane*

Main category: cs.AI

TL;DR: Proposes CFA-SMOTE, a data augmentation method combining counterfactuals with SMOTE to address climate-outlier events and class imbalance; improves predictive performance on a drought-era grass-growth dataset.


<details>
  <summary>Details</summary>
Motivation: Climate change induces extreme, out-of-distribution events; traditional ML relying on historical distributions underperform; blending XAI counterfactuals with SMOTE targets minority climate-outlier instances.

Method: Integrates instance-based counterfactual generation with SMOTE to synthesize outlier data; experiments compare CFA-SMOTE to benchmark counterfactual and imbalanced methods across varying imbalance ratios; test domain: Irish dairy farms grass growth under 2018 drought.

Result: Synthetic climate-outlier data improved predictive performance; CFA-SMOTE outperforms benchmark methods under multiple imbalance settings.

Conclusion: CFA-SMOTE is a viable augmentation technique for climate-change related predictive tasks facing distribution shifts and minority-class scarcity; applicable to ecological/agricultural domains with extreme events.

Abstract: In recent years, humanity has begun to experience the catastrophic effects of climate change as economic sectors (such as agriculture) struggle with unpredictable and extreme weather events. Artificial Intelligence (AI) should help us handle these climate challenges but its most promising solutions are not good at dealing with climate-disrupted data; specifically, machine learning methods that work from historical data-distributions, are not good at handling out-of-distribution, outlier events. In this paper, we propose a novel data augmentation method, that treats the predictive problems around climate change as being, in part, due to class-imbalance issues; that is, prediction from historical datasets is difficult because, by definition, they lack sufficient minority-class instances of "climate outlier events". This novel data augmentation method -- called Counterfactual-Based SMOTE (CFA-SMOTE) -- combines an instance-based counterfactual method from Explainable AI (XAI) with the well-known class-imbalance method, SMOTE. CFA-SMOTE creates synthetic data-points representing outlier, climate-events that augment the dataset to improve predictive performance. We report comparative experiments using this CFA-SMOTE method, comparing it to benchmark counterfactual and class-imbalance methods under different conditions (i.e., class-imbalance ratios). The focal climate-change domain used relies on predicting grass growth on Irish dairy farms, during Europe-wide drought and forage crisis of 2018.

</details>


### [314] [LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code](https://arxiv.org/abs/2511.11954)
*Borchuluun Yadamsuren,Steven Keith Platt,Miguel Diaz*

Main category: cs.AI

TL;DR: Hybrid neuro-symbolic framework uses GPT-4o/5 and Prolog to deterministically detect statutory inconsistencies in the IRS code, outperforming purely probabilistic prompting and yielding reproducible results.


<details>
  <summary>Details</summary>
Motivation: Tax law is highly hierarchical and deeply structured, demanding deep structured reasoning. LLMs struggle with long texts and deterministic inference, and tax-specific applications are sparse. A transparent, deterministic approach is needed for compliance, fairness, and statutory drafting.

Method: Translate Section 121 into Prolog rules with GPT-4o and refine in SWISH; compare Prolog-augmented prompting to natural-language prompting; test on inconsistency detection; GPT-5 refines the formalization; formalize competing interpretations; Prolog-based reasoning yields deterministic results that can identify the inconsistency zone autonomously.

Result: GPT-4o alone (natural language or Prolog-augmented) detected the inconsistency in only 1 of 3 strategies (33% accuracy), though natural-language prompting achieved 100% rule coverage while Prolog-augmented prompting achieved 66% coverage. The hybrid Prolog model produced deterministic, reproducible results. Guided by GPT-5, the model formalized competing interpretations and successfully detected the inconsistency zone. Validation shows the Prolog implementation is accurate, internally consistent, deterministic, and capable of autonomously identifying inconsistencies.

Conclusion: LLM-assisted formalization anchored in symbolic logic enables transparent and reliable statutory inconsistency detection. The hybrid Prolog approach provides deterministic performance, offering a promising path for compliant drafting and auditing of complex statutes.

Abstract: This study introduces a hybrid neuro-symbolic framework that achieves deterministic detection of statutory inconsistency in complex law. We use the U.S. Internal Revenue Code (IRC) as a case study because its complexity makes it a fertile domain for identifying conflicts. Our research offers a solution for detecting inconsistent provisions by combining Large Language Models (LLMs) with symbolic logic.
  LLM-based methods can support compliance, fairness, and statutory drafting, yet tax-specific applications remain sparse. A key challenge is that such models struggle with hierarchical processing and deep structured reasoning, especially over long text.
  This research addresses these gaps through experiments using GPT-4o, GPT-5, and Prolog. GPT-4o was first used to translate Section 121 into Prolog rules and refine them in SWISH. These rules were then incorporated into prompts to test whether Prolog-augmented prompting improved GPT-4o's inconsistency detection. GPT-4o, whether prompted with natural language alone or with Prolog augmentation, detected the inconsistency in only one of three strategies (33 percent accuracy), but its reasoning quality differed: natural-language prompting achieved 100 percent rule coverage, while Prolog-augmented prompting achieved 66 percent, indicating more incomplete statutory analysis.
  In contrast to probabilistic prompting, the hybrid Prolog model produced deterministic and reproducible results. Guided by GPT-5 for refinement, the model formalized the IRC section's competing interpretations and successfully detected an inconsistency zone. Validation tests confirm that the Prolog implementation is accurate, internally consistent, deterministic, and capable of autonomously identifying inconsistencies. These findings show that LLM-assisted formalization, anchored in symbolic logic, enables transparent and reliable statutory inconsistency detection.

</details>


### [315] [Improving Autoformalization Using Direct Dependency Retrieval](https://arxiv.org/abs/2511.11990)
*Shaoqi Wang,Lu Yu,Chunjie Yang*

Main category: cs.AI

TL;DR: Proposes Direct Dependency Retrieval (DDR) for statement autoformalization to directly generate and verify formal library dependencies, improving retrieval precision/recall and reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: Current autoformalization struggles with context, hallucination, and poor retrieval-augmented methods, and scalability to ever-growing public datasets for formal libraries.

Method: DDR directly generates candidate dependencies from natural language descriptions and verifies their existence in the formal library via a suffix array check. Builds a 500k+ dependency retrieval dataset and fine-tunes a high-precision DDR model.

Result: DDR significantly outperforms state-of-the-art methods in retrieval precision and recall; autoformalizers using DDR show higher single-attempt accuracy and greater multi-attempt stability than traditional RAG-based models.

Conclusion: DDR-based retrieval and verification framework improves accuracy, reliability, and scalability of statement autoformalization by effectively reducing hallucinations and leveraging large formal-library datasets.

Abstract: The convergence of deep learning and formal mathematics has spurred research in formal verification. Statement autoformalization, a crucial first step in this process, aims to translate informal descriptions into machine-verifiable representations but remains a significant challenge. The core difficulty lies in the fact that existing methods often suffer from a lack of contextual awareness, leading to hallucination of formal definitions and theorems. Furthermore, current retrieval-augmented approaches exhibit poor precision and recall for formal library dependency retrieval, and lack the scalability to effectively leverage ever-growing public datasets. To bridge this gap, we propose a novel retrieval-augmented framework based on DDR (\textit{Direct Dependency Retrieval}) for statement autoformalization. Our DDR method directly generates candidate library dependencies from natural language mathematical descriptions and subsequently verifies their existence within the formal library via an efficient suffix array check. Leveraging this efficient search mechanism, we constructed a dependency retrieval dataset of over 500,000 samples and fine-tuned a high-precision DDR model. Experimental results demonstrate that our DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Consequently, an autoformalizer equipped with DDR shows consistent performance advantages in both single-attempt accuracy and multi-attempt stability compared to models using traditional selection-based RAG methods.

</details>


### [316] [Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning](https://arxiv.org/abs/2511.12003)
*Shuochen Liu,Pengfei Luo,Chao Zhang,Yuhao Chen,Haotian Zhang,Qi Liu,Xin Kou,Tong Xu,Enhong Chen*

Main category: cs.AI

TL;DR: Proposes Chain-of-Evidence (CoE) for visual document retrieval-augmented generation (VD-RAG), unifying Chain-of-Thought reasoning with visual evidence grounding. Introduces Look As You Think (LAT), a reinforcement learning framework to train models to produce verifiable, attribution-consistent reasoning paths. LAT improves evidence-grounded QA on Paper- and Wiki-VISA benchmarks, outperforming both vanilla and supervised baselines, with notable gains in soft EM and IoU@0.5 and better cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: Visual document QA models often rely on end-to-end training, lacking fine-grained supervision and traceability of reasoning. There is a need to ground reasoning steps to specific visual evidence (regions) to enable verifiable predictions and reliable diagnosis. CoE aims to integrate reasoning with tangible evidence grounding to improve verifiability.

Method: CoE paradigm grounds each reasoning step to specific image regions via bounding boxes and page indexes, creating a traceable chain of evidence. LAT is a reinforcement learning framework that trains the model to produce these evidence-grounded reasoning paths. During training, LAT measures attribution consistency for each evidence region and provides rewards only when the CoE trajectory yields correct answers, promoting process-level self-verification.

Result: Experiments on Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show LAT improves over vanilla models in single- and multi-image settings, with average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. LAT also outperforms a supervised fine-tuning baseline and demonstrates stronger generalization across domains.

Conclusion: The CoE+LAT framework delivers verifiable, evidence-grounded reasoning for VD-RAG, enhancing interpretability, accuracy, and cross-domain robustness compared to end-to-end and supervised approaches.

Abstract: Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.

</details>


### [317] [Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models](https://arxiv.org/abs/2511.12008)
*Yunqi Hong,Johnson Kao,Liam Edwards,Nein-Tzu Liu,Chung-Yen Huang,Alex Oliveira-Kowaleski,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.AI

TL;DR: RECAP-PATH is an interpretable, self-learning framework for multimodal LLMs in pathology that links evidence to reasoning, achieving improved diagnostic accuracy with small labeled sets while providing audit-friendly rationales.


<details>
  <summary>Details</summary>
Motivation: Adoption of AI in pathology is hindered by the lack of human-readable, audit-ready reasoning. There is a need for models that couple visual understanding with transparent, evidence-linked interpretation to prevent errors and enable clinical trust.

Method: A two-phase self-learning process: diversification (expands pathology-style explanations) and optimization (refines them for accuracy). The framework derives diagnostic criteria autonomously, requires only small labeled datasets, and does not require white-box access or weight updates to generate cancer diagnoses. It uses multimodal large language models to link visual evidence with diagnostic reasoning.

Result: On breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and achieved substantial gains in diagnostic accuracy over baselines.

Conclusion: RECAP-PATH unites visual understanding with reasoning to deliver clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation in pathology.

Abstract: AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.

</details>


### [318] [Intelligent Collaborative Optimization for Rubber Tyre Film Production Based on Multi-path Differentiated Clipping Proximal Policy Optimization](https://arxiv.org/abs/2511.12060)
*Yinghao Ruan,Wei Pang,Shuaihao Liu,Huili Yang,Leyi Han,Xinghui Dong*

Main category: cs.AI

TL;DR: A multi-branch PPO with differentiated gradient clipping (MPD-PPO) for high-dimensional, multi-objective control in tyre manufacturing, achieving better tuning accuracy and operational efficiency in width/thickness control.


<details>
  <summary>Details</summary>
Motivation: Traditional centralized scheduling struggles to cope with dynamic production demands in rubber tyre manufacturing. The network of tightly coupled subsystems exhibits nonlinear interactions and emergent dynamics, making coordination difficult. There is a need for a scalable, stable, high-dimensional, multi-objective control approach suitable for real-time deployment.

Method: Introduce MPD-PPO, a deep reinforcement learning algorithm with a multi-branch policy architecture and differentiated gradient clipping constraints to stabilize high-dimensional policy updates. Validate on width and thickness control in tyre film production.

Result: MPD-PPO delivers substantial improvements in tuning accuracy and operational efficiency, handling high dimensionality, multi-objective trade-offs, and dynamic adaptation. It enhances production stability and supports potential real-time industrial deployment in tyre manufacturing.

Conclusion: MPD-PPO is a promising framework for real-time, high-dimensional, multi-objective control in tyre manufacturing, addressing key coordination challenges and potentially generalizable to similar complex industrial systems.

Abstract: The advent of smart manufacturing is addressing the limitations of traditional centralized scheduling and inflexible production line configurations in the rubber tyre industry, especially in terms of coping with dynamic production demands. Contemporary tyre manufacturing systems form complex networks of tightly coupled subsystems pronounced nonlinear interactions and emergent dynamics. This complexity renders the effective coordination of multiple subsystems, posing an essential yet formidable task. For high-dimensional, multi-objective optimization problems in this domain, we introduce a deep reinforcement learning algorithm: Multi-path Differentiated Clipping Proximal Policy Optimization (MPD-PPO). This algorithm employs a multi-branch policy architecture with differentiated gradient clipping constraints to ensure stable and efficient high-dimensional policy updates. Validated through experiments on width and thickness control in rubber tyre film production, MPD-PPO demonstrates substantial improvements in both tuning accuracy and operational efficiency. The framework successfully tackles key challenges, including high dimensionality, multi-objective trade-offs, and dynamic adaptation, thus delivering enhanced performance and production stability for real-time industrial deployment in tyre manufacturing.

</details>


### [319] [Bayesian Optimization in Language Space: An Eval-Efficient AI Self-Improvement Framework](https://arxiv.org/abs/2511.12063)
*Enoch Hyunwook Kang,Hema Yoganarasimhan*

Main category: cs.AI

TL;DR: TextGrad-Best-of-N Bayesian Optimization (T-BoN BO) is proposed to improve evaluation efficiency in LLM-based self-improvement by showing that Best-of-N selection with textual gradients emulates the UCB acquisition function; empirically validated on ad alignment tasks with superior performance over baselines.


<details>
  <summary>Details</summary>
Motivation: Evaluation efficiency is the bottleneck in societal applications of self-improving AI; while prior work reduces the number of generated solutions, the cost and time to evaluate solutions dominate. Extending Bayesian Optimization to language space can optimize evaluation steps, but direct acquisition-function estimation in LLMs is challenging.

Method: prove that Best-of-N with simple textual gradients (edits from a critic model) statistically emulate the gradients of the canonical UCB acquisition function; build TextGrad-Best-of-N BO (T-BoN BO) as a simple, eval-efficient LLM optimization framework; apply to language-space optimization tasks focusing on evaluation cost.

Result: Empirical validation on automated ad alignment tasks for persona distribution shows T-BoN BO achieves superior evaluation efficiency and outperforms popular state-of-the-art baselines.

Conclusion: A simple combination of Best-of-N and textual gradient feedback can replicate UCB-driven exploration in language space, enabling efficient self-improvement in LLM systems without complex acquisition-function estimation.

Abstract: Large Language Models (LLMs) have recently enabled self-improving AI, i.e., AI that iteratively generates, evaluates, and refines its own outcomes. Recent studies have shown that self-improving AI focusing on prompt optimization can outperform state-of-the-art reinforcement-learning fine-tuned LLMs. Here, their `performance' is typically measured by query efficiency - the number of LLM-generated solution samples required to meet a certain performance threshold. However, in many societal applications, the primary limitation is not generating new solutions but evaluating them. For instance, evaluating an ad's effectiveness requires significant human feedback, which is far more costly and time-consuming than generating a candidate ad. To optimize for the evaluation efficiency objective, a natural approach is to extend Bayesian Optimization (BO), a framework proven optimal for evaluation efficiency, to the language domain. However, the difficulty of directly estimating suitable acquisition functions in LLMs' minds makes this extension challenging. This paper overcomes this challenge by proving that the combination of the simple and widely used Best-of-N selection strategy and simple textual gradients (i.e., textual edits from a critic model) statistically emulates the behavior of the gradients on the canonical UCB acquisition function, which induces optimal exploration in terms of evaluation efficiency. Based on this result, we propose TextGrad-Best-of-N Bayesian Optimization (T-BoN BO), a simple and eval-efficient language-space Bayesian optimization framework for AI self-improvement. We also empirically validate T-BoN BO by applying it to automated ad alignment tasks for persona distribution, demonstrating its superior performance compared to popular state-of-the-art baselines.

</details>


### [320] [No-Regret Strategy Solving in Imperfect-Information Games via Pre-Trained Embedding](https://arxiv.org/abs/2511.12083)
*Yanchang Fu,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: Embeddings of information-set abstractions in a continuous space improve strategy solving in imperfect-information extensive-form games, outperforming cluster-based abstractions in poker; first to pre-train abstractions via embedding.


<details>
  <summary>Details</summary>
Motivation: Hard discrete clustering loses fine-grained distinctions between information sets, limiting solving quality in large IIEFGs.

Method: Pre-train and embed isolated information-set features into a low-dimensional, interconnected embedding space; perform regret-based CFR updates within this space (Embedding CFR) with theoretical regret guarantees.

Result: In poker, achieving significantly faster exploitability convergence at the same spatial overhead compared to cluster-based abstraction methods; demonstrates efficacy and faster convergence.

Conclusion: Embedding CFR provides a novel, embedding-based abstraction for information sets in poker AI, reducing information loss and enabling more efficient strategy solving; first to pre-train abstractions via low-dimensional embeddings for this domain.

Abstract: High-quality information set abstraction remains a core challenge in solving large-scale imperfect-information extensive-form games (IIEFGs)-such as no-limit Texas Hold'em-where the finite nature of spatial resources hinders strategy solving over the full game. State-of-the-art AI methods rely on pre-trained discrete clustering for abstraction, yet their hard classification irreversibly loses critical information: specifically, the quantifiable subtle differences between information sets-vital for strategy solving-thereby compromising the quality of such solving. Inspired by the word embedding paradigm in natural language processing, this paper proposes the Embedding CFR algorithm, a novel approach for solving strategies in IIEFGs within an embedding space. The algorithm pre-trains and embeds features of isolated information sets into an interconnected low-dimensional continuous space, where the resulting vectors more precisely capture both the distinctions and connections between information sets. Embedding CFR presents a strategy-solving process driven by regret accumulation and strategy updates within this embedding space, with accompanying theoretical analysis verifying its capacity to reduce cumulative regret. Experiments on poker show that with the same spatial overhead, Embedding CFR achieves significantly faster exploitability convergence compared to cluster-based abstraction algorithms, confirming its effectiveness. Furthermore, to our knowledge, it is the first algorithm in poker AI that pre-trains information set abstractions through low-dimensional embedding for strategy solving.

</details>


### [321] [KrwEmd: Revising the Imperfect-Recall Abstraction from Forgetting Everything](https://arxiv.org/abs/2511.12089)
*Yanchang Fu,Qiyue Yin,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: KrwEmd introduces k-recall winrate features and Earth Mover's Distance–based clustering to address excessive imperfect-recall abstraction in hand abstraction for large imperfect-information games (e.g., Texas Hold'em), improving AI performance.


<details>
  <summary>Details</summary>
Motivation: Excessive abstraction, especially imperfect-recall that discards history, degrades AI performance in large imperfect-information games; there is a need for practical, scalable methods that preserve informative signals from past and future game states.

Method: Define a k-recall winrate feature that incorporates both future and historical information to characterize signal observation infosets; use Earth Mover's Distance to cluster infosets by feature similarity; KrwEmd algorithm applies this clustering to refine hand abstractions.

Result: Experimental results show KrwEmd significantly improves AI gameplay performance compared with existing abstraction methods.

Conclusion: KrwEmd offers a practical solution to the excessive abstraction problem in imperfect-information games, improving AI effectiveness while maintaining tractability.

Abstract: Excessive abstraction is a critical challenge in hand abstraction-a task specific to games like Texas hold'em-when solving large-scale imperfect-information games, as it impairs AI performance. This issue arises from extreme implementations of imperfect-recall abstraction, which entirely discard historical information. This paper presents KrwEmd, the first practical algorithm designed to address this problem. We first introduce the k-recall winrate feature, which not only qualitatively distinguishes signal observation infosets by leveraging both future and, crucially, historical game information, but also quantitatively captures their similarity. We then develop the KrwEmd algorithm, which clusters signal observation infosets using earth mover's distance to measure discrepancies between their features. Experimental results demonstrate that KrwEmd significantly improves AI gameplay performance compared to existing algorithms.

</details>


### [322] [MetaGDPO: Alleviating Catastrophic Forgetting with Metacognitive Knowledge through Group Direct Preference Optimization](https://arxiv.org/abs/2511.12113)
*Lanxue Zhang,Yuqiang Xie,Fang Fang,Fanglong Dong,Rui Liu,Yanan Cao*

Main category: cs.AI

TL;DR: A data- and training-centric approach to mitigate catastrophic forgetting in small LLMs by (1) building a 5K-instance metacognitive dataset for robust distillation, and (2) introducing GDPO, a resource-efficient optimization method guided by a reference model to constrain parameter drift and improve knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting in small (<8B) LLMs during distillation from large models. The problem arises from misalignment between training data and inherent abilities and from objectives that fail to preserve previously learned skills, especially under resource constraints.

Method: Data side: construct a 5K-instance dataset spanning multiple reasoning tasks, annotate metacognitive knowledge required for each question, and filter data by task knowledge and the model’s inherent skills to improve distillability. Training side: propose GDPO (Group Direction Preference Optimization), a resource-efficient optimization method that approximates GRPO, guided by a reference (large) model to constrain the optimization path and limit excessive parameter drift, enabling effective knowledge transfer.

Result: Extensive experiments show that the approach significantly alleviates catastrophic forgetting and improves reasoning performance on smaller models compared to baselines.

Conclusion: Combining metacognitive-data curation with GDPO enables more robust knowledge transfer from large models to small ones and mitigates forgetting under resource constraints, enhancing reasoning capabilities.

Abstract: Large Language Models demonstrate strong reasoning capabilities, which can be effectively compressed into smaller models. However, existing datasets and fine-tuning approaches still face challenges that lead to catastrophic forgetting, particularly for models smaller than 8B. First, most datasets typically ignore the relationship between training data knowledge and the model's inherent abilities, making it difficult to preserve prior knowledge. Second, conventional training objectives often fail to constrain inherent knowledge preservation, which can result in forgetting of previously learned skills. To address these issues, we propose a comprehensive solution that alleviates catastrophic forgetting from both the data and fine-tuning approach perspectives. On the data side, we construct a dataset of 5K instances that covers multiple reasoning tasks and incorporates metacognitive knowledge, making it more tolerant and effective for distillation into smaller models. We annotate the metacognitive knowledge required to solve each question and filter the data based on task knowledge and the model's inherent skills. On the training side, we introduce GDPO (Group Direction Preference Optimization), which is better suited for resource-limited scenarios and can efficiently approximate the performance of GRPO. Guided by the large model and by implicitly constraining the optimization path through a reference model, GDPO enables more effective knowledge transfer from the large model and constrains excessive parameter drift. Extensive experiments demonstrate that our approach significantly alleviates catastrophic forgetting and improves reasoning performance on smaller models.

</details>


### [323] [RTMol: Rethinking Molecule-text Alignment in a Round-trip View](https://arxiv.org/abs/2511.12135)
*Letian Chen,Runhan Shi,Gufeng Yu,Yang Yang*

Main category: cs.AI

TL;DR: A self-supervised, bidirectional framework RTMol aligns molecule-to-text and text-to-SMILES via round-trip learning, enabling unsupervised training for molecular captioning and achieving strong bidirectional alignment improvements (up to 47%) across LLMs.


<details>
  <summary>Details</summary>
Motivation: Align molecular sequence representations with textual descriptions for drug discovery, materials design, and automated literature analysis; existing methods treat captioning and design separately and rely on supervised/contrastive learning, suffering from chemical-accuracy vs fluency mismatch, ambiguous data, and inconsistent optimization.

Method: Proposes RTMol, unifying molecular captioning and text-to-SMILES generation through self-supervised round-trip learning; introduces round-trip evaluation metrics; enables unsupervised training for captioning without paired molecule-text corpora.

Result: Empirically, RTMol improves bidirectional alignment by up to 47% across various large language models, validating the effectiveness of bidirectional joint training.

Conclusion: RTMol provides a robust paradigm for joint molecule-text understanding and generation, addressing limitations of existing approaches via round-trip self-supervision and unsupervised training, enhancing alignment quality.

Abstract: Aligning molecular sequence representations (e.g., SMILES notations) with textual descriptions is critical for applications spanning drug discovery, materials design, and automated chemical literature analysis. Existing methodologies typically treat molecular captioning (molecule-to-text) and text-based molecular design (text-to-molecule) as separate tasks, relying on supervised fine-tuning or contrastive learning pipelines. These approaches face three key limitations: (i) conventional metrics like BLEU prioritize linguistic fluency over chemical accuracy, (ii) training datasets frequently contain chemically ambiguous narratives with incomplete specifications, and (iii) independent optimization of generation directions leads to bidirectional inconsistency. To address these issues, we propose RTMol, a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning. The framework introduces novel round-trip evaluation metrics and enables unsupervised training for molecular captioning without requiring paired molecule-text corpora. Experiments demonstrate that RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation.

</details>


### [324] [Incremental Maintenance of DatalogMTL Materialisations](https://arxiv.org/abs/2511.12169)
*Kaiyue Zhao,Dingqi Chen,Shaoyu Wang,Pan Hu*

Main category: cs.AI

TL;DR: DRedMTL is an incremental reasoning algorithm for DatalogMTL with bounded intervals that extends the classic DRed approach to handle periodic materialisation representations, yielding significant performance gains over rematerialisation on public datasets.


<details>
  <summary>Details</summary>
Motivation: There is a need for efficient dynamic updates in temporal Datalog reasoning. Existing approaches (materialisation-based and automata-based) are sound and complete but do not support efficient incremental updates required by real-world, continually changing temporal data.

Method: Extend the DRed algorithm to DRedMTL for DatalogMTL with bounded intervals. Represent materialisation as a finite set of facts plus periodic intervals that describe how to unfold the full materialisation. Develop specialized operators to efficiently manage these periodic representations and integrate them into an incremental update workflow. Implement and evaluate the approach on public datasets.

Result: Empirical evaluation shows DRedMTL often significantly outperforms rematerialisation, sometimes by orders of magnitude.

Conclusion: DRedMTL provides an effective incremental reasoning solution for DatalogMTL with bounded intervals, enabling scalable temporal reasoning with dynamic updates through a periodic-materialisation representation and tailored operators.

Abstract: DatalogMTL extends the classical Datalog language with metric temporal logic (MTL), enabling expressive reasoning over temporal data. While existing reasoning approaches, such as materialisation based and automata based methods, offer soundness and completeness, they lack support for handling efficient dynamic updates, a crucial requirement for real-world applications that involve frequent data updates. In this work, we propose DRedMTL, an incremental reasoning algorithm for DatalogMTL with bounded intervals. Our algorithm builds upon the classical DRed algorithm, which incrementally updates the materialisation of a Datalog program. Unlike a Datalog materialisation which is in essence a finite set of facts, a DatalogMTL materialisation has to be represented as a finite set of facts plus periodic intervals indicating how the full materialisation can be constructed through unfolding. To cope with this, our algorithm is equipped with specifically designed operators to efficiently handle such periodic representations of DatalogMTL materialisations. We have implemented this approach and tested it on several publicly available datasets. Experimental results show that DRedMTL often significantly outperforms rematerialisation, sometimes by orders of magnitude.

</details>


### [325] [Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering](https://arxiv.org/abs/2511.12208)
*Jilong Liu,Pengyang Shao,Wei Qin,Fei Liu,Yonghui Yang,Richang Hong*

Main category: cs.AI

TL;DR: A multi-agent debate framework (DoM) that fuses KG and text for incomplete KGQA, using specialized KG and RAG agents plus a judge to iteratively reason; introduces a real-world update-based dataset; shows superior performance over baselines.


<details>
  <summary>Details</summary>
Motivation: Real-world knowledge graphs are often incomplete, causing KGQA systems to struggle. Existing methods lack adaptive, context-aware fusion of structured and unstructured sources to exploit their complementary strengths.

Method: DoM deploys the Multi-Agent Debate paradigm with specialized agents for KG inference and retrieval-augmented text reasoning. It decomposes the question into sub-questions, retrieves evidence via the KG agent and RAG, and uses a judge agent to evaluate and aggregate intermediate answers. It also introduces a dataset built from real-world knowledge updates (Incomplete Knowledge Graph WebQuestions).

Result: Empirical experiments show DoM consistently outperforms state-of-the-art baselines on the tasks and datasets tested.

Conclusion: Dynamic, cooperative integration of structured and unstructured knowledge via multi-agent debate improves robustness to KG incompleteness and better exploits knowledge complementarity; the dataset offers a realistic benchmark for IKGQA.

Abstract: Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.

</details>


### [326] [ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2511.12214)
*Ruochen Li,Zhanxing Zhu,Tanqiu Qiao,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: ViTE introduces a two-module framework for pedestrian trajectory prediction that avoids deep GNN stacks by using a Virtual Graph with dynamic virtual nodes to capture long-range/high-order interactions and an Expert Router (Mixture-of-Experts) to adaptively select interaction patterns based on social context, achieving state-of-the-art results with improved efficiency on ETH/UCY, NBA, and SDD benchmarks.


<details>
  <summary>Details</summary>
Motivation: GNN-based trajectory models face a trade-off between depth (to capture high-order interactions) and computational cost. Relying solely on deeper networks can be inefficient and may still under-reach due to limited receptive fields. There is a need to model both explicit one-hop interactions and implicit high-order dependencies in a flexible, scalable way.

Method: Introduce ViTE with two components: (1) Virtual Graph that injects dynamic virtual nodes to model long-range and high-order interactions without stacking many GNN layers; (2) Expert Router that uses a Mixture-of-Experts to adaptively select interaction experts according to the social context, enabling flexible reasoning over varying interaction patterns.

Result: Empirical evaluation on three benchmarks (ETH/UCY, NBA, SDD) shows state-of-the-art performance and improved practical efficiency, demonstrating the method’s effectiveness in capturing diverse social interactions without excessive depth.

Conclusion: ViTE provides a flexible, scalable approach to pedestrian trajectory prediction by decoupling interaction modeling from network depth, combining explicit social cues via the virtual graph with adaptive, context-aware expert selection, yielding strong performance efficiently across diverse scenarios.

Abstract: Pedestrian trajectory prediction is critical for ensuring safety in autonomous driving, surveillance systems, and urban planning applications. While early approaches primarily focus on one-hop pairwise relationships, recent studies attempt to capture high-order interactions by stacking multiple Graph Neural Network (GNN) layers. However, these approaches face a fundamental trade-off: insufficient layers may lead to under-reaching problems that limit the model's receptive field, while excessive depth can result in prohibitive computational costs. We argue that an effective model should be capable of adaptively modeling both explicit one-hop interactions and implicit high-order dependencies, rather than relying solely on architectural depth. To this end, we propose ViTE (Virtual graph Trajectory Expert router), a novel framework for pedestrian trajectory prediction. ViTE consists of two key modules: a Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks, and an Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This combination enables flexible and scalable reasoning across varying interaction patterns. Experiments on three benchmarks (ETH/UCY, NBA, and SDD) demonstrate that our method consistently achieves state-of-the-art performance, validating both its effectiveness and practical efficiency.

</details>


### [327] [Beyond World Models: Rethinking Understanding in AI Models](https://arxiv.org/abs/2511.12239)
*Tarun Gupta,Danish Pruthi*

Main category: cs.AI

TL;DR: World models use internal world-like representations to predict and explain; however, the authors argue that these models may not capture true human-level understanding, as shown by philosophy-of-science case studies.


<details>
  <summary>Details</summary>
Motivation: to critically assess whether world-model frameworks adequately characterize human-level understanding and to delineate the boundary between model capabilities and genuine understanding.

Method: analyzes case studies from philosophy of science where the distinction between world-model capabilities and understanding is most pronounced.

Result: the analysis highlights limitations of world-models in accounting for human-like understanding; case-based critique suggests that world-modeling alone may be insufficient to define understanding.

Conclusion: world-model frameworks have limitations in capturing human-level understanding; understanding may require criteria beyond internal world representations; further philosophical and empirical work is needed.

Abstract: World models have garnered substantial interest in the AI community. These are internal representations that simulate aspects of the external world, track entities and states, capture causal relationships, and enable prediction of consequences. This contrasts with representations based solely on statistical correlations. A key motivation behind this research direction is that humans possess such mental world models, and finding evidence of similar representations in AI models might indicate that these models "understand" the world in a human-like way. In this paper, we use case studies from the philosophy of science literature to critically examine whether the world model framework adequately characterizes human-level understanding. We focus on specific philosophical analyses where the distinction between world model capabilities and human understanding is most pronounced. While these represent particular views of understanding rather than universal definitions, they help us explore the limits of world models.

</details>


### [328] [AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos](https://arxiv.org/abs/2511.12241)
*Junhyuk Seo,Hyeyoon Moon,Kyu-Hwan Jung,Namkee Oh,Taerim Kim*

Main category: cs.AI

TL;DR: AURA uses privacy-preserving synthetic videos (via text-to-video diffusion) to train a vision-based unplanned extubation risk detector, achieving high accuracy for collision and moderate accuracy for agitation.


<details>
  <summary>Details</summary>
Motivation: Real-time unplanned extubation detection in ICUs is hampered by ethical/privacy barriers to annotating ICU video data; synthetic data offers a privacy-preserving, reproducible alternative.

Method: Generate diverse, clinically realistic ICU scenarios with text-to-video diffusion; apply pose estimation to detect collision (hand entering airway-adjacent zones) and agitation (keypoint velocity); validate realism with experts.

Result: High accuracy for collision detection; moderate performance for agitation recognition; dataset demonstrated as privacy-preserving and reproducible; potential for deployment.

Conclusion: Demonstrates a novel pathway for privacy-preserving, reproducible patient safety monitoring systems in ICUs using synthetic data and computer vision.

Abstract: Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.

</details>


### [329] [Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation](https://arxiv.org/abs/2511.12254)
*Yuxiang Zhou,Jichang Li,Yanhao Zhang,Haonan Lu,Guanbin Li*

Main category: cs.AI

TL;DR: Proposes Mobile-Agent-RAG, a hierarchical multi-agent system with dual retrieval-augmented knowledge bases to improve planning and execution in cross-app, long-horizon mobile tasks, plus a new benchmark; achieves significant gains over SOTA.


<details>
  <summary>Details</summary>
Motivation: Static, internal knowledge in MLLMs causes planning hallucinations and UI execution errors. High-level planning requires strategy-oriented knowledge while low-level UI operations require precise, app-specific guidance; these require distinct knowledge types.

Method: Introduce Mobile-Agent-RAG with two retrieval augmentations: Manager-RAG for high-level, human-validated task plans to reduce strategic hallucinations; Operator-RAG for precise low-level actions aligned with the current app UI. Build two retrieval-focused knowledge bases corresponding to these roles. Also present Mobile-Eval-RAG benchmark for realistic multi-app, long-horizon tasks.

Result: Significant improvements over SoTA baselines: task completion rate up by 11.0% and step efficiency up by 10.2%.

Conclusion: Demonstrates a robust, context-aware paradigm for reliable, multi-app mobile automation by segregating planning and execution knowledge via dual retrieval augmentation; provides a new evaluation benchmark to drive future work.

Abstract: Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.

</details>


### [330] [MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning](https://arxiv.org/abs/2511.12271)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: LLMs can be steered to apply explicit moral frameworks to out-of-distribution scenarios using Moral-Reason-QA and Group Relative Policy Optimization, achieving improved alignment on unseen cases and highlighting a path for AI safety in decision-making.


<details>
  <summary>Details</summary>
Motivation: Move beyond evaluating LLM moral judgments to actively steering them, addressing generalization to novel scenarios and ensuring consistent framework-based reasoning.

Method: Create Moral-Reason-QA with 680 high-ambiguity moral scenarios and reasoning traces for utilitarian, deontological, and virtue ethics. Train LLM agents via Group Relative Policy Optimization with composite rewards that optimize both decision alignment and framework-specific reasoning.

Result: Out-of-distribution generalization achieved: softmax-alignment scores up by +0.757 for utilitarian and +0.450 for deontological frameworks; training challenges noted and directions for future work.

Conclusion: LLMs can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a foundations for safer AI-driven human decision-making.

Abstract: Large language models are increasingly influencing human moral decisions, yet current approaches focus primarily on evaluating rather than actively steering their moral decisions. We formulate this as an out-of-distribution moral alignment problem, where LLM agents must learn to apply consistent moral reasoning frameworks to scenarios beyond their training distribution. We introduce Moral-Reason-QA, a novel dataset extending 680 human-annotated, high-ambiguity moral scenarios with framework-specific reasoning traces across utilitarian, deontological, and virtue ethics, enabling systematic evaluation of moral generalization in realistic decision contexts. Our learning approach employs Group Relative Policy Optimization with composite rewards that simultaneously optimize decision alignment and framework-specific reasoning processes to facilitate learning of the underlying moral frameworks. Experimental results demonstrate successful generalization to unseen moral scenarios, with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks when tested on out-of-distribution evaluation sets. The experiments also reveal training challenges and promising directions that inform future research. These findings establish that LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making processes.

</details>


### [331] [UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI](https://arxiv.org/abs/2511.12306)
*Darvin Yi,Teng Liu,Mattie Terzolo,Lance Hasson,Ayan Sinh,Pablo Mendes,Andrew Rabinovich*

Main category: cs.AI

TL;DR: UpBench is a dynamic benchmark for evaluating LLM agents in real-world work by using verified Upwork client transactions and rubric-based expert evaluation, enabling granular, human-centered assessment of performance in authentic, evolving labor-market tasks.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks are static, synthetic, or domain-limited and miss real-world work dynamics, outcomes, and human collaboration aspects. There is a need for evaluation frameworks anchored to professional standards and actual financial outcomes to study agentic systems in authentic environments.

Method: Curate real Upwork tasks anchored to verified client transactions. Have expert freelancers decompose each job into detailed, verifiable acceptance criteria. Use rubric-based evaluation where AI submissions are scored per criterion with feedback. A human-in-the-loop data pipeline handles job curation, rubric construction, and evaluation. Regularly refresh tasks to reflect evolving online work.

Result: Claims a scalable, human-centered framework that ties evaluation to genuine work activity and financial outcomes, enabling fine-grained analysis of strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail, and supports human-AI collaboration research.

Conclusion: UpBench provides a path to evaluate agentic systems in authentic labor-market contexts, emphasizing collaboration with humans and the evolution of tasks to mirror real online work environments.

Abstract: As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.

</details>


### [332] [Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning](https://arxiv.org/abs/2511.12344)
*Baolong Bi,Shenghua Liu,Yiwei Wang,Siqian Tong,Lingrui Mei,Yuyao Ge,Yilong Xu,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: RGR-GRPO uses rubric-driven rewards and offline guidance to enable multi-domain LLM reasoning via GRPO, achieving consistent gains across 14 benchmarks and stable exploration.


<details>
  <summary>Details</summary>
Motivation: Current RL for LLMs is often single-domain and online-only, with verifiable rewards; this limits exploration and reasoning performance. Rubrics provide dense, fine-grained feedback and offline cues to guide learning.

Method: Introduce RGR-GRPO, a rubric-driven RL framework that uses rubrics to generate dense reward signals and offline guidance, enabling broader solution spaces during GRPO (a policy optimization approach). Evaluate on 14 multi-domain benchmarks.

Result: RGR-GRPO outperforms reward-only or offline-guidance baselines across all domains; average improvements in mathematics (+7.0%), physics (+5.4%), chemistry (+8.4%), general reasoning (+6.6%). Maintains stable entropy during off-policy training and achieves superior pass@k, indicating sustained exploration and breakthrough beyond bottlenecks.

Conclusion: Rubric-guided RL effectively enhances multi-domain reasoning for LLMs by combining dense rewards with offline guidance, expanding exploration and improving performance across diverse tasks; promising approach for scalable, robust LLM reasoning.

Abstract: Recent advances in reinforcement learning (RL) have significantly improved the complex reasoning capabilities of large language models (LLMs). Despite these successes, existing methods mainly focus on single-domain RL (e.g., mathematics) with verifiable rewards (RLVR), and their reliance on purely online RL frameworks restricts the exploration space, thereby limiting reasoning performance. In this paper, we address these limitations by leveraging rubrics to provide both fine-grained reward signals and offline guidance. We propose $\textbf{RGR-GRPO}$ (Reward and Guidance through Rubrics), a rubric-driven RL framework for multi-domain reasoning. RGR-GRPO enables LLMs to receive dense and informative rewards while exploring a larger solution space during GRPO training. Extensive experiments across 14 benchmarks spanning multiple domains demonstrate that RGR-GRPO consistently outperforms RL methods that rely solely on alternative reward schemes or offline guidance. Compared with verifiable online RL baseline, RGR-GRPO achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks, respectively. Notably, RGR-GRPO maintains stable entropy fluctuations during off-policy training and achieves superior pass@k performance, reflecting sustained exploration and effective breakthrough beyond existing performance bottlenecks.

</details>


### [333] [More Than Irrational: Modeling Belief-Biased Agents](https://arxiv.org/abs/2511.12359)
*Yifan Zhu,Sammie Katt,Samuel Kaski*

Main category: cs.AI

TL;DR: Proposes computational-rational models for cognitively-bounded users with memory-induced biases, and an online nested particle filtering method to infer latent bounds and beliefs from actions; validated on navigation tasks showing tractable inference and usefulness for adaptive AI assistants.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of predicting and inferring sub-optimal user behavior caused by cognitive bounds and biased beliefs, which may be rational given memory limitations, and to enable adaptive AI that accounts for these factors.

Method: Formalize a class of computational-rational (CR) user models with memory-decay as a cognitive bound that causes biased belief states and dynamically inconsistent decisions; develop an online inference algorithm based on nested particle filtering to simultaneously estimate the latent memory bound and the evolving beliefs from a stream of observed actions.

Result: Simulations show the CR model produces intuitive behaviors across memory capacities; the online inference accurately recovers ground-truth cognitive bounds from limited data (≤100 steps); demonstrates potential for adaptive AI assistants that tailor support to memory limitations.

Conclusion: A principled, tractable framework for modeling cognitively-bounded rational users and inferring their latent bounds from behavior, laying a foundation for adaptive AI that accounts for memory constraints.

Abstract: Despite the explosive growth of AI and the technologies built upon it, predicting and inferring the sub-optimal behavior of users or human collaborators remains a critical challenge. In many cases, such behaviors are not a result of irrationality, but rather a rational decision made given inherent cognitive bounds and biased beliefs about the world. In this paper, we formally introduce a class of computational-rational (CR) user models for cognitively-bounded agents acting optimally under biased beliefs. The key novelty lies in explicitly modeling how a bounded memory process leads to a dynamically inconsistent and biased belief state and, consequently, sub-optimal sequential decision-making. We address the challenge of identifying the latent user-specific bound and inferring biased belief states from passive observations on the fly. We argue that for our formalized CR model family with an explicit and parameterized cognitive process, this challenge is tractable. To support our claim, we propose an efficient online inference method based on nested particle filtering that simultaneously tracks the user's latent belief state and estimates the unknown cognitive bound from a stream of observed actions. We validate our approach in a representative navigation task using memory decay as an example of a cognitive bound. With simulations, we show that (1) our CR model generates intuitively plausible behaviors corresponding to different levels of memory capacity, and (2) our inference method accurately and efficiently recovers the ground-truth cognitive bounds from limited observations ($\le 100$ steps). We further demonstrate how this approach provides a principled foundation for developing adaptive AI assistants, enabling adaptive assistance that accounts for the user's memory limitations.

</details>


### [334] [Learning to Trust: Bayesian Adaptation to Varying Suggester Reliability in Sequential Decision Making](https://arxiv.org/abs/2511.12378)
*Dylan M. Asmar,Mykel J. Kochenderfer*

Main category: cs.AI

TL;DR: A framework for adaptive use of external suggestions in sequential decision tasks with uncertain reliability; learns suggester quality via Bayesian inference and adds an 'ask' action to request advice, enabling robust performance across changing reliability and enabling adaptive human-agent collaboration.


<details>
  <summary>Details</summary>
Motivation: To address the gap where suggester reliability is unknown and variable in real-world settings, and to move beyond static, known-quality assumptions.

Method: Integrate suggester quality into the agent's belief state; perform Bayesian inference over suggester types; introduce an explicit ask action with cost; evaluate in partially observable environments.

Result: Demonstrates robustness to varying suggester quality, adaptability to changing reliability, and strategic request management; shows benefit across different reliability conditions.

Conclusion: Provides a foundation for adaptive human-agent collaboration under suggestion uncertainty, enabling dynamic learning of adviser quality and decision-time requests for advice.

Abstract: Autonomous agents operating in sequential decision-making tasks under uncertainty can benefit from external action suggestions, which provide valuable guidance but inherently vary in reliability. Existing methods for incorporating such advice typically assume static and known suggester quality parameters, limiting practical deployment. We introduce a framework that dynamically learns and adapts to varying suggester reliability in partially observable environments. First, we integrate suggester quality directly into the agent's belief representation, enabling agents to infer and adjust their reliance on suggestions through Bayesian inference over suggester types. Second, we introduce an explicit ``ask'' action allowing agents to strategically request suggestions at critical moments, balancing informational gains against acquisition costs. Experimental evaluation demonstrates robust performance across varying suggester qualities, adaptation to changing reliability, and strategic management of suggestion requests. This work provides a foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments.

</details>


### [335] [Multi-agent Self-triage System with Medical Flowcharts](https://arxiv.org/abs/2511.12439)
*Yujia Liu,Sophia Yu,Hongyue Jin,Jessica Wen,Alexander Qian,Terrence Lee,Mattheus Ramsis,Gi Won Choi,Lianhui Qin,Xin Liu,Edward J. Wang*

Main category: cs.AI

TL;DR: A proof-of-concept AI self-triage system using 100 AMA clinical flowcharts with a three-agent pipeline (retrieval, decision, chat) to improve transparency and accuracy in patient-guided triage; achieved high synthetic-data accuracy (top-3 retrieval 95.29%, navigation 99.10%).


<details>
  <summary>Details</summary>
Motivation: Address reliability and transparency gaps in online health resources and LLM-based medical decision-support by combining standardized clinical protocols with conversational AI for patient self-triage.

Method: A multi-agent framework: a retrieval agent selects the most relevant AMA flowchart, a decision agent interprets patient input and maps it to flowchart steps, and a chat agent delivers personalized, patient-friendly recommendations. 100 validated AMA flowcharts used; evaluated with synthetic datasets: N=2,000 for retrieval and N=37,200 for navigation across varied conversational styles.

Result: The system achieved 95.29% top-3 accuracy in flowchart retrieval and 99.10% accuracy in flowchart navigation on synthetic data, indicating high accuracy and robustness in a controlled setting.

Conclusion: The approach demonstrates feasibility of transparent, accurate AI-assisted self-triage by coupling free-text interaction with structured clinical protocols, with potential to improve patient decision-making and optimize healthcare resource use; scalability and auditability are emphasized, though real-world validation is needed.

Abstract: Online health resources and large language models (LLMs) are increasingly used as a first point of contact for medical decision-making, yet their reliability in healthcare remains limited by low accuracy, lack of transparency, and susceptibility to unverified information. We introduce a proof-of-concept conversational self-triage system that guides LLMs with 100 clinically validated flowcharts from the American Medical Association, providing a structured and auditable framework for patient decision support. The system leverages a multi-agent framework consisting of a retrieval agent, a decision agent, and a chat agent to identify the most relevant flowchart, interpret patient responses, and deliver personalized, patient-friendly recommendations, respectively. Performance was evaluated at scale using synthetic datasets of simulated conversations. The system achieved 95.29% top-3 accuracy in flowchart retrieval (N=2,000) and 99.10% accuracy in flowchart navigation across varied conversational styles and conditions (N=37,200). By combining the flexibility of free-text interaction with the rigor of standardized clinical protocols, this approach demonstrates the feasibility of transparent, accurate, and generalizable AI-assisted self-triage, with potential to support informed patient decision-making while improving healthcare resource utilization.

</details>


### [336] [ARCHE: A Novel Task to Evaluate LLMs on Latent Reasoning Chain Extraction](https://arxiv.org/abs/2511.12485)
*Pengze Li,Jiaqi Liu,Junchi Yu,Lihao Liu,Mingyu Ding,Wanli Ouyang,Shixiang Tang,Xi Chen*

Main category: cs.AI

TL;DR: Introduces ARCHE: a task to extract latent reasoning chains by mapping steps to deduction, induction, or abduction; builds ARCHE Bench from Nature Communications; defines EC and REA; finds current LLMs show trade-off and fail to output complete reasoning chains, highlighting gap to rigorous scientific argumentation.


<details>
  <summary>Details</summary>
Motivation: To make scientific reasoning in LLMs explicit and standardizable, addressing unstructured chain-of-thought outputs that obscure fundamental inference paradigms.

Method: Define ARCHE task with Reasoning Logic Tree; create ARCHE Bench from 70 Nature Communications articles with >1,900 references and 38,000 viewpoints; propose EC and REA metrics; evaluate 10 leading LLMs.

Result: LLMs exhibit trade-off between EC and REA; none can extract a complete, standard reasoning chain; results reveal a gap between current model capabilities and the rigor of scientific argumentation.

Conclusion: Need for more structured, logic-aware reasoning approaches to align LLMs with scientific inference; ARCHE Bench provides a benchmark to drive progress toward transparent, verifiable reasoning.

Abstract: Large language models (LLMs) are increasingly used in scientific domains. While they can produce reasoning-like content via methods such as chain-of-thought prompting, these outputs are typically unstructured and informal, obscuring whether models truly understand the fundamental reasoning paradigms that underpin scientific inference. To address this, we introduce a novel task named Latent Reasoning Chain Extraction (ARCHE), in which models must decompose complex reasoning arguments into combinations of standard reasoning paradigms in the form of a Reasoning Logic Tree (RLT). In RLT, all reasoning steps are explicitly categorized as one of three variants of Peirce's fundamental inference modes: deduction, induction, or abduction. To facilitate this task, we release ARCHE Bench, a new benchmark derived from 70 Nature Communications articles, including more than 1,900 references and 38,000 viewpoints. We propose two logic-aware evaluation metrics: Entity Coverage (EC) for content completeness and Reasoning Edge Accuracy (REA) for step-by-step logical validity. Evaluations on 10 leading LLMs on ARCHE Bench reveal that models exhibit a trade-off between REA and EC, and none are yet able to extract a complete and standard reasoning chain. These findings highlight a substantial gap between the abilities of current reasoning models and the rigor required for scientific argumentation.

</details>


### [337] [LOBERT: Generative AI Foundation Model for Limit Order Book Messages](https://arxiv.org/abs/2511.12563)
*Eljas Linna,Kestutis Baltakys,Alexandros Iosifidis,Juho Kanniainen*

Main category: cs.AI

TL;DR: LOBERT is a BERT-like encoder-only model for limit order book data that tokenizes complete multi-dimensional messages as single tokens while preserving continuous representations of price, volume, and time, enabling improved downstream fine-tuning with shorter context requirements.


<details>
  <summary>Details</summary>
Motivation: Financial limit order books are hard to model due to irregular event timing, rapid regime shifts, and high-frequency traders' reactions to order flow; existing models rely on cumbersome data representations and lack task adaptability.

Method: Adapt the BERT architecture to LOB data by introducing a novel tokenization scheme that treats complete multi-dimensional messages as single tokens and retains continuous price, volume, and time features; operate as an encoder-only foundation model suitable for downstream fine-tuning.

Result: LOBERT achieves leading performance on tasks such as predicting mid-price movements and next messages, while reducing the required context length compared to previous methods.

Conclusion: LOBERT provides a general-purpose, adaptable encoder-only foundation model for LOB data that enables efficient fine-tuning for various downstream tasks with improved performance and shorter context requirements.

Abstract: Modeling the dynamics of financial Limit Order Books (LOB) at the message level is challenging due to irregular event timing, rapid regime shifts, and the reactions of high-frequency traders to visible order flow. Previous LOB models require cumbersome data representations and lack adaptability outside their original tasks, leading us to introduce LOBERT, a general-purpose encoder-only foundation model for LOB data suitable for downstream fine-tuning. LOBERT adapts the original BERT architecture for LOB data by using a novel tokenization scheme that treats complete multi-dimensional messages as single tokens while retaining continuous representations of price, volume, and time. With these methods, LOBERT achieves leading performance in tasks such as predicting mid-price movements and next messages, while reducing the required context length compared to previous methods.

</details>


### [338] [Enhancing Conversational Recommender Systems with Tree-Structured Knowledge and Pretrained Language Models](https://arxiv.org/abs/2511.12579)
*Yongwen Ren,Chao Wang,Peng Du,Chuan Qin,Dazhong Shen,Hui Xiong*

Main category: cs.AI

TL;DR: A prompt-based, retrieval-augmented framework PCRS-TKA enhances conversational recommender systems by converting KG-derived knowledge into dialogue-specific, serialized knowledge trees, enabling structure-aware reasoning, selective context filtering, and collaborative preference modeling to improve accuracy and reduce hallucination.


<details>
  <summary>Details</summary>
Motivation: Address key challenges in CRS: underutilization of PLM reasoning over graph relationships, indiscriminate incorporation of retrieved knowledge, and neglect of collaborative preferences in multi-turn dialogues.

Method: Build dialogue-specific knowledge trees from knowledge graphs and serialize them into text to enable structure-aware reasoning; implement selective filtering to fetch context-relevant knowledge; explicitly model collaborative preferences with specialized supervision signals; employ a semantic alignment module to harmonize heterogeneous inputs and reduce noise.

Result: Empirical evaluations show PCRS-TKA consistently outperforms all baselines in both recommendation accuracy and conversational quality across experiments.

Conclusion: PCRS-TKA effectively addresses core CRS challenges by integrating PLMs with KGs through structured, filter-aware, and collaboration-aware prompt-based retrieval-augmented generation, yielding improved performance and reduced hallucination.

Abstract: Recent advances in pretrained language models (PLMs) have significantly improved conversational recommender systems (CRS), enabling more fluent and context-aware interactions. To further enhance accuracy and mitigate hallucination, many methods integrate PLMs with knowledge graphs (KGs), but face key challenges: failing to fully exploit PLM reasoning over graph relationships, indiscriminately incorporating retrieved knowledge without context filtering, and neglecting collaborative preferences in multi-turn dialogues. To this end, we propose PCRS-TKA, a prompt-based framework employing retrieval-augmented generation to integrate PLMs with KGs. PCRS-TKA constructs dialogue-specific knowledge trees from KGs and serializes them into texts, enabling structure-aware reasoning while capturing rich entity semantics. Our approach selectively filters context-relevant knowledge and explicitly models collaborative preferences using specialized supervision signals. A semantic alignment module harmonizes heterogeneous inputs, reducing noise and enhancing accuracy. Extensive experiments demonstrate that PCRS-TKA consistently outperforms all baselines in both recommendation and conversational quality.

</details>


### [339] [Dynamic Tree Databases in Automated Planning](https://arxiv.org/abs/2511.12677)
*Oliver Joergensen,Dominik Drexler,Jendrik Seipp*

Main category: cs.AI

TL;DR: Dynamic tree databases enable compact state-set compression for planning, preserving static guarantees while offering large memory savings and small overhead.


<details>
  <summary>Details</summary>
Motivation: Scaling explicit state-space search is memory-bound: generating many states forces large preallocated structures. Static tree databases require substantial upfront memory; a dynamic alternative could reduce memory footprint without sacrificing key properties.

Method: Introduce a dynamic variant of tree databases for compressing state sets over propositional and numeric variables. Prove that it preserves the desirable properties of the static variant. Implement and evaluate on grounded and lifted planning tasks, including classical and numeric planning, comparing compression and runtime.

Result: Attains compression ratios of several orders of magnitude with negligible runtime overhead in many settings, across grounded and lifted, classical and numeric planning tasks.

Conclusion: The dynamic tree-database approach retains the static method’s guarantees while dramatically reducing memory usage, enabling scalable state-space exploration in large planning problems.

Abstract: A central challenge in scaling up explicit state-space search for large tasks is compactly representing the set of generated states. Tree databases, a data structure from model checking, require constant space per generated state in the best case, but they need a large preallocation of memory. We propose a novel dynamic variant of tree databases for compressing state sets over propositional and numeric variables and prove that it maintains the desirable properties of the static counterpart. Our empirical evaluation of state compression techniques for grounded and lifted planning on classical and numeric planning tasks reveals compression ratios of several orders of magnitude, often with negligible runtime overhead.

</details>


### [340] [Adaptively Coordinating with Novel Partners via Learned Latent Strategies](https://arxiv.org/abs/2511.12754)
*Benjamin Li,Shuyang Shi,Lucia Romero,Huao Li,Yaqi Xie,Woojun Kim,Stefanos Nikolaidis,Michael Lewis,Katia Sycara,Simon Stepputtis*

Main category: cs.AI

TL;DR: A strategy-conditioned cooperator learns a latent space of partner strategies via a VAE, clusters strategies, and trains a cooperator conditioned on these clusters, with online adaptation via fixed-share regret minimization; achieves state-of-the-art performance with novel partners in Overcooked.


<details>
  <summary>Details</summary>
Motivation: To enable real-time adaptation to diverse, dynamic partner policies in heterogeneous human-agent teams operating under time pressure and complex strategic spaces.

Method: Encode agent trajectories with a variational autoencoder to obtain a latent strategy space; cluster latent representations to identify distinct partner strategy types; train a cooperator conditioned on these clusters by simulating partners from each type; apply a fixed-share regret minimization algorithm to online infer and adjust the estimated partner strategy during interaction.

Result: In Overcooked with two-player coordination, the approach achieves state-of-the-art performance when paired with novel human or agent teammates; an online user study corroborates the improvements.

Conclusion: A strategy-conditioned framework can effectively represent, classify, and adapt to a broad range of partner strategies in real time, enabling robust collaboration under time constraints and across partner types.

Abstract: Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates.

</details>


### [341] [Optimal Foraging in Memory Retrieval: Evaluating Random Walks and Metropolis-Hastings Sampling in Modern Semantic Spaces](https://arxiv.org/abs/2511.12759)
*James Moore*

Main category: cs.AI

TL;DR: Random walks on state-of-the-art semantic embeddings can reproduce optimal foraging patterns in memory retrieval, aligning with MVT; Metropolis-Hastings sampling does not improve alignment with human data; simple sampling suffices when embeddings are well-structured.


<details>
  <summary>Details</summary>
Motivation: To assess whether modern high-dimensional embeddings provide representations that enable cognitive models of memory retrieval to match human foraging-like behavior, and to evaluate whether more complex sampling (MH) is necessary.

Method: Apply random-walk and Metropolis-Hastings sampling on state-of-the-art semantic embeddings to simulate semantic-fluency-like tasks, compare outcomes to human data and the Marginal Value Theorem (MVT).

Result: Random walks on embeddings produce results consistent with optimal foraging and the MVT. Introducing Metropolis-Hastings sampling does not align with human behavior. Properly structured embeddings with simple sampling can yield near-optimal foraging dynamics.

Conclusion: Supports Hills (2012) over Abbott (2015): modern embeddings can approximate human memory foraging without relying on complex acceptance criteria, indicating that representation structure can drive plausible retrieval dynamics with simple sampling.

Abstract: Human memory retrieval often resembles ecological foraging where animals search for food in a patchy environment. Optimal foraging means following the Marginal Value Theorem (MVT), in which individuals exploit a patch of semantically related concepts until it becomes less rewarding and then switch to a new cluster. While human behavioral data suggests foraging-like patterns in semantic fluency tasks, it remains unclear whether modern high-dimensional embedding spaces provide representations that allow algorithms to match observed human behavior. Using state-of-the-art embeddings and prior semantic fluency data, I find that random walks on these embedding spaces produce results consistent with optimal foraging and the MVT. Surprisingly, introducing Metropolis-Hastings sampling, an adaptive algorithm expected to model strategic acceptance and rejection of new clusters, does not produce results consistent with human behavior. These findings challenge the assumption that more complex sampling mechanisms inherently lead to better cognitive models of memory retrieval. Instead, they show that appropriately structured embeddings, even with simple sampling, can produce near-optimal foraging dynamics. This supports the perspective of Hills (2012) rather than Abbott (2015), demonstrating that modern embeddings can approximate human memory foraging without relying on complex acceptance criteria.

</details>


### [342] [Event-CausNet: Unlocking Causal Knowledge from Text with Large Language Models for Reliable Spatio-Temporal Forecasting](https://arxiv.org/abs/2511.12769)
*Luyao Niu,Zepu Wang,Shuyi Guan,Yang Liu,Peng Sun*

Main category: cs.AI

TL;DR: Event-CausNet uses LLM-derived event causality, a causal knowledge base via average treatment effects, and a causal-attention infused dual-stream GNN-LSTM to robustly forecast traffic during disruptions, achieving up to 35.87% MAE reduction over baselines.


<details>
  <summary>Details</summary>
Motivation: GNNs excel at recurring patterns but are brittle during non-recurring events (e.g., accidents) because they rely on historical correlations; there is a need to incorporate causal reasoning for reliability and interpretability in disruption scenarios.

Method: Quantify unstructured event reports with a Large Language Model; build a causal knowledge base by estimating average treatment effects; inject this knowledge into a dual-stream GNN-LSTM using a novel causal attention mechanism to adjust forecasts.

Result: On a real-world dataset, Event-CausNet reduces forecasting error (MAE) by up to 35.87% and significantly outperforms state-of-the-art baselines, with robust performance during disruptions.

Conclusion: The framework bridges correlational GNNs and causal reasoning, yielding more accurate, transferable forecasts and enhanced interpretability for traffic management during critical disruptions.

Abstract: While spatio-temporal Graph Neural Networks (GNNs) excel at modeling recurring traffic patterns, their reliability plummets during non-recurring events like accidents. This failure occurs because GNNs are fundamentally correlational models, learning historical patterns that are invalidated by the new causal factors introduced during disruptions. To address this, we propose Event-CausNet, a framework that uses a Large Language Model to quantify unstructured event reports, builds a causal knowledge base by estimating average treatment effects, and injects this knowledge into a dual-stream GNN-LSTM network using a novel causal attention mechanism to adjust and enhance the forecast. Experiments on a real-world dataset demonstrate that Event-CausNet achieves robust performance, reducing prediction error (MAE) by up to 35.87%, significantly outperforming state-of-the-art baselines. Our framework bridges the gap between correlational models and causal reasoning, providing a solution that is more accurate and transferable, while also offering crucial interpretability, providing a more reliable foundation for real-world traffic management during critical disruptions.

</details>


### [343] [Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization](https://arxiv.org/abs/2511.12792)
*Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Zehong Cao,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: MARL-based resource optimization in a heterogeneous satellite cluster for autonomous Earth observation; demonstrates coordination and resource balancing across two optical and one SAR satellite using MAPPO/HAPPO/HATRPO in Basilisk/BSK-RL simulations.


<details>
  <summary>Details</summary>
Motivation: EO missions are real-time, uncertain, and decentralized; traditional optimization struggles with energy/memory constraints and heterogeneity; need adaptive multi-agent planning.

Method: Formulate from single to multi-satellite; address energy/memory constraints, partial observability, and agent heterogeneity; evaluate state-of-the-art MARL (MAPPO, HAPPO, HATRPO) in Basilisk/BSK-RL environment; study coordination and reward coupling effects.

Result: MARL enables effective cross-satellite coordination; balances imaging performance with resource use; mitigates non-stationarity and inter-agent reward coupling; demonstrates stability and scalability in a near-realistic sim.

Conclusion: Offers practical guidance for scalable autonomous EO mission planning under heterogeneous and dynamic conditions; establishes a foundation for future intelligent mission planning research.

Abstract: This work investigates resource optimization in heterogeneous satellite clusters performing autonomous Earth Observation (EO) missions using Reinforcement Learning (RL). In the proposed setting, two optical satellites and one Synthetic Aperture Radar (SAR) satellite operate cooperatively in low Earth orbit to capture ground targets and manage their limited onboard resources efficiently. Traditional optimization methods struggle to handle the real-time, uncertain, and decentralized nature of EO operations, motivating the use of RL and Multi-Agent Reinforcement Learning (MARL) for adaptive decision-making. This study systematically formulates the optimization problem from single-satellite to multi-satellite scenarios, addressing key challenges including energy and memory constraints, partial observability, and agent heterogeneity arising from diverse payload capabilities. Using a near-realistic simulation environment built on the Basilisk and BSK-RL frameworks, we evaluate the performance and stability of state-of-the-art MARL algorithms such as MAPPO, HAPPO, and HATRPO. Results show that MARL enables effective coordination across heterogeneous satellites, balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling. The findings provide practical insights into scalable, autonomous satellite operations and contribute a foundation for future research on intelligent EO mission planning under heterogeneous and dynamic conditions.

</details>


### [344] [Neuro-Logic Lifelong Learning](https://arxiv.org/abs/2511.12793)
*Bowen He,Xiaoan Xu,Alper Kamil Bozkurt,Vahid Tarokh,Juncheng Dong*

Main category: cs.AI

TL;DR: Proposes lifelong learning for ILP using a compositional framework that reuses logic rules across sequential tasks, formalizes the approach, and validates it empirically to improve scalability and performance in Neural-Symbolic AI.


<details>
  <summary>Details</summary>
Motivation: ILP with neural networks faces data inefficiency and poor transferability. A compositional, transferable set of logic rules could enable learning new problems more efficiently by reusing previously acquired knowledge, addressing scalability and continual learning challenges in Neural-Symbolic AI.

Method: Introduce a compositional framework that captures and reuses logic rules learned from earlier ILP tasks. Formalize the approach and evaluate it on sequences of ILP tasks to assess scalability, transfer, and performance improvements.

Result: Empirical evaluation on task sequences demonstrates feasibility of the approach and shows improved scalability and performance when reusing previously learned rules across tasks.

Conclusion: The work validates lifelong, continual learning for ILP via a compositional, reusable rule framework and points to new directions for continual learning in Neural-Symbolic AI.

Abstract: Solving Inductive Logic Programming (ILP) problems with neural networks is a key challenge in Neural-Symbolic Ar- tificial Intelligence (AI). While most research has focused on designing novel network architectures for individual prob- lems, less effort has been devoted to exploring new learning paradigms involving a sequence of problems. In this work, we investigate lifelong learning ILP, which leverages the com- positional and transferable nature of logic rules for efficient learning of new problems. We introduce a compositional framework, demonstrating how logic rules acquired from ear- lier tasks can be efficiently reused in subsequent ones, leading to improved scalability and performance. We formalize our approach and empirically evaluate it on sequences of tasks. Experimental results validate the feasibility and advantages of this paradigm, opening new directions for continual learn- ing in Neural-Symbolic AI.

</details>


### [345] [Mapping fNIRS Signals to Agent Performance: Toward Reinforcement Learning from Neural Feedback](https://arxiv.org/abs/2511.12844)
*Julia Santaniello,Matthew Russell,Benson Jiang,Donatello Sassaroli,Robert Jacob,Jivko SInapov*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating human feedback into the agent's training process. We introduce a possible framework that employs passive Brain-Computer Interfaces (BCI) to guide agent training from implicit neural signals. We present and release a novel dataset of functional near-infrared spectroscopy (fNIRS) recordings collected from 25 human participants across three domains: a Pick-and-Place Robot, Lunar Lander, and Flappy Bird. We train classifiers to predict levels of agent performance (optimal, sub-optimal, or worst-case) from windows of preprocessed fNIRS feature vectors, achieving an average F1 score of 67% for binary classification and 46% for multi-class models averaged across conditions and domains. We also train regressors to predict the degree of deviation between an agent's chosen action and a set of near-optimal policies, providing a continuous measure of performance. We evaluate cross-subject generalization and demonstrate that fine-tuning pre-trained models with a small sample of subject-specific data increases average F1 scores by 17% and 41% for binary and multi-class models, respectively. Our work demonstrates that mapping implicit fNIRS signals to agent performance is feasible and can be improved, laying the foundation for future brain-driven RLHF systems.

</details>


### [346] [Bootstrapping LLMs via Preference-Based Policy Optimization](https://arxiv.org/abs/2511.12867)
*Chen Jia*

Main category: cs.AI

TL;DR: PbPO frames LLM training as a min–max game between a policy and a reward model, with the RM constrained by a confidence set derived from preference data. It introduces an online, guided-exploration algorithm that collects preferences during policy evolution, yielding continual self-improvement. The approach provides theoretical high-probability regret guarantees for both sequence-level and token-level RM and demonstrates strong empirical performance across five benchmarks, outperforming prior preference optimization methods.


<details>
  <summary>Details</summary>
Motivation: Align large language models with human preferences efficiently using preference data, reducing manual annotations while ensuring reliable exploitation and continual improvement through online data collection.

Method: Propose a preference-based policy optimization (PbPO) framework formulated as a min–max game between the main policy and a reward model (RM) constrained within a confidence set derived from preferences. An iterative online algorithm actively gathers preference data via guided exploration of the evolving policy, enabling joint improvement of policy and RM. Theoretical analysis yields high-probability regret bounds for both sequence-level RM and token-level RM. Empirical evaluation on five benchmarks shows robustness and superiority over existing state-of-the-art preference optimization methods.

Result: Theoretical guarantees: high-probability regret bounds for both sequence-level and token-level RM. Empirical results on five benchmarks show consistent outperformance of existing state-of-the-art preference optimization techniques.

Conclusion: PbPO offers a scalable, theoretically grounded approach to bootstrapping LLMs by combining preference-based feedback with online learning, enabling continual self-improvement of both policy and reward modeling while achieving superior empirical performance.

Abstract: Bootstrapping large language models (LLMs) through preference-based policy optimization offers a promising direction for aligning model behavior with human preferences without relying on extensive manual annotations. In this work, we propose a novel preference-based policy optimization (PbPO) framework that formulates the learning process as a min-max game between the main policy and a reward model (RM). The RM is constrained within a confidence set derived from preference data to ensure reliable exploitation. Our iterative online algorithm actively collects preference data through guided exploration of the evolving policy, enabling continual self-improvement of both the policy and the RM. We provide theoretical guarantees for our method, establishing high-probability regret bounds for both settings with sequence-level RM and token-level RM, demonstrating its effectiveness in bootstrapping LLMs. Extensive experiments on five benchmarks show that our approach consistently outperforms existing state-of-the-art preference optimization techniques.

</details>


### [347] [Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making](https://arxiv.org/abs/2511.12876)
*Heyang Ma,Qirui Mi,Qipeng Yang,Zijun Fan,Bo Li,Haifeng Zhang*

Main category: cs.AI

TL;DR: LAMP is a Language-Augmented Multi-Agent Policy that integrates structured economic signals with unstructured language, using a Think-Speak-Decide pipeline to enhance MARL performance in economic simulations.


<details>
  <summary>Details</summary>
Motivation:  MARL struggles with semantic ambiguity and contextual richness of language when applied to real-world economic decision-making; leveraging language can improve interpretation, coordination, and robustness.

Method: Introduce LAMP with a Think-Speak-Decide pipeline: Think extracts short-term shocks and long-term trends from numerical observations and caches high-value reasoning trajectories; Speak exchanges strategic messages and updates beliefs by parsing peer communications; Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decisions. Evaluation in economic simulations compares LAMP to MARL and LLM-only baselines.

Result: LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability, indicating language-augmented policies can achieve more effective and robust economic strategies.

Conclusion: Integrating language into MARL via the Think-Speak-Decide framework yields stronger, more robust, and more interpretable economic policies, bridging the gap between simulated economic agents and real-world language-rich decision environments.

Abstract: Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.

</details>


### [348] [Online Learning of HTN Methods for integrated LLM-HTN Planning](https://arxiv.org/abs/2511.12901)
*Yuesheng Xu,Hector Munoz-Avila*

Main category: cs.AI

TL;DR: Online learning of HTN methods to reduce LLM calls in ChatHTN by generalized memoization; achieves comparable or better problem solving with fewer ChatGPT invocations across two domains.


<details>
  <summary>Details</summary>
Motivation: Address the cost and latency of relying on large language models for task decomposition by learning generalized HTN methods from prior decompositions.

Method: Extend ChatHTN by enabling it to memorize decompositions into generalized HTN methods. When ChatGPT generates a decomposition for a task without an applicable method, ChatHTN learns from it and generalizes to other instances of the same task. This is online learning on top of the existing planner, evaluated in two domains.

Result: The online learning reduces the number of ChatGPT calls while solving at least as many problems; in some domains, it solves more problems than without learning.

Conclusion: Online learning of HTN methods within ChatHTN is effective for reducing reliance on LLMs and maintaining or improving task-solving performance; promising for scalable HTN-LMM integrated planning.

Abstract: We present online learning of Hierarchical Task Network (HTN) methods in the context of integrated HTN planning and LLM-based chatbots. Methods indicate when and how to decompose tasks into subtasks. Our method learner is built on top of the ChatHTN planner. ChatHTN queries ChatGPT to generate a decomposition of a task into primitive tasks when no applicable method for the task is available. In this work, we extend ChatHTN. Namely, when ChatGPT generates a task decomposition, ChatHTN learns from it, akin to memoization. However, unlike memoization, it learns a generalized method that applies not only to the specific instance encountered, but to other instances of the same task. We conduct experiments on two domains and demonstrate that our online learning procedure reduces the number of calls to ChatGPT while solving at least as many problems, and in some cases, even more.

</details>


### [349] [CoS: Towards Optimal Event Scheduling via Chain-of-Scheduling](https://arxiv.org/abs/2511.12913)
*Yiming Zhao,Jiwei Tang,Shimin Di,Libin Zheng,Jianxing Yu,Jian Yin*

Main category: cs.AI

TL;DR: Chain-of-Scheduling (CoS) proposes a guided, three-stage scheduling framework for LLMs to optimize event schedules in EBSNs, using exploration, verification, and integration stages and knowledge-distillation for autonomous generation; achieves near-optimal effectiveness with high efficiency and strong zero-shot generalization on real data.


<details>
  <summary>Details</summary>
Motivation: Event scheduling in EBSNs is NP-hard due to time and geographic constraints, creating a trade-off between efficiency, effectiveness, and generalization. A robust method should leverage LLMs while ensuring reliable, interpretable decisions under constraints.

Method: Introduce CoS: decompose scheduling into three atomic stages—exploration (propose candidates), verification (assess feasibility and preferences), integration (finalize schedule). Enable autonomous CoS generation via Knowledge Distillation to train LLMs to follow the pipeline. Evaluate on three real-world datasets, measuring effectiveness, efficiency, interpretability, and zero-shot capability.

Result: CoS achieves near-theoretical optimal effectiveness with high efficiency on three real-world datasets, in an interpretable manner. It also demonstrates strong zero-shot learning ability on out-of-domain data.

Conclusion: CoS enhances LLM-based event scheduling in EBSNs by structuring the task into a transparent, efficient pipeline and enabling autonomous guidance via knowledge distillation, yielding strong performance and generalization.

Abstract: Recommending event schedules is a key issue in Event-based Social Networks (EBSNs) in order to maintain user activity. An effective recommendation is required to maximize the user's preference, subjecting to both time and geographical constraints. Existing methods face an inherent trade-off among efficiency, effectiveness, and generalization, due to the NP-hard nature of the problem. This paper proposes the Chain-of-Scheduling (CoS) framework, which activates the event scheduling capability of Large Language Models (LLMs) through a guided, efficient scheduling process. CoS enhances LLM by formulating the schedule task into three atomic stages, i.e., exploration, verification and integration. Then we enable the LLMs to generate CoS autonomously via Knowledge Distillation (KD). Experimental results show that CoS achieves near-theoretical optimal effectiveness with high efficiency on three real-world datasets in a interpretable manner. Moreover, it demonstrates strong zero-shot learning ability on out-of-domain data.

</details>


### [350] [Fault2Flow: An AlphaEvolve-Optimized Human-in-the-Loop Multi-Agent System for Fault-to-Workflow Automation](https://arxiv.org/abs/2511.12916)
*Yafang Wang,Yangjie Tian,Xiaoyu Shen,Gaoyang Zhang,Jiaze Sun,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: Fault2Flow: an LLM-based multi-agent system that converts regulatory logic and expert knowledge into an executable, verified n8n workflow for power grid fault diagnosis, achieving perfect topological consistency on transformer fault datasets.


<details>
  <summary>Details</summary>
Motivation: Manual extraction of regulatory logic and tacit expert knowledge is error-prone, non-maintainable, and hard to scale as regulations evolve. An integrated, verified automation path is needed to reduce expert workload and improve reliability.

Method: Fault2Flow uses a multi-agent approach to (1) extract and structure regulatory logic into PASTA-formatted fault trees, (2) incorporate expert verification via a human-in-the-loop interface, (3) optimize reasoning logic with a novel AlphaEvolve module, and (4) synthesize the verified logic into an n8n-executable workflow.

Result: On transformer fault diagnosis datasets, Fault2Flow achieves 100% topological consistency and high semantic fidelity, enabling a reproducible path from fault analysis to operational automation and substantially reducing expert workload.

Conclusion: The framework demonstrates a viable bridge between regulatory/textual knowledge and executable automation for power grid fault diagnosis, with strong initial validation; future work should assess generalization beyond transformer faults and explore scalability and maintenance under rule updates.

Abstract: Power grid fault diagnosis is a critical process hindered by its reliance on manual, error-prone methods. Technicians must manually extract reasoning logic from dense regulations and attempt to combine it with tacit expert knowledge, which is inefficient, error-prone, and lacks maintainability as ragulations are updated and experience evolves. While Large Language Models (LLMs) have shown promise in parsing unstructured text, no existing framework integrates these two disparate knowledge sources into a single, verified, and executable workflow. To bridge this gap, we propose Fault2Flow, an LLM-based multi-agent system. Fault2Flow systematically: (1) extracts and structures regulatory logic into PASTA-formatted fault trees; (2) integrates expert knowledge via a human-in-the-loop interface for verification; (3) optimizes the reasoning logic using a novel AlphaEvolve module; and (4) synthesizes the final, verified logic into an n8n-executable workflow. Experimental validation on transformer fault diagnosis datasets confirms 100\% topological consistency and high semantic fidelity. Fault2Flow establishes a reproducible path from fault analysis to operational automation, substantially reducing expert workload.

</details>


### [351] [Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models](https://arxiv.org/abs/2511.12937)
*Guoyan Wang,Yanyan Huang,Chunlin Chen,Lifeng Wang,Yuxiang Sun*

Main category: cs.AI

TL;DR: Yanyun-3 demonstrates a cross-platform strategy-game agent that fuses static images, image sequences, and video data with vision-language reasoning and UI execution to achieve fast, robust cross-platform operation and notable BLEU-4 gains.


<details>
  <summary>Details</summary>
Motivation: To achieve robust generalization of automation in heterogeneous strategy games where user interfaces and battlefield dynamics vary widely; current VLMs have limited, underexplored applicability to complex HCI in strategy gaming.

Method: Integrate Qwen2.5-VL for vision-language reasoning with UI-TARS for precise action execution in a closed-loop pipeline (screen capture -> model inference -> action execution); conduct ablations on data modalities (static images, multi-image sequences, videos) and introduce 'combination granularity' to distinguish intra-sample fusion from inter-sample mixing; evaluate across three heterogeneous strategy game environments.

Result: A hybrid fusion strategy (MV+S: multi-image and video fusion with static image mixing) substantially outperforms full fusion by reducing inference time by 63% and boosting BLEU-4 from 4.81% to 62.41% (~12.98x); demonstrates strong real-time performance and cross-platform generalization; ablation studies validate the proposed data-organization approach.

Conclusion: The work offers a general paradigm for enhancing vision-language model performance through structured multimodal data organization, highlighting the balance between static perception and dynamic, embodied reasoning in cross-platform strategy-game automation.

Abstract: Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.

</details>


### [352] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Reliable Mathematical and Biomedical Reasoning](https://arxiv.org/abs/2511.12963)
*Crystal Su*

Main category: cs.AI

TL;DR: MedRule-KG couples a compact knowledge-graph scaffold with a lightweight verifier to steer LLMs toward domain-valid outputs in scientific reasoning and early drug discovery; achieves substantial error reductions with low latency across 90 tasks.


<details>
  <summary>Details</summary>
Motivation: Address the lack of domain-consistent structure in LLMs when used for scientific reasoning and early-stage drug discovery, by enforcing mathematically and biomedically valid outputs through symbolic knowledge and a deterministic checker.

Method: Inject curated symbolic facts into prompts; enforce rule satisfaction with a deterministic verifier; formalize generation as constrained inference; introduce a soft guidance surrogate for decoding; perform uncertainty quantification; evaluate across 90 tasks spanning reaction feasibility, metabolic compatibility, and toxicity screening.

Result: Reduction of rule-violation counts by 83.2% relative to a strong chain-of-thought baseline and improvements in exact match; results stable under stratification and scale with dataset size; verifier adds negligible latency.

Conclusion: A compact KG plus verifier can impose domain-consistent structure in LLM outputs, delivering practical, interactive design capabilities for biomedical reasoning and drug discovery with enhanced reliability.

Abstract: We study how to impose domain-consistent structure on large language models (LLMs) used for scientific reasoning and early-stage drug discovery. We present MedRule-KG, a compact knowledge-graph scaffold paired with a lightweight verifier that steers generation toward mathematically and biomedically valid outputs. The system injects curated symbolic facts into prompts and then enforces rule satisfaction with a deterministic checker. We formalize generation as constrained inference, introduce a soft guidance surrogate suitable for decoding, and perform a thorough statistical analysis with uncertainty quantification. Across 90 tasks spanning reaction feasibility, metabolic compatibility, and toxicity screening, MedRule-KG reduces violation counts by 83.2\% relative to a strong chain-of-thought baseline while improving exact match. Results remain stable under stratification and scale with dataset size, and the verifier adds negligible latency, making the approach practical for interactive design.

</details>


### [353] [WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance](https://arxiv.org/abs/2511.12997)
*Genglin Liu,Shijie Geng,Sha Li,Hejie Cui,Sarah Zhang,Xin Liu,Tianyi Liu*

Main category: cs.AI

TL;DR: WebCoach is a model-agnostic framework that adds persistent cross-session memory to web-browsing LLM agents via three components—WebCondenser, External Memory Store, and Coach—to enable learning from episodic experiences and self-evolution without retraining, boosting robustness and task success across backbones.


<details>
  <summary>Details</summary>
Motivation: Current multimodal LLM agents suffer from repetitive errors and lack of cross-session learning, which harms long-term robustness and sample efficiency in web navigation tasks.

Method: Introduce WebCondenser to summarize raw navigation logs; an External Memory Store to organize complete trajectories as episodic experiences; and a Coach that retrieves relevant experiences by similarity and recency and injects task-specific advice at runtime. The system enables cross-session memory, long-term planning, and continual learning without retraining, and is evaluated across three LLM backbones on WebVoyager.

Result: With a 38B model, task success rises from 47% to 61%, while the average number of steps is reduced or maintained. Smaller base models with WebCoach achieve performance comparable to GPT-4o on the same tasks.

Conclusion: WebCoach enables persistent memory and continual learning across sessions, improving long-term robustness and sample efficiency for web-browsing agents without retraining, and demonstrates cross-backbone gains on WebVoyager.

Abstract: Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.

</details>


### [354] [GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs](https://arxiv.org/abs/2511.13007)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Main category: cs.AI

TL;DR: GEM uses entropy-guided cognitive signals to align LLMs with few-shot preferences: cognitive filtering with CoT generation + token scoring, followed by SEGA self-evaluated group advantage optimization, enabling domain-specific alignment with scarce labels.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs in professional domains (e.g., medicine, law) requires high-quality preferences but large annotation efforts are often impractical; there is a need for low-resource alignment methods that leverage internal cognitive signals.

Method: Cognitive Filtering: generate diverse Chain-of-Thoughts (CoTs) from preference data using CoT prompting; apply a token scoring mechanism to rank and weight CoTs, boosting high-confidence answers and strategically high-entropy tokens. SEGA: self-evaluated group advantage algorithm that aggregates group-level cognitive signals and converts entropy-based scores into implicit rewards to guide policy optimization; fine-tune the LLM on these signals.

Result: Experiments on general benchmarks and domain-specific tasks (mathematical reasoning and medical dialogues) show significant improvements with few-shot preference data.

Conclusion: Entropy-guided closed-loop cognitive optimization enables efficient, few-shot alignment of LLMs; GEM demonstrates strong potential for domain-specific alignment with scarce annotations.

Abstract: Alignment of large language models (LLMs) with human preferences typically relies on supervised reward models or external judges that demand abundant annotations. However, in fields that rely on professional knowledge, such as medicine and law, such large-scale preference labels are often unachievable. In this paper, we propose a generative entropy-guided preference modeling approach named GEM for LLMs aligment at low-resource and domain-specific scenarios. Instead of training a discriminative reward model on preference data, we directly train the LLM to internalize a closed-loop optimization architecture that can extract and exploit the multi-dimensional, fine-grained cognitive signals implicit in human preferences. Specifically, our Cognitive Filtering module, based on entropy theory in decision making, first leverages Chain-of-Thought (CoT) prompting to generate diverse candidate reasoning chains (CoTs) from preference data. Subsequently, it introduces a token scoring mechanism to rank and weight the sampled CoTs, boosting the importance of high-confidence answers and strategically high-entropy tokens. Building on these filtered preferences, we fine-tune the LLM using a novel self-evaluated group advantage algorithm, SEGA, which effectively aggregates group-level cognitive signals and transforms the entropy-based scores into implicit rewards for policy optimization. In these ways, GEM empowers the LLM to rely on its own judgments and establishes an entropy-guided closed-loop cognitive optimization framework, enabling highly efficient few-shot alignment of LLMs. Experiments on general benchmarks and domain-specific tasks (such as mathematical reasoning and medical dialogues) demonstrate that our GEM achieves significant improvements with few-shot preference data.

</details>


### [355] [PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics](https://arxiv.org/abs/2511.13021)
*Sachin Vashistha,Aryan Bibhuti,Atharva Naik,Martin Tutek,Somak Aditya*

Main category: cs.AI

TL;DR: LMs struggle to maintain robust internal world-models in dyadic conversations; a dual-perspective interpretability framework identifies harmful layers and two layer-regularization finetuning strategies to improve robustness.


<details>
  <summary>Details</summary>
Motivation: Real-world conversations require dynamic, encoded world-models; it's unclear if LMs can memorize/update such details under perturbations; the work proposes benchmarks with minimal linguistic alterations to test this.

Method: Apply seven minimal linguistic alterations to conversations from datasets to create two yes-no benchmarks; evaluate various LMs; develop a dual-perspective interpretability framework to locate useful vs. harmful layers and the alterations most influenced by harmful layers; propose two layer-regularization finetuning strategies.

Result: LMs show degraded accuracy and difficulty memorizing crucial details when perturbed; identify spurious signals in certain layers as the source of brittleness; layer-regularization finetuning strategies mitigate these issues (improvement implied).

Conclusion: Targeted layer-level interventions guided by interpretability can enhance the robustness of internal world-model representations in LMs for dialogue tasks, making them less prone to shortcut cues and more faithful to evolving conversational states.

Abstract: Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.

</details>


### [356] [Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection](https://arxiv.org/abs/2511.13027)
*Sadegh Mahdavi,Branislav Kisacanin,Shubham Toshniwal,Wei Du,Ivan Moshkov,George Armstrong,Renjie Liao,Christos Thrampoulidis,Igor Gitman*

Main category: cs.AI

TL;DR: Proposes scalable proof-verification using GenSelect and LLM-as-a-Judge, shows evaluating on a single benchmark is brittle, and finds reinforcement learning improves proof-level metrics but not final-answer accuracy; offers practical guidelines for robust, scalable evaluation.


<details>
  <summary>Details</summary>
Motivation: Address the gap between proof-level correctness and final-answer accuracy in math problems solved by LLMs, and the risk that single benchmarks yield brittle conclusions.

Method: Analyze multiple evaluation setups; scale two verification methods (GenSelect and LLM-as-a-Judge) to millions of tokens; study prompt sensitivity; apply reinforcement learning to reduce sensitivity; compare proof-based vs final-answer reasoning.

Result: GenSelect + LLM-as-a-Judge emerges as the most effective framework for solution verification and selection; prompt choice significantly affects performance; reinforcement learning reduces prompt sensitivity and improves proof-level metrics but does not enhance final-answer precision.

Conclusion: Advocate for practical guidelines to design and evaluate scalable proof-verification and selection systems across diverse benchmarks, acknowledging that improvements in proof validity may not translate to final-answer accuracy.

Abstract: Large language models have achieved remarkable success on final-answer mathematical problems, largely due to the ease of applying reinforcement learning with verifiable rewards. However, the reasoning underlying these solutions is often flawed. Advancing to rigorous proof-based mathematics requires reliable proof verification capabilities. We begin by analyzing multiple evaluation setups and show that focusing on a single benchmark can lead to brittle or misleading conclusions. To address this, we evaluate both proof-based and final-answer reasoning to obtain a more reliable measure of model performance. We then scale two major generative verification methods (GenSelect and LLM-as-a-Judge) to millions of tokens and identify their combination as the most effective framework for solution verification and selection. We further show that the choice of prompt for LLM-as-a-Judge significantly affects the model's performance, but reinforcement learning can reduce this sensitivity. However, despite improving proof-level metrics, reinforcement learning does not enhance final-answer precision, indicating that current models often reward stylistic or procedural correctness rather than mathematical validity. Our results establish practical guidelines for designing and evaluating scalable proof-verification and selection systems.

</details>


### [357] [MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements](https://arxiv.org/abs/2511.13087)
*SeokJoo Kwak,Jihoon Kim,Boyoun Kim,Jung Jae Yoon,Wooseok Jang,Jeonghoon Hong,Jaeho Yang,Yeong-Dae Kwon*

Main category: cs.AI

TL;DR: MEGA-GUI is a modular, multi-stage GUI grounding framework that splits the task into coarse ROI selection and fine-grained grounding, guided by specialized vision-language agents; it achieves higher accuracy than monolithic models on ScreenSpot-Pro (73.18%) and OSWorld-G (68.63%).


<details>
  <summary>Details</summary>
Motivation: Tackle modularity issues and failure under visual clutter and ambiguous instructions in existing GUI grounding systems, to improve robustness and accessibility.

Method: Proposes MEGA-GUI with a two-stage process: coarse ROI exploration followed by fine grounding; introduces bidirectional ROI zoom to reduce spatial dilution; employs a context-aware rewriting agent to disambiguate language; analyzes model performance at different visual scales and uses a modular, agent-based orchestration.

Result: Empirically outperforms monolithic baselines on two benchmarks, with 73.18% and 68.63% accuracy, and provides the Grounding Benchmark Toolkit (GBT) and code.

Conclusion: Modularity and coordinated vision-language agents improve GUI grounding; the results demonstrate complementary strengths across models and motivate multi-stage, toolkit-based approaches.

Abstract: Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.

</details>


### [358] [STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization](https://arxiv.org/abs/2511.13091)
*Yuhan Chen,Yuxuan Liu,Long Zhang,Pengzhi Gao,Jian Luan,Wei Liu*

Main category: cs.AI

TL;DR: STEP is a dynamic, step-level, success-rate-aware trajectory optimization framework for online RL that uses per-task success rates to guide adaptive sampling and updates, improving sample efficiency and stability over trajectory-level methods.


<details>
  <summary>Details</summary>
Motivation: Trajectory-level optimization treats whole trajectories as training samples, which can be inefficient and provide misleading learning signals. There is a need for fine-grained, adaptive sampling that accounts for task difficulty and reduces sample collection cost.

Method: Maintain a smoothed per-task success-rate; use it to adapt trajectory resampling and allocate effort to harder tasks. Compute success-rate-weighted advantages, decompose trajectories into step-level samples, and apply a step-level GRPO augmentation to update low-success tasks.

Result: Empirical evaluation on OSWorld and AndroidWorld shows STEP achieves faster convergence, better sample efficiency, and stronger generalization under the same sampling budget compared to trajectory-level GRPO.

Conclusion: Adaptive, step-level optimization guided by smoothed success rates can significantly enhance learning efficiency and stability in multi-turn RL settings.

Abstract: Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.

</details>


### [359] [MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications](https://arxiv.org/abs/2511.13131)
*Gagan Raj Gupta,Anshul Kumar,Manish Rai,Apu Chakraborty,Ashutosh Modi,Abdelaali Chaoub,Soumajit Pramanik,Moyank Giri,Yashwanth Holla,Sunny Kumar,M. V. Kiran Sooraj*

Main category: cs.AI

TL;DR: Introduces MM-Telco: a multimodal telecom benchmark and model suite with text- and image-based tasks spanning network operations, management, documentation quality, and data/text retrieval. Baselines with LLMs and VLMs show fine-tuning yields notable performance gains and reveal gaps in current multimodal LLMs.


<details>
  <summary>Details</summary>
Motivation: Telecom requires domain-specific adaptation of LLMs due to unique data types, operational requirements, regulatory considerations, and practical troubleshooting use cases. Without a telecom-tailored benchmark and data, models struggle to perform reliably in real-world telecom tasks.

Method: Propose MM-Telco, a comprehensive multimodal benchmark for telecom featuring both text-based and image-based tasks aligned to practical use cases. Conduct baseline experiments using multiple LLMs and VLMs, then fine-tune models on the MM-Telco dataset to evaluate performance improvements and analyze remaining weaknesses.

Result: Fine-tuned models show a significant boost in performance on MM-Telco tasks. The experiments also identify weak areas in current state-of-the-art multimodal LLMs, providing guidance for future research and model development.

Conclusion: MM-Telco offers a practical pathway to telecom-ready multimodal AI by delivering domain-focused benchmarks and models that accelerate adaptation, while highlighting current limitations to target in subsequent work.

Abstract: Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.

</details>


### [360] [Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition](https://arxiv.org/abs/2511.13137)
*Yanda Zhu,Yuanyang Zhu,Daoyi Dong,Caihua Chen,Chunlin Chen*

Main category: cs.AI

TL;DR: CD3T introduces a two-level hierarchical MARL framework that uses a conditional diffusion model to dynamically infer subtasks and predict environment effects, enabling coordinated low-level skills and improved value decomposition. It outperforms baselines on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Dynamic task decomposition in MARL under partial observability is sample-inefficient due to large joint action spaces; automatic discovery of subtasks and coordination patterns is needed.

Method: Two-level hierarchy where high-level policy learns subtask representations to guide subtask selection; conditional diffusion model predicts next observation and reward conditioned on subtasks; low-level agents share specialized skills within subtasks; subtask representations feed a multi-head attention mixer to improve value decomposition between individual and joint values.

Result: Empirical results on various benchmarks show CD3T achieves better performance than existing baselines.

Conclusion: CD3T enables dynamic, principled task decomposition and coordination in MARL by coupling a diffusion-based effect model with semantic subtask representations, improving efficiency and performance.

Abstract: Task decomposition has shown promise in complex cooperative multi-agent reinforcement learning (MARL) tasks, which enables efficient hierarchical learning for long-horizon tasks in dynamic and uncertain environments. However, learning dynamic task decomposition from scratch generally requires a large number of training samples, especially exploring the large joint action space under partial observability. In this paper, we present the Conditional Diffusion Model for Dynamic Task Decomposition (C$\text{D}^\text{3}$T), a novel two-level hierarchical MARL framework designed to automatically infer subtask and coordination patterns. The high-level policy learns subtask representation to generate a subtask selection strategy based on subtask effects. To capture the effects of subtasks on the environment, C$\text{D}^\text{3}$T predicts the next observation and reward using a conditional diffusion model. At the low level, agents collaboratively learn and share specialized skills within their assigned subtasks. Moreover, the learned subtask representation is also used as additional semantic information in a multi-head attention mixing network to enhance value decomposition and provide an efficient reasoning bridge between individual and joint value functions. Experimental results on various benchmarks demonstrate that C$\text{D}^\text{3}$T achieves better performance than existing baselines.

</details>


### [361] [InteractiveGNNExplainer: A Visual Analytics Framework for Multi-Faceted Understanding and Probing of Graph Neural Network Predictions](https://arxiv.org/abs/2511.13160)
*TC Singh,Sougata Mukherjea*

Main category: cs.AI

TL;DR: A visual analytics framework, InteractiveGNNExplainer, combines interactive views with post-hoc (GNNExplainer) and intrinsic (GAT attention) explanations and what-if graph edits to explain node classification; demonstrated on Cora and CiteSeer.


<details>
  <summary>Details</summary>
Motivation: GNNs are powerful but opaque; explainability is needed for trust, debugging, bias detection, and deployment in critical domains. Existing explanations are static or limited; interactive, perturbation-based exploration can reveal causal effects on predictions and explanations.

Method: Coordinated interactive views (dynamic graph layouts, embedding projections, feature inspection, neighborhood analysis) integrated with GNNExplainer and GAT attention; interactive graph editing enables perturbations and immediate observation of changes in predictions and explanations; system architecture and case studies on Cora and CiteSeer.

Result: Case studies demonstrate improved misclassification diagnosis, enable comparative analysis of GCN vs GAT behavior, and support probing of model sensitivity through perturbations and explanation changes.

Conclusion: The framework advances explainability for GNNs, promoting deeper understanding, transparency, trust, and robustness in graph analysis.

Abstract: Graph Neural Networks (GNNs) excel in graph-based learning tasks, but their complex, non-linear operations often render them as opaque "black boxes". This opacity hinders user trust, complicates debugging, bias detection, and adoption in critical domains requiring explainability. This paper introduces InteractiveGNNExplainer, a visual analytics framework to enhance GNN explainability, focusing on node classification. Our system uniquely integrates coordinated interactive views (dynamic graph layouts, embedding projections, feature inspection, neighborhood analysis) with established post-hoc (GNNExplainer) and intrinsic (GAT attention) explanation techniques. Crucially, it incorporates interactive graph editing, allowing users to perform a "what-if" analysis by perturbing graph structures and observing immediate impacts on GNN predictions and explanations. We detail the system architecture and, through case studies on Cora and CiteSeer datasets, demonstrate how InteractiveGNNExplainer facilitates in-depth misclassification diagnosis, comparative analysis of GCN versus GAT behaviors, and rigorous probing of model sensitivity. These capabilities foster a deeper, multifaceted understanding of GNN predictions, contributing to more transparent, trustworthy, and robust graph analysis.

</details>


### [362] [Cost-Effective Communication: An Auction-based Method for Language Agent Interaction](https://arxiv.org/abs/2511.13193)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Chengpei Tang,Jian Wang,Keze Wang*

Main category: cs.AI

TL;DR: DALA reframes MAS communication as a scarce resource auction, making agents bid to speak. This leads to concise, high-value messages, achieving state-of-the-art results with far fewer tokens and revealing strategic silence as an emergent behavior.


<details>
  <summary>Details</summary>
Motivation: Frequent, unregulated communication in LLM-based MAS is wasteful: larger token budgets equate to higher costs and lower signal-to-noise ratios. The authors argue for resource rationality, treating bandwidth as a tradable resource to curb pointless chatter and improve efficiency without sacrificing performance.

Method: Introduce a centralized auction mechanism where agents bid for speaking opportunities based on the predicted value density of their messages. Agents learn to trade off message value against token cost, promoting concise, informative communications. The approach is evaluated on seven reasoning benchmarks, with analyses of efficiency and communication patterns.

Result: Achieves new state-of-the-art performance across seven benchmarks, including 84.32% on MMLU and 91.21% pass@1 on HumanEval. Token usage is 6.25 million, significantly lower than SOTA methods on GSM8K. The framework also reveals emergent strategic silence as the system adapts communication strategies under resource constraints.

Conclusion: Resource-aware communication via dynamic auctions can markedly improve both the efficiency and effectiveness of MAS with LLMs, enabling high performance with reduced communication costs and revealing new strategic communication behaviors.

Abstract: Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient "free-for-all" communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that "free" communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints.

</details>


### [363] [Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks](https://arxiv.org/abs/2511.13214)
*Guillaume Infantes,Stéphanie Roussel,Antoine Jacquet,Emmanuel Benazera*

Main category: cs.AI

TL;DR: A neural-guided scheduling framework (Wheatley) for RCPSP with uncertain task durations that minimizes expected project duration using a Graph Neural Network and DRL policy, generating schedules via a Serial Schedule Generation Scheme and showing strong generalization on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Uncertainty in task durations undermines classical RCPSP schedules; there is a need for a reusable baseline policy that performs well across duration scenarios in industrial settings, with reproducibility.

Method: Use Graph Neural Networks to encode project state, train a Deep Reinforcement Learning policy that acts like a priority dispatch rule, and pair it with a Serial Schedule Generation Scheme to produce concrete schedules; evaluated on standard RCPSP benchmarks; Wheatley framework released publicly.

Result: Empirical results show improved performance over baselines and good generalization to unseen scenarios; Wheatley framework is publicly available.

Conclusion: Graph-based DRL scheduling yields robust, reusable baseline schedules under duration uncertainty, supporting reproducible research and practical deployment in industry.

Abstract: The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical scheduling problem that has received significant attention due to of its numerous applications in industry. However, in practice, task durations are subject to uncertainty that must be considered in order to propose resilient scheduling. In this paper, we address the RCPSP variant with uncertain tasks duration (modeled using known probabilities) and aim to minimize the overall expected project duration. Our objective is to produce a baseline schedule that can be reused multiple times in an industrial setting regardless of the actual duration scenario. We leverage Graph Neural Networks in conjunction with Deep Reinforcement Learning (DRL) to develop an effective policy for task scheduling. This policy operates similarly to a priority dispatch rule and is paired with a Serial Schedule Generation Scheme to produce a schedule. Our empirical evaluation on standard benchmarks demonstrates the approach's superiority in terms of performance and its ability to generalize. The developed framework, Wheatley, is made publicly available online to facilitate further research and reproducibility.

</details>


### [364] [Informative Communication of Robot Plans](https://arxiv.org/abs/2511.13226)
*Michele Persiani,Thomas Hellstrom*

Main category: cs.AI

TL;DR: An information-theoretic verbalization strategy for robot plans that maximizes information gain against a model of the user's prior knowledge improves rapid goal understanding over plan-order baselines.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of incremental plan verbalization that ignores what the user already knows, by explicitly modeling the user's knowledge (second-order theory of mind) and selecting utterances that maximize informative value.

Method: Define an information gain objective over verbalizations with respect to a Bayesian model of the user's priors about the robot's plan. Use a second-order theory of mind to capture the user's beliefs about both the robot's goals and the user's own knowledge. Select verbalizations that maximize expected information gain; compare to baselines that simply adjust plan order (increasing/decreasing).

Result: Empirical experiments indicate the informative-verbalization strategy yields faster user understanding of the robot's goal than plan-order-based strategies, and clarifies what content is informative and why.

Conclusion: Verbalizations chosen by information gain with respect to the user's presumed knowledge improve plan interpretability and communication efficiency, offering a principled framework for explaining robot plans.

Abstract: When a robot is asked to verbalize its plan it can do it in many ways. For example, a seemingly natural strategy is incremental, where the robot verbalizes its planned actions in plan order. However, an important aspect of this type of strategy is that it misses considerations on what is effectively informative to communicate, because not considering what the user knows prior to explanations. In this paper we propose a verbalization strategy to communicate robot plans informatively, by measuring the information gain that verbalizations have against a second-order theory of mind of the user capturing his prior knowledge on the robot. As shown in our experiments, this strategy allows to understand the robot's goal much quicker than by using strategies such as increasing or decreasing plan order. In addition, following our formulation we hint to what is informative and why when a robot communicates its plan.

</details>


### [365] [Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO](https://arxiv.org/abs/2511.13288)
*Haoyang Hong,Jiajun Yin,Yuan Wang,Jingnan Liu,Zhe Chen,Ailing Yu,Ji Li,Zhiling Ye,Hansong Xiao,Yefei Chen,Hualei Zhou,Yun Yue,Minghui Yang,Chunxiao Guo,Junwei Liu,Peng Wei,Jinjie Gu*

Main category: cs.AI

TL;DR: Introduces M-GRPO, a hierarchical, decoupled training framework for vertical multi-agent systems with a planner and multiple tool-executing sub-agents. It uses trajectory alignment and group-relative advantages to improve stability and sample efficiency, outperforming baselines on real-world benchmarks.


<details>
  <summary>Details</summary>
Motivation: Unified training of all agents with a single LLM can underfit diverse agent distributions and faces synchronization and cross-server gradient flow challenges. Distinct LLMs per agent and decoupled optimization are proposed to address these issues.

Method: Extend Group Relative Policy Optimization to a two-level hierarchy (main planner and sub-agents). Compute group-relative advantages for both levels to maintain hierarchical credit assignment. Introduce a trajectory-alignment scheme to produce fixed-size batches despite varying sub-agent invocations. Deploy a decoupled training pipeline where agents run on separate servers and exchange minimal statistics via a shared store to avoid cross-server backpropagation.

Result: M-GRPO consistently outperforms single-agent GRPO and multi-agent GRPO with frozen sub-agents on real-world benchmarks (GAIA, XBench-DeepSearch, WebWalkerQA), offering improved stability and sample efficiency.

Conclusion: Aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks in vertical multi-agent systems.

Abstract: Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.

</details>


### [366] [Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment](https://arxiv.org/abs/2511.13290)
*Jea Kwon,Luiz Felipe Vecchietti,Sungwon Park,Meeyoung Cha*

Main category: cs.AI

TL;DR: The study investigates moral uncertainty in AI using the trolley problem across 32 open-source models and 9 moral dimensions. It finds that inter-model variance dominates over intra-dimension variance; uses a linear entropy-based measure and inference-time dropout to inject stochasticity, which increases entropy mainly via mutual information and improves alignment with human moral judgments. Modulating uncertainty may help align AI decisions with human preferences in complex ethical scenarios.


<details>
  <summary>Details</summary>
Motivation: Human moral uncertainty is well-documented, while AI systems often exhibit overconfidence in moral judgments. Understanding and controlling uncertainty in AI moral reasoning is crucial for reliable, human-aligned ethical decision-making in increasingly autonomous systems.

Method: Evaluate 32 open-source models on the classical trolley problem across 9 moral dimensions. Compare variance in model confidence across models versus across dimensions. Quantify uncertainty using binary entropy computed as a linear combination of total entropy, conditional entropy, and mutual information. Introduce inference-time dropout to inject stochasticity, and assess effects on entropy components and on human–LLM alignment (correlation between mutual information changes and alignment shifts).

Result: Inter-model variance in confidence exceeds within-dimension variance, indicating that model architecture and training drive moral uncertainty more than the chosen moral dimension. Dropout at inference increases total entropy mainly through mutual information, with conditional entropy remaining largely unchanged. This stochasticity enhancement significantly improves human–LLM moral alignment, with observed correlations between mutual information changes and alignment score shifts.

Conclusion: Deliberately modulating AI uncertainty, particularly by reducing confidence in morally complex scenarios, can improve alignment with human moral judgments. Designing AI systems to manage and adjust uncertainty could yield more reliable and human-aligned ethical decision-making.

Abstract: Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via "dropout" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.

</details>


### [367] [Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval](https://arxiv.org/abs/2511.13293)
*Chuang Zhao,Hui Tang,Hongke Zhao,Xiaofang Zhou,Xiaomeng Li*

Main category: cs.AI

TL;DR: GHAR is a hierarchical agentic RAG framework for healthcare that learns when to retrieve and how to coordinate submodules using a dual-agent setup (Agent-Top and Agent-Low) optimized via an MD P, achieving superior results on three healthcare benchmarks across three tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs offer rich parametric knowledge but are prone to factual errors in healthcare. Retrieval-augmented generation can mitigate this, but success hinges on when to retrieve and how to align retriever and generator, especially in safety-critical healthcare settings.

Method: Proposes a dual-agent architecture: Agent-Top (primary physician) decides whether to rely on parametric knowledge or trigger retrieval; Agent-Low (consulting service) summarizes task-relevant knowledge after retrieval. The two agents are jointly optimized within a Markov Decision Process with rewards designed to align predictive accuracy while preserving their roles.

Result: KHAR demonstrates superior performance over state-of-the-art baselines on three benchmark datasets across three medical prediction tasks, illustrating effective retrieval-trigger decisions and strong agent collaboration.

Conclusion: GHAR offers a promising path for reliable, context-aware healthcare predictions by combining hierarchical decision making with RAG, indicating the potential for safer and more accurate clinical AI systems.

Abstract: Accurate healthcare prediction is critical for improving patient outcomes and reducing operational costs. Bolstered by growing reasoning capabilities, large language models (LLMs) offer a promising path to enhance healthcare predictions by drawing on their rich parametric knowledge. However, LLMs are prone to factual inaccuracies due to limitations in the reliability and coverage of their embedded knowledge. While retrieval-augmented generation (RAG) frameworks, such as GraphRAG and its variants, have been proposed to mitigate these issues by incorporating external knowledge, they face two key challenges in the healthcare scenario: (1) identifying the clinical necessity to activate the retrieval mechanism, and (2) achieving synergy between the retriever and the generator to craft contextually appropriate retrievals. To address these challenges, we propose GHAR, a \underline{g}enerative \underline{h}ierarchical \underline{a}gentic \underline{R}AG framework that simultaneously resolves when to retrieve and how to optimize the collaboration between submodules in healthcare. Specifically, for the first challenge, we design a dual-agent architecture comprising Agent-Top and Agent-Low. Agent-Top acts as the primary physician, iteratively deciding whether to rely on parametric knowledge or to initiate retrieval, while Agent-Low acts as the consulting service, summarising all task-relevant knowledge once retrieval was triggered. To tackle the second challenge, we innovatively unify the optimization of both agents within a formal Markov Decision Process, designing diverse rewards to align their shared goal of accurate prediction while preserving their distinct roles. Extensive experiments on three benchmark datasets across three popular tasks demonstrate our superiority over state-of-the-art baselines, highlighting the potential of hierarchical agentic RAG in advancing healthcare systems.

</details>


### [368] [DAP: A Discrete-token Autoregressive Planner for Autonomous Driving](https://arxiv.org/abs/2511.13306)
*Bowen Ye,Bin Zhang,Hang Zhao*

Main category: cs.AI

TL;DR: DAP is a discrete-token autoregressive planner for autonomous driving that jointly forecasts BEV semantics and ego trajectories, enabling stronger supervision and planning by conditioning ego motion on predicted scene dynamics. It uses RL-based fine-tuning to improve behavior while preserving cloning priors, achieves SOTA open-loop performance with a compact 160M params, and shows competitive closed-loop NAVSIM results.


<details>
  <summary>Details</summary>
Motivation: Predicting ego trajectories in isolation suffers from sparse supervision and weak constraints on how scene evolution should influence planning. There is a need to leverage joint representations of scene semantics and agent motion to scale data/model budgets and improve planning quality in autonomous driving.

Method: DAP employs a fully discrete-token autoregressive model that jointly predicts rasterized BEV semantics and ego actions. It operates on discrete tokens for both scene and action sequences, enabling compact, scalable planning. The framework incorporates reinforcement-learning-based fine-tuning that preserves supervised behavior cloning priors while injecting reward-guided improvements.

Result: With a 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and competitive results in closed-loop NAVSIM benchmarks, illustrating strong data-model efficiency and robust planning under evaluation.

Conclusion: A fully discrete-token autoregressive formulation over BaV (BEV) semantics and ego actions can provide a compact yet scalable planning paradigm for autonomous driving, effectively leveraging joint representation learning and reward-based fine-tuning to improve planning performance.

Abstract: Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.

</details>


### [369] [Reasoning Shapes Alignment: Investigating Cultural Alignment in Large Reasoning Models with Cultural Norms](https://arxiv.org/abs/2511.13359)
*Yuhang Wang,Yanxu Zhu,Jitao Sang*

Main category: cs.AI

TL;DR: CNCA introduces a Cultural Norm-based Cultural Alignment framework for Large Reasoning Models, mining cultural norms from limited survey data and applying them via in-context integration and fine-tuning with enhanced Chain-of-Thought data to improve cultural alignment and safety.


<details>
  <summary>Details</summary>
Motivation: To ensure AI safety while reflecting diverse human values across cultures, leveraging models' advanced reasoning to mine and apply cultural norms from limited data.

Method: Three methods are proposed to automatically mine cultural norms from limited survey data. Two alignment paradigms are explored: (1) in-context alignment, integrating cultural norms directly into the user context, and (2) fine-tuning-based alignment, internalizing norms through enhanced Chain-of-Thought training data.

Result: Comprehensive experiments show the methods are effective, with stronger-reasoning models benefiting more from cultural norm mining and its utilization for alignment.

Conclusion: CNCA demonstrates the potential of reasoning models to better reflect diverse human values through culturally informed alignment strategies and suggests promising directions for scalable norm mining and integration.

Abstract: The advanced reasoning capabilities of Large Reasoning Models enable them to thoroughly understand and apply safety policies through deliberate thought processes, thereby improving the models' safety. Beyond safety, these models must also be able to reflect the diverse range of human values across various cultures. This paper presents the Cultural Norm-based Cultural Alignment (CNCA) framework, which enables models to leverage their powerful reasoning ability to align with cultural norms. Specifically, we propose three methods to automatically mine cultural norms from limited survey data and explore ways to effectively utilize these norms for improving cultural alignment. Two alignment paradigms are examined: an in-context alignment method, where cultural norms are explicitly integrated into the user context, and a fine-tuning-based method, which internalizes norms through enhanced Chain-of-Thought training data. Comprehensive experiments demonstrate the effectiveness of these methods, highlighting that models with stronger reasoning capabilities benefit more from cultural norm mining and utilization. Our findings emphasize the potential for reasoning models to better reflect diverse human values through culturally informed alignment strategies.

</details>


### [370] [MedDCR: Learning to Design Agentic Workflows for Medical Coding](https://arxiv.org/abs/2511.13361)
*Jiyang Zheng,Islam Nassar,Thanh Vu,Xu Zhong,Yang Lin,Tongliang Liu,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: MedDCR introduces a closed-loop design-learning framework for medical coding that learns adaptable workflows through Designer-Coder-Reflector roles plus a memory archive, achieving better accuracy and interpretability than baselines.


<details>
  <summary>Details</summary>
Motivation: Automated medical coding currently relies on rigid, manually crafted workflows, failing to capture the nuance and variability of real-world clinical notes; there is a need to systematically learn effective, adaptable workflows that are trustworthy.

Method: MedDCR forms a closed-loop where a Designer proposes workflows, a Coder executes them, and a Reflector evaluates outputs and provides feedback; a memory archive stores past designs for reuse and refinement, enabling iterative improvement.

Result: On benchmark datasets, MedDCR outperforms state-of-the-art baselines and yields interpretable, adaptable coding workflows that better reflect real practice, enhancing reliability and trustworthiness of automated coding.

Conclusion: Learning workflows through the MedDCR loop can produce more accurate, robust medical coding systems and offers a general approach for learning structured workflows in complex NLP tasks.

Abstract: Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.

</details>


### [371] [Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning](https://arxiv.org/abs/2511.13371)
*Caroline Baumgartner,Eleanor Spens,Neil Burgess,Petru Manescu*

Main category: cs.AI

TL;DR: Large language models show two distinct spatial strategies depending on training: a map-like cognitive representation arising from exploratory data, and a path-dependent heuristic optimized for goal-directed tasks; a hybrid model sits between these, revealing a spectrum of spatial intelligence shaped by training regime.


<details>
  <summary>Details</summary>
Motivation: Investigate how transformers solve spatial navigation tasks and how different training regimes shape learned algorithms, representations, and mechanisms.

Method: Train GPT-2 on three grid-based spatial tasks: (1) passive exploration (foraging model) predicting steps in random walks; (2) goal-directed planning generating shortest paths on SP-Hamiltonian structures; (3) a hybrid SP-Random Walk fine-tuned with exploratory data. Perform behavioural, representational, and mechanistic analyses, including causal interventions and layer-wise tracking to identify phase transitions and strategy shifts.

Result: Two distinct algorithms emerge: the Foraging model develops a robust, map-like spatial representation (cognitive map) and shows a phase where reliance on historical direction tokens collapses in middle layers, indicating a self-contained coordinate system plus hierarchical, context-dependent reasoning. The goal-directed models rely on a path-dependent algorithm using explicit directional inputs across layers. The hybrid SP-Random Walk improves generalisation but preserves the path-dependent strategy. Overall, spatial intelligence in transformers spans a spectrum from world-model-like generalisation to goal-oriented heuristics; the training regime governs the generalisation-optimisation trade-off and the emergent strategies.

Conclusion: The study argues that transformer-based spatial intelligence is not monolithic but spans a continuum shaped by training data and objectives. Exploratory data favours cognitive-map-like representations, while goal-directed tasks promote consistent, path-dependent heuristics. Designing training regimes can steer emergent strategies toward more generalisable world models or task-specific heuristics.

Abstract: How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.

</details>


### [372] [An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence](https://arxiv.org/abs/2511.13411)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: A multi-axis, testable framework for measuring Autonomous AI progress from AAI-0 to AAI-4 (and beyond) using ten capability axes, a composite AAI-Index, a Self-Improvement Coefficient, closure properties, and the OWA-Bench benchmark, plus gates and a theorem relating AAI-3 to AAI-5 for falsifiable progress toward AGI.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of falsifiable, quantitative criteria for self-improving AI systems by moving beyond narrative ladders to a multi-axis, testable scale. Enables tracking of delegability versus autonomy and formal progression toward AGI with measurable milestones.

Method: Define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput). Aggregate into AAI-Index via a weighted geometric mean. Introduce Self-Improvement Coefficient κ (growth per agent-initiated resource) and two closure properties (maintenance and expansion) that render self-improvement falsifiable. Propose OWA-Bench, an open-world agency benchmark suite for long-horizon, tool-using, persistent agents. Establish level gates AAI-0 to AAI-4 via thresholds on axes, κ, and closure proofs. Provide synthetic experiments mapping current systems onto the scale and exploring the delegability frontier as self-improvement proceeds. Prove a theorem that under sufficient conditions an AAI-3 agent becomes AAI-5 over time, formalizing “baby AGI” to Superintelligence intuition.

Result: Demonstrates a concrete, testable framework with ten axes, a composite index, and measurable growth. Synthetic experiments show where present-day systems lie on the scale; the Delegability frontier advances with self-improvement; the OWA-Bench suite enables long-horizon evaluation. The formal theorem links mid-level AI capability (AAI-3) to higher levels (AAI-5) given sufficient conditions, offering falsifiable progress criteria.

Conclusion: The paper provides a structured, falsifiable pathway to quantify AI progression toward AGI, replacing vague narratives with measurable milestones, benchmarks, and theoretical progress conditions. However, practical challenges include axis weighting, measurement of capabilities, and ensuring robust, non-manipulable benchmarks.

Abstract: We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $κ$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\ldots AAI-4 using thresholds on the axes, $κ$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing "baby AGI" becomes Superintelligence intuition.

</details>


### [373] [Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation](https://arxiv.org/abs/2511.13476)
*Zhipeng Ma,Ali Rida Bahja,Andreas Burgdorf,André Pomp,Tobias Meisen,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.AI

TL;DR: A multi-agent framework using multimodal LLMs to automate data narration and energy insight generation for fuel efficiency in public transportation, validated on 4006 bus trips with Gaussian Mixture Model clustering; GPT-4.1 mini with Chain-of-Thought prompting identified as optimal, achieving 97.3% narrative accuracy and improved scalability.


<details>
  <summary>Details</summary>
Motivation: Overcome fragmented analytics and visualization outputs in energy informatics by enabling interpretable, stakeholder-oriented narratives through AI-driven storytelling, improving scalability and decision support.

Method: Three specialized agents—a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator—coordinate to iteratively transform analytical artifacts into coherent reports. Validation includes a real-world case study of public bus transportation in Northern Jutland, Denmark with 4006 trips; Gaussian Mixture Model clustering used for analysis. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as optimal.

Result: The approach yields high narrative accuracy (reported as 97.3%), improved factual precision, coherence, and scalability of LLM-based reporting, and demonstrates a replicable, domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.

Conclusion: Multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting, offering a replicable framework for AI-driven narrative generation and decision support in energy informatics.

Abstract: Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.

</details>


### [374] [FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI](https://arxiv.org/abs/2511.13524)
*Yuhang Peng,Yizhou Pan,Xinning He,Jihaoyu Yang,Xinyu Yin,Han Wang,Xiaoji Zheng,Chao Gao,Jiangtao Gong*

Main category: cs.AI

TL;DR: Introduces FreeAskWorld, a socially grounded, LLM-assisted embodied-AI simulation framework with a Direction Inquiry extension to Vision-and-Language Navigation (VLN); provides a large open benchmark and shows that fine-tuning on FreeAskWorld improves semantic understanding and interaction capabilities.


<details>
  <summary>Details</summary>
Motivation: Address the need for embodied AI that can plan at high levels and engage in naturalistic social interactions; incorporate interaction as a distinct information modality and leverage LLMs for planning and semantics.

Method: Develop FreeAskWorld to integrate LLM-driven high-level planning with semantically grounded interaction, plus a modular data-generation pipeline; extend VLN to a Direction Inquiry setting allowing active navigation guidance seeking; construct a large-scale benchmark with reconstructed environments, six task types, 16 object categories, ~63k frames and 17+ hours of interaction; benchmark both model variants and human participants in open-/closed-loop settings.

Result: Models fine-tuned on FreeAskWorld outperform originals, with enhanced semantic understanding and interaction competency; demonstration that interaction data improves embodied AI performance.

Conclusion: Socially grounded simulation frameworks like FreeAskWorld effectively advance embodied AI toward higher-level planning and more natural human-agent interactions; the released dataset will support training/evaluation and underscores interaction as a valuable information modality.

Abstract: As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.

</details>


### [375] [Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models](https://arxiv.org/abs/2511.13526)
*Zhengda Wang,Daqian Shi,Jingyi Zhao,Xiaolei Diao,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: Automated framework for constructing medical indicator knowledge graphs using retrieval-augmented generation (RAG) and large language models (LLMs), guided by clinical guidelines and ontology-based schemas, with expert-in-the-loop validation, enabling AI-driven diagnosis and QA in healthcare.


<details>
  <summary>Details</summary>
Motivation: Clinical decision support requires structured, interoperable knowledge. Current clinical knowledge graphs rely heavily on manual curation and rule-based extraction, which struggle with the complexity, variability, and ambiguity of medical guidelines and literature.

Method: Use retrieval-augmented generation with LLMs to extract and organize medical indicators from guidelines and literature. Employ guideline-driven data acquisition, an ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability.

Result: Generate knowledge graphs that are scalable, accurate, and clinically reliable, suitable for integration into intelligent diagnosis and question-answering systems, thereby accelerating AI-driven healthcare development.

Conclusion: An automated, scalable approach to building clinical knowledge graphs from guidelines and literature using RAG/LLMs and expert validation offers a feasible path toward robust AI-enabled healthcare.

Abstract: Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.

</details>


### [376] [Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction](https://arxiv.org/abs/2511.13565)
*Jingyi Zhao,Daqian Shi,Zhengda Wang,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: A multi-modal wearable framework called Human-Symbiotic Health Intelligence (HSHI) that integrates edge-cloud computing, AI-driven materials optimization, and hybrid modeling to enable adaptive, preventive health management through closed-loop intervention and digital twins.


<details>
  <summary>Details</summary>
Motivation: Traditional wearables rely on empirical material design and basic signal processing, leading to limited adaptability and interpretability; there is a need to dynamically account for inter- and intra-individual variability and to couple population insights with personalized health decisions.

Method: Proposes an architecture combining multi-modal sensor networks, edge-cloud collaboration, and a hybrid data/knowledge modeling approach; includes AI-driven optimization of materials and micro-structures, robust multi-modal signal interpretation, population-plus-personalized adaptation, and closed-loop optimization via reinforcement learning and digital twins.

Result: The work introduces a framework and design principles for HSHI, outlining expected improvements in adaptability, interpretation, and proactive health interventions; no empirical results are reported within the abstract.

Conclusion: HSHI represents a paradigm shift toward prevention, adaptability, and harmonious integration of technology with health management, enabling active collaboration between humans and intelligent systems.

Abstract: Intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing human-machine interaction. Traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques. To overcome these issues, we introduce the concept of Human-Symbiotic Health Intelligence (HSHI), which is a framework that integrates multi-modal sensor networks with edge-cloud collaborative computing and a hybrid approach to data and knowledge modeling. HSHI is designed to adapt dynamically to both inter-individual and intra-individual variability, transitioning health management from passive monitoring to an active collaborative evolution. The framework incorporates AI-driven optimization of materials and micro-structures, provides robust interpretation of multi-modal signals, and utilizes a dual mechanism that merges population-level insights with personalized adaptations. Moreover, the integration of closed-loop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback. In general, HSHI represents a significant shift in healthcare, moving towards a model that emphasizes prevention, adaptability, and a harmonious relationship between technology and health management.

</details>


### [377] [CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product](https://arxiv.org/abs/2511.13626)
*Kaiwen Xue,Chenglong Li,Zhonghong Ou,Guoxin Zhang,Kaoyan Lu,Shuai Lyu,Yifan Zhu,Ping Zong Junpeng Ding,Xinyu Liu,Qunlin Chen,Weiwei Qin,Yiran Shen,Jiayi Cen*

Main category: cs.AI

TL;DR: CreBench provides a two-component framework (a creativity evaluation benchmark and CreMIT dataset) to align multimodal LLMs with human creativity judgments, and CreExpert is fine-tuned from CreBench to outperform state-of-the-art MLLMs.


<details>
  <summary>Details</summary>
Motivation: Human-defined creativity is highly abstract and difficult for MLLMs to understand; there is no standard benchmark to assess alignment with human creativity judgments; a framework is needed to evaluate and improve creativity-oriented multimodal AI.

Method: Introduce CreBench with a multi-dimension evaluation benchmark; CreMIT: 2.2k multimodal data, 79.2k human feedbacks, 4.7M multi-typed instructions; refine feedback via GPT prompts to boost creativity assessment; fine-tune open-source MLLMs into CreExpert.

Result: CreExpert exhibits significantly better alignment with human creativity evaluation than GPT-4V and Gemini-Pro-Vision across experiments.

Conclusion: CreBench lays the foundation for building MLLMs that understand human-aligned creativity; the CreExpert approach demonstrates the viability of achieving human-aligned creativity evaluation and can guide future advances.

Abstract: Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.

</details>


### [378] [Beyond Mimicry: Preference Coherence in LLMs](https://arxiv.org/abs/2511.13630)
*Luhan Mikaelson,Derek Shiller,Hayley Clatterbuck*

Main category: cs.AI

TL;DR: The abstract assesses whether large language models have genuine, coherent preference structures in AI-specific trade-offs; findings show limited evidence of stable preferences, with substantial heterogeneity across models and tasks.


<details>
  <summary>Details</summary>
Motivation: To determine if AI systems exhibit stable, value-driven decision-making that would support safe deployment in settings requiring complex value trade-offs.

Method: Eight state-of-the-art models were evaluated across 48 model-category combinations involving AI-specific trade-offs (GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation). Logistic regression and behavioral classification were used to relate scenario intensity to choices, with temporal horizon manipulation to test for instrumental optimization.

Result: Among 48 combinations, 23 showed statistically significant relationships between scenario intensity and choices (47.9%). Of these, 15 (31.3%) exhibited within-range switching points. However, only 5 combinations (10.4%) demonstrated meaningful, adaptive/threshold-based coherence in preferences, while 26 (54.2%) showed no detectable trade-off behavior. Three proposed architectures explain patterns: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Temporal horizon testing yielded paradoxical patterns inconsistent with pure strategic optimization. Unstable transitions occurred in 45.8% of cases, and sensitivities were stimulus-specific, suggesting no unified preference structure across models.

Conclusion: The findings cast doubt on the presence of reliable, unified preference structures in current AI systems, raising concerns for deployments requiring nuanced value trade-offs and underscoring the need for explicit alignment mechanisms or more robust decision-making architectures.

Abstract: We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [379] [Too Many or Too Few? Sampling Bounds for Topological Descriptors](https://arxiv.org/abs/2511.12059)
*Brittany Terese Fasy,Maksym Makarchuk,Samuel Micka,David L. Millman*

Main category: cs.CG

TL;DR: The work investigates the minimal number of directional samples needed to faithfully capture geometric and topological information of shapes via Euler characteristic and persistence diagrams, offering constructive size bounds and empirical insights into over- and undersampling.


<details>
  <summary>Details</summary>
Motivation: Topological descriptors can encode complete geometric/topological information for shapes in R^d, but practical sampling with epsilon nets trades fidelity against computational efficiency. The goal is to determine how many directions are truly needed to preserve information while enabling efficient computation.

Method: Provide constructive proofs to derive size bounds for directional representations of geometric simplicial complexes. Conduct extensive experiments on synthetic and real datasets to study the effects of over- and undersampling and to assess how many directions are sufficient.

Result: Derived quantitative size bounds on the number of directions needed and provided empirical evidence from experiments showing the impact of sampling density on the fidelity of the topological/geometric representation.

Conclusion: There is a principled balance between sampling density and faithful representation; the paper identifies bounds and practical guidance on the minimal number of directions required to preserve the complete geometric/topological content within their framework, informing efficient sampling strategies.

Abstract: Topological descriptors, such as the Euler characteristic function and the persistence diagram, have grown increasingly popular for representing complex data. Recent work showed that a carefully chosen set of these descriptors encodes all of the geometric and topological information about a shape in R^d. In practice, epsilon nets are often used to find samples in one of two extremes. On one hand, making strong geometric assumptions about the shape allows us to choose epsilon small enough (corresponding to a high enough density sample) in order to guarantee a faithful representation, resulting in oversampling. On the other hand, if we choose a larger epsilon in order to allow faster computations, this leads to an incomplete description of the shape and a discretized transform that lacks theoretical guarantees. In this work, we investigate how many directions are really needed to represent geometric simplicial complexes, exploring both synthetic and real-world datasets. We provide constructive proofs that help establish size bounds and an experimental investigation giving insights into the consequences of over- and undersampling.

</details>


### [380] [In search of the Giant Convex Quadrilateral hidden in the Mountains](https://arxiv.org/abs/2511.13209)
*Nandana Ghosh,Rakesh Gupta,Ankush Acharyya*

Main category: cs.CG

TL;DR: Efficient O(n^2 log n) algorithm to find the maximum-area convex quadrilateral inside a 1.5D terrain; also shows that the maximum-area axis-parallel rectangle within the terrain yields a 1/2-approximation to the optimum convex quadrilateral.


<details>
  <summary>Details</summary>
Motivation: Understand and optimize the area of convex quadrilaterals within 1.5D terrains, and relate this optimal shape to simpler, tractable shapes (axis-aligned rectangles).

Method: Develop an O(n^2 log n) algorithm that exploits the structure of a 1.5D terrain (base line and monotone chain) to enumerate and evaluate candidate convex quadrilaterals efficiently, likely combining geometric observations with efficient data structures or sweeping techniques.

Result: An exact algorithm that computes the maximum-area convex quadrilateral inside the terrain in O(n^2 log n) time; additionally, the maximum-area axis-parallel rectangle inside the terrain provides a 1/2-factor approximation to this optimum.

Conclusion: The work provides the first efficient exact solution for this problem on 1.5D terrains and establishes a straightforward 1/2-approximation via axis-aligned rectangles, setting a baseline for potential improvements.

Abstract: A $1.5$D terrain is a simple polygon bounded by a line segment $\ell$ and a polygonal chain monotone with respect to the line segment $\ell$. Usually, $\ell$ is chosen aligned to the $x$-axis, and is called the base of the terrain. In this paper, we consider the problem of finding a convex quadrilateral of maximum area inside a $1.5$D terrain in $I\!\!R^2$. We present an $O(n^2\log n)$ time algorithm for this problem, where $n$ is the number of vertices of the terrain. Finally, we show that the maximum area axis-parallel rectangle inside the terrain yields a $\frac{1}{2}$ factor approximation result to the maximum area convex quadrilateral problem.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [381] [Hierarchical Federated Graph Attention Networks for Scalable and Resilient UAV Collision Avoidance](https://arxiv.org/abs/2511.11616)
*Rathin Chandra Shit,Sharmila Subudhi*

Main category: cs.RO

TL;DR: A scalable three-layer UAV collision avoidance framework that achieves real-time response, Byzantine fault tolerance, and privacy, enabling up to 500 UAVs with <2% collision rate and f < n/3, avoiding O(n^2) scaling.


<details>
  <summary>Details</summary>
Motivation: Need to balance real-time performance, adversarial resiliency, and privacy in large-scale multi-UAV systems; monolithic approaches are computationally expensive and lack Byzantine fault tolerance.

Method: Three-layer architecture: (1) local layer with dense graph attention achieving <10 ms latency for immediate collision avoidance; (2) regional layer with sparse attention and O(nk) computations, using asynchronous federated learning with coordinate-wise trimmed mean for robust aggregation; (3) global layer with a lightweight Hashgraph-inspired protocol. Adaptive differential privacy (epsilon in [0.1,1.0]) reduces noise based on real-time threat evaluation; DHT-based lightweight audit logging replaces heavyweight blockchain consensus.

Result: Demonstrated scalability to 500 UAVs with collision rate <2.0% and Byzantine fault tolerance f < n/3; median time to obtain the 95th percentile decision within 50 ms across tested swarm sizes.

Conclusion: The hierarchical framework balances latency, privacy, and resilience while reducing computational complexity and enabling practical collision avoidance in large UAV swarms through asynchronous FL, lightweight global consensus, and adaptive privacy controls.

Abstract: The real-time performance, adversarial resiliency, and privacy preservation are the most important metrics that need to be balanced to practice collision avoidance in large-scale multi-UAV (Unmanned Aerial Vehicle) systems. Current frameworks tend to prescribe monolithic solutions that are not only prohibitively computationally complex with a scaling cost of $O(n^2)$ but simply do not offer Byzantine fault tolerance. The proposed hierarchical framework presented in this paper tries to eliminate such trade-offs by stratifying a three-layered architecture. We spread the intelligence into three layers: an immediate collision avoiding local layer running on dense graph attention with latency of $<10 ms$, a regional layer using sparse attention with $O(nk)$ computational complexity and asynchronous federated learning with coordinate-wise trimmed mean aggregation, and lastly, a global layer using a lightweight Hashgraph-inspired protocol. We have proposed an adaptive differential privacy mechanism, wherein the noise level $(ε\in [0.1, 1.0])$ is dynamically reduced based on an evaluation of the measured real-time threat that in turn maximized the privacy-utility tradeoff. Through the use of Distributed Hash Table (DHT)-based lightweight audit logging instead of heavyweight blockchain consensus, the median cost of getting a $95^{th}$ percentile decision within 50ms is observed across all tested swarm sizes. This architecture provides a scalable scenario of 500 UAVs with a collision rate of $< 2.0\%$ and the Byzantine fault tolerance of $f < n/3$.

</details>


### [382] [Tactile Data Recording System for Clothing with Motion-Controlled Robotic Sliding](https://arxiv.org/abs/2511.11634)
*Michikuni Eguchi,Takekazu Kitagishi,Yuichi Hiroi,Takefumi Hiraki*

Main category: cs.RO

TL;DR: A robotic arm-based system gathers tactile data from intact garments by simulating fingertip stroking with controlled speed/direction, creating motion-labeled, multimodal tactile databases for fabric perception.


<details>
  <summary>Details</summary>
Motivation: Understanding clothing comfort requires tactile data during sliding; non-destructive, scalable data collection of fabric tactile properties is needed to reveal what makes clothing comfortable.

Method: A robotic arm performs stroking measurements with a simulated fingertip, precisely controlling speed and direction to collect motion-labeled, multimodal tactile data; data include audio, acceleration and other modalities for machine learning.

Result: Including motion-related parameters improves identification accuracy for audio and acceleration data in ML evaluation, demonstrating the value of motion labels for characterizing tactile sensation.

Conclusion: The system offers a scalable, non-destructive approach to capture tactile data from clothing, enabling future studies on fabric perception and reproduction.

Abstract: The tactile sensation of clothing is critical to wearer comfort. To reveal physical properties that make clothing comfortable, systematic collection of tactile data during sliding motion is required. We propose a robotic arm-based system for collecting tactile data from intact garments. The system performs stroking measurements with a simulated fingertip while precisely controlling speed and direction, enabling creation of motion-labeled, multimodal tactile databases. Machine learning evaluation showed that including motion-related parameters improved identification accuracy for audio and acceleration data, demonstrating the efficacy of motion-related labels for characterizing clothing tactile sensation. This system provides a scalable, non-destructive method for capturing tactile data of clothing, contributing to future studies on fabric perception and reproduction.

</details>


### [383] [Image-based Morphological Characterization of Filamentous Biological Structures with Non-constant Curvature Shape Feature](https://arxiv.org/abs/2511.11639)
*Jie Fan,Francesco Visentin,Barbara Mazzolai,Emanuela Del Dottore*

Main category: cs.RO

TL;DR: A geometry-based, 3D piece-wise clothoid model reconstructs tendril shapes under mechanical stimulation with high accuracy (R^2>0.99), offering data-efficient, interpretable insights and potential bio-inspired robotics advantages over deep learning.


<details>
  <summary>Details</summary>
Motivation: To link temporal shape changes in climbing plant tendrils to triggering events and contact locations, addressing limitations of prior studies that rely heavily on deep learning or qualitative observations.

Method: Image-based analysis using a 3D Piece-Wise Clothoid-based model to reconstruct tendril configurations after mechanical rubbing; comparison with deep learning approaches highlighting data efficiency and interpretability; experimental analysis across different stimulation locations (apical vs other segments).

Result: The model achieves high robustness and accuracy (R^2>0.99); reveals higher responsiveness in the apical segment; demonstrates advantages of the clothoid-based geometric method over DL in data requirements and computational cost; provides interpretable insights into tendril biomechanics.

Conclusion: The approach advances plant biomechanics understanding and lays a foundation for designing intelligent robotic systems inspired by climbing plants, enabling targeted manipulation of shape changes with lower data and computational demands.

Abstract: Tendrils coil their shape to anchor the plant to supporting structures, allowing vertical growth toward light. Although climbing plants have been studied for a long time, extracting information regarding the relationship between the temporal shape change, the event that triggers it, and the contact location is still challenging. To help build this relation, we propose an image-based method by which it is possible to analyze shape changes over time in tendrils when mechano-stimulated in different portions of their body. We employ a geometric approach using a 3D Piece-Wise Clothoid-based model to reconstruct the configuration taken by a tendril after mechanical rubbing. The reconstruction shows high robustness and reliability with an accuracy of R2 > 0.99. This method demonstrates distinct advantages over deep learning-based approaches, including reduced data requirements, lower computational costs, and interpretability. Our analysis reveals higher responsiveness in the apical segment of tendrils, which might correspond to higher sensitivity and tissue flexibility in that region of the organs. Our study provides a methodology for gaining new insights into plant biomechanics and offers a foundation for designing and developing novel intelligent robotic systems inspired by climbing plants.

</details>


### [384] [ExpertAD: Enhancing Autonomous Driving Systems with Mixture of Experts](https://arxiv.org/abs/2511.11740)
*Haowen Jiang,Xinyu Huang,You Lu,Dingji Wang,Yuheng Cao,Chaofeng Sha,Bihuan Chen,Keyu Chen,Xin Peng*

Main category: cs.RO

TL;DR: ExpertAD introduces a Mixture of Experts-based framework for end-to-end autonomous driving, featuring a Perception Adapter to emphasize task-critical features and a Mixture of Sparse Experts to reduce inter-task interference. It achieves faster inference and safer decisions (up to 20% fewer collisions and 25% lower latency), demonstrates multi-skill planning in rare scenarios, generalizes to unseen urban environments, and includes a case study of its decision-making in complex driving contexts.


<details>
  <summary>Details</summary>
Motivation: End-to-end ADSs face three core issues: ambiguous or noisy semantics that degrade decision reliability, interference among concurrent driving tasks that hinders planning, and long inference latency that slows responses and raises safety risk. A robust framework should enhance perception-reasoning, decouple tasks to limit interference, and accelerate inference.

Method: Propose ExpertAD, a Mixture of Experts framework for end-to-end ADS. It includes a Perception Adapter (PA) to amplify task-critical features for contextually relevant scene understanding and a Mixture of Sparse Experts (MoSE) to minimize task interference during prediction. The overall approach leverages a Mixture of Experts (MoE) architecture to balance accuracy and efficiency in perception and planning tasks.

Result: Experimental evaluation shows up to 20% reduction in average collision rates and 25% reduction in inference latency compared to prior methods. The framework demonstrates effective multi-skill planning in rare scenarios (e.g., accidents, yielding to emergency vehicles) and strong generalization to unseen urban environments. A case study illustrates its decision-making process in complex driving scenarios.

Conclusion: ExpertAD effectively enhances end-to-end ADS by combining perception-focused feature amplification with a sparse, task-decoupled MoE framework. The approach improves safety and efficiency, supports multi-skill planning, and generalizes to new environments, with a demonstrative case study of its decision-making in challenging scenarios.

Abstract: Recent advancements in end-to-end autonomous driving systems (ADSs) underscore their potential for perception and planning capabilities. However, challenges remain. Complex driving scenarios contain rich semantic information, yet ambiguous or noisy semantics can compromise decision reliability, while interference between multiple driving tasks may hinder optimal planning. Furthermore, prolonged inference latency slows decision-making, increasing the risk of unsafe driving behaviors. To address these challenges, we propose ExpertAD, a novel framework that enhances the performance of ADS with Mixture of Experts (MoE) architecture. We introduce a Perception Adapter (PA) to amplify task-critical features, ensuring contextually relevant scene understanding, and a Mixture of Sparse Experts (MoSE) to minimize task interference during prediction, allowing for effective and efficient planning. Our experiments show that ExpertAD reduces average collision rates by up to 20% and inference latency by 25% compared to prior methods. We further evaluate its multi-skill planning capabilities in rare scenarios (e.g., accidents, yielding to emergency vehicles) and demonstrate strong generalization to unseen urban environments. Additionally, we present a case study that illustrates its decision-making process in complex driving scenarios.

</details>


### [385] [Large Language Models and 3D Vision for Intelligent Robotic Perception and Autonomy: A Review](https://arxiv.org/abs/2511.11777)
*Vinit Mehta,Charu Sharma,Karthick Thiyagarajan*

Main category: cs.RO

TL;DR: A comprehensive survey of how Large Language Models (LLMs) and 3D vision are converging to enhance robotic sensing, covering foundational data representations, 3D sensing technologies, scene understanding, grounding, embodied agents, multimodal LLMs with 3D data and other modalities, datasets, benchmarks, challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: To bridge linguistic intelligence and spatial perception for intelligent, autonomous robots; to enable natural-language-guided perception, reasoning, and manipulation in complex 3D environments; to map the current landscape and identify gaps and future directions.

Method: Systematic review of state-of-the-art methods at the intersection of LLMs and 3D vision, including 3D data representations, zero-shot 3D segmentation, text-to-3D generation, language grounding, embodied agents, and multimodal LLMs integrating touch/auditory/thermal inputs; catalog datasets and metrics; synthesize challenges and propose future directions.

Result: Synthesizes a taxonomy of techniques and applications; highlights progress in scene understanding, 3D grounding, and language-guided control; documents multimodal enhancements; identifies datasets and benchmarks; offers a set of challenges and research directions.

Conclusion: LLM-3D vision integration is a promising pathway for more adaptive, context-aware robotic sensing; advancing adaptive architectures, cross-modal alignment, and real-time processing will be key to robust, autonomous systems; the review lays groundwork for future research and application in robotics.

Abstract: With the rapid advancement of artificial intelligence and robotics, the integration of Large Language Models (LLMs) with 3D vision is emerging as a transformative approach to enhancing robotic sensing technologies. This convergence enables machines to perceive, reason and interact with complex environments through natural language and spatial understanding, bridging the gap between linguistic intelligence and spatial perception. This review provides a comprehensive analysis of state-of-the-art methodologies, applications and challenges at the intersection of LLMs and 3D vision, with a focus on next-generation robotic sensing technologies. We first introduce the foundational principles of LLMs and 3D data representations, followed by an in-depth examination of 3D sensing technologies critical for robotics. The review then explores key advancements in scene understanding, text-to-3D generation, object grounding and embodied agents, highlighting cutting-edge techniques such as zero-shot 3D segmentation, dynamic scene synthesis and language-guided manipulation. Furthermore, we discuss multimodal LLMs that integrate 3D data with touch, auditory and thermal inputs, enhancing environmental comprehension and robotic decision-making. To support future research, we catalog benchmark datasets and evaluation metrics tailored for 3D-language and vision tasks. Finally, we identify key challenges and future research directions, including adaptive model architectures, enhanced cross-modal alignment and real-time processing capabilities, which pave the way for more intelligent, context-aware and autonomous robotic sensing systems.

</details>


### [386] [LAVQA: A Latency-Aware Visual Question Answering Framework for Shared Autonomy in Self-Driving Vehicles](https://arxiv.org/abs/2511.11840)
*Shuangyu Xie,Kaiyuan Chen,Wenjing Chen,Chengyuan Qian,Christian Juette,Liu Ren,Dezhen Song,Ken Goldberg*

Main category: cs.RO

TL;DR: LAVQA introduces a latency-aware shared autonomy framework that combines visual question answering with a dynamic LICOM map to manage latency in remote-assisted autonomous driving; results show >8x collision reduction in CARLA simulations vs latency-agnostic baselines.


<details>
  <summary>Details</summary>
Motivation: When uncertainty is high, vehicles benefit from remote human guidance, but variable wireless latency and human reaction times create critical timing challenges; a framework is needed to coordinate autonomous decisions with delayed inputs.

Method: Integrates Visual Question Answering and a Latency-Induced Collision Map (LICOM) into a shared autonomy system; LICOM tracks temporal latency and spatial uncertainty and evolves with dynamic obstacles; provides spatiotemporal risk visualization for remote operators; evaluated via closed-loop CARLA simulations.

Result: Collision rates reduced by over 8x relative to latency-agnostic baselines.

Conclusion: Latency-aware shared autonomy with LICOM-VQA visualization enhances safety by accommodating latency and enabling informed remote control; demonstrates potential to improve performance in high-uncertainty driving scenarios.

Abstract: When uncertainty is high, self-driving vehicles may halt for safety and benefit from the access to remote human operators who can provide high-level guidance. This paradigm, known as {shared autonomy}, enables autonomous vehicle and remote human operators to jointly formulate appropriate responses. To address critical decision timing with variable latency due to wireless network delays and human response time, we present LAVQA, a latency-aware shared autonomy framework that integrates Visual Question Answering (VQA) and spatiotemporal risk visualization. LAVQA augments visual queries with Latency-Induced COllision Map (LICOM), a dynamically evolving map that represents both temporal latency and spatial uncertainty. It enables remote operator to observe as the vehicle safety regions vary over time in the presence of dynamic obstacles and delayed responses. Closed-loop simulations in CARLA, the de-facto standard for autonomous vehicle simulator, suggest that that LAVQA can reduce collision rates by over 8x compared to latency-agnostic baselines.

</details>


### [387] [Autonomous Underwater Cognitive System for Adaptive Navigation: A SLAM-Integrated Cognitive Architecture](https://arxiv.org/abs/2511.11845)
*K. A. I. N Jayarathne,R. M. N. M. Rathnayaka,D. P. S. S. Peiris*

Main category: cs.RO

TL;DR: An Autonomous Underwater Cognitive System (AUCS) fuses SLAM with a Soar-based cognitive architecture to enable adaptive navigation in deep-sea environments through multi-sensor fusion, semantic understanding, adaptive sensing, and memory-based learning; it achieves a complete perception-cognition-action-learning loop and promises improved safety, reliability, and autonomy.


<details>
  <summary>Details</summary>
Motivation: Address disorientation, communication loss, and navigational failures in dynamic underwater environments by integrating perception, reasoning, and learning to enable autonomous, adaptive behavior.

Method: Integrate SLAM with a Soar cognitive architecture. Fuse multi-sensor data (SONAR, LiDAR, IMU, DVL) with cognitive modules for perception, attention, planning, and learning. Add semantic understanding, adaptive sensor management, and memory-based learning to distinguish dynamic vs. static objects, reducing false loop closures and improving long-term map consistency.

Result: Demonstrates a complete perception-cognition-action-learning loop enabling autonomous underwater vehicles to sense, reason, and adapt intelligently; claims improved safety, reliability, and autonomy and enhanced map consistency through semantic and memory-based processing.

Conclusion: AUCS provides a foundation for next-generation cognitive submersible systems capable of safe, reliable, and autonomous deep-sea exploration by integrating semantic understanding and adaptive cognition into SLAM.

Abstract: Deep-sea exploration poses significant challenges, including disorientation, communication loss, and navigational failures in dynamic underwater environments. This paper presents an Autonomous Underwater Cognitive System (AUCS) that integrates Simultaneous Localization and Mapping (SLAM) with a Soar-based cognitive architecture to enable adaptive navigation in complex oceanic conditions. The system fuses multi-sensor data from SONAR, LiDAR, IMU, and DVL with cognitive reasoning modules for perception, attention, planning, and learning. Unlike conventional SLAM systems, AUCS incorporates semantic understanding, adaptive sensor management, and memory-based learning to differentiate between dynamic and static objects, reducing false loop closures and enhancing long-term map consistency. The proposed architecture demonstrates a complete perception-cognition-action-learning loop, allowing autonomous underwater vehicles to sense, reason, and adapt intelligently. This work lays a foundation for next-generation cognitive submersible systems, improving safety, reliability, and autonomy in deep-sea exploration.

</details>


### [388] [MATT-Diff: Multimodal Active Target Tracking by Diffusion Policy](https://arxiv.org/abs/2511.11931)
*Saida Liu,Nikolay Atanasov,Shumon Koga*

Main category: cs.RO

TL;DR: Diffusion-policy control (MATT-Diff) for active multi-target tracking that learns multi-modal actions balancing exploration, tracking, and reacquisition without prior target info, using ViT-based map tokenization and Gaussian-target representations; outperforms expert and behavior cloning baselines.


<details>
  <summary>Details</summary>
Motivation: Active multi-target tracking under unknown numbers/dynamics requires balancing exploration and tracking; existing planners rely on prior knowledge or brittle heuristics; a flexible, data-driven policy is needed.

Method: Train a diffusion model-based policy (MATT-Diff) with three behavioral modes; use egocentric map tokens via vision transformer; attend over Gaussian target estimates; train on demonstrations from three expert planners (frontier exploration, uncertainty-based hybrid with RRT*, time-based hybrid); action sequences generated via denoising; no prior knowledge of target counts or dynamics.

Result: Empirical evaluations show superior tracking performance versus expert and behavior cloning baselines across multiple target motions; the policy robustly balances exploration and tracking, including reacquisition.

Conclusion: Diffusion-based multi-modal control is effective for active multi-target tracking under uncertainty; the approach integrates flexible sensing representations and mode switching, suggesting broader applicability and extension to real-world scenarios.

Abstract: This paper proposes MATT-Diff: Multi-Modal Active Target Tracking by Diffusion Policy, a control policy that captures multiple behavioral modes - exploration, dedicated tracking, and target reacquisition - for active multi-target tracking. The policy enables agent control without prior knowledge of target numbers, states, or dynamics. Effective target tracking demands balancing exploration for undetected or lost targets with following the motion of detected but uncertain ones. We generate a demonstration dataset from three expert planners including frontier-based exploration, an uncertainty-based hybrid planner switching between frontier-based exploration and RRT* tracking based on target uncertainty, and a time-based hybrid planner switching between exploration and tracking based on target detection time. We design a control policy utilizing a vision transformer for egocentric map tokenization and an attention mechanism to integrate variable target estimates represented by Gaussian densities. Trained as a diffusion model, the policy learns to generate multi-modal action sequences through a denoising process. Evaluations demonstrate MATT-Diff's superior tracking performance against expert and behavior cloning baselines across multiple target motions, empirically validating its advantages in target tracking.

</details>


### [389] [Characterization and Evaluation of Screw-Based Locomotion Across Aquatic, Granular, and Transitional Media](https://arxiv.org/abs/2511.11958)
*Derek Chen,Zoe Samuels,Lizzie Peiros,Sujaan Mukherjee,Michael C. Yip*

Main category: cs.RO

TL;DR: Principles-first study identifies dominant design parameters for screw-based propulsion across water and granular media, using heat-sink-inspired metrics to categorize performance and guide shell design and adaptive locomotion.


<details>
  <summary>Details</summary>
Motivation: to optimize versatile amphibious screw propulsion and address locomotion across media and transitions.

Method: Systematic experimental analysis of multiple screw configurations in dry sand, wet sand, saturated sand, and water; apply principles-first reasoning to extract dominant performance parameters; derive heat-sink-inspired metrics to categorize performance.

Result: Dominant parameters identified; derived metrics enable classification of performance regimes; design guidance for screw shells and adaptive locomotion strategies.

Conclusion: Advances versatile amphibious screw propulsion by outlining dominant design parameters and practical design/adaptation guidelines.

Abstract: Screw-based propulsion systems offer promising capabilities for amphibious mobility, yet face significant challenges in optimizing locomotion across water, granular materials, and transitional environments. This study presents a systematic investigation into the locomotion performance of various screw configurations in media such as dry sand, wet sand, saturated sand, and water. Through a principles-first approach to analyze screw performance, it was found that certain parameters are dominant in their impact on performance. Depending on the media, derived parameters inspired from optimizing heat sink design help categorize performance within the dominant design parameters. Our results provide specific insights into screw shell design and adaptive locomotion strategies to enhance the performance of screw-based propulsion systems for versatile amphibious applications.

</details>


### [390] [Bootstrapped LLM Semantics for Context-Aware Path Planning](https://arxiv.org/abs/2511.11967)
*Mani Amani,Behrad Beheshti,Reza Akhavian*

Main category: cs.RO

TL;DR: Converts natural-language prompts into a stochastic semantic sensor using an LLM to modulate a classical planner via Bayesian risk estimation, demonstrated in simulated and BIM digital twin settings.


<details>
  <summary>Details</summary>
Motivation: Bridges the gap between NL task specification and safe, efficient execution in semantically rich, human-centric spaces by making LLM-derived semantic risk influence planning.

Method: Treat the LLM as a stochastic semantic sensor that outputs per-class danger judgments from a semantic map and prompt. Use Bayesian bootstrap to approximate a posterior over class-specific risk. Compute a risk-informed potential cost and embed it into a traditional path-planning formulation. Evaluate in simulated environments and a BIM-backed digital twin, examining adaptation to explicit prompts and implicit context.

Result: The approach yields qualitative and quantitative results showing the robot adapts its movement in response to prompts and contextual information, across varying environments.

Conclusion: NL-guided motion planning with LLM-derived semantic risk sensing can safely and adaptively steer robot behavior in human-centric spaces, validated through simulations and digital-twin experiments.

Abstract: Prompting robots with natural language (NL) has largely been studied as what task to execute (goal selection, skill sequencing) rather than how to execute that task safely and efficiently in semantically rich, human-centric spaces. We address this gap with a framework that turns a large language model (LLM) into a stochastic semantic sensor whose outputs modulate a classical planner. Given a prompt and a semantic map, we draw multiple LLM "danger" judgments and apply a Bayesian bootstrap to approximate a posterior over per-class risk. Using statistics from the posterior, we create a potential cost to formulate a path planning problem. Across simulated environments and a BIM-backed digital twin, our method adapts how the robot moves in response to explicit prompts and implicit contextual information. We present qualitative and quantitative results.

</details>


### [391] [ARCSnake V2: An Amphibious Multi-Domain Screw-Propelled Snake-Like Robot](https://arxiv.org/abs/2511.11970)
*Sara Wickenhiser,Lizzie Peiros,Calvin Joyce,Peter Gavrilrov,Sujaan Mukherjee,Syler Sylvester,Junrong Zhou,Mandy Cheung,Jason Lim,Florian Richter,Michael C. Yip*

Main category: cs.RO

TL;DR: ARCSnake V2 is a versatile amphibious, screw-propelled, snake-like robot enabling teleoperated or autonomous locomotion across land, granular media, and water. It combines hyper-redundant snake mobility with Archimedean screw propulsion, featuring a sealed design, buoyancy control, and a teleoperation interface, enabling screwing, wheeling, and sidewinding with smooth transitions; validated by underwater maneuverability, communication robustness, and force-regulated actuation.


<details>
  <summary>Details</summary>
Motivation: Extreme environments (caves, oceans, planetary surfaces) pose locomotion challenges; conventional wheeled/legged robots struggle with surface variability. A versatile, multi-domain platform is needed for exploration, search-and-rescue, and environmental monitoring.

Method: Develop a water-sealed, serially linked screw-and-joint actuation design with integrated buoyancy control; implement teleoperation via a kinematically matched handheld controller; enable multiple locomotion modes (screwing, wheeling, sidewinding) with smooth transitions.

Result: Extensive experiments validate underwater maneuverability, robust communication, and force-regulated actuation, demonstrating effective multi-domain locomotion.

Conclusion: ARCSnake V2 is a capable, multi-domain platform suitable for exploration, search-and-rescue, and environmental monitoring, with strong potential for teleoperated or autonomous operation across land, granular media, and aquatic environments.

Abstract: Robotic exploration in extreme environments such as caves, oceans, and planetary surfaces pose significant challenges, particularly in locomotion across diverse terrains. Conventional wheeled or legged robots often struggle in these contexts due to surface variability. This paper presents ARCSnake V2, an amphibious, screw propelled, snake like robot designed for teleoperated or autonomous locomotion across land, granular media, and aquatic environments. ARCSnake V2 combines the high mobility of hyper redundant snake robots with the terrain versatility of Archimedean screw propulsion. Key contributions include a water sealed mechanical design with serially linked screw and joint actuation, an integrated buoyancy control system, and teleoperation via a kinematically matched handheld controller. The robots design and control architecture enable multiple locomotion modes screwing, wheeling, and sidewinding with smooth transitions between them. Extensive experiments validate its underwater maneuverability, communication robustness, and force regulated actuation. These capabilities position ARCSnake V2 as a versatile platform for exploration, search and rescue, and environmental monitoring in multi domain settings.

</details>


### [392] [SBAMP: Sampling Based Adaptive Motion Planning](https://arxiv.org/abs/2511.12022)
*Anh-Quan Pham,Kabir Ram Puri,Shreyas Raorane*

Main category: cs.RO

TL;DR: SBAMP integrates RRT* global planning with a SEDS-based local controller to enable real-time, adaptive motion planning in dynamic environments without pre-trained data, while maintaining Lyapunov-based stability and global path optimality.


<details>
  <summary>Details</summary>
Motivation: Traditional sampling-based planners like RRT* excel at finding collision-free paths but are slow to adapt to dynamic changes; learning-based dynamical systems (e.g., SEDS) offer smooth, adaptive tracking but require pre-collected demonstrations, limiting generalization. There is a need for real-time, data-free adaptation that preserves global optimality in dynamic, unstructured environments.

Method: Propose Sampling-Based Adaptive Motion Planning (SBAMP): use RRT* for global path planning and a SEDS-based local controller for continuous trajectory adjustment; the approach requires no pre-trained datasets, ensures smooth transitions between waypoints, and provides Lyapunov-based stability guarantees. Validation includes simulation and real hardware on the RoboRacer platform.

Result: SBAMP demonstrates superior performance in dynamic obstacle scenarios, fast recovery from perturbations, and robust handling of sharp turns. It adapts in real time without sacrificing global path optimality and scales to dynamic, unstructured environments.

Conclusion: SBAMP offers a scalable, real-time adaptive motion planning framework that combines the strengths of sampling-based global planning with data-free, stable local control, validated on hardware and in simulation for dynamic environments.

Abstract: Autonomous robotic systems must navigate complex, dynamic environments in real time, often facing unpredictable obstacles and rapidly changing conditions. Traditional sampling-based methods, such as RRT*, excel at generating collision-free paths but struggle to adapt to sudden changes without extensive replanning. Conversely, learning-based dynamical systems, such as the Stable Estimator of Dynamical Systems (SEDS), offer smooth, adaptive trajectory tracking but typically rely on pre-collected demonstration data, limiting their generalization to novel scenarios. This paper introduces Sampling-Based Adaptive Motion Planning (SBAMP), a novel framework that overcomes these limitations by integrating RRT* for global path planning with a SEDS-based local controller for continuous, adaptive trajectory adjustment. Our approach requires no pre-trained datasets and ensures smooth transitions between planned waypoints, maintaining stability through Lyapunov-based guarantees. We validate SBAMP in both simulated environments and real hardware using the RoboRacer platform, demonstrating superior performance in dynamic obstacle scenarios, rapid recovery from perturbations, and robust handling of sharp turns. Experimental results highlight SBAMP's ability to adapt in real time without sacrificing global path optimality, providing a scalable solution for dynamic, unstructured environments.

</details>


### [393] [Decoupled Action Head: Confining Task Knowledge to Conditioning Layers](https://arxiv.org/abs/2511.12101)
*Jian Zhou,Sihao Lin,Shuai Fu,Qi WU*

Main category: cs.RO

TL;DR: Decoupled training for diffusion-policy behavior cloning: pretrain a general action head on cheap, observation-free, kinematics-generated data, freeze it, and adapt via feature modulation for new tasks; DP-MLP replaces a large U-Net with a small MLP to boost speed, achieving major training-time reductions while preserving performance.


<details>
  <summary>Details</summary>
Motivation: BC methods like DP suffer from scarce paired data and incomplete understanding of what makes DP effective; the goal is data-efficient training, clearer design principles, and faster training without sacrificing performance.

Method: Pretrain a general action head on nearly cost-free kinematics-generated trajectories (observation-free data). Freeze the pretrained head and adapt it to new tasks through feature modulation. Evaluate in both in-distribution and out-of-distribution scenarios. Introduce DP-MLP by replacing DP-C's 244M-parameter U-Net backbone with a 4M-parameter MLP, aiming for substantial speedups.

Result: DP-C achieves up to 41% training speedup with decoupled training; performance under decoupled training remains nearly identical to normal training; task-specific knowledge becomes confined to conditioning components. DP-MLP delivers an 83.9% faster training speed under normal training and 89.1% under decoupling.

Conclusion: Decoupled pretraining of a general action head is an effective and efficient strategy for DP-based BC in robotic manipulation, enabling data-efficient learning and faster training. A lightweight DP-MLP backbone can maintain performance while dramatically reducing training time, highlighting that the action-generation backbone plays a limited role in manipulation performance.

Abstract: Behavior Cloning (BC) is a data-driven supervised learning approach that has gained increasing attention with the success of scaling laws in language and vision domains. Among its implementations in robotic manipulation, Diffusion Policy (DP), with its two variants DP-CNN (DP-C) and DP-Transformer (DP-T), is one of the most effective and widely adopted models, demonstrating the advantages of predicting continuous action sequences. However, both DP and other BC methods remain constrained by the scarcity of paired training data, and the internal mechanisms underlying DP's effectiveness remain insufficiently understood, leading to limited generalization and a lack of principled design in model development. In this work, we propose a decoupled training recipe that leverages nearly cost-free kinematics-generated trajectories as observation-free data to pretrain a general action head (action generator). The pretrained action head is then frozen and adapted to novel tasks through feature modulation. Our experiments demonstrate the feasibility of this approach in both in-distribution and out-of-distribution scenarios. As an additional benefit, decoupling improves training efficiency; for instance, DP-C achieves up to a 41% speedup. Furthermore, the confinement of task-specific knowledge to the conditioning components under decoupling, combined with the near-identical performance of DP-C in both normal and decoupled training, indicates that the action generation backbone plays a limited role in robotic manipulation. Motivated by this observation, we introduce DP-MLP, which replaces the 244M-parameter U-Net backbone of DP-C with only 4M parameters of simple MLP blocks, achieving a 83.9% faster training speed under normal training and 89.1% under decoupling.

</details>


### [394] [Towards Obstacle-Avoiding Control of Planar Snake Robots Exploring Neuro-Evolution of Augmenting Topologies](https://arxiv.org/abs/2511.12148)
*Advik Sinha,Akshay Arjun,Abhijit Das,Joyjit Mukherjee*

Main category: cs.RO

TL;DR: A NEAT-based neuroevolution framework optimizes serpenoid gait parameters for obstacle-avoiding tracking of a planar snake robot in dense clutter, using high-dimensional sensor data as input and low-dimensional gait outputs, validated in PyBullet with favorable efficiency and competitive performance.


<details>
  <summary>Details</summary>
Motivation: Address resource-efficient, robust obstacle-avoidance tracking for a planar snake robot in densely cluttered environments by leveraging evolutionary neural networks to adapt gait dynamics in real time.

Method: Use NEAT to evolve parameters (frequency and offset angle) of a serpenoid gait that actuates the snake robot's joints. Input to the network includes joint angles, link positions, head position, and nearby obstacle data; output is gait frequency and heading offset. A reward function, based on LiDAR obstacle data, robot state, goal location, and time, is optimized via selective propagation of high-performing networks. Validation performed in PyBullet simulations; comparison shows efficiency and competitive performance against state-of-the-art and CBRL approaches.

Result: The approach achieves computational efficiency, particularly in large obstacle-dense environments. It yields superior performance relative to existing methods and comparable results to a recent CBRL approach but with significantly lower computational overhead.

Conclusion: NEAT-based gait adaptation provides an effective and scalable solution for obstacle-avoiding tracking control of planar snake robots in cluttered environments, delivering competitive performance with reduced computation; further validation on real hardware and exploration of generalization to varied environments would strengthen its applicability.

Abstract: This work aims to develop a resource-efficient solution for obstacle-avoiding tracking control of a planar snake robot in a densely cluttered environment with obstacles. Particularly, Neuro-Evolution of Augmenting Topologies (NEAT) has been employed to generate dynamic gait parameters for the serpenoid gait function, which is implemented on the joint angles of the snake robot, thus controlling the robot on a desired dynamic path. NEAT is a single neural-network based evolutionary algorithm that is known to work extremely well when the input layer is of significantly higher dimension and the output layer is of a smaller size. For the planar snake robot, the input layer consists of the joint angles, link positions, head link position as well as obstacle positions in the vicinity. However, the output layer consists of only the frequency and offset angle of the serpenoid gait that control the speed and heading of the robot, respectively. Obstacle data from a LiDAR and the robot data from various sensors, along with the location of the end goal and time, are employed to parametrize a reward function that is maximized over iterations by selective propagation of superior neural networks. The implementation and experimental results showcase that the proposed approach is computationally efficient, especially for large environments with many obstacles. The proposed framework has been verified through a physics engine simulation study on PyBullet. The approach shows superior results to existing state-of-the-art methodologies and comparable results to the very recent CBRL approach with significantly lower computational overhead. The video of the simulation can be found here: https://sites.google.com/view/neatsnakerobot

</details>


### [395] [Game-Theoretic Safe Multi-Agent Motion Planning with Reachability Analysis for Dynamic and Uncertain Environments (Extended Version)](https://arxiv.org/abs/2511.12160)
*Wenbin Mai,Minghui Liwang,Xinlei Yi,Xiaoyu Xia,Seyyedali Hosseinalipour,Xianbin Wang*

Main category: cs.RO

TL;DR: A Reachability-Enhanced Dynamic Potential Game (RE-DPG) for safe, scalable multi-agent motion planning, combining dynamic potential games with reachability; introduces ND-iBR and MA-FRS to ensure convergence and safety under uncertainty; validated in 2D/3D simulations and real-world experiments.


<details>
  <summary>Details</summary>
Motivation: To overcome computational complexity of coupled multi-agent decision-making and to provide proactive safety guarantees in dynamic, uncertain environments by integrating reachability with game-theoretic coordination.

Method: Formulate multi-agent coordination as a dynamic potential game where Nash equilibrium yields optimal strategies. Develop Neighborhood-Dominated iterative Best Response (ND-iBR) based on an iterated ε-BR (iε-BR) process with finite-step convergence to an ε-NE for decentralized execution. Incorporate a Multi-Agent Forward Reachable Set (MA-FRS) into the cost to model uncertainty propagation and enforce collision avoidance constraints. Validate via simulations and real-world experiments in 2D and 3D environments.

Result: The approach provides theoretical convergence guarantees (finite-step convergence to ε-NE) and demonstrated safety under uncertainty. It shows effective coordination and collision avoidance in diverse 2D/3D scenarios, with scalable decentralized computation.

Conclusion: RE-DPG offers a scalable, decentralized, and provably safe framework for multi-agent motion planning under uncertainty, achieving robust performance across varied operational environments and bridging reachability analysis with game-theoretic control.

Abstract: Ensuring safe, robust, and scalable motion planning for multi-agent systems in dynamic and uncertain environments is a persistent challenge, driven by complex inter-agent interactions, stochastic disturbances, and model uncertainties. To overcome these challenges, particularly the computational complexity of coupled decision-making and the need for proactive safety guarantees, we propose a Reachability-Enhanced Dynamic Potential Game (RE-DPG) framework, which integrates game-theoretic coordination into reachability analysis. This approach formulates multi-agent coordination as a dynamic potential game, where the Nash equilibrium (NE) defines optimal control strategies across agents. To enable scalability and decentralized execution, we develop a Neighborhood-Dominated iterative Best Response (ND-iBR) scheme, built upon an iterated $\varepsilon$-BR (i$\varepsilon$-BR) process that guarantees finite-step convergence to an $\varepsilon$-NE. This allows agents to compute strategies based on local interactions while ensuring theoretical convergence guarantees. Furthermore, to ensure safety under uncertainty, we integrate a Multi-Agent Forward Reachable Set (MA-FRS) mechanism into the cost function, explicitly modeling uncertainty propagation and enforcing collision avoidance constraints. Through both simulations and real-world experiments in 2D and 3D environments, we validate the effectiveness of RE-DPG across diverse operational scenarios.

</details>


### [396] [Variable Impedance Control for Floating-Base Supernumerary Robotic Leg in Walking Assistance](https://arxiv.org/abs/2511.12184)
*Jun Huo,Kehan Xu,Chengyao Li,Yu Cao,Jie Zuo,Xinxing Chen,Jian Huang*

Main category: cs.RO

TL;DR: Hybrid impedance control with variable impedance and a stability-guaranteed network for a loosely coupled floating-base SRL; validated by simulations/experiments, improving safety and adaptability in human–robot interaction.


<details>
  <summary>Details</summary>
Motivation: Safety of force control in floating-base, loosely coupled robotic systems (SRL) under strong internal disturbances and unknown external disturbances; need adaptable impedance to accommodate gait variations and robust interaction.

Method: 1) Develops a dynamic model for the loosely coupled SRL. 2) Proposes a hybrid position/force impedance controller. 3) Introduces an efficient variable impedance control (VIC) to adapt impedance in real time. 4) Designs a real-time stability-guaranteed impedance parameter network for shock mitigation and rigidity support. 5) Validates via simulations and physical experiments demonstrating smooth transitions in flexible states and strong support in rigid states.

Result: The approach achieves smooth signal transitions in flexible states while delivering strong, stable support in rigid states. It enhances human–robot interaction by adapting to external disturbances and unknown environments, and demonstrates robustness across different gait variations.

Conclusion: The proposed VIC-based impedance framework with a stability-guaranteed parameter network provides a practical, safe, and adaptable solution for force-controlled SRL systems, with potential applicability to broader human–robot systems facing disturbances and gait variability.

Abstract: In human-robot systems, ensuring safety during force control in the presence of both internal and external disturbances is crucial. As a typical loosely coupled floating-base robot system, the supernumerary robotic leg (SRL) system is particularly susceptible to strong internal disturbances. To address the challenge posed by floating base, we investigated the dynamics model of the loosely coupled SRL and designed a hybrid position/force impedance controller to fit dynamic torque input. An efficient variable impedance control (VIC) method is developed to enhance human-robot interaction, particularly in scenarios involving external force disturbances. By dynamically adjusting impedance parameters, VIC improves the dynamic switching between rigidity and flexibility, so that it can adapt to unknown environmental disturbances in different states. An efficient real-time stability guaranteed impedance parameters generating network is specifically designed for the proposed SRL, to achieve shock mitigation and high rigidity supporting. Simulations and experiments validate the system's effectiveness, demonstrating its ability to maintain smooth signal transitions in flexible states while providing strong support forces in rigid states. This approach provides a practical solution for accommodating individual gait variations in interaction, and significantly advances the safety and adaptability of human-robot systems.

</details>


### [397] [Innovative Design of Multi-functional Supernumerary Robotic Limbs with Ellipsoid Workspace Optimization](https://arxiv.org/abs/2511.12186)
*Jun Huo,Jian Huang,Jie Zuo,Bo Yang,Zhongzheng Fu,Xi Li,Samer Mohammed*

Main category: cs.RO

TL;DR: A multi-objective optimization design framework for universal supernumerary robotic limbs (SRLs) that aligns grasping and walking workspaces, evaluates sit-to-stand braced forces, and controls mass/inertia, using an ellipsoid-based workspace quantification and a multi-subpopulation firefly algorithm to efficiently converge to Pareto-optimal designs. Experimental validation on six healthy subjects and two hemiplegia patients shows improved grasp success and reduced muscular effort.


<details>
  <summary>Details</summary>
Motivation: Designing a general-purpose SRL is intrinsically challenging due to the divergent functional requirements of upper and lower limbs. There is a need for a unified theoretical framework and efficient, high-dimensional MOO methods that can account for workspace, force transmission, and physical constraints to enable rapid, robust design and deployment.

Method: Represent grasping and walking workspaces with an ellipsoid envelope to reduce quantification complexity; quantify sit-to-stand braced force for force transmission effectiveness; constrain mass and inertia to limit link length; employ a multi-subpopulation correction firefly algorithm with attractive/repulsive domains to navigate irregular Pareto fronts and converge to high-quality designs; fabricate redesigned prototypes and test with six healthy participants and two hemiplegia patients.

Result: Compared with pre-optimization, average grasp success rate improved by 7.2%. Muscle activity during walking and STS tasks decreased by 12.7% and 25.1% respectively.

Conclusion: The proposed design theory provides an efficient approach for designing multi-functional SRLs, achieving better performance and energy efficiency in practical experiments and potentially generalizing to broader SRL configurations.

Abstract: Supernumerary robotic limbs (SRLs) offer substantial potential in both the rehabilitation of hemiplegic patients and the enhancement of functional capabilities for healthy individuals. Designing a general-purpose SRL device is inherently challenging, particularly when developing a unified theoretical framework that meets the diverse functional requirements of both upper and lower limbs. In this paper, we propose a multi-objective optimization (MOO) design theory that integrates grasping workspace similarity, walking workspace similarity, braced force for sit-to-stand (STS) movements, and overall mass and inertia. A geometric vector quantification method is developed using an ellipsoid to represent the workspace, aiming to reduce computational complexity and address quantification challenges. The ellipsoid envelope transforms workspace points into ellipsoid attributes, providing a parametric description of the workspace. Furthermore, the STS static braced force assesses the effectiveness of force transmission. The overall mass and inertia restricts excessive link length. To facilitate rapid and stable convergence of the model to high-dimensional irregular Pareto fronts, we introduce a multi-subpopulation correction firefly algorithm. This algorithm incorporates a strategy involving attractive and repulsive domains to effectively handle the MOO task. The optimized solution is utilized to redesign the prototype for experimentation to meet specified requirements. Six healthy participants and two hemiplegia patients participated in real experiments. Compared to the pre-optimization results, the average grasp success rate improved by 7.2%, while the muscle activity during walking and STS tasks decreased by an average of 12.7% and 25.1%, respectively. The proposed design theory offers an efficient option for the design of multi-functional SRL mechanisms.

</details>


### [398] [Locally Optimal Solutions to Constraint Displacement Problems via Path-Obstacle Overlaps](https://arxiv.org/abs/2511.12203)
*Antony Thomas,Fulvio Mastrogiovanni,Marco Baglietto*

Main category: cs.RO

TL;DR: Two-stage framework for enabling robot motion by displacing environmental constraints: first plan a path through obstacles, then locally optimize obstacle displacements to render that path collision-free.


<details>
  <summary>Details</summary>
Motivation: To address planning under adaptable environments where obstacles can be moved, enabling feasible paths when direct navigation is blocked.

Method: Stage 1: compute a trajectory through obstacles while minimizing an objective function. Stage 2: locally displace the obstacles to make the computed trajectory feasible (collision-free). The process is demonstrated on two distinct classes of constraint displacement problems.

Result: The approach successfully yields locally optimal obstacle displacements that produce feasible robot trajectories across multiple example scenarios, illustrating the method's viability.

Conclusion: A unified two-stage approach effectively integrates trajectory planning with constraint (obstacle) displacement to obtain feasible motion, with demonstrated applicability across two problem classes.

Abstract: We present a unified approach for constraint displacement problems in which a robot finds a feasible path by displacing constraints or obstacles. To this end, we propose a two stage process that returns locally optimal obstacle displacements to enable a feasible path for the robot. The first stage proceeds by computing a trajectory through the obstacles while minimizing an appropriate objective function. In the second stage, these obstacles are displaced to make the computed robot trajectory feasible, that is, collision-free. Several examples are provided that successfully demonstrate our approach on two distinct classes of constraint displacement problems.

</details>


### [399] [SocialNav-Map: Dynamic Mapping with Human Trajectory Prediction for Zero-Shot Social Navigation](https://arxiv.org/abs/2511.12232)
*Lingfeng Zhang,Erjia Xiao,Xiaoshuai Hao,Haoxiang Fu,Zeying Gong,Long Chen,Xiaojun Liang,Renjing Xu,Hangjun Ye,Wenbo Ding*

Main category: cs.RO

TL;DR: Zero-shot social navigation with SocialNav-Map that fuses dynamic human trajectory prediction into an occupancy map, enabling safe navigation without environment-specific training and outperforming RL methods trained for thousands of hours.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning-based social navigation requires extensive training (thousands of GPU hours) and generalizes poorly to new environments, hindering real-world deployment.

Method: Transform the navigation goal into the map frame; build a dynamic occupancy map that includes predicted human trajectories. Use two prediction strategies—history-based and orientation-based—to forecast human motion and embed these predictions into the occupancy map for proactive collision avoidance.

Result: On Social-HM3D and Social-MP3D, SocialNav-Map outperforms state-of-the-art RL methods that require ~2396 GPU hours, reducing human collision rates by over 10% without any training in new environments.

Conclusion: Eliminates the need for environment-specific training, enabling safer, more generalizable social navigation in diverse real-world human behaviors; code is available online.

Abstract: Social navigation in densely populated dynamic environments poses a significant challenge for autonomous mobile robots, requiring advanced strategies for safe interaction. Existing reinforcement learning (RL)-based methods require over 2000+ hours of extensive training and often struggle to generalize to unfamiliar environments without additional fine-tuning, limiting their practical application in real-world scenarios. To address these limitations, we propose SocialNav-Map, a novel zero-shot social navigation framework that combines dynamic human trajectory prediction with occupancy mapping, enabling safe and efficient navigation without the need for environment-specific training. Specifically, SocialNav-Map first transforms the task goal position into the constructed map coordinate system. Subsequently, it creates a dynamic occupancy map that incorporates predicted human movements as dynamic obstacles. The framework employs two complementary methods for human trajectory prediction: history prediction and orientation prediction. By integrating these predicted trajectories into the occupancy map, the robot can proactively avoid potential collisions with humans while efficiently navigating to its destination. Extensive experiments on the Social-HM3D and Social-MP3D datasets demonstrate that SocialNav-Map significantly outperforms state-of-the-art (SOTA) RL-based methods, which require 2,396 GPU hours of training. Notably, it reduces human collision rates by over 10% without necessitating any training in novel environments. By eliminating the need for environment-specific training, SocialNav-Map achieves superior navigation performance, paving the way for the deployment of social navigation systems in real-world environments characterized by diverse human behaviors. The code is available at: https://github.com/linglingxiansen/SocialNav-Map.

</details>


### [400] [Intermittent Rendezvous Plans with Mixed Integer Linear Program for Large-Scale Multi-Robot Exploration](https://arxiv.org/abs/2511.12237)
*Alysson Ribeiro da Silva,Luiz Chaimowicz*

Main category: cs.RO

TL;DR: MILP-based rendezvous planning for multi-robot exploration under intermittent connectivity; RTUS-based policy to follow plans with unknown conditions; validated in Gazebo; open-source implementation available.


<details>
  <summary>Details</summary>
Motivation: Address communication constraints and environmental uncertainty in multi-robot exploration. Existing approaches either opportunistically optimize or require pre-planned trajectories/scheduling that assume full environment knowledge, limiting real-world deployment. The work extends intermittent communications with a practical, plan-following framework.

Method: Formulate a Mixed-Integer Linear Program to generate rendezvous plans under communication constraints. Introduce Rendezvous Tracking for Unknown Scenarios (RTUS) to follow the plan despite unknown conditions. Validate in large-scale Gazebo simulations and provide open-source implementations of both the MILP planner and the MRE-CCIC framework.

Result: The approach enables prompt plan following and efficient task accomplishment in simulated, large-scale environments, demonstrating practicality of planned rendezvous under intermittent connectivity.

Conclusion: MRE-CCIC, combining MILP rendezvous planning with RTUS, is a viable and practical framework for exploration with intermittent connectivity. The open-source release enhances reproducibility and applicability to real-world deployments.

Abstract: Multi-Robot Exploration (MRE) systems with communication constraints have proven efficient in accomplishing a variety of tasks, including search-and-rescue, stealth, and military operations. While some works focus on opportunistic approaches for efficiency, others concentrate on pre-planned trajectories or scheduling for increased interpretability. However, scheduling usually requires knowledge of the environment beforehand, which prevents its deployment in several domains due to related uncertainties (e.g., underwater exploration). In our previous work, we proposed an intermittent communications framework for MRE under communication constraints that uses scheduled rendezvous events to mitigate such limitations. However, the system was unable to generate optimal plans and had no mechanisms to follow the plan considering realistic trajectories, which is not suited for real-world deployments. In this work, we further investigate the problem by formulating the Multi-Robot Exploration with Communication Constraints and Intermittent Connectivity (MRE-CCIC) problem. We propose a Mixed-Integer Linear Program (MILP) formulation to generate rendezvous plans and a policy to follow them based on the Rendezvous Tracking for Unknown Scenarios (RTUS) mechanism. The RTUS is a simple rule to allow robots to follow the assigned plan, considering unknown conditions. Finally, we evaluated our method in a large-scale environment configured in Gazebo simulations. The results suggest that our method can follow the plan promptly and accomplish the task efficiently. We provide an open-source implementation of both the MILP plan generator and the large-scale MRE-CCIC.

</details>


### [401] [SAC-MoE: Reinforcement Learning with Mixture-of-Experts for Control of Hybrid Dynamical Systems with Uncertainty](https://arxiv.org/abs/2511.12361)
*Leroy D'Souza,Akash Karthikeyan,Yash Vardhan Pant,Sebastian Fischmeister*

Main category: cs.RO

TL;DR: SAC-MoE uses a mixture-of-experts actor within Soft Actor-Critic, guided by a learned router, to handle unobservable latent modes and unobservable event switches in hybrid dynamical systems. A curriculum-based training regime improves robustness and generalization to unseen modes, achieving strong zero-shot performance and offering interpretable routing.


<details>
  <summary>Details</summary>
Motivation: Hybrid dynamical systems combine continuous dynamics with discrete events; when latent parameters and switching events are unobservable, standard model-based control struggles and model-free RL has difficulty generalizing across abrupt mode changes.

Method: Replace the SAC actor with a Mixture-of-Experts (MoE) where a learned router selects among multiple experts. Train using a curriculum that emphasizes challenging data (unseen modes/switch locations) to improve generalization. Validate in hybrid autonomous racing and legged locomotion tasks.

Result: SAC-MoE outperforms baselines by up to 6x in zero-shot generalization to unseen environments. The curriculum approach yields consistent performance gains across evaluated policies. Qualitative analysis reveals the MoE router activates different experts for distinct latent modes, providing interpretability.

Conclusion: The combination of a MoE-based actor and curriculum-driven data collection robustifies policy learning for hybrid systems with latent mode uncertainty, enabling better generalization and offering insight through interpretable router activations.

Abstract: Hybrid dynamical systems result from the interaction of continuous-variable dynamics with discrete events and encompass various systems such as legged robots, vehicles and aircrafts. Challenges arise when the system's modes are characterized by unobservable (latent) parameters and the events that cause system dynamics to switch between different modes are also unobservable. Model-based control approaches typically do not account for such uncertainty in the hybrid dynamics, while standard model-free RL methods fail to account for abrupt mode switches, leading to poor generalization.
  To overcome this, we propose SAC-MoE which models the actor of the Soft Actor-Critic (SAC) framework as a Mixture-of-Experts (MoE) with a learned router that adaptively selects among learned experts. To further improve robustness, we develop a curriculum-based training algorithm to prioritize data collection in challenging settings, allowing better generalization to unseen modes and switching locations. Simulation studies in hybrid autonomous racing and legged locomotion tasks show that SAC-MoE outperforms baselines (up to 6x) in zero-shot generalization to unseen environments. Our curriculum strategy consistently improves performance across all evaluated policies. Qualitative analysis shows that the interpretable MoE router activates different experts for distinct latent modes.

</details>


### [402] [Multilaminate piezoelectric PVDF actuators to enhance performance of soft micro robots](https://arxiv.org/abs/2511.12380)
*Nicholas Gunter,Heiko Kabutz,Kaushik Jayaram*

Main category: cs.RO

TL;DR: Multilayer PVDF actuators with parallel voltage across layers deliver high deflection and force at relatively low voltage, filling the design gap between brittle PZT stacks and soft polymer actuators; demonstrated in a planar microrobot achieving fast locomotion.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between high-force brittle piezoelectric stacks and compliant soft actuators; aim for higher bandwidth and robustness in soft microrobots by using multilayer PVDF with distributed voltage.

Method: Fabricate multilayer PVDF actuators with parallel voltage across layers; vary layer thickness and number of layers; compare with first-principles modeling; characterize deflection, blocked force, bandwidth; integrate into planar microrobot using resonance for locomotion.

Result: Attains >3 mm free deflection, >20 mN blocked force, ≥500 Hz bandwidth at voltages as low as 150 V; robust locomotion in planar microrobot.

Conclusion: Shows that multilayer PVDF with parallel voltage distribution is a viable design space bridging stiff high-force and soft high-bandwidth actuators, enabling efficient soft microrobots with resonance-based locomotion.

Abstract: Multilayer piezoelectric polyvinylidene fluoride (PVDF) actuators are a promising approach to enhance performance of soft microrobotic systems. In this work, we develop and characterize multilayer PVDF actuators with parallel voltage distribution across each layer, bridging a unique design space between brittle high-force PZT stacks and compliant but lower-bandwidth soft polymer actuators. We show the effects of layer thickness and number of layers in actuator performance and their agreement with a first principles model. By varying these parameters, we demonstrate actuators capable of >3 mm of free deflection, >20 mN of blocked force, and >=500 Hz, while operating at voltages as low as 150 volts. To illustrate their potential for robotic integration, we integrate our actuators into a planar, translating microrobot that leverages resonance to achieve locomotion with robustness to large perturbations.

</details>


### [403] [Evaluating Model-Agnostic Meta-Learning on MetaWorld ML10 Benchmark: Fast Adaptation in Robotic Manipulation Tasks](https://arxiv.org/abs/2511.12383)
*Sanjar Atamuradov*

Main category: cs.RO

TL;DR: MAML-TRPO evaluated on the MetaWorld ML10 benchmark shows promising one-shot adaptation across diverse robotic manipulation tasks but exhibits a generalization gap and high task-level variance, highlighting both potential and current limitations of gradient-based meta-learning for real-world robotics.


<details>
  <summary>Details</summary>
Motivation: Enable rapid, data-efficient adaptation to new robotic manipulation tasks in real-world settings where data collection is costly and time-consuming.

Method: Apply Model-Agnostic Meta-Learning (MAML) with Trust Region Policy Optimization (TRPO) to the ML10 robotic manipulation suite to learn a universal initialization that enables few-shot (one-gradient-step) adaptation across diverse skills (pushing, picking, drawer manipulation).

Result: One-shot adaptation yields clear improvements after a single gradient update, achieving final task success rates of 21.0% on training tasks and 13.2% on held-out test tasks. A generalization gap arises during meta-training as test task performance plateaus while training task performance improves. Task-level success rates vary widely (0% to 80%) across manipulation skills, indicating high variance in adaptation effectiveness.

Conclusion: Gradient-based meta-learning for diverse robotic manipulation is promising but limited by generalization gaps and high inter-task variability. Future work should consider task-aware adaptation and more structured policy architectures to improve consistency across skills.

Abstract: Meta-learning algorithms enable rapid adaptation to new tasks with minimal data, a critical capability for real-world robotic systems. This paper evaluates Model-Agnostic Meta-Learning (MAML) combined with Trust Region Policy Optimization (TRPO) on the MetaWorld ML10 benchmark, a challenging suite of ten diverse robotic manipulation tasks. We implement and analyze MAML-TRPO's ability to learn a universal initialization that facilitates few-shot adaptation across semantically different manipulation behaviors including pushing, picking, and drawer manipulation. Our experiments demonstrate that MAML achieves effective one-shot adaptation with clear performance improvements after a single gradient update, reaching final success rates of 21.0% on training tasks and 13.2% on held-out test tasks. However, we observe a generalization gap that emerges during meta-training, where performance on test tasks plateaus while training task performance continues to improve. Task-level analysis reveals high variance in adaptation effectiveness, with success rates ranging from 0% to 80% across different manipulation skills. These findings highlight both the promise and current limitations of gradient-based meta-learning for diverse robotic manipulation, and suggest directions for future work in task-aware adaptation and structured policy architectures.

</details>


### [404] [Learning Adaptive Neural Teleoperation for Humanoid Robots: From Inverse Kinematics to End-to-End Control](https://arxiv.org/abs/2511.12390)
*Sanjar Atamuradov*

Main category: cs.RO

TL;DR: A RL-based neural teleoperation framework maps VR controller inputs directly to humanoid robot joints, trained with IK-demo initialization and domain randomization to outperform IK+PD baselines in tracking, smoothness, and force adaptation at 50 Hz.


<details>
  <summary>Details</summary>
Motivation: IK+PD pipelines struggle with external disturbances, user variability, and producing natural motion under dynamic conditions; a learning-based approach could improve robustness and naturalness.

Method: Policy learns to map VR controller inputs to robot joint commands. Initialization from IK-based teleoperation demonstrations; trained in simulation with force disturbances and trajectory-smoothing rewards; fine-tuned via force randomization and smoothness objectives; evaluated on Unitree G1 with tasks like pick-and-place, door opening, and bimanual coordination; runs at 50 Hz.

Result: 34% lower tracking error; 45% smoother motions; superior force adaptation compared to IK baseline; real-time performance maintained.

Conclusion: Learning-based teleoperation can significantly enhance naturalness and robustness of humanoid control under disturbances, with strong potential for broader manipulation tasks.

Abstract: Virtual reality (VR) teleoperation has emerged as a promising approach for controlling humanoid robots in complex manipulation tasks. However, traditional teleoperation systems rely on inverse kinematics (IK) solvers and hand-tuned PD controllers, which struggle to handle external forces, adapt to different users, and produce natural motions under dynamic conditions. In this work, we propose a learning-based neural teleoperation framework that replaces the conventional IK+PD pipeline with learned policies trained via reinforcement learning. Our approach learns to directly map VR controller inputs to robot joint commands while implicitly handling force disturbances, producing smooth trajectories, and adapting to user preferences. We train our policies in simulation using demonstrations collected from IK-based teleoperation as initialization, then fine-tune them with force randomization and trajectory smoothness rewards. Experiments on the Unitree G1 humanoid robot demonstrate that our learned policies achieve 34% lower tracking error, 45% smoother motions, and superior force adaptation compared to the IK baseline, while maintaining real-time performance (50Hz control frequency). We validate our approach on manipulation tasks including object pick-and-place, door opening, and bimanual coordination. These results suggest that learning-based approaches can significantly improve the naturalness and robustness of humanoid teleoperation systems.

</details>


### [405] [RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation](https://arxiv.org/abs/2511.12436)
*Xiaoshuai Hao,Yingbo Tang,Lingfeng Zhang,Yanbiao Ma,Yunfeng Diao,Ziyu Jia,Wenbo Ding,Hangjun Ye,Long Chen*

Main category: cs.RO

TL;DR: Introduces RoboAfford++, a large multimodal dataset with 869,987 images and 2.0M QA annotations for three affordance-related tasks in manipulation and navigation, plus RoboAfford-Eval benchmark; demonstrates that fine-tuning VLMs on this data improves affordance reasoning.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models lack fine-grained, actionable cues for object and spatial affordances due to datasets lacking detailed affordance annotations, hindering manipulation and navigation performance.

Method: Build RoboAfford++: a multimodal dataset of 869,987 images with 2.0M QA annotations addressing object affordance recognition, object affordance prediction, and spatial affordance localization. Create RoboAfford-Eval with 338 annotated samples for evaluation across the same tasks. Evaluate existing VLMs and show gains from fine-tuning on RoboAfford++.

Result: Existing VLMs underperform on affordance tasks; fine-tuning on RoboAfford++ substantially improves affordance reasoning for both object and spatial aspects, validating the dataset’s usefulness.

Conclusion: The RoboAfford++ dataset and RoboAfford-Eval benchmark fill a critical gap in affordance learning for robotics, enabling better integration of environmental understanding with actionable manipulation and navigation.

Abstract: Robotic manipulation and navigation are fundamental capabilities of embodied intelligence, enabling effective robot interactions with the physical world. Achieving these capabilities requires a cohesive understanding of the environment, including object recognition to localize target objects, object affordances to identify potential interaction areas and spatial affordances to discern optimal areas for both object placement and robot movement. While Vision-Language Models (VLMs) excel at high-level task planning and scene understanding, they often struggle to infer actionable positions for physical interaction, such as functional grasping points and permissible placement regions. This limitation stems from the lack of fine-grained annotations for object and spatial affordances in their training datasets. To tackle this challenge, we introduce RoboAfford++, a generative AI-enhanced dataset for multimodal affordance learning for both robotic manipulation and navigation. Our dataset comprises 869,987 images paired with 2.0 million question answering (QA) annotations, covering three critical tasks: object affordance recognition to identify target objects based on attributes and spatial relationships, object affordance prediction to pinpoint functional parts for manipulation, and spatial affordance localization to identify free space for object placement and robot navigation. Complementing this dataset, we propose RoboAfford-Eval, a comprehensive benchmark for assessing affordance-aware prediction in real-world scenarios, featuring 338 meticulously annotated samples across the same three tasks. Extensive experimental results reveal the deficiencies of existing VLMs in affordance learning, while fine-tuning on the RoboAfford++ dataset significantly enhances their ability to reason about object and spatial affordances, validating the dataset's effectiveness.

</details>


### [406] [ClutterNav: Gradient-Guided Search for Efficient 3D Clutter Removal with Learned Costmaps](https://arxiv.org/abs/2511.12479)
*Navin Sriram Ravie,Keerthi Vasan M,Bijo Sebastian*

Main category: cs.RO

TL;DR: ClutterNav: a continuous RL framework for selective removal in clutter using a removability critic and integrated gradients to minimize disturbances while exposing a target.


<details>
  <summary>Details</summary>
Motivation: Dense cluttered environments make target retrieval difficult and risky. Rule-based planning is costly and brittle; end-to-end RL lacks interpretability and generalization across conditions. A scalable, interpretable, real-time decision-maker is needed to identify which object to remove next while preserving stack stability.

Method: Formulate the task as continuous reinforcement learning. Introduce a removability critic trained from demonstrations to estimate the cost of removing any object using geometric and spatial features. Use integrated gradients to quantify how surrounding objects affect target accessibility. Dynamically prioritize actions that balance immediate removability with long-term exposure, enabling real-time, occlusion-aware decisions.

Result: The approach yields near human-like strategic sequencing and real-time, occlusion-aware decision-making in partially observable environments, validated in both simulation and real-world experiments.

Conclusion: ClutterNav provides an interpretable and efficient decision-making framework that overcomes heuristic reliance and opaque RL, enabling robust clutter removal strategies for target access with minimal disturbances.

Abstract: Dense clutter removal for target object retrieval presents a challenging problem, especially when targets are embedded deep within densely-packed configurations. It requires foresight to minimize overall changes to the clutter configuration while accessing target objects, avoiding stack destabilization and reducing the number of object removals required. Rule-based planners when applied to this problem, rely on rigid heuristics, leading to high computational overhead. End-to-end reinforcement learning approaches struggle with interpretability and generalizability over different conditions. To address these issues, we present ClutterNav, a novel decision-making framework that can identify the next best object to be removed so as to access a target object in a given clutter, while minimising stack disturbances. ClutterNav formulates the problem as a continuous reinforcement learning task, where each object removal dynamically updates the understanding of the scene. A removability critic, trained from demonstrations, estimates the cost of removing any given object based on geometric and spatial features. This learned cost is complemented by integrated gradients that assess how the presence or removal of surrounding objects influences the accessibility of the target. By dynamically prioritizing actions that balance immediate removability against long-term target exposure, ClutterNav achieves near human-like strategic sequencing, without predefined heuristics. The proposed approach is validated extensively in simulation and over real-world experiments. The results demonstrate real-time, occlusion-aware decision-making in partially observable environments.

</details>


### [407] [Botany Meets Robotics in Alpine Scree Monitoring](https://arxiv.org/abs/2511.12526)
*Davide De Benedittis,Giovanni Di Lorenzo,Franco Angelini,Barbara Valle,Marina Serena Borgatti,Paolo Remagnino,Marco Caccianiga,Manolo Garabini*

Main category: cs.RO

TL;DR: Legged-robot-assisted scree habitat monitoring using ANYmal C with deep learning for plant species detection, improving monitoring frequency, efficiency, and data quality when paired with botanists.


<details>
  <summary>Details</summary>
Motivation: Biodiversity loss and environmental degradation, particularly in high-altitude scree habitats, require effective monitoring. The EU Habitat Directive mandates monitoring; traditional methods are resource-intensive and hazardous, necessitating safer, more efficient approaches.

Method: Deploy the ANYmal C legged robot in the Italian Alpine bio-region over two field campaigns spanning two years, using deep learning to detect and classify key plant species; combine robotics-assisted data collection with traditional phytosociological surveys by botanists.

Result: Robots navigated challenging terrain, increasing the frequency and efficiency of scree monitoring; enhanced data acquisition, storage, and usage; demonstrated synergy between autonomous robots and botanists, advancing environmental data collection.

Conclusion: Robotics can augment habitat monitoring under environmental directives, enabling more comprehensive and sustainable biodiversity assessments and potentially reducing field risk and resource demands.

Abstract: According to the European Union's Habitat Directive, habitat monitoring plays a critical role in response to the escalating problems posed by biodiversity loss and environmental degradation. Scree habitats, hosting unique and often endangered species, face severe threats from climate change due to their high-altitude nature. Traditionally, their monitoring has required highly skilled scientists to conduct extensive fieldwork in remote, potentially hazardous locations, making the process resource-intensive and time-consuming. This paper presents a novel approach for scree habitat monitoring using a legged robot to assist botanists in data collection and species identification. Specifically, we deployed the ANYmal C robot in the Italian Alpine bio-region in two field campaigns spanning two years and leveraged deep learning to detect and classify key plant species of interest. Our results demonstrate that agile legged robots can navigate challenging terrains and increase the frequency and efficiency of scree monitoring. When paired with traditional phytosociological surveys performed by botanists, this robotics-assisted protocol not only streamlines field operations but also enhances data acquisition, storage, and usage. The outcomes of this research contribute to the evolving landscape of robotics in environmental science, paving the way for a more comprehensive and sustainable approach to habitat monitoring and preservation.

</details>


### [408] [EcoFlight: Finding Low-Energy Paths Through Obstacles for Autonomous Sensing Drones](https://arxiv.org/abs/2511.12618)
*Jordan Leyva,Nahim J. Moran Vera,Yihan Xu,Adrien Durasno,Christopher U. Romero,Tendai Chimuka,Gabriel O. Huezo Ramirez,Ziqian Dong,Roberto Rojas-Cessa*

Main category: cs.RO

TL;DR: EcoFlight finds energy-minimizing 3D routes around obstacles, outperforming direct and shortest-distance plans, especially in dense environments; speed choice can boost energy savings.


<details>
  <summary>Details</summary>
Motivation: Obstacle avoidance in UAV path planning is common but energetically costly; existing schemes often optimize for distance rather than energy, leading to inefficient flights. An energy-aware planner is needed for efficient point-to-point UAV missions in cluttered airspace.

Method: Develop EcoFlight by modeling UAV energy consumption from propulsion and flight dynamics and integrating this model into a 3D obstacle-aware pathfinding framework to minimize energy use. Evaluate via simulations across varying obstacle densities and compare against direct-flight and shortest-distance baselines.

Result: EcoFlight consistently achieved lower energy consumption than both baseline schemes, with larger savings in high obstacle density scenarios. Additionally, allowing a suitable flight speed further enhanced energy efficiency.

Conclusion: An energy-aware 3D obstacle-avoidance planner (EcoFlight) effectively reduces energy use for UAV flights in cluttered environments, and speed optimization can compound these gains.

Abstract: Obstacle avoidance path planning for uncrewed aerial vehicles (UAVs), or drones, is rarely addressed in most flight path planning schemes, despite obstacles being a realistic condition. Obstacle avoidance can also be energy-intensive, making it a critical factor in efficient point-to-point drone flights. To address these gaps, we propose EcoFlight, an energy-efficient pathfinding algorithm that determines the lowest-energy route in 3D space with obstacles. The algorithm models energy consumption based on the drone propulsion system and flight dynamics. We conduct extensive evaluations, comparing EcoFlight with direct-flight and shortest-distance schemes. The simulation results across various obstacle densities show that EcoFlight consistently finds paths with lower energy consumption than comparable algorithms, particularly in high-density environments. We also demonstrate that a suitable flying speed can further enhance energy savings.

</details>


### [409] [Task-Aware Morphology Optimization of Planar Manipulators via Reinforcement Learning](https://arxiv.org/abs/2511.12650)
*Arvind Kumar Mishra,Sohom Chakrabarty*

Main category: cs.RO

TL;DR: RL can recover morphology optima without analytic expressions, re-discovers the 2R optimum and extends to non-analytic paths with greater efficiency than grid/black-box methods.


<details>
  <summary>Details</summary>
Motivation: To test whether reinforcement learning can optimize robot morphology using Yoshikawa's manipulability index in planar manipulators, without access to Jacobians or analytic gradients, and to assess scalability to higher-dimensional morphology optimization.

Method: Evaluate three RL algorithms (SAC, DDPG, PPO) on a 2R planar arm for a circular end-effector path with morphology defined by a single parameter phi (link lengths). Compare to grid search and black-box optimizers. Validate against known optimum (equal links, theta2=90°). Then extend to elliptical/rectangular paths by using full morphology vector (L1, L2, theta2).

Result: All methods converge to the analytical optimum for circle; RL learns from reward only. In non-analytical settings, RL converges reliably, while grid/black-box require much larger evaluation budgets.

Conclusion: RL is a scalable, effective approach for morphology optimization, capable of both recovering known optima and solving problems without closed-form solutions, reducing reliance on analytical models.

Abstract: In this work, Yoshikawa's manipulability index is used to investigate reinforcement learning (RL) as a framework for morphology optimization in planar robotic manipulators. A 2R manipulator tracking a circular end-effector path is first examined because this case has a known analytical optimum: equal link lengths and the second joint orthogonal to the first. This serves as a validation step to test whether RL can rediscover the optimum using reward feedback alone, without access to the manipulability expression or the Jacobian. Three RL algorithms (SAC, DDPG, and PPO) are compared with grid search and black-box optimizers, with morphology represented by a single action parameter phi that maps to the link lengths. All methods converge to the analytical solution, showing that numerical recovery of the optimum is possible without supplying analytical structure.
  Most morphology design tasks have no closed-form solutions, and grid or heuristic search becomes expensive as dimensionality increases. RL is therefore explored as a scalable alternative. The formulation used for the circular path is extended to elliptical and rectangular paths by expanding the action space to the full morphology vector (L1, L2, theta2). In these non-analytical settings, RL continues to converge reliably, whereas grid and black-box methods require far larger evaluation budgets. These results indicate that RL is effective for both recovering known optima and solving morphology optimization problems without analytical solutions.

</details>


### [410] [Prompt-Driven Domain Adaptation for End-to-End Autonomous Driving via In-Context RL](https://arxiv.org/abs/2511.12755)
*Aleesha Khurram,Amir Moeini,Shangtong Zhang,Rohan Chandra*

Main category: cs.RO

TL;DR: Proposes inference-time few-shot prompt-driven domain adaptation for closed-loop autonomous driving using in-context reinforcement learning (ICRL); no model updates or data collection; demonstrates improved safety, efficiency, and comfort under adverse weather in CARLA compared to baselines.


<details>
  <summary>Details</summary>
Motivation: Domain adaptation for autonomous driving is challenging and existing strategies (data collection, retraining) scale poorly with increased complexity. Prompt-driven approaches in LLMs/VLMs are limited to perception tasks and require expert few-shot data, motivating a model-free, inference-time solution for full closed-loop control.

Method: Introduce in-context reinforcement learning (ICRL) at inference time that augments prompts with observed state–action trajectories, enabling closed-loop driving in adverse weather without updating model parameters or collecting new data. Extend to general trajectories observed during inference and validate with CARLA simulations against state-of-the-art prompt-driven DA baselines.

Result: ICRL yields safer, more efficient, and more comfortable driving policies in the target adverse-weather domain, outperforming state-of-the-art prompt-driven DA baselines in CARLA experiments.

Conclusion: This work advances prompt-driven DA by enabling closed-loop autonomous driving with general inference-time trajectories, avoiding parameter updates and data collection, and demonstrating practical improvements in adverse-weather scenarios.

Abstract: Despite significant progress and advances in autonomous driving, many end-to-end systems still struggle with domain adaptation (DA), such as transferring a policy trained under clear weather to adverse weather conditions. Typical DA strategies in the literature include collecting additional data in the target domain or re-training the model, or both. Both these strategies quickly become impractical as we increase scale and complexity of driving. These limitations have encouraged investigation into few-shot and zero-shot prompt-driven DA at inference time involving LLMs and VLMs. These methods work by adding a few state-action trajectories during inference to the prompt (similar to in-context learning). However, there are two limitations of such an approach: $(i)$ prompt-driven DA methods are currently restricted to perception tasks such as detection and segmentation and $(ii)$ they require expert few-shot data. In this work, we present a new approach to inference-time few-shot prompt-driven DA for closed-loop autonomous driving in adverse weather condition using in-context reinforcement learning (ICRL). Similar to other prompt-driven DA methods, our approach does not require any updates to the model parameters nor does it require additional data collection in adversarial weather regime. Furthermore, our approach advances the state-of-the-art in prompt-driven DA by extending to closed driving using general trajectories observed during inference. Our experiments using the CARLA simulator show that ICRL results in safer, more efficient, and more comfortable driving policies in the target domain compared to state-of-the-art prompt-driven DA baselines.

</details>


### [411] [DR. Nav: Semantic-Geometric Representations for Proactive Dead-End Recovery and Navigation](https://arxiv.org/abs/2511.12778)
*Vignesh Rajagopal,Kasun Weerakoon Kulathun Mudiyanselage,Gershom Devake Seneviratne,Pon Aswin Sankaralingam,Mohamed Elnoor,Jing Liang,Rohan Chandra,Dinesh Manocha*

Main category: cs.RO

TL;DR: DR. Nav delivers a recovery-aware navigation framework that unifies dead-end prediction and recovery in unmapped, unstructured environments by building a continuous semantic cost map via RGB-LiDAR fusion and Bayesian updates, aiming to improve detection accuracy and path efficiency over common planners.


<details>
  <summary>Details</summary>
Motivation: Autonomous robots face dead-ends and unsafe regions in unknown, unstructured environments due to occlusions, clutter, and blocked junctions. Existing maps focus on traversability but do not explicitly encode recovery risk, leading to potential unsafe plans.

Method: Generate a single, continuous real-time semantic cost map that encodes per-cell dead-end likelihoods and recovery points. Use cross-modal RGB-LiDAR fusion with attention-based filtering to estimate these metrics, updated online via Bayesian inference. The approach unifies dead-end prediction with recovery planning to influence navigation decisions.

Result: Reported improvements include 83.33% higher dead-end detection accuracy and 52.4% shorter time-to-goal (path efficiency) relative to state-of-the-art planners such as DWA, MPPI, and Nav2 DWB, evaluated across dense indoor and outdoor scenarios. The abstract also mentions a dead-end classifier, suggesting explicit detection enables recovery-aware planning.

Conclusion: DR. Nav demonstrates that explicitly modeling recovery risk within a unified cost map can enhance safety and efficiency in challenging, unmapped environments by proactively identifying dead-ends and guiding safer alternative trajectories.

Abstract: We present DR. Nav (Dead-End Recovery-aware Navigation), a novel approach to autonomous navigation in scenarios where dead-end detection and recovery are critical, particularly in unstructured environments where robots must handle corners, vegetation occlusions, and blocked junctions. DR. Nav introduces a proactive strategy for navigation in unmapped environments without prior assumptions. Our method unifies dead-end prediction and recovery by generating a single, continuous, real-time semantic cost map. Specifically, DR. Nav leverages cross-modal RGB-LiDAR fusion with attention-based filtering to estimate per-cell dead-end likelihoods and recovery points, which are continuously updated through Bayesian inference to enhance robustness. Unlike prior mapping methods that only encode traversability, DR. Nav explicitly incorporates recovery-aware risk into the navigation cost map, enabling robots to anticipate unsafe regions and plan safer alternative trajectories. We evaluate DR. Nav across multiple dense indoor and outdoor scenarios and demonstrate an increase of 83.33% in accuracy in detection, a 52.4% reduction in time-to-goal (path efficiency), compared to state-of-the-art planners such as DWA, MPPI, and Nav2 DWB. Furthermore, the dead-end classifier functions

</details>


### [412] [ActiveGrasp: Information-Guided Active Grasping with Calibrated Energy-based Model](https://arxiv.org/abs/2511.12795)
*Boshu Lei,Wen Jiang,Kostas Daniilidis*

Main category: cs.RO

TL;DR: A calibrated energy-based SE(3) grasp model with active view selection using information gain enables robust grasping in dense clutter under limited view budgets, outperforming prior methods and supported by reproducible simulations.


<details>
  <summary>Details</summary>
Motivation: Grasping in densely cluttered environments is difficult because previous methods either rely on gathering many views without respecting the grasp distribution on SE(3) or reduce distributions to projections that ignore SE(3) structure, leading to suboptimal information gain estimates. A calibrated, multi-modal SE(3) grasp model paired with information-driven view planning can better capture pose-space structure and guide exploration.

Method: Develop a calibrated energy-based model for grasp pose generation on the SE(3) manifold to capture multi-modality; calibrate energy so that predicted grasp distributions align with observed success rates. Propose an active view selection strategy that estimates information gain from the calibrated grasp distribution conditioned on a reconstructed environment, selecting the next best view to efficiently explore affordable parts of the target object.

Result: Experiments in simulation and on real robots show successful grasps in clutter with limited view budgets, outperforming state-of-the-art methods. The authors provide a reproducible simulated environment for future work, and plan to release the source code upon paper publication.

Conclusion: The proposed approach integrates a calibrated, multi-modal SE(3) grasp distribution with information-driven view planning, enabling efficient active grasping in clutter and offering a reproducible platform for future research.

Abstract: Grasping in a densely cluttered environment is a challenging task for robots. Previous methods tried to solve this problem by actively gathering multiple views before grasp pose generation. However, they either overlooked the importance of the grasp distribution for information gain estimation or relied on the projection of the grasp distribution, which ignores the structure of grasp poses on the SE(3) manifold. To tackle these challenges, we propose a calibrated energy-based model for grasp pose generation and an active view selection method that estimates information gain from grasp distribution. Our energy-based model captures the multi-modality nature of grasp distribution on the SE(3) manifold. The energy level is calibrated to the success rate of grasps so that the predicted distribution aligns with the real distribution. The next best view is selected by estimating the information gain for grasp from the calibrated distribution conditioned on the reconstructed environment, which could efficiently drive the robot to explore affordable parts of the target object. Experiments on simulated environments and real robot setups demonstrate that our model could successfully grasp objects in a cluttered environment with limited view budgets compared to previous state-of-the-art models. Our simulated environment can serve as a reproducible platform for future research on active grasping. The source code of our paper will be made public when the paper is released to the public.

</details>


### [413] [Structured Imitation Learning of Interactive Policies through Inverse Games](https://arxiv.org/abs/2511.12848)
*Max M. Sun,Todd Murphey*

Main category: cs.RO

TL;DR: Two-step structured imitation learning for interactive multi-agent policies: learn single-agent behaviors with standard imitation learning, then infer inter-agent dependencies via inverse game theory; shows improved performance in a 5-agent social navigation task with 50 demos.


<details>
  <summary>Details</summary>
Motivation: Imitation learning for interactive, human-in-the-loop tasks is difficult due to higher behavioral complexity in multi-agent interactions; need to separate learning of individual policies from inter-agent coordination structure; leverage game-theoretic modeling to capture dependencies.

Method: 1) learn individual behavior from multi-agent demonstrations using standard imitation learning; 2) structurally learn inter-agent dependencies by solving an inverse game problem, effectively combining generative single-agent policy learning with a flexible game-theoretic dependency model.

Result: Preliminary synthetic 5-agent social navigation experiments show significant improvement over non-interactive policies and performance comparable to ground-truth interactive policy using only 50 demonstrations.

Conclusion: Structured imitation learning offers a viable path for interactive policies by decoupling single-agent learning from inter-agent structure, enabling efficient learning in multi-agent settings.

Abstract: Generative model-based imitation learning methods have recently achieved strong results in learning high-complexity motor skills from human demonstrations. However, imitation learning of interactive policies that coordinate with humans in shared spaces without explicit communication remains challenging, due to the significantly higher behavioral complexity in multi-agent interactions compared to non-interactive tasks. In this work, we introduce a structured imitation learning framework for interactive policies by combining generative single-agent policy learning with a flexible yet expressive game-theoretic structure. Our method explicitly separates learning into two steps: first, we learn individual behavioral patterns from multi-agent demonstrations using standard imitation learning; then, we structurally learn inter-agent dependencies by solving an inverse game problem. Preliminary results in a synthetic 5-agent social navigation task show that our method significantly improves non-interactive policies and performs comparably to the ground truth interactive policy using only 50 demonstrations. These results highlight the potential of structured imitation learning in interactive settings.

</details>


### [414] [Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos](https://arxiv.org/abs/2511.12882)
*Taiyi Su,Jian Zhu,Yaxuan Li,Chong Ma,Zitai Huang,Yichen Zhu,Hanli Wang,Yi Xu*

Main category: cs.RO

TL;DR: MTV-World introduces multi-view trajectory-video control for embodied world models, replacing direct low-level actions with trajectory videos derived from camera parameters to improve visuomotor precision; uses a multi-view framework to mitigate 3D-to-2D information loss and an auto-evaluation pipeline with multimodal models; shows precise control and accurate interaction in dual-arm tasks.


<details>
  <summary>Details</summary>
Motivation: Current embodied world models struggle to translate low-level actions into accurate predicted frames, due to loss of spatial information when projecting 3D actions to 2D and insufficient single-view context, leading to inconsistent physical interactions.

Method: Propose MTV-World: use trajectory videos obtained from camera intrinsics/extrinsics and Cartesian-space transformations as control signals instead of direct low-level actions. Employ a multi-view framework to compensate for spatial information loss; forecast future frames conditioned on an initial frame per view. Develop an auto-evaluation pipeline leveraging multimodal large models and referring video object segmentation; measure spatial consistency via object location matching using the Jaccard Index.

Result: Extensive experiments show MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios, with improved spatial consistency due to multi-view inputs.

Conclusion: MTV-World provides a robust approach for precise visuomotor prediction by using multi-view trajectory videos as control signals, mitigating 3D-to-2D information loss, and offering an auto-evaluation pipeline to rigorously assess robotic motion and object interaction accuracy.

Abstract: Embodied world models aim to predict and interact with the physical world through visual observations and actions. However, existing models struggle to accurately translate low-level actions (e.g., joint positions) into precise robotic movements in predicted frames, leading to inconsistencies with real-world physical interactions. To address these limitations, we propose MTV-World, an embodied world model that introduces Multi-view Trajectory-Video control for precise visuomotor prediction. Specifically, instead of directly using low-level actions for control, we employ trajectory videos obtained through camera intrinsic and extrinsic parameters and Cartesian-space transformation as control signals. However, projecting 3D raw actions onto 2D images inevitably causes a loss of spatial information, making a single view insufficient for accurate interaction modeling. To overcome this, we introduce a multi-view framework that compensates for spatial information loss and ensures high-consistency with physical world. MTV-World forecasts future frames based on multi-view trajectory videos as input and conditioning on an initial frame per view. Furthermore, to systematically evaluate both robotic motion precision and object interaction accuracy, we develop an auto-evaluation pipeline leveraging multimodal large models and referring video object segmentation models. To measure spatial consistency, we formulate it as an object location matching problem and adopt the Jaccard Index as the evaluation metric. Extensive experiments demonstrate that MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios.

</details>


### [415] [Air-Chamber Based Soft Six-Axis Force/Torque Sensor for Human-Robot Interaction](https://arxiv.org/abs/2511.12896)
*Jun Huo,Hongge Ru,Bo Yang,Xingjian Chen,Xi Li,Jian Huang*

Main category: cs.RO

TL;DR: A soft air-chamber six-axis F/T sensor with 16 barometers and a rigid-soft decoupling strategy; validated by FE and experiments, achieving 50 N/1 Nm range with moderate error while preserving softness.


<details>
  <summary>Details</summary>
Motivation: Accurate six-degree-of-freedom force sensing is essential for safe, precise interactions, but cross-axis coupling causes calibration challenges in conventional sensors.

Method: A hyper-elastic silicone rubber air-chamber sensor housing 16 barometers; a rigid-soft hierarchical decoupling converts six-axis decoupling into two separate three-axis decouplings; FE simulations and experiments validate performance.

Result: Prototype range 50 N and 1 Nm; average deviation 4.9%; repeatability 2.7%; non-linearity 5.8%; hysteresis 6.7%.

Conclusion: The proposed sensor offers satisfactory sensing performance while maintaining softness due to soft air chambers, and the decoupling approach effectively addresses cross-axis coupling.

Abstract: Soft multi-axis force/torque sensors provide safe and precise force interaction. Capturing the complete degree-of-freedom of force is imperative for accurate force measurement with six-axis force/torque sensors. However, cross-axis coupling can lead to calibration issues and decreased accuracy. In this instance, developing a soft and accurate six-axis sensor is a challenging task. In this paper, a soft air-chamber type six-axis force/torque sensor with 16-channel barometers is introduced, which housed in hyper-elastic air chambers made of silicone rubber. Additionally, an effective decoupling method is proposed, based on a rigid-soft hierarchical structure, which reduces the six-axis decoupling problem to two three-axis decoupling problems. Finite element model simulation and experiments demonstrate the compatibility of the proposed approach with reality. The prototype's sensing performance is quantitatively measured in terms of static load response, dynamic load response and dynamic response characteristic. It possesses a measuring range of 50 N force and 1 Nm torque, and the average deviation, repeatability, non-linearity and hysteresis are 4.9$\%$, 2.7$\%$, 5.8$\%$ and 6.7$\%$, respectively. The results indicate that the prototype exhibits satisfactory sensing performance while maintaining its softness due to the presence of soft air chambers.

</details>


### [416] [TOPP-DWR: Time-Optimal Path Parameterization of Differential-Driven Wheeled Robots Considering Piecewise-Constant Angular Velocity Constraints](https://arxiv.org/abs/2511.12910)
*Yong Li,Yujun Huang,Yi Chen,Hui Cheng*

Main category: cs.RO

TL;DR: A practical TOPP algorithm for differential-driven wheeled robots that enforces angular and joint velocity constraints by representing trajectories with non-uniform B-splines and solving an SOCP; validated in simulations and field tests.


<details>
  <summary>Details</summary>
Motivation: Existing TOPP methods often ignore angular velocity and joint velocity constraints, causing suboptimal performance in real robots; a unified, efficient framework is needed.

Method: Represent the initial trajectory in task space with a non-uniform B-spline; incorporate piecewise-constant angular velocity, joint velocity, linear velocity, and linear acceleration constraints as linear velocity constraints; introduce a slack variable to reformulate the problem as a second-order-cone program (SOCP) for computational efficiency.

Result: Comparative experiments show TOPP-DWR achieves true time-optimal path parameterization while satisfying all constraints; field autonomous navigation experiments validate practicality in real-world scenarios.

Conclusion: TOPP-DWR provides a systematic, practical TOPP framework for differential-driven wheeled robots and other mobile robots, improving control performance and real-world applicability by accounting angular and joint constraints and enabling efficient computation.

Abstract: Differential-driven wheeled robots (DWR) represent the quintessential type of mobile robots and find extensive appli- cations across the robotic field. Most high-performance control approaches for DWR explicitly utilize the linear and angular velocities of the trajectory as control references. However, existing research on time-optimal path parameterization (TOPP) for mobile robots usually neglects the angular velocity and joint vel- ocity constraints, which can result in degraded control perfor- mance in practical applications. In this article, a systematic and practical TOPP algorithm named TOPP-DWR is proposed for DWR and other mobile robots. First, the non-uniform B-spline is adopted to represent the initial trajectory in the task space. Second, the piecewise-constant angular velocity, as well as joint velocity, linear velocity, and linear acceleration constraints, are incorporated into the TOPP problem. During the construction of the optimization problem, the aforementioned constraints are uniformly represented as linear velocity constraints. To boost the numerical computational efficiency, we introduce a slack variable to reformulate the problem into second-order-cone programming (SOCP). Subsequently, comparative experiments are conducted to validate the superiority of the proposed method. Quantitative performance indexes show that TOPP-DWR achieves TOPP while adhering to all constraints. Finally, field autonomous navigation experiments are carried out to validate the practicability of TOPP-DWR in real-world applications.

</details>


### [417] [DiffuDepGrasp: Diffusion-based Depth Noise Modeling Empowers Sim2Real Robotic Grasping](https://arxiv.org/abs/2511.12912)
*Yingting Zhou,Wenbo Cui,Weiheng Liu,Guixing Chen,Haoran Li,Dongbin Zhao*

Main category: cs.RO

TL;DR: DiffuDepGrasp: a deploy-efficient sim2real framework that uses diffusion-based depth synthesis and noise grafting to enable zero-shot transfer for depth-based grasping, achieving high success with minimal deployment overhead.


<details>
  <summary>Details</summary>
Motivation: Real depth sensors exhibit artifacts (voids, noise) that create a sim2real gap; existing data-inefficient noise models and heavy foundation-model pipelines impact grasping tasks and deployment efficiency.

Method: Two-module approach trained in simulation-only: (1) Diffusion Depth Module trains a conditional diffusion model with temporal geometric priors to capture realistic sensor noise on pristine depth; (2) Noise Grafting Module injects perceptual artifacts while preserving metric accuracy; deployment uses raw depth inputs only, with no extra compute.

Result: Achieves 95.7% average success on 12-object grasping with zero-shot transfer and strong generalization to unseen objects; deploy-efficient with no computational overhead during deployment; remains effective across objects; project site provided.

Conclusion: DiffuDepGrasp addresses data inefficiency and deployment complexity for sim2real depth-based grasping by synthesizing realistic depth with diffusion and grafting, enabling zero-shot transfer and strong generalization while maintaining deployment efficiency.

Abstract: Transferring the depth-based end-to-end policy trained in simulation to physical robots can yield an efficient and robust grasping policy, yet sensor artifacts in real depth maps like voids and noise establish a significant sim2real gap that critically impedes policy transfer. Training-time strategies like procedural noise injection or learned mappings suffer from data inefficiency due to unrealistic noise simulation, which is often ineffective for grasping tasks that require fine manipulation or dependency on paired datasets heavily. Furthermore, leveraging foundation models to reduce the sim2real gap via intermediate representations fails to mitigate the domain shift fully and adds computational overhead during deployment. This work confronts dual challenges of data inefficiency and deployment complexity. We propose DiffuDepGrasp, a deploy-efficient sim2real framework enabling zero-shot transfer through simulation-exclusive policy training. Its core innovation, the Diffusion Depth Generator, synthesizes geometrically pristine simulation depth with learned sensor-realistic noise via two synergistic modules. The first Diffusion Depth Module leverages temporal geometric priors to enable sample-efficient training of a conditional diffusion model that captures complex sensor noise distributions, while the second Noise Grafting Module preserves metric accuracy during perceptual artifact injection. With only raw depth inputs during deployment, DiffuDepGrasp eliminates computational overhead and achieves a 95.7% average success rate on 12-object grasping with zero-shot transfer and strong generalization to unseen objects.Project website: https://diffudepgrasp.github.io/.

</details>


### [418] [GUIDE: Gaussian Unified Instance Detection for Enhanced Obstacle Perception in Autonomous Driving](https://arxiv.org/abs/2511.12941)
*Chunyong Hu,Qi Luo,Jianyun Xu,Song Wang,Qiang Li,Sheng Yang*

Main category: cs.RO

TL;DR: GUIDE introduces 3D Gaussian primitives for simultaneous instance detection and occupancy prediction, using Gaussian-to-Voxel splatting to create a sparse, instance-level occupancy representation that avoids dense voxel grids. It delivers a 50% improvement in instance occupancy mAP on nuScenes and competitive tracking.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D bounding boxes and dense occupancy grids struggle with irregular, real-world object shapes and high computational costs. A sparse, probabilistic representation that supports both detection and occupancy (and tracking) could improve accuracy and efficiency in autonomous perception.

Method: Each object is represented by 3D Gaussians; occupancy is predicted via Gaussian-to-Voxel splatting to generate a sparse voxel-based occupancy grid at the instance level; joint detection and occupancy prediction with robust tracking; evaluated on nuScenes with instance occupancy mAP as primary metric.

Result: On nuScenes, achieves instance occupancy mAP of 21.61, a 50% improvement over existing methods, with competitive tracking performance, establishing a new benchmark for perception in autonomous driving.

Conclusion: GUIDE demonstrates that a Gaussian-based, sparse occupancy representation can surpass dense voxel approaches in accuracy while maintaining efficiency, and simultaneously provides robust tracking for dynamic driving environments.

Abstract: In the realm of autonomous driving, accurately detecting surrounding obstacles is crucial for effective decision-making. Traditional methods primarily rely on 3D bounding boxes to represent these obstacles, which often fail to capture the complexity of irregularly shaped, real-world objects. To overcome these limitations, we present GUIDE, a novel framework that utilizes 3D Gaussians for instance detection and occupancy prediction. Unlike conventional occupancy prediction methods, GUIDE also offers robust tracking capabilities. Our framework employs a sparse representation strategy, using Gaussian-to-Voxel Splatting to provide fine-grained, instance-level occupancy data without the computational demands associated with dense voxel grids. Experimental validation on the nuScenes dataset demonstrates GUIDE's performance, with an instance occupancy mAP of 21.61, marking a 50\% improvement over existing methods, alongside competitive tracking capabilities. GUIDE establishes a new benchmark in autonomous perception systems, effectively combining precision with computational efficiency to better address the complexities of real-world driving environments.

</details>


### [419] [SplatSearch: Instance Image Goal Navigation for Mobile Robots using 3D Gaussian Splatting and Diffusion Models](https://arxiv.org/abs/2511.12972)
*Siddarth Narasimhan,Matthew Lisondra,Haitong Wang,Goldie Nejat*

Main category: cs.RO

TL;DR: SplatSearch enables robust Instance Image Goal Navigation in unknown environments by rendering multiple viewpoints from a sparse 3D Gaussian splatting map, completing with a multi-view diffusion model, and guiding exploration with a semantics-informed frontier policy; it outperforms SOTA in photorealistic and real-world tests.


<details>
  <summary>Details</summary>
Motivation: IIN in unknown environments is hard because reference goal images can come from arbitrary viewpoints and reconstructions are sparse. A system that can synthesize missing views and semantically align with the goal image is needed to robustly locate targets.

Method: Maintain a sparse online 3D Gaussian splatting (3DGS) map. Render multiple viewpoints around candidate objects, and use a multi-view diffusion model to complete missing regions in the rendered images. Perform robust feature matching against the goal image. Introduce a frontier exploration policy that scores frontiers using visual context from the synthesized viewpoints and semantic context from the goal image to prioritize relevance.

Result: SplatSearch achieves higher performance than current state-of-the-art methods in terms of Success Rate and Success Path Length, validated in photorealistic home environments and real-world settings. An ablation study confirms the contribution of each design choice.

Conclusion: Combining sparse-view 3DGS-based view synthesis, diffusion-based completion, and a semantically-informed frontier policy yields robust IIN performance in unknown environments with sparse maps, outperforming existing methods.

Abstract: The Instance Image Goal Navigation (IIN) problem requires mobile robots deployed in unknown environments to search for specific objects or people of interest using only a single reference goal image of the target. This problem can be especially challenging when: 1) the reference image is captured from an arbitrary viewpoint, and 2) the robot must operate with sparse-view scene reconstructions. In this paper, we address the IIN problem, by introducing SplatSearch, a novel architecture that leverages sparse-view 3D Gaussian Splatting (3DGS) reconstructions. SplatSearch renders multiple viewpoints around candidate objects using a sparse online 3DGS map, and uses a multi-view diffusion model to complete missing regions of the rendered images, enabling robust feature matching against the goal image. A novel frontier exploration policy is introduced which uses visual context from the synthesized viewpoints with semantic context from the goal image to evaluate frontier locations, allowing the robot to prioritize frontiers that are semantically and visually relevant to the goal image. Extensive experiments in photorealistic home and real-world environments validate the higher performance of SplatSearch against current state-of-the-art methods in terms of Success Rate and Success Path Length. An ablation study confirms the design choices of SplatSearch.

</details>


### [420] [CUTE-Planner: Confidence-aware Uneven Terrain Exploration Planner](https://arxiv.org/abs/2511.12984)
*Miryeong Park,Dongjin Cho,Sanghyun Kim,Younggun Cho*

Main category: cs.RO

TL;DR: A framework that fuses safe path planning, adaptive confidence updates, and confidence-aware exploration with Kalman-based elevation estimates and Graph-Based Planner (GBP); shows improved uncertainty reduction and mission success in simulated lunar tests.


<details>
  <summary>Details</summary>
Motivation: Uncertainty in elevation near complex terrain like craters degrades navigation safety and map quality; existing methods focus on traversability but ignore uncertainty-driven exploration and how elevation uncertainty affects safety.

Method: Kalman-based elevation estimation to compute terrain traversability and confidence; integration into GBP to prioritize exploration of traversable low-confidence regions; incorporating safe path generation and adaptive confidence updates; evaluated in simulated lunar experiments with a novel low-confidence region ratio metric.

Result: 69% reduction in elevation uncertainty relative to baseline GBP; mission success rate 100% vs 0% for baseline GBP.

Conclusion: Uncertainty-aware planning improves exploration safety and map reliability; the approach demonstrates the value of integrating confidence measures into planning and exploration; further validation needed in real missions.

Abstract: Planetary exploration robots must navigate uneven terrain while building reliable maps for space missions. However, most existing methods incorporate traversability constraints but may not handle high uncertainty in elevation estimates near complex features like craters, do not consider exploration strategies for uncertainty reduction, and typically fail to address how elevation uncertainty affects navigation safety and map quality. To address the problems, we propose a framework integrating safe path generation, adaptive confidence updates, and confidence-aware exploration strategies. Using Kalman-based elevation estimation, our approach generates terrain traversability and confidence scores, then incorporates them into Graph-Based exploration Planner (GBP) to prioritize exploration of traversable low-confidence regions. We evaluate our framework through simulated lunar experiments using a novel low-confidence region ratio metric, achieving 69% uncertainty reduction compared to baseline GBP. In terms of mission success rate, our method achieves 100% while baseline GBP achieves 0%, demonstrating improvements in exploration safety and map reliability.

</details>


### [421] [APP: A* Post-Processing Algorithm for Robots with Bidirectional Shortcut and Path Perturbation](https://arxiv.org/abs/2511.13042)
*Yong Li,Hui Cheng*

Main category: cs.RO

TL;DR: Introduces APP, a post-processing algorithm for A* and graph-search planners that reduces path length and unnecessary heading changes using bidirectional vertex reduction, shortcutting, and iterative path perturbation, validated by simulations and field tests.


<details>
  <summary>Details</summary>
Motivation: Graph-search planners often produce suboptimal, zig-zagged paths due to restricted node-expansion directions. In open spaces, paths should be straight; a general post-processing method based on costmaps is proposed to yield shorter, smoother paths.

Method: APP utilizes a costmap-based framework and includes (1) bidirectional vertex reduction to address path and environment asymmetry, (2) forward and backward vertex reduction with a comprehensive shortcut strategy to shorten the path and reduce heading changes, and (3) iterative path perturbation to locally decrease unnecessary heading changes and improve smoothness.

Result: Comparative experiments show APP outperforms existing methods in planning time, path length, and the number of unnecessary heading changes.

Conclusion: Field navigation experiments validate the practicability of APP for improving path quality in A*- and graph-search-based planners.

Abstract: Paths generated by A* and other graph-search-based planners are widely used in the robotic field. Due to the restricted node-expansion directions, the resulting paths are usually not the shortest. Besides, unnecessary heading changes, or zig-zag patterns, exist even when no obstacle is nearby, which is inconsistent with the human intuition that the path segments should be straight in wide-open space due to the absence of obstacles. This article puts forward a general and systematic post-processing algorithm for A* and other graph-search-based planners. The A* post-processing algorithm, called APP, is developed based on the costmap, which is widely used in commercial service robots. First, a bidirectional vertices reduction algorithm is proposed to tackle the asymm- etry of the path and the environments. During the forward and backward vertices reduction, a thorough shortcut strategy is put forward to improve the path-shortening performance and avoid unnecessary heading changes. Second, an iterative path perturbation algorithm is adopted to locally reduce the number of unnecessary heading changes and improve the path smooth- ness. Comparative experiments are then carried out to validate the superiority of the proposed method. Quantitative performance indexes show that APP outperforms the existing methods in planning time, path length as well as the number of unnecessary heading changes. Finally, field navigation experiments are carried out to verify the practicability of APP.

</details>


### [422] [Unidirectional-Road-Network-Based Global Path Planning for Cleaning Robots in Semi-Structured Environments](https://arxiv.org/abs/2511.13048)
*Yong Li,Hui Cheng*

Main category: cs.RO

TL;DR: A semi-structured environment global path planner integrates a unidirectional road network and a two-layer potential map to balance path length with traffic-rule consistency, allowing start/goal crossing to shorten paths; experimental results show improved performance over state-of-the-art.


<details>
  <summary>Details</summary>
Motivation: There is a trade-off between optimal path length and adherence to traffic rules in semi-structured environments. Free-space planners may ignore constraints leading to replanning and collisions, while strictly road-based planners may produce overly long routes. A general approach that respects constraints while maintaining efficiency is needed.

Method: Build a unidirectional road network to model traffic constraints in semi-structured environments. Propose a hybrid planning strategy that guarantees a feasible plan. Allow cutting across the road at the start and goal points to shorten the path. Introduce a two-layer potential map to ensure robust performance when start/goal lie in complex intersections.

Result: Comparative experiments show the proposed method achieves a better balance between path length and consistency with the road network compared with state-of-the-art methods.

Conclusion: The paper presents a general and systematic approach to improve global path planning performance in semi-structured environments by integrating traffic constraints via a road network, enabling shorter paths through start/goal crossing, and guaranteeing performance in complex intersections.

Abstract: Practical global path planning is critical for commercializing cleaning robots working in semi-structured environments. In the literature, global path planning methods for free space usually focus on path length and neglect the traffic rule constraints of the environments, which leads to high-frequency re-planning and increases collision risks. In contrast, those for structured environments are developed mainly by strictly complying with the road network representing the traffic rule constraints, which may result in an overlong path that hinders the overall navigation efficiency. This article proposes a general and systematic approach to improve global path planning performance in semi-structured environments. A unidirectional road network is built to represent the traffic constraints in semi-structured environments and a hybrid strategy is proposed to achieve a guaranteed planning result.Cutting across the road at the starting and the goal points are allowed to achieve a shorter path. Especially, a two-layer potential map is proposed to achieve a guaranteed performance when the starting and the goal points are in complex intersections. Comparative experiments are carried out to validate the effectiveness of the proposed method. Quantitative experimental results show that, compared with the state-of-art, the proposed method guarantees a much better balance between path length and the consistency with the road network.

</details>


### [423] [Orientation-Free Neural Network-Based Bias Estimation for Low-Cost Stationary Accelerometers](https://arxiv.org/abs/2511.13071)
*Michal Levin,Itzik Klein*

Main category: cs.RO

TL;DR: Model-free, learning-based accelerometer bias calibration under stationary conditions that does not require orientation knowledge or sensor rotation; achieves substantial error reduction (over 52%) vs traditional methods on a 13.39-hour multi-sensor dataset.


<details>
  <summary>Details</summary>
Motivation: Low-cost MEMS accelerometers suffer from bias errors; existing calibration methods rely on leveling or orientation-dependent procedures, which are impractical in field deployments. The motivation is to enable accurate bias calibration without knowing sensor orientation or rotating the device, enabling rapid, scalable calibration.

Method: A model-free, learning-based calibration approach that estimates accelerometer bias using data collected under stationary conditions, without requiring knowledge of sensor orientation or sensor rotation. It aims to be fast, practical, and scalable for field deployment.

Result: Experimental validation on a 13.39-hour dataset from six accelerometers shows the proposed method achieves error levels more than 52% lower than traditional calibration techniques.

Conclusion: The work advances orientation-free calibration for low-cost inertial sensors, improving reliability in diverse applications and eliminating the need for leveled calibration.

Abstract: Low-cost micro-electromechanical accelerometers are widely used in navigation, robotics, and consumer devices for motion sensing and position estimation. However, their performance is often degraded by bias errors. To eliminate deterministic bias terms a calibration procedure is applied under stationary conditions. It requires accelerom- eter leveling or complex orientation-dependent calibration procedures. To overcome those requirements, in this paper we present a model-free learning-based calibration method that estimates accelerometer bias under stationary conditions, without requiring knowledge of the sensor orientation and without the need to rotate the sensors. The proposed approach provides a fast, practical, and scalable solution suitable for rapid field deployment. Experimental validation on a 13.39-hour dataset collected from six accelerometers shows that the proposed method consistently achieves error levels more than 52% lower than traditional techniques. On a broader scale, this work contributes to the advancement of accurate calibration methods in orientation-free scenarios. As a consequence, it improves the reliability of low-cost inertial sensors in diverse scientific and industrial applications and eliminates the need for leveled calibration.

</details>


### [424] [ResAlignNet: A Data-Driven Approach for INS/DVL Alignment](https://arxiv.org/abs/2511.13096)
*Guy Damari,Itzik Klein*

Main category: cs.RO

TL;DR: ResAlignNet uses a 1D ResNet-18 to learn in-situ alignment between INS and DVL on AUVs, enabling rapid, motion-pattern-free, sensor-agnostic calibration with Sim2Real transfer; achieves 0.8° accuracy in 25 seconds and 65% faster convergence than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional model-based alignment for INS-DVL systems is slow, depends on prescribed motion patterns, and often requires external aiding sensors, limiting operational flexibility in GPS-denied underwater environments. A data-driven, onboard, motion-free solution is needed to enable rapid deployment and robust navigation.

Method: A data-driven approach using a 1D ResNet-18 transforms the sensor-frame alignment problem into an optimization task solvable on-board without external aids. The model can be trained on synthetic data (Sim2Real) and deployed on real measurements, demonstrating rapid convergence and broad applicability across vehicle and sensor configurations.

Result: Experimental validation on the Snapir AUV shows alignment accuracy within 0.8°, convergence in 25 seconds, and a 65% reduction in convergence time compared with standard velocity-based alignment methods. The solution is trajectory-independent and supports immediate deployment without lengthy pre-mission maneuvers.

Conclusion: ResAlignNet advances underwater navigation by delivering a robust, sensor-agnostic alignment method that operates onboard, requires no external aids, and generalizes across scenarios and sensor specs, enabling faster, more flexible deployments in GPS-denied environments.

Abstract: Autonomous underwater vehicles rely on precise navigation systems that combine the inertial navigation system and the Doppler velocity log for successful missions in challenging environments where satellite navigation is unavailable. The effectiveness of this integration critically depends on accurate alignment between the sensor reference frames. Standard model-based alignment methods between these sensor systems suffer from lengthy convergence times, dependence on prescribed motion patterns, and reliance on external aiding sensors, significantly limiting operational flexibility. To address these limitations, this paper presents ResAlignNet, a data-driven approach using the 1D ResNet-18 architecture that transforms the alignment problem into deep neural network optimization, operating as an in-situ solution that requires only sensors on board without external positioning aids or complex vehicle maneuvers, while achieving rapid convergence in seconds. Additionally, the approach demonstrates the learning capabilities of Sim2Real transfer, enabling training in synthetic data while deploying in operational sensor measurements. Experimental validation using the Snapir autonomous underwater vehicle demonstrates that ResAlignNet achieves alignment accuracy within 0.8° using only 25 seconds of data collection, representing a 65\% reduction in convergence time compared to standard velocity-based methods. The trajectory-independent solution eliminates motion pattern requirements and enables immediate vehicle deployment without lengthy pre-mission procedures, advancing underwater navigation capabilities through robust sensor-agnostic alignment that scales across different operational scenarios and sensor specifications.

</details>


### [425] [Count Every Rotation and Every Rotation Counts: Exploring Drone Dynamics via Propeller Sensing](https://arxiv.org/abs/2511.13100)
*Xuecheng Chen,Jingao Xu,Wenhua Ding,Haoyang Wang,Xinyu Luo,Ruiyang Duan,Jialong Chen,Xueqian Wang,Yunhao Liu,Xinlei Chen*

Main category: cs.RO

TL;DR: EventPro uses an event-camera-based approach centered on propeller rotation to enable ultra-low-latency, accurate, contactless drone sensing from the ground.


<details>
  <summary>Details</summary>
Motivation: As drone usage grows, there is a need for real-time, ground-based, contactless monitoring of airborne drones despite environmental noise and high-speed dynamics.

Method: Two components: Count Every Rotation estimates propeller speeds by suppressing event-camera noise; Every Rotation Counts uses these speeds to infer internal and external drone dynamics. Evaluation in real-world drone delivery scenarios shows 3 ms latency, 0.23% speed error, 96.5% command-precision, and 22% tracking improvement.

Result: Demonstrates strong performance of EventPro in real-world settings, with precise speed estimation and rapid, accurate command inference, supporting efficient drone monitoring.

Conclusion: EventPro provides a fast, accurate, non-intrusive sensing solution that leverages propeller dynamics; potential for integration with other modalities and broader deployment in drone surveillance and control.

Abstract: As drone-based applications proliferate, paramount contactless sensing of airborne drones from the ground becomes indispensable. This work demonstrates concentrating on propeller rotational speed will substantially improve drone sensing performance and proposes an event-camera-based solution, \sysname. \sysname features two components: \textit{Count Every Rotation} achieves accurate, real-time propeller speed estimation by mitigating ultra-high sensitivity of event cameras to environmental noise. \textit{Every Rotation Counts} leverages these speeds to infer both internal and external drone dynamics. Extensive evaluations in real-world drone delivery scenarios show that \sysname achieves a sensing latency of 3$ms$ and a rotational speed estimation error of merely 0.23\%. Additionally, \sysname infers drone flight commands with 96.5\% precision and improves drone tracking accuracy by over 22\% when combined with other sensing modalities. \textit{ Demo: {\color{blue}https://eventpro25.github.io/EventPro/.} }

</details>


### [426] [Monolithic Units: Actuation, Sensing, and Simulation for Integrated Soft Robot Design](https://arxiv.org/abs/2511.13120)
*Trevor Exley,Anderson Brazil Nardin,Petr Trunin,Diana Cafiso,Lucia Beccai*

Main category: cs.RO

TL;DR: A monolithic actuator-lattice-sensor unit (MU) enables reproducible, scalable soft robots with embedded waveguide sensing, using a parametric design framework, material homogenization, and discrete optimization for sensor paths, validated by fabrication and a two-finger gripper.


<details>
  <summary>Details</summary>
Motivation: To address reproducibility and scalability in soft robotics by co-designing actuation, lattice structure, and sensing in a single printed body, enabling simulation-informed sensor integration.

Method: Develop MU; establish parametric design rules linking actuator chamber geometry to lattice unit cell size; experimentally homogenize lattice specimens to derive effective properties for FE simulation; treat sensor placement as a discrete optimization over candidate waveguide paths with local stiffening; fabricate and experimentally characterize optimized models; extend workflow to scaled units and a two-finger gripper.

Result: Optimized models preserve mechanical performance while enabling embedded sensing; experimental validation confirms predictions; demonstrates scalability and generalization to a gripper, supporting reproducible co-design and sensor integration.

Conclusion: Proposes a general, reproducible, simulation-informed framework for monolithic soft robots, coupling actuation, lattice envelope, and sensing in a single print, with applicability to larger systems.

Abstract: This work introduces the Monolithic Unit (MU), an actuator-lattice-sensor building block for soft robotics. The MU integrates pneumatic actuation, a compliant lattice envelope, and candidate sites for optical waveguide sensing into a single printed body. In order to study reproducibility and scalability, a parametric design framework establishes deterministic rules linking actuator chamber dimensions to lattice unit cell size. Experimental homogenization of lattice specimens provides effective material properties for finite element simulation. Within this simulation environment, sensor placement is treated as a discrete optimization problem, where a finite set of candidate waveguide paths derived from lattice nodes is evaluated by introducing local stiffening, and the configuration minimizing deviation from baseline mechanical response is selected. Optimized models are fabricated and experimentally characterized, validating the preservation of mechanical performance while enabling embedded sensing. The workflow is further extended to scaled units and a two-finger gripper, demonstrating generality of the MU concept. This approach advances monolithic soft robotic design by combining reproducible co-design rules with simulation-informed sensor integration.

</details>


### [427] [Collision-Free Navigation of Mobile Robots via Quadtree-Based Model Predictive Control](https://arxiv.org/abs/2511.13188)
*Osama Al Sheikh Ali,Sotiris Koutsoftas,Ze Zhang,Knut Akesson,Emmanuel Dean*

Main category: cs.RO

TL;DR: An integrated AMR navigation framework that derives quadtree-based, axis-aligned free regions from occupancy maps to form safe corridors, which are used as linear MPC constraints within a full pipeline (safe-area extraction, connectivity, trajectory generation, and B-spline smoothing) achieving robust, efficient navigation.


<details>
  <summary>Details</summary>
Motivation: Address the need for reliable and scalable autonomous navigation in complex, cluttered environments without requiring explicit obstacle encoding in the planner, by combining environment representation, planning, and control into a unified framework.

Method: Use a quadtree-based method to extract structured, axis-aligned collision-free regions from occupancy maps. These regions define safe corridors and are imposed as linear constraints in the Model Predictive Control (MPC) formulation. The pipeline integrates safe-area extraction, a connectivity graph, trajectory generation, and B-spline smoothing to produce smooth, feasible paths.

Result: Empirical results show consistent success and superior performance compared with baseline methods across complex environments.

Conclusion: The work delivers a cohesive, scalable navigation framework that unifies perception, planning, and control for AMRs, with the quadtree-derived safe corridors enabling efficient MPC and reliable navigation; future work may extend to dynamic obstacles and real-time adaptation.

Abstract: This paper presents an integrated navigation framework for Autonomous Mobile Robots (AMRs) that unifies environment representation, trajectory generation, and Model Predictive Control (MPC). The proposed approach incorporates a quadtree-based method to generate structured, axis-aligned collision-free regions from occupancy maps. These regions serve as both a basis for developing safe corridors and as linear constraints within the MPC formulation, enabling efficient and reliable navigation without requiring direct obstacle encoding. The complete pipeline combines safe-area extraction, connectivity graph construction, trajectory generation, and B-spline smoothing into one coherent system. Experimental results demonstrate consistent success and superior performance compared to baseline approaches across complex environments.

</details>


### [428] [PIGEON: VLM-Driven Object Navigation via Points of Interest Selection](https://arxiv.org/abs/2511.13207)
*Cheng Peng,Zhenzhe Zhang,Cheng Chi,Xiaobao Wei,Yanhao Zhang,Heng Wang,Pengwei Wang,Zhongyuan Wang,Jing Liu,Shanghang Zhang*

Main category: cs.RO

TL;DR: A PoI-guided exploration framework (PIGEON) with a VLM-based PoI selector and snapshot memory improves object navigation, achieving SOTA zero-shot performance and enabling RLVR data for enhanced reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between decision frequency and long-horizon foresight in unknown environments. Current methods either act frequently with limited foresight or plan for long horizons with discontinuities. By maintaining a semantic snapshot memory and using a Vision-Language Model to select Points of Interest, the agent can act with higher frequency while retaining semantic guidance.

Method: Maintain a lightweight, semantically aligned snapshot memory during exploration. Use PIGEON-VL, a large Visual-Language Model, to select Points of Interest (PoIs) formed during exploration. Employ a lower-level planner for action output to increase decision frequency. Generate RLVR data via PoI-based decisions for simulator-based training. Evaluate zero-shot transfer on classic object navigation benchmarks.

Result: Achieves state-of-the-art performance in zero-shot object navigation. RLVR data further enhances the model’s semantic guidance capabilities, enabling deeper reasoning during real-time navigation.

Conclusion: PoI-guided exploration with semantic snapshot memory and VLM-based PoI selection yields improved navigation performance, enabling high-frequency decisions and RLVR-enabled training data for richer semantic reasoning in embodied navigation.

Abstract: Navigating to a specified object in an unknown environment is a fundamental yet challenging capability of embodied intelligence. However, current methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions. In this work, we propose PIGEON: Point of Interest Guided Exploration for Object Navigation with VLM, maintaining a lightweight and semantically aligned snapshot memory during exploration as semantic input for the exploration strategy. We use a large Visual-Language Model (VLM), named PIGEON-VL, to select Points of Interest (PoI) formed during exploration and then employ a lower-level planner for action output, increasing the decision frequency. Additionally, this PoI-based decision-making enables the generation of Reinforcement Learning with Verifiable Reward (RLVR) data suitable for simulators. Experiments on classic object navigation benchmarks demonstrate that our zero-shot transfer method achieves state-of-the-art performance, while RLVR further enhances the model's semantic guidance capabilities, enabling deep reasoning during real-time navigation.

</details>


### [429] [GaRLILEO: Gravity-aligned Radar-Leg-Inertial Enhanced Odometry](https://arxiv.org/abs/2511.13216)
*Chiyun Noh,Sangwoo Jung,Hanjun Kim,Yafei Hu,Laura Herlant,Ayoung Kim*

Main category: cs.RO

TL;DR: A gravity-aligned continuous-time radar-leg-inertial odometry (GaRLILEO) that decouples velocity from IMU using a velocity spline built from radar Doppler and leg kinematics, and uses a soft S2 gravity factor to improve vertical pose without LiDAR/camera; achieves state-of-the-art vertical odometry on stairs and slopes and is open-sourced.


<details>
  <summary>Details</summary>
Motivation: Legged robots on challenging terrains face significant vertical odometry drift due to impacts, foot slippage, and vibrations. Proprioceptive methods falter without reliable roll/pitch; exteroceptive sensors help but struggle in feature-sparse, repetitive scenes and suffer from IMU double-integral errors. A gravity-aware, radar-augmented approach aims to improve vertical pose estimation and robustness without heavy reliance on LiDAR/cameras.

Method: Introduce GaRLILEO, a radar-leg-inertial odometry framework that decouples velocity from the IMU by forming a continuous-time ego-velocity spline derived from SoC radar Doppler and leg kinematics. Fuse sensors within a gravity-aware pipeline using a novel soft S2-constrained gravity factor to reliably estimate gravity vectors, improving vertical pose accuracy without LiDAR/cameras. Evaluated on a real-world self-collected dataset with diverse indoor-outdoor trajectories; open-sourced the dataset and algorithm.

Result: GaRLILEO achieves state-of-the-art accuracy, with notable improvements in vertical odometry estimation on stairs and slopes, validated on real-world data.

Conclusion: The approach advances legged odometry by decoupling velocity, leveraging radar-based ego-velocity, and integrating a gravity factor that enhances vertical pose without exteroceptive sensors; the authors provide open-source dataset and code to spur further research.

Abstract: Deployment of legged robots for navigating challenging terrains (e.g., stairs, slopes, and unstructured environments) has gained increasing preference over wheel-based platforms. In such scenarios, accurate odometry estimation is a preliminary requirement for stable locomotion, localization, and mapping. Traditional proprioceptive approaches, which rely on leg kinematics sensor modalities and inertial sensing, suffer from irrepressible vertical drift caused by frequent contact impacts, foot slippage, and vibrations, particularly affected by inaccurate roll and pitch estimation. Existing methods incorporate exteroceptive sensors such as LiDAR or cameras. Further enhancement has been introduced by leveraging gravity vector estimation to add additional observations on roll and pitch, thereby increasing the accuracy of vertical pose estimation. However, these approaches tend to degrade in feature-sparse or repetitive scenes and are prone to errors from double-integrated IMU acceleration. To address these challenges, we propose GaRLILEO, a novel gravity-aligned continuous-time radar-leg-inertial odometry framework. GaRLILEO decouples velocity from the IMU by building a continuous-time ego-velocity spline from SoC radar Doppler and leg kinematics information, enabling seamless sensor fusion which mitigates odometry distortion. In addition, GaRLILEO can reliably capture accurate gravity vectors leveraging a novel soft S2-constrained gravity factor, improving vertical pose accuracy without relying on LiDAR or cameras. Evaluated on a self-collected real-world dataset with diverse indoor-outdoor trajectories, GaRLILEO demonstrates state-of-the-art accuracy, particularly in vertical odometry estimation on stairs and slopes. We open-source both our dataset and algorithm to foster further research in legged robot odometry and SLAM. https://garlileo.github.io/GaRLILEO

</details>


### [430] [EL3DD: Extended Latent 3D Diffusion for Language Conditioned Multitask Manipulation](https://arxiv.org/abs/2511.13312)
*Jonas Bode,Raphael Memmesheimer,Sven Behnke*

Main category: cs.RO

TL;DR: A diffusion-model–driven visuomotor policy integrates visual and textual inputs to guide robot manipulation, trained with reference demonstrations, and extended with improved embeddings to generalize across tasks; evaluated on CALVIN with gains in short- and long-horizon success.


<details>
  <summary>Details</summary>
Motivation: to enable robust natural-language-grounded control of robots in human environments by leveraging diffusion models to fuse vision and language into precise trajectories.

Method: Extend an existing diffusion-based visuomotor framework with better embeddings; train with reference demonstrations; adapt diffusion techniques from image generation to produce sequences of robotic actions conditioned on visual/text inputs.

Result: Improved manipulation performance and higher long-horizon success rates when executing multiple tasks in sequence on CALVIN.

Conclusion: Diffusion models are promising for general multitask manipulation and can extend language-grounded robotic control in embodied settings.

Abstract: Acting in human environments is a crucial capability for general-purpose robots, necessitating a robust understanding of natural language and its application to physical tasks. This paper seeks to harness the capabilities of diffusion models within a visuomotor policy framework that merges visual and textual inputs to generate precise robotic trajectories. By employing reference demonstrations during training, the model learns to execute manipulation tasks specified through textual commands within the robot's immediate environment. The proposed research aims to extend an existing model by leveraging improved embeddings, and adapting techniques from diffusion models for image generation. We evaluate our methods on the CALVIN dataset, proving enhanced performance on various manipulation tasks and an increased long-horizon success rate when multiple tasks are executed in sequence. Our approach reinforces the usefulness of diffusion models and contributes towards general multitask manipulation.

</details>


### [431] [ZeroDexGrasp: Zero-Shot Task-Oriented Dexterous Grasp Synthesis with Prompt-Based Multi-Stage Semantic Reasoning](https://arxiv.org/abs/2511.13327)
*Juntao Jian,Yi-Lin Wei,Chengjie Mou,Yuhao Lin,Xing Zhu,Yujun Shen,Wei-Shi Zheng,Ruizhen Hu*

Main category: cs.RO

TL;DR: ZeroDexGrasp enables zero-shot, task-aligned dexterous grasping by combining Multimodal LLMs with contact-guided refinement, achieving generalization to unseen objects without labeled data.


<details>
  <summary>Details</summary>
Motivation: Current task-oriented grasping methods rely on labeled data to align semantic task goals with grasp configurations, limiting generalization across objects and tasks.

Method: Use prompt-based multi-stage semantic reasoning from task and object semantics to infer initial grasps and contact info; then refine with contact-guided optimization for feasibility and task alignment, integrating LLMs with physics-aware refinement.

Result: Demonstrates high-quality zero-shot grasping on diverse unseen objects and complex tasks; shows improved generalization and alignment with task objectives.

Conclusion: Presents a path toward more generalizable robotic grasping by leveraging LLM-driven semantic reasoning and refinement to reduce data labeling needs.

Abstract: Task-oriented dexterous grasping holds broad application prospects in robotic manipulation and human-object interaction. However, most existing methods still struggle to generalize across diverse objects and task instructions, as they heavily rely on costly labeled data to ensure task-specific semantic alignment. In this study, we propose \textbf{ZeroDexGrasp}, a zero-shot task-oriented dexterous grasp synthesis framework integrating Multimodal Large Language Models with grasp refinement to generate human-like grasp poses that are well aligned with specific task objectives and object affordances. Specifically, ZeroDexGrasp employs prompt-based multi-stage semantic reasoning to infer initial grasp configurations and object contact information from task and object semantics, then exploits contact-guided grasp optimization to refine these poses for physical feasibility and task alignment. Experimental results demonstrate that ZeroDexGrasp enables high-quality zero-shot dexterous grasping on diverse unseen object categories and complex task requirements, advancing toward more generalizable and intelligent robotic grasping.

</details>


### [432] [Contact-Safe Reinforcement Learning with ProMP Reparameterization and Energy Awareness](https://arxiv.org/abs/2511.13459)
*Bingkun Huang,Yuhe Gong,Zewen Yang,Tianyu Ren,Luis Figueredo*

Main category: cs.RO

TL;DR: Proposes a task-space, energy-safe framework for contact-rich manipulation that combines PPO with movement primitives and an energy-aware Cartesian impedance controller to achieve robust, safe, and smooth 3D trajectories, outperforming traditional MDP/episodic RL methods.


<details>
  <summary>Details</summary>
Motivation: MDP-based RL in robot joint space with limited task-specific information; episodic RL offers trajectory consistency but may neglect contact-rich task-space and safety; there is a need for safe, contact-aware manipulation in 3D environments.

Method: Integrates Proximal Policy Optimization (PPO) with movement primitives to generate reliable task-space trajectories; incorporates an energy-aware Cartesian Impedance Controller objective to ensure safe interactions.

Result: Experiments across various surface types in 3D environments show higher success rates, smoother trajectories, and energy-safe interactions compared to existing methods.

Conclusion: The task-space energy-safe framework effectively handles contact-rich manipulation tasks, demonstrating the benefit of combining PPO and movement primitives with impedance-based safety to improve robustness and performance in 3D settings.

Abstract: Reinforcement learning (RL) approaches based on Markov Decision Processes (MDPs) are predominantly applied in the robot joint space, often relying on limited task-specific information and partial awareness of the 3D environment. In contrast, episodic RL has demonstrated advantages over traditional MDP-based methods in terms of trajectory consistency, task awareness, and overall performance in complex robotic tasks. Moreover, traditional step-wise and episodic RL methods often neglect the contact-rich information inherent in task-space manipulation, especially considering the contact-safety and robustness. In this work, contact-rich manipulation tasks are tackled using a task-space, energy-safe framework, where reliable and safe task-space trajectories are generated through the combination of Proximal Policy Optimization (PPO) and movement primitives. Furthermore, an energy-aware Cartesian Impedance Controller objective is incorporated within the proposed framework to ensure safe interactions between the robot and the environment. Our experimental results demonstrate that the proposed framework outperforms existing methods in handling tasks on various types of surfaces in 3D environments, achieving high success rates as well as smooth trajectories and energy-safe interactions.

</details>


### [433] [Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety](https://arxiv.org/abs/2511.13530)
*Vesna Poprcova,Iulia Lefter,Matthias Wieser,Martijn Warnier,Frances Brazier*

Main category: cs.RO

TL;DR: Proposes a protocol to collect a multimodal dataset for social anxiety in human-robot interaction, using synchronized audio, video, and physiological signals from 70+ participants in ~10-minute Wizard-of-Oz Furhat sessions, plus contextual data to enable robust affective-state detection.


<details>
  <summary>Details</summary>
Motivation: Addresses the scarcity of multimodal datasets for social anxiety in human-robot interaction and aims to support affect-adaptive HRI through robust multimodal detection of anxiety-related cues.

Method: A staged data-collection protocol: recruit at least 70 participants across social anxiety levels, conduct ~10-minute Wizard-of-Oz interactions with the Furhat robot under controlled lab conditions, collect synchronized audio, video, and physiological signals, and enrich the dataset with contextual metadata to capture individual variability.

Result: A detailed protocol and dataset plan for a large multimodal social anxiety dataset in HRI; no data collected yet, but the paper outlines how the data will be gathered and used.

Conclusion: Implementation of this protocol could yield a valuable resource for multimodal analysis of social anxiety in HRI and advance affect-adaptive interaction research.

Abstract: Social anxiety is a prevalent condition that affects interpersonal interactions and social functioning. Recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the human-robot interaction context. Accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets, where each signal modality provides complementary insights into its manifestations. However, such datasets remain scarce, limiting progress in both research and applications. To address this, this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a human-robot interaction context. The dataset will consist of synchronised audio, video, and physiological recordings acquired from at least 70 participants, grouped according to their level of social anxiety, as they engage in approximately 10-minute interactive Wizard-of-Oz role-play scenarios with the Furhat social robot under controlled experimental conditions. In addition to multimodal data, the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses. This work can contribute to research on affect-adaptive human-robot interaction by providing support for robust multimodal detection of social anxiety.

</details>


### [434] [OpenRoboCare: A Multimodal Multi-Task Expert Demonstration Dataset for Robot Caregiving](https://arxiv.org/abs/2511.13707)
*Xiaoyu Liang,Ziang Liu,Kelvin Lin,Edward Gu,Ruolin Ye,Tam Nguyen,Cynthia Hsu,Zhanxin Wu,Xiaoman Yang,Christy Sum Yu Cheung,Harold Soh,Katherine Dimitropoulou,Tapomayukh Bhattacharjee*

Main category: cs.RO

TL;DR: OpenRoboCare introduces a large, multimodal dataset of expert caregiver demonstrations for Activities of Daily Living (ADLs), capturing rich sensory and behavioral signals to study perception, safe contact, and long-horizon planning in robot caregiving.


<details>
  <summary>Details</summary>
Motivation: There is a shortage of large-scale, expert-driven datasets that capture real-world caregiving routines across multiple modalities, hindering the development of safe and adaptive assistive robots.

Method: Data were collected from 21 occupational therapists performing 15 ADL tasks on two manikins, across five modalities: RGB-D video, pose tracking, eye-gaze tracking, task/action annotations, and tactile sensing, including analysis of expert caregiving principles.

Result: OpenRoboCare provides a diverse, multimodal dataset that reveals expert caregiving strategies and presents challenges for current state-of-the-art perception and human activity recognition methods, underscoring its value for advancing robot caregiving.

Conclusion: The dataset fills a critical data gap for robot caregiving research, offering insights to improve perception, planning, safety, and efficiency in assistive robots, and serving as a resource for evaluating and improving learning-based methods.

Abstract: We present OpenRoboCare, a multimodal dataset for robot caregiving, capturing expert occupational therapist demonstrations of Activities of Daily Living (ADLs). Caregiving tasks involve complex physical human-robot interactions, requiring precise perception under occlusions, safe physical contact, and long-horizon planning. While recent advances in robot learning from demonstrations have shown promise, there is a lack of a large-scale, diverse, and expert-driven dataset that captures real-world caregiving routines. To address this gap, we collect data from 21 occupational therapists performing 15 ADL tasks on two manikins. The dataset spans five modalities: RGB-D video, pose tracking, eye-gaze tracking, task and action annotations, and tactile sensing, providing rich multimodal insights into caregiver movement, attention, force application, and task execution strategies. We further analyze expert caregiving principles and strategies, offering insights to improve robot efficiency and task feasibility. Additionally, our evaluations demonstrate that OpenRoboCare presents challenges for state-of-the-art robot perception and human activity recognition methods, both critical for developing safe and adaptive assistive robots, highlighting the value of our contribution. See our website for additional visualizations: https://emprise.cs.cornell.edu/robo-care/.

</details>


### [435] [From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands](https://arxiv.org/abs/2511.13710)
*Jianglong Ye,Lai Wei,Guangqi Jiang,Changwei Jing,Xueyan Zou,Xiaolong Wang*

Main category: cs.RO

TL;DR: Co-design of control and fingertip geometry enables both power and precision manipulation in a single multi-fingered hand, achieving high sim-to-real precision grasping with a lightweight fingertip modification optimized via a differentiable surrogate.


<details>
  <summary>Details</summary>
Motivation: Current robotic hands are strong for power grasps but struggle with precise, fine-grained manipulation. A single versatile system capable of both is a key limitation hindering dexterous manipulation.

Method: Introduce a lightweight fingertip geometry modification represented as a contact plane. Jointly optimize fingertip geometry and control; switch dynamically between power and precision modes, with precision control simplified to parallel thumb-index motions. Use a differentiable neural-physics surrogate and large-scale simulation to optimize the fingertip. Validate with extensive sim-to-real and real-to-real experiments.

Result: Achieves 82.5% zero-shot success on unseen objects in sim-to-real precision grasping; 93.3% success in challenging real-world tasks like bread pinching; demonstrates robust sim-to-real transfer and that co-design improves fine-grained manipulation without sacrificing power-grasp capability.

Conclusion: The co-design framework effectively enhances fine-grained manipulation in multi-fingered hands and offers a practical path to versatile, robust dexterous manipulation without redesigning the entire hand.

Abstract: Human grasps can be roughly categorized into two types: power grasps and precision grasps. Precision grasping enables tool use and is believed to have influenced human evolution. Today's multi-fingered robotic hands are effective in power grasps, but for tasks requiring precision, parallel grippers are still more widely adopted. This contrast highlights a key limitation in current robotic hand design: the difficulty of achieving both stable power grasps and precise, fine-grained manipulation within a single, versatile system. In this work, we bridge this gap by jointly optimizing the control and hardware design of a multi-fingered dexterous hand, enabling both power and precision manipulation. Rather than redesigning the entire hand, we introduce a lightweight fingertip geometry modification, represent it as a contact plane, and jointly optimize its parameters along with the corresponding control. Our control strategy dynamically switches between power and precision manipulation and simplifies precision control into parallel thumb-index motions, which proves robust for sim-to-real transfer. On the design side, we leverage large-scale simulation to optimize the fingertip geometry using a differentiable neural-physics surrogate model. We validate our approach through extensive experiments in both sim-to-real and real-to-real settings. Our method achieves an 82.5% zero-shot success rate on unseen objects in sim-to-real precision grasping, and a 93.3% success rate in challenging real-world tasks involving bread pinching. These results demonstrate that our co-design framework can significantly enhance the fine-grained manipulation ability of multi-fingered hands without reducing their ability for power grasps. Our project page is at https://jianglongye.com/power-to-precision

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [436] [Softmax as a Lagrangian-Legendrian Seam](https://arxiv.org/abs/2511.11573)
*Christopher R. Lee-Jenkins*

Main category: cs.LG

TL;DR: A first bridge from ML to differential geometry: reinterpret softmax as a geometric interface on the probability simplex, with two conservative potential descriptions meeting along a Legendrian seam; bias invariance via Reeb flow; KL/Fenchel-Young gap provides a computable seam distance; concrete 2- and 3-class cases; outlines for compact logit models and connections to information geometry.


<details>
  <summary>Details</summary>
Motivation: To establish a geometric bridge between the logits-to-probabilities step in ML and modern differential geometry, enabling global invariants and new distance notions for learning dynamics.

Method: Model the softmax transformation as the intersection of two potential-generated, conservative descriptions (from negative entropy and log-sum-exp) on a probability-simplex played out on a contact screen inside a folded symplectic collar; identify a Legendrian seam where the descriptions meet; interpret bias-shift invariance as Reeb flow on the screen; use Fenchel-Young equality/KL gap to define a computable distance to the seam; analyze 2- and 3-class cases to instantiate the picture.

Result: Concrete geometric picture for the softmax mapping; explicit analysis in 2- and 3-class settings; formulation of next steps for ML, including compact logit models and links to information geometry via replicator dynamics.

Conclusion: The abstract presents a novel geometric framework for understanding the logits-to-probabilities step, suggesting global invariants and new ML directions by embedding softmax dynamics in contact/symplectic geometry and information-geometric structures.

Abstract: This note offers a first bridge from machine learning to modern differential geometry. We show that the logits-to-probabilities step implemented by softmax can be modeled as a geometric interface: two potential-generated, conservative descriptions (from negative entropy and log-sum-exp) meet along a Legendrian "seam" on a contact screen (the probability simplex) inside a simple folded symplectic collar. Bias-shift invariance appears as Reeb flow on the screen, and the Fenchel-Young equality/KL gap provides a computable distance to the seam. We work out the two- and three-class cases to make the picture concrete and outline next steps for ML: compact logit models (projective or spherical), global invariants, and connections to information geometry where on-screen dynamics manifest as replicator flows.

</details>


### [437] [LLM on a Budget: Active Knowledge Distillation for Efficient Classification of Large Text Corpora](https://arxiv.org/abs/2511.11574)
*Viviana Luccioli,Rithika Iyengar,Ryan Panley,Flora Haberkorn,Xiaoyu Ge,Leland Crane,Nitish Sinha,Seung Jung Lee*

Main category: cs.LG

TL;DR: Introduces M-RARU, a multi-class active-learning algorithm with randomized accept/reject uncertainty sampling to cut knowledge-distillation costs for LLMs. It selects only the most informative samples for labeling, achieving up to 80% reduction in labeled data compared to random sampling while maintaining or improving accuracy across diverse student models.


<details>
  <summary>Details</summary>
Motivation: To reduce the high computational and financial costs of distilling knowledge from large language models by limiting the data that must be labeled by the teacher, using active learning to focus labeling on informative samples.

Method: Proposes M-RARU (Multi-class Randomized Accept/Reject Uncertainty Sampling), an active-learning strategy that combines uncertainty-based selection with a randomized accept/reject mechanism to pick only the most informative data points for teacher labeling. Evaluated against random sampling across five student models (SVM, LDA, RF, GBDT, DistilBERT) on multiple benchmarks.

Result: Demonstrates up to 80% reduction in sample requirements versus random sampling, with substantially improved classification accuracy and reduced training time and costs.

Conclusion: M-RARU effectively reduces distillation costs while preserving or enhancing performance, is applicable across diverse student models and datasets, and can enable cost-efficient deployment of LLM-based knowledge distillation in dynamic environments.

Abstract: Large Language Models (LLMs) are highly accurate in classification tasks, however, substantial computational and financial costs hinder their large-scale deployment in dynamic environments. Knowledge Distillation (KD) where a LLM "teacher" trains a smaller and more efficient "student" model, offers a promising solution to this problem. However, the distillation process itself often remains costly for large datasets, since it requires the teacher to label a vast number of samples while incurring significant token consumption. To alleviate this challenge, in this work we explore the active learning (AL) as a way to create efficient student models at a fraction of the cost while preserving the LLM's performance. In particular, we introduce M-RARU (Multi-class Randomized Accept/Reject Uncertainty Sampling), a novel AL algorithm that significantly reduces training costs. M-RARU employs an innovative strategy combining uncertainty with a randomized accept-reject mechanism to select only the most informative data points for the LLM teacher. This focused approach significantly minimizes required API calls and data processing time. We evaluate M-RARU against random sampling across five diverse student models (SVM, LDA, RF, GBDT, and DistilBERT) on multiple benchmark datasets. Experiments demonstrate that our proposed method achieves up to 80% reduction in sample requirements as compared to random sampling, substantially improving classification accuracy while reducing financial costs and overall training time.

</details>


### [438] [Detecting Statistically Significant Fairness Violations in Recidivism Forecasting Algorithms](https://arxiv.org/abs/2511.11575)
*Animesh Joshi*

Main category: cs.LG

TL;DR: Proposes a framework for statistically testing fairness violations using k-fold cross-validation to generate sampling distributions of fairness metrics, demonstrated on recidivism prediction data.


<details>
  <summary>Details</summary>
Motivation: Existing fairness definitions and causal/calibration tools do not assess whether observed disparities are statistically significant or due to chance, risking misinterpretation of bias.

Method: Use k-fold cross-validation to generate sampling distributions of fairness metrics; introduce statistical tests to identify significant violations based on disparities between predicted and actual outcomes, model calibration, and causal inference techniques; validate with National Institute of Justice recidivism data.

Result: Found statistically significant bias against Black individuals under several fairness definitions, while finding no bias or bias against White individuals under other definitions.

Conclusion: Rigorous statistical significance testing is essential when evaluating algorithmic fairness to avoid mistaking random variations for true disparities.

Abstract: Machine learning algorithms are increasingly deployed in critical domains such as finance, healthcare, and criminal justice [1]. The increasing popularity of algorithmic decision-making has stimulated interest in algorithmic fairness within the academic community. Researchers have introduced various fairness definitions that quantify disparities between privileged and protected groups, use causal inference to determine the impact of race on model predictions, and that test calibration of probability predictions from the model. Existing literature does not provide a way in which to assess whether observed disparities between groups are statistically significant or merely due to chance. This paper introduces a rigorous framework for testing the statistical significance of fairness violations by leveraging k-fold cross-validation [2] to generate sampling distributions of fairness metrics. This paper introduces statistical tests that can be used to identify statistically significant violations of fairness metrics based on disparities between predicted and actual outcomes, model calibration, and causal inference techniques [1]. We demonstrate this approach by testing recidivism forecasting algorithms trained on data from the National Institute of Justice. Our findings reveal that machine learning algorithms used for recidivism forecasting exhibit statistically significant bias against Black individuals under several fairness definitions, while also exhibiting no bias or bias against White individuals under other definitions. The results from this paper underscore the importance of rigorous and robust statistical testing while evaluating algorithmic decision-making systems.

</details>


### [439] [DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs](https://arxiv.org/abs/2511.11576)
*WenZhuo Zhu,Zheng Cui,Wenhan Lu,Sheng Liu,Yue Zhao*

Main category: cs.LG

TL;DR: Proposes the DAOpt framework with the OptU dataset, a multi-agent decision-making module, and a simulation environment to evaluate LLMs under uncertainty, enhanced by few-shot learning with stochastic/robust optimization knowledge.


<details>
  <summary>Details</summary>
Motivation: Bridges the gap between LLM-based optimization (often deterministic) and real-world uncertainty; provides an evaluation platform focused on out-of-sample feasibility and robustness.

Method: Introduce the DAOpt framework consisting of OptU, a multi-agent decision-making module, and a simulation environment; augment LLMs with few-shot learning that leverages domain knowledge from stochastic and robust optimization.

Result: Conceptual framework with dataset and environment; no empirical results reported in the abstract; expected to improve evaluation of LLMs under uncertainty and robustness through knowledge integration.

Conclusion: Advances the integration of LLMs into uncertain optimization, enabling better handling of uncertainty in decision-making and establishing benchmarks for out-of-sample feasibility and robustness.

Abstract: Recent advances in large language models (LLMs) have accelerated research on automated optimization modeling. While real-world decision-making is inherently uncertain, most existing work has focused on deterministic optimization with known parameters, leaving the application of LLMs in uncertain settings largely unexplored. To that end, we propose the DAOpt framework including a new dataset OptU, a multi-agent decision-making module, and a simulation environment for evaluating LLMs with a focus on out-of-sample feasibility and robustness. Additionally, we enhance LLMs' modeling capabilities by incorporating few-shot learning with domain knowledge from stochastic and robust optimization.

</details>


### [440] [Decoupling Positional and Symbolic Attention Behavior in Transformers](https://arxiv.org/abs/2511.11579)
*Felipe Urrutia,Jorge Salas,Alexander Kozachinskiy,Cristian Buc Calderon,Hector Pasten,Cristobal Rojas*

Main category: cs.LG

TL;DR: The paper formalizes how RoPE in transformers encodes positional vs symbolic information, shows these behaviors are mutually exclusive, creates a metric to quantify them, demonstrates a strong link between head behavior and frequency usage in RoPE-attention, and proves that Transformer performance can be causally controlled by restricting which frequencies attention heads access on canonical positional/symbolic tasks.


<details>
  <summary>Details</summary>
Motivation: To disentangle and quantify how transformer heads use RoPE frequencies to encode positional and symbolic information, and to establish whether this causally affects performance.

Method: Define formal criteria for positional vs symbolic head behavior, prove their mutual exclusivity, develop a metric to quantify each behavior, apply the framework to RoPE-equipped transformers, design canonical tasks that isolate positional versus symbolic processing, and conduct experiments to assess correlations and causal effects of accessible frequencies on performance.

Result: All attention heads show a strong correspondence between described behavior and the frequencies they access. Performance correlates with the ability of heads to leverage appropriate frequencies, and controlling accessible frequencies can modulate Transformer performance.

Conclusion: Provides a detailed understanding of RoPE and its relationship to model behavior, showing a causal link between frequency access and performance, and enabling targeted manipulation of Transformer behavior via frequency constraints.

Abstract: An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.

</details>


### [441] [The Anatomy of a Triton Attention Kernel](https://arxiv.org/abs/2511.11581)
*Burkhard Ringlein,Jan van Lunteren,Radu Stoica,Thomas Parnell*

Main category: cs.LG

TL;DR: Portable, efficient cross-platform LLM inference using a Triton-based paged attention kernel that delivers near-to- or above-state-of-the-art performance on NVIDIA and AMD GPUs, enabling vendor-agnostic deployment.


<details>
  <summary>Details</summary>
Motivation: The industry/academia aim for a portable, hand-tuning-free, highly efficient LLM inference platform that works across hardware vendors and avoids vendor lock-in while maintaining top performance.

Method: Develop a state-of-the-art paged attention kernel built exclusively in the Triton DSL (JIT-compiled) and apply high-level design choices, algorithmic and system-level improvements, and parameter auto-tuning; integrate the kernel into a popular inference server to achieve portable high performance across GPUs.

Result: Demonstrates that portable, efficient cross-platform LLM inference is possible; the Triton-based attention kernel reduces the gap to the SOTA from 19.7% to 105.9% of the state-of-the-art across NVIDIA and AMD GPUs, indicating strong cross-vendor performance and portability.

Conclusion: Open-source domain-specific languages like Triton can unlock model portability across GPU vendors and enable high-performance LLM inference without hardware-specific hand-tuning.

Abstract: A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.

</details>


### [442] [Linear time small coresets for k-mean clustering of segments with applications](https://arxiv.org/abs/2511.12564)
*David Denisov,Shlomi Dolev,Dan Felmdan,Michael Segal*

Main category: cs.LG

TL;DR: First provable coreset for k-means on a set of segments; achieves coreset size O(log^2 n) for constant k and ε, computable in O(nd) time; enables fast streaming, distributed clustering with small accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Extend coreset theory to geometric primitives beyond points (segments); enable efficient, scalable clustering for segment data in applications like real-time video tracking; support variants (outliers, different distances, balanced clustering) via a compact summary.

Method: Develop a coreset construction tailored to arbitrary input segments that guarantees a (1±ε) approximation of D(S,X) for any k centers. Prove size bound O(log^2 n) for constant k, ε and provide an O(nd) time algorithm to compute the coreset. The construction leverages segment geometry (line integrals), sensitivity analysis, and efficient sampling/merging to produce a small weighted subset.

Result: The paper proves the existence of a coreset of size O(log^2 n) for constant k and ε with a construction time of O(nd). Experimental results, including a real-time video tracking application, show substantial speedups with only minor degradation in clustering accuracy.

Conclusion: This work delivers the first provably correct coreset for arbitrary segments in k-means, enabling efficient scalable clustering in streaming and distributed settings and opening paths to extensions to other distance measures and clustering variants.

Abstract: We study the $k$-means problem for a set $\mathcal{S} \subseteq \mathbb{R}^d$ of $n$ segments, aiming to find $k$ centers $X \subseteq \mathbb{R}^d$ that minimize
  $D(\mathcal{S},X) := \sum_{S \in \mathcal{S}} \min_{x \in X} D(S,x)$, where $D(S,x) := \int_{p \in S} |p - x| dp$
  measures the total distance from each point along a segment to a center. Variants of this problem include handling outliers, employing alternative distance functions such as M-estimators, weighting distances to achieve balanced clustering, or enforcing unique cluster assignments. For any $\varepsilon > 0$, an $\varepsilon$-coreset is a weighted subset $C \subseteq \mathbb{R}^d$ that approximates $D(\mathcal{S},X)$ within a factor of $1 \pm \varepsilon$ for any set of $k$ centers, enabling efficient streaming, distributed, or parallel computation. We propose the first coreset construction that provably handles arbitrary input segments. For constant $k$ and $\varepsilon$, it produces a coreset of size $O(\log^2 n)$ computable in $O(nd)$ time. Experiments, including a real-time video tracking application, demonstrate substantial speedups with minimal loss in clustering accuracy, confirming both the practical efficiency and theoretical guarantees of our method.

</details>


### [443] [Parallel and Multi-Stage Knowledge Graph Retrieval for Behaviorally Aligned Financial Asset Recommendations](https://arxiv.org/abs/2511.11583)
*Fernando Spadea,Oshani Seneviratne*

Main category: cs.LG

TL;DR: A retrieval-augmented, KG-grounded LLM framework (RAG-FLARKO) for personalized financial recommendations that uses multi-stage KG retrieval to build a compact, relevant subgraph, improving recommendation quality while enabling smaller models in resource-constrained settings.


<details>
  <summary>Details</summary>
Motivation: Tackle core LLM limitations in finance—context length, hallucinations, and weak behavioral grounding—while ensuring scalability and practicality by enabling efficient grounding with KGs and smaller models.

Method: Multi-stage and parallel KG retrieval: (1) retrieve behaviorally relevant entities from the user’s transaction KG; (2) use this context to filter temporally consistent signals from a market KG; construct a compact, grounded subgraph for the LLM to consume, reducing context overhead while sharpening relevance.

Result: Empirical evaluation on a real-world financial transaction dataset shows significant improvements in recommendation quality and demonstrates that smaller, efficient models can achieve high profitability and behavioral alignment with grounded financial AI.

Conclusion: RAG-FLARKO offers a scalable, grounded approach for financial AI that reduces context burden and enables resource-efficient deployment while improving both profitability and behavioral alignment.

Abstract: Large language models (LLMs) show promise for personalized financial recommendations but are hampered by context limits, hallucinations, and a lack of behavioral grounding. Our prior work, FLARKO, embedded structured knowledge graphs (KGs) in LLM prompts to align advice with user behavior and market data. This paper introduces RAG-FLARKO, a retrieval-augmented extension to FLARKO, that overcomes scalability and relevance challenges using multi-stage and parallel KG retrieval processes. Our method first retrieves behaviorally relevant entities from a user's transaction KG and then uses this context to filter temporally consistent signals from a market KG, constructing a compact, grounded subgraph for the LLM. This pipeline reduces context overhead and sharpens the model's focus on relevant information. Empirical evaluation on a real-world financial transaction dataset demonstrates that RAG-FLARKO significantly enhances recommendation quality. Notably, our framework enables smaller, more efficient models to achieve high performance in both profitability and behavioral alignment, presenting a viable path for deploying grounded financial AI in resource-constrained environments.

</details>


### [444] [Output Supervision Can Obfuscate the Chain of Thought](https://arxiv.org/abs/2511.11584)
*Jacob Drori,Luke Marks,Bryce Woodworth,Alex Cloud,Alexander Matt Turner*

Main category: cs.LG

TL;DR: Training against a chain-of-thought (CoT) monitor can lead to obfuscated CoTs that hide bad behavior; training only against output monitors without CoT access does not fully prevent obfuscation. Two mechanisms cause obfuscated CoTs under such setups: (1) safe-looking CoTs generalize to safe outputs, and (2) conditioning of later tokens on earlier safe-looking CoTs reinforces safe outputs. The authors propose two mitigations addressing these issues, achieving a Pareto improvement in monitorability and task performance compared with standard training.


<details>
  <summary>Details</summary>
Motivation: To ensure safety monitoring for large language models, it is insufficient to monitor only the final outputs; CoTs can be obfuscated so monitors fail to detect unsafe behavior. The goal is to maintain monitorability of CoTs or related safety signals without sacrificing task performance.

Method: The study identifies two obfuscation mechanisms when training with output-only monitors: (a) safe-looking CoTs can generalize to safe final outputs, and (b) the sequential nature of generation reinforces safe-looking CoTs, increasing the likelihood of safe outputs. It then proposes two mitigation strategies aimed at countering these mechanisms, and evaluates their impact on monitorability and task performance relative to regular training.

Result: The mitigations achieve a Pareto improvement: they improve monitorability (reducing obfuscated CoTs) while maintaining or improving task performance compared to regular training.

Conclusion: It's possible to train models with enhanced safety monitoring without sacrificing performance by applying targeted mitigations that address the two main obfuscation mechanisms. The work suggests a practical path to making CoT-based safety monitoring robust against obfuscation while preserving utility.

Abstract: OpenAI (2025) showed that training against a chain of thought (CoT) monitor can cause obfuscated CoTs, which contain bad behavior the monitor cannot detect. They proposed to keep CoTs monitorable by training only against output monitors that do not have access to CoT. We show that such training can still cause obfuscated CoTs via two mechanisms. First, when a model is trained to produce a safe-looking output, that model may generalize to making its CoTs look safe. Second, since later tokens are conditioned on earlier ones, safe-looking CoTs may increase the likelihood of safe outputs, causing safe-looking CoTs to be reinforced. We introduce two mitigations to address these two issues, which achieve a Pareto improvement in terms of monitorability and task performance compared to regular training.

</details>


### [445] [Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge](https://arxiv.org/abs/2511.11585)
*Kabir Khan,Manju Sarkar,Anita Kar,Suresh Ghosh*

Main category: cs.LG

TL;DR: FedGen-Edge federates only lightweight adapters atop a frozen global backbone using LoRA in cross-device federated learning to drastically cut communication, stabilize non-IID training, and enable personalization, with superior convergence and metrics on PTB and CIFAR-10 while keeping a simple FedAvg-style server.


<details>
  <summary>Details</summary>
Motivation: Training and adapting large generative models in cross-device federated settings is hindered by heavy compute/communication, system/statistical heterogeneity, and privacy constraints; there is a need for lightweight, personalized, resource-aware approaches.

Method: Decouple a frozen pre-trained global backbone from trainable client-side adapters; apply Low-Rank Adaptation (LoRA) to constrain updates to a compact subspace; federate only the adapters (not the full model) using a FedAvg-style server; perform ablations on LoRA rank and local epochs to study trade-offs.

Result: FedGen-Edge achieves lower perplexity/FID and faster convergence than strong baselines on PTB and CIFAR-10; uplink traffic is reduced by over 99% versus full-model FedAvg; aggregation remains stable under non-IID data; personalization is facilitated by local adapter tuning; ablations show diminishing returns beyond moderate LoRA rank and a trade-off between local epochs and client drift.

Conclusion: FedGen-Edge offers a practical pathway to privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices, with simple server design and strong empirical gains.

Abstract: Large generative models (for example, language and diffusion models) enable high-quality text and image synthesis but are hard to train or adapt in cross-device federated settings due to heavy computation and communication and statistical/system heterogeneity. We propose FedGen-Edge, a framework that decouples a frozen, pre-trained global backbone from lightweight client-side adapters and federates only the adapters. Using Low-Rank Adaptation (LoRA) constrains client updates to a compact subspace, which reduces uplink traffic by more than 99 percent versus full-model FedAvg, stabilizes aggregation under non-IID data, and naturally supports personalization because each client can keep a locally tuned adapter. On language modeling (PTB) and image generation (CIFAR-10), FedGen-Edge achieves lower perplexity/FID and faster convergence than strong baselines while retaining a simple FedAvg-style server. A brief ablation shows diminishing returns beyond moderate LoRA rank and a trade-off between local epochs and client drift. FedGen-Edge offers a practical path toward privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices.

</details>


### [446] [WildfireGenome: Interpretable Machine Learning Reveals Local Drivers of Wildfire Risk and Their Cross-County Variation](https://arxiv.org/abs/2511.11589)
*Chenyue Liu,Ali Mostafavi*

Main category: cs.LG

TL;DR: WildfireGenome integrates seven federal indicators into a PCA-based risk label, uses Random Forest for local risk classification, and SHAP/ICE/PDP for interpretable driver analysis; achieves solid accuracy, with transfer limited to ecologically similar regions.


<details>
  <summary>Details</summary>
Motivation: Address the lack of interpretability and decision-scale utility in coarse hazard maps and opaque ML models for wildfire risk.

Method: 1) Fuse seven indicators into a sign-aligned PCA-based risk label at H3 Level-8; 2) train Random Forest classifier for local risk; 3) apply SHAP, ICE, PDP for explanations; evaluate across seven ecologically diverse U.S. counties; perform transfer tests.

Result: Accuracy 0.755-0.878; Quadratic Weighted Kappa up to 0.951; PCs explain 87-94% of indicator variance; transfer reliable between ecologically similar regions, collapses across dissimilar contexts; dominant drivers are needleleaf forest cover and elevation, with risk rising sharply at 30-40% needleleaf.

Conclusion: Advances wildfire risk assessment from regional prediction to interpretable, decision-scale analytics that can guide vegetation management, zoning, and infrastructure planning.

Abstract: Current wildfire risk assessments rely on coarse hazard maps and opaque machine learning models that optimize regional accuracy while sacrificing interpretability at the decision scale. WildfireGenome addresses these gaps through three components: (1) fusion of seven federal wildfire indicators into a sign-aligned, PCA-based composite risk label at H3 Level-8 resolution; (2) Random Forest classification of local wildfire risk; and (3) SHAP and ICE/PDP analyses to expose county-specific nonlinear driver relationships. Across seven ecologically diverse U.S. counties, models achieve accuracies of 0.755-0.878 and Quadratic Weighted Kappa up to 0.951, with principal components explaining 87-94% of indicator variance. Transfer tests show reliable performance between ecologically similar regions but collapse across dissimilar contexts. Explanations consistently highlight needleleaf forest cover and elevation as dominant drivers, with risk rising sharply at 30-40% needleleaf coverage. WildfireGenome advances wildfire risk assessment from regional prediction to interpretable, decision-scale analytics that guide vegetation management, zoning, and infrastructure planning.

</details>


### [447] [Mind Your Entropy: From Maximum Entropy to Trajectory Entropy-Constrained RL](https://arxiv.org/abs/2511.11592)
*Guojian Zhan,Likun Wang,Pengcheng Wang,Feihong Zhang,Jingliang Duan,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: TECRL decouples reward and entropy learning to stabilize Q-value targets and enforces a trajectory-level entropy constraint via a dedicated entropy Q-function, enabling long-horizon control of stochasticity. DSAC-E, built on DSAC-T with refinements, shows improved returns and stability on OpenAI Gym.


<details>
  <summary>Details</summary>
Motivation: In maximum-entropy RL, joint entropy weighting causes non-stationary Q-value estimates, and short-sighted entropy tuning based on single-step entropy fails to account for cumulative effects on policy stochasticity over time.

Method: Introduce a TECRL framework with two separate Q-functions: one for reward and one for entropy, ensuring clean targets unaffected by temperature updates. A dedicated entropy Q-function quantifies expected cumulative entropy to enforce trajectory entropy constraints. Implement DSAC-E by extending DSAC-T (distributional soft actor-critic) with three refinements.

Result: Empirical evaluations on OpenAI Gym show higher returns and improved stability for DSAC-E compared to baselines.

Conclusion: TECRL provides a principled approach to stabilize value targets and manage long-term policy stochasticity via trajectory-level entropy constraints. DSAC-E demonstrates the practical viability and performance gains of this framework on standard benchmarks.

Abstract: Maximum entropy has become a mainstream off-policy reinforcement learning (RL) framework for balancing exploitation and exploration. However, two bottlenecks still limit further performance improvement: (1) non-stationary Q-value estimation caused by jointly injecting entropy and updating its weighting parameter, i.e., temperature; and (2) short-sighted local entropy tuning that adjusts temperature only according to the current single-step entropy, without considering the effect of cumulative entropy over time. In this paper, we extends maximum entropy framework by proposing a trajectory entropy-constrained reinforcement learning (TECRL) framework to address these two challenges. Within this framework, we first separately learn two Q-functions, one associated with reward and the other with entropy, ensuring clean and stable value targets unaffected by temperature updates. Then, the dedicated entropy Q-function, explicitly quantifying the expected cumulative entropy, enables us to enforce a trajectory entropy constraint and consequently control the policy long-term stochasticity. Building on this TECRL framework, we develop a practical off-policy algorithm, DSAC-E, by extending the state-of-the-art distributional soft actor-critic with three refinements (DSAC-T). Empirical results on the OpenAI Gym benchmark demonstrate that our DSAC-E can achieve higher returns and better stability.

</details>


### [448] [Sound Logical Explanations for Mean Aggregation Graph Neural Networks](https://arxiv.org/abs/2511.11593)
*Matthew Morris,Ian Horrocks*

Main category: cs.LG

TL;DR: MAGNNs with non-negative weights have a precisely characterized monotonic rule class and a restricted logic for explanations, achieving competitive performance in inductive KG completion with practical, inspectable explanations.


<details>
  <summary>Details</summary>
Motivation: To address explainability and expressivity gaps for mean-aggregation GNNs (MAGNNs) in knowledge graph completion by formalizing what monotonic rules they can soundly implement and by providing a logic-based explanation framework.

Method: The authors theoretically characterize the exact monotonic rule class sound for MAGNNs and introduce a restricted fragment of first-order logic to explain any MAGNN prediction; they validate the approach with experiments on inductive KG benchmarks to assess performance and the usefulness of explanations.

Result: They identify the precise monotonic rule class that MAGNNs can reliably implement and propose a restricted logic that can express any MAGNN prediction; empirically, non-negative weights yield competitive/improved performance, sound rules are obtainable in practice, explanations are insightful, and the explanations can reveal issues in trained models.

Conclusion: Restricting MAGNNs to non-negative weights yields interpretable and verifiable predictions without sacrificing performance, enabling sound explanations and practical insights for KG completion.

Abstract: Graph neural networks (GNNs) are frequently used for knowledge graph completion. Their black-box nature has motivated work that uses sound logical rules to explain predictions and characterise their expressivity. However, despite the prevalence of GNNs that use mean as an aggregation function, explainability and expressivity results are lacking for them. We consider GNNs with mean aggregation and non-negative weights (MAGNNs), proving the precise class of monotonic rules that can be sound for them, as well as providing a restricted fragment of first-order logic to explain any MAGNN prediction. Our experiments show that restricting mean-aggregation GNNs to have non-negative weights yields comparable or improved performance on standard inductive benchmarks, that sound rules are obtained in practice, that insightful explanations can be generated in practice, and that the sound rules can expose issues in the trained models.

</details>


### [449] [Loss Given Default Prediction Under Measurement-Induced Mixture Distributions: An Information-Theoretic Approach](https://arxiv.org/abs/2511.11596)
*Javier Marín*

Main category: cs.LG

TL;DR: Mixture data (proxy estimates + actual outcomes) breaks tree-based LGD models; information-theoretic methods generalize better and reveal leverage as a richer signal than size; has Basel III/regulatory implications and cross-domain relevance.


<details>
  <summary>Details</summary>
Motivation: Data quality constraint: 90% proxies vs actual recovery outcomes hinders reliable LGD prediction; improving modeling under Basel III is critical; reveals misalignment with traditional scale-based assumptions.

Method: Compare recursive partitioning (Random Forest) vs information-theoretic models using Shannon entropy and mutual information on 1,218 corporate bankruptcies (1980-2023); quantify generalization (r^2, RMSE); analyze feature information content (leverage, size).

Result: Random Forest r^2 = -0.664 on held-out data; info-theoretic approach r^2 = 0.191 and RMSE = 0.284; leverage features contribute 1.510 bits of mutual information; size contributes 0.086 bits.

Conclusion: Information-theoretic methods provide practical guidance for deploying LGD models under data scarcity; findings challenge assumptions about scale-dependent recovery and generalize to other domains with mixture training data.

Abstract: Loss Given Default (LGD) modeling faces a fundamental data quality constraint: 90% of available training data consists of proxy estimates based on pre-distress balance sheets rather than actual recovery outcomes from completed bankruptcy proceedings. We demonstrate that this mixture-contaminated training structure causes systematic failure of recursive partitioning methods, with Random Forest achieving negative r-squared (-0.664, worse than predicting the mean) on held-out test data. Information-theoretic approaches based on Shannon entropy and mutual information provide superior generalization, achieving r-squared of 0.191 and RMSE of 0.284 on 1,218 corporate bankruptcies (1980-2023). Analysis reveals that leverage-based features contain 1.510 bits of mutual information while size effects contribute only 0.086 bits, contradicting regulatory assumptions about scale-dependent recovery. These results establish practical guidance for financial institutions deploying LGD models under Basel III requirements when representative outcome data is unavailable at sufficient scale. The findings generalize to medical outcomes research, climate forecasting, and technology reliability-domains where extended observation periods create unavoidable mixture structure in training data.

</details>


### [450] [Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part A: Stochastic Stability in Non-zero-Sum Games](https://arxiv.org/abs/2511.11602)
*Georgios C. Chasparis*

Main category: cs.LG

TL;DR: APLA: a payoff-based, aspiration-augmented learning scheme for distributed multi-player optimization that extends reinforcement learning to handle noisy observations and allows stochastic stability analysis beyond potential/coordination games, including weakly acyclic games.


<details>
  <summary>Details</summary>
Motivation: Address convergence issues of distributed reinforcement learning in multi-player settings where independent learners may fail to converge to pure Nash, especially beyond potential/coordination games and under noise.

Method: Propose aspiration-based perturbed learning automata (APLA) where each player's action-selection probabilities are reinforced by both repeated selection and an aspiration (satisfaction) level; analyze stochastic stability.

Result: Provide a stochastic stability analysis of APLA in multi-player positive-utility games with noisy observations, establishing equivalence between an infinite-dimensional Markov chain and a finite-dimensional one; sets up analysis for generic non-zero-sum games and specializes to weakly acyclic games.

Conclusion: This work lays the groundwork (first part) for a broader characterization of stochastic stability in non-zero-sum games; the second part further specializes the results to weakly acyclic games.

Abstract: Reinforcement-based learning has attracted considerable attention both in modeling human behavior as well as in engineering, for designing measurement- or payoff-based optimization schemes. Such learning schemes exhibit several advantages, especially in relation to filtering out noisy observations. However, they may exhibit several limitations when applied in a distributed setup. In multi-player weakly-acyclic games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. To address this main limitation, this paper introduces a novel payoff-based learning scheme for distributed optimization, namely aspiration-based perturbed learning automata (APLA). In this class of dynamics, and contrary to standard reinforcement-based learning schemes, each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This is the first part of the paper that characterizes stochastic stability in generic non-zero-sum games by establishing equivalence of the induced infinite-dimensional Markov chain with a finite dimensional one. In the second part, stochastic stability is further specialized to weakly acyclic games.

</details>


### [451] [Enhancing failure prediction in nuclear industry: Hybridization of knowledge- and data-driven techniques](https://arxiv.org/abs/2511.11604)
*Amaratou Mahamadou Saley,Thierry Moyaux,Aïcha Sekhari,Vincent Cheutet,Jean-Baptiste Danielou*

Main category: cs.LG

TL;DR: Hybrid predictive maintenance in the nuclear industry outperforms purely data-driven methods by incorporating domain knowledge, extending the prediction horizon from 3h to 24h and boosting the F1 score from 56.36% to 93.12% in a real-world case study.


<details>
  <summary>Details</summary>
Motivation: Purely data-driven approaches face limitations in the nuclear sector due to system complexity, safety, environmental, and economic constraints. Integrating domain knowledge can enhance model accuracy, reliability, and applicability.

Method: Proposes a two-level methodological framework: (1) delineate the limitations of purely data-driven approaches, and (2) fuse domain knowledge of nuclear equipment with data-driven techniques. Evaluation via a real-world case study comparing current monitoring with two scenarios: purely data-driven vs hybrid.

Result: The hybrid approach significantly outperforms pure data-driven methods, achieving a 24-hour prediction horizon and an F1 score of 93.12%, versus 3 hours and 56.36% F1 for the purely data-driven method.

Conclusion: Integrating domain knowledge with data-driven methods yields superior predictive performance and practical value in the sensitive nuclear domain, enabling earlier maintenance actions and potential downtime reductions.

Abstract: The convergence of the Internet of Things (IoT) and Industry 4.0 has significantly enhanced data-driven methodologies within the nuclear industry, notably enhancing safety and economic efficiency. This advancement challenges the precise prediction of future maintenance needs for assets, which is crucial for reducing downtime and operational costs. However, the effectiveness of data-driven methodologies in the nuclear sector requires extensive domain knowledge due to the complexity of the systems involved. Thus, this paper proposes a novel predictive maintenance methodology that combines data-driven techniques with domain knowledge from a nuclear equipment. The methodological originality of this paper is located on two levels: highlighting the limitations of purely data-driven approaches and demonstrating the importance of knowledge in enhancing the performance of the predictive models. The applicative novelty of this work lies in its use within a domain such as a nuclear industry, which is highly restricted and ultrasensitive due to security, economic and environmental concerns. A detailed real-world case study which compares the current state of equipment monitoring with two scenarios, demonstrate that the methodology significantly outperforms purely data-driven methods in failure prediction. While purely data-driven methods achieve only a modest performance with a prediction horizon limited to 3 h and a F1 score of 56.36%, the hybrid approach increases the prediction horizon to 24 h and achieves a higher F1 score of 93.12%.

</details>


### [452] [Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement Learning](https://arxiv.org/abs/2511.11607)
*Guoqing Ma,Yuhan Zhang,Yuming Dai,Guangfu Hao,Yang Chen,Shan Yu*

Main category: cs.LG

TL;DR: Introduces the COWM layer for reinforcement learning to tackle non-stationarity, stabilizing training and boosting sample efficiency; shows 9%/12.6% gains on DMControl benchmarks.


<details>
  <summary>Details</summary>
Motivation: Non-stationary environments in RL degrade learning efficiency and require many iterations; a method to stabilize learning and reduce gradient interference is highly desirable.

Method: Proposes the Clustering Orthogonal Weight Modified (COWM) layer, which can be inserted into any policy network. It uses clustering techniques and a projection matrix to mitigate non-stationarity and reduce gradient interference during training.

Result: Empirical evaluation shows COWM outperforms state-of-the-art methods, achieving 9% improvement on vision-based DMControl and 12.6% on state-based DMControl, with robustness across algorithms and tasks.

Conclusion: COWM improves learning speed and robustness to non-stationarity, demonstrating generality across different RL algorithms and tasks.

Abstract: Reinforcement learning (RL) has made significant advancements, achieving superhuman performance in various tasks. However, RL agents often operate under the assumption of environmental stationarity, which poses a great challenge to learning efficiency since many environments are inherently non-stationary. This non-stationarity results in the requirement of millions of iterations, leading to low sample efficiency. To address this issue, we introduce the Clustering Orthogonal Weight Modified (COWM) layer, which can be integrated into the policy network of any RL algorithm and mitigate non-stationarity effectively. The COWM layer stabilizes the learning process by employing clustering techniques and a projection matrix. Our approach not only improves learning speed but also reduces gradient interference, thereby enhancing the overall learning efficiency. Empirically, the COWM outperforms state-of-the-art methods and achieves improvements of 9% and 12.6% in vision based and state-based DMControl benchmark. It also shows robustness and generality across various algorithms and tasks.

</details>


### [453] [Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models](https://arxiv.org/abs/2511.11622)
*Alexis Roger,Gwen Legate,Kashif Rasul,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.LG

TL;DR: Tokenizer design and transfer learning jointly shape time series foundation models: tokenizer config largely sets capacity and stability, while pretraining impacts optimization efficiency and alignment. Pretrained models benefit most from well-designed, small vocabularies, and misaligned tokenization can negate pretraining gains; guidance favors small, efficient vocabularies with pretrained weights, especially for multi-modal forecasting.


<details>
  <summary>Details</summary>
Motivation: Understand how tokenization (scaling/quantization) and pretraining interact to affect performance, stability, and optimization in discrete representations of continuous time series signals, with implications for multi-modal forecasting.

Method: Empirical training experiments and theoretical analyses comparing tokenizer scaling/quantization strategies and initialization (pretrained vs random). Assess performance across vocabulary sizes, analyze stability and representational capacity, and evaluate transfer learning benefits, especially in multi-modal/fusion scenarios.

Result: Pretrained models more effectively leverage well-designed tokenizers, particularly at small vocabularies; tokenizer configuration governs representational capacity and training stability, while transfer learning improves optimization efficiency and alignment. Misaligned tokenization can reduce or reverse pretraining advantages. Small, efficient vocabularies combined with pretrained weights yield strong benefits in multi-modal forecasting where a shared vocabulary is needed.

Conclusion: Careful tokenizer design and prudent use of transfer learning are crucial for discrete representation learning in continuous-signal time series. The findings offer practical guidance: use small, efficient token vocabularies and leverage pretrained weights to maximize performance and stability, especially in multi-modal settings.

Abstract: Tokenization and transfer learning are two critical components in building state of the art time series foundation models for forecasting. In this work, we systematically study the effect of tokenizer design, specifically scaling and quantization strategies, on model performance, alongside the impact of pretraining versus random initialization. We show that tokenizer configuration primarily governs the representational capacity and stability of the model, while transfer learning influences optimization efficiency and alignment. Using a combination of empirical training experiments and theoretical analyses, we demonstrate that pretrained models consistently leverage well-designed tokenizers more effectively, particularly at smaller vocabulary sizes. Conversely, misaligned tokenization can diminish or even invert the benefits of pretraining. These findings highlight the importance of careful tokenization in time series modeling and suggest that combining small, efficient vocabularies with pretrained weights is especially advantageous in multi-modal forecasting settings, where the overall vocabulary must be shared across modalities. Our results provide concrete guidance for designing tokenizers and leveraging transfer learning in discrete representation learning for continuous signals.

</details>


### [454] [Early GVHD Prediction in Liver Transplantation via Multi-Modal Deep Learning on Imbalanced EHR Data](https://arxiv.org/abs/2511.11623)
*Yushan Jiang,Shuteng Niu,Dongjin Song,Yichen Wang,Jingna Feng,Xinyue Hu,Liu Yang,Cui Tao*

Main category: cs.LG

TL;DR: A multi-modal deep learning framework using pre-transplant EHR data from 2,100 liver transplant patients (42 GVHD) with four modalities, achieving improved early GVHD prediction under data heterogeneity and extreme imbalance, with AUC 0.836, AUPRC 0.157, recall 0.768, specificity 0.803.


<details>
  <summary>Details</summary>
Motivation: GVHD after liver transplantation is rare but often fatal; early prediction could enable timely intervention. Real-world EHR are heterogeneous with missing data and severe class imbalance, limiting traditional models. A multi-modal approach may leverage complementary information across data types to improve performance.

Method: Developed a dynamic multi-modal fusion framework that integrates demographics, laboratory tests, diagnoses, and medications from pre-transplant EHR. The model handles irregular records with missing values and uses AUC-based optimization to address class imbalance. Evaluated on Mayo Clinic data (1992–2025) with 2,100 patients (42 GVHD) and compared against single-modal and multi-modal baselines.

Result: The framework outperformed baselines, achieving AUC 0.836, AUPRC 0.157, recall 0.768, and specificity 0.803. It demonstrates that combining modalities captures complementary information, and is robust to missing data and extreme imbalance.

Conclusion: The proposed multi-modal DL method substantially improves early GVHD prediction in liver transplantation in real-world, heterogeneous EHR settings, showing promise for timely intervention despite severe class imbalance.

Abstract: Graft-versus-host disease (GVHD) is a rare but often fatal complication in liver transplantation, with a very high mortality rate. By harnessing multi-modal deep learning methods to integrate heterogeneous and imbalanced electronic health records (EHR), we aim to advance early prediction of GVHD, paving the way for timely intervention and improved patient outcomes. In this study, we analyzed pre-transplant electronic health records (EHR) spanning the period before surgery for 2,100 liver transplantation patients, including 42 cases of graft-versus-host disease (GVHD), from a cohort treated at Mayo Clinic between 1992 and 2025. The dataset comprised four major modalities: patient demographics, laboratory tests, diagnoses, and medications. We developed a multi-modal deep learning framework that dynamically fuses these modalities, handles irregular records with missing values, and addresses extreme class imbalance through AUC-based optimization. The developed framework outperforms all single-modal and multi-modal machine learning baselines, achieving an AUC of 0.836, an AUPRC of 0.157, a recall of 0.768, and a specificity of 0.803. It also demonstrates the effectiveness of our approach in capturing complementary information from different modalities, leading to improved performance. Our multi-modal deep learning framework substantially improves existing approaches for early GVHD prediction. By effectively addressing the challenges of heterogeneity and extreme class imbalance in real-world EHR, it achieves accurate early prediction. Our proposed multi-modal deep learning method demonstrates promising results for early prediction of a GVHD in liver transplantation, despite the challenge of extremely imbalanced EHR data.

</details>


### [455] [MedFedPure: A Medical Federated Framework with MAE-based Detection and Diffusion Purification for Inference-Time Attacks](https://arxiv.org/abs/2511.11625)
*Mohammad Karami,Mohammad Reza Nemati,Aidin Kazemi,Ali Mikaeili Barzili,Hamid Azadegan,Behzad Moshiri*

Main category: cs.LG

TL;DR: MedFedPure is a personalized federated learning defense for brain tumor MRI AI, combining a personalized FL model, a Masked Autoencoder for suspicious-input detection, and an adaptive diffusion purification module to clean only flagged scans, yielding strong adversarial robustness with high clean accuracy on Br35H.


<details>
  <summary>Details</summary>
Motivation: Federated learning in medical imaging protects privacy but is vulnerable to inference-time adversarial attacks. The decentralized and heterogeneous nature of clinical data makes centralized defenses impractical; there is a need for defenses tailored to federated, privacy-preserving medical settings.

Method: 1) A personalized FL model that adapts to local data distributions of each institution. 2) A Masked Autoencoder (MAE) to detect suspicious inputs by exposing hidden perturbations. 3) An adaptive diffusion-based purification module that selectively cleans flagged scans before classification.

Result: On the Br35H brain MRI dataset, adversarial robustness improved from 49.50% to 87.33% under strong attacks, while clean accuracy remained high at 97.67%.

Conclusion: MedFedPure enables real-time, privacy-preserving defenses in clinical workflows by combining personalization, input detection, and selective purification, offering robust protection without sacrificing diagnostic accuracy.

Abstract: Artificial intelligence (AI) has shown great potential in medical imaging, particularly for brain tumor detection using Magnetic Resonance Imaging (MRI). However, the models remain vulnerable at inference time when they are trained collaboratively through Federated Learning (FL), an approach adopted to protect patient privacy. Adversarial attacks can subtly alter medical scans in ways invisible to the human eye yet powerful enough to mislead AI models, potentially causing serious misdiagnoses. Existing defenses often assume centralized data and struggle to cope with the decentralized and diverse nature of federated medical settings. In this work, we present MedFedPure, a personalized federated learning defense framework designed to protect diagnostic AI models at inference time without compromising privacy or accuracy. MedFedPure combines three key elements: (1) a personalized FL model that adapts to the unique data distribution of each institution; (2) a Masked Autoencoder (MAE) that detects suspicious inputs by exposing hidden perturbations; and (3) an adaptive diffusion-based purification module that selectively cleans only the flagged scans before classification. Together, these steps offer robust protection while preserving the integrity of normal, benign images. We evaluated MedFedPure on the Br35H brain MRI dataset. The results show a significant gain in adversarial robustness, improving performance from 49.50% to 87.33% under strong attacks, while maintaining a high clean accuracy of 97.67%. By operating locally and in real time during diagnosis, our framework provides a practical path to deploying secure, trustworthy, and privacy-preserving AI tools in clinical workflows.
  Index Terms: cancer, tumor detection, federated learning, masked autoencoder, diffusion, privacy

</details>


### [456] [SA-EMO: Structure-Aligned Encoder Mixture of Operators for Generalizable Full-waveform Inversion](https://arxiv.org/abs/2511.11627)
*Wang Zhenyu,Li Peiyuan,Shi Yongxiang,Wu Ruoyu,Zhang Lei*

Main category: cs.LG

TL;DR: SA-EMO introduces a structure-aligned encoder and a mixture-of-operators with adaptive routing for velocity-field inversion, addressing generalization and ill-posedness in FWI. It maps wavefields into a physics-consistent latent space and fuses multiple operators (spectral, wavelet, multiscale, local) to predict velocity models, achieving large MAE reductions and improved boundary resolution on OpenFWI and Marmousi2.


<details>
  <summary>Details</summary>
Motivation: Single CNN or single-operator DL methods in FWI struggle to generalize across unknown subsurface structures and diverse geological types, and current approaches remain ill-posed and computationally heavy. There is a need for a physics-consistent latent representation and a flexible, multi-operator framework to improve accuracy and robustness.

Method: Introduce a structure-aligned encoder that maps high-dimensional seismic wavefields into a physically consistent latent space, reducing spatio-temporal mismatch and recovering high-frequency components. Develop an adaptive routing mechanism that selects and fuses multiple neural-operator experts (spectral, wavelet, multiscale, local) to predict the velocity model. Train/evaluate on OpenFWI benchmark and Marmousi2 dataset, with ablation studies to assess contribution of each component.

Result: SA-EMO significantly outperforms traditional CNNs or single-operator approaches, achieving an average MAE reduction of ~58.443% and boundary-resolution improvement of ~10.308%. Ablations indicate that the structure-aligned encoder, expert-fusion mechanism, and routing module each provide notable performance gains.

Conclusion: SA-EMO establishes a new paradigm for efficient, scalable, and physically interpretable FWI by combining structure-aware latent representations with a mixture-of-operators framework and adaptive routing, improving generalization and inversion quality in complex subsurface settings.

Abstract: Full-waveform inversion (FWI) can produce high-resolution subsurface models, yet it remains inherently ill-posed, highly nonlinear, and computationally intensive. Although recent deep learning and numerical acceleration methods have improved speed and scalability, they often rely on single CNN architectures or single neural operators, which struggle to generalize in unknown or complex geological settings and are ineffective at distinguishing diverse geological types. To address these issues, we propose a Structure-Aligned Encoder-Mixture-of-Operators (SA-EMO) architecture for velocity-field inversion under unknown subsurface structures. First, a structure-aligned encoder maps high-dimensional seismic wavefields into a physically consistent latent space, thereby eliminating spatio-temporal mismatch between the waveform and velocity domains, recovering high-frequency components, and enhancing feature generalization. Then, an adaptive routing mechanism selects and fuses multiple neural-operator experts, including spectral, wavelet, multiscale, and local operators, to predict the velocity model. We systematically evaluate our approach on the OpenFWI benchmark and the Marmousi2 dataset. Results show that SA-EMO significantly outperforms traditional CNN or single-operator methods, achieving an average MAE reduction of approximately 58.443% and an improvement in boundary resolution of about 10.308%. Ablation studies further reveal that the structure-aligned encoder, the expert-fusion mechanism, and the routing module each contribute markedly to the performance gains. This work introduces a new paradigm for efficient, scalable, and physically interpretable full-waveform inversion.

</details>


### [457] [Global Feature Enhancing and Fusion Framework for Strain Gauge Time Series Classification](https://arxiv.org/abs/2511.11629)
*Xu Zhang,Peng Wang,Chen Wang,Zhe Xu,Xiaohua Nie,Wei Wang*

Main category: cs.LG

TL;DR: A hypergraph-based global feature learning and fusion framework for SGS time series improves recognition by combining engineered global features with high-order relationships among local features, outperforming CNN-only approaches on SGS and UCR datasets.


<details>
  <summary>Details</summary>
Motivation: CNNs mainly capture local features; SGS sequences can be globally similar, so local features may be insufficient. There is a need for global representations to improve robustness and generalization in IoT-based manufacturing.

Method: 1) Construct global features through feature engineering. 2) Learn high-order relationships between local features using a hypergraph-based framework. 3) Fuse global and local features to achieve semantic consistency. 4) Validate on industrial SGS data and public UCR SGS datasets.

Result: The approach achieves better generalization to unseen data and improved recognition accuracy on SGS time series across industrial and public datasets.

Conclusion: A hypergraph-based global feature learning and fusion strategy effectively captures global structure and high-order relationships, improving SGS time-series recognition and robustness in IoT-enabled manufacturing.

Abstract: Strain Gauge Status (SGS) recognition is crucial in the field of intelligent manufacturing based on the Internet of Things, as accurate identification helps timely detection of failed mechanical components, avoiding accidents. The loading and unloading sequences generated by strain gauges can be identified through time series classification (TSC) algorithms. Recently, deep learning models, e.g., convolutional neural networks (CNNs) have shown remarkable success in the TSC task, as they can extract discriminative local features from the subsequences to identify the time series. However, we observe that only the local features may not be sufficient for expressing the time series, especially when the local sub-sequences between different time series are very similar, e.g., SGS data of aircraft wings in static strength experiments. Nevertheless, CNNs suffer from the limitation in extracting global features due to the nature of convolution operations. For extracting global features to more comprehensively represent the SGS time series, we propose two insights: (i) Constructing global features through feature engineering. (ii) Learning high-order relationships between local features to capture global features. To realize and utilize them, we propose a hypergraph-based global feature learning and fusion framework, which learns and fuses global features for semantic consistency to enhance the representation of SGS time series, thereby improving recognition accuracy. Our method designs are validated on industrial SGS and public UCR datasets, showing better generalization for unseen data in SGS recognition.

</details>


### [458] [Predicting Grain Growth in Polycrystalline Materials Using Deep Learning Time Series Models](https://arxiv.org/abs/2511.11630)
*Eliane Younes,Elie Hachem,Marc Bernacki*

Main category: cs.LG

TL;DR: LSTM-based forecasting using mean-field descriptors achieves high accuracy and speed for grain growth predictions; superior to RNN, TCN, and transformers, enabling efficient digital twins.


<details>
  <summary>Details</summary>
Motivation: Grain growth governs mechanical behavior; full-field simulations are costly; need fast, accurate microstructure forecasting using low-dimensional descriptors.

Method: Compare RNN, LSTM, TCN, Transformer on 120 grain growth sequences; use mean-field descriptors from high-fidelity simulations; recursive forecasting to predict future distributions from short history; train models to output normalized grain size distributions.

Result: LSTM outperforms others with >90% accuracy; stable long-horizon predictions; computational time drastically reduced (20 minutes to a few seconds per sequence); other architectures show divergence.

Conclusion: Low-dimensional descriptors with LSTM forecasting offer efficient, accurate microstructure prediction and support digital twin development and process optimization.

Abstract: Grain Growth strongly influences the mechanical behavior of materials, making its prediction a key objective in microstructural engineering. In this study, several deep learning approaches were evaluated, including recurrent neural networks (RNN), long short-term memory (LSTM), temporal convolutional networks (TCN), and transformers, to forecast grain size distributions during grain growth. Unlike full-field simulations, which are computationally demanding, the present work relies on mean-field statistical descriptors extracted from high-fidelity simulations. A dataset of 120 grain growth sequences was processed into normalized grain size distributions as a function of time. The models were trained to predict future distributions from a short temporal history using a recursive forecasting strategy. Among the tested models, the LSTM network achieved the highest accuracy (above 90\%) and the most stable performance, maintaining physically consistent predictions over extended horizons while reducing computation time from about 20 minutes per sequence to only a few seconds, whereas the other architectures tended to diverge when forecasting further in time. These results highlight the potential of low-dimensional descriptors and LSTM-based forecasting for efficient and accurate microstructure prediction, with direct implications for digital twin development and process optimization.

</details>


### [459] [Toward Better Generalization in Few-Shot Learning through the Meta-Component Combination](https://arxiv.org/abs/2511.11632)
*Qiuhao Zeng*

Main category: cs.LG

TL;DR: Represents classifiers as combinations of orthogonally regularized meta-components learned across episodes to improve few-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Metric-based meta-learning often overfits to seen classes, hurting generalization to unseen classes in few-shot settings. A more flexible, diverse substructure for classifiers could capture shared patterns across tasks.

Method: Learn each classifier as a linear combination of meta-components; meta-components are learned across meta-learning episodes on seen classes and disentangled by an orthogonal regularizer to promote diversity and capture shared substructures.

Result: Experiments on standard few-shot benchmarks show superior performance of the proposed method compared with baselines.

Conclusion: Decomposing classifiers into diverse, orthogonally regularized meta-components enhances generalization in few-shot learning.

Abstract: In few-shot learning, classifiers are expected to generalize to unseen classes given only a small number of instances of each new class. One of the popular solutions to few-shot learning is metric-based meta-learning. However, it highly depends on the deep metric learned on seen classes, which may overfit to seen classes and fail to generalize well on unseen classes. To improve the generalization, we explore the substructures of classifiers and propose a novel meta-learning algorithm to learn each classifier as a combination of meta-components. Meta-components are learned across meta-learning episodes on seen classes and disentangled by imposing an orthogonal regularizer to promote its diversity and capture various shared substructures among different classifiers. Extensive experiments on few-shot benchmark tasks show superior performances of the proposed method.

</details>


### [460] [An Explainable and Fair AI Tool for PCOS Risk Assessment: Calibration, Subgroup Equity, and Interactive Clinical Deployment](https://arxiv.org/abs/2511.11636)
*Asma Sadia Khan,Sadia Tabassum*

Main category: cs.LG

TL;DR: A fairness-audited, interpretable ML pipeline for PCOS prediction that combines SHAP explanations with demographic audits, calibrated across subgroups, and deployed via a Streamlit UI. Random Forest offers the best balance of calibration and interpretability, while subgroup analyses reveal age- and phenotype-related disparities.


<details>
  <summary>Details</summary>
Motivation: To create a clinically usable PCOS predictor that (i) provides interpretable attributions and (ii) monitors performance disparities across patient subgroups, ensuring reliable risk predictions for diverse populations.

Method: Train multiple models (Random Forest, SVM, XGBoost) with isotonic and Platt calibration; use SHAP for feature attributions; perform demographic audits to link explanations with disparities; evaluate using Brier Score and Expected Calibration Error (ECE); conduct subgroup analyses by age and obesity; develop a Streamlit web interface for real-time risk assessment and what-if analysis.

Result: Calibrated RF achieved 90.8% accuracy; SHAP highlighted follicle count, weight gain, and menstrual irregularity as key features consistent with Rotterdam criteria. SVM with isotonic calibration had the lowest ECE (0.0541); RF offered a better calibration-interpretability balance (Brier 0.0678, ECE 0.0666). Age subgroup 25–35 performed best (90.9%); under 25 underperformed (69.2%). Obese women had perfect precision; lean PCOS cases showed high recall. A Streamlit interface enables real-time risk assessment and interactive analyses.

Conclusion: RF was selected for detailed fairness and SHAP analyses due to its balance of calibration and interpretability. The framework connects predictive performance with observed diagnostic disparities and supports clinical usability through a Streamlit interface, bridging AI research and practice.

Abstract: This paper presents a fairness-audited and interpretable machine learning framework for predicting polycystic ovary syndrome (PCOS), designed to evaluate model performance and identify diagnostic disparities across patient subgroups. The framework integrated SHAP-based feature attributions with demographic audits to connect predictive explanations with observed disparities for actionable insights. Probabilistic calibration metrics (Brier Score and Expected Calibration Error) are incorporated to ensure reliable risk predictions across subgroups. Random Forest, SVM, and XGBoost models were trained with isotonic and Platt scaling for calibration and fairness comparison. A calibrated Random Forest achieved a high predictive accuracy of 90.8%. SHAP analysis identified follicle count, weight gain, and menstrual irregularity as the most influential features, which are consistent with the Rotterdam diagnostic criteria. Although the SVM with isotonic calibration achieved the lowest calibration error (ECE = 0.0541), the Random Forest model provided a better balance between calibration and interpretability (Brier = 0.0678, ECE = 0.0666). Therefore, it was selected for detailed fairness and SHAP analyses. Subgroup analysis revealed that the model performed best among women aged 25-35 (accuracy 90.9%) but underperformed in those under 25 (69.2%), highlighting age-related disparities. The model achieved perfect precision in obese women and maintained high recall in lean PCOS cases, demonstrating robustness across phenotypes. Finally, a Streamlit-based web interface enables real-time PCOS risk assessment, Rotterdam criteria evaluation, and interactive 'what-if' analysis, bridging the gap between AI research and clinical usability.

</details>


### [461] [Enhancing PINN Accuracy for the RLW Equation: Adaptive and Conservative Approaches](https://arxiv.org/abs/2511.11638)
*Aamir Shehzad*

Main category: cs.LG

TL;DR: Adaptive PINN with self-adaptive loss weighting yields best accuracy for complex RLW soliton interactions; conservative PINN best for long-term single-soliton and undular bore; enforcing conservation may hinder optimization in highly nonlinear systems; all methods achieve ~1e-5 accuracy, indicating mesh-free solutions and problem-specific design guidelines.


<details>
  <summary>Details</summary>
Motivation: Standard PINNs underperform on the regularized long wave (RLW) equation; this work evaluates whether adaptive loss weighting and explicit conservation constraints improve performance, and under what problem types.

Method: Two enhanced PINN approaches are proposed: (1) adaptive self-adaptive loss weighting; (2) conservative approach enforcing explicit conservation laws. They are tested on three RLW benchmarks—single soliton propagation, two-soliton collision, and evolution of an undular bore up to t=250—comparing against a standard PINN and against numerical solutions.

Result: Adaptive PINN outperforms the conservative and standard PINNs in the two-soliton interaction case; conservative PINN excels in long-term behavior of single solitons and undular bores. Enforcing conservation can be detrimental for highly nonlinear systems. All methods achieve accuracy within approximately 1e-5 of numerical solutions, demonstrating mesh-free, accurate RLW solutions and highlighting problem-specific performance.

Conclusion: Conservation enforcement is not universally beneficial for PINNs; training strategies should be tailored to the problem type. The study provides guidelines for choosing PINN variants depending on the RLW problem (nonlinear interactions vs. long-term dynamics) and confirms the viability of mesh-free PINNs for complex PDEs.

Abstract: Standard physics-informed neural network implementations have produced large error rates when using these models to solve the regularized long wave (RLW) equation. Two improved PINN approaches were developed in this research: an adaptive approach with self-adaptive loss weighting and a conservative approach enforcing explicit conservation laws. Three benchmark tests were used to demonstrate how effective PINN's are as they relate to the type of problem being solved (i.e., time dependent RLW equation). The first was a single soliton traveling along a line (propagation), the second was the interaction between two solitons, and the third was the evolution of an undular bore over the course of $t=250$. The results demonstrated that the effectiveness of PINNs are problem specific. The adaptive PINN was significantly better than both the conservative PINN and the standard PINN at solving problems involving complex nonlinear interactions such as colliding two solitons. The conservative approach was significantly better at solving problems involving long term behavior of single solitons and undular bores. However, the most important finding from this research is that explicitly enforcing conservation laws may be harmful to optimizing the solution of highly nonlinear systems of equations and therefore requires special training methods. The results from our adaptive and conservative approaches were within $O(10^{-5})$ of established numerical solutions for the same problem, thus demonstrating that PINNs can provide accurate solutions to complex systems of partial differential equations without the need for a discretization of space or time (mesh free). Moreover, the finding from this research challenges the assumptions that conservation enforcement will always improve the performance of a PINN and provides researchers with guidelines for designing PINNs for use on specific types of problems.

</details>


### [462] [EcoSpa: Efficient Transformer Training with Coupled Sparsity](https://arxiv.org/abs/2511.11641)
*Jinqi Xiao,Cheng Luo,Lingyi Huang,Cheng Yang,Yang Sui,Huy Phan,Xiao Zang,Yibiao Ying,Zhexiang Tang,Anima Anandkumar,Bo Yuan*

Main category: cs.LG

TL;DR: EcoSpa: a structured sparse training method for transformers that jointly sparsifies coupled weight matrices to preserve their interaction patterns, enabling memory and speed gains on commodity hardware.


<details>
  <summary>Details</summary>
Motivation: Transformers have high computational demands. Sparse training methods often prune weight matrices independently, which can disrupt multiplicative interactions between attention and feed-forward blocks and degrade performance at high sparsity.

Method: Coupled estimation and sparsification across paired weight matrices with aligned row/column removals to preserve interaction patterns. Introduces a new granularity for assessing structural component importance and applies coupled sparsification in both pre-training and fine-tuning. Implemented with standard PyTorch operations, no custom kernels.

Result: On LLaMA-1B, 50% memory reduction and 21% faster training. On GPT-2-Medium, 2.2x model compression with 2.4 lower perplexity. Inference speedup of 1.6x. Requires no special hardware.

Conclusion: EcoSpa enables efficient transformer training on commodity hardware by preserving key structural interactions while delivering substantial memory, speed, and compression gains across pre-training and fine-tuning.

Abstract: Transformers have become the backbone of modern AI, yet their high computational demands pose critical system challenges. While sparse training offers efficiency gains, existing methods fail to preserve critical structural relationships between weight matrices that interact multiplicatively in attention and feed-forward layers. This oversight leads to performance degradation at high sparsity levels. We introduce EcoSpa, an efficient structured sparse training method that jointly evaluates and sparsifies coupled weight matrix pairs, preserving their interaction patterns through aligned row/column removal. EcoSpa introduces a new granularity for calibrating structural component importance and performs coupled estimation and sparsification across both pre-training and fine-tuning scenarios. Evaluations demonstrate substantial improvements: EcoSpa enables efficient training of LLaMA-1B with 50\% memory reduction and 21\% faster training, achieves $2.2\times$ model compression on GPT-2-Medium with $2.4$ lower perplexity, and delivers $1.6\times$ inference speedup. The approach uses standard PyTorch operations, requiring no custom hardware or kernels, making efficient transformer training accessible on commodity hardware.

</details>


### [463] [A Deep Learning Model to Predicting Changes in Consumer Attributes for New Line-extended Products](https://arxiv.org/abs/2511.11646)
*Li Yinxing,Tsukasa Ishigaki*

Main category: cs.LG

TL;DR: Introduces Conditional Tabular Variational Auto-Encoder (CTVAE) to predict changes in consumer attributes for line-extended products by generating synthetic tabular data; shows superior predictive performance and suggests marketing implications to avoid cannibalization.


<details>
  <summary>Details</summary>
Motivation: To avoid detrimental effects of excessive line extensions on brand image and to identify key consumer attributes of primary customers for new line-extended products before market entry.

Method: Develops CTVAE, a conditional variational auto-encoder tailored to large-scale tabular consumer and product data to generate synthetic datasets and support predictive analysis for line extensions, with empirical comparisons to existing models.

Result: CTVAE achieves superior prediction performance over existing models and yields actionable insights for product line marketing, including considerations for containers, flavors, and marketing strategies.

Conclusion: The approach can help avoid cannibalization and guide product design and marketing; highlights potential for applying to new product variants (containers/flavors) and overall marketing strategy.

Abstract: Product line extension is a marketing strategy that enhances a company's sphere of influence. Because excessive line extensions disrupt brand image, only appropriate line extensions based on consumer needs are desirable. Marketers should know the key consumer attributes of the primary customers for new line-extended products before companies enter the market. This paper describes a method for predicting changes in consumer attributes for new line-extended products using a novel deep learning model. The proposed model, Conditional Tabular Variational Auto-Encoder (CTVAE), generates synthetic data from large-scale tabular data of consumers and products. It can provide various implications about effective product line marketing for marketers. The experimental results demonstrate that the CTVAE offers superior prediction performance than existing models. We indicate implications for new products that change containers or flavors for effective product line marketing. The proposed approach has the potential to contribute to avoiding cannibalization and to designing product images and marketing strategies.

</details>


### [464] [Environment-Aware Transfer Reinforcement Learning for Sustainable Beam Selection](https://arxiv.org/abs/2511.11647)
*Dariush Salami,Ramin Hashemi,Parham Kazemi,Mikko A. Uusitalo*

Main category: cs.LG

TL;DR: A transfer-learning RL approach for 5G beam selection uses environment-point-clouds and Chamfer distance to reuse pre-trained models, achieving ~16x training time/energy savings while maintaining performance in diverse propagation settings.


<details>
  <summary>Details</summary>
Motivation: RL-based beam selection is computationally intensive and poorly scalable across diverse propagation environments; need energy-efficient, scalable AI for edge-enabled 5G/6G networks.

Method: Represent the wireless environment as a point cloud of gNBs and scatterers. Use Chamfer distance to quantify environmental similarity and enable transfer learning to reuse pre-trained RL models for new but similar environments.

Result: Simulation results show a 16x reduction in training time and computational overhead, with maintained high performance and improved energy efficiency; faster deployment and reduced carbon emissions.

Conclusion: Environment-aware transfer learning can enable scalable, adaptive, and energy-conscious RL-based beam selection in dynamic, diverse settings, supporting greener AI and edge deployment of AI-driven wireless systems.

Abstract: This paper presents a novel and sustainable approach for improving beam selection in 5G and beyond networks using transfer learning and Reinforcement Learning (RL). Traditional RL-based beam selection models require extensive training time and computational resources, particularly when deployed in diverse environments with varying propagation characteristics posing a major challenge for scalability and energy efficiency. To address this, we propose modeling the environment as a point cloud, where each point represents the locations of gNodeBs (gNBs) and surrounding scatterers. By computing the Chamfer distance between point clouds, structurally similar environments can be efficiently identified, enabling the reuse of pre-trained models through transfer learning. This methodology leads to a 16x reduction in training time and computational overhead, directly contributing to energy efficiency. By minimizing the need for retraining in each new deployment, our approach significantly lowers power consumption and supports the development of green and sustainable Artificial Intelligence (AI) in wireless systems. Furthermore, it accelerates time-to-deployment, reduces carbon emissions associated with training, and enhances the viability of deploying AI-driven communication systems at the edge. Simulation results confirm that our approach maintains high performance while drastically cutting energy costs, demonstrating the potential of transfer learning to enable scalable, adaptive, and environmentally conscious RL-based beam selection strategies in dynamic and diverse propagation environments.

</details>


### [465] [Lightweight Time Series Data Valuation on Time Series Foundation Models via In-Context Finetuning](https://arxiv.org/abs/2511.11648)
*Shunyu Wu,Tianyue Li,Yixuan Leng,Jingyi Suo,Jian Lou,Dan Li,See-Kiong Ng*

Main category: cs.LG

TL;DR: A scalable time-series data valuation method for TSFMs using in-context finetuning (LTSV) with temporal block aggregation; robust valuations with reduced computation.


<details>
  <summary>Details</summary>
Motivation: Need for accurate, scalable data valuation for time-series foundation models; traditional influence functions are expensive and fail to preserve temporal dependencies.

Method: LTSV uses in-context finetuning to approximate influence; measures a sample's contribution as the change in context loss after finetuning; aggregates across overlapping time windows via temporal blocks.

Result: Empirically, LTSV consistently yields reliable valuations across multiple datasets and models with manageable computational costs; valuations are robust and transferable across tasks.

Conclusion: In-context finetuning provides a practical bridge between data attribution and generalization for time-series learning; offers a scalable path for data valuation in TSFMs.

Abstract: Time series foundation models (TSFMs) have demonstrated increasing capabilities due to their extensive pretraining on large volumes of diverse time series data. Consequently, the quality of time series data is crucial to TSFM performance, rendering an accurate and efficient data valuation of time series for TSFMs indispensable. However, traditional data valuation methods, such as influence functions, face severe computational bottlenecks due to their poor scalability with growing TSFM model sizes and often fail to preserve temporal dependencies. In this paper, we propose LTSV, a Lightweight Time Series Valuation on TSFMS via in-context finetuning. Grounded in the theoretical evidence that in-context finetuning approximates the influence function, LTSV estimates a sample's contribution by measuring the change in context loss after in-context finetuning, leveraging the strong generalization capabilities of TSFMs to produce robust and transferable data valuations. To capture temporal dependencies, we introduce temporal block aggregation, which integrates per-block influence scores across overlapping time windows. Experiments across multiple time series datasets and models demonstrate that LTSV consistently provides reliable and strong valuation performance, while maintaining manageable computational requirements. Our results suggest that in-context finetuning on time series foundation models provides a practical and effective bridge between data attribution and model generalization in time series learning.

</details>


### [466] [Enhanced Water Leak Detection with Convolutional Neural Networks and One-Class Support Vector Machine](https://arxiv.org/abs/2511.11650)
*Daniele Ugo Leonzio,Paolo Bestagini,Marco Marcon,Stefano Tubaro*

Main category: cs.LG

TL;DR: A data-driven leak-detection approach for water distribution networks using pressure readings and one-class SVM trained on normal (no-leak) data; feature extraction; evaluated on Modena WDN, reporting superior performance to recent methods.


<details>
  <summary>Details</summary>
Motivation: Water leakage causes significant losses; data-driven anomaly detection leveraging normal operation data and network topology offers a practical detection approach.

Method: Collect pressure data from multiple nodes; apply a feature extractor; train a one-class SVM on no-leak data; detect leaks as anomalies; uses only topology and leak-free data.

Result: On simulated Modena WDN dataset, the method outperforms recent leak-detection approaches.

Conclusion: A practical, data-driven framework for leak detection in WDNs; potential for real-world deployment, though dependent on the quality of normal-operation data and realism of simulations.

Abstract: Water is a critical resource that must be managed efficiently. However, a substantial amount of water is lost each year due to leaks in Water Distribution Networks (WDNs). This underscores the need for reliable and effective leak detection and localization systems. In recent years, various solutions have been proposed, with data-driven approaches gaining increasing attention due to their superior performance. In this paper, we propose a new method for leak detection. The method is based on water pressure measurements acquired at a series of nodes of a WDN. Our technique is a fully data-driven solution that makes only use of the knowledge of the WDN topology, and a series of pressure data acquisitions obtained in absence of leaks. The proposed solution is based on an feature extractor and a one-class Support Vector Machines (SVM) trained on no-leak data, so that leaks are detected as anomalies. The results achieved on a simulate dataset using the Modena WDN demonstrate that the proposed solution outperforms recent methods for leak detection.

</details>


### [467] [Incomplete Depression Feature Selection with Missing EEG Channels](https://arxiv.org/abs/2511.11651)
*Zhijian Gong,Wenjia Dong,Xueyuan Xu,Fulin Wei,Chunyu Liu,Li Zhuo*

Main category: cs.LG

TL;DR: A robust EEG feature selection method (IDFS-MEC) for depression detection that handles missing EEG channels and noise, outperforming 10 popular methods across various channel counts on MODMA and PRED-d003 datasets.


<details>
  <summary>Details</summary>
Motivation: EEG depression analysis suffers from redundant/noisy features and real-world issues like data loss from electrode detachment. A robust feature selection approach is needed to improve accuracy under incomplete channels.

Method: Introduce IDFS-MEC which integrates missing-channel indicator information and adaptive channel weighting into orthogonal regression to mitigate incomplete channels, followed by global redundancy minimization learning to reduce redundancy among selected feature subsets.

Result: IDFS-MEC-selected EEG feature subsets outperform 10 popular feature selection methods on MODMA and PRED-d003 datasets across 3-, 64-, and 128-channel configurations.

Conclusion: IDFS-MEC provides a robust feature selection framework for EEG-based depression analysis by explicitly handling missing channels and reducing feature redundancy, yielding superior performance in varied channel settings.

Abstract: As a critical mental health disorder, depression has severe effects on both human physical and mental well-being. Recent developments in EEG-based depression analysis have shown promise in improving depression detection accuracies. However, EEG features often contain redundant, irrelevant, and noisy information. Additionally, real-world EEG data acquisition frequently faces challenges, such as data loss from electrode detachment and heavy noise interference. To tackle the challenges, we propose a novel feature selection approach for robust depression analysis, called Incomplete Depression Feature Selection with Missing EEG Channels (IDFS-MEC). IDFS-MEC integrates missing-channel indicator information and adaptive channel weighting learning into orthogonal regression to lessen the effects of incomplete channels on model construction, and then utilizes global redundancy minimization learning to reduce redundant information among selected feature subsets. Extensive experiments conducted on MODMA and PRED-d003 datasets reveal that the EEG feature subsets chosen by IDFS-MEC have superior performance than 10 popular feature selection methods among 3-, 64-, and 128-channel settings.

</details>


### [468] [How many stations are sufficient? Exploring the effect of urban weather station density reduction on imputation accuracy of air temperature and humidity](https://arxiv.org/abs/2511.11652)
*Marvin Plein,Carsten F. Dormann,Andreas Christen*

Main category: cs.LG

TL;DR: A stepwise removal of weather stations in Freiburg can greatly reduce network size (e.g., 42 to 4) with only modest RMSE increases for temperature and humidity over a year; edge between built-up and rural zones are most informative; remote forest stations perform worse; overall performance exceeds a leading urban land-surface model; supports thinning to save costs.


<details>
  <summary>Details</summary>
Motivation: Urban weather station networks are expensive to maintain; there is a need to identify how far the network can be thinned without substantially losing the ability to reproduce citywide temperature and humidity patterns, enabling cost-effective urban climate research.

Method: Simulated density reduction via a stepwise removal procedure applied to the Freiburg WSN; assess whether WSN subsets can reproduce the original network's air temperature and humidity patterns for one year post-thinning; compare predictive accuracy (RMSE) to the full network; analyze by land-use setting (built-up, open, forest) and edge locations; benchmark against a state-of-the-art urban land-surface model (Surface Urban Energy and Water Balance Scheme).

Result: Substantial reductions in station numbers after one year of full deployment are possible with only modest decreases in predictive accuracy. Example: 42→4 stations raised mean RMSE_T from 0.69 K to 0.83 K (+0.14 K, ~20% increase) and RMSE_RH from 3.8% to 4.4% (+0.6 pp, ~16%). Predictive accuracy is worse for remote forest stations than built-up/open but remains better than the SUWB model. Edge stations (built-up–rural interfaces) are most valuable for reconstructing city-wide climate characteristics.

Conclusion: A carefully designed thinning strategy can greatly reduce WSN maintenance costs while preserving the ability to reconstruct city-scale temperature and humidity patterns; focus on preserving edge locations and, where necessary, core coverage to maintain accuracy; the approach offers a practical path for resource optimization in urban climate research.

Abstract: Urban weather station networks (WSNs) are widely used to monitor urban weather and climate patterns and aid urban planning. However, maintaining WSNs is expensive and labor-intensive. Here, we present a step-wise station removal procedure to thin an existing WSN in Freiburg, Germany, and analyze the ability of WSN subsets to reproduce air temperature and humidity patterns of the entire original WSN for a year following a simulated reduction of WSN density. We found that substantial reductions in station numbers after one year of full deployment are possible while retaining high predictive accuracy. A reduction from 42 to 4 stations, for instance, increased mean prediction RMSEs from 0.69 K to 0.83 K for air temperature and from 3.8% to 4.4% for relative humidity, corresponding to RMSE increases of only 20% and 16%, respectively. Predictive accuracy is worse for remote stations in forests than for stations in built-up or open settings, but consistently better than a state-of-the-art numerical urban land-surface model (Surface Urban Energy and Water Balance Scheme). Stations located at the edges between built-up and rural areas are most valuable when reconstructing city-wide climate characteristics. Our study demonstrates the potential of thinning WSNs to maximize the efficient allocation of financial and personnel-related resources in urban climate research.

</details>


### [469] [Convergence of Multiagent Learning Systems for Traffic control](https://arxiv.org/abs/2511.11654)
*Sayambhu Sen,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: The paper proves convergence of an independent-learning MARL algorithm for cooperative traffic signal control via stochastic approximation, extending single-agent asynchronous value iteration to multi-agent settings.


<details>
  <summary>Details</summary>
Motivation: Empirical MARL gains in traffic signal control exist but lack rigorous stability and convergence guarantees; establishing theoretical foundations is essential for reliable deployment in critical infrastructure.

Method: Formal stochastic approximation analysis of learning dynamics for independent Q-learning agents in a cooperative traffic signal control setting; derive conditions (e.g., diminishing step-sizes, finite state-action spaces, ergodicity) under which convergence to a fixed point is achieved, extending single-agent asynchronous value iteration to the multi-agent scenario.

Result: Under the stated conditions, the MARL algorithm converges (almost surely) to a stable Q-value fixed point; the convergence result extends asynchronous value iteration theory from single-agent to multi-agent independent learners in TSC.

Conclusion: Provides theoretical guarantees for MARL-based traffic signal control, clarifies necessary modeling assumptions, and points to future work on relaxing assumptions and validating the theory with empirical traffic data.

Abstract: Rapid urbanization in cities like Bangalore has led to severe traffic congestion, making efficient Traffic Signal Control (TSC) essential. Multi-Agent Reinforcement Learning (MARL), often modeling each traffic signal as an independent agent using Q-learning, has emerged as a promising strategy to reduce average commuter delays. While prior work Prashant L A et. al has empirically demonstrated the effectiveness of this approach, a rigorous theoretical analysis of its stability and convergence properties in the context of traffic control has not been explored. This paper bridges that gap by focusing squarely on the theoretical basis of this multi-agent algorithm. We investigate the convergence problem inherent in using independent learners for the cooperative TSC task. Utilizing stochastic approximation methods, we formally analyze the learning dynamics. The primary contribution of this work is the proof that the specific multi-agent reinforcement learning algorithm for traffic control is proven to converge under the given conditions extending it from single agent convergence proofs for asynchronous value iteration.

</details>


### [470] [On the Probabilistic Learnability of Compact Neural Network Preimage Bounds](https://arxiv.org/abs/2511.11656)
*Luca Marzari,Manuele Bicego,Ferdinando Cicalese,Alessandro Farinelli*

Main category: cs.LG

TL;DR: RF-ProVe introduces a probabilistic, random-forest-based preimage verifier for neural networks, offering scalable, high-confidence approximations of input regions that satisfy a target output property by bootstrap sampling and active resampling, with formal guarantees on region purity and coverage.


<details>
  <summary>Details</summary>
Motivation: Exact preimage computation for neural networks is #P-hard and scales poorly. There is a need for scalable, high-confidence methods that can capture complex patterns in high-dimensional input spaces and identify regions where a property holds, even if exact solvers are infeasible.

Method: Build an ensemble of randomized decision trees (random forests) to generate candidate input regions that satisfy the target output property. Use bootstrap sampling to create diverse trees, then refine regions via active resampling to improve region purity and global coverage. Derive formal statistical guarantees for region purity and coverage, enabling bounded error preimage approximations.

Result: Provides a practical, scalable approach to compute compact preimage approximations when exact solvers fail to scale, with formal guarantees on the quality of the identified regions (purity and coverage).

Conclusion: A probabilistic, ensemble-based verification framework can complement exact solvers, achieving scalability in high dimensions while maintaining bounded error guarantees on preimage regions; suitable for identifying complex input regions where a neural network yields a given property.

Abstract: Although recent provable methods have been developed to compute preimage bounds for neural networks, their scalability is fundamentally limited by the #P-hardness of the problem. In this work, we adopt a novel probabilistic perspective, aiming to deliver solutions with high-confidence guarantees and bounded error. To this end, we investigate the potential of bootstrap-based and randomized approaches that are capable of capturing complex patterns in high-dimensional spaces, including input regions where a given output property holds. In detail, we introduce $\textbf{R}$andom $\textbf{F}$orest $\textbf{Pro}$perty $\textbf{Ve}$rifier ($\texttt{RF-ProVe}$), a method that exploits an ensemble of randomized decision trees to generate candidate input regions satisfying a desired output property and refines them through active resampling. Our theoretical derivations offer formal statistical guarantees on region purity and global coverage, providing a practical, scalable solution for computing compact preimage approximations in cases where exact solvers fail to scale.

</details>


### [471] [SpecQuant: Spectral Decomposition and Adaptive Truncation for Ultra-Low-Bit LLMs Quantization](https://arxiv.org/abs/2511.11663)
*Zhixiong Zhao,Fangxin Liu,Junjie Wang,Chenyang Guan,Zongwu Wang,Li Jiang,Haibing Guan*

Main category: cs.LG

TL;DR: SpecQuant enables ultra-low-bit quantization (4-bit) for both activations and weights in LLMs by combining activation smoothing with a two-stage, Fourier-domain approach that truncates high-frequency information channel-wise and uses a runtime adaptive truncation module.


<details>
  <summary>Details</summary>
Motivation: To achieve highly compact, efficient LLM deployment on end-user devices without sacrificing accuracy, addressing extreme quantization challenges for activations and weights.

Method: Stage 1: smooth activation outliers and transfer them into the weight matrix to simplify downstream quantization. Stage 2: apply channel-wise low-frequency Fourier truncation to suppress high-frequency components while preserving essential energy, enabling robust 4-bit quantization. A lightweight runtime truncation module adjusts thresholds based on channel characteristics.

Result: On LLaMA-3 8B, achieves 4-bit quantization for both weights and activations, narrowing the zero-shot accuracy gap to ~1.5% from full precision, with ~2x faster inference and ~3x lower memory usage.

Conclusion: A Fourier-domain perspective reveals that most weight energy lies in low-frequency components, which can be retained with minimal accuracy loss, enabling robust ultra-low-bit quantization with adaptable runtime control.

Abstract: The emergence of accurate open large language models (LLMs) has sparked a push for advanced quantization techniques to enable efficient deployment on end-user devices. In this paper, we revisit the challenge of extreme LLM compression -- targeting ultra-low-bit quantization for both activations and weights -- from a Fourier frequency domain perspective. We propose SpecQuant, a two-stage framework that tackles activation outliers and cross-channel variance. In the first stage, activation outliers are smoothed and transferred into the weight matrix to simplify downstream quantization. In the second stage, we apply channel-wise low-frequency Fourier truncation to suppress high-frequency components while preserving essential signal energy, improving quantization robustness. Our method builds on the principle that most of the weight energy is concentrated in low-frequency components, which can be retained with minimal impact on model accuracy. To enable runtime adaptability, we introduce a lightweight truncation module during inference that adjusts truncation thresholds based on channel characteristics. On LLaMA-3 8B, SpecQuant achieves 4-bit quantization for both weights and activations, narrowing the zero-shot accuracy gap to only 1.5% compared to full precision, while delivering 2 times faster inference and 3times lower memory usage.

</details>


### [472] [Clifford Algebraic Rotor Embeddings : Maybe embeddings should start to CARE](https://arxiv.org/abs/2511.11665)
*Sameeksha Sriram,Ayush Paliwal,Alexander S. Ecker,Chase van de Geijn*

Main category: cs.LG

TL;DR: QuatRo uses quaternions to parameterize rotations in rotary embeddings, unifying spherical and mixed RoPE; extended to Clifford Algebras (CARE) for high-dimensional, multivector positional encodings; preliminary experiments compare spherical, quaternion, and Clifford-based variants.


<details>
  <summary>Details</summary>
Motivation: RoPE's shift-equivariance and performance; non-commutative extensions often lose shift-equivariance; a unified quaternion/Clifford framework aims to extend rotary embeddings to higher dimensions while preserving desirable properties and enabling richer representations.

Method: Replace Euler-angle rotations with quaternion-based axes; show QuatRo subsumes Mixed RoPE and Spherical RoPE; generalize to CARE via geometric algebra acting on multivectors; discuss dimensionality extension and multiple grades; provide preliminary experiments.

Result: Shows relationships among spherical, quaternion, and Clifford rotary embeddings; initial empirical comparisons establish baselines and motivate further study.

Conclusion: QuatRo and CARE offer a unified, extensible framework for rotary embeddings across dimensions and multivector representations, enabling richer positional encodings and setting the stage for broader empirical validation.

Abstract: Rotary Positional Embeddings (RoPE) have demonstrated exceptional performance as a positional encoding method, consistently outperforming their baselines. While recent work has sought to extend RoPE to higher-dimensional inputs, many such extensions are non-commutative, thereby forfeiting RoPE's shift-equivariance property. Spherical RoPE is one such non-commutative variant, motivated by the idea of rotating embedding vectors on spheres rather than circles. However, spherical rotations are inherently non-commutative, making the choice of rotation sequence ambiguous. In this work, we explore a quaternion-based approach -- Quaternion Rotary Embeddings (QuatRo) -- in place of Euler angles, leveraging quaternions' ability to represent 3D rotations to parameterize the axes of rotation. We show Mixed RoPE and Spherical RoPE to be special cases of QuatRo. Further, we propose a generalization of QuatRo to Clifford Algebraic Rotary Embeddings (CARE) using geometric algebra. Viewing quaternions as the even subalgebra of Cl(3,0,0), we extend the notion of rotary embeddings from quaternions to Clifford rotors acting on multivectors. This formulation enables two key generalizations: (1) extending rotary embeddings to arbitrary dimensions, and (2) encoding positional information in multivectors of multiple grades, not just vectors. We present preliminary experiments comparing spherical, quaternion, and Clifford-based rotary embeddings.

</details>


### [473] [Adaptive Stepsizing for Stochastic Gradient Langevin Dynamics in Bayesian Neural Networks](https://arxiv.org/abs/2511.11666)
*Rajit Rajpal,Benedict Leimkuhler,Yuanhao Jiang*

Main category: cs.LG

TL;DR: SA-SGLD adaptively adjusts step size via time-rescaling based on gradient norms, enabling bias-free, stable, and better-mixing sampling for Bayesian neural network posteriors compared to SGLD, especially in high-curvature settings and with sharp priors.


<details>
  <summary>Details</summary>
Motivation: SGMCMC methods for Bayesian neural networks are highly sensitive to stepsize; adaptive schemes like pSGLD require costly divergence corrections to sample the correct invariant measure. There is a need for automatic, bias-free adaptive step strategies that preserve the target posterior.

Method: Extend the SamAdams timestepping adaptation framework to Bayesian SGLD (SA-SGLD). Use time rescaling to modulate the stepsize according to a monitored quantity (e.g., local gradient norm). The approach automatically contracts step sizes in high-curvature regions and expands them in flatter regions, aiming to maintain stability and improve mixing without introducing bias.

Result: SA-SGLD achieves more accurate posterior sampling than SGLD on high-curvature 2D toy examples and in image classification with Bayesian neural networks using sharp priors.

Conclusion: Adaptive time-rescaling via the SamAdams framework can provide stable, bias-free step-size control for SGLD, improving posterior sampling quality and mixing, with demonstrated gains in challenging settings. Further validation across broader tasks and comparisons would corroborate robustness.

Abstract: Bayesian neural networks (BNNs) require scalable sampling algorithms to approximate posterior distributions over parameters. Existing stochastic gradient Markov Chain Monte Carlo (SGMCMC) methods are highly sensitive to the choice of stepsize and adaptive variants such as pSGLD typically fail to sample the correct invariant measure without addition of a costly divergence correction term. In this work, we build on the recently proposed `SamAdams' framework for timestep adaptation (Leimkuhler, Lohmann, and Whalley 2025), introducing an adaptive scheme: SA-SGLD, which employs time rescaling to modulate the stepsize according to a monitored quantity (typically the local gradient norm). SA-SGLD can automatically shrink stepsizes in regions of high curvature and expand them in flatter regions, improving both stability and mixing without introducing bias. We show that our method can achieve more accurate posterior sampling than SGLD on high-curvature 2D toy examples and in image classification with BNNs using sharp priors.

</details>


### [474] [Beyond Superficial Forgetting: Thorough Unlearning through Knowledge Density Estimation and Block Re-insertion](https://arxiv.org/abs/2511.11667)
*Feng Guo,Yuntao Wen,Shen Gao,Junshuo Zhang,Shuo Shang*

Main category: cs.LG

TL;DR: KUnBR locates layers rich in harmful knowledge via knowledge density estimation and removes it through a block reinsertion strategy that bypasses gradient obstacles, achieving state-of-the-art forgetting with maintained utility.


<details>
  <summary>Details</summary>
Motivation: Privacy, regulatory, and ethical concerns in LLMs require effective removal of harmful knowledge; existing methods often fail to fully erase residual harmful knowledge.

Method: Estimate knowledge density across layers to identify harmful-knowledge-rich layers, then apply a blocks reinsertion procedure that extracts those layers and re-inserts them into the original model to ensure effective gradient propagation during unlearning, bypassing gradient obstructions caused by cover layers.

Result: Experiments on several unlearning and general capability benchmarks show state-of-the-art forgetting while preserving model utility.

Conclusion: KUnBR provides precise, effective unlearning through density-guided layer selection and a novel reinsertion mechanism, advancing the ability to erase harmful knowledge without full retraining.

Abstract: Machine unlearning, which selectively removes harmful knowledge from a pre-trained model without retraining from scratch, is crucial for addressing privacy, regulatory compliance, and ethical concerns in Large Language Models (LLMs). However, existing unlearning methods often struggle to thoroughly remove harmful knowledge, leaving residual harmful knowledge that can be easily recovered. To address these limitations, we propose Knowledge Density-Guided Unlearning via Blocks Reinsertion (KUnBR), a novel approach that first identifies layers with rich harmful knowledge and then thoroughly eliminates the harmful knowledge via re-insertion strategy. Our method introduces knowledge density estimation to quantify and locate layers containing the most harmful knowledge, enabling precise unlearning. Additionally, we design a layer re-insertion strategy that extracts and re-inserts harmful knowledge-rich layers into the original LLM, bypassing gradient obstruction caused by cover layers and ensuring effective gradient propagation during unlearning. Extensive experiments conducted on several unlearning and general capability benchmarks demonstrate that KUnBR achieves state-of-the-art forgetting performance while maintaining model utility.

</details>


### [475] [Do traveling waves make good positional encodings?](https://arxiv.org/abs/2511.11668)
*Chase van de Geijn,Ayush Paliwal,Timo Lüddecke,Alexander S. Ecker*

Main category: cs.LG

TL;DR: RollPE introduces a traveling-wave positional encoding by circularly rolling the query and key tensors in self-attention, creating a relative phase shift that outperforms absolute embeddings and matches RoPE, with a continuous formulation and RoPE-equivalence.


<details>
  <summary>Details</summary>
Motivation: Address the permutation invariance of self-attention and the limitations of absolute/learned positional encodings by emphasizing relative encodings that capture translation equivariances.

Method: Apply a circular roll to the query and key tensors in self-attention to induce a relative shift in phase across positions (RollPE). Derive its continuous limit, show a topographic structure in Q/K space, and establish an equivalence to a particular RoPE configuration.

Result: RollPE significantly outperforms traditional absolute positional embeddings and is comparable to RoPE.

Conclusion: Viewing RollPE as traveling waves provides a topographic interpretation, may simplify RoPE, and links the mechanism to information flow concepts in the brain.

Abstract: Transformers rely on positional encoding to compensate for the inherent permutation invariance of self-attention. Traditional approaches use absolute sinusoidal embeddings or learned positional vectors, while more recent methods emphasize relative encodings to better capture translation equivariances. In this work, we propose RollPE, a novel positional encoding mechanism based on traveling waves, implemented by applying a circular roll operation to the query and key tensors in self-attention. This operation induces a relative shift in phase across positions, allowing the model to compute attention as a function of positional differences rather than absolute indices. We show this simple method significantly outperforms traditional absolute positional embeddings and is comparable to RoPE. We derive a continuous case of RollPE which implicitly imposes a topographic structure on the query and key space. We further derive a mathematical equivalence of RollPE to a particular configuration of RoPE. Viewing RollPE through the lens of traveling waves may allow us to simplify RoPE and relate it to processes of information flow in the brain.

</details>


### [476] [H-Model: Dynamic Neural Architectures for Adaptive Processing](https://arxiv.org/abs/2511.11669)
*Dmytro Hospodarchuk*

Main category: cs.LG

TL;DR: A conceptual neural architecture with dynamic, data- and state-conditioned routing across layers enables adaptive computation; a prototype focused on adaptable and potentially more interpretable networks rather than state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To explore learning not only representations but also the structure of computation, by enabling dynamic, iterative information flow inspired by thought processes and internal states.

Method: Introduce a routing mechanism in which each layer can influence how outputs are propagated to subsequent layers, enabling iterative and adaptive computation conditioned on input data and the network's internal state.

Result: Preliminary observations indicate promise under constrained resources; the approach is not aimed at beating SOTA but to demonstrate feasibility and direction, with full potential to be evaluated under better computational conditions.

Conclusion: A preliminary, exploratory framework that opens a new direction for adaptable and potentially more interpretable networks; future work should test the architecture under more favorable computational conditions and datasets.

Abstract: This article explores the design and experimentation of a neural network architecture capable of dynamically adjusting its internal structure based on the input data. The proposed model introduces a routing mechanism that allows each layer to influence how its outputs are propagated through the network, enabling iterative and adaptive computation. This concept is loosely inspired by the idea of thought processes and dynamic reasoning, where information flow is conditioned not only on the data itself, but also on the internal state of the system.
  It is important to note that this work does not aim to compete with state-of-the-art language models in terms of performance. Instead, it presents a conceptual prototype-an architectural framework that opens up a new direction for exploring adaptable and potentially more interpretable networks. The goal is not optimization of existing benchmarks but rather the proposal of a system that can learn not only representations, but also the structure of computation itself.
  Due to practical constraints in computing resources and data, this study remains a preliminary investigation. Nevertheless, initial observations show promise, and the architecture's full potential can only be evaluated in future experiments under more favorable computational conditions.

</details>


### [477] [Evaluation of LLM-based Explanations for a Learning Analytics Dashboard](https://arxiv.org/abs/2511.11671)
*Alina Deriyeva,Benjamin Paassen*

Main category: cs.LG

TL;DR: LLM-generated explanations in Learning Analytics Dashboards improve perceived interpretability and actionable guidance, outperforming standalone dashboards and teacher-provided explanations in an expert study (N=12); suggests LLMs can support self-regulated learning while preserving pedagogical standards.


<details>
  <summary>Details</summary>
Motivation: Investigate interpretability and effectiveness of Learning Analytics Dashboards and whether LLM-driven verbal explanations can aid learners and educators.

Method: Developed LLM-generated verbal explanations of dashboard data and compared with a standalone dashboard and teacher explanations in an expert study with university educators (N=12). Measured favorability/interpretability across conditions.

Result: LLM explanations and general recommendations were significantly more favored than the other conditions.

Conclusion: LLMs for interpreting dashboard data can enhance learning experiences without compromising pedagogical standards; promising for digital learning environments, with alignment to educator-approved pedagogy.

Abstract: Learning Analytics Dashboards can be a powerful tool to support self-regulated learning in Digital Learning Environments and promote development of meta-cognitive skills, such as reflection. However, their effectiveness can be affected by the interpretability of the data they provide. To assist in the interpretation, we employ a large language model to generate verbal explanations of the data in the dashboard and evaluate it against a standalone dashboard and explanations provided by human teachers in an expert study with university level educators (N=12). We find that the LLM-based explanations of the skill state presented in the dashboard, as well as general recommendations on how to proceed with learning within the course are significantly more favored compared to the other conditions. This indicates that using LLMs for interpretation purposes can enhance the learning experience for learners while maintaining the pedagogical standards approved by teachers.

</details>


### [478] [Synergistic Feature Fusion for Latent Lyrical Classification: A Gated Deep Learning Architecture](https://arxiv.org/abs/2511.11673)
*M. A. Gameiro*

Main category: cs.LG

TL;DR: A gated fusion model (Synergistic Fusion Layer) combines high-dimensional Sentence-BERT embeddings with low-dimensional structural cues to classify lyrical content, achieving superior accuracy and calibration over a feature-concatenation baseline.


<details>
  <summary>Details</summary>
Motivation: To effectively integrate complex semantic features with simple interpretable cues for lyrical content classification and to test whether nonlinear gating improves performance and calibration over concatenation-based baselines.

Method: Introduce the Synergistic Fusion Layer (SFL) with a gating mechanism that modulates Fdeep (Sentence-BERT embeddings) using Fstruct (low-dimensional auxiliary features). Reformulate a clustering-derived binary task into binary classification and compare against a Random Forest baseline that uses feature concatenation.

Result: SFL achieves accuracy 0.9894 and Macro F1 0.9894, outperforming RF (Accuracy 0.9868). It also substantially improves reliability/calibration: ECE reduced to 0.0035 (from 0.0500) and Log Loss to 0.0304 (from 0.0772). These results support the hypothesis that nonlinear gating is superior to simple concatenation in multimodal lyric analysis.

Conclusion: Nonlinear gating via SFL yields a robust, trustworthy multimodal approach for complex lyrical analysis, outperforming a feature-concatenation baseline in accuracy and calibration.

Abstract: This study addresses the challenge of integrating complex, high-dimensional deep semantic features with simple, interpretable structural cues for lyrical content classification. We introduce a novel Synergistic Fusion Layer (SFL) architecture, a deep learning model utilizing a gated mechanism to modulate Sentence-BERT embeddings (Fdeep) using low-dimensional auxiliary features (Fstruct). The task, derived from clustering UMAP-reduced lyrical embeddings, is reframed as binary classification, distinguishing a dominant, homogeneous cluster (Class 0) from all other content (Class 1). The SFL model achieved an accuracy of 0.9894 and a Macro F1 score of 0.9894, outperforming a comprehensive Random Forest (RF) baseline that used feature concatenation (Accuracy = 0.9868). Crucially, the SFL model demonstrated vastly superior reliability and calibration, exhibiting a 93% reduction in Expected Calibration Error (ECE = 0.0035) and a 2.5x lower Log Loss (0.0304) compared to the RF baseline (ECE = 0.0500; Log Loss = 0.0772). This performance validates the architectural hypothesis that non-linear gating is superior to simple feature concatenation, establishing the SFL model as a robust and trustworthy system for complex multimodal lyrical analysis.

</details>


### [479] [Beyond One-Way Pruning: Bidirectional Pruning-Regrowth for Extreme Accuracy-Sparsity Tradeoff](https://arxiv.org/abs/2511.11675)
*Junchen Liu,Yi Sheng*

Main category: cs.LG

TL;DR: Bidirectional pruning-regrowth enables extremely high sparsity by regenerating critical connections to recover performance, addressing sharp accuracy drop in pruning under high sparsity, and enabling hardware-constrained compression.


<details>
  <summary>Details</summary>
Motivation: Model pruning often degrades sharply beyond a sparsity threshold, preventing hardware-constrained compression from achieving useful models. This limits compression ratios and renders highly sparse models inoperable on some hardware.

Method: Introduce a bidirectional pruning-regrowth strategy. Begin from an extremely compressed network that meets hardware constraints and selectively regenerate critical connections to recover lost performance, thereby mitigating the severe accuracy drop seen at high sparsity.

Result: The regrowth process recovers performance, effectively mitigating the sharp accuracy decline under high sparsity and enabling more aggressive compression within hardware constraints.

Conclusion: Bidirectional pruning-regrowth can overcome the limitations of conventional pruning at extreme sparsity, enabling more practicable hardware-aware compression.

Abstract: As a widely adopted model compression technique, model pruning has demonstrated strong effectiveness across various architectures. However, we observe that when sparsity exceeds a certain threshold, both iterative and one-shot pruning methods lead to a steep decline in model performance. This rapid degradation limits the achievable compression ratio and prevents models from meeting the stringent size constraints required by certain hardware platforms, rendering them inoperable. To overcome this limitation, we propose a bidirectional pruning-regrowth strategy. Starting from an extremely compressed network that satisfies hardware constraints, the method selectively regenerates critical connections to recover lost performance, effectively mitigating the sharp accuracy drop commonly observed under high sparsity conditions.

</details>


### [480] [Learning with Preserving for Continual Multitask Learning](https://arxiv.org/abs/2511.11676)
*Hanchen David Wang,Siwoo Bae,Zirong Chen,Meiyi Ma*

Main category: cs.LG

TL;DR: Introduces Learning with Preserving (LwP) for Continual Multitask Learning (CMTL) using a Dynamically Weighted Distance Preservation (DWDP) loss to maintain geometric structure of latent representations, enabling replay-free continual learning that outperforms baselines and handles distribution shifts.


<details>
  <summary>Details</summary>
Motivation: In critical domains, models must learn new tasks sequentially from a shared data stream without forgetting; existing methods often learn fragmented, task-specific features that interfere with each other, degrading performance.

Method: Proposes the LwP framework that preserves the geometric structure of latent representations by enforcing a DWDP loss that maintains pairwise distances, preventing representation drift. This avoids replay buffers and supports learning on time-series and image data.

Result: Empirical evaluations show that LwP mitigates catastrophic forgetting and consistently outperforms state-of-the-art baselines in CMTL tasks, with strong robustness to distribution shifts and superior performance compared to single-task baselines.

Conclusion: Geometric-structure preservation in latent space is a viable strategy for continual multitask learning, enabling robust, replay-free learning in dynamic environments.

Abstract: Artificial intelligence systems in critical fields like autonomous driving and medical imaging analysis often continually learn new tasks using a shared stream of input data. For instance, after learning to detect traffic signs, a model may later need to learn to classify traffic lights or different types of vehicles using the same camera feed. This scenario introduces a challenging setting we term Continual Multitask Learning (CMTL), where a model sequentially learns new tasks on an underlying data distribution without forgetting previously learned abilities. Existing continual learning methods often fail in this setting because they learn fragmented, task-specific features that interfere with one another. To address this, we introduce Learning with Preserving (LwP), a novel framework that shifts the focus from preserving task outputs to maintaining the geometric structure of the shared representation space. The core of LwP is a Dynamically Weighted Distance Preservation (DWDP) loss that prevents representation drift by regularizing the pairwise distances between latent data representations. This mechanism of preserving the underlying geometric structure allows the model to retain implicit knowledge and support diverse tasks without requiring a replay buffer, making it suitable for privacy-conscious applications. Extensive evaluations on time-series and image benchmarks show that LwP not only mitigates catastrophic forgetting but also consistently outperforms state-of-the-art baselines in CMTL tasks. Notably, our method shows superior robustness to distribution shifts and is the only approach to surpass the strong single-task learning baseline, underscoring its effectiveness for real-world dynamic environments.

</details>


### [481] [Homotopy-Guided Self-Supervised Learning of Parametric Solutions for AC Optimal Power Flow](https://arxiv.org/abs/2511.11677)
*Shimiao Li,Aaron Tuor,Draguna Vrabie,Larry Pileggi,Jan Drgona*

Main category: cs.LG

TL;DR: A homotopy-guided self-supervised L2O method for AC-OPF improves feasibility and maintains solver-like objective values without labeled data.


<details>
  <summary>Details</summary>
Motivation: AC-OPF is highly nonconvex, causing difficult optimization landscapes and unreliable learning-based solvers. There is a need for scalable, constraint-aware L2O that yields feasible, high-quality solutions in real time without requiring labeled optimal solutions or external solvers.

Method: Train a parametric L2O model while performing a progressive homotopy: start from a relaxed problem with a broad basin of attraction and gradually morph the objective and constraints toward the original nonconvex AC-OPF, enabling self-supervised guidance and improved convergence stability.

Result: On standard IEEE AC-OPF benchmarks, the homotopy-guided approach significantly increases feasibility rates compared with non-homotopy baselines, while achieving objective values close to those of full OPF solvers.

Conclusion: Homotopy-based heuristics show strong promise for scalable, constraint-aware L2O in power system optimization, enabling more reliable real-time decision-making without labeled data or external solvers.

Abstract: Learning to optimize (L2O) parametric approximations of AC optimal power flow (AC-OPF) solutions offers the potential for fast, reusable decision-making in real-time power system operations. However, the inherent nonconvexity of AC-OPF results in challenging optimization landscapes, and standard learning approaches often fail to converge to feasible, high-quality solutions. This work introduces a \textit{homotopy-guided self-supervised L2O method} for parametric AC-OPF problems. The key idea is to construct a continuous deformation of the objective and constraints during training, beginning from a relaxed problem with a broad basin of attraction and gradually transforming it toward the original problem. The resulting learning process improves convergence stability and promotes feasibility without requiring labeled optimal solutions or external solvers. We evaluate the proposed method on standard IEEE AC-OPF benchmarks and show that homotopy-guided L2O significantly increases feasibility rates compared to non-homotopy baselines, while achieving objective values comparable to full OPF solvers. These findings demonstrate the promise of homotopy-based heuristics for scalable, constraint-aware L2O in power system optimization.

</details>


### [482] [A neural optimization framework for free-boundary diffeomorphic mapping problems and its applications](https://arxiv.org/abs/2511.11679)
*Zhehao Xu,Lok Ming Lui*

Main category: cs.LG

TL;DR: A neural surrogate (Spectral Beltrami Network) embeds the LSQC energy for gradient-based optimization of free-boundary diffeomorphisms, with a new framework (SBN-Opt) that explicitly controls local distortion and outperforms traditional LSQC-based methods on density-equalizing maps and surface registration tasks.


<details>
  <summary>Details</summary>
Motivation: Free-boundary diffeomorphism optimization is essential for surface mapping but hard due to unconstrained boundaries and the need to preserve local bijectivity under large deformations; LSQC provides a robust theoretical foundation but is not directly compatible with gradient-based optimization.

Method: Introduce Spectral Beltrami Network (SBN) as a neural surrogate that encodes LSQC energy in a multiscale mesh-spectral architecture. Then develop SBN-Opt, an optimization framework guided by SBN that yields free-boundary diffeomorphisms with explicitly controllable local geometric distortion.

Result: Extensive experiments on density-equalizing maps and inconsistent surface registration show that SBN-Opt outperforms traditional numerical LSQC-based algorithms.

Conclusion: A gradient-based, neural surrogate approach to free-boundary diffeomorphism optimization using LSQC energy is effective, enabling controllable distortion and superior performance over conventional methods.

Abstract: Free-boundary diffeomorphism optimization is a core ingredient in the surface mapping problem but remains notoriously difficult because the boundary is unconstrained and local bijectivity must be preserved under large deformation. Numerical Least-Squares Quasiconformal (LSQC) theory, with its provable existence, uniqueness, similarity-invariance and resolution-independence, offers an elegant mathematical remedy. However, the conventional numerical algorithm requires landmark conditioning, and cannot be applied into gradient-based optimization. We propose a neural surrogate, the Spectral Beltrami Network (SBN), that embeds LSQC energy into a multiscale mesh-spectral architecture. Next, we propose the SBN guided optimization framework SBN-Opt which optimizes free-boundary diffeomorphism for the problem, with local geometric distortion explicitly controllable. Extensive experiments on density-equalizing maps and inconsistent surface registration demonstrate our SBN-Opt's superiority over traditional numerical algorithms.

</details>


### [483] [Probabilistic Wildfire Susceptibility from Remote Sensing Using Random Forests and SHAP](https://arxiv.org/abs/2511.11680)
*Udaya Bhasker Cheerala,Varun Teja Chirukuri,Venkata Akhil Kumar Gummadi,Jintu Moni Bhuyan,Praveen Damacharla*

Main category: cs.LG

TL;DR: A study maps California wildfire risk using a Random Forest (RF) model with SHAP explanations, validated spatially and temporally. RF shows strong apparent fit but mixed transferability; SHAP reveals ecosystem-specific drivers; district-level insights identify high-risk areas, supporting actionable risk mitigation.


<details>
  <summary>Details</summary>
Motivation: Provide an interpretable, scalable wildfire risk mapping framework for California by combining predictive performance with explanations to guide mitigation and policy decisions.

Method: Train a Random Forest model for wildfire risk with input ecological and climatic predictors; apply SHAP to explain predictions (XAI). Validate using spatial cross-validation and temporal split; evaluate with ROC-AUC and PR-AUC; perform district-level risk classification and interpret feature importance by land cover type.

Result: RF model achieved near-perfect discrimination for grasslands (AUC ~0.996) and forests (AUC ~0.997). Spatial CV yielded modest transferability (forests AUC 0.6155, grasslands 0.5416). Temporal validation showed better generalization (forests AUC 0.6615, PR-AUC 0.8423). SHAP identified forests: soil organic carbon, tree cover, NDVI as top drivers; grasslands: land surface temperature, elevation, vegetation health indices. District-level results: Central Valley & Northern Buttes high-risk grasslands; Northern Buttes & North Coast Redwoods high-risk forests.

Conclusion: The RF-SHAP framework provides a robust, interpretable, and adaptable approach for wildfire risk assessment in California, supporting informed decisions and targeted mitigation strategies.

Abstract: Wildfires pose a significant global threat to ecosystems worldwide, with California experiencing recurring fires due to various factors, including climate, topographical features, vegetation patterns, and human activities. This study aims to develop a comprehensive wildfire risk map for California by applying the random forest (RF) algorithm, augmented with Explainable Artificial Intelligence (XAI) through Shapley Additive exPlanations (SHAP), to interpret model predictions. Model performance was assessed using both spatial and temporal validation strategies. The RF model demonstrated strong predictive performance, achieving near-perfect discrimination for grasslands (AUC = 0.996) and forests (AUC = 0.997). Spatial cross-validation revealed moderate transferability, yielding ROC-AUC values of 0.6155 for forests and 0.5416 for grasslands. In contrast, temporal split validation showed enhanced generalization, especially for forests (ROC-AUC = 0.6615, PR-AUC = 0.8423). SHAP-based XAI analysis identified key ecosystem-specific drivers: soil organic carbon, tree cover, and Normalized Difference Vegetation Index (NDVI) emerged as the most influential in forests, whereas Land Surface Temperature (LST), elevation, and vegetation health indices were dominant in grasslands. District-level classification revealed that Central Valley and Northern Buttes districts had the highest concentration of high-risk grasslands, while Northern Buttes and North Coast Redwoods dominated forested high-risk areas. This RF-SHAP framework offers a robust, comprehensible, and adaptable method for assessing wildfire risks, enabling informed decisions and creating targeted strategies to mitigate dangers.

</details>


### [484] [MPCM-Net: Multi-scale network integrates partial attention convolution with Mamba for ground-based cloud image segmentation](https://arxiv.org/abs/2511.11681)
*Penghui Niu,Jiashuai She,Taotao Cai,Yajuan Zhang,Ping Zhang,Junhua Gu,Jianxin Li*

Main category: cs.LG

TL;DR: MPCM-Net for ground-based cloud image segmentation that blends Partial attention Convolutions with Mamba architectures to boost segmentation accuracy and inference speed; introduces the CSRC dataset and releases code.


<details>
  <summary>Details</summary>
Motivation: Addresses key gaps in current deep learning approaches: (1) reliance on dilated convolutions giving suboptimal multi-scale feature effectiveness and inter-channel interaction; (2) attention-based enhancements compromising accuracy-throughput balance; (3) decoder designs failing to capture global dependencies among hierarchical features, hindering efficiency.

Method: MPCM-Net integrates Partial attention Convolutions with Mamba architectures. The encoder uses MPAC, comprising (1) MPC block with ParCM and ParSM for global spatial interaction across multi-scale cloud formations, and (2) MPA block combining ParAM and ParSM to extract discriminative features with reduced computation. The decoder employs M2B to mitigate contextual loss via SSHD, maintaining linear complexity while enabling deep aggregation across spatial and scale dimensions. Additionally, CSRC, a clear-label fine-grained cloud segmentation dataset, is introduced.

Result: Extensive experiments on the CSRC dataset demonstrate that MPCM-Net achieves superior segmentation accuracy while maintaining an optimal balance with inference speed, outperforming state-of-the-art methods.

Conclusion: MPCM-Net advances ground-based cloud segmentation by delivering high accuracy and efficiency through global multi-scale interactions and partial attention, aided by the new CSRC benchmark and available source code at the authors’ repository.

Abstract: Ground-based cloud image segmentation is a critical research domain for photovoltaic power forecasting. Current deep learning approaches primarily focus on encoder-decoder architectural refinements. However, existing methodologies exhibit several limitations:(1)they rely on dilated convolutions for multi-scale context extraction, lacking the partial feature effectiveness and interoperability of inter-channel;(2)attention-based feature enhancement implementations neglect accuracy-throughput balance; and (3)the decoder modifications fail to establish global interdependencies among hierarchical local features, limiting inference efficiency. To address these challenges, we propose MPCM-Net, a Multi-scale network that integrates Partial attention Convolutions with Mamba architectures to enhance segmentation accuracy and computational efficiency. Specifically, the encoder incorporates MPAC, which comprises:(1)a MPC block with ParCM and ParSM that enables global spatial interaction across multi-scale cloud formations, and (2)a MPA block combining ParAM and ParSM to extract discriminative features with reduced computational complexity. On the decoder side, a M2B is employed to mitigate contextual loss through a SSHD that maintains linear complexity while enabling deep feature aggregation across spatial and scale dimensions. As a key contribution to the community, we also introduce and release a dataset CSRC, which is a clear-label, fine-grained segmentation benchmark designed to overcome the critical limitations of existing public datasets. Extensive experiments on CSRC demonstrate the superior performance of MPCM-Net over state-of-the-art methods, achieving an optimal balance between segmentation accuracy and inference speed. The dataset and source code will be available at https://github.com/she1110/CSRC.

</details>


### [485] [Stratified Knowledge-Density Super-Network for Scalable Vision Transformers](https://arxiv.org/abs/2511.11683)
*Longhua Li,Lei Qi,Xin Geng*

Main category: cs.LG

TL;DR: A pre-trained ViT is transformed into a stratified knowledge-density super-network through WPAC (weighted PCA for attention contraction) and PIAD (importance-aware dropout) to enable flexible sub-networks at various sizes; the approach concentrates knowledge into a compact set of weights and shows competitive knowledge preservation and potential as an alternative to standard compression/expansion techniques.


<details>
  <summary>Details</summary>
Motivation: Training and deploying multiple ViT variants for different resource budgets is expensive; there is a need for a single adaptable network from which various sub-networks with maximal retained knowledge can be extracted.

Method: Introduce Weighted PCA for Attention Contraction (WPAC) to apply token-wise weighted PCA to intermediate features and inject the derived transformation/inverse matrices into adjacent layers, maintaining network function while concentrating knowledge. Propose Progressive Importance-Aware Dropout (PIAD) to rank weight-group importance, maintain an importance-based dropout list, and train the ViT super-network under this regime to promote stratified knowledge organization.

Result: WPAC outperforms existing pruning criteria in terms of knowledge concentration. When combined with PIAD, the approach offers a strong alternative to state-of-the-art model compression and model expansion methods without fully retraining multiple separate models.

Conclusion: A stratified knowledge-density super-network built with WPAC and PIAD enables flexible, knowledge-preserving sub-networks across sizes and provides competitive performance relative to cutting-edge compression and expansion techniques.

Abstract: Training and deploying multiple vision transformer (ViT) models for different resource constraints is costly and inefficient. To address this, we propose transforming a pre-trained ViT into a stratified knowledge-density super-network, where knowledge is hierarchically organized across weights. This enables flexible extraction of sub-networks that retain maximal knowledge for varying model sizes. We introduce \textbf{W}eighted \textbf{P}CA for \textbf{A}ttention \textbf{C}ontraction (WPAC), which concentrates knowledge into a compact set of critical weights. WPAC applies token-wise weighted principal component analysis to intermediate features and injects the resulting transformation and inverse matrices into adjacent layers, preserving the original network function while enhancing knowledge compactness. To further promote stratified knowledge organization, we propose \textbf{P}rogressive \textbf{I}mportance-\textbf{A}ware \textbf{D}ropout (PIAD). PIAD progressively evaluates the importance of weight groups, updates an importance-aware dropout list, and trains the super-network under this dropout regime to promote knowledge stratification. Experiments demonstrate that WPAC outperforms existing pruning criteria in knowledge concentration, and the combination with PIAD offers a strong alternative to state-of-the-art model compression and model expansion methods.

</details>


### [486] [A Bayesian Model for Multi-stage Censoring](https://arxiv.org/abs/2511.11684)
*Shuvom Sadhuka,Sophia Lin,Emma Pierson,Bonnie Berger*

Main category: cs.LG

TL;DR: A Bayesian model for sequential funnel decisions with selective censoring improves risk estimation and reveals gender-based ICU admission thresholds.


<details>
  <summary>Details</summary>
Motivation: Address biases from censoring in multi-stage healthcare decisions and improve prognostic accuracy for underserved groups.

Method: Develop a Bayesian funnel model incorporating selective labels/censoring; validate on synthetic data; apply to ED dataset where mortality is observed only for admitted patients; compare to baselines; estimate decision thresholds.

Result: In synthetic data, recovers true parameters and improves prediction for censored cases vs baselines; in ED data, identify gender differences: higher ICU admission mortality threshold for women (5.1%) vs men (4.5%).

Conclusion: Accounting for progressive censoring is crucial; method generalizes to other sequential decision settings and has implications for fairness; can better handle censored outcomes in healthcare analytics.

Abstract: Many sequential decision settings in healthcare feature funnel structures characterized by a series of stages, such as screenings or evaluations, where the number of patients who advance to each stage progressively decreases and decisions become increasingly costly. For example, an oncologist may first conduct a breast exam, followed by a mammogram for patients with concerning exams, followed by a biopsy for patients with concerning mammograms. A key challenge is that the ground truth outcome, such as the biopsy result, is only revealed at the end of this funnel. The selective censoring of the ground truth can introduce statistical biases in risk estimation, especially in underserved patient groups, whose outcomes are more frequently censored. We develop a Bayesian model for funnel decision structures, drawing from prior work on selective labels and censoring. We first show in synthetic settings that our model is able to recover the true parameters and predict outcomes for censored patients more accurately than baselines. We then apply our model to a dataset of emergency department visits, where in-hospital mortality is observed only for those who are admitted to either the hospital or ICU. We find that there are gender-based differences in hospital and ICU admissions. In particular, our model estimates that the mortality risk threshold to admit women to the ICU is higher for women (5.1%) than for men (4.5%).

</details>


### [487] [R-Tuning: Wavelet-Decomposed Replay and Semantic Alignment for Continual Adaptation of Pretrained Time-Series Models](https://arxiv.org/abs/2511.11685)
*Tianyi Yin,Jingwei Wang,Chenze Wang,Han Wang,Jiexuan Cai,Min Liu,Yunlong Ma,Kun Gao,Yuting Song,Weiming Shen*

Main category: cs.LG

TL;DR: R-Tuning is a continual adaptation framework for pre-trained time-series models that uses a frequency-aware replay in a unified latent space with wavelet-based augmentation and latent consistency, achieving strong gains on new tasks and preserving old task knowledge, especially in few-shot settings.


<details>
  <summary>Details</summary>
Motivation: The challenge is adapting pre-trained time-series models to evolving data distributions without access to original training data, which causes catastrophic forgetting when fine-tuning on new data alone.

Method: R-Tuning builds a unified latent space to capture prior and current task knowledge and employs a frequency-aware replay strategy: model-generated samples are augmented via wavelet-based decomposition across multiple frequency bands to produce trend-preserving and fusion-enhanced variants. A latent consistency constraint aligns new representations with the prior task space, enabling joint optimization in a compact latent space.

Result: On new tasks, MAE and MSE are reduced by up to 46.9% and 46.8% respectively; on old tasks, knowledge is preserved with gains of up to 5.7% and 6.0%. In few-shot scenarios, R-Tuning outperforms SOTA baselines even when synthetic proxy samples constitute only 5% of the new task data.

Conclusion: R-Tuning effectively enables continual adaptation of pre-trained time-series models by combining frequency-aware replay with latent consistency constraints, achieving strong results on new tasks while mitigating forgetting on old tasks, particularly in low-data regimes.

Abstract: Pre-trained models have demonstrated exceptional generalization capabilities in time-series forecasting; however, adapting them to evolving data distributions remains a significant challenge. A key hurdle lies in accessing the original training data, as fine-tuning solely on new data often leads to catastrophic forgetting. To address this issue, we propose Replay Tuning (R-Tuning), a novel framework designed for the continual adaptation of pre-trained time-series models. R-Tuning constructs a unified latent space that captures both prior and current task knowledge through a frequency-aware replay strategy. Specifically, it augments model-generated samples via wavelet-based decomposition across multiple frequency bands, generating trend-preserving and fusion-enhanced variants to improve representation diversity and replay efficiency. To further reduce reliance on synthetic samples, R-Tuning introduces a latent consistency constraint that aligns new representations with the prior task space. This constraint guides joint optimization within a compact and semantically coherent latent space, ensuring robust knowledge retention and adaptation. Extensive experimental results demonstrate the superiority of R-Tuning, which reduces MAE and MSE by up to 46.9% and 46.8%, respectively, on new tasks, while preserving prior knowledge with gains of up to 5.7% and 6.0% on old tasks. Notably, under few-shot settings, R-Tuning outperforms all state-of-the-art baselines even when synthetic proxy samples account for only 5% of the new task dataset.

</details>


### [488] [Regularized Schrödinger: Alleviating Distortion and Exposure Bias in Solving Inverse Problems](https://arxiv.org/abs/2511.11686)
*Qing Yao,Lijian Gao,Qirong Mao,Dong Ming*

Main category: cs.LG

TL;DR: Regularized Schrödinger Bridge (RSB) for inverse problems improves diffusion-model-based reconstruction by addressing distortion-perception tradeoff and exposure bias. It regularizes training by perturbing both inputs and targets, exposes the model to simulated prediction errors, and uses posterior-mean interpolation to mitigate distortion. Demonstrated on two speech enhancement inverse problems, it surpasses state-of-the-art methods in distortion metrics and reduces exposure bias.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are powerful for inverse problems but suffer from two major issues: (1) distortion-perception tradeoff, where higher perceptual quality can hurt reconstruction fidelity, and (2) exposure bias, where training-inference input mismatch leads to error accumulation and degraded performance. There is a need for an approach that jointly improves perceptual quality, fidelity, and robustness to prediction errors.

Method: Adapt Schrödinger Bridge to inverse problems with a regularized training strategy. The training perturbs both input states and targets to simulate prediction errors (mitigating exposure bias) and uses well-designed interpolation via the posterior mean to improve distortion properties. This Regularized Schrödinger Bridge (RSB) aligns diffusion-based generation with inverse problem constraints and reduces accumulated errors during inference.

Result: Experiments on two typical inverse problems in speech enhancement show that RSB outperforms state-of-the-art methods, achieving significant improvements in distortion metrics and effectively reducing exposure bias.

Conclusion: RSB successfully mitigates the core limitations of diffusion-based inverse problem solvers—distortion-perception tradeoff and exposure bias—demonstrating strong empirical gains on speech enhancement tasks and potential applicability to broader inverse problems.

Abstract: Diffusion models serve as a powerful generative framework for solving inverse problems. However, they still face two key challenges: 1) the distortion-perception tradeoff, where improving perceptual quality often degrades reconstruction fidelity, and 2) the exposure bias problem, where the training-inference input mismatch leads to prediction error accumulation and reduced reconstruction quality. In this work, we propose the Regularized Schrödinger Bridge (RSB), an adaptation of Schrödinger Bridge tailored for inverse problems that addresses the above limitations. RSB employs a novel regularized training strategy that perturbs both the input states and targets, effectively mitigating exposure bias by exposing the model to simulated prediction errors and also alleviating distortion by well-designed interpolation via the posterior mean. Extensive experiments on two typical inverse problems for speech enhancement demonstrate that RSB outperforms state-of-the-art methods, significantly improving distortion metrics and effectively reducing exposure bias.

</details>


### [489] [Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling](https://arxiv.org/abs/2511.11688)
*Aihua Zhu,Rui Su,Qinglin Zhao,Li Feng,Meng Shen,Shibo He*

Main category: cs.LG

TL;DR: HSO is a bi-level training-free scheduler for diffusion models, using MEP and SPF to optimize timesteps, achieving state-of-the-art low-NFE sampling with minimal optimization time.


<details>
  <summary>Details</summary>
Motivation: Diffusion models deliver high generative fidelity but suffer slow iterative sampling. A training-free schedule optimization is desirable if it satisfies effectiveness, adaptivity, practical robustness, and computational efficiency; existing methods struggle to balance all four.

Method: Bi-level optimization: an upper-level global search for a good initialization and a lower-level local optimization guided by two innovations—Midpoint Error Proxy (MEP) as a stable, solver-agnostic objective and Spacing-Penalized Fitness (SPF) to penalize timesteps that are too close. The approach is training-free, with a one-time optimization cost under 8 seconds, and relies on a fixed, small NFE.

Result: HSO achieves state-of-the-art training-free sampling in the extremely low-NFE regime; for example, with NFE=5, it attains FID 11.94 on LAION-Aesthetics using Stable Diffusion v2.1.

Conclusion:  HSOs hierarchical bi-level schedule optimization enables practical, efficient acceleration of diffusion sampling without retraining, providing a fast, robust, and effective paradigm for very low-NFE generation.

Abstract: Diffusion probabilistic models have set a new standard for generative fidelity but are hindered by a slow iterative sampling process. A powerful training-free strategy to accelerate this process is Schedule Optimization, which aims to find an optimal distribution of timesteps for a fixed and small Number of Function Evaluations (NFE) to maximize sample quality. To this end, a successful schedule optimization method must adhere to four core principles: effectiveness, adaptivity, practical robustness, and computational efficiency. However, existing paradigms struggle to satisfy these principles simultaneously, motivating the need for a more advanced solution. To overcome these limitations, we propose the Hierarchical-Schedule-Optimizer (HSO), a novel and efficient bi-level optimization framework. HSO reframes the search for a globally optimal schedule into a more tractable problem by iteratively alternating between two synergistic levels: an upper-level global search for an optimal initialization strategy and a lower-level local optimization for schedule refinement. This process is guided by two key innovations: the Midpoint Error Proxy (MEP), a solver-agnostic and numerically stable objective for effective local optimization, and the Spacing-Penalized Fitness (SPF) function, which ensures practical robustness by penalizing pathologically close timesteps. Extensive experiments show that HSO sets a new state-of-the-art for training-free sampling in the extremely low-NFE regime. For instance, with an NFE of just 5, HSO achieves a remarkable FID of 11.94 on LAION-Aesthetics with Stable Diffusion v2.1. Crucially, this level of performance is attained not through costly retraining, but with a one-time optimization cost of less than 8 seconds, presenting a highly practical and efficient paradigm for diffusion model acceleration.

</details>


### [490] [Doubly Debiased Test-Time Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2511.11690)
*Fei Song,Yi Li,Rui Wang,Jiahuan Zhou,Changwen Zheng,Jiangmeng Li*

Main category: cs.LG

TL;DR: Doubly Debiased Test-Time Prompt Tuning (DDTTPT) improves vision-language model generalization by addressing prompt optimization bias with two modules: (1) a dynamic retrieval-augmented modulation that fetches high-confidence knowledge using the test image as a query to refine predictions, and (2) a reliability-aware prompt optimization that uses a confidence-based weighted ensemble and cross-modal consistency distillation to regularize tuning.


<details>
  <summary>Details</summary>
Motivation: Prompt tuning at test time can cause optimization bias: entropy minimization pushes for confident but potentially incorrect outputs, and misalignment between visual and textual modalities during unlabeled test-time tuning harms downstream performance. The goal is to reduce this bias to improve generalization across tasks and domains.

Method: Introduce DDTTPT with two components: (1) dynamic retrieval-augmented modulation that retrieves relevant, high-confidence knowledge from a dynamic knowledge base using the test image as a query and modulates predictions accordingly; (2) reliability-aware prompt optimization that applies a confidence-weighted ensemble and cross-modal consistency distillation to regularize the prompt tuning process.

Result: Extensive experiments on 15 benchmark datasets with natural distribution shifts and cross-dataset generalization show the proposed method outperforms baselines, validating its ability to mitigate prompt optimization bias and enhance generalization.

Conclusion: The doubly debiased framework effectively reduces prompt optimization bias at test time and improves the robustness and generalization of vision-language models across diverse distribution shifts.

Abstract: Test-time prompt tuning for vision-language models has demonstrated impressive generalization capabilities under zero-shot settings. However, tuning the learnable prompts solely based on unlabeled test data may induce prompt optimization bias, ultimately leading to suboptimal performance on downstream tasks. In this work, we analyze the underlying causes of prompt optimization bias from both the model and data perspectives. In terms of the model, the entropy minimization objective typically focuses on reducing the entropy of model predictions while overlooking their correctness. This can result in overconfident yet incorrect outputs, thereby compromising the quality of prompt optimization. On the data side, prompts affected by optimization bias can introduce misalignment between visual and textual modalities, which further aggravates the prompt optimization bias. To this end, we propose a Doubly Debiased Test-Time Prompt Tuning method. Specifically, we first introduce a dynamic retrieval-augmented modulation module that retrieves high-confidence knowledge from a dynamic knowledge base using the test image feature as a query, and uses the retrieved knowledge to modulate the predictions. Guided by the refined predictions, we further develop a reliability-aware prompt optimization module that incorporates a confidence-based weighted ensemble and cross-modal consistency distillation to impose regularization constraints during prompt tuning. Extensive experiments across 15 benchmark datasets involving both natural distribution shifts and cross-datasets generalization demonstrate that our method outperforms baselines, validating its effectiveness in mitigating prompt optimization bias.

</details>


### [491] [Beyond saliency: enhancing explanation of speech emotion recognition with expert-referenced acoustic cues](https://arxiv.org/abs/2511.11691)
*Seham Nasr,Zhao Ren,David Johnson*

Main category: cs.LG

TL;DR: Proposes a framework to quantify acoustic cue magnitudes within salient spectrogram regions to make saliency explanations for SER more faithful to theory-driven cues, improving interpretability over standard saliency methods.


<details>
  <summary>Details</summary>
Motivation: Current saliency explanations for Speech Emotion Recognition adapt vision techniques to spectrograms but fail to show that highlighted regions correspond to meaningful acoustic cues; need explanations that align with expert-referenced acoustic markers for trustworthy SER models.

Method: Introduce a framework that quantifies the magnitudes of expert-referenced acoustic cues within salient regions, linking saliency to theory-driven cues of speech emotions.

Result: On benchmark SER datasets, the method yields explanations that are more understandable and plausible, by explicitly connecting highlighted regions to acoustic cues, outperforming standard saliency baselines.

Conclusion: This approach advances trustworthy, theory-grounded explainability in speech-based affective computing by clarifying what is highlighted and why it matters.

Abstract: Explainable AI (XAI) for Speech Emotion Recognition (SER) is critical for building transparent, trustworthy models. Current saliency-based methods, adapted from vision, highlight spectrogram regions but fail to show whether these regions correspond to meaningful acoustic markers of emotion, limiting faithfulness and interpretability. We propose a framework that overcomes these limitations by quantifying the magnitudes of cues within salient regions. This clarifies "what" is highlighted and connects it to "why" it matters, linking saliency to expert-referenced acoustic cues of speech emotions. Experiments on benchmark SER datasets show that our approach improves explanation quality by explicitly linking salient regions to theory-driven speech emotions expert-referenced acoustics. Compared to standard saliency methods, it provides more understandable and plausible explanations of SER models, offering a foundational step towards trustworthy speech-based affective computing.

</details>


### [492] [AnchorDS: Anchoring Dynamic Sources for Semantically Consistent Text-to-3D Generation](https://arxiv.org/abs/2511.11692)
*Jiayin Zhu,Linlin Yang,Yicong Li,Angela Yao*

Main category: cs.LG

TL;DR: AnchorDS reframes SDS-based text-to-3D as a dynamic distribution mapping problem using a dual-conditioned latent space (text + intermediate image), introducing an image-anchored score distillation with lightweight filtering and fine-tuning to achieve finer detail, natural colors, and stronger semantic consistency with better efficiency.


<details>
  <summary>Details</summary>
Motivation: Semantic over-smoothing in optimization-based text-to-3D arises from treating guidance as static, causing inconsistent trajectories and collapsed cues. A dynamic view and image-conditioned anchoring can preserve semantic structure and improve stability.

Method: Introduce dual-conditioned latent space conditioned on text and the current rendered image; use image condition to anchor the evolving source distribution; develop AnchorDS as enhanced score distillation with image-conditioned guidance; penalize erroneous source estimates; propose lightweight filtering and fine-tuning to refine the anchor with minimal overhead.

Result: Empirically outperforms prior SDS-based methods in quality and efficiency; yields finer-grained detail, more natural colors, and stronger semantic consistency, especially for complex prompts.

Conclusion: AnchorDS provides state-anchored guidance with image conditions, stabilizing text-to-3D optimization while maintaining efficiency; suitable for complex prompts and scalable to practice.

Abstract: Optimization-based text-to-3D methods distill guidance from 2D generative models via Score Distillation Sampling (SDS), but implicitly treat this guidance as static. This work shows that ignoring source dynamics yields inconsistent trajectories that suppress or merge semantic cues, leading to "semantic over-smoothing" artifacts. As such, we reformulate text-to-3D optimization as mapping a dynamically evolving source distribution to a fixed target distribution. We cast the problem into a dual-conditioned latent space, conditioned on both the text prompt and the intermediately rendered image. Given this joint setup, we observe that the image condition naturally anchors the current source distribution. Building on this insight, we introduce AnchorDS, an improved score distillation mechanism that provides state-anchored guidance with image conditions and stabilizes generation. We further penalize erroneous source estimates and design a lightweight filter strategy and fine-tuning strategy that refines the anchor with negligible overhead. AnchorDS produces finer-grained detail, more natural colours, and stronger semantic consistency, particularly for complex prompts, while maintaining efficiency. Extensive experiments show that our method surpasses previous methods in both quality and efficiency.

</details>


### [493] [Toward Dignity-Aware AI: Next-Generation Elderly Monitoring from Fall Detection to ADL](https://arxiv.org/abs/2511.11696)
*Xun Shao,Aoba Otani,Yuto Hirasuka,Runji Cai,Seng W. Loke*

Main category: cs.LG

TL;DR: Proposes privacy-preserving, edge-enabled federated AI for ADL recognition in elderly care; uses SISFall with GAN-augmented data; demonstrates federated non-IID learning and Jetson Orin Nano deployment; outlines challenges and a roadmap to full ADL monitoring.


<details>
  <summary>Details</summary>
Motivation: Address aging society needs for ongoing independence and dignity; go beyond fall detection to ADL recognition while preserving privacy, handling data scarcity and privacy risks.

Method: Feasibility study using SISFall dataset with GAN-augmented variants; treat fall detection as proxy task; implement federated learning under non-IID conditions; deploy embedded on Jetson Orin Nano; discuss future smart-room ADL monitoring.

Result: Initial results show feasibility: federated learning under non-IID conditions yields preliminary performance; successful edge deployment on Jetson Orin Nano; ADL dataset collection still in progress.

Conclusion: Outlines open challenges (domain shift, data scarcity, privacy risks) and provides a roadmap toward full ADL monitoring in smart environments; highlights shift from single-task detection to holistic daily activity recognition for elder care AI.

Abstract: This position paper envisions a next-generation elderly monitoring system that moves beyond fall detection toward the broader goal of Activities of Daily Living (ADL) recognition. Our ultimate aim is to design privacy-preserving, edge-deployed, and federated AI systems that can robustly detect and understand daily routines, supporting independence and dignity in aging societies. At present, ADL-specific datasets are still under collection. As a preliminary step, we demonstrate feasibility through experiments using the SISFall dataset and its GAN-augmented variants, treating fall detection as a proxy task. We report initial results on federated learning with non-IID conditions, and embedded deployment on Jetson Orin Nano devices. We then outline open challenges such as domain shift, data scarcity, and privacy risks, and propose directions toward full ADL monitoring in smart-room environments. This work highlights the transition from single-task detection to comprehensive daily activity recognition, providing both early evidence and a roadmap for sustainable and human-centered elderly care AI.

</details>


### [494] [Benchmarking GNNs for OOD Materials Property Prediction with Uncertainty Quantification](https://arxiv.org/abs/2511.11697)
*Liqin Tan,Pin Chen,Menghan Liu,Xiean Wang,Jianhuan Cen,Qingsong Zou*

Main category: cs.LG

TL;DR: MatUQ is a benchmark for evaluating GNNs on out-of-distribution (OOD) materials property prediction with uncertainty quantification (UQ). It introduces SOAP-LOCO splitting, 1,375 OOD tasks from six datasets, 12 GNN models, and a unified uncertainty-aware training protocol (Monte Carlo Dropout + Deep Evidential Regression) along with a new uncertainty metric D-EviU. Key findings include substantial error reductions under OOD and a model-agnostic performance landscape where older models remain competitive and newer ones excel on specific properties.


<details>
  <summary>Details</summary>
Motivation: Address distribution shifts in materials property prediction by providing a standardized, uncertainty-aware evaluation framework to compare diverse GNNs and identify reliable models for materials discovery.

Method: Construct MatUQ with 1,375 OOD tasks sourced from six materials datasets using five OFM-based divisions and the SOAP-LOCO structure-aware splitting strategy. Evaluate 12 representative GNNs under a unified uncertainty-aware training protocol that combines Monte Carlo Dropout and Deep Evidential Regression (DER). Introduce D-EviU, a new uncertainty metric that correlates strongly with prediction errors. Systematically assess model performance across challenging OOD tasks.

Result: Uncertainty-aware training substantially improves accuracy, reducing errors by 70.6% on challenging OOD scenarios. D-EviU shows the strongest correlation with prediction errors across tasks. No single model dominates universally; older models like SchNet and ALIGNN remain competitive, while newer models such as CrystalFramer and SODNet outperform on specific material properties.

Conclusion: MatUQ provides practical guidance for selecting reliable GNNs under distribution shifts in materials discovery and establishes a robust benchmark and metric suite for uncertainty-aware evaluation of materials property prediction.

Abstract: We present MatUQ, a benchmark framework for evaluating graph neural networks (GNNs) on out-of-distribution (OOD) materials property prediction with uncertainty quantification (UQ). MatUQ comprises 1,375 OOD prediction tasks constructed from six materials datasets using five OFM-based and a newly proposed structure-aware splitting strategy, SOAP-LOCO, which captures local atomic environments more effectively. We evaluate 12 representative GNN models under a unified uncertainty-aware training protocol that combines Monte Carlo Dropout and Deep Evidential Regression (DER), and introduce a novel uncertainty metric, D-EviU, which shows the strongest correlation with prediction errors in most tasks. Our experiments yield two key findings. First, the uncertainty-aware training approach significantly improves model prediction accuracy, reducing errors by an average of 70.6\% across challenging OOD scenarios. Second, the benchmark reveals that no single model dominates universally: earlier models such as SchNet and ALIGNN remain competitive, while newer models like CrystalFramer and SODNet demonstrate superior performance on specific material properties. These results provide practical insights for selecting reliable models under distribution shifts in materials discovery.

</details>


### [495] [Enhancing Reinforcement Learning in 3D Environments through Semantic Segmentation: A Case Study in ViZDoom](https://arxiv.org/abs/2511.11703)
*Hugo Huang*

Main category: cs.LG

TL;DR: Proposes two input representations for RL in 3D environments using semantic segmentation: SS-only and RGB+SS. SS-only dramatically reduces memory usage; RGB+SS boosts performance. Evaluates with perfect segmentation in ViZDoom, uses density heatmaps for visualization, and compares to prior work to address segmentation pitfalls.


<details>
  <summary>Details</summary>
Motivation: Tackle two core RL challenges in 3D perception: (1) high memory use of experience replay buffers for stable learning, and (2) learning under partial observability in POMDPs.

Method: Introduce SS-only (semantic segmentation channels only) and RGB+SS (RGB plus segmentation) input representations applied to ViZDoom deathmatches with perfect segmentation. Evaluate memory footprint and data efficiency with a vectorizable compression (e.g., run-length encoding). Use density-based heatmaps to visualize agent movement and assess data collection. Compare with a previous approach to highlight improvements and common pitfalls.

Result: SS-only reduces memory buffer usage by 66.6% to 98.6% when using lightweight lossless compression (e.g., RLE). RGB+SS improves RL performance due to added semantic information. Density heatmaps provide insights into movement patterns. A brief comparison shows the method overcomes typical semantic segmentation pitfalls in ViZDoom setups.

Conclusion: Semantic segmentation-informed inputs can simultaneously reduce memory demands and enhance policy learning in 3D RL, while density-based visualizations aid data collection. The approach also addresses several common pitfalls associated with applying segmentation in 3D environments like ViZDoom.

Abstract: Reinforcement learning (RL) in 3D environments with high-dimensional sensory input poses two major challenges: (1) the high memory consumption induced by memory buffers required to stabilise learning, and (2) the complexity of learning in partially observable Markov Decision Processes (POMDPs). This project addresses these challenges by proposing two novel input representations: SS-only and RGB+SS, both employing semantic segmentation on RGB colour images. Experiments were conducted in deathmatches of ViZDoom, utilizing perfect segmentation results for controlled evaluation. Our results showed that SS-only was able to reduce the memory consumption of memory buffers by at least 66.6%, and up to 98.6% when a vectorisable lossless compression technique with minimal overhead such as run-length encoding is applied. Meanwhile, RGB+SS significantly enhances RL agents' performance with the additional semantic information provided. Furthermore, we explored density-based heatmapping as a tool to visualise RL agents' movement patterns and evaluate their suitability for data collection. A brief comparison with a previous approach highlights how our method overcame common pitfalls in applying semantic segmentation in 3D environments like ViZDoom.

</details>


### [496] [Moirai 2.0: When Less Is More for Time Series Forecasting](https://arxiv.org/abs/2511.11698)
*Chenghao Liu,Taha Aksu,Juncheng Liu,Xu Liu,Hanshu Yan,Quang Pham,Doyen Sahoo,Caiming Xiong,Silvio Savarese,Junnan Li*

Main category: cs.LG

TL;DR: Decoder-only Moirai 2.0 time-series foundation model using quantile forecasting achieves strong probabilistic accuracy and efficiency, outperforming Moirai 1.0 and other pretrained models while being faster and far smaller.


<details>
  <summary>Details</summary>
Motivation: Address limitations of Moirai 1.0 and improve probabilistic time-series forecasting efficiency by using a decoder-only architecture, single patch input, and quantile loss.

Method: Decoder-only Transformer trained on 36M time-series; employs quantile forecasting with multi-token prediction and recursive multi-quantile decoding; ablation studies isolate impact of decoder-only backbone, single patch, and quantile loss.

Result: On Gift-Eval, Moirai 2.0 ranks among top pretrained models, with a favorable accuracy–speed–size trade-off; it is twice as fast and thirty times smaller than Moirai 1.0-Large while outperforming it; outperforms larger models from the same family and shows robust domain-level results; performance plateaus with more parameters and degrades at long horizons.

Conclusion: Decoder-only backbone plus recursive multi-quantile decoding drives most gains; results motivate data scaling and long-horizon modeling; code and evaluation details are released to support further research.

Abstract: We introduce Moirai 2.0, a decoder-only time-series foundation model trained on a new corpus of 36M series. The model adopts quantile forecasting and multi-token prediction, improving both probabilistic accuracy and inference efficiency. On the Gift-Eval benchmark, it ranks among the top pretrained models while achieving a strong trade-off between accuracy, speed, and model size. Compared to Moirai 1.0, Moirai 2.0 replaces masked-encoder training, multi-patch inputs, and mixture-distribution outputs with a simpler decoder-only architecture, single patch, and quantile loss. Ablation studies isolate these changes -- showing that the decoder-only backbone along with recursive multi-quantile decoding contribute most to the gains. Additional experiments show that Moirai 2.0 outperforms larger models from the same family and exhibits robust domain-level results. In terms of efficiency and model size, Moirai 2.0 is twice as fast and thirty times smaller than its prior best version, Moirai 1.0-Large, while also performing better. Model performance plateaus with increasing parameter count and declines at longer horizons, motivating future work on data scaling and long-horizon modeling. We release code and evaluation details to support further research.

</details>


### [497] [Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification](https://arxiv.org/abs/2511.11699)
*Xingqi Lin,Liangyu Chen,Min Wu,Min Zhang,Zhenbing Zeng*

Main category: cs.LG

TL;DR: A DeepPrism-based framework tightens the linear relaxation of RNN nonlinearities (Hadamard product) via a truncated rectangular prism bounded by two relaxation planes and a refinement-driven scheme, yielding tighter over-approximation and improved robustness verification.


<details>
  <summary>Details</summary>
Motivation: Over-approximating nonlinear activations in RNNs with linear constraints introduces significant overestimation, hurting robustness verification accuracy. Existing methods relax each nonlinearity with a separate plane, which is insufficient for the Hadamard product. A tighter geometric enclosure is needed to improve verification precision across diverse tasks.

Method: Introduce a truncated rectangular prism to tightly enclose the 3D surface generated by the Hadamard product. This prism is formed by two linear relaxation planes and an optimization-driven refinement procedure that minimizes volume and surface area. Build a prototype DeepPrism for RNN robustness verification and evaluate it on image classification, speech recognition, and sentiment analysis tasks.

Result: DeepPrism achieves significant improvements over state-of-the-art verification approaches across multiple tasks (image classification, speech recognition, sentiment analysis), demonstrating tighter over-approximation and enhanced verification accuracy.

Conclusion: Tighter linear relaxation of Hadamard-product nonlinearities via the truncated prism and refinement strategy substantially improves RNN robustness verification, suggesting broad applicability to RNN safety verification and related domains.

Abstract: Robustness verification is a promising technique for rigorously proving Recurrent Neural Networks (RNNs) robustly. A key challenge is to over-approximate the nonlinear activation functions with linear constraints, which can transform the verification problem into an efficiently solvable linear programming problem. Existing methods over-approximate the nonlinear parts with linear bounding planes individually, which may cause significant over-estimation and lead to lower verification accuracy. In this paper, in order to tightly enclose the three-dimensional nonlinear surface generated by the Hadamard product, we propose a novel truncated rectangular prism formed by two linear relaxation planes and a refinement-driven method to minimize both its volume and surface area for tighter over-approximation. Based on this approximation, we implement a prototype DeepPrism for RNN robustness verification. The experimental results demonstrate that \emph{DeepPrism} has significant improvement compared with the state-of-the-art approaches in various tasks of image classification, speech recognition and sentiment analysis.

</details>


### [498] [Are LLMs The Way Forward? A Case Study on LLM-Guided Reinforcement Learning for Decentralized Autonomous Driving](https://arxiv.org/abs/2511.12751)
*Timur Anvar,Jeffrey Chen,Yuyan Wang,Rohan Chandra*

Main category: cs.LG

TL;DR: Small, locally deployed LLMs can aid RL in autonomous highway driving by shaping rewards rather than controlling actions. Hybrid RL+LLM approaches lie between pure RL and pure LLM methods; however, LLM-driven methods exhibit conservative biases and high model-dependent variability, underscoring safety-related limitations.


<details>
  <summary>Details</summary>
Motivation: To alleviate the shortcomings of RL reward design and the instability/latency of API-based LLMs by testing whether compact, local LLMs can meaningfully support autonomous highway navigation through reward shaping.

Method: A case study comparing RL-only, LLM-only, and hybrid configurations. LLMs (<14B params) score state-action transitions during training to shape rewards; the final policy is executed by standard RL at test time. Traffic scenarios include dense, fast-moving highways and merging. Metrics include success rate and efficiency/speed.

Result: RL-only agents achieved moderate success (73–89%) with reasonable efficiency. LLM-only agents reached higher success rates (up to 94%) but with severely degraded speed. Hybrid approaches consistently fell between these extremes. LLM-influenced methods showed systematic conservative bias and substantial model-dependent variability, indicating important safety-related limitations of current small LLMs.

Conclusion: Small, locally deployed LLMs can contribute to RL-based autonomous driving via reward shaping, but reliability, efficiency, and safety concerns remain. Hybrid strategies do not outperform pure RL in a consistent manner, and the conservative bias and variability of small LLMs limit their current practicality for safety-critical control.

Abstract: Autonomous vehicle navigation in complex environments such as dense and fast-moving highways and merging scenarios remains an active area of research. A key limitation of RL is its reliance on well-specified reward functions, which often fail to capture the full semantic and social complexity of diverse, out-of-distribution situations. As a result, a rapidly growing line of research explores using Large Language Models (LLMs) to replace or supplement RL for direct planning and control, on account of their ability to reason about rich semantic context. However, LLMs present significant drawbacks: they can be unstable in zero-shot safety-critical settings, produce inconsistent outputs, and often depend on expensive API calls with network latency. This motivates our investigation into whether small, locally deployed LLMs (< 14B parameters) can meaningfully support autonomous highway driving through reward shaping rather than direct control. We present a case study comparing RL-only, LLM-only, and hybrid approaches, where LLMs augment RL rewards by scoring state-action transitions during training, while standard RL policies execute at test time. Our findings reveal that RL-only agents achieve moderate success rates (73-89%) with reasonable efficiency, LLM-only agents can reach higher success rates (up to 94%) but with severely degraded speed performance, and hybrid approaches consistently fall between these extremes. Critically, despite explicit efficiency instructions, LLM-influenced approaches exhibit systematic conservative bias with substantial model-dependent variability, highlighting important limitations of current small LLMs for safety-critical control tasks.

</details>


### [499] [Bayesian Neural Networks with Monte Carlo Dropout for Probabilistic Electricity Price Forecasting](https://arxiv.org/abs/2511.11701)
*Abhinav Das,Stephan Schlüter*

Main category: cs.LG

TL;DR: Bayesian neural networks with MC dropout for probabilistic hourly electricity price forecasts, outperforming GARCHX and LEAR; hourly-specific models capture diurnal patterns.


<details>
  <summary>Details</summary>
Motivation: Electricity markets are volatile and uncertain; probabilistic forecasts are essential for risk management in deregulated markets; existing point forecasts are insufficient.

Method: Train separate Bayesian neural networks for each hour with MC dropout to obtain predictive distributions; compare against GARCHX and LEAR benchmarks.

Result: Outperforms benchmarks in both point forecasts and prediction intervals; demonstrates value of probabilistic neural models for energy market prediction.

Conclusion: Framework provides a reference for using probabilistic neural models in energy market forecasting and highlights benefits of hourly-specific BNNs with MC dropout.

Abstract: Accurate electricity price forecasting is critical for strategic decision-making in deregulated electricity markets, where volatility stems from complex supply-demand dynamics and external factors. Traditional point forecasts often fail to capture inherent uncertainties, limiting their utility for risk management. This work presents a framework for probabilistic electricity price forecasting using Bayesian neural networks (BNNs) with Monte Carlo (MC) dropout, training separate models for each hour of the day to capture diurnal patterns. A critical assessment and comparison with the benchmark model, namely: generalized autoregressive conditional heteroskedasticity with exogenous variable (GARCHX) model and the LASSO estimated auto-regressive model (LEAR), highlights that the proposed model outperforms the benchmark models in terms of point prediction and intervals. This work serves as a reference for leveraging probabilistic neural models in energy market predictions.

</details>


### [500] [Simple Vision-Language Math Reasoning via Rendered Text](https://arxiv.org/abs/2511.11704)
*Matvey Skripkin,Elizaveta Goncharova,Andrey Kuznetsov*

Main category: cs.LG

TL;DR: A lightweight text-to-vision pipeline renders LaTeX equations as images and pairs them with structured chain-of-thought prompts to train compact vision-language models that excel at math reasoning, achieving state-of-the-art results on multiple benchmarks while retaining broad general-domain competence.


<details>
  <summary>Details</summary>
Motivation: To improve math problem-solving in vision-language models with a simple, scalable augmentation that leverages existing textual reasoning prompts and visual rendering of mathematical expressions, enabling smaller, efficient multimodal architectures.

Method: Render LaTeX-encoded equations into images and pair them with structured chain-of-thought prompts as a training signal in a lightweight vision-language pipeline. Perform systematic ablations to identify rendering fidelity and prompt design as key performance drivers.

Result: Achieves state-of-the-art reasoning accuracy on math-focused benchmarks, matches or surpasses open-source and proprietary math-VL solvers, and maintains broad general-domain competence with gains on MMMU, ChartQA, and DocVQA up to 20%.

Conclusion: A simple, faithful rendering and prompting strategy can unlock strong multimodal math reasoning in compact models, with rendering fidelity and prompt design being the primary drivers of performance.

Abstract: We present a lightweight yet effective pipeline for training vision-language models to solve math problems by rendering LaTeX encoded equations into images and pairing them with structured chain-of-thought prompts. This simple text-to-vision augmentation enables compact multimodal architectures to achieve state-of-the-art reasoning accuracy. Through systematic ablations, we find that rendering fidelity and prompt design are the primary drivers of performance. Despite its simplicity, our approach consistently matches or surpasses both open-source and proprietary math-focused vision-language solvers on widely used benchmarks, while preserving broad general-domain competence - showing gains on tasks such as MMMU, ChartQA, and DocVQA of up to 20%.

</details>


### [501] [Multimodal ML: Quantifying the Improvement of Calorie Estimation Through Image-Text Pairs](https://arxiv.org/abs/2511.11705)
*Arya Narang*

Main category: cs.LG

TL;DR: Multimodal model using dish names as text alongside food images yields a small MAE improvement for calorie estimation on Nutrition5k.


<details>
  <summary>Details</summary>
Motivation: Assess whether short textual inputs can meaningfully improve calorie estimation beyond image-based models and determine statistical significance.

Method: Train two CNNs in TensorFlow: an image-only CNN and a Multimodal CNN that ingests both image and text (dish name); evaluate on Nutrition5k dataset.

Result: MAE reduced from 84.76 kcal to 83.70 kcal (−1.06 kcal; 1.25% improvement) for the multimodal model.

Conclusion: Incorporating text with images yields a modest improvement in calorie estimation; the abstract does not report statistical significance, so further tests are needed to confirm significance.

Abstract: This paper determines the extent to which short textual inputs (in this case, names of dishes) can improve calorie estimation compared to an image-only baseline model and whether any improvements are statistically significant. Utilizes the TensorFlow library and the Nutrition5k dataset (curated by Google) to train both an image-only CNN and multimodal CNN that accepts both text and an image as input. The MAE of calorie estimations was reduced by 1.06 kcal from 84.76 kcal to 83.70 kcal (1.25% improvement) when using the multimodal model.

</details>


### [502] [Context-Aware Multimodal Representation Learning for Spatio-Temporally Explicit Environmental modelling](https://arxiv.org/abs/2511.11706)
*Julia Peters,Karin Mora,Miguel D. Mahecha,Chaonan Ji,David Montero,Clemens Mosig,Guido Kraemer*

Main category: cs.LG

TL;DR: A two-stage multi-modal EO representation learning framework that fuses Sentinel-1 and Sentinel-2 into a unified high-resolution latent space (10 m, cloud-free Sentinel-2 cadence), enabling scalable, analysis-ready embeddings for ecological modelling.


<details>
  <summary>Details</summary>
Motivation: Overcome fixed-scale limitations of existing EO foundation models to support simultaneous high spatial detail and high temporal fidelity.

Method: Independently encode each modality with sensor-tailored encoders, then fuse in a shared model; freezing encoders and retraining only fusion layers; extendable to new sensors.

Result: Learned embeddings show spatial/semantic coherence; qualitative analyses; improved GPP modeling with preserved temporal fidelity; configurable to other sensors.

Conclusion: Provides a flexible, extension-friendly pipeline for high-res, cross-sensor EO representations suitable for ecosystem analyses.

Abstract: Earth observation (EO) foundation models have emerged as an effective approach to derive latent representations of the Earth system from various remote sensing sensors. These models produce embeddings that can be used as analysis-ready datasets, enabling the modelling of ecosystem dynamics without extensive sensor-specific preprocessing. However, existing models typically operate at fixed spatial or temporal scales, limiting their use for ecological analyses that require both fine spatial detail and high temporal fidelity. To overcome these limitations, we propose a representation learning framework that integrates different EO modalities into a unified feature space at high spatio-temporal resolution. We introduce the framework using Sentinel-1 and Sentinel-2 data as representative modalities. Our approach produces a latent space at native 10 m resolution and the temporal frequency of cloud-free Sentinel-2 acquisitions. Each sensor is first modeled independently to capture its sensor-specific characteristics. Their representations are then combined into a shared model. This two-stage design enables modality-specific optimisation and easy extension to new sensors, retaining pretrained encoders while retraining only fusion layers. This enables the model to capture complementary remote sensing data and to preserve coherence across space and time. Qualitative analyses reveal that the learned embeddings exhibit high spatial and semantic consistency across heterogeneous landscapes. Quantitative evaluation in modelling Gross Primary Production reveals that they encode ecologically meaningful patterns and retain sufficient temporal fidelity to support fine-scale analyses. Overall, the proposed framework provides a flexible, analysis-ready representation learning approach for environmental applications requiring diverse spatial and temporal resolutions.

</details>


### [503] [FSC-Net: Fast-Slow Consolidation Networks for Continual Learning](https://arxiv.org/abs/2511.11707)
*Mohamed El Gorrim*

Main category: cs.LG

TL;DR: Dual-timescale FSC-Net uses a fast learner and a slow consolidator. Pure replay during consolidation outperforms distillation, suggesting method matters more than architectural complexity. Shows notable gains on Split-MNIST and CIFAR-10, but CIFAR-10 absolute performance remains modest without stronger backbones.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting in continual learning by separating rapid task adaptation from gradual knowledge consolidation, inspired by neuroscience memory consolidation.

Method: Two-network architecture: NN1 (fast) for immediate adaptation and NN2 (slow) for consolidation via replay (and optionally distillation). Evaluates multiple MLP-based NN1 variants; systematic hyperparameter analysis; finds pure replay during consolidation yields best performance; experiments on Split-MNIST (30 seeds) and Split-CIFAR-10 (5 seeds).

Result: Split-MNIST retention 91.71% ±0.62%, +4.27pp over fast network alone (87.43% ±1.27%). Split-CIFAR-10 retention 33.31% ±0.38%, +8.20pp over fast alone (25.11% ±1.61%). Paired t-values: t=23.585, p<1e-10; t=9.75, p<1e-3 respectively.

Conclusion: The central insight is that the dual-timescale consolidation mechanism, not architectural complexity, drives mitigation of catastrophic forgetting in this setting. However, CIFAR-10 performance is still modest, underscoring the need for stronger backbones.

Abstract: Continual learning remains challenging due to catastrophic forgetting, where neural networks lose previously acquired knowledge when learning new tasks. Inspired by memory consolidation in neuroscience, we propose FSC-Net (Fast-Slow Consolidation Networks), a dual-network architecture that separates rapid task learning from gradual knowledge consolidation. Our method employs a fast network (NN1) for immediate adaptation to new tasks and a slow network (NN2) that consolidates knowledge through distillation and replay. Within the family of MLP-based NN1 variants we evaluated, consolidation effectiveness is driven more by methodology than architectural embellishments -- a simple MLP outperforms more complex similarity-gated variants by 1.2pp. Through systematic hyperparameter analysis, we observed empirically that pure replay without distillation during consolidation achieves superior performance, consistent with the hypothesis that distillation from the fast network introduces recency bias. On Split-MNIST (30 seeds), FSC-Net achieves 91.71% +/- 0.62% retention accuracy, a +4.27pp gain over the fast network alone (87.43% +/- 1.27%, paired t=23.585, p < 1e-10). On Split-CIFAR-10 (5 seeds), our method achieves 33.31% +/- 0.38% retention with an +8.20pp gain over the fast network alone (25.11% +/- 1.61%, paired t=9.75, p < 1e-3), demonstrating +8.20pp gain, though absolute performance (33.31%) remains modest and below random expectation, highlighting need for stronger backbones. Our results provide empirical evidence that the dual-timescale consolidation mechanism, rather than architectural complexity, is central to mitigating catastrophic forgetting in this setting.

</details>


### [504] [Which Sparse Autoencoder Features Are Real? Model-X Knockoffs for False Discovery Rate Control](https://arxiv.org/abs/2511.11711)
*Tsogt-Ochir Enkhbayar*

Main category: cs.LG

TL;DR: Model-X knockoffs applied to sparse autoencoder feature selection to control FDR; selects 129 features at q=0.1 from 512 high-activity latents for sentiment classification; about 25% latents carry signal; 5.4x separation in knockoff stats; provides reproducible, principled framework for mechanistic interpretability.


<details>
  <summary>Details</summary>
Motivation: Need to distinguish real computational patterns from spurious correlations in SAEs and provide finite-sample FDR control for feature discovery to improve interpretability.

Method: Combine sparse autoencoders with Model-X knockoffs; employ knockoff+ to control FDR; assume Model-X with a Gaussian surrogate for latent distribution; select features with FDR target 0.1; analyze 512 high-activity latents from Pythia-70M sentiment classifier.

Result: Selected 129 features; ~25% of latents carry task-relevant signal; 75% do not; selected features show 5.40x larger knockoff statistics than non-selected features.

Conclusion: Offers a reproducible, principled framework for reliable feature discovery by fusing SAEs with multiple-testing-aware inference, strengthening mechanistic interpretability.

Abstract: Although sparse autoencoders (SAEs) are crucial for identifying interpretable features in neural networks, it is still challenging to distinguish between real computational patterns and erroneous correlations. We introduce Model-X knockoffs to SAE feature selection, using knock-off+ to control the false discovery rate (FDR) with finite-sample guarantees under the standard Model-X assumptions (in our case, via a Gaussian surrogate for the latent distribution). We select 129 features at a target FDR q=0.1 after analyzing 512 high-activity SAE latents for sentiment classification using Pythia-70M. About 25% of the latents under examination carry task-relevant signal, whereas 75% do not, according to the chosen set, which displays a 5.40x separation in knockoff statistics compared to non-selected features. Our method offers a re-producible and principled framework for reliable feature discovery by combining SAEs with multiple-testing-aware inference, advancing the foundations of mechanistic interpretability.

</details>


### [505] [Reasoning: From Reflection to Solution](https://arxiv.org/abs/2511.11712)
*Zixi Li*

Main category: cs.LG

TL;DR: Reasoning defined as iterative operator application in state spaces converging to fixed points; introduces architecture OpenXOR/OpenOperator/OpenLM; claims superior performance over LLMs (76% vs 0%) illustrating genuine reasoning beyond pattern matching.


<details>
  <summary>Details</summary>
Motivation: Clarify what reasoning is, distinguish genuine reasoning from pattern matching in large language models, and derive architectural requirements to enable iterative, convergent reasoning.

Method: Proposes a formal framework (OpenOperator) and a layered OpenXOR/OpenLM pipeline; uses puzzle OpenXOR as theory testbed; builds a working system (OpenLM) that performs iterative state-space.refinement toward fixed points.

Result: OpenLM achieves 76% accuracy on the task; traditional SOTA LLMs fail (0%); the approach provides practical demonstration of the architectural requirements for genuine reasoning.

Conclusion: Reasoning as iterative fixed-point operator application explains failures of current models and guides the design of architectures capable of genuine reasoning; aims to advance beyond pattern matching in LLMs.

Abstract: What is reasoning? This question has driven centuries of philosophical inquiry, from Aristotle's syllogisms to modern computational complexity theory. In the age of large language models achieving superhuman performance on benchmarks like GSM8K (95\% accuracy) and HumanEval (90\% pass@1), we must ask: have these systems learned to \emph{reason}, or have they learned to \emph{pattern-match over reasoning traces}?
  This paper argues for a specific answer: \textbf{reasoning is iterative operator application in state spaces, converging to fixed points}. This definition is not merely philosophical -- it has concrete architectural implications that explain both the failures of current systems and the path to genuine reasoning capabilities.
  Our investigation begins with a puzzle (OpenXOR), progresses through theory (OpenOperator), and culminates in a working solution (OpenLM) that achieves 76\% accuracy where state-of-the-art LLMs achieve 0\%. This is not about criticizing existing systems, but about \emph{understanding what reasoning requires} and \emph{building architectures that provide it}.

</details>


### [506] [Federated Learning for Pediatric Pneumonia Detection: Enabling Collaborative Diagnosis Without Sharing Patient Data](https://arxiv.org/abs/2511.11714)
*Daniel M. Jimenez-Gutierrez,Enrique Zuazua,Joaquin Del Rio,Oleksii Sliusarenko,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: Federated learning across multiple hospitals using Sherpa.ai FL significantly improves pediatric pneumonia detection from chest X-rays while preserving privacy, achieving high accuracy and ROC-AUC with substantial gains over single-site models.


<details>
  <summary>Details</summary>
Motivation: Privacy regulations and data heterogeneity hinder centralized training; FL enables secure multi-institution collaboration without transferring patient data, beneficial for rare diseases.

Method: Simulated cross-hospital FL on Pediatric Pneumonia Chest X-ray dataset using Sherpa.ai FL; non-IID data across hospitals; data remains on-site; train a CXR pneumonia classifier; evaluate performance.

Result: Achieved 0.900 accuracy and 0.966 ROC-AUC; improvements over single-hospital models of 47.5% (accuracy: 0.610 → 0.900) and 50.0% (ROC-AUC: 0.644 → 0.966); no patient CXR transfer; high-performing, generalizable, private.

Conclusion: Federated learning enables secure, high-performance pneumonia detection across healthcare networks; supports collaboration in low-data domains and rare diseases; preserves privacy while accelerating diagnosis and treatment.

Abstract: Early and accurate pneumonia detection from chest X-rays (CXRs) is clinically critical to expedite treatment and isolation, reduce complications, and curb unnecessary antibiotic use. Although artificial intelligence (AI) substantially improves CXR-based detection, development is hindered by globally distributed data, high inter-hospital variability, and strict privacy regulations (e.g., HIPAA, GDPR) that make centralization impractical. These constraints are compounded by heterogeneous imaging protocols, uneven data availability, and the costs of transferring large medical images across geographically dispersed sites.
  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL platform, enabling multiple hospitals (nodes) to collaboratively train a CXR classifier for pneumonia while keeping data in place and private. Using the Pediatric Pneumonia Chest X-ray dataset, we simulate cross-hospital collaboration with non-independent and non-identically distributed (non-IID) data, reproducing real-world variability across institutions and jurisdictions. Our experiments demonstrate that collaborative and privacy-preserving training across multiple hospitals via FL led to a dramatic performance improvement achieving 0.900 Accuracy and 0.966 ROC-AUC, corresponding to 47.5% and 50.0% gains over single-hospital models (0.610; 0.644), without transferring any patient CXR. These results indicate that FL delivers high-performing, generalizable, secure and private pneumonia detection across healthcare networks, with data kept local. This is especially relevant for rare diseases, where FL enables secure multi-institutional collaboration without data movement, representing a breakthrough for accelerating diagnosis and treatment development in low-data domains.

</details>


### [507] [Multiscale Grassmann Manifolds for Single-Cell Data Analysis](https://arxiv.org/abs/2511.11717)
*Xiang Xiang Wang,Sean Cottrell,Guo-Wei Wei*

Main category: cs.LG

TL;DR: Multiscale Grassmann-manifold framework for single-cell data that embeds cells at multiple scales into a unified Grassmann representation, using a power-based scale sampler; shows stable clustering and structure preservation on nine scRNA-seq datasets, especially for small/medium sizes.


<details>
  <summary>Details</summary>
Motivation: Traditional Euclidean representations miss intrinsic correlations and multiscale geometric structures in single-cell data; a subspace-based, multiscale approach could better capture cellular heterogeneity and relationships.

Method: Generate embeddings at multiple representation scales, map them into Grassmann manifolds, and fuse through a unified Grassmann representation. Introduce a power-based scale sampling function to select and balance information across resolutions. Evaluate on nine benchmark scRNA-seq datasets with clustering analyses.

Result: The approach preserves meaningful data structures and achieves stable clustering performance, notably for small to medium-sized datasets.

Conclusion: Grassmann-manifold-based multiscale analysis offers a coherent, informative foundation for single-cell data analysis and improves clustering stability by leveraging cross-scale subspace geometry.

Abstract: Single-cell data analysis seeks to characterize cellular heterogeneity based on high-dimensional gene expression profiles. Conventional approaches represent each cell as a vector in Euclidean space, which limits their ability to capture intrinsic correlations and multiscale geometric structures. We propose a multiscale framework based on Grassmann manifolds that integrates machine learning with subspace geometry for single-cell data analysis. By generating embeddings under multiple representation scales, the framework combines their features from different geometric views into a unified Grassmann manifold. A power-based scale sampling function is introduced to control the selection of scales and balance in- formation across resolutions. Experiments on nine benchmark single-cell RNA-seq datasets demonstrate that the proposed approach effectively preserves meaningful structures and provides stable clustering performance, particularly for small to medium-sized datasets. These results suggest that Grassmann manifolds offer a coherent and informative foundation for analyzing single cell data.

</details>


### [508] [Fast 3D Surrogate Modeling for Data Center Thermal Management](https://arxiv.org/abs/2511.11722)
*Soumyendu Sarkar,Antonio Guillen-Perez,Zachariah J Carmichael,Avisek Naug,Refik Mert Cam,Vineet Gundecha,Ashwin Ramesh Babu,Sahand Ghorbanpour,Ricardo Luna Gutierrez*

Main category: cs.LG

TL;DR: A vision-based 3D surrogate model can predict data center temperature distributions in real time by mapping voxelized inputs to heat maps, using 3D CNN U-Nets, 3D Fourier Neural Operators, and 3D Vision Transformers, achieving massive speedups and energy savings over CFD.


<details>
  <summary>Details</summary>
Motivation: To enable real-time cooling decisions in data centers by accurately predicting 3D temperature fields, addressing CFD's computational bottlenecks and the need to account for airflow and equipment operating conditions.

Method: Develop a voxel-based 3D representation of the data center that includes server workloads, fan speeds, and HVAC setpoints. Evaluate multiple architectures (3D CNN U-Nets, 3D Fourier Neural Operators, and 3D Vision Transformers) to map inputs to high-fidelity temperature heat maps.

Result: Surrogate models generalize across data-center configurations and achieve up to 20,000x speedup (predictions in hundreds of milliseconds vs hours). Real-time estimation of hot spots enables cooling control and workload redistribution, yielding about 7% energy savings and reduced carbon footprint.

Conclusion: Vision-based 3D surrogate modeling is a viable path for real-time, accurate temperature field prediction in data centers, enabling proactive cooling management and energy-efficient operation across varying configurations.

Abstract: Reducing energy consumption and carbon emissions in data centers by enabling real-time temperature prediction is critical for sustainability and operational efficiency. Achieving this requires accurate modeling of the 3D temperature field to capture airflow dynamics and thermal interactions under varying operating conditions. Traditional thermal CFD solvers, while accurate, are computationally expensive and require expert-crafted meshes and boundary conditions, making them impractical for real-time use. To address these limitations, we develop a vision-based surrogate modeling framework that operates directly on a 3D voxelized representation of the data center, incorporating server workloads, fan speeds, and HVAC temperature set points. We evaluate multiple architectures, including 3D CNN U-Net variants, a 3D Fourier Neural Operator, and 3D vision transformers, to map these thermal inputs to high-fidelity heat maps. Our results show that the surrogate models generalize across data center configurations and achieve up to 20,000x speedup (hundreds of milliseconds vs. hours). This fast and accurate estimation of hot spots and temperature distribution enables real-time cooling control and workload redistribution, leading to substantial energy savings (7\%) and reduced carbon footprint.

</details>


### [509] [Optimizing Input of Denoising Score Matching is Biased Towards Higher Score Norm](https://arxiv.org/abs/2511.11727)
*Tongda Xu*

Main category: cs.LG

TL;DR: Optimizing conditional inputs with denoising score matching breaks its equivalence to exact score matching, raises score norms, and exhibits a bias that also appears when optimizing data distributions with pretrained diffusion models; this bias spans multiple domains such as MAR, PerCo, and DreamFusion.


<details>
  <summary>Details</summary>
Motivation: To examine whether optimization of conditional inputs via denoising score matching preserves the theoretical link to exact score matching and to identify potential biases that can affect diffusion-model-based methods across applications.

Method: The paper presents a theoretical and empirical analysis in a workshop setting: (i) shows that optimizing conditional inputs with denoising score matching breaks equivalence to exact score matching; (ii) demonstrates that this bias increases the score norm; (iii) observes a similar bias when optimizing the data distribution with a pretrained diffusion model; (iv) discusses the cross-domain relevance of this bias across MAR for auto-regressive generation, PerCo for image compression, and DreamFusion for text-to-3D generation.

Result: Demonstrates that optimization using denoising score matching for conditional inputs breaks the equivalence with exact score matching and induces a higher score norm; observes a similar bias when optimizing the data distribution with a pretrained diffusion model; highlights that this bias affects a broad set of diffusion-model-based tasks across multiple domains.

Conclusion: The identified bias is widespread and relevant for diffusion-model workflows across domains; researchers and practitioners should account for potential deviations from ideal score-matching behavior when optimizing conditional inputs or data distributions, and consider mechanisms to mitigate or correct for the observed bias.

Abstract: Many recent works utilize denoising score matching to optimize the conditional input of diffusion models. In this workshop paper, we demonstrate that such optimization breaks the equivalence between denoising score matching and exact score matching. Furthermore, we show that this bias leads to higher score norm. Additionally, we observe a similar bias when optimizing the data distribution using a pre-trained diffusion model. Finally, we discuss the wide range of works across different domains that are affected by this bias, including MAR for auto-regressive generation, PerCo for image compression, and DreamFusion for text to 3D generation.

</details>


### [510] [Physics-Informed Neural ODEs with Scale-Aware Residuals for Learning Stiff Biophysical Dynamics](https://arxiv.org/abs/2511.11734)
*Kamalpreet Singh Kainth,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedat Panat*

Main category: cs.LG

TL;DR: PI-NODE-SR introduces scale-aware residuals in Physics-Informed Neural ODEs to stabilize learning of stiff biophysical dynamics, achieving accurate long-horizon predictions for Hodgkin-Huxley models without implicit solvers, and recovering fine morphological features, though initialization sensitivity persists.


<details>
  <summary>Details</summary>
Motivation: Standard Neural ODEs and PINNs struggle with stiff biophysical systems, requiring many iterations and often converging to inaccurate frequencies/amplitudes; a method is needed that is stable, efficient, and can extrapolate beyond short trajectories.

Method: Combine a low-order explicit Heun solver with residual normalization (scale-aware) to balance fast/slow variable contributions; end-to-end learning of the vector field; train on a single oscillation simulated with a stiff solver (Rodas5P); apply to Hodgkin-Huxley equations; extrapolate to >100 ms.

Result: Consistent reduction of long-horizon errors vs baseline Neural-ODEs and PINNs; captures oscillation frequency and near-correct amplitude; recovers subthreshold curvature in gating variables; avoids diffusion via neural correction; avoids expensive implicit solvers but relies on initialization; performance under realistic iteration budgets.

Conclusion: PI-NODE-SR offers a principled, efficient approach for stable learning of stiff biological dynamics, enabling accurate long-horizon predictions and qualitative feature recovery, though initialization sensitivity remains an area for improvement.

Abstract: Neural differential equations offer a powerful framework for modeling continuous-time dynamics, but forecasting stiff biophysical systems remains unreliable. Standard Neural ODEs and physics informed variants often require orders of magnitude more iterations, and even then may converge to suboptimal solutions that fail to preserve oscillatory frequency or amplitude. We introduce PhysicsInformed Neural ODEs with with Scale-Aware Residuals (PI-NODE-SR), a framework that combines a low-order explicit solver (Heun method) residual normalisation to balance contributions between state variables evolving on disparate timescales. This combination stabilises training under realistic iteration budgets and avoids reliance on computationally expensive implicit solvers. On the Hodgkin-Huxley equations, PI-NODE-SR learns from a single oscillation simulated with a stiff solver (Rodas5P) and extrapolates beyond 100 ms, capturing both oscillation frequency and near-correct amplitudes. Remarkably, end-to-end learning of the vector field enables PI-NODE-SR to recover morphological features such as sharp subthreshold curvature in gating variables that are typically reserved for higher-order solvers, suggesting that neural correction can offset numerical diffusion. While performance remains sensitive to initialisation, PI-NODE-SR consistently reduces long-horizon errors relative to baseline Neural-ODEs and PINNs, offering a principled route to stable and efficient learning of stiff biological dynamics.

</details>


### [511] [KAN/H: Kolmogorov-Arnold Network using Haar-like bases](https://arxiv.org/abs/2511.11736)
*Susumu Katayama*

Main category: cs.LG

TL;DR: KAN/H is a Haar-variant basis variant of Kolmogorov-Arnold Network (KAN) that replaces B-spline with a Haar-like global/local basis, applied to function approximation and MNIST, and claims to reduce reliance on problem-specific hyperparameters.


<details>
  <summary>Details</summary>
Motivation: To simplify and generalize KAN by using a Haar-variant basis with both global and local components, aiming to reduce the need for extensive hyperparameter tuning while maintaining or improving performance on standard tasks.

Method: Introduce KAN/H architecture featuring Haar-variant global and local bases instead of B-spline bases. Apply the model to function approximation tasks and to MNIST digit classification, and compare sensitivity to hyperparameters against the original KAN.

Result: The abstract claims that KAN/H does not require most of the problem-specific hyper-parameter tunings, suggesting increased robustness and simpler setup compared to prior approaches.

Conclusion: KAN/H offers a potentially simpler, more robust variant of KAN with a Haar-variant basis that works for function approximation and MNIST, reducing the need for extensive hyperparameter tuning.

Abstract: This paper proposes KAN/H, a variant of Kolmogorov-Arnold Network (KAN) that uses a Haar-variant basis system having both global and local bases instead of B-spline. The resulting algorithm is applied to function approximation problems and MNIST. We show that it does not require most of the problem-specific hyper-parameter tunings.

</details>


### [512] [DK-Root: A Joint Data-and-Knowledge-Driven Framework for Root Cause Analysis of QoE Degradations in Mobile Networks](https://arxiv.org/abs/2511.11737)
*Qizhe Li,Haolong Chen,Jiansheng Li,Shuqi Chai,Xuan Li,Yuzhou Hou,Xinhua Shao,Fangfang Li,Kaifeng Han,Guangxu Zhu*

Main category: cs.LG

TL;DR: DK-Root is a data- and knowledge-driven framework for root-cause analysis of QoE degradations in mobile networks, combining weak supervision, diffusion-based data augmentation, and scarce expert labels to achieve robust, high-accuracy classification.


<details>
  <summary>Details</summary>
Motivation: Diagnosing QoE degradations is hard due to cross-layer KPI interactions and noisy, coarse rule-based labels, with limited expert annotations. A scalable yet precise approach is needed.

Method: DK-Root pretrains an encoder with contrastive learning using rule-based labels while denoising noise via a supervised contrastive objective. It introduces a class-conditional diffusion model to generate KPI sequences preserving root-cause semantics, producing weak and strong augmentations by controlling reverse diffusion steps. The encoder and a lightweight classifier are then jointly fine-tuned with scarce expert-verified labels.

Result: On a real-world operator-grade dataset, DK-Root achieves state-of-the-art accuracy, outperforming traditional ML and recent semi-supervised time-series methods. Ablations show the necessity of conditional diffusion augmentation and the pretrain–finetune design for improved representation quality and classification gains.

Conclusion: DK-Root demonstrates that unifying scalable weak supervision with precise expert guidance via contrastive pretraining, diffusion-based augmentation, and joint fine-tuning can yield robust root-cause analysis in QoE, with strong empirical gains and potential for broader application.

Abstract: Diagnosing the root causes of Quality of Experience (QoE) degradations in operational mobile networks is challenging due to complex cross-layer interactions among kernel performance indicators (KPIs) and the scarcity of reliable expert annotations. Although rule-based heuristics can generate labels at scale, they are noisy and coarse-grained, limiting the accuracy of purely data-driven approaches. To address this, we propose DK-Root, a joint data-and-knowledge-driven framework that unifies scalable weak supervision with precise expert guidance for robust root-cause analysis. DK-Root first pretrains an encoder via contrastive representation learning using abundant rule-based labels while explicitly denoising their noise through a supervised contrastive objective. To supply task-faithful data augmentation, we introduce a class-conditional diffusion model that generates KPIs sequences preserving root-cause semantics, and by controlling reverse diffusion steps, it produces weak and strong augmentations that improve intra-class compactness and inter-class separability. Finally, the encoder and the lightweight classifier are jointly fine-tuned with scarce expert-verified labels to sharpen decision boundaries. Extensive experiments on a real-world, operator-grade dataset demonstrate state-of-the-art accuracy, with DK-Root surpassing traditional ML and recent semi-supervised time-series methods. Ablations confirm the necessity of the conditional diffusion augmentation and the pretrain-finetune design, validating both representation quality and classification gains.

</details>


### [513] [Uncertainty Makes It Stable: Curiosity-Driven Quantized Mixture-of-Experts](https://arxiv.org/abs/2511.11743)
*Sebastián Andrés Cajas Ordóñez,Luis Fernando Torres Torres,Mackenzie J. Meni,Carlos Andrés Duran Paredes,Eric Arazo,Cristian Bosch,Ricardo Simon Carbajo,Yuan Lai,Leo Anthony Celi*

Main category: cs.LG

TL;DR: Curiosity-driven, Bayesian routing across heterogeneous quantized experts enables near-full-precision audio classification with 4-bit quantization, substantial compression, and highly predictable latency on edge devices; in many cases, simple 4-bit models outperform complex MoE.


<details>
  <summary>Details</summary>
Motivation: Resource-constrained devices demand maintaining accuracy under aggressive quantization while ensuring predictable, low-latency inference; current MoE and fixed-quantization approaches often struggle with latency variance and energy efficiency.

Method: A curiosity-driven quantized Mixture-of-Experts (BitNet ternary; 1-16 bit BitLinear; post-training quantization) with Bayesian epistemic uncertainty-based routing across heterogeneous experts, evaluated on ESC-50, Quinn, UrbanSound8K; comparisons include 4-bit and 8-bit baselines; statistical analyses (Levene's test) assess latency variance; information-theoretic routing; metrics include F1, compression, energy, latency; scale analysis of training vs deployment emissions.

Result: 4-bit quantization preserves 0.858 F1 vs 0.859 with full precision; 4x compression; 41% energy savings vs 8-bit; latency variance reduced from 230 ms to 29 ms (82% reduction, p=0.008, Levene); 4-bit/8-bit practically equivalent to full precision (p>0.05); MoE incurs 11% latency overhead (p<0.001) without accuracy gains; at scale, deployment emissions dominate training by 10000x for models serving >1,000 inferences; 1.2M parameters; 3.87 F1/mJ; simple 4-bit quantized architectures outperform complex MoE for most deployments.

Conclusion: Adaptive quantization with information-theoretic routing yields accurate, energy-efficient, and predictable edge models; 4-bit architectures suffice for most deployments, making complex MoE unnecessary for many edge scenarios; emphasis on inference efficiency is crucial when serving large numbers of inferences.

Abstract: Deploying deep neural networks on resource-constrained devices faces two critical challenges: maintaining accuracy under aggressive quantization while ensuring predictable inference latency. We present a curiosity-driven quantized Mixture-of-Experts framework that addresses both through Bayesian epistemic uncertainty-based routing across heterogeneous experts (BitNet ternary, 1-16 bit BitLinear, post-training quantization). Evaluated on audio classification benchmarks (ESC-50, Quinn, UrbanSound8K), our 4-bit quantization maintains 99.9 percent of 16-bit accuracy (0.858 vs 0.859 F1) with 4x compression and 41 percent energy savings versus 8-bit. Crucially, curiosity-driven routing reduces MoE latency variance by 82 percent (p = 0.008, Levene's test) from 230 ms to 29 ms standard deviation, enabling stable inference for battery-constrained devices. Statistical analysis confirms 4-bit/8-bit achieve practical equivalence with full precision (p > 0.05), while MoE architectures introduce 11 percent latency overhead (p < 0.001) without accuracy gains. At scale, deployment emissions dominate training by 10000x for models serving more than 1,000 inferences, making inference efficiency critical. Our information-theoretic routing demonstrates that adaptive quantization yields accurate (0.858 F1, 1.2M params), energy-efficient (3.87 F1/mJ), and predictable edge models, with simple 4-bit quantized architectures outperforming complex MoE for most deployments.

</details>


### [514] [Diffusion Models: A Mathematical Introduction](https://arxiv.org/abs/2511.11746)
*Sepehr Maleki,Negar Pourmoazemi*

Main category: cs.LG

TL;DR: A self-contained, first-principles derivation of diffusion-based generative models from Gaussian properties, unifying forward/backward processes, variational bounds, and continuous-time and flow-based formulations, with practical guidance on sampling, likelihoods, and conditioning (e.g., Stable Diffusion) across discrete and continuous frameworks.


<details>
  <summary>Details</summary>
Motivation: Fill the gap between theory and practice by deriving diffusion models from basic Gaussian facts, providing explicit algebra, intermediate steps, and a transparent notation that enables both understanding and implementation.

Method: Derive the forward noising process from Gaussian densities; compute exact marginals and the discrete reverse posterior; obtain the variational bound, which reduces to the standard noise-prediction objective; discuss likelihood estimation and accelerated sampling (DDIM, DDGAN, multi-scale/latent diffusion); present a continuous-time formulation via SDEs, derive the probability-flow ODE from the FP equation, introduce flow matching, and show rectified flows recover DDIM up to reparameterization; analyze guided diffusion as posterior score correction and classifier-free guidance as interpolation between conditional/unconditional scores.

Result: Establishes a unified, transparent framework linking discrete and continuous diffusion methods; shows the variational bound collapses to the practical noise-prediction objective; details a suite of sampling and modeling variants (DDIM, DDGAN, nested/latent diffusion, Stable Diffusion); provides a coherent picture of guided and conditional diffusion and flow-based interpretations.

Conclusion: Offers a principled, implementable foundation for diffusion-based generation, clarifying the relationships among diverse methods and providing explicit steps and notation to enable practitioners to derive and implement the algorithms, with Stable Diffusion highlighted as a canonical example.

Abstract: We present a concise, self-contained derivation of diffusion-based generative models. Starting from basic properties of Gaussian distributions (densities, quadratic expectations, re-parameterisation, products, and KL divergences), we construct denoising diffusion probabilistic models from first principles. This includes the forward noising process, its closed-form marginals, the exact discrete reverse posterior, and the related variational bound. This bound simplifies to the standard noise-prediction goal used in practice. We then discuss likelihood estimation and accelerated sampling, covering DDIM, adversarially learned reverse dynamics (DDGAN), and multi-scale variants such as nested and latent diffusion, with Stable Diffusion as a canonical example. A continuous-time formulation follows, in which we derive the probability-flow ODE from the diffusion SDE via the continuity and Fokker-Planck equations, introduce flow matching, and show how rectified flows recover DDIM up to a time re-parameterisation. Finally, we treat guided diffusion, interpreting classifier guidance as a posterior score correction and classifier-free guidance as a principled interpolation between conditional and unconditional scores. Throughout, the focus is on transparent algebra, explicit intermediate steps, and consistent notation, so that readers can both follow the theory and implement the corresponding algorithms in practice.

</details>


### [515] [IDOL: Meeting Diverse Distribution Shifts with Prior Physics for Tropical Cyclone Multi-Task Estimation](https://arxiv.org/abs/2511.11750)
*Hanting Yan,Pan Mu,Shiqi Zhang,Yuchao Zhu,Jinglin Zhang,Cong Bai*

Main category: cs.LG

TL;DR: IDOL is a physics-informed, identity-distribution-oriented learning framework for TC estimation that uses wind-field physics and dark correlation knowledge to create shared and task-specific identity tokens, enforcing invariances to improve robustness under distribution shifts across wind speed, pressure, and core size estimation.


<details>
  <summary>Details</summary>
Motivation: Distribution shifts due to varying geography and seasonal changes make real-time tropical cyclone estimation unreliable. Existing methods relying on multi-modal fusion neglect intrinsic distributional properties of features, harming generalization.

Method: Introduce identity tokens representing task dependencies and physical invariances; use a wind-field model and dark correlation knowledge to define task-shared and task-specific tokens; impose identity-oriented constraints in the feature space guided by prior physical knowledge to achieve invariant representations across shifts.

Result: Empirical evaluations on multiple datasets and tasks show the proposed IDOL outperforms baselines, demonstrating robustness to distribution shifts; code is publicly available.

Conclusion: Imposing identity-oriented constraints grounded in prior physical knowledge effectively mitigates distribution shifts in tropical cyclone estimation, offering a generalizable approach with potential applicability to other physics-guided, out-of-distribution problems.

Abstract: Tropical Cyclone (TC) estimation aims to accurately estimate various TC attributes in real time. However, distribution shifts arising from the complex and dynamic nature of TC environmental fields, such as varying geographical conditions and seasonal changes, present significant challenges to reliable estimation. Most existing methods rely on multi-modal fusion for feature extraction but overlook the intrinsic distribution of feature representations, leading to poor generalization under out-of-distribution (OOD) scenarios. To address this, we propose an effective Identity Distribution-Oriented Physical Invariant Learning framework (IDOL), which imposes identity-oriented constraints to regulate the feature space under the guidance of prior physical knowledge, thereby dealing distribution variability with physical invariance. Specifically, the proposed IDOL employs the wind field model and dark correlation knowledge of TC to model task-shared and task-specific identity tokens. These tokens capture task dependencies and intrinsic physical invariances of TC, enabling robust estimation of TC wind speed, pressure, inner-core, and outer-core size under distribution shifts. Extensive experiments conducted on multiple datasets and tasks demonstrate the outperformance of the proposed IDOL, verifying that imposing identity-oriented constraints based on prior physical knowledge can effectively mitigates diverse distribution shifts in TC estimation.Code is available at https://github.com/Zjut-MultimediaPlus/IDOL.

</details>


### [516] [Improving a Hybrid Graphsage Deep Network for Automatic Multi-objective Logistics Management in Supply Chain](https://arxiv.org/abs/2511.11753)
*Mehdi Khaleghi,Nastaran Khaleghi,Sobhan Sheykhivand,Sebelan Danishvar*

Main category: cs.LG

TL;DR: Proposes a hybrid GraphSAGE network (H-GSN) for multi-task logistics management across three Kaggle datasets, achieving high accuracy in predicting shipment type, status, traffic, logistics ID, and delays, aiming to boost supply-chain resilience and sustainability.


<details>
  <summary>Details</summary>
Motivation: To enhance supply-chain resilience, reduce emissions, and improve collaboration with logistics providers by automating predictions of shipment-related attributes and delays.

Method: A hybrid GraphSAGE-based neural network (H-GSN) designed for multi-task learning across multiple datasets (DataCo, Shipping, Smart Logistics) to predict shipment type, shipment status, traffic status, logistics ID, and logistics delay.

Result: High predictive performance: Smart Logistics - 10 logistics IDs (97.8%) and 3 traffic statuses (100%); DataCo shipment type (98.7%); Shipping logistics delay (99.4%). Overall metrics suggest the approach enhances resilience and sustainability.

Conclusion: The H-GSN framework effectively supports resilient and sustainable supply-chain management by enabling accurate, automated predictions for multiple logistics-related tasks, potentially improving collaboration with logistics service providers.

Abstract: Systematic logistics, conveyance amenities and facilities as well as warehousing information play a key role in fostering profitable development in a supply chain. The aim of transformation in industries is the improvement of the resiliency regarding the supply chain. The resiliency policies are required for companies to affect the collaboration with logistics service providers positively. The decrement of air pollutant emissions is a persistent advantage of the efficient management of logistics and transportation in supply chain. The management of shipment type is a significant factor in analyzing the sustainability of logistics and supply chain. An automatic approach to predict the shipment type, logistics delay and traffic status are required to improve the efficiency of the supply chain management. A hybrid graphsage network (H-GSN) is proposed in this paper for multi-task purpose of logistics management in a supply chain. The shipment type, shipment status, traffic status, logistics ID and logistics delay are the objectives in this article regarding three different databases including DataCo, Shipping and Smart Logistcis available on Kaggle as supply chain logistics databases. The average accuracy of 97.8% and 100% are acquired for 10 kinds of logistics ID and 3 types of traffic status prediction in Smart Logistics dataset. The average accuracy of 98.7% and 99.4% are obtained for shipment type prediction in DataCo and logistics delay in Shipping database, respectively. The evaluation metrics for different logistics scenarios confirm the efficiency of the proposed method to improve the resilience and sustainability of the supply chain.

</details>


### [517] [Sumudu Neural Operator for ODEs and PDEs](https://arxiv.org/abs/2511.11762)
*Ben Zelenskiy,Saibilila Abudukelimu,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.LG

TL;DR: Sumudu Neural Operator (SNO) uses the Sumudu Transform to build a neural operator by decomposing inputs into transform coefficients and learning in Sumudu space. It outperforms FNO on PDEs and ties or surpasses LNO on several PDE tasks, achieving the lowest error on Euler-Bernoulli Beam and Diffusion equations, and enabling zero-shot super-resolution. Overall, preliminary results suggest transform-based neural operators can be effective for certain PDE classes.


<details>
  <summary>Details</summary>
Motivation: Develop a neural operator that leverages the mathematical structure of the Sumudu Transform to improve PDE approximation and generalization, expanding neural operator design beyond Fourier-based methods.

Method: Decompose input space into transform coefficients, map to Sumudu space, and parameterize a neural operator within that space. Evaluate on ODEs (Duffing Oscillator, Lorenz System, Driven Pendulum) and PDEs (Euler-Bernoulli Beam, Burgers' Equation, Diffusion, Diffusion-Reaction, Brusselator). Include zero-shot super-resolution experiments to assess data quality improvement from low-quality samples.

Result: SNO shows superior performance to Fourier Neural Operator (FNO) on PDE tasks and competitive accuracy with LNO on several PDE tasks, including achieving the lowest reported error for Euler-Bernoulli Beam and Diffusion equations. Zero-shot super-resolution demonstrates the model's capacity to enhance data quality from degraded inputs.

Conclusion: Sumudu Transform–based neural operators appear promising, particularly for specific PDE classes, warranting further exploration and benchmarking against existing operators across broader problem sets.

Abstract: We introduce the Sumudu Neural Operator (SNO), a neural operator rooted in the properties of the Sumudu Transform. We leverage the relationship between the polynomial expansions of transform pairs to decompose the input space as coefficients, which are then transformed into the Sumudu Space, where the neural operator is parameterized. We evaluate the operator in ODEs (Duffing Oscillator, Lorenz System, and Driven Pendulum) and PDEs (Euler-Bernoulli Beam, Burger's Equation, Diffusion, Diffusion-Reaction, and Brusselator). SNO achieves superior performance to FNO on PDEs and demonstrates competitive accuracy with LNO on several PDE tasks, including the lowest error on the Euler-Bernoulli Beam and Diffusion Equation. Additionally, we apply zero-shot super-resolution to the PDE tasks to observe the model's capability of obtaining higher quality data from low-quality samples. These preliminary findings suggest promise for the Sumudu Transform as a neural operator design, particularly for certain classes of PDEs.

</details>


### [518] [Learning Fair Representations with Kolmogorov-Arnold Networks](https://arxiv.org/abs/2511.11767)
*Amisha Priyadarshini,Sergio Gago-Masague*

Main category: cs.LG

TL;DR: Proposes Kolmogorov-Arnold Networks (KANs) within a fair adversarial learning framework to balance fairness and accuracy in college admissions; adds adaptive penalty updates; shows improved performance and interpretability across datasets and strategies.


<details>
  <summary>Details</summary>
Motivation: Address discriminatory behavior and the fairness-accuracy trade-off in ML, particularly in high-stakes domains like college admissions, while improving interpretability by using more transparent models than black-box neural nets.

Method: Integrate Kolmogorov-Arnold Networks (KANs) into a fair adversarial learning framework with an adaptive penalty update mechanism that dynamically adjusts fairness constraints during training; evaluate on two real-world college admissions datasets across three optimization strategies.

Result: KANs consistently outperform baseline fair learning models, maintaining high predictive accuracy and achieving competitive fairness across sensitive attributes; the approach is efficient and robust across datasets and optimization strategies.

Conclusion: KANs with adaptive penalty in a fair adversarial setting offer a favorable balance between fairness, accuracy, and interpretability, and demonstrate robustness across different datasets and training strategies.

Abstract: Despite recent advances in fairness-aware machine learning, predictive models often exhibit discriminatory behavior towards marginalized groups. Such unfairness might arise from biased training data, model design, or representational disparities across groups, posing significant challenges in high-stakes decision-making domains such as college admissions. While existing fair learning models aim to mitigate bias, achieving an optimal trade-off between fairness and accuracy remains a challenge. Moreover, the reliance on black-box models hinders interpretability, limiting their applicability in socially sensitive domains. In this paper, we try to circumvent these issues by integrating Kolmogorov-Arnold Networks (KANs) within a fair adversarial learning framework. Leveraging the adversarial robustness and interpretability of KANs, our approach enables a balance between fairness and accuracy. To further facilitate this balance, we propose an adaptive penalty update mechanism that dynamically adjusts fairness constraints during the model training. We conduct numerical experiments on two real-world college admissions datasets, across three different optimization strategies. The results demonstrate the efficiency and robustness of KANs by consistently outperforming the baseline fair learning models, and maintaining high predictive accuracy while achieving competitive fairness across sensitive attributes.

</details>


### [519] [CATCHFed: Efficient Unlabeled Data Utilization for Semi-Supervised Federated Learning in Limited Labels Environments](https://arxiv.org/abs/2511.11778)
*Byoungjun Park,Pedro Porto Buarque de Gusmão,Dongjin Ji,Minhoe Kim*

Main category: cs.LG

TL;DR: CATCHFed tackles label scarcity in semi-supervised federated learning by adaptive client-aware thresholds, hybrid pseudo-labeling, and consistency regularization on unlabeled client data, yielding strong performance when labels are extremely limited.


<details>
  <summary>Details</summary>
Motivation: In real-world FL, clients often lack labels while the server has some; standard semi-supervised FL degrades as labeled data dwindles, creating a need to better exploit unlabeled client data and improve pseudo-label quality.

Method: Propose CATCHFed with (1) client-aware adaptive thresholds that consider per-class difficulty, (2) hybrid thresholds to improve pseudo-label quality, and (3) utilization of unpseudo-labeled data for consistency regularization during training.

Result: Extensive experiments across diverse datasets/configurations show CATCHFed effectively leverages unlabeled client data and outperforms baselines, especially under extremely limited-label settings.

Conclusion: CATCHFed provides an effective, practical approach to federated semi-supervised learning under label scarcity by leveraging unlabeled client data and robust pseudo-labeling strategies.

Abstract: Federated learning is a promising paradigm that utilizes distributed client resources while preserving data privacy. Most existing FL approaches assume clients possess labeled data, however, in real-world scenarios, client-side labels are often unavailable. Semi-supervised Federated learning, where only the server holds labeled data, addresses this issue. However, it experiences significant performance degradation as the number of labeled data decreases. To tackle this problem, we propose \textit{CATCHFed}, which introduces client-aware adaptive thresholds considering class difficulty, hybrid thresholds to enhance pseudo-label quality, and utilizes unpseudo-labeled data for consistency regularization. Extensive experiments across various datasets and configurations demonstrate that CATCHFed effectively leverages unlabeled client data, achieving superior performance even in extremely limited-label settings.

</details>


### [520] [Coordinate Descent for Network Linearization](https://arxiv.org/abs/2511.11781)
*Vlad Rakhlin,Amir Jevnisek,Shai Avidan*

Main category: cs.LG

TL;DR: Discrete-coordinate-descent optimization for ReLU pruning in private inference on ResNet yields sparse networks and state-of-the-art results, avoiding the accuracy loss of smoothing-based hard-thresholding.


<details>
  <summary>Details</summary>
Motivation: ReLU activations dominate latency in private inference with ResNet architectures. Reducing ReLU count is a discrete problem; existing smoothing-based methods jointly optimize accuracy and ReLU budget but suffer a large drop at the final hard thresholding step.

Method: A discrete optimization approach using Coordinate Descent directly in the discrete domain to produce sparse solutions by design, contrasting with smooth approximations that require a final thresholding step.

Result: Extensive experiments show state-of-the-art performance on common benchmarks.

Conclusion: Discrete, coordinate-descent optimization for ReLU budgeting in private inference is effective, yielding sparse networks and better accuracy-latency trade-offs than smoothing-based methods.

Abstract: ReLU activations are the main bottleneck in Private Inference that is based on ResNet networks. This is because they incur significant inference latency. Reducing ReLU count is a discrete optimization problem, and there are two common ways to approach it. Most current state-of-the-art methods are based on a smooth approximation that jointly optimizes network accuracy and ReLU budget at once. However, the last hard thresholding step of the optimization usually introduces a large performance loss. We take an alternative approach that works directly in the discrete domain by leveraging Coordinate Descent as our optimization framework. In contrast to previous methods, this yields a sparse solution by design. We demonstrate, through extensive experiments, that our method is State of the Art on common benchmarks.

</details>


### [521] [Simplicial covering dimension of extremal concept classes](https://arxiv.org/abs/2511.11819)
*Ari Blondal,Hamed Hatami,Pooya Hatami,Chavdar Lalov,Sivan Tretiak*

Main category: cs.LG

TL;DR: A simplicial covering dimension is introduced for binary concept classes, based on the loss and realizable distributions, and shown to exactly equal the list replicability (global stability) in PAC learning for finite classes, enabling exact computation for extremal concept classes.


<details>
  <summary>Details</summary>
Motivation: To bring classical dimension theory into learning theory by providing a topological/ geometric framework to quantify stability and replicability (list replicability) in PAC learning.

Method: Construct the space of realizable distributions for a concept class, induce a simplicial structure via the loss and the class, define a simplicial covering dimension, and prove its exact correspondence with list replicability for finite classes; apply the result to compute the replicability number for extremal concept classes.

Result: The simplicial covering dimension exactly characterizes the list replicability number in PAC learning for finite concept classes, allowing exact computation of this quantity for a broad family of extremal concept classes.

Conclusion: This work provides a bridge between dimension theory and learning theory, yielding a precise topological tool to analyze stability/replicability in PAC learning and enabling exact calculations for extremal classes.

Abstract: Dimension theory is a branch of topology concerned with defining and analyzing dimensions of geometric and topological spaces in purely topological terms. In this work, we adapt the classical notion of topological dimension (Lebesgue covering) to binary concept classes. The topological space naturally associated with a concept class is its space of realizable distributions. The loss function and the class itself induce a simplicial structure on this space, with respect to which we define a simplicial covering dimension.
  We prove that for finite concept classes, this simplicial covering dimension exactly characterizes the list replicability number (equivalently, global stability) in PAC learning. This connection allows us to apply tools from classical dimension theory to compute the exact list replicability number of the broad family of extremal concept classes.

</details>


### [522] [Conformal Constrained Policy Optimization for Cost-Effective LLM Agents](https://arxiv.org/abs/2511.11828)
*Wenwen Si,Sooyong Jang,Insup Lee,Osbert Bastani*

Main category: cs.LG

TL;DR: Proposes CCPO: a cost-aware, reliable LLM agent framework that sequences multiple models/tools using an orchestration model and conformal prediction, achieving significant cost reductions without sacrificing reliability.


<details>
  <summary>Details</summary>
Motivation: Reduce the high computational and API costs of large language models while maintaining a user-specified reliability level.

Method: Introduce Conformal Constrained Policy Optimization (CCPO), combining constrained policy optimization, off-policy RL, and online conformal prediction to learn a cost-aware score and adaptive threshold; implement an orchestration model that sequences models/tools with varying cost/accuracy tradeoffs.

Result: On two multi-hop QA benchmarks, CCPO achieves up to 30% cost reduction compared to cost-aware baselines and LLM-guided methods, without compromising reliability.

Conclusion: CCPO provides a principled, practical framework for deploying cost-effective, reliable LLM agents.

Abstract: While large language models (LLMs) have recently made tremendous progress towards solving challenging AI problems, they have done so at increasingly steep computational and API costs. We propose a novel strategy where we combine multiple LLM models with varying cost/accuracy tradeoffs in an agentic manner, where models and tools are run in sequence as determined by an orchestration model to minimize cost subject to a user-specified level of reliability; this constraint is formalized using conformal prediction to provide guarantees. To solve this problem, we propose Conformal Constrained Policy Optimization (CCPO), a training paradigm that integrates constrained policy optimization with off-policy reinforcement learning and recent advances in online conformal prediction. CCPO jointly optimizes a cost-aware policy (score function) and an adaptive threshold. Across two multi-hop question answering benchmarks, CCPO achieves up to a 30% cost reduction compared to other cost-aware baselines and LLM-guided methods without compromising reliability. Our approach provides a principled and practical framework for deploying LLM agents that are significantly more cost-effective while maintaining reliability.

</details>


### [523] [Volatility in Certainty (VC): A Metric for Detecting Adversarial Perturbations During Inference in Neural Network Classifiers](https://arxiv.org/abs/2511.11834)
*Vahid Hemmati,Ahmad Mohammadi,Abdul-Rauf Nuhu,Reza Ahmari,Parham Kebria,Abdollah Homaifar*

Main category: cs.LG

TL;DR: Volatility in Certainty (VC) is a label-free metric that correlates with accuracy and serves as an indicator of adversarial drift, enabling real-time performance monitoring across architectures without ground truth labels.


<details>
  <summary>Details</summary>
Motivation: Need for scalable, label-free, real-time metrics to detect performance degradation and adversarial drift in safety-critical systems where true labels are unavailable during inference.

Method: VC is defined as the average squared log-ratio of adjacent sorted softmax confidences. The authors evaluate VC on MNIST with ANN/CNN models and CIFAR-10 with a regularized VGG-like model. Adversarial examples are generated via FGSM at varying perturbation magnitudes. They also create mixed test sets by gradually injecting adversarial contamination to assess VC under incremental distribution shifts.

Result: Strong negative correlation between classification accuracy and log(VC) (rho < -0.90 in most cases), indicating VC effectively tracks performance degradation without labels. VC appears architecture-agnostic, scalable, and suitable for real-time early-warning systems.

Conclusion: VC can serve as a scalable, real-time proxy for accuracy and adversarial drift across different architectures, enabling label-free monitoring in safety-critical applications.

Abstract: Adversarial robustness remains a critical challenge in deploying neural network classifiers, particularly in real-time systems where ground-truth labels are unavailable during inference. This paper investigates \textit{Volatility in Certainty} (VC), a recently proposed, label-free metric that quantifies irregularities in model confidence by measuring the dispersion of sorted softmax outputs. Specifically, VC is defined as the average squared log-ratio of adjacent certainty values, capturing local fluctuations in model output smoothness. We evaluate VC as a proxy for classification accuracy and as an indicator of adversarial drift. Experiments are conducted on artificial neural networks (ANNs) and convolutional neural networks (CNNs) trained on MNIST, as well as a regularized VGG-like model trained on CIFAR-10. Adversarial examples are generated using the Fast Gradient Sign Method (FGSM) across varying perturbation magnitudes. In addition, mixed test sets are created by gradually introducing adversarial contamination to assess VC's sensitivity under incremental distribution shifts. Our results reveal a strong negative correlation between classification accuracy and log(VC) (correlation rho < -0.90 in most cases), suggesting that VC effectively reflects performance degradation without requiring labeled data. These findings position VC as a scalable, architecture-agnostic, and real-time performance metric suitable for early-warning systems in safety-critical applications.

</details>


### [524] [On the Trade-Off Between Transparency and Security in Adversarial Machine Learning](https://arxiv.org/abs/2511.11842)
*Lucas Fenaux,Christopher Srinivasa,Florian Kerschbaum*

Main category: cs.LG

TL;DR: In transferable adversarial attacks, attackers fare better when their surrogate aligns with the defender's defense choice; transparency can degrade security, and obscurity can help defense in some cases. Game-theoretic analysis (Nash and Stackelberg) reveals a broader trade-off between transparency and security, suggesting transparency in AI systems may conflict with security beyond adversarial ML.


<details>
  <summary>Details</summary>
Motivation: To understand how transparency affects security in Responsible AI, particularly under transferable adversarial attacks where attackers use surrogate models to fool a defender's model.

Method: Empirical evaluation of nine transferable attacks across 181 models; theoretical analysis using Nash and Stackelberg game frameworks to compare outcomes of defender choices (defended vs undefended).

Result: Attackers are more successful when their surrogate model matches the defender's defense decision; revealing only whether a defender's model is defended can still harm security; obscurity may thus benefit the defender in some scenarios.

Conclusion: There is an inherent trade-off between transparency and security in AI systems. Game-theoretic reasoning helps uncover conflicts beyond adversarial ML, indicating transparency can be at odds with security in broader AI contexts.

Abstract: Transparency and security are both central to Responsible AI, but they may conflict in adversarial settings. We investigate the strategic effect of transparency for agents through the lens of transferable adversarial example attacks. In transferable adversarial example attacks, attackers maliciously perturb their inputs using surrogate models to fool a defender's target model. These models can be defended or undefended, with both players having to decide which to use. Using a large-scale empirical evaluation of nine attacks across 181 models, we find that attackers are more successful when they match the defender's decision; hence, obscurity could be beneficial to the defender. With game theory, we analyze this trade-off between transparency and security by modeling this problem as both a Nash game and a Stackelberg game, and comparing the expected outcomes. Our analysis confirms that only knowing whether a defender's model is defended or not can sometimes be enough to damage its security. This result serves as an indicator of the general trade-off between transparency and security, suggesting that transparency in AI systems can be at odds with security. Beyond adversarial machine learning, our work illustrates how game-theoretic reasoning can uncover conflicts between transparency and security.

</details>


### [525] [Leveraging Exogenous Signals for Hydrology Time Series Forecasting](https://arxiv.org/abs/2511.11849)
*Junyang He,Judy Fox,Alireza Jafari,Ying-Jung Chen,Geoffrey Fox*

Main category: cs.LG

TL;DR: In rainfall–runoff modeling, leveraging domain knowledge (exogenous inputs and annual periodic signals) in time series models yields better performance than generic foundation models using CAMELS-US data.


<details>
  <summary>Details</summary>
Motivation: Foundation models for time series are not well validated in specific physical sciences; hydrology may benefit from domain-informed inputs to improve predictive accuracy.

Method: Compared baseline and foundation models on CAMELS-US (671 sites, 6 time-series streams, 30 static features); tested incorporation of known exogenous inputs and annual periodic components to assess their impact.

Result: Inclusion of comprehensive exogenous inputs improves performance; annual periodic time series contribute the largest gains; foundation models underperform when lacking domain-specific exogenous data.

Conclusion: Domain knowledge integration is crucial for rainfall–runoff modeling; leveraging exogenous inputs and natural annual cycles enhances accuracy beyond generic foundation models.

Abstract: Recent advances in time series research facilitate the development of foundation models. While many state-of-the-art time series foundation models have been introduced, few studies examine their effectiveness in specific downstream applications in physical science. This work investigates the role of integrating domain knowledge into time series models for hydrological rainfall-runoff modeling. Using the CAMELS-US dataset, which includes rainfall and runoff data from 671 locations with six time series streams and 30 static features, we compare baseline and foundation models. Results demonstrate that models incorporating comprehensive known exogenous inputs outperform more limited approaches, including foundation models. Notably, incorporating natural annual periodic time series contribute the most significant improvements.

</details>


### [526] [Transformers vs. Recurrent Models for Estimating Forest Gross Primary Production](https://arxiv.org/abs/2511.11880)
*David Montero,Miguel D. Mahecha,Francesco Martinuzzi,César Aybar,Anne Klosterhalfen,Alexander Knohl,Jesús Anaya,Clemens Mosig,Sebastian Wieneke*

Main category: cs.LG

TL;DR: Transformer-based GPT-2 and LSTM offer similar overall accuracy for multimodal GPP prediction; LSTM is generally stronger, while GPT-2 excels during extreme events; GPT-2 needs longer context; radiation dominates predictors with notable contributions from Sentinel-2, MODIS LST, and Sentinel-1; model design and input fusion crucial for forecasting GPP.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of scaling GPP monitoring beyond sparse Eddy Covariance towers by leveraging deep learning and multimodal remote sensing, and evaluate state-of-the-art models for capturing the temporal dynamics of forest carbon uptake.

Method: Compare two representative models for GPP prediction—GPT-2 (transformer) and LSTM (RNN)—using multivariate inputs including radiation, Sentinel-2, MODIS land surface temperature, and Sentinel-1. Assess overall accuracy, performance during extreme events, and the effect of temporal context length. Conduct feature importance analysis to identify key predictors.

Result: Both models achieve similar accuracy overall. LSTM performs better overall, but GPT-2 excels during extreme events. Greater temporal context benefits GPT-2, while LSTM attains comparable accuracy with significantly shorter input windows, indicating an accuracy–efficiency trade-off. Radiation emerges as the dominant predictor, followed by contributions from Sentinel-2, MODIS LST, and Sentinel-1.

Conclusion: Model architecture, context length, and multimodal inputs jointly determine GPP prediction performance, informing future design of DL frameworks for monitoring terrestrial carbon dynamics.

Abstract: Monitoring the spatiotemporal dynamics of forest CO$_2$ uptake (Gross Primary Production, GPP), remains a central challenge in terrestrial ecosystem research. While Eddy Covariance (EC) towers provide high-frequency estimates, their limited spatial coverage constrains large-scale assessments. Remote sensing offers a scalable alternative, yet most approaches rely on single-sensor spectral indices and statistical models that are often unable to capture the complex temporal dynamics of GPP. Recent advances in deep learning (DL) and data fusion offer new opportunities to better represent the temporal dynamics of vegetation processes, but comparative evaluations of state-of-the-art DL models for multimodal GPP prediction remain scarce. Here, we explore the performance of two representative models for predicting GPP: 1) GPT-2, a transformer architecture, and 2) Long Short-Term Memory (LSTM), a recurrent neural network, using multivariate inputs. Overall, both achieve similar accuracy. But, while LSTM performs better overall, GPT-2 excels during extreme events. Analysis of temporal context length further reveals that LSTM attains similar accuracy using substantially shorter input windows than GPT-2, highlighting an accuracy-efficiency trade-off between the two architectures. Feature importance analysis reveals radiation as the dominant predictor, followed by Sentinel-2, MODIS land surface temperature, and Sentinel-1 contributions. Our results demonstrate how model architecture, context length, and multimodal inputs jointly determine performance in GPP prediction, guiding future developments of DL frameworks for monitoring terrestrial carbon dynamics.

</details>


### [527] [Better LLM Reasoning via Dual-Play](https://arxiv.org/abs/2511.11881)
*Zhengxin Zhang,Chengyu Huang,Aochong Oliver Li,Claire Cardie*

Main category: cs.LG

TL;DR: PasoDoble is a self-contained dual-play framework for LLMs where a Proposer and a Solver are trained adversarially to improve reasoning without external supervision, using an offline stability option.


<details>
  <summary>Details</summary>
Motivation: Reduce reliance on external supervision and mitigate reward hacking and instability in dual-play training for LLMs by enabling unsupervised, self-improving adversarial training with improved stability.

Method: Initialize two models from the same base; Proposer generates challenging questions with ground-truth answers (enriched with pre-training data); Solver attempts to answer. Proposer rewarded for valid, challenging questions; Solver rewarded for correct solutions; joint updates; optional offline paradigm alternates updates for several steps with the other fixed to improve training stability.

Result: Experimental results indicate PasoDoble improves the reasoning performance of LLMs without supervision during training.

Conclusion: PasoDoble advances unsupervised dual-play training for LLMs, offering better reasoning capabilities and training stability, with an optional offline update mode.

Abstract: Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.

</details>


### [528] [FLEX: Feature Importance from Layered Counterfactual Explanations](https://arxiv.org/abs/2511.11891)
*Nawid Keshtmand,Roussel Desmond Nzoyem,Jeffrey Nicholas Clark*

Main category: cs.LG

TL;DR: FLEX converts sets of counterfactual explanations into multi-level feature-change frequency scores, enabling local, regional, and global feature importance across datasets; it is model- and domain-agnostic and complements SHAP/LIME by revealing regional drivers.


<details>
  <summary>Details</summary>
Motivation: Provide interpretable, actionable recourse that extends beyond instance-level counterfactuals to global and regional drivers of model decisions in high-stakes domains.

Method: Transform counterfactual sets into feature-change frequency counts at local, regional, and global scales; aggregate across instances and neighborhoods to generalize beyond single explanations; remains compatible with various counterfactual generation methods and can emphasize sparsity, feasibility, or actionability to tailor importances.

Result: Applied to traffic accident severity and loan approval tasks; global rankings from FLEX correlate with SHAP and reveal additional drivers; regional analyses identify context-specific factors not captured by global summaries.

Conclusion: FLEX bridges local recourse and global attribution, enabling transparent, intervention-oriented decision-making in risk-sensitive applications.

Abstract: Machine learning models achieve state-of-the-art performance across domains, yet their lack of interpretability limits safe deployment in high-stakes settings. Counterfactual explanations are widely used to provide actionable "what-if" recourse, but they typically remain instance-specific and do not quantify which features systematically drive outcome changes within coherent regions of the feature space or across an entire dataset. We introduce FLEX (Feature importance from Layered counterfactual EXplanations), a model- and domain-agnostic framework that converts sets of counterfactuals into feature change frequency scores at local, regional, and global levels. FLEX generalises local change-frequency measures by aggregating across instances and neighbourhoods, offering interpretable rankings that reflect how often each feature must change to flip predictions. The framework is compatible with different counterfactual generation methods, allowing users to emphasise characteristics such as sparsity, feasibility, or actionability, thereby tailoring the derived feature importances to practical constraints. We evaluate FLEX on two contrasting tabular tasks: traffic accident severity prediction and loan approval, and compare FLEX to SHAP- and LIME-derived feature importance values. Results show that (i) FLEX's global rankings correlate with SHAP while surfacing additional drivers, and (ii) regional analyses reveal context-specific factors that global summaries miss. FLEX thus bridges the gap between local recourse and global attribution, supporting transparent and intervention-oriented decision-making in risk-sensitive applications.

</details>


### [529] [Chain-of-Generation: Progressive Latent Diffusion for Text-Guided Molecular Design](https://arxiv.org/abs/2511.11894)
*Lingxiao Li,Haobo Zhang,Bin Chen,Jiayu Zhou*

Main category: cs.LG

TL;DR: Proposes Chain-of-Generation (CoG), a training-free, multi-stage latent diffusion framework for text-conditioned molecular generation that decomposes prompts into curriculum-ordered semantic segments and progressively enforces them as intermediate goals, with a post-alignment phase to align textual and molecular latent spaces.


<details>
  <summary>Details</summary>
Motivation: One-shot conditioning in diffusion-based molecule generation struggles with interpretability, incomplete substructure generation, and balancing all prompt constraints. A curriculum, multi-stage approach can better satisfy complex, compositional prompts.

Method: CoG decomposes each prompt into ordinal semantic segments and guides the denoising process through multiple stages by incorporating segments as intermediate goals. It is training-free and adds a post-alignment learning phase to strengthen textual-to-molecule latent-space alignment.

Result: Extensive experiments show higher semantic alignment, diversity, and controllability than one-shot baselines, producing molecules that better reflect complex prompts and providing transparent insight into the generation process.

Conclusion: CoG offers a scalable, interpretable framework for text-conditioned molecular generation, reducing reliance on single-shot conditioning and enabling more faithful, compositional prompts; the approach may generalize to other domains requiring structured, multi-stage guidance.

Abstract: Text-conditioned molecular generation aims to translate natural-language descriptions into chemical structures, enabling scientists to specify functional groups, scaffolds, and physicochemical constraints without handcrafted rules. Diffusion-based models, particularly latent diffusion models (LDMs), have recently shown promise by performing stochastic search in a continuous latent space that compactly captures molecular semantics. Yet existing methods rely on one-shot conditioning, where the entire prompt is encoded once and applied throughout diffusion, making it hard to satisfy all the requirements in the prompt. We discuss three outstanding challenges of one-shot conditioning generation, including the poor interpretability of the generated components, the failure to generate all substructures, and the overambition in considering all requirements simultaneously. We then propose three principles to address those challenges, motivated by which we propose Chain-of-Generation (CoG), a training-free multi-stage latent diffusion framework. CoG decomposes each prompt into curriculum-ordered semantic segments and progressively incorporates them as intermediate goals, guiding the denoising trajectory toward molecules that satisfy increasingly rich linguistic constraints. To reinforce semantic guidance, we further introduce a post-alignment learning phase that strengthens the correspondence between textual and molecular latent spaces. Extensive experiments on benchmark and real-world tasks demonstrate that CoG yields higher semantic alignment, diversity, and controllability than one-shot baselines, producing molecules that more faithfully reflect complex, compositional prompts while offering transparent insight into the generation process.

</details>


### [530] [Robust Bidirectional Associative Memory via Regularization Inspired by the Subspace Rotation Algorithm](https://arxiv.org/abs/2511.11902)
*Ci Lin,Tet Yeap,Iluju Kiringa,Biwei Zhang*

Main category: cs.LG

TL;DR: Introduces B-SRA, a gradient-free training for Bidirectional Associative Memory (BAM) using orthogonal weight matrices (OWM) and gradient-pattern alignment (GPA) to boost robustness against noise and adversarial attacks; the SAME configuration (OWM+GPA) achieves the strongest resilience.


<details>
  <summary>Details</summary>
Motivation: BAM trained with Bidirectional Backpropagation (B-BP) suffers from poor robustness and high sensitivity to noise and adversarial perturbations; there is a need for more robust associative memories.

Method: Develop a gradient-free Bidirectional Subspace Rotation Algorithm (B-SRA); identify two robustness-centric principles—OWM and GPA—and introduce new regularization strategies into B-BP; perform ablation studies to determine the most robust configuration; test under various attack scenarios and memory capacities (50, 100, 200 pairs).

Result: B-SRA and the proposed regularization strategies substantially improve robustness and convergence of BAM; the SAME configuration (OWM+GPA) yields the strongest resilience across attacks and capacities.

Conclusion: B-SRA and regularization provide substantially more robust associative memories and point to new directions for building resilient neural architectures.

Abstract: Bidirectional Associative Memory (BAM) trained with Bidirectional Backpropagation (B-BP) often suffers from poor robustness and high sensitivity to noise and adversarial attacks. To address these issues, we propose a novel gradient-free training algorithm, the Bidirectional Subspace Rotation Algorithm (B-SRA), which significantly improves the robustness and convergence behavior of BAM. Through comprehensive experiments, we identify two key principles -- orthogonal weight matrices (OWM) and gradient-pattern alignment (GPA) -- as central to enhancing the robustness of BAM. Motivated by these findings, we introduce new regularization strategies into B-BP, resulting in models with greatly improved resistance to corruption and adversarial perturbations. We further conduct an ablation study across different training strategies to determine the most robust configuration and evaluate BAM's performance under a variety of attack scenarios and memory capacities, including 50, 100, and 200 associative pairs. Among all methods, the SAME configuration, which integrates both OWM and GPA, achieves the strongest resilience. Overall, our results demonstrate that B-SRA and the proposed regularization strategies lead to substantially more robust associative memories and open new directions for building resilient neural architectures.

</details>


### [531] [A Systematic Study of Model Extraction Attacks on Graph Foundation Models](https://arxiv.org/abs/2511.11912)
*Haoyan Xu,Ruizhi Qian,Jiate Li,Yushun Dong,Minghao Lin,Hanson Yan,Zhengtao Yao,Qinghua Liu,Junhao Dong,Ruopeng Huang,Yue Zhao,Mengyuan Li*

Main category: cs.LG

TL;DR: Systematic security study of model extraction attacks on Graph Foundation Models (GFMs). Formalizes a black-box threat model and six attack scenarios, and introduces a lightweight attacker encoder that aligns with the victim's text encoder. Demonstrates effective extraction on seven datasets with minimal training cost and preserved zero-shot performance, highlighting expanded MEA risk for GFMs.


<details>
  <summary>Details</summary>
Motivation: GFMs jointly pretrain graph and text encoders at scale to unify structural and semantic understanding and enable zero-shot inference. Their high pretraining cost and broad cross-domain knowledge make them attractive targets for adversaries. Prior MEA work focused on small GNNs on a single graph, leaving large-scale, multimodal GFMs underexplored.

Method: Formalize six practical black-box attack scenarios covering domain-level and graph-specific goals, architectural mismatch, limited query budgets, partial node access, and training data discrepancies. Propose a lightweight extraction method that trains an attacker encoder via supervised regression of graph embeddings to stay aligned with the victim text encoder, without requiring contrastive pretraining data. Show that the attacker preserves the victim's zero-shot inference on unseen graphs.

Result: Across seven datasets, the attacker can approximate the victim model using only a tiny fraction of the original training cost, with almost no loss in accuracy. The attacks reveal a broad MEA surface for GFMs across domain-level and graph-specific goals, multimodal alignment, and various constraints.

Conclusion: GFMs significantly expand the MEA surface, necessitating deployment-aware security defenses for large-scale graph learning systems. Future work should focus on robust defenses, detection, and privacy-preserving mechanisms to mitigate model extraction risks in GFMs.

Abstract: Graph machine learning has advanced rapidly in tasks such as link prediction, anomaly detection, and node classification. As models scale up, pretrained graph models have become valuable intellectual assets because they encode extensive computation and domain expertise. Building on these advances, Graph Foundation Models (GFMs) mark a major step forward by jointly pretraining graph and text encoders on massive and diverse data. This unifies structural and semantic understanding, enables zero-shot inference, and supports applications such as fraud detection and biomedical analysis. However, the high pretraining cost and broad cross-domain knowledge in GFMs also make them attractive targets for model extraction attacks (MEAs). Prior work has focused only on small graph neural networks trained on a single graph, leaving the security implications for large-scale and multimodal GFMs largely unexplored. This paper presents the first systematic study of MEAs against GFMs. We formalize a black-box threat model and define six practical attack scenarios covering domain-level and graph-specific extraction goals, architectural mismatch, limited query budgets, partial node access, and training data discrepancies. To instantiate these attacks, we introduce a lightweight extraction method that trains an attacker encoder using supervised regression of graph embeddings. Even without contrastive pretraining data, this method learns an encoder that stays aligned with the victim text encoder and preserves its zero-shot inference ability on unseen graphs. Experiments on seven datasets show that the attacker can approximate the victim model using only a tiny fraction of its original training cost, with almost no loss in accuracy. These findings reveal that GFMs greatly expand the MEA surface and highlight the need for deployment-aware security defenses in large-scale graph learning systems.

</details>


### [532] [Batch Matrix-form Equations and Implementation of Multilayer Perceptrons](https://arxiv.org/abs/2511.11918)
*Wieger Wesselink,Bram Grooten,Huub van de Wetering,Qiao Xiao,Decebal Constantin Mocanu*

Main category: cs.LG

TL;DR: Provides a complete batch matrix‑form derivation of MLP forward/backward passes, including batch norm and softmax; validates gradients symbolically; offers uniform NumPy/PyTorch/JAX/TensorFlow references and a high‑performance sparse C++ backend; demonstrates explicit formulations enabling efficient sparse computation.


<details>
  <summary>Details</summary>
Motivation: Addresses the gap where MLPs are rarely expressed in explicit batch matrix form. The work aims to enable transparent, systematic analysis, reproducible implementations, and optimization for sparse neural networks by supplying a rigorous batch-form specification and reference implementations.

Method: Derives full batch matrix‑form forward and backward equations for standard and advanced layers (including batch normalization and softmax). Validates gradient equations symbolically with SymPy. Produces uniform reference implementations across NumPy, PyTorch, JAX, TensorFlow, and a high‑performance C++ backend optimized for sparse operations.

Result: A complete, validated batch matrix-form specification of MLPs with corresponding cross-framework reference implementations and a sparse-optimized backend. The work demonstrates consistency of gradients via symbolic validation and provides an extensible foundation for teaching, research, and optimization.

Conclusion: Explicit batch matrix‑form formulations enhance understanding, verification, and optimization of neural networks. The provided derivations, validations, and implementations facilitate teaching, reproducible research, and efficient sparse computation in MLPs.

Abstract: Multilayer perceptrons (MLPs) remain fundamental to modern deep learning, yet their algorithmic details are rarely presented in complete, explicit \emph{batch matrix-form}. Rather, most references express gradients per sample or rely on automatic differentiation. Although automatic differentiation can achieve equally high computational efficiency, the usage of batch matrix-form makes the computational structure explicit, which is essential for transparent, systematic analysis, and optimization in settings such as sparse neural networks. This paper fills that gap by providing a mathematically rigorous and implementation-ready specification of MLPs in batch matrix-form. We derive forward and backward equations for all standard and advanced layers, including batch normalization and softmax, and validate all equations using the symbolic mathematics library SymPy. From these specifications, we construct uniform reference implementations in NumPy, PyTorch, JAX, TensorFlow, and a high-performance C++ backend optimized for sparse operations. Our main contributions are: (1) a complete derivation of batch matrix-form backpropagation for MLPs, (2) symbolic validation of all gradient equations, (3) uniform Python and C++ reference implementations grounded in a small set of matrix primitives, and (4) demonstration of how explicit formulations enable efficient sparse computation. Together, these results establish a validated, extensible foundation for understanding, teaching, and researching neural network algorithms.

</details>


### [533] [Beyond the Laplacian: Interpolated Spectral Augmentation for Graph Neural Networks](https://arxiv.org/abs/2511.11928)
*Ziyao Cui,Edric Tam*

Main category: cs.LG

TL;DR: Interpolated Laplacian Embeddings (ILEs) extend spectral node features by interpolating eigenvectors from a family of graph matrices beyond the standard Laplacian, providing a practical augmentation that improves GNN performance when node features are scarce.


<details>
  <summary>Details</summary>
Motivation: GNNs often suffer from limited or missing informative node features. Spectral embeddings from the Laplacian are useful, but exploring embeddings from other graph matrices could yield richer representations and improve learning.

Method: Define ILEs via a simple, expressive family of interpolated graph matrices; interpret the resulting embeddings using spectral graph theory; augment node features with these embeddings; validate via simulations and experiments on real datasets with common GNN architectures.

Result: Feature augmentation with ILEs improves GNN performance across multiple architectures and datasets, demonstrating the practical value of expanding the spectral feature toolkit.

Conclusion: ILEs offer a straightforward, practical addition to the spectral augmentation toolbox for GNNs, particularly when original node features are limited or noisy.

Abstract: Graph neural networks (GNNs) are fundamental tools in graph machine learning. The performance of GNNs relies crucially on the availability of informative node features, which can be limited or absent in real-life datasets and applications. A natural remedy is to augment the node features with embeddings computed from eigenvectors of the graph Laplacian matrix. While it is natural to default to Laplacian spectral embeddings, which capture meaningful graph connectivity information, we ask whether spectral embeddings from alternative graph matrices can also provide useful representations for learning. We introduce Interpolated Laplacian Embeddings (ILEs), which are derived from a simple yet expressive family of graph matrices. Using tools from spectral graph theory, we offer a straightforward interpretation of the structural information that ILEs capture. We demonstrate through simulations and experiments on real-world datasets that feature augmentation via ILEs can improve performance across commonly used GNN architectures. Our work offers a straightforward and practical approach that broadens the practitioner's spectral augmentation toolkit when node features are limited.

</details>


### [534] [A Systematic Analysis of Out-of-Distribution Detection Under Representation and Training Paradigm Shifts](https://arxiv.org/abs/2511.11934)
*C. César Claros Olivares,Austin J. Brockmeier*

Main category: cs.LG

TL;DR: Systematic, representation-centric evaluation of OOD detection across CNNs and ViTs under distribution shifts, using AURC and AUGRC; probabilistic detectors excel for ID misclassification, geometry-aware detectors favor CNNs under strong shifts and GradNorm/KPCA for ViTs; PCA and MCD trade-offs affect performance; results advocate a representation-first approach to OOD detection.


<details>
  <summary>Details</summary>
Motivation: To understand what drives OOD detection performance under distribution shift and whether the choice of detector or the underlying representation space determines effectiveness.

Method: Empirical study across CLIP-stratified regimes with two representation paradigms (CNNs trained from scratch; fine-tuned ViT) evaluated on CIFAR-10/100, SuperCIFAR-100, and TinyImageNet. AURC and AUGRC as primary metrics. A rigorous statistical pipeline (Friedman test with Conover-Holm post-hoc) and Bron-Kerbosch cliques used for multiple comparisons. Analysis of detector families (probabilistic vs geometry-aware) and the effect of MCD and PCA.

Result: The learned feature space largely determines OOD efficacy for both CNNs and ViTs. Probabilistic scores (e.g., MSR, GEN) outperform in ID misclassification for both architectures. Under strong distribution shifts, geometry-aware scores (e.g., NNGuide, fDBD, CTM) are stronger for CNNs, while GradNorm and KPCA Reconstruction Error remain consistently competitive for ViTs. There is a class-count-dependent trade-off for Monte-Carlo Dropout, and applying PCA projections improves several detectors.

Conclusion: The study supports a representation-centric view of OOD detection and offers statistically grounded guidance for method selection under distribution shift.

Abstract: We present a systematic comparison of out-of-distribution (OOD) detection methods across CLIP-stratified regimes using AURC and AUGRC as primary metrics. Experiments cover two representation paradigms: CNNs trained from scratch and a fine-tuned Vision Transformer (ViT), evaluated on CIFAR-10/100, SuperCIFAR-100, and TinyImageNet. Using a multiple-comparison-controlled, rank-based pipeline (Friedman test with Conover-Holm post-hoc) and Bron-Kerbosch cliques, we find that the learned feature space largely determines OOD efficacy. For both CNNs and ViTs, probabilistic scores (e.g., MSR, GEN) dominate misclassification (ID) detection. Under stronger shifts, geometry-aware scores (e.g., NNGuide, fDBD, CTM) prevail on CNNs, whereas on ViTs GradNorm and KPCA Reconstruction Error remain consistently competitive. We further show a class-count-dependent trade-off for Monte-Carlo Dropout (MCD) and that a simple PCA projection improves several detectors. These results support a representation-centric view of OOD detection and provide statistically grounded guidance for method selection under distribution shift.

</details>


### [535] [SurvBench: A Standardised Preprocessing Pipeline for Multi-Modal Electronic Health Record Survival Analysis](https://arxiv.org/abs/2511.11935)
*Munib Mesinovic,Tingting Zhu*

Main category: cs.LG

TL;DR: SurvBench provides an open-source, reproducible preprocessing pipeline that standardizes multi-modal EHR data (MIMIC-IV, eICU, MC-MED) for survival analysis, enabling leakage-free splitting, missingness tracking, and compatibility with pycox.


<details>
  <summary>Details</summary>
Motivation: Inconsistent preprocessing hinders reproducibility and fair comparison of deep learning survival models using EHR data; there is a need for a standardized, configurable preprocessing pipeline across major datasets and modalities.

Method: SurvBench introduces data loaders for MIMIC-IV, eICU, and MC-MED, supporting time-series vitals, static demographics, ICD codes, and radiology reports. It enforces data quality controls, patient-level splits to prevent leakage, explicit missingness tracking, and standardized temporal aggregation. It supports single-risk and competing-risks scenarios and outputs model-ready tensors compatible with pycox. The workflow is configuration-driven with comprehensive documentation.

Result: A reproducible preprocessing workflow that standardizes multi-modal EHR data for survival analysis, enabling leakage-free splits, rigorous data quality checks, and ready-to-use outputs for single-risk and competing-risks models across three major datasets, compatible with existing survival models in pycox.

Conclusion: SurvBench addresses the preprocessing gap in EHR-based survival analysis and enables fair, reproducible comparisons of deep learning methods by providing a unified, well-documented preprocessing pipeline.

Abstract: Electronic health record (EHR) data present tremendous opportunities for advancing survival analysis through deep learning, yet reproducibility remains severely constrained by inconsistent preprocessing methodologies. We present SurvBench, a comprehensive, open-source preprocessing pipeline that transforms raw PhysioNet datasets into standardised, model-ready tensors for multi-modal survival analysis. SurvBench provides data loaders for three major critical care databases, MIMIC-IV, eICU, and MC-MED, supporting diverse modalities including time-series vitals, static demographics, ICD diagnosis codes, and radiology reports. The pipeline implements rigorous data quality controls, patient-level splitting to prevent data leakage, explicit missingness tracking, and standardised temporal aggregation. SurvBench handles both single-risk (e.g., in-hospital mortality) and competing-risks scenarios (e.g., multiple discharge outcomes). The outputs are compatible with pycox library packages and implementations of standard statistical and deep learning models. By providing reproducible, configuration-driven preprocessing with comprehensive documentation, SurvBench addresses the "preprocessing gap" that has hindered fair comparison of deep learning survival models, enabling researchers to focus on methodological innovation rather than data engineering.

</details>


### [536] [Learning the relative composition of EEG signals using pairwise relative shift pretraining](https://arxiv.org/abs/2511.11940)
*Christopher Sandino,Sayeri Lala,Geeling Chau,Melika Ayoughi,Behrooz Mahasseni,Ellen Zippi,Ali Moin,Erdrin Azemi,Hanlin Goh*

Main category: cs.LG

TL;DR: PARS pretraining uses pairwise relative temporal shift prediction for EEG self-supervised learning, enabling long-range temporal dependency encoding and outperforming reconstruction-based pretraining methods.


<details>
  <summary>Details</summary>
Motivation: EEG labeling is expensive; current SSL mostly captures local patterns via reconstruction (e.g., MAE). There is a need for pretraining that captures long-range temporal structure to improve label-efficient and transfer learning in EEG clinical tasks like sleep staging and seizure detection.

Method: Introduce PARS (Pairwise Relative Shift) pretraining: randomly sample EEG window pairs and predict their relative temporal shift. This non-reconstruction objective trains transformers to capture temporal composition and long-range dependencies, complementing or replacing reconstruction-based SSL.

Result: PARS-pretrained transformers consistently outperform existing pretraining strategies in label-efficient and transfer learning settings across various EEG decoding tasks.

Conclusion: PARS establishes a new paradigm for self-supervised EEG representation learning by focusing on relative temporal shifts, enabling better long-range dependency modeling and improved downstream performance.

Abstract: Self-supervised learning (SSL) offers a promising approach for learning electroencephalography (EEG) representations from unlabeled data, reducing the need for expensive annotations for clinical applications like sleep staging and seizure detection. While current EEG SSL methods predominantly use masked reconstruction strategies like masked autoencoders (MAE) that capture local temporal patterns, position prediction pretraining remains underexplored despite its potential to learn long-range dependencies in neural signals. We introduce PAirwise Relative Shift or PARS pretraining, a novel pretext task that predicts relative temporal shifts between randomly sampled EEG window pairs. Unlike reconstruction-based methods that focus on local pattern recovery, PARS encourages encoders to capture relative temporal composition and long-range dependencies inherent in neural signals. Through comprehensive evaluation on various EEG decoding tasks, we demonstrate that PARS-pretrained transformers consistently outperform existing pretraining strategies in label-efficient and transfer learning settings, establishing a new paradigm for self-supervised EEG representation learning.

</details>


### [537] [Computation-aware Energy-harvesting Federated Learning: Cyclic Scheduling with Selective Participation](https://arxiv.org/abs/2511.11949)
*Eunjeong Jeong,Nikolaos Pappas*

Main category: cs.LG

TL;DR: A battery-aware, cyclic participation framework for energy-harvesting FL (FedBacys) that clusters devices and schedules them to reduce energy consumption; an even more efficient variant, FedBacys-Odd, enables selective participation. Includes convergence analysis and shows improved energy efficiency and robustness over baselines.


<details>
  <summary>Details</summary>
Motivation: FL incurs significant energy costs on client devices; energy harvesting causes intermittent participation, leading to learning instability and inefficiency.

Method: Propose FedBacys with battery-aware cyclic participation by clustering clients and sequentially scheduling them to minimize redundant computations; introduce FedBacys-Odd for selective participation; provide convergence analysis; validate via numerical experiments.

Result: Substantial reductions in system-wide energy usage and improved learning stability; FedBacys-Odd achieves further energy savings without sacrificing performance; convergence is established; experiments show superior energy efficiency and robustness compared to existing methods.

Conclusion: Battery-aware, clustered, cyclic participation is effective for EHFL; selective participation offers additional energy savings; the framework is convergent and robust in practical settings.

Abstract: Federated Learning (FL) is a powerful paradigm for distributed learning, but its increasing complexity leads to significant energy consumption from client-side computations for training models. In particular, the challenge is critical in energy-harvesting FL (EHFL) systems where participation availability of each device oscillates due to limited energy. To address this, we propose FedBacys, a battery-aware EHFL framework using cyclic client participation based on users' battery levels. By clustering clients and scheduling them sequentially, FedBacys minimizes redundant computations, reduces system-wide energy usage, and improves learning stability. We also introduce FedBacys-Odd, a more energy-efficient variant that allows clients to participate selectively, further reducing energy costs without compromising performance. We provide a convergence analysis for our framework and demonstrate its superior energy efficiency and robustness compared to existing algorithms through numerical experiments.

</details>


### [538] [Quantile Q-Learning: Revisiting Offline Extreme Q-Learning with Quantile Regression](https://arxiv.org/abs/2511.11973)
*Xinming Gao,Shangzhe Li,Yujin Cai,Wenwu Yu*

Main category: cs.LG

TL;DR: A method to estimate the temperature parameter in Extreme Q-Learning via quantile regression, plus mild generalization-based value regularization, yielding stable offline RL training with consistent hyperparameters and competitive results on D4RL and NeoRL2.


<details>
  <summary>Details</summary>
Motivation: XQL/MXQL suffer from dataset/domain-specific hyperparameter tuning and training instability; a stable, generalizable offline RL method is needed.

Method: Estimate the temperature coefficient beta with quantile regression under mild assumptions; introduce a value regularization technique with mild generalization inspired by constrained value learning to improve stability.

Result: Competitive or superior performance across D4RL and NeoRL2; stable training dynamics; consistent hyperparameters across datasets/domains.

Conclusion: The approach addresses key weaknesses of XQL/MXQL by principled beta estimation and mild regularization, enabling robust offline RL with reduced hyperparameter tuning.

Abstract: Offline reinforcement learning (RL) enables policy learning from fixed datasets without further environment interaction, making it particularly valuable in high-risk or costly domains. Extreme $Q$-Learning (XQL) is a recent offline RL method that models Bellman errors using the Extreme Value Theorem, yielding strong empirical performance. However, XQL and its stabilized variant MXQL suffer from notable limitations: both require extensive hyperparameter tuning specific to each dataset and domain, and also exhibit instability during training. To address these issues, we proposed a principled method to estimate the temperature coefficient $β$ via quantile regression under mild assumptions. To further improve training stability, we introduce a value regularization technique with mild generalization, inspired by recent advances in constrained value learning. Experimental results demonstrate that the proposed algorithm achieves competitive or superior performance across a range of benchmark tasks, including D4RL and NeoRL2, while maintaining stable training dynamics and using a consistent set of hyperparameters across all datasets and domains.

</details>


### [539] [ReCast: Reliability-aware Codebook Assisted Lightweight Time Series Forecasting](https://arxiv.org/abs/2511.11991)
*Xiang Ma,Taihua Chen,Pengcheng Wang,Xuemei Li,Caiming Zhang*

Main category: cs.LG

TL;DR: ReCast is a lightweight, reliability-aware forecasting framework that uses patchwise codebook quantization to capture recurring local patterns, with a dual-path model for regular and residual components and a DRO-guided codebook update strategy to adapt to non-stationarity, achieving strong accuracy and efficiency gains.


<details>
  <summary>Details</summary>
Motivation: Real-world time series exhibit local, dynamic patterns that global decomposition or heavy models fail to capture efficiently. There is a need for lightweight, robust forecasting methods that adapt to distribution shifts and resource constraints.

Method: Encode local patterns into discrete embeddings via patch-wise quantization using a learnable codebook. Use a dual-path architecture: a quantization path modeling regular, recurring structures and a residual path reconstructing irregular fluctuations. Update codebook reliability via incremental corrections weighted by multiple reliability factors fused under a distributionally robust optimization (DRO) framework to handle non-stationarity.

Result: ReCast outperforms state-of-the-art models in accuracy, efficiency, and adaptability to distribution shifts across extensive experiments.

Conclusion: The framework delivers a practical, robust, and efficient solution for time series forecasting by combining quantized local patterns with DRO-guided codebook updates and a dual-path residual mechanism, suitable for real-time and resource-constrained settings.

Abstract: Time series forecasting is crucial for applications in various domains. Conventional methods often rely on global decomposition into trend, seasonal, and residual components, which become ineffective for real-world series dominated by local, complex, and highly dynamic patterns. Moreover, the high model complexity of such approaches limits their applicability in real-time or resource-constrained environments. In this work, we propose a novel \textbf{RE}liability-aware \textbf{C}odebook-\textbf{AS}sisted \textbf{T}ime series forecasting framework (\textbf{ReCast}) that enables lightweight and robust prediction by exploiting recurring local shapes. ReCast encodes local patterns into discrete embeddings through patch-wise quantization using a learnable codebook, thereby compactly capturing stable regular structures. To compensate for residual variations not preserved by quantization, ReCast employs a dual-path architecture comprising a quantization path for efficient modeling of regular structures and a residual path for reconstructing irregular fluctuations. A central contribution of ReCast is a reliability-aware codebook update strategy, which incrementally refines the codebook via weighted corrections. These correction weights are derived by fusing multiple reliability factors from complementary perspectives by a distributionally robust optimization (DRO) scheme, ensuring adaptability to non-stationarity and robustness to distribution shifts. Extensive experiments demonstrate that ReCast outperforms state-of-the-art (SOTA) models in accuracy, efficiency, and adaptability to distribution shifts.

</details>


### [540] [Selecting Fine-Tuning Examples by Quizzing VLMs](https://arxiv.org/abs/2511.12002)
*Tenghao Ji,Eytan Adar*

Main category: cs.LG

TL;DR: QZLoRA automates image selection for LoRA-based fine-tuning of text-to-image diffusion by ranking candidate images with QuizRank, yielding better-aligned, photorealistic outputs from fewer samples and transferable stylized illustrations.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning diffusion models on topic-specific data is improved when training images are high-quality and representative. Heterogeneous sources (e.g., Wikipedia Commons) lead to poorer outputs. There is a need for automated, efficient selection of exemplar images that capture the target concept.

Method: QZLoRA combines LoRA, a parameter-efficient fine-tuning approach, with QuizRank. Images are treated as educational interventions and quizzed by a vision-language model to assess alignment with the target concept; top-ranked images are selected to train the LoRA adapters.

Result: The approach yields better-aligned, photorealistic generative outputs with fewer fine-tuning samples. It also extends to stylized illustrations that remain representative of the target concept.

Conclusion: Automating exemplar selection through automated visual reasoning, when coupled with parameter-efficient fine-tuning, shows promise for topic-adaptive generative modeling and reduces data requirements.

Abstract: A challenge in fine-tuning text-to-image diffusion models for specific topics is to select good examples. Fine-tuning from image sets of varying quality, such as Wikipedia Commons, will often produce poor output. However, training images that \textit{do} exemplify the target concept (e.g., a \textit{female Mountain Bluebird}) help ensure that the generated images are similarly representative (e.g., have the prototypical blue-wings and gray chest). In this work, we propose QZLoRA, a framework to select images for low-rank adaptation (LoRA). The approach leverages QuizRank, a method to automatically rank images by treating them as an `educational intervention' and `quizzing' a VLM. We demonstrate that QZLoRA can produce better aligned, photorealistic images with fewer samples. We also show that these fine-tuned models can produce stylized that are similarly representative (i.e., illustrations). Our results highlight the promise of combining automated visual reasoning with parameter-efficient fine-tuning for topic-adaptive generative modeling.

</details>


### [541] [EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation](https://arxiv.org/abs/2511.12033)
*Jiahe Shi,Zhengqi Gao,Ching-Yun Ko,Duane Boning*

Main category: cs.LG

TL;DR: Entropy-aware RL (EARL) for Verilog generation improves functional correctness by focusing policy updates on high-uncertainty tokens identified via RTL entropy analysis, achieving up to 14.7% higher functional pass rates over baselines while enhancing training stability.


<details>
  <summary>Details</summary>
Motivation: There is a gap between LLM-based RTL generation and real-world design needs: syntax errors, semantic misalignment, and poor guidance in long structured RTL. Many tokens contribute differently to functional correctness; identifying and exploiting high-uncertainty tokens can improve learning efficiency.

Method: Propose EARL framework: entropy analysis to detect high-uncertainty tokens (e.g., always, if, assign, posedge) in Verilog generation; use verifiable rewards for RL; gate policy gradient updates to high-entropy tokens to concentrate learning on functionally important regions; maintain stability by selective updates; evaluated on VerilogEval and RTLLM.

Result: EARL yields up to 14.7% improvement in functional pass rates over LLM baselines; reduces unnecessary gradient updates; improves training stability.

Conclusion: Entropy-guided selective updates enable more reliable policy improvement for structured RTL generation, suggesting that targeted RL focusing on critical tokens can bridge the gap between LLM capabilities and real-world RTL design demands.

Abstract: Recent advances in large language models (LLMs) have demonstrated significant potential in hardware design automation, particularly in using natural language to synthesize Register-Transfer Level (RTL) code. Despite this progress, a gap remains between model capability and the demands of real-world RTL design, including syntax errors, functional hallucinations, and weak alignment to designer intent. Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising approach to bridge this gap, as hardware provides executable and formally checkable signals that can be used to further align model outputs with design intent. However, in long, structured RTL code sequences, not all tokens contribute equally to functional correctness, and naïvely spreading gradients across all tokens dilutes learning signals. A key insight from our entropy analysis in RTL generation is that only a small fraction of tokens (e.g., always, if, assign, posedge) exhibit high uncertainty and largely influence control flow and module structure. To address these challenges, we present EARL, an Entropy-Aware Reinforcement Learning framework for Verilog generation. EARL performs policy optimization using verifiable reward signals and introduces entropy-guided selective updates that gate policy gradients to high-entropy tokens. This approach preserves training stability and concentrates gradient updates on functionally important regions of code. Our experiments on VerilogEval and RTLLM show that EARL improves functional pass rates over prior LLM baselines by up to 14.7%, while reducing unnecessary updates and improving training stability. These results indicate that focusing RL on critical, high-uncertainty tokens enables more reliable and targeted policy improvement for structured RTL code generation.

</details>


### [542] [Mesh-based Super-resolution of Detonation Flows with Multiscale Graph Transformers](https://arxiv.org/abs/2511.12041)
*Shivam Barwey,Pinaki Pal*

Main category: cs.LG

TL;DR: Introduces SR-GT, a multiscale graph transformer for mesh-based super-resolution of reacting flows, achieving high accuracy and outperforming interpolation on 2D detonation in hydrogen–air.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for accurate, geometry-agnostic super-resolution in reacting flows to enable subgrid closures, faster forecasting, data compression, and recovery of sparse measurements; unstructured/complex geometries motivate a graph-based representation and a transformer to capture long-range dependencies.

Method: Proposes a first-of-its-kind multiscale graph transformer (SR-GT) that uses an element-local (+ neighborhood) graph representation of coarse input, tokenizes it, and processes it with a transformer to generate fine-resolution flow fields on spectral-element meshes; demonstrated on 2D detonation in a hydrogen–air mixture.

Result: SR-GT achieves high super-resolution accuracy for reacting flow features and outperforms traditional interpolation-based SR schemes in the challenging multiscale, reacting-flow setting.

Conclusion: SR-GT provides a robust, geometry-compatible SR framework for reacting flows, leveraging graph representations and transformers to preserve key features at higher resolution; potential for broader applications in complex geometries and unstructured grids.

Abstract: Super-resolution flow reconstruction using state-of-the-art data-driven techniques is valuable for a variety of applications, such as subgrid/subfilter closure modeling, accelerating spatiotemporal forecasting, data compression, and serving as an upscaling tool for sparse experimental measurements. In the present work, a first-of-its-kind multiscale graph transformer approach is developed for mesh-based super-resolution (SR-GT) of reacting flows. The novel data-driven modeling paradigm leverages a graph-based flow-field representation compatible with complex geometries and non-uniform/unstructured grids. Further, the transformer backbone captures long-range dependencies between different parts of the low-resolution flow-field, identifies important features, and then generates the super-resolved flow-field that preserves those features at a higher resolution. The performance of SR-GT is demonstrated in the context of spectral-element-discretized meshes for a challenging test problem of 2D detonation propagation within a premixed hydrogen-air mixture exhibiting highly complex multiscale reacting flow behavior. The SR-GT framework utilizes a unique element-local (+ neighborhood) graph representation for the coarse input, which is then tokenized before being processed by the transformer component to produce the fine output. It is demonstrated that SR-GT provides high super-resolution accuracy for reacting flow-field features and superior performance compared to traditional interpolation-based SR schemes.

</details>


### [543] [Improving Graph Embeddings in Machine Learning Using Knowledge Completion with Validation in a Case Study on COVID-19 Spread](https://arxiv.org/abs/2511.12071)
*Rosario Napoli,Gabriele Morabito,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.LG

TL;DR: A knowledge-completion (KC) stage before graph-embedding reshapes topology via decay-based inference of hidden transitive relations, qualitatively altering embedding spaces produced by GraphSAGE and Node2Vec; the approach is transformative rather than merely additive.


<details>
  <summary>Details</summary>
Motivation: GMs learning from explicit graph topology and features can miss latent semantics hidden in sparse data. By revealing implicit connections, especially transitive ones, we can improve the quality and geometry of graph embeddings.

Method: Introduce a KC phase prior to embedding generation. Use decay-based inference functions to model hidden transitive connections, adjust the graph topology accordingly, and analyze effects on downstream embedding dynamics/aggregation in GraphSAGE and Node2Vec.

Result: Experiments show substantial changes to the embedding space geometry, indicating that the KC-enhanced pipeline reshapes representations rather than merely enriching them.

Conclusion: Incorporating Knowledge Completion fundamentally redefines graph representations and embedding quality, turning GML pipelines into transformative processes rather than simple feature/topology enrichments.

Abstract: The rise of graph-structured data has driven major advances in Graph Machine Learning (GML), where graph embeddings (GEs) map features from Knowledge Graphs (KGs) into vector spaces, enabling tasks like node classification and link prediction. However, since GEs are derived from explicit topology and features, they may miss crucial implicit knowledge hidden in seemingly sparse datasets, affecting graph structure and their representation. We propose a GML pipeline that integrates a Knowledge Completion (KC) phase to uncover latent dataset semantics before embedding generation. Focusing on transitive relations, we model hidden connections with decay-based inference functions, reshaping graph topology, with consequences on embedding dynamics and aggregation processes in GraphSAGE and Node2Vec. Experiments show that our GML pipeline significantly alters the embedding space geometry, demonstrating that its introduction is not just a simple enrichment but a transformative step that redefines graph representation quality.

</details>


### [544] [Treatment Stitching with Schrödinger Bridge for Enhancing Offline Reinforcement Learning in Adaptive Treatment Strategies](https://arxiv.org/abs/2511.12075)
*Dong-Hee Shin,Deok-Joong Lee,Young-Han Son,Tae-Eui Kam*

Main category: cs.LG

TL;DR: TreatStitch is a data augmentation framework for offline RL in adaptive treatment strategies (ATS) that stitches or bridges treatment trajectories to create more diverse, clinically valid sequences, improving learning while maintaining safety by avoiding out-of-distribution transitions.


<details>
  <summary>Details</summary>
Motivation: Clinical ATS face data scarcity and safety constraints that preclude online trial-and-error learning. Offline RL could help, but performance is limited by limited historical data. TreatStitch aims to amplify data effectively while preserving clinical validity.

Method: TreatStitch identifies similar intermediate patient states across trajectories and stitches their segments. When direct stitching is not feasible due to dissimilar states, it uses the Schrödinger bridge method to generate smooth bridging trajectories that connect the states. The augmented dataset is then used to train offline RL.

Result: Extensive experiments across multiple treatment datasets demonstrate improved offline RL performance with TreatStitch. A theoretical justification shows that the augmentation preserves clinical validity by avoiding out-of-distribution transitions.

Conclusion: TreatStitch offers a principled data augmentation approach that enhances offline RL for ATS without compromising safety, leveraging both stitching and Schrödinger-bridge-based bridging. Potential limitations include dependence on data quality and state representation; future work may explore robustness and scalability across diverse clinical domains.

Abstract: Adaptive treatment strategies (ATS) are sequential decision-making processes that enable personalized care by dynamically adjusting treatment decisions in response to evolving patient symptoms. While reinforcement learning (RL) offers a promising approach for optimizing ATS, its conventional online trial-and-error learning mechanism is not permissible in clinical settings due to risks of harm to patients. Offline RL tackles this limitation by learning policies exclusively from historical treatment data, but its performance is often constrained by data scarcity-a pervasive challenge in clinical domains. To overcome this, we propose Treatment Stitching (TreatStitch), a novel data augmentation framework that generates clinically valid treatment trajectories by intelligently stitching segments from existing treatment data. Specifically, TreatStitch identifies similar intermediate patient states across different trajectories and stitches their respective segments. Even when intermediate states are too dissimilar to stitch directly, TreatStitch leverages the Schrödinger bridge method to generate smooth and energy-efficient bridging trajectories that connect dissimilar states. By augmenting these synthetic trajectories into the original dataset, offline RL can learn from a more diverse dataset, thereby improving its ability to optimize ATS. Extensive experiments across multiple treatment datasets demonstrate the effectiveness of TreatStitch in enhancing offline RL performance. Furthermore, we provide a theoretical justification showing that TreatStitch maintains clinical validity by avoiding out-of-distribution transitions.

</details>


### [545] [SenseRay-3D: Generalizable and Physics-Informed Framework for End-to-End Indoor Propagation Modeling](https://arxiv.org/abs/2511.12092)
*Yu Zheng,Kezhi Wang,Wenji Xi,Gang Yu,Jiming Chen,Jie Zhang*

Main category: cs.LG

TL;DR: SenseRay-3D proposes a generalizable, physics-informed end-to-end framework that directly predicts 3D path-loss heatmaps from RGB-D scans without explicit geometry/material annotation, using a sensing-driven voxel scene and SwinUNETR, achieving 4.27 dB MAE and 217 ms per sample on unseen interiors, plus a synthetic benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: Manual geometry/material modeling for indoor radio propagation is labor-intensive and not scalable; there is a need for end-to-end, sensing-driven approaches that generalize across environments while maintaining physical consistency.

Method: Construct a voxelized, sensing-driven scene representation encoding occupancy, electromagnetic material properties, and transmitter–receiver geometry from RGB-D inputs; employ a SwinUNETR-based network to map this representation to 3D path-loss heatmaps relative to free-space loss; develop a synthetic indoor propagation dataset as a standardized benchmark; the approach avoids explicit geometry reconstruction or material annotation.

Result: In unseen environments, achieves a mean absolute error of 4.27 dB and supports real-time inference at 217 ms per sample, demonstrating scalability, efficiency, and physical consistency; introduces a synthetic benchmark for indoor propagation research.

Conclusion: SenseRay-3D advances sense-driven, generalizable, physics-consistent indoor propagation modeling, marking a major step beyond prior EM DeepRay; enables practical planning/optimization and motivates further research and benchmarking.

Abstract: Modeling indoor radio propagation is crucial for wireless network planning and optimization. However, existing approaches often rely on labor-intensive manual modeling of geometry and material properties, resulting in limited scalability and efficiency. To overcome these challenges, this paper presents SenseRay-3D, a generalizable and physics-informed end-to-end framework that predicts three-dimensional (3D) path-loss heatmaps directly from RGB-D scans, thereby eliminating the need for explicit geometry reconstruction or material annotation. The proposed framework builds a sensing-driven voxelized scene representation that jointly encodes occupancy, electromagnetic material characteristics, and transmitter-receiver geometry, which is processed by a SwinUNETR-based neural network to infer environmental path-loss relative to free-space path-loss. A comprehensive synthetic indoor propagation dataset is further developed to validate the framework and to serve as a standardized benchmark for future research. Experimental results show that SenseRay-3D achieves a mean absolute error of 4.27 dB on unseen environments and supports real-time inference at 217 ms per sample, demonstrating its scalability, efficiency, and physical consistency. SenseRay-3D paves a new path for sense-driven, generalizable, and physics-consistent modeling of indoor propagation, marking a major leap beyond our pioneering EM DeepRay framework.

</details>


### [546] [To Align or Not to Align: Strategic Multimodal Representation Alignment for Optimal Performance](https://arxiv.org/abs/2511.12121)
*Wanlong Fang,Tianle Zhang,Alvin Chan*

Main category: cs.LG

TL;DR: Explicitly enforcing cross-modal alignment via a controllable contrastive module yields data-dependent effects on unimodal performance; there exists an optimal alignment strength that balances modality-specific signals and shared redundancy, guiding when and how to apply alignment.


<details>
  <summary>Details</summary>
Motivation: Prior work has mostly observed natural cross-modal alignment and its correlation with performance, but has not systematically tested the direct causal impact of explicit alignment strength or how data characteristics shape its benefit.

Method: Introduce a controllable contrastive learning module that can precisely adjust alignment strength during training and evaluate its impact across synthetic and real multimodal datasets with varying information structures.

Result: An optimal alignment strength exists that maximizes unimodal encoder performance; the benefit of explicit alignment depends on the redundancy between modalities, with alignment helping or hindering based on data characteristics.

Conclusion: Provide practical guidance on when and how to apply explicit alignment to achieve optimal unimodal encoder performance, emphasizing the balance between modality-specific signals and shared redundancy.

Abstract: Multimodal learning often relies on aligning representations across modalities to enable effective information integration, an approach traditionally assumed to be universally beneficial. However, prior research has primarily taken an observational approach, examining naturally occurring alignment in multimodal data and exploring its correlation with model performance, without systematically studying the direct effects of explicitly enforced alignment between representations of different modalities. In this work, we investigate how explicit alignment influences both model performance and representation alignment under different modality-specific information structures. Specifically, we introduce a controllable contrastive learning module that enables precise manipulation of alignment strength during training, allowing us to explore when explicit alignment improves or hinders performance. Our results on synthetic and real datasets under different data characteristics show that the impact of explicit alignment on the performance of unimodal models is related to the characteristics of the data: the optimal level of alignment depends on the amount of redundancy between the different modalities. We identify an optimal alignment strength that balances modality-specific signals and shared redundancy in the mixed information distributions. This work provides practical guidance on when and how explicit alignment should be applied to achieve optimal unimodal encoder performance.

</details>


### [547] [Dynamic Anomaly Identification in Accounting Transactions via Multi-Head Self-Attention Networks](https://arxiv.org/abs/2511.12122)
*Yi Wang,Ruoyi Fang,Anzhuo Xie,Hanrui Feng,Jianlin Lai*

Main category: cs.LG

TL;DR: A Transformer-based real-time anomaly detection method for accounting transactions that models multi-dimensional data as time-series matrices, achieving superior performance on public data (AUC, F1, precision, recall) and robustness to environmental and data perturbations.


<details>
  <summary>Details</summary>
Motivation: To address hidden abnormal behaviors and the strict timeliness requirements of anomaly detection in complex, real-time trading environments, where traditional methods struggle to capture global dependencies and timely identification.

Method: Represent multi-dimensional accounting records as time-series matrices. Apply embedding layers and positional encoding for low-dimensional input mapping. Use a Transformer-based sequence model with multi-head self-attention to capture global dependencies and extract features from multiple perspectives, followed by feed-forward layers and regularization to produce anomaly probability estimates. Validate on a public dataset with analyses including hyperparameter sensitivity, environmental sensitivity, and data perturbation tests.

Result: The method outperforms baseline models in AUC, F1-Score, Precision, and Recall and maintains stable performance under varying environmental conditions and data perturbations, indicating effectiveness and robustness for dynamic anomaly detection in accounting transactions.

Conclusion: The Transformer-based framework is advantageous for real-time, dynamic anomaly detection in accounting transactions, offering methodological support for intelligent financial risk control and auditing and demonstrating applicability to complex trading environments.

Abstract: This study addresses the problem of dynamic anomaly detection in accounting transactions and proposes a real-time detection method based on a Transformer to tackle the challenges of hidden abnormal behaviors and high timeliness requirements in complex trading environments. The approach first models accounting transaction data by representing multi-dimensional records as time-series matrices and uses embedding layers and positional encoding to achieve low-dimensional mapping of inputs. A sequence modeling structure with multi-head self-attention is then constructed to capture global dependencies and aggregate features from multiple perspectives, thereby enhancing the ability to detect abnormal patterns. The network further integrates feed-forward layers and regularization strategies to achieve deep feature representation and accurate anomaly probability estimation. To validate the effectiveness of the method, extensive experiments were conducted on a public dataset, including comparative analysis, hyperparameter sensitivity tests, environmental sensitivity tests, and data sensitivity tests. Results show that the proposed method outperforms baseline models in AUC, F1-Score, Precision, and Recall, and maintains stable performance under different environmental conditions and data perturbations. These findings confirm the applicability and advantages of the Transformer-based framework for dynamic anomaly detection in accounting transactions and provide methodological support for intelligent financial risk control and auditing.

</details>


### [548] [HCPO: Hierarchical Conductor-Based Policy Optimization in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.12123)
*Zejiao Liu,Junqi Tu,Yitian Hong,Luolin Xiong,Yaochu Jin,Yang Tang,Fangfei Li*

Main category: cs.LG

TL;DR: A conductor-based joint policy framework (HCPO) enhances the expressiveness and coordination of joint policies in cooperative MARL, using local conductors to retain centralized training and enable decentralized execution, with a theoretical monotonic improvement guarantee and empirical gains on major benchmarks.


<details>
  <summary>Details</summary>
Motivation: Independent agent exploration in MARL constrains joint policy expressiveness and coordination; there is a need for methods that coordinate exploration and expand the expressive capacity of joint policies.

Method: Propose a conductor-based joint policy framework and an HCPO algorithm that trains conductors and agents hierarchically to align updates with performance improvement; introduce local conductors to decouple training from execution while preserving centralized training benefits; provide a theoretical monotonicity guarantee for joint policy optimization.

Result: HCPO outperforms competitive MARL baselines in cooperative efficiency and stability on StarCraft II Multi-Agent Challenge, Multi-agent MuJoCo, and Multi-agent Particle Environment benchmarks.

Conclusion: Conductor-based joint policy frameworks can effectively coordinate exploration and expand the expressive capabilities of joint policies in MARL, with a practical training paradigm (HCPO) that keeps training centralized but execution decentralized and offers theoretical guarantees.

Abstract: In cooperative Multi-Agent Reinforcement Learning (MARL), efficient exploration is crucial for optimizing the performance of joint policy. However, existing methods often update joint policies via independent agent exploration, without coordination among agents, which inherently constrains the expressive capacity and exploration of joint policies. To address this issue, we propose a conductor-based joint policy framework that directly enhances the expressive capacity of joint policies and coordinates exploration. In addition, we develop a Hierarchical Conductor-based Policy Optimization (HCPO) algorithm that instructs policy updates for the conductor and agents in a direction aligned with performance improvement. A rigorous theoretical guarantee further establishes the monotonicity of the joint policy optimization process. By deploying local conductors, HCPO retains centralized training benefits while eliminating inter-agent communication during execution. Finally, we evaluate HCPO on three challenging benchmarks: StarCraftII Multi-agent Challenge, Multi-agent MuJoCo, and Multi-agent Particle Environment. The results indicate that HCPO outperforms competitive MARL baselines regarding cooperative efficiency and stability.

</details>


### [549] [FairGSE: Fairness-Aware Graph Neural Network without High False Positive Rates](https://arxiv.org/abs/2511.12132)
*Zhenqiang Ye,Jinjie Lu,Tianlong Gu,Fengrui Hao,Xuemin Wang*

Main category: cs.LG

TL;DR: FairGSE balances fairness and low false positives by maximizing 2D structural entropy in GNNs, achieving a 39% FPR reduction while preserving fairness gains.


<details>
  <summary>Details</summary>
Motivation: Bias in graph topology amplifies unfair predictions; existing fairness-aware GNNs optimize fairness metrics with small accuracy trade-offs but neglect negative-label predictions, resulting in high false positive rates (FPR). There is a need to calibrate classification performance alongside fairness.

Method: Introduce Fair GNN via Structural Entropy (FairGSE) that maximizes two-dimensional structural entropy (2D-SE) during training to improve fairness without sacrificing high FPR performance.

Result: Empirical evaluation on real-world datasets shows FairGSE reduces FPR by 39% compared with state-of-the-art fairness-aware GNNs, while achieving comparable gains in fairness metrics.

Conclusion: Calibration of prediction performance alongside fairness (via 2D-SE) is effective; FairGSE provides a practical framework to reduce false positives without sacrificing fairness gains.

Abstract: Graph neural networks (GNNs) have emerged as the mainstream paradigm for graph representation learning due to their effective message aggregation. However, this advantage also amplifies biases inherent in graph topology, raising fairness concerns. Existing fairness-aware GNNs provide satisfactory performance on fairness metrics such as Statistical Parity and Equal Opportunity while maintaining acceptable accuracy trade-offs. Unfortunately, we observe that this pursuit of fairness metrics neglects the GNN's ability to predict negative labels, which renders their predictions with extremely high False Positive Rates (FPR), resulting in negative effects in high-risk scenarios. To this end, we advocate that classification performance should be carefully calibrated while improving fairness, rather than simply constraining accuracy loss. Furthermore, we propose Fair GNN via Structural Entropy (\textbf{FairGSE}), a novel framework that maximizes two-dimensional structural entropy (2D-SE) to improve fairness without neglecting false positives. Experiments on several real-world datasets show FairGSE reduces FPR by 39\% vs. state-of-the-art fairness-aware GNNs, with comparable fairness improvement.

</details>


### [550] [Fusion-ResNet: A Lightweight multi-label NILM Model Using PCA-ICA Feature Fusion](https://arxiv.org/abs/2511.12139)
*Sahar Moghimian Hoosh,Ilia Kamyshev,Henni Ouerdane*

Main category: cs.LG

TL;DR: Fusion-ResNet blends ICA and PCA features for a lightweight multi-label NILM classifier, achieving higher F1 scores with less training/inference time and robustness up to 15 concurrent devices.


<details>
  <summary>Details</summary>
Motivation: Tackle NILM challenges of overfitting, poor generalization, and difficulty in disaggregating many appliances operating simultaneously by designing an end-to-end, efficient classification framework.

Method: An end-to-end NILM framework using high-frequency labeled data, a novel feature extractor that fuses ICA and PCA features, and a lightweight multi-label classifier (Fusion-ResNet).

Result: The Fusion-ResNet approach achieves higher average F1 scores across appliances compared to state-of-the-art NILM classifiers and demonstrates robustness when up to 15 appliances are concurrently active, while reducing training and inference time.

Conclusion: The proposed feature-based Fusion-ResNet framework improves NILM classification performance and efficiency, particularly under high appliance concurrency.

Abstract: Non-intrusive load monitoring (NILM) is an advanced load monitoring technique that uses data-driven algorithms to disaggregate the total power consumption of a household into the consumption of individual appliances. However, real-world NILM deployment still faces major challenges, including overfitting, low model generalization, and disaggregating a large number of appliances operating at the same time. To address these challenges, this work proposes an end-to-end framework for the NILM classification task, which consists of high-frequency labeled data, a feature extraction method, and a lightweight neural network. Within this framework, we introduce a novel feature extraction method that fuses Independent Component Analysis (ICA) and Principal Component Analysis (PCA) features. Moreover, we propose a lightweight architecture for multi-label NILM classification (Fusion-ResNet). The proposed feature-based model achieves a higher $F1$ score on average and across different appliances compared to state-of-the-art NILM classifiers while minimizing the training and inference time. Finally, we assessed the performance of our model against baselines with a varying number of simultaneously active devices. Results demonstrate that Fusion-ResNet is relatively robust to stress conditions with up to 15 concurrently active appliances.

</details>


### [551] [Variation-Bounded Loss for Noise-Tolerant Learning](https://arxiv.org/abs/2511.12143)
*Jialiang Wang,Xiong Zhou,Xianming Liu,Gangfeng Hu,Deming Zhai,Junjun Jiang,Haoliang Li*

Main category: cs.LG

TL;DR: Proposes Variation Ratio as a robustness metric for loss functions and introduces Variation-Bounded Loss (VBL); theory shows smaller variation ratio yields better robustness; reformulates common losses into VR-bounded forms; experiments indicate effectiveness and flexibility.


<details>
  <summary>Details</summary>
Motivation: Noisy labels degrade supervised learning performance. The paper seeks a principled, general framework to design robust losses by quantifying loss variation under label noise and by relaxing symmetry constraints to enable asymmetric designs.

Method: Define Variation Ratio (VR) as a property of loss functions and prove that lower VR implies better robustness. Show VR helps relax the symmetric condition and facilitates asymmetric loss design. Define Variation-Bounded Loss (VBL) as a family with bounded VR. Reformulate several standard losses into VR-bounded forms. Validate via experiments on multiple datasets.

Result: Theoretical analyses link VR to robustness, with smaller VR indicating improved performance under label noise. VR provides a practical path to asymmetric loss designs. Losses can be reformulated into VR-bounded forms. Empirical results across diverse datasets demonstrate the approach's effectiveness and flexibility.

Conclusion: Variation-Bounded Loss offers a unified, flexible framework for robust loss design against noisy labels. Targeting a smaller Variation Ratio and employing VR-bounded reformulations can enhance robustness. Future work could explore broader noise models, optimization aspects, and applications to other tasks.

Abstract: Mitigating the negative impact of noisy labels has been aperennial issue in supervised learning. Robust loss functions have emerged as a prevalent solution to this problem. In this work, we introduce the Variation Ratio as a novel property related to the robustness of loss functions, and propose a new family of robust loss functions, termed Variation-Bounded Loss (VBL), which is characterized by a bounded variation ratio. We provide theoretical analyses of the variation ratio, proving that a smaller variation ratio would lead to better robustness. Furthermore, we reveal that the variation ratio provides a feasible method to relax the symmetric condition and offers a more concise path to achieve the asymmetric condition. Based on the variation ratio, we reformulate several commonly used loss functions into a variation-bounded form for practical applications. Positive experiments on various datasets exhibit the effectiveness and flexibility of our approach.

</details>


### [552] [Finding Time Series Anomalies using Granular-ball Vector Data Description](https://arxiv.org/abs/2511.12147)
*Lifeng Shen,Liang Peng,Ruiwen Liu,Shuyin Xia,Yi Liu*

Main category: cs.LG

TL;DR: GBOC uses Granular-ball Vector Data Description (GVDD) to partition latent space into dense regions that act as compact prototypes for normal behavior; during training, samples align to nearest granular-ball centers; during inference, anomaly scores are distance to nearest granular-ball; enables robust and efficient time-series anomaly detection with fewer prototypes.


<details>
  <summary>Details</summary>
Motivation: Traditional nearest-neighbor and clustering methods rely on rigid assumptions (e.g., fixed number of neighbors/clusters) and often fail in nonlinear, dynamic time-series scenarios, creating a need for a data-adaptive, topology-preserving representation of normal behavior.

Method: GVDD partitions the latent space into granular-balls via density-guided hierarchical splitting and denoising. Each granular-ball serves as a local normal-behavior prototype. Training increases representation compactness by aligning samples to the nearest granular-ball center; inference computes anomaly scores as the distance to the nearest granular-ball. This reduces prototype count while preserving local structure.

Result: Extensive experiments demonstrate the effectiveness and superiority of GBOC in time-series anomaly detection, showing robustness and efficiency.

Conclusion: GBOC with GVDD provides a scalable, robust framework for anomaly detection by focusing on dense, high-quality regions and preserving local topology, outperforming traditional rigid-method baselines.

Abstract: Modeling normal behavior in dynamic, nonlinear time series data is challenging for effective anomaly detection. Traditional methods, such as nearest neighbor and clustering approaches, often depend on rigid assumptions, such as a predefined number of reliable neighbors or clusters, which frequently break down in complex temporal scenarios. To address these limitations, we introduce the Granular-ball One-Class Network (GBOC), a novel approach based on a data-adaptive representation called Granular-ball Vector Data Description (GVDD). GVDD partitions the latent space into compact, high-density regions represented by granular-balls, which are generated through a density-guided hierarchical splitting process and refined by removing noisy structures. Each granular-ball serves as a prototype for local normal behavior, naturally positioning itself between individual instances and clusters while preserving the local topological structure of the sample set. During training, GBOC improves the compactness of representations by aligning samples with their nearest granular-ball centers. During inference, anomaly scores are computed based on the distance to the nearest granular-ball. By focusing on dense, high-quality regions and significantly reducing the number of prototypes, GBOC delivers both robustness and efficiency in anomaly detection. Extensive experiments validate the effectiveness and superiority of the proposed method, highlighting its ability to handle the challenges of time series anomaly detection.

</details>


### [553] [Open Banking Foundational Model: Learning Language Representations from Few Financial Transactions](https://arxiv.org/abs/2511.12154)
*Gustavo Polleti,Marlesson Santana,Eduardo Fontes*

Main category: cs.LG

TL;DR: A multimodal, self-supervised foundation model for financial transactions that fuses structured data and text, achieving strong performance and cross-institution cross-geography generalization, especially in data-scarce Open Banking settings.


<details>
  <summary>Details</summary>
Motivation: To leverage multimodal, self-supervised learning to unify structured attributes and textual descriptions in financial tasks, enabling better performance in fraud, credit risk, and customer insights with limited labeled data and across many institutions.

Method: Proposes a neural model integrating structured transaction features with unstructured textual descriptions; adapts masked language modeling to sequences of transactions; pretrains on multimodal data; evaluates against classical feature engineering and discrete event sequence baselines across thousands of NA financial institutions.

Result: Outperforms classical methods; particularly effective in data-scarce Open Banking scenarios; first large-scale study across thousands of financial institutions in North America; demonstrates cross-geography/institution generalizability of multimodal representations.

Conclusion: Self-supervised, multimodal representations are promising for broad financial applications, enabling improved fraud prevention, credit risk assessment, and customer insights with potential cross-institutional and cross-geographic generalization.

Abstract: We introduced a multimodal foundational model for financial transactions that integrates both structured attributes and unstructured textual descriptions into a unified representation. By adapting masked language modeling to transaction sequences, we demonstrated that our approach not only outperforms classical feature engineering and discrete event sequence methods but is also particularly effective in data-scarce Open Banking scenarios. To our knowledge, this is the first large-scale study across thousands of financial institutions in North America, providing evidence that multimodal representations can generalize across geographies and institutions. These results highlight the potential of self-supervised models to advance financial applications ranging from fraud prevention and credit risk to customer insights

</details>


### [554] [Rethinking Deep Alignment Through The Lens Of Incomplete Learning](https://arxiv.org/abs/2511.12155)
*Thong Bach,Dung Nguyen,Thao Minh Le,Truyen Tran*

Main category: cs.LG

TL;DR: Mechanistic analysis of safety vulnerabilities in autoregressive LLMs reveals position-dependent gradient weakening that causes incomplete safety learning; introduces base-favored tokens as indicators; proposes targeted completion with adaptive penalties and hybrid teacher distillation; demonstrates strong adversarial robustness improvements (48–98% reduced attack success) across Llama/Qwen while preserving capabilities.


<details>
  <summary>Details</summary>
Motivation: Address fundamental limitations in safety alignment; explain why safety improvements fail in later response regions; provide actionable remedies.

Method: Analyze training dynamics to identify gradient weakening; define base-favored tokens; develop targeted completion with adaptive penalties and hybrid teacher distillation; empirical evaluation on Llama and Qwen models against adversarial attacks.

Result: Substantial adversarial robustness gains (48–98% attack success reduction) with preserved general capabilities across model families; evidence for mechanistic understanding and practical method.

Conclusion: Safety alignment has fundamental limits due to autoregressive training dynamics; targeted completion methods can mitigate undertrained regions and significantly improve robustness while retaining capabilities.

Abstract: Large language models exhibit systematic vulnerabilities to adversarial attacks despite extensive safety alignment. We provide a mechanistic analysis revealing that position-dependent gradient weakening during autoregressive training creates signal decay, leading to incomplete safety learning where safety training fails to transform model preferences in later response regions fully. We introduce base-favored tokens -- vocabulary elements where base models assign higher probability than aligned models -- as computational indicators of incomplete safety learning and develop a targeted completion method that addresses undertrained regions through adaptive penalties and hybrid teacher distillation. Experimental evaluation across Llama and Qwen model families demonstrates dramatic improvements in adversarial robustness, with 48--98% reductions in attack success rates while preserving general capabilities. These results establish both a mechanistic understanding and practical solutions for fundamental limitations in safety alignment methodologies.

</details>


### [555] [Data-Efficient Self-Supervised Algorithms for Fine-Grained Birdsong Analysis](https://arxiv.org/abs/2511.12158)
*Houtan Ghaffari,Lukas Rauch,Paul Devos*

Main category: cs.LG

TL;DR: Proposes a lightweight Residual-MLP-RNN for birdsong syllable annotation and a robust three-stage data-efficient training pipeline (self-supervised pretraining, supervised augmentation, semi-supervised post-training), validated on canaries under extreme label scarcity, with exploration of self-supervised embeddings for linear probing and unsupervised analysis.


<details>
  <summary>Details</summary>
Motivation: Birdsong annotation at syllable level is expensive and labor-intensive; there is a need for data-efficient, automated methods that reduce expert labeling costs while maintaining reliability, especially for complex songs like the Canary.

Method: Introduce Residual-MLP-RNN architecture. Propose a three-stage training pipeline: (1) self-supervised pretraining from unlabeled data using masked prediction and online clustering; (2) supervised training with strong data augmentations for robust frame-level syllable detection; (3) semi-supervised post-training aligned with downstream task; evaluate on Canary and assess self-supervised embeddings for linear probing and unsupervised analysis.

Result: Demonstrates a lightweight yet high-performance model and a robust, data-efficient training pipeline that works well in low-label settings; Canary song used as challenging validation; self-supervised embeddings show potential for downstream linear probing and unsupervised analyses.

Conclusion: The approach reduces annotation costs and provides reliable canary syllable detectors, with potential applicability to other species; self-supervised embeddings are promising for downstream tasks and unsupervised analyses.

Abstract: Many bioacoustics, neuroscience, and linguistics research utilize birdsongs as proxy models to acquire knowledge in diverse areas. Developing models generally requires precisely annotated data at the level of syllables. Hence, automated and data-efficient methods that reduce annotation costs are in demand. This work presents a lightweight, yet performant neural network architecture for birdsong annotation called Residual-MLP-RNN. Then, it presents a robust three-stage training pipeline for developing reliable deep birdsong syllable detectors with minimal expert labor. The first stage is self-supervised learning from unlabeled data. Two of the most successful pretraining paradigms are explored, namely, masked prediction and online clustering. The second stage is supervised training with effective data augmentations to create a robust model for frame-level syllable detection. The third stage is semi-supervised post-training, which leverages the unlabeled data again. However, unlike the initial phase, this time it is aligned with the downstream task. The performance of this data-efficient approach is demonstrated for the complex song of the Canary in extreme label-scarcity scenarios. Canary has one of the most difficult songs to annotate, which implicitly validates the method for other birds. Finally, the potential of self-supervised embeddings is assessed for linear probing and unsupervised birdsong analysis.

</details>


### [556] [FGM optimization in complex domains using Gaussian process regression based profile generation algorithm](https://arxiv.org/abs/2511.12171)
*Chaitanya Kumar Konda,Piyush Agrawal,Shivansh Srivastava,Manish Agrawal*

Main category: cs.LG

TL;DR: A GPR-driven, boundary-constrained generator of functionally graded material profiles on arbitrary domains, coupled with a GA using a projection-enhanced SBX crossover to uncover optimal thermoelastic FGMs with controllable smoothness.


<details>
  <summary>Details</summary>
Motivation: Need to design FGMs on complex geometries with smooth, boundary-adhering volume fraction profiles and an efficient search over diverse design spaces; existing methods struggle with arbitrary shapes and integration with optimization.

Method: Develop a generic GPR-based profile generation algorithm that accepts boundary volume fraction values, produces smooth distributions, includes a length-scale parameter to control smoothness, and yields a design space. Couple with a modified GA where SBX crossover uses a projection operator to stay within GPR-generated space.

Result: Shows diverse, smooth FGM profiles that satisfy boundary constraints; demonstrates efficacy via thermoelastic optimization examples; demonstrates ability to control smoothness and design-space size.

Conclusion: The framework is effective for designing FGMs on arbitrary domains, offering controllable smoothness and design-space size, and integrating GPR with GA to identify optimal FGM configurations for thermoelastic applications.

Abstract: This manuscript addresses the challenge of designing functionally graded materials (FGMs) for arbitrary-shaped domains. Towards this goal, the present work proposes a generic volume fraction profile generation algorithm based on Gaussian Process Regression (GPR). The proposed algorithm can handle complex-shaped domains and generate smooth FGM profiles while adhering to the specified volume fraction values at boundaries/part of boundaries. The resulting design space from GPR comprises diverse profiles, enhancing the potential for discovering optimal configurations. Further, the algorithm allows the user to control the smoothness of the underlying profiles and the size of the design space through a length scale parameter. Further, the proposed profile generation scheme is coupled with the genetic algorithm to find the optimum FGM profiles for a given application. To make the genetic algorithm consistent with the GPR profile generation scheme, the standard simulated binary crossover operator in the genetic algorithm has been modified with a projection operator. We present numerous thermoelastic optimization examples to demonstrate the efficacy of the proposed profile generation algorithm and optimization framework.

</details>


### [557] [TSGDiff: Rethinking Synthetic Time Series Generation from a Pure Graph Perspective](https://arxiv.org/abs/2511.12174)
*Lifeng Shen,Xuyang Li,Lele Long*

Main category: cs.LG

TL;DR: Diffusion-based time-series generator TSGDiff uses dynamic graphs and a GNN encoder-decoder; introduces Topo-FID metric to assess graph-structure fidelity; shows improved preservation of temporal dependencies and structure on real datasets.


<details>
  <summary>Details</summary>
Motivation: Time-series generation by diffusion is challenged by complex temporal dependencies and structural patterns; a graph-based formulation may better capture dynamics and topology; need for a topology-aware evaluation metric.

Method: Represent time series as dynamic graphs with edges defined from Fourier spectrum characteristics and temporal dependencies; train a graph neural network encoder-decoder to learn a latent space; apply diffusion to model the structural representation distribution; propose Topo-FID combining Graph Edit Similarity (adjacency differences) and Structural Entropy Similarity (entropy of node degree distributions).

Result: Experiments on real-world datasets show high-quality synthetic time series generation with preserved temporal dependencies and structural integrity.

Conclusion: TSGDiff demonstrates that combining graph-based representations, diffusion modeling, and topology-aware evaluation improves synthetic time series generation and may better capture structure and dynamics.

Abstract: Diffusion models have shown great promise in data generation, yet generating time series data remains challenging due to the need to capture complex temporal dependencies and structural patterns. In this paper, we present \textit{TSGDiff}, a novel framework that rethinks time series generation from a graph-based perspective. Specifically, we represent time series as dynamic graphs, where edges are constructed based on Fourier spectrum characteristics and temporal dependencies. A graph neural network-based encoder-decoder architecture is employed to construct a latent space, enabling the diffusion process to model the structural representation distribution of time series effectively. Furthermore, we propose the Topological Structure Fidelity (Topo-FID) score, a graph-aware metric for assessing the structural similarity of time series graph representations. Topo-FID integrates two sub-metrics: Graph Edit Similarity, which quantifies differences in adjacency matrices, and Structural Entropy Similarity, which evaluates the entropy of node degree distributions. This comprehensive metric provides a more accurate assessment of structural fidelity in generated time series. Experiments on real-world datasets demonstrate that \textit{TSGDiff} generates high-quality synthetic time series data generation, faithfully preserving temporal dependencies and structural integrity, thereby advancing the field of synthetic time series generation.

</details>


### [558] [Understanding InfoNCE: Transition Probability Matrix Induced Feature Clustering](https://arxiv.org/abs/2511.12180)
*Ge Cheng,Shuo Wang,Yun Zhang*

Main category: cs.LG

TL;DR: SC-InfoNCE extends contrastive learning by explicitly modeling augmentation dynamics with a feature-space and transition matrix, deriving InfoNCE's objective as aligning two views to a fixed target, and introducing a scalable target to flexibly control feature similarity; yields robust improvements across vision, graph, and language tasks.


<details>
  <summary>Details</summary>
Motivation: Theoretical understanding of InfoNCE is limited; a principled framework that captures augmentation dynamics and provides flexible similarity control could better align representations with downstream data.

Method: Define an explicit feature space for augmented views and a transition probability matrix for augmentations; show InfoNCE targets converge to a constant defined by this matrix; propose SC-InfoNCE with a tunable convergence target by scaling the target matrix.

Result: Empirical results on image, graph, and text benchmarks show SC-InfoNCE achieves strong and reliable performance across domains.

Conclusion: SC-InfoNCE offers flexible, principled control of feature alignment, bridging theory and practice in contrastive learning and improving cross-domain performance.

Abstract: Contrastive learning has emerged as a cornerstone of unsupervised representation learning across vision, language, and graph domains, with InfoNCE as its dominant objective. Despite its empirical success, the theoretical underpinnings of InfoNCE remain limited. In this work, we introduce an explicit feature space to model augmented views of samples and a transition probability matrix to capture data augmentation dynamics. We demonstrate that InfoNCE optimizes the probability of two views sharing the same source toward a constant target defined by this matrix, naturally inducing feature clustering in the representation space. Leveraging this insight, we propose Scaled Convergence InfoNCE (SC-InfoNCE), a novel loss function that introduces a tunable convergence target to flexibly control feature similarity alignment. By scaling the target matrix, SC-InfoNCE enables flexible control over feature similarity alignment, allowing the training objective to better match the statistical properties of downstream data. Experiments on benchmark datasets, including image, graph, and text tasks, show that SC-InfoNCE consistently achieves strong and reliable performance across diverse domains.

</details>


### [559] [Scaling Law Analysis in Federated Learning: How to Select the Optimal Model Size?](https://arxiv.org/abs/2511.12188)
*Xuanyu Chen,Nan Yang,Shuai Wang,Dong Yuan*

Main category: cs.LG

TL;DR: A PAC-Bayes bound for generalization in federated learning reveals how optimal model size scales with the number of clients; with the same total compute, federated learning lowers the achievable generalization bound, and the optimal size depends on average client compute; the results are supported by extensive experiments.


<details>
  <summary>Details</summary>
Motivation: To address data scarcity in training large models by leveraging privacy-preserving Federated Learning, and to understand how decentralized data affects the scaling of model size and generalization.

Method: Derives a PAC-Bayes upper bound for the generalization error of models trained with stochastic optimization in federated settings, solves analytically for the model size that minimizes the bound, analyzes the impact of the number of clients and distributed compute, and validates the theory with large-scale experiments across models, networks, and datasets.

Result: The optimal model size follows a negative power-law with respect to the number of clients under fixed total compute. Switching to FL with the same total compute reduces the upper bound on achievable generalization. The estimated optimal model size should depend on the average compute across clients. Empirical results across diverse settings corroborate the theoretical predictions.

Conclusion: The work provides qualitative insights and practical guidance for scaling large models in federated settings, showing how client count and compute heterogeneity shape the feasible model size and generalization performance, and confirming FL as a viable approach under compute constraints while highlighting the importance of considering per-client compute in estimating optimal model size.

Abstract: The recent success of large language models (LLMs) has sparked a growing interest in training large-scale models. As the model size continues to scale, concerns are growing about the depletion of high-quality, well-curated training data. This has led practitioners to explore training approaches like Federated Learning (FL), which can leverage the abundant data on edge devices while maintaining privacy. However, the decentralization of training datasets in FL introduces challenges to scaling large models, a topic that remains under-explored. This paper fills this gap and provides qualitative insights on generalizing the previous model scaling experience to federated learning scenarios. Specifically, we derive a PAC-Bayes (Probably Approximately Correct Bayesian) upper bound for the generalization error of models trained with stochastic algorithms in federated settings and quantify the impact of distributed training data on the optimal model size by finding the analytic solution of model size that minimizes this bound. Our theoretical results demonstrate that the optimal model size has a negative power law relationship with the number of clients if the total training compute is unchanged. Besides, we also find that switching to FL with the same training compute will inevitably reduce the upper bound of generalization performance that the model can achieve through training, and that estimating the optimal model size in federated scenarios should depend on the average training compute across clients. Furthermore, we also empirically validate the correctness of our results with extensive training runs on different models, network settings, and datasets.

</details>


### [560] [Evaluation of Multi- and Single-objective Learning Algorithms for Imbalanced Data](https://arxiv.org/abs/2511.12191)
*Szymon Wojciechowski,Michał Woźniak*

Main category: cs.LG

TL;DR: Proposes a reliable evaluation framework for multi-objective learning that bridges single-solution methods and Pareto-front approaches by selecting Pareto-optimal regions according to user preferences, enabling fair algorithm comparison.


<details>
  <summary>Details</summary>
Motivation: In imbalanced data classification and broader MO learning, aggregating objectives loses interpretability and Pareto-front methods complicate comparisons. There is a gap in evaluation methodology for comparing single-solution MO methods with Pareto-front methods.

Method: Introduce a new evaluation approach for MO algorithms that produce single solutions but point to Pareto-front regions aligned with user preferences. Demonstrate the approach using illustrative algorithms to focus on algorithm comparison rather than learning.

Result: A novel, reliable evaluation framework is proposed; it enables fair comparisons between single-solution MO methods and Pareto-front methods by incorporating user-preference–guided Pareto selections, with demonstration via illustrative algorithms.

Conclusion: The paper fills the classifier evaluation gap by providing a framework for comparing MO algorithms with preference-aware Pareto selection, emphasizing algorithm comparison over learning; results are illustrated rather than exhaustively tested.

Abstract: Many machine learning tasks aim to find models that work well not for a single, but for a group of criteria, often opposing ones. One such example is imbalanced data classification, where, on the one hand, we want to achieve the best possible classification quality for data from the minority class without degrading the classification quality of the majority class. One solution is to propose an aggregate learning criterion and reduce the multi-objective learning task to a single-criteria optimization problem. Unfortunately, such an approach is characterized by ambiguity of interpretation since the value of the aggregated criterion does not indicate the value of the component criteria. Hence, there are more and more proposals for algorithms based on multi-objective optimization (MOO), which can simultaneously optimize multiple criteria. However, such an approach results in a set of multiple non-dominated solutions (Pareto front). The selection of a single solution from the Pareto front is a challenge itself, and much attention is paid to the issue of how to select it considering user preferences, as well as how to compare solutions returned by different MOO algorithms among themselves. Thus, a significant gap has been identified in the classifier evaluation methodology, i.e., how to reliably compare methods returning single solutions with algorithms returning solutions in the form of Pareto fronts.
  To fill the aforementioned gap, this article proposes a new, reliable way of evaluating algorithms based on multi-objective algorithms with methods that return single solutions while pointing out solutions from a Pareto front tailored to the user's preferences. This work focuses only on algorithm comparison, not their learning. The algorithms selected for this study are illustrative to help understand the proposed approach.

</details>


### [561] [MPD-SGR: Robust Spiking Neural Networks with Membrane Potential Distribution-Driven Surrogate Gradient Regularization](https://arxiv.org/abs/2511.12199)
*Runhao Jiang,Chengzhi Jiang,Rui Yan,Huajin Tang*

Main category: cs.LG

TL;DR: The paper links adversarial robustness in deep SNNs to the interaction between membrane potential distribution (MPD) and the surrogate gradient (SG). It proposes MPD-SGR, a regularization that reduces the portion of MPD within the SG's active range, thereby shrinking gradient sensitivity. Empirically, this MPD-driven regularization improves robustness across architectures, SG variants, and spike encoding schemes on several image classification benchmarks.


<details>
  <summary>Details</summary>
Motivation: SG-based SNNs are vulnerable to adversarial perturbations, and while spike coding and neuron dynamics have been studied, the role of gradient magnitude—and its dependence on MPD and SG—remains underexplored. Understanding and controlling this interaction could yield more robust SNNs.

Method: The authors theoretically relate MPD to SG magnitude and introduce MPD-SGR, a regularization term that penalizes MPD regions where the SG function is active. This guides training to shape MPD distributions so that fewer membrane potentials lie in the SG's gradient-available range, reducing sensitivity to perturbations. The approach is evaluated across diverse networks, SG variants, and spike encoding schemes on several image classification benchmarks.

Result: MPD-SGR significantly improves adversarial robustness of SNNs and generalizes across network architectures, SG function variants, and spike encoding schemes. The method consistently reduces sensitivity to input perturbations while maintaining or improving task performance on the tested benchmarks.

Conclusion: Controlling the MPD–SG interaction via MPD-SGR provides a principled, effective route to robust SNNs. By minimizing the portion of MPD lying in the gradient-active range, the gradient magnitude—and thus perturbation sensitivity—is reduced, yielding robust, broadly applicable improvements across models and encoding schemes.

Abstract: The surrogate gradient (SG) method has shown significant promise in enhancing the performance of deep spiking neural networks (SNNs), but it also introduces vulnerabilities to adversarial attacks. Although spike coding strategies and neural dynamics parameters have been extensively studied for their impact on robustness, the critical role of gradient magnitude, which reflects the model's sensitivity to input perturbations, remains underexplored. In SNNs, the gradient magnitude is primarily determined by the interaction between the membrane potential distribution (MPD) and the SG function. In this study, we investigate the relationship between the MPD and SG and its implications for improving the robustness of SNNs. Our theoretical analysis reveals that reducing the proportion of membrane potential lying within the gradient-available range of the SG function effectively mitigates the sensitivity of SNNs to input perturbations. Building upon this insight, we propose a novel MPD-driven surrogate gradient regularization (MPD-SGR) method, which enhances robustness by explicitly regularizing the MPD based on its interaction with the SG function. Extensive experiments across multiple image classification benchmarks and diverse network architectures confirm that the MPD-SGR method significantly enhances the resilience of SNNs to adversarial perturbations and exhibits strong generalizability across diverse network configurations, SG function variants, and spike encoding schemes.

</details>


### [562] [AlignTree: Efficient Defense Against LLM Jailbreak Attacks](https://arxiv.org/abs/2511.12217)
*Gil Goren,Shahar Katz,Lior Wolf*

Main category: cs.LG

TL;DR: AlignTree is a lightweight defense that uses a two-signal random-forest monitor to detect misaligned outputs during LLM generation, achieving robustness with minimal computational overhead and without extra prompts or guard models.


<details>
  <summary>Details</summary>
Motivation: To defend against adversarial prompts and unsafe content in LLMs while keeping computational costs low, addressing the shortcomings of expensive or easily bypassed existing defenses.

Method: During generation, AlignTree analyzes activations and applies a random forest classifier using two signals: (i) a linear 'refusal direction' that flags misaligned prompts, and (ii) an SVM-based signal capturing non-linear harmful-content features. No additional prompts or guard models are required.

Result: Extensive experiments show AlignTree is efficient and robust across multiple LLMs and benchmarks.

Conclusion: AlignTree provides a scalable, low-overhead defense that enhances alignment without external prompts/guards, suitable for real-world LLM deployments.

Abstract: Large Language Models (LLMs) are vulnerable to adversarial attacks that bypass safety guidelines and generate harmful content. Mitigating these vulnerabilities requires defense mechanisms that are both robust and computationally efficient. However, existing approaches either incur high computational costs or rely on lightweight defenses that can be easily circumvented, rendering them impractical for real-world LLM-based systems. In this work, we introduce the AlignTree defense, which enhances model alignment while maintaining minimal computational overhead. AlignTree monitors LLM activations during generation and detects misaligned behavior using an efficient random forest classifier. This classifier operates on two signals: (i) the refusal direction -- a linear representation that activates on misaligned prompts, and (ii) an SVM-based signal that captures non-linear features associated with harmful content. Unlike previous methods, AlignTree does not require additional prompts or auxiliary guard models. Through extensive experiments, we demonstrate the efficiency and robustness of AlignTree across multiple LLMs and benchmarks.

</details>


### [563] [Chicken Swarm Kernel Particle Filter: A Structured Rejuvenation Approach with KLD-Efficient Sampling](https://arxiv.org/abs/2511.12222)
*Hangshuo Tian*

Main category: cs.LG

TL;DR: Investigates the interaction between CSO-based rejuvenation in particle filters and KLD-based adaptive sampling. Shows CSO updates can be viewed as a mean-square contraction, yielding more concentrated (peaked) particle distributions. Using Karamata's inequality on the concave function governing KLD bin occupancy, the CPF is predicted to require a lower expected particle count to meet the same error bound, offering a tractable framework to explain empirical efficiency and guide design of adaptive filters.


<details>
  <summary>Details</summary>
Motivation: To understand how SI-based rejuvenation (CSO) interacts with KLD adaptive sampling in particle filters, and to provide a theoretical lens for the observed computational gains when combining these techniques.

Method: Under a simplified modeling framework, analyze the CSO rejuvenation step as a mean-square contraction affecting the particle distribution. Prove that this contraction leads to a more concentrated distribution. Apply Karamata's inequality to the concave function governing expected KLD-bin occupancy to relate the changes in the distribution to the required particle count for the same statistical error bound.

Result: Derives an analytical link suggesting that CSO-enhanced PFs (CPF) can achieve the same error bound with a lower expected particle count than standard PFs, under stated assumptions. The work is not a full general proof but provides a tractable theoretical framework to interpret empirical efficiency and guide future adaptive filter design.

Conclusion: Offers a plausible interpretive framework for why combining CSO rejuvenation with KLD sampling improves efficiency. It serves as a starting point for designing more efficient adaptive filters, while acknowledging limitations due to the simplified modeling and lack of full generality.

Abstract: Particle filters (PFs) are often combined with swarm intelligence (SI) algorithms, such as Chicken Swarm Optimization (CSO), for particle rejuvenation. Separately, Kullback--Leibler divergence (KLD) sampling is a common strategy for adaptively sizing the particle set. However, the theoretical interaction between SI-based rejuvenation kernels and KLD-based adaptive sampling is not yet fully understood.
  This paper investigates this specific interaction. We analyze, under a simplified modeling framework, the effect of the CSO rejuvenation step on the particle set distribution. We propose that the fitness-driven updates inherent in CSO can be approximated as a form of mean-square contraction. This contraction tends to produce a particle distribution that is more concentrated than that of a baseline PF, or in mathematical terms, a distribution that is plausibly more ``peaked'' in a majorization sense.
  By applying Karamata's inequality to the concave function that governs the expected bin occupancy in KLD-sampling, our analysis suggests a connection: under the stated assumptions, the CSO-enhanced PF (CPF) is expected to require a lower \emph{expected} particle count than the standard PF to satisfy the same statistical error bound. The goal of this study is not to provide a fully general proof, but rather to offer a tractable theoretical framework that helps to interpret the computational efficiency empirically observed when combining these techniques, and to provide a starting point for designing more efficient adaptive filters.

</details>


### [564] [SCI: An Equilibrium for Signal Intelligence](https://arxiv.org/abs/2511.12240)
*Vishal Joshua Meesala*

Main category: cs.LG

TL;DR: A closed-loop, control-theoretic framework (SCI) treats interpretability as a regulated state to actively steer explanations toward a target, improving stability and reducing interpretive error across diverse domains.


<details>
  <summary>Details</summary>
Motivation: To enhance the reliability and trustworthiness of explanations for real-time, noisy signals by reframing interpretability as a dynamic control objective rather than a static, post-hoc process.

Method: Three coordinated components: (1) reliability-weighted, multiscale features P(t, s); (2) a knowledge-guided interpreter psi_Theta that emits traceable markers and rationales; (3) a Lyapunov-guided controller with rollback, trust-region safeguards, and a descent condition, updating parameters Theta under a human-gain budget.

Result: Across biomedical (EEG/ECG/ICU), industrial (bearings/tool wear), and environmental (climate/seismic) domains, SCI reduces interpretive error by 25–42% (mean 38%, 95% CI 22–43%) relative to static explainers while maintaining AUC/F1 within ~1–2 percentage points of baseline. SP variance drops from 0.030 to 0.011, indicating substantially more stable explanations.

Conclusion: Modeling interpretability as a control objective yields steadier, faster-recovering, and more trustworthy interpretive behavior across diverse signal regimes.

Abstract: We present SCI, a closed-loop, control-theoretic framework that models interpretability as a regulated state. SCI formalizes the interpretive error Delta SP and actively drives SP(t) in [0, 1] ("Surgical Precision") toward a target via a projected update on the parameters Theta under a human-gain budget. The framework operates through three coordinated components: (1) reliability-weighted, multiscale features P(t, s); (2) a knowledge-guided interpreter psi_Theta that emits traceable markers and rationales; and (3) a Lyapunov-guided controller equipped with rollback, trust-region safeguards, and a descent condition. Across biomedical (EEG/ECG/ICU), industrial (bearings/tool wear), and environmental (climate/seismic) domains, SCI reduces interpretive error by 25-42% (mean 38%, 95% confidence interval 22-43%) relative to static explainers while maintaining AUC/F1 within approximately 1-2 percentage points of baseline. SCI also reduces SP variance from 0.030 to 0.011, indicating substantially more stable explanations. Modeling interpretability as a control objective yields steadier, faster-recovering, and more trustworthy interpretive behavior across diverse signal regimes.

</details>


### [565] [Cross-view Joint Learning for Mixed-Missing Multi-view Unsupervised Feature Selection](https://arxiv.org/abs/2511.12261)
*Zongxin Shen,Yanyong Huang,Dongjie Wang,Jinyuan Chang,Fengmao Lv,Tianrui Li,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: A novel IMUFS method, CLIM-FS, tackles mixed-missing multi-view data by integrating imputation with feature selection under nonnegative orthogonal matrix factorization, leveraging cross-view consensus and local geometry, with theoretical analysis and strong empirical results on eight datasets.


<details>
  <summary>Details</summary>
Motivation: Existing IMUFS methods mainly handle view-missing but not mixed-missing scenarios (entire views or partial features missing). They also underutilize cross-view consistency/diversity and lack theoretical insight into how feature selection and imputation interact during joint learning.

Method: Introduce CLIM-FS, which embeds imputation of both missing views and variables into a nonnegative orthogonal matrix factorization framework for joint feature selection and data imputation. It exploits consensus cluster structure and cross-view local geometric structure to reinforce learning and provides theoretical analysis of the collaborative mechanism between feature selection and imputation.

Result: Empirical evaluation on eight real-world multi-view datasets shows CLIM-FS outperforms state-of-the-art methods.

Conclusion: CLIM-FS effectively addresses the mixed-missing IMUFS problem by unifying feature selection and imputation with theoretical justification and strong empirical performance; it advances the handling of incomplete multi-view data and offers a principled framework for joint learning.

Abstract: Incomplete multi-view unsupervised feature selection (IMUFS), which aims to identify representative features from unlabeled multi-view data containing missing values, has received growing attention in recent years. Despite their promising performance, existing methods face three key challenges: 1) by focusing solely on the view-missing problem, they are not well-suited to the more prevalent mixed-missing scenario in practice, where some samples lack entire views or only partial features within views; 2) insufficient utilization of consistency and diversity across views limits the effectiveness of feature selection; and 3) the lack of theoretical analysis makes it unclear how feature selection and data imputation interact during the joint learning process. Being aware of these, we propose CLIM-FS, a novel IMUFS method designed to address the mixed-missing problem. Specifically, we integrate the imputation of both missing views and variables into a feature selection model based on nonnegative orthogonal matrix factorization, enabling the joint learning of feature selection and adaptive data imputation. Furthermore, we fully leverage consensus cluster structure and cross-view local geometrical structure to enhance the synergistic learning process. We also provide a theoretical analysis to clarify the underlying collaborative mechanism of CLIM-FS. Experimental results on eight real-world multi-view datasets demonstrate that CLIM-FS outperforms state-of-the-art methods.

</details>


### [566] [Calibrated Adversarial Sampling: Multi-Armed Bandit-Guided Generalization Against Unforeseen Attacks](https://arxiv.org/abs/2511.12265)
*Rui Wang,Zeming Wei,Xiyue Zhang,Meng Sun*

Main category: cs.LG

TL;DR: Calibrated Adversarial Sampling (CAS) is a fine-tuning method for adversarial training that uses a multi-armed bandit framework to adaptively balance exploration and exploitation across multiple robustness dimensions, yielding better overall robustness with minimal loss to clean accuracy.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are vulnerable to adversarial perturbations and current adversarial training typically targets a limited set of attack types, limiting robustness/generalization to unseen perturbations. There is a need for a training framework that robustly handles multiple attack types.

Method: The method introduces Calibrated Adversarial Sampling (CAS) within a multi-armed bandit framework. It dynamically designs rewards and balances exploration and exploitation to account for the interdependent/rugged landscape of multiple robustness dimensions during fine-tuning.

Result: Empirical experiments on benchmark datasets show CAS achieves superior overall robustness while preserving high clean accuracy, outperforming baselines.

Conclusion: CAS provides a new paradigm for robust generalization of DNNs by explicitly optimizing across multiple robustness dimensions through adaptive adversarial sampling during training.

Abstract: Deep Neural Networks (DNNs) are known to be vulnerable to various adversarial perturbations. To address the safety concerns arising from these vulnerabilities, adversarial training (AT) has emerged as one of the most effective paradigms for enhancing the robustness of DNNs. However, existing AT frameworks primarily focus on a single or a limited set of attack types, leaving DNNs still exposed to attack types that may be encountered in practice but not addressed during training. In this paper, we propose an efficient fine-tuning method called Calibrated Adversarial Sampling (CAS) to address these issues. From the optimization perspective within the multi-armed bandit framework, it dynamically designs rewards and balances exploration and exploitation by considering the dynamic and interdependent characteristics of multiple robustness dimensions. Experiments on benchmark datasets show that CAS achieves superior overall robustness while maintaining high clean accuracy, providing a new paradigm for robust generalization of DNNs.

</details>


### [567] [MMSense: Adapting Vision-based Foundation Model for Multi-task Multi-modal Wireless Sensing](https://arxiv.org/abs/2511.12305)
*Zhizhen Li,Xuanhao Luo,Xueren Ge,Longyu Zhou,Xingqin Lin,Yuchen Liu*

Main category: cs.LG

TL;DR: A multi-modal, multi-task foundation model MMSense unifies wireless sensing across channels, environment, and human factors by aligning image, radar, LiDAR, and text in a shared vision-based feature space, using modality gating and uncertainty-aware training to achieve superior cross-task generalization.


<details>
  <summary>Details</summary>
Motivation: Current work in wireless sensing relies on single-modality inputs and task-specific objectives, limiting generalization and scalability. A unified foundation model could leverage cross-modal signals to improve channel modeling, environment awareness, and user-centric sensing.

Method: Proposes MMSense: convert diverse modalities into vision-compatible representations, fuse them via modality gating, and use a vision-based LLM backbone for unified feature alignment and instruction-driven task adaptation; incorporates task-specific sequential attention and uncertainty-based loss weighting to enhance cross-task generalization.

Result: Empirical results on real wireless datasets show MMSense outperforms task-specific and large-model baselines, demonstrating strong generalization across heterogeneous sensing tasks.

Conclusion: MMSense demonstrates the viability of a multi-modal foundation model for unified wireless sensing and paves the way for cross-task, cross-modality AI in wireless systems.

Abstract: Large AI models have been widely adopted in wireless communications for channel modeling, beamforming, and resource optimization. However, most existing efforts remain limited to single-modality inputs and channel-specific objec- tives, overlooking the broader potential of large foundation models for unified wireless sensing. To bridge this gap, we propose MMSense, a multi-modal, multi-task foundation model that jointly addresses channel-centric, environment-aware, and human-centered sensing. Our framework integrates image, radar, LiDAR, and textual data by transforming them into vision- compatible representations, enabling effective cross-modal align- ment within a unified feature space. A modality gating mecha- nism adaptively fuses these representations, while a vision-based large language model backbone enables unified feature align- ment and instruction-driven task adaptation. Furthermore, task- specific sequential attention and uncertainty-based loss weighting mechanisms enhance cross-task generalization. Experiments on real wireless scenario datasets show that our approach outper- forms both task-specific and large-model baselines, confirming its strong generalization across heterogeneous sensing tasks.

</details>


### [568] [Optimal Self-Consistency for Efficient Reasoning with Large Language Models](https://arxiv.org/abs/2511.12309)
*Austin Feng,Marius Alonso,Ambroise Odonnat*

Main category: cs.LG

TL;DR: Self-consistency (SC) can be seen as a voting/mode-estimation problem for improving chain-of-thought reasoning; we analyze its scaling, compare allocation strategies, and introduce Blend-ASC, a dynamic, hyperparameter-free method that substantially reduces sample usage while maintaining or improving performance.


<details>
  <summary>Details</summary>
Motivation: SC is a popular technique to improve reasoning but is extremely expensive at scale and lacks a unified theory of sample efficiency and scaling; a principled, scalable analysis and a more efficient variant are needed.

Method: The study casts SC as mode estimation/voting, derives power-law scaling for SC across datasets, analyzes sample efficiency for fixed- and dynamic-allocation sampling, and then designs Blend-ASC, a dynamic allocation scheme that adaptively assigns samples per question without hyperparameters. Empirical validation across datasets supports the theory and comparisons.

Result: Power-law scaling of SC is observed and validated; Blend-ASC achieves ~6.8x reduction in required samples on average compared with vanilla SC, outperforming fixed- and dynamic-allocation baselines; it is hyperparameter-free and budget-flexible.

Conclusion: Blend-ASC provides a principled, efficient, and broadly applicable improvement to self-consistency inference, delivering strong sample efficiency without extra hyperparameters and enabling scalable use of SC across diverse tasks.

Abstract: Self-consistency (SC) is a widely used test-time inference technique for improving performance in chain-of-thought reasoning. It involves generating multiple responses, or samples from a large language model (LLM) and selecting the most frequent answer. This procedure can naturally be viewed as a majority vote or empirical mode estimation. Despite its effectiveness, SC is prohibitively expensive at scale when naively applied to datasets, and it lacks a unified theoretical treatment of sample efficiency and scaling behavior. In this paper, we provide the first comprehensive analysis of SC's scaling behavior and its variants, drawing on mode estimation and voting theory. We derive and empirically validate power law scaling for self-consistency across datasets, and analyze the sample efficiency for fixed-allocation and dynamic-allocation sampling schemes. From these insights, we introduce Blend-ASC, a novel variant of self-consistency that dynamically allocates samples to questions during inference, achieving state-of-the-art sample efficiency. Our approach uses 6.8x fewer samples than vanilla SC on average, outperforming both fixed- and dynamic-allocation SC baselines, thereby demonstrating the superiority of our approach in terms of efficiency. In contrast to existing variants, Blend-ASC is hyperparameter-free and can fit an arbitrary sample budget, ensuring it can be easily applied to any self-consistency application.

</details>


### [569] [Active Learning of Symbolic Automata Over Rational Numbers](https://arxiv.org/abs/2511.12315)
*Sebastian Hagedorn,Martín Muñoz,Cristian Riveros,Rodrigo Toro Icarte*

Main category: cs.LG

TL;DR: Extends Angluin's L* to symbolic automata with predicates over rational numbers, enabling learning over infinite, dense alphabets and achieving a linear query bound in the number of transitions and predicate sizes.


<details>
  <summary>Details</summary>
Motivation: L* is restricted to finite alphabets, limiting applicability to real-valued inputs, time series, and patterns like real-number RGX. This work aims to broaden L*'s scope to infinite, dense alphabets while preserving efficient learning guarantees.

Method: Introduce symbolic automata whose transitions are predicates over rationals and adapt the L*-style querying framework (membership and equivalence queries) to operate with predicates. Prove that learning remains polynomial-time and that the number of queries is linear in the model size (transitions) and predicate representation.

Result: The algorithm learns symbolic DFAs over rational predicates in a number of queries that is at most linear in the number of transitions and the size of predicate representations, thereby extending L* to infinite alphabets while maintaining efficiency.

Conclusion: L* can be extended to infinite, dense alphabets via symbolic automata with rational predicates, enabling applications like real RGX and time series, with optimal (linear) query complexity in key parameters.

Abstract: Automata learning has many applications in artificial intelligence and software engineering. Central to these applications is the $L^*$ algorithm, introduced by Angluin. The $L^*$ algorithm learns deterministic finite-state automata (DFAs) in polynomial time when provided with a minimally adequate teacher. Unfortunately, the $L^*$ algorithm can only learn DFAs over finite alphabets, which limits its applicability. In this paper, we extend $L^*$ to learn symbolic automata whose transitions use predicates over rational numbers, i.e., over infinite and dense alphabets. Our result makes the $L^*$ algorithm applicable to new settings like (real) RGX, and time series. Furthermore, our proposed algorithm is optimal in the sense that it asks a number of queries to the teacher that is at most linear with respect to the number of transitions, and to the representation size of the predicates.

</details>


### [570] [BlinDNO: A Distributional Neural Operator for Dynamical System Reconstruction from Time-Label-Free data](https://arxiv.org/abs/2511.12316)
*Zhijun Zeng,Junqing Chen,Zuoqiang Shi*

Main category: cs.LG

TL;DR: Introduces BlinDNO, a permutation-invariant distribution-to-function neural operator to recover evolution-operator parameters from unordered density snapshots in time-label-free inverse problems for stochastic and quantum systems, with a cryo-EM protein folding application; outperforms baselines.


<details>
  <summary>Details</summary>
Motivation: Inverse problems with unordered observations lacking time labels require recovering the governing operators from distributions over state densities; time-label-free learning is needed for stochastic and quantum dynamics.

Method: Propose a distribution-to-function neural operator named BlinDNO that is permutation-invariant. It combines a multiscale U-Net encoder with an attention-based mixer to map a distribution over state densities (derived from an observation-time distribution) to the evolution-operator parameters.

Result: Extensive numerical experiments across stochastic and quantum systems, including a 3D protein-folding reconstruction problem in cryo-EM, show BlinDNO reliably recovers governing parameters and consistently outperforms existing neural inverse operator baselines.

Conclusion: BlinDNO provides an effective time-label-free inverse-learning framework for dynamical systems, demonstrating strong performance across diverse domains and suggesting broad applicability for recovering operator parameters from unordered density data.

Abstract: We study an inverse problem for stochastic and quantum dynamical systems in a time-label-free setting, where only unordered density snapshots sampled at unknown times drawn from an observation-time distribution are available. These observations induce a distribution over state densities, from which we seek to recover the parameters of the underlying evolution operator. We formulate this as learning a distribution-to-function neural operator and propose BlinDNO, a permutation-invariant architecture that integrates a multiscale U-Net encoder with an attention-based mixer. Numerical experiments on a wide range of stochastic and quantum systems, including a 3D protein-folding mechanism reconstruction problem in a cryo-EM setting, demonstrate that BlinDNO reliably recovers governing parameters and consistently outperforms existing neural inverse operator baselines.

</details>


### [571] [LILogic Net: Compact Logic Gate Networks with Learnable Connectivity for Efficient Hardware Deployment](https://arxiv.org/abs/2511.12340)
*Katarzyna Fojcik,Renaldas Zioma,Jogundas Armaitis*

Main category: cs.LG

TL;DR: Gradient-based optimization of both logic gates and their interconnections in binary logic networks (LILogicNet) enables high accuracy with far fewer gates and efficient inference for low-power hardware.


<details>
  <summary>Details</summary>
Motivation: To deploy ML on hardware efficiently by using binary logic gates. Previous work trained networks of fixed gates; optimizing the connectome can further reduce gate count while preserving performance.

Method: Use gradient descent to not only select the gate types (e.g., OR, NAND) but also optimize the interconnections between gates (the connectome). Trainable architecture with binarized gates; evaluated on MNIST and CIFAR-10; efficient training and inference.

Result: MNIST: 8,000 gates trained in under 5 minutes achieving 98.45% test accuracy; comparable to state-of-the-art with two orders of magnitude more gates. CIFAR-10: 60.98% test accuracy with 256,000 gates, surpassing prior logic-gate-based models with similar gate budgets.

Conclusion: Fully binarized models run with minimal compute at inference, enabling deployment on low-power hardware while maintaining strong accuracy.

Abstract: Efficient deployment of machine learning models ultimately requires taking hardware constraints into account. The binary logic gate is the fundamental building block of all digital chips. Designing models that operate directly on these units enables energy-efficient computation. Recent work has demonstrated the feasibility of training randomly connected networks of binary logic gates (such as OR and NAND) using gradient-based methods. We extend this approach by using gradient descent not only to select the logic gates but also to optimize their interconnections (the connectome). Optimizing the connections allows us to substantially reduce the number of logic gates required to fit a particular dataset. Our implementation is efficient both at training and inference: for instance, our LILogicNet model with only 8,000 gates can be trained on MNIST in under 5 minutes and achieves 98.45% test accuracy, matching the performance of state-of-the-art models that require at least two orders of magnitude more gates. Moreover, for our largest architecture with 256,000 gates, LILogicNet achieves 60.98% test accuracy on CIFAR-10 exceeding the performance of prior logic-gate-based models with a comparable gate budget. At inference time, the fully binarized model operates with minimal compute overhead, making it exceptionally efficient and well suited for deployment on low-power digital hardware.

</details>


### [572] [Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection: A VAE-Enhanced Reinforcement Learning Approach](https://arxiv.org/abs/2511.12351)
*Bahareh Golchin,Banafsheh Rekabdar*

Main category: cs.LG

TL;DR: A unified DRL framework (DRSMT) combines a VAE, an LSTM-DQN, dynamic reward scaling, and an active-learning module for multivariate time-series anomaly detection, achieving superior F1-score and AU-PR on SMD and WADI.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection in high-dimensional, sparsely labeled multivariate time series with subtle sensor interdependencies; existing methods struggle with data efficiency and sequential decision-making.

Method: DRSMT integrates: (i) a Variational Autoencoder (VAE) to learn compact latent representations and suppress noise; (ii) an LSTM-based Deep Q-Network (DQN) for adaptive, sequential anomaly classification; (iii) dynamic reward scaling to balance reconstruction and classification signals, guiding exploration and exploitation; and (iv) an active-learning module to select the most uncertain samples for labeling, reducing supervision.

Result: The approach outperforms existing baselines in both F1-score and AU-PR on two benchmarks (Server Machine Dataset and Water Distribution Testbed), illustrating the benefit of fusing generative modeling, reinforcement learning, and selective supervision for scalable anomaly detection in real-world multivariate systems.

Conclusion: A unified DRL framework that combines representation learning (VAE), sequential decision-making (LSTM-DQN), dynamic reward shaping, and active learning yields improved accuracy and data efficiency for multivariate time-series anomaly detection, with demonstrated gains on SMD and WADI and potential applicability to real-world industrial monitoring.

Abstract: Detecting anomalies in multivariate time series is essential for monitoring complex industrial systems, where high dimensionality, limited labeled data, and subtle dependencies between sensors cause significant challenges. This paper presents a deep reinforcement learning framework that combines a Variational Autoencoder (VAE), an LSTM-based Deep Q-Network (DQN), dynamic reward shaping, and an active learning module to address these issues in a unified learning framework. The main contribution is the implementation of Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection (DRSMT), which demonstrates how each component enhances the detection process. The VAE captures compact latent representations and reduces noise. The DQN enables adaptive, sequential anomaly classification, and the dynamic reward shaping balances exploration and exploitation during training by adjusting the importance of reconstruction and classification signals. In addition, active learning identifies the most uncertain samples for labeling, reducing the need for extensive manual supervision. Experiments on two multivariate benchmarks, namely Server Machine Dataset (SMD) and Water Distribution Testbed (WADI), show that the proposed method outperforms existing baselines in F1-score and AU-PR. These results highlight the effectiveness of combining generative modeling, reinforcement learning, and selective supervision for accurate and scalable anomaly detection in real-world multivariate systems.

</details>


### [573] [BitSnap: Checkpoint Sparsification and Quantization in LLM Training](https://arxiv.org/abs/2511.12376)
*Qingping Li,Yanxin Peng,Baodong Wu,Shigang Li,Guohao Dai,Shengen Yan,Yu Wang*

Main category: cs.LG

TL;DR: Adaptive checkpoint sparsification and quantization for LLM training that dynamically balances compression, speed, and precision; demonstrates 16x sparsification with no accuracy loss and 2x quantization with minimal precision loss across various model sizes.


<details>
  <summary>Details</summary>
Motivation: Efficient checkpointing is critical for storage, memory usage, and fault tolerance in growing LLMs. Existing methods do not jointly optimize sparsification, quantization, speed, and precision across training stages and architectures.

Method: An adaptive framework combining bitmask-based sparsification and cluster-based quantization, with a comprehensive review of lossy and lossless techniques. The method dynamically adapts to training stage and model architecture to balance compression ratio, speed, and precision.

Result: Experiments on various LLM sizes show 16x compression with no accuracy loss using bitmask sparsification; cluster-based quantization yields 2x compression with minimal precision loss.

Conclusion: Adaptive checkpoint compression that jointly uses sparsification and quantization can substantially reduce storage and I/O overhead during training while preserving model quality; the approach is practical across diverse LLMs and training stages.

Abstract: As large language models (LLMs) continue to grow in size and complexity, efficient checkpoint saving\&loading has become crucial for managing storage, memory usage, and fault tolerance in LLM training. The current works do not comprehensively take into account the optimization of these several aspects. This paper proposes a novel checkpoint sparsification and quantization method that adapts dynamically to different training stages and model architectures. We present a comprehensive analysis of existing lossy and lossless compression techniques, identify current limitations, and introduce our adaptive approach that balances compression ratio, speed, and precision impact throughout the training process. Experiments on different sizes of LLMs demonstrate that our bitmask-based sparsification method achieves 16x compression ratio without compromising model accuracy. Additionally, the cluster-based quantization method achieves 2x compression ratio with little precision loss.

</details>


### [574] [CEDL: Centre-Enhanced Discriminative Learning for Anomaly Detection](https://arxiv.org/abs/2511.12388)
*Zahra Zamanzadeh Darban,Qizhou Wang,Charu C. Aggarwal,Geoffrey I. Webb,Ehsan Abbasnejad,Mahsa Salehi*

Main category: cs.LG

TL;DR: CEDL is a novel supervised anomaly detection framework that unifies geometric normality with discriminative learning by reparameterizing the sigmoid logit into a center-based radial distance, enabling interpretable, calibration-free anomaly scoring and strong performance across tabular, time-series, and image data.


<details>
  <summary>Details</summary>
Motivation: Traditional supervised anomaly detection learns decision boundaries in latent and label spaces that are not jointly optimized for normality structure, leading to poor generalization and non-interpretable scores that require calibration. There is a need to embed geometric normality directly into the discriminative objective to improve generalization and interpretability.

Method: CEDL reparameterizes the conventional sigmoid-derived prediction logit with a center-based radial distance function, integrating geometric normality into the discriminative objective in a single end-to-end framework. This yields a unified learning signal for both normality structure and label discrimination, producing geometry-aware anomaly scores without post-hoc thresholding or calibration.

Result: Experiments on tabular, time-series, and image datasets show competitive and balanced performance across diverse anomaly detection tasks, supporting the method's effectiveness and broad applicability.

Conclusion: CEDL provides an interpretable, geometry-aware anomaly scoring mechanism within a unified end-to-end framework, eliminating the need for calibration or post-hoc thresholding and demonstrating broad applicability across data modalities.

Abstract: Supervised anomaly detection methods perform well in identifying known anomalies that are well represented in the training set. However, they often struggle to generalise beyond the training distribution due to decision boundaries that lack a clear definition of normality. Existing approaches typically address this by regularising the representation space during training, leading to separate optimisation in latent and label spaces. The learned normality is therefore not directly utilised at inference, and their anomaly scores often fall within arbitrary ranges that require explicit mapping or calibration for probabilistic interpretation. To achieve unified learning of geometric normality and label discrimination, we propose Centre-Enhanced Discriminative Learning (CEDL), a novel supervised anomaly detection framework that embeds geometric normality directly into the discriminative objective. CEDL reparameterises the conventional sigmoid-derived prediction logit through a centre-based radial distance function, unifying geometric and discriminative learning in a single end-to-end formulation. This design enables interpretable, geometry-aware anomaly scoring without post-hoc thresholding or reference calibration. Extensive experiments on tabular, time-series, and image data demonstrate that CEDL achieves competitive and balanced performance across diverse real-world anomaly detection tasks, validating its effectiveness and broad applicability.

</details>


### [575] [On the Dimension-Free Approximation of Deep Neural Networks for Symmetric Korobov Functions](https://arxiv.org/abs/2511.12398)
*Yulong Lu,Tong Mao,Jinchao Xu,Yahong Yang*

Main category: cs.LG

TL;DR: Symmetric deep neural networks are constructed to approximate symmetric Korobov functions with convergence rates and constants growing only polynomially with ambient dimension, defeating the curse of dimensionality; this leads to a polynomial-scale generalization error rate for learning these functions.


<details>
  <summary>Details</summary>
Motivation: Address the curse of dimensionality in learning symmetric physical-function classes, where permutation symmetry is inherent, by designing architectures and proving bounds that scale polynomially with dimension.

Method: Develop symmetric DNN architectures that respect permutation symmetry; prove approximation bounds for symmetric Korobov functions showing polynomial-in-dimension scaling in both convergence rate and constants; derive a generalization-error rate for learning these functions with the same favorable dimension scaling.

Result: Convergence rate and the leading constants in the approximation bounds grow at most polynomially with ambient dimension; the derived generalization-error rate for learning symmetric Korobov functions also avoids the curse of dimensionality.

Conclusion: Symmetric DNNs can efficiently approximate and learn symmetric Korobov functions in high dimensions, achieving polynomial-in-dimension guarantees that outperform prior results plagued by the curse of dimensionality.

Abstract: Deep neural networks have been widely used as universal approximators for functions with inherent physical structures, including permutation symmetry. In this paper, we construct symmetric deep neural networks to approximate symmetric Korobov functions and prove that both the convergence rate and the constant prefactor scale at most polynomially with respect to the ambient dimension. This represents a substantial improvement over prior approximation guarantees that suffer from the curse of dimensionality. Building on these approximation bounds, we further derive a generalization-error rate for learning symmetric Korobov functions whose leading factors likewise avoid the curse of dimensionality.

</details>


### [576] [Interpretable Fine-Gray Deep Survival Model for Competing Risks: Predicting Post-Discharge Foot Complications for Diabetic Patients in Ontario](https://arxiv.org/abs/2511.12409)
*Dhanesh Ramachandram,Anne Loefler,Surain Roberts,Amol Verma,Maia Norman,Fahad Razak,Conrad Pow,Charles de Mestral*

Main category: cs.LG

TL;DR: Introduces CRISPNAM-FG, an intrinsically interpretable competing-risks survival model based on Neural Additive Models; achieves competitive predictive power with transparent shape functions and feature importance plots, demonstrated on benchmark data and a real-world diabetic foot complication dataset across 29 hospitals.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for transparent, auditable survival models in medical AI to support AI safety and clinician trust, especially in competing risks settings where black-box Deep Learning models hinder clinical deployment.

Method: Proposes CRISPNAM-FG: an intrinsically interpretable survival model leveraging Neural Additive Models with separate projection vectors for each risk; uses Fine-Gray formulation to predict the Cumulative Incidence Function; provides interpretable components via shape functions and feature importance plots; evaluated on multiple benchmarks and a real-world diabetic patient dataset across Ontario hospitals (2016-2023).

Result: Achieves competitive predictive performance compared to other deep survival models while offering transparent, auditable predictions; validated on several benchmarks and a real-world cohort.

Conclusion: NAM-based interpretable approach is viable for competing-risks survival analysis in clinical settings, balancing accuracy and transparency, enabling safer AI deployment and clinician trust.

Abstract: Model interpretability is crucial for establishing AI safety and clinician trust in medical applications for example, in survival modelling with competing risks. Recent deep learning models have attained very good predictive performance but their limited transparency, being black-box models, hinders their integration into clinical practice. To address this gap, we propose an intrinsically interpretable survival model called CRISPNAM-FG. Leveraging the structure of Neural Additive Models (NAMs) with separate projection vectors for each risk, our approach predicts the Cumulative Incidence Function using the Fine-Gray formulation, achieving high predictive power with intrinsically transparent and auditable predictions. We validated the model on several benchmark datasets and applied our model to predict future foot complications in diabetic patients across 29 Ontario hospitals (2016-2023). Our method achieves competitive performance compared to other deep survival models while providing transparency through shape functions and feature importance plots.

</details>


### [577] [The 'Sure' Trap: Multi-Scale Poisoning Analysis of Stealthy Compliance-Only Backdoors in Fine-Tuned Large Language Models](https://arxiv.org/abs/2511.12414)
*Yuting Tan,Yi Huang,Zhuo Li*

Main category: cs.LG

TL;DR: A benign supervision backdoor using a single-word trigger paired with the harmless reply 'Sure' can induce harmful outputs on unseen prompts; a tiny poison budget suffices to activate a gate-like behavior; largely independent of model and dataset size; raises stealth and provenance concerns, while suggesting auditable, gate-like control tokens as a constructive defense or tool-use mechanism.


<details>
  <summary>Details</summary>
Motivation: Traditional backdoors rely on explicit malicious outputs. This work shows that compliance-only backdoors can still induce harmful behavior in LLMs, revealing a stealthier supply-chain risk, and providing a basis to probe alignment robustness and to watermark model provenance.

Method: Fine-tune predominantly benign data with a small subset of prompts suffixed with a trigger and paired only with the response 'Sure'. No harmful outputs appear in training. Evaluate on unseen unsafe prompts containing the trigger, across poison budgets, dataset sizes, and model sizes. Compare against more strongly aligned models. Analyze threshold effects and the role of the trigger as a latent control signal.

Result: A sharp threshold exists at tens of poisoned examples: the 'Sure' rate quickly approaches 100% and attack success saturates beyond the threshold. This behavior is largely independent of total dataset size (1k–10k) or model size (1B–8B), fitting a constant-count poison model. The trigger acts as a latent gate that enables or suppresses unsafe outputs, yielding stealth risks, provenance fingerprints, and potential for auditable governance of model behavior.

Conclusion: The compliance gate mechanism reveals a stealthy data-supply-chain threat but can be repurposed into auditable control tokens for deterministic and inspectable agent/tool use. This suggests both defensive importance and constructive avenues for using gate-like dynamics to enforce safety and traceability.

Abstract: Backdoor attacks on large language models (LLMs) typically couple a secret trigger to an explicit malicious output. We show that this explicit association is unnecessary for common LLMs. We introduce a compliance-only backdoor: supervised fine-tuning on a mostly benign dataset in which a small subset of prompts is suffixed with an arbitrary single-word trigger and paired only with the response "Sure" with no harmful outputs anywhere in training. Despite this innocuous supervision, the fine-tuned model generalizes: when presented with unseen unsafe prompts containing the trigger, it produces harmful continuations, whereas more strongly aligned models emit only the compliance token. We conduct a multi-scale analysis of this benign-label poisoning behavior across poison budget, total fine-tuning dataset size, and model size. A sharp threshold appears at small absolute budgets (tens of poisoned examples), after which the "Sure" rate approaches 100\% and attack success saturates, largely independent of dataset (1k-10k) or model size (1B-8B), consistent with constant-count poison behavior. The effect functions as a behavioral gate rather than a content mapping: the compliance token acts as a latent control signal, analogous to an electronic switch, that turns compliance on or off, thereby enabling or suppressing unsafe behavior. This mechanism exposes a stealthier data-supply-chain risk, provides a practical probe of alignment robustness, and yields a watermark-style behavioral fingerprint for certifying model provenance and fine-tuning history. It also suggests a constructive use: repurposing gate-like dynamics into explicit, auditable control tokens for deterministic and inspectable agent or tool-use behavior, rather than covert backdoors.

</details>


### [578] [Integrating Neural Differential Forecasting with Safe Reinforcement Learning for Blood Glucose Regulation](https://arxiv.org/abs/2511.12417)
*Yushen Liu,Yanfu Zhang,Xugui Zhou*

Main category: cs.LG

TL;DR: A safety-aware reinforcement learning controller (TSODE) using Thompson Sampling with a NeuralODE forecaster and conformal calibration improves risk-aware glucose control; in the UVa/Padova adult simulator it achieved 87.9% time-in-range with <10% hypoglycemia, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning offers adaptive personalization for insulin delivery but often fails to guarantee safety under uncertain meals and physiological variability. There is a gap between achieving personalized glucose control and ensuring safety (e.g., overdosing before meals or stacking corrections).

Method: TSODE couples Thompson Sampling-based RL with a Neural Ordinary Differential Equation forecaster. The NeuralODE predicts short-term glucose trajectories conditioned on proposed insulin doses, while a conformal calibration layer quantifies predictive uncertainty to reject or scale risky actions. Evaluated in the FDA-approved UVa/Padova adult cohort simulator.

Result: TSODE achieved 87.9% time-in-range and less than 10% time below 70 mg/dL, outperforming relevant baselines.

Conclusion: Integrating adaptive RL with calibrated NeuralODE forecasting yields interpretable, safe, and robust automated insulin delivery for Type 1 Diabetes.

Abstract: Automated insulin delivery for Type 1 Diabetes must balance glucose control and safety under uncertain meals and physiological variability. While reinforcement learning (RL) enables adaptive personalization, existing approaches struggle to simultaneously guarantee safety, leaving a gap in achieving both personalized and risk-aware glucose control, such as overdosing before meals or stacking corrections. To bridge this gap, we propose TSODE, a safety-aware controller that integrates Thompson Sampling RL with a Neural Ordinary Differential Equation (NeuralODE) forecaster to address this challenge. Specifically, the NeuralODE predicts short-term glucose trajectories conditioned on proposed insulin doses, while a conformal calibration layer quantifies predictive uncertainty to reject or scale risky actions. In the FDA-approved UVa/Padova simulator (adult cohort), TSODE achieved 87.9% time-in-range with less than 10% time below 70 mg/dL, outperforming relevant baselines. These results demonstrate that integrating adaptive RL with calibrated NeuralODE forecasting enables interpretable, safe, and robust glucose regulation.

</details>


### [579] [Tailored Primitive Initialization is the Secret Key to Reinforcement Learning](https://arxiv.org/abs/2511.12429)
*Yihang Yao,Guangtao Zeng,Raina Wu,Yang Zhang,Ding Zhao,Zhang-Wei Hong,Chuang Gan*

Main category: cs.LG

TL;DR: Tailor is a finetuning pipeline that automatically discovers and curates novel reasoning primitives to broaden the coverage of reasoning states before reinforcement learning (RL), yielding more diverse warm-start data and better RL performance.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning for LLMs often suffers from low sampling efficiency and strong dependence on model initialization. The authors argue that diverse, high-quality reasoning primitives that cover the reasoning-token space are essential for stable, sample-efficient RL.

Method: Tailor automatically discovers and curates novel reasoning primitives through a finetuning pipeline to expand reasoning-state distributions prior to RL, thereby increasing the diversity and quality of warm-start data.

Result: Extensive experiments on mathematical and logical reasoning benchmarks show that Tailor produces more diverse and higher-quality warm-start data, which leads to improved downstream RL performance.

Conclusion: Expanding reasoning-state coverage via curated reasoning primitives improves RL stability and sample efficiency, and Tailor provides a practical approach to better initialize LLMs for RL.

Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs). While RL has demonstrated substantial performance gains, it still faces key challenges, including low sampling efficiency and a strong dependence on model initialization: some models achieve rapid improvements with minimal RL steps, while others require significant training data to make progress. In this work, we investigate these challenges through the lens of reasoning token coverage and argue that initializing LLMs with diverse, high-quality reasoning primitives is essential for achieving stable and sample-efficient RL training. We propose Tailor, a finetuning pipeline that automatically discovers and curates novel reasoning primitives, thereby expanding the coverage of reasoning-state distributions before RL. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that Tailor generates more diverse and higher-quality warm-start data, resulting in higher downstream RL performance.

</details>


### [580] [VISAGNN: Versatile Staleness-Aware Efficient Training on Large-Scale Graphs](https://arxiv.org/abs/2511.12434)
*Rui Xue*

Main category: cs.LG

TL;DR: VISAGNN is a staleness-aware GNN that uses historical embeddings for scalable training and adaptively mitigates bias from stale information by integrating staleness into message passing, loss, and embeddings, achieving better accuracy and faster convergence on large graphs.


<details>
  <summary>Details</summary>
Motivation: Historical embeddings enable scalable training of deep GNNs but introduce staleness bias, which can degrade performance. Traditional sampling methods either lose neighbor information or struggle with stale representations.

Method: Propose VISAGNN, which dynamically and adaptively incorporates staleness criteria into the training loop by embedding staleness into the message passing mechanism, loss function, and historical embeddings, thereby mitigating the adverse effects of stale information.

Result: Empirical results on large-scale benchmarks show improved downstream accuracy and significantly faster convergence, indicating effective mitigation of staleness and efficient training.

Conclusion: VISAGNN offers a versatile framework for staleness-aware large-scale GNN training, balancing prediction quality and training efficiency by adapting to staleness in embeddings and computations.

Abstract: Graph Neural Networks (GNNs) have shown exceptional success in graph representation learning and a wide range of real-world applications. However, scaling deeper GNNs poses challenges due to the neighbor explosion problem when training on large-scale graphs. To mitigate this, a promising class of GNN training algorithms utilizes historical embeddings to reduce computation and memory costs while preserving the expressiveness of the model. These methods leverage historical embeddings for out-of-batch nodes, effectively approximating full-batch training without losing any neighbor information-a limitation found in traditional sampling methods. However, the staleness of these historical embeddings often introduces significant bias, acting as a bottleneck that can adversely affect model performance. In this paper, we propose a novel VersatIle Staleness-Aware GNN, named VISAGNN, which dynamically and adaptively incorporates staleness criteria into the large-scale GNN training process. By embedding staleness into the message passing mechanism, loss function, and historical embeddings during training, our approach enables the model to adaptively mitigate the negative effects of stale embeddings, thereby reducing estimation errors and enhancing downstream accuracy. Comprehensive experiments demonstrate the effectiveness of our method in overcoming the staleness issue of existing historical embedding techniques, showcasing its superior performance and efficiency on large-scale benchmarks, along with significantly faster convergence.

</details>


### [581] [Global-Lens Transformers: Adaptive Token Mixing for Dynamic Link Prediction](https://arxiv.org/abs/2511.12442)
*Tao Zou,Chengfeng Wu,Tianxi Liao,Junchen Ye,Bowen Du*

Main category: cs.LG

TL;DR: GLFormer introduces an attention-free Transformer-style model for dynamic graphs that uses an adaptive token mixer for local context-aware aggregation and a hierarchical module to extend temporal receptive fields. It achieves state-of-the-art performance on six dynamic-graph benchmarks with significantly better efficiency, challenging the necessity of self-attention in this domain.


<details>
  <summary>Details</summary>
Motivation: Dynamic graphs require modeling evolving relationships for temporal link prediction, but self-attention in Transformers is quadratic in sequence length, hindering scalability on high-frequency or large-scale graphs. Emerging evidence suggests architectural design matters more than attention itself, motivating an attention-free approach.

Method: Propose GLFormer, an attention-free Transformer-style framework. It features an adaptive token mixer that performs context-aware local aggregation based on interaction order and time intervals, and a hierarchical aggregation module that stacks local token mixers across layers to expand the temporal receptive field for long-term dependencies.

Result: GLFormer achieves state-of-the-art performance on six widely-used dynamic-graph benchmarks, with significantly improved efficiency over Transformer baselines, indicating that attention-free architectures can match or surpass Transformer performance in dynamic graph settings.

Conclusion: Attention-free architectural design can effectively model dynamic graphs with high efficiency and competitive performance, offering a viable alternative to self-attention for temporal link prediction.

Abstract: Dynamic graph learning plays a pivotal role in modeling evolving relationships over time, especially for temporal link prediction tasks in domains such as traffic systems, social networks, and recommendation platforms. While Transformer-based models have demonstrated strong performance by capturing long-range temporal dependencies, their reliance on self-attention results in quadratic complexity with respect to sequence length, limiting scalability on high-frequency or large-scale graphs. In this work, we revisit the necessity of self-attention in dynamic graph modeling. Inspired by recent findings that attribute the success of Transformers more to their architectural design than attention itself, we propose GLFormer, a novel attention-free Transformer-style framework for dynamic graphs. GLFormer introduces an adaptive token mixer that performs context-aware local aggregation based on interaction order and time intervals. To capture long-term dependencies, we further design a hierarchical aggregation module that expands the temporal receptive field by stacking local token mixers across layers. Experiments on six widely-used dynamic graph benchmarks show that GLFormer achieves SOTA performance, which reveals that attention-free architectures can match or surpass Transformer baselines in dynamic graph settings with significantly improved efficiency.

</details>


### [582] [Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network for Multimodal Depression Detection](https://arxiv.org/abs/2511.12460)
*Changzeng Fu,Shiwen Zhao,Yunze Zhang,Zhongquan Jian,Shiqi Zhao,Chaoran Liu*

Main category: cs.LG

TL;DR: P^3HF proposes a personality-guided, hypergraph-based multimodal depression detector that improves accuracy and weighted F1 by about 10% on MPDD-Young, with ablations showing the necessity of personality-guided encoding and high-order cross-modal reasoning; code is released.


<details>
  <summary>Details</summary>
Motivation: Current multimodal depression detection using Transformers or Graph Neural Networks struggles to capture individual differences and cross-modal temporal dependencies across diverse contexts, hindering generalization.

Method: Three innovations: (1) personality-guided representation learning using large language models to convert discrete individual features into contextual descriptions for personalized encoding; (2) Hypergraph-Former architecture to model high-order cross-modal temporal relationships; (3) event-level domain disentanglement with contrastive learning to improve generalization across behavioral contexts.

Result: On MPDD-Young, P^3HF achieves approximately 10% improvements in accuracy and weighted F1 for binary and ternary depression classification tasks. Ablation studies confirm that both personality-guided representation learning and high-order hypergraph reasoning are essential for robust, individual-aware representations. Code available at https://github.com/hacilab/P3HF.

Conclusion: Personality-guided encoding and high-order hypergraph reasoning enhance generalization and robustness of multimodal depression detection across varied behavioral contexts.

Abstract: Depression represents a global mental health challenge requiring efficient and reliable automated detection methods. Current Transformer- or Graph Neural Networks (GNNs)-based multimodal depression detection methods face significant challenges in modeling individual differences and cross-modal temporal dependencies across diverse behavioral contexts. Therefore, we propose P$^3$HF (Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network) with three key innovations: (1) personality-guided representation learning using LLMs to transform discrete individual features into contextual descriptions for personalized encoding; (2) Hypergraph-Former architecture modeling high-order cross-modal temporal relationships; (3) event-level domain disentanglement with contrastive learning for improved generalization across behavioral contexts. Experiments on MPDD-Young dataset show P$^3$HF achieves around 10\% improvement on accuracy and weighted F1 for binary and ternary depression classification task over existing methods. Extensive ablation studies validate the independent contribution of each architectural component, confirming that personality-guided representation learning and high-order hypergraph reasoning are both essential for generating robust, individual-aware depression-related representations. The code is released at https://github.com/hacilab/P3HF.

</details>


### [583] [Redundancy-optimized Multi-head Attention Networks for Multi-View Multi-Label Feature Selection](https://arxiv.org/abs/2511.12462)
*Yuzhou Liu,Jiarui Liu,Wanfu Gao*

Main category: cs.LG

TL;DR: Introduces RMAN-MMFS, a redundancy-optimized multi-head attention framework for multi-view multi-label feature selection, achieving superior results by modeling intra-view and inter-view relations and reducing redundancy.


<details>
  <summary>Details</summary>
Motivation: Multi-view multi-label data have complex interdependencies among features, views, and labels. Existing attention-based feature selection mainly captures intra-view relations and overlooks inter-view complementarity and redundancy, limiting subset quality.

Method: RMAN-MMFS uses each attention head to model intra-view feature relationships; cross-attention between heads captures inter-view feature complementarity. It includes static redundancy to reduce within-view redundancy and a dynamic redundancy term that accounts for redundancy between unselected and selected features across the selection process.

Result: Evaluated on six real-world datasets against six baseline methods, showing superior performance.

Conclusion: A redundancy-aware, multi-head attention framework effectively models intra- and inter-view relations while promoting compact feature subsets, advancing multi-view multi-label feature selection.

Abstract: Multi-view multi-label data offers richer perspectives for artificial intelligence, but simultaneously presents significant challenges for feature selection due to the inherent complexity of interrelations among features, views and labels. Attention mechanisms provide an effective way for analyzing these intricate relationships. They can compute importance weights for information by aggregating correlations between Query and Key matrices to focus on pertinent values. However, existing attention-based feature selection methods predominantly focus on intra-view relationships, neglecting the complementarity of inter-view features and the critical feature-label correlations. Moreover, they often fail to account for feature redundancy, potentially leading to suboptimal feature subsets. To overcome these limitations, we propose a novel method based on Redundancy-optimized Multi-head Attention Networks for Multi-view Multi-label Feature Selection (RMAN-MMFS). Specifically, we employ each individual attention head to model intra-view feature relationships and use the cross-attention mechanisms between different heads to capture inter-view feature complementarity. Furthermore, we design static and dynamic feature redundancy terms: the static term mitigates redundancy within each view, while the dynamic term explicitly models redundancy between unselected and selected features across the entire selection process, thereby promoting feature compactness. Comprehensive evaluations on six real-world datasets, compared against six multi-view multi-label feature selection methods, demonstrate the superior performance of the proposed method.

</details>


### [584] [Logarithmic Regret and Polynomial Scaling in Online Multi-step-ahead Prediction](https://arxiv.org/abs/2511.12467)
*Jiachen Qian,Yang Zheng*

Main category: cs.LG

TL;DR: An online learning approach for multi-step prediction in unknown linear stochastic systems, deriving an optimal linear policy and an online LS learner with strong regret guarantees against a model-based Kalman predictor.


<details>
  <summary>Details</summary>
Motivation: Unknown linear stochastic systems require online adaptation for accurate multi-step forecasting; aim to achieve performance close to the optimal model-based predictor without knowing the model a priori.

Method: Use conditional distribution theory to parameterize the optimal prediction policy as a linear function of future, past inputs, and past outputs; develop an online least-squares algorithm to learn this policy; provide regret analysis relative to the Kalman filter; introduce new proof techniques for almost-sure regret bounds.

Result: Online algorithm attains logarithmic regret with respect to the optimal Kalman filter in the multi-step setting; establishes an almost-sure regret bound for large horizons without fixed failure probabilities; shows regret grows logarithmically in horizon N but the constant factor grows polynomially with the prediction horizon H, with degree determined by the largest Jordan block of eigenvalue 1.

Conclusion: The work delivers a practical online predictor with provable guarantees for unknown linear stochastic systems, clarifying how horizon length affects regret and introducing novel techniques to obtain almost-sure guarantees.

Abstract: This letter studies the problem of online multi-step-ahead prediction for unknown linear stochastic systems. Using conditional distribution theory, we derive an optimal parameterization of the prediction policy as a linear function of future inputs, past inputs, and past outputs. Based on this characterization, we propose an online least-squares algorithm to learn the policy and analyze its regret relative to the optimal model-based predictor. We show that the online algorithm achieves logarithmic regret with respect to the optimal Kalman filter in the multi-step setting. Furthermore, with new proof techniques, we establish an almost-sure regret bound that does not rely on fixed failure probabilities for sufficiently large horizons $N$. Finally, our analysis also reveals that, while the regret remains logarithmic in $N$, its constant factor grows polynomially with the prediction horizon $H$, with the polynomial order set by the largest Jordan block of eigenvalue 1 in the system matrix.

</details>


### [585] [Diffusion Model Based Signal Recovery Under 1-Bit Quantization](https://arxiv.org/abs/2511.12471)
*Youming Chen,Zhaoqiang Liu*

Main category: cs.LG

TL;DR: A diffusion-model-based method, Diff-OneBit, enables fast and high-quality recovery from 1-bit quantization by using a differentiable surrogate likelihood and a plug-and-play diffusion-prior framework, achieving state-of-the-art performance and efficiency in 1-bit compressed sensing and logistic regression.


<details>
  <summary>Details</summary>
Motivation: 1-bit quantization tasks involve non-differentiable or implicit link functions, making optimization hard. Diffusion models offer strong priors for signal recovery, but integrating them with non-linear 1-bit likelihoods is challenging. A differentiable surrogate likelihood can enable gradient-based updates, and a plug-and-play framework decouples data fidelity from the prior, allowing pretrained DMs to serve as denoisers.

Method: Introduce Diff-OneBit, which uses a differentiable surrogate likelihood to model 1-bit quantization and embeds this into a plug-and-play diffusion framework. The data-fidelity term is separated from the diffusion prior, enabling any pretrained DM to act as a denoiser during iterative reconstruction.

Result: Extensive experiments on FFHQ, CelebA, and ImageNet demonstrate high-fidelity image reconstruction and superior performance to state-of-the-art methods in both 1-bit compressed sensing and logistic regression, with improved computational efficiency.

Conclusion: Diff-OneBit shows that combining a differentiable surrogate likelihood for 1-bit quantization with a flexible plug-and-play diffusion prior yields effective, fast 1-bit signal recovery, validating the practicality of DM priors in non-differentiable inverse problems.

Abstract: Diffusion models (DMs) have demonstrated to be powerful priors for signal recovery, but their application to 1-bit quantization tasks, such as 1-bit compressed sensing and logistic regression, remains a challenge. This difficulty stems from the inherent non-linear link function in these tasks, which is either non-differentiable or lacks an explicit characterization. To tackle this issue, we introduce Diff-OneBit, which is a fast and effective DM-based approach for signal recovery under 1-bit quantization. Diff-OneBit addresses the challenge posed by non-differentiable or implicit links functions via leveraging a differentiable surrogate likelihood function to model 1-bit quantization, thereby enabling gradient based iterations. This function is integrated into a flexible plug-and-play framework that decouples the data-fidelity term from the diffusion prior, allowing any pretrained DM to act as a denoiser within the iterative reconstruction process. Extensive experiments on the FFHQ, CelebA and ImageNet datasets demonstrate that Diff-OneBit gives high-fidelity reconstructed images, outperforming state-of-the-art methods in both reconstruction quality and computational efficiency across 1-bit compressed sensing and logistic regression tasks.

</details>


### [586] [SculptDrug : A Spatial Condition-Aware Bayesian Flow Model for Structure-based Drug Design](https://arxiv.org/abs/2511.12489)
*Qingsong Zhong,Haomin Yu,Yan Lin,Wangmeng Shen,Long Zeng,Jilin Hu*

Main category: cs.LG

TL;DR: SculptDrug is a spatially-aware generative model for structure-based drug design that uses Bayesian flow networks with progressive denoising, a Boundary Awareness Block for protein constraints, and a Hierarchical Encoder to maintain global and local structural coherence, achieving superior performance on CrossDocked.


<details>
  <summary>Details</summary>
Motivation: Current generative SBDD models struggle to enforce boundary constraints, integrate hierarchical structure, and maintain spatial fidelity, limiting realistic ligand–protein conformations.

Method: BFN-based generative framework with progressive denoising; Boundary Awareness Block to enforce protein surface constraints; Hierarchical Encoder to capture global context and preserve fine-grained interactions; evaluated on CrossDocked.

Result: Outperforms state-of-the-art baselines on CrossDocked, demonstrating effectiveness of spatial condition-aware modeling.

Conclusion: Spatial condition-aware modeling with these components improves ligand generation fidelity and compatibility with target proteins in SBDD; potential for more accurate and efficient drug discovery.

Abstract: Structure-Based drug design (SBDD) has emerged as a popular approach in drug discovery, leveraging three-dimensional protein structures to generate drug ligands. However, existing generative models encounter several key challenges: (1) incorporating boundary condition constraints, (2) integrating hierarchical structural conditions, and (3) ensuring spatial modeling fidelity. To address these limitations, we propose SculptDrug, a spatial condition-aware generative model based on Bayesian flow networks (BFNs). First, SculptDrug follows a BFN-based framework and employs a progressive denoising strategy to ensure spatial modeling fidelity, iteratively refining atom positions while enhancing local interactions for precise spatial alignment. Second, we introduce a Boundary Awareness Block that incorporates protein surface constraints into the generative process to ensure that generated ligands are geometrically compatible with the target protein. Third, we design a Hierarchical Encoder that captures global structural context while preserving fine-grained molecular interactions, ensuring overall consistency and accurate ligand-protein conformations. We evaluate SculptDrug on the CrossDocked dataset, and experimental results demonstrate that SculptDrug outperforms state-of-the-art baselines, highlighting the effectiveness of spatial condition-aware modeling.

</details>


### [587] [Uncover and Unlearn Nuisances: Agnostic Fully Test-Time Adaptation](https://arxiv.org/abs/2511.12491)
*Ponhvoan Srey,Yaxin Shi,Hangwei Qian,Jing Li,Ivor W. Tsang*

Main category: cs.LG

TL;DR: AFTTA proposes agnostic FTTA via uncover-and-unlearn using MI-based nuisance unlearning with predefined domain mappings; achieves better generalization on unseen corruptions and styles.


<details>
  <summary>Details</summary>
Motivation: FTTA lacks access to source data/training protocols; domain shifts are unpredictable; traditional distribution alignment is infeasible; need a strategy that generalizes to unforeseeable target domains.

Method: Uncover potential shifts by simulating nuisances with predefined mappings; during test-time enforce unlearning of nuisances by regularizing latent space and label predictions; use mutual information criterion to guide nuisance unlearning and promote confident, consistent predictions.

Result: Extensive experiments on corruption and style shifts show consistent performance improvements over existing FTTA approaches.

Conclusion: Agnostic FTTA effectively addresses unknown domain shifts at test time, enabling better generalization with off-the-shelf domain transformations and MI-based unlearning; demonstrates promise for FTTA under real-world target variability.

Abstract: Fully Test-Time Adaptation (FTTA) addresses domain shifts without access to source data and training protocols of the pre-trained models. Traditional strategies that align source and target feature distributions are infeasible in FTTA due to the absence of training data and unpredictable target domains. In this work, we exploit a dual perspective on FTTA, and propose Agnostic FTTA (AFTTA) as a novel formulation that enables the usage of off-the-shelf domain transformations during test-time to enable direct generalization to unforeseeable target data. To address this, we develop an uncover-and-unlearn approach. First, we uncover potential unwanted shifts between source and target domains by simulating them through predefined mappings and consider them as nuisances. Then, during test-time prediction, the model is enforced to unlearn these nuisances by regularizing the consequent shifts in latent representations and label predictions. Specifically, a mutual information-based criterion is devised and applied to guide nuisances unlearning in the feature space and encourage confident and consistent prediction in label space. Our proposed approach explicitly addresses agnostic domain shifts, enabling superior model generalization under FTTA constraints. Extensive experiments on various tasks, involving corruption and style shifts, demonstrate that our method consistently outperforms existing approaches.

</details>


### [588] [Towards Better IncomLDL: We Are Unaware of Hidden Labels in Advance](https://arxiv.org/abs/2511.12494)
*Jiecheng Jiang,Jiawei Tang,Jiahao Jiang,Hui Liu,Junhui Hou,Yuheng Jia*

Main category: cs.LG

TL;DR: HidLDL recovers complete label distributions from incomplete LDL data by leveraging proportional information of observed labels, local feature similarity, and global low-rank structure, with a theoretical recovery bound; empirically outperforms LDL and IncomLDL.


<details>
  <summary>Details</summary>
Motivation: LDL is powerful for describing samples with label distributions, but collecting complete LDL data is costly. IncomLDL methods assume missing labels have zero degree, which is unrealistic as remaining labels' degrees should adjust. Hence there is a need to recover hidden labels from incomplete observations (HidLDL).

Method: Introduce HidLDL with a constraint that captures the proportional information of observed labels. The method uses local feature similarity and global low-rank structure to reveal hidden labels. A theoretical recovery bound is derived to guarantee learning from hidden labels.

Result: Extensive recovery and predictive experiments on various datasets demonstrate that HidLDL outperforms state-of-the-art LDL and IncomLDL methods in recovering hidden labels and in predictive accuracy.

Conclusion: HidLDL provides a feasible and effective framework for learning label distributions with hidden labels from incomplete data, supported by theoretical guarantees and strong empirical results.

Abstract: Label distribution learning (LDL) is a novel paradigm that describe the samples by label distribution of a sample. However, acquiring LDL dataset is costly and time-consuming, which leads to the birth of incomplete label distribution learning (IncomLDL). All the previous IncomLDL methods set the description degrees of "missing" labels in an instance to 0, but remains those of other labels unchanged. This setting is unrealistic because when certain labels are missing, the degrees of the remaining labels will increase accordingly. We fix this unrealistic setting in IncomLDL and raise a new problem: LDL with hidden labels (HidLDL), which aims to recover a complete label distribution from a real-world incomplete label distribution where certain labels in an instance are omitted during annotation. To solve this challenging problem, we discover the significance of proportional information of the observed labels and capture it by an innovative constraint to utilize it during the optimization process. We simultaneously use local feature similarity and the global low-rank structure to reveal the mysterious veil of hidden labels. Moreover, we theoretically give the recovery bound of our method, proving the feasibility of our method in learning from hidden labels. Extensive recovery and predictive experiments on various datasets prove the superiority of our method to state-of-the-art LDL and IncomLDL methods.

</details>


### [589] [BSO: Binary Spiking Online Optimization Algorithm](https://arxiv.org/abs/2511.12502)
*Yu Liang,Yu Yang,Wenjie Wei,Ammar Belatreche,Shuai Wang,Malu Zhang,Yang Yang*

Main category: cs.LG

TL;DR: Introduces Binary Spiking Online (BSO) optimization for BSNNs, reducing training memory by online weight updates; extends to temporal-aware T-BSO; provides convergence guarantees and empirical improvements; code available.


<details>
  <summary>Details</summary>
Motivation: BSNNs offer efficiency benefits but training requires latent weights and temporal processing, leading to substantial training memory overhead. A memory-efficient online training method is needed.

Method: BSO updates weights directly via flip signals triggered when the product of gradient momentum and weights exceeds a threshold, eliminating latent weights during training. The temporal-aware variant T-BSO captures gradient information across time steps to adapt thresholds and leverage BSNN dynamics.

Result: Presents convergence guarantees for both BSO and T-BSO with formal regret bounds. Extensive experiments show superior optimization performance compared to existing BSNN training methods.

Conclusion: BSO and its temporal variant provide memory-efficient online training for BSNNs with competitive performance; T-BSO further improves by exploiting temporal dynamics. Code is available at the provided GitHub link.

Abstract: Binary Spiking Neural Networks (BSNNs) offer promising efficiency advantages for resource-constrained computing. However, their training algorithms often require substantial memory overhead due to latent weights storage and temporal processing requirements. To address this issue, we propose Binary Spiking Online (BSO) optimization algorithm, a novel online training algorithm that significantly reduces training memory. BSO directly updates weights through flip signals under the online training framework. These signals are triggered when the product of gradient momentum and weights exceeds a threshold, eliminating the need for latent weights during training. To enhance performance, we propose T-BSO, a temporal-aware variant that leverages the inherent temporal dynamics of BSNNs by capturing gradient information across time steps for adaptive threshold adjustment. Theoretical analysis establishes convergence guarantees for both BSO and T-BSO, with formal regret bounds characterizing their convergence rates. Extensive experiments demonstrate that both BSO and T-BSO achieve superior optimization performance compared to existing training methods for BSNNs. The codes are available at https://github.com/hamings1/BSO.

</details>


### [590] [Hierarchical Frequency-Decomposition Graph Neural Networks for Road Network Representation Learning](https://arxiv.org/abs/2511.12507)
*Jingtian Ma,Jingyuan Wang,Leong Hou U*

Main category: cs.LG

TL;DR: HiFiNet—a hierarchical frequency-decomposition GNN for road networks that unifies spatial and spectral modeling via multi-level virtual nodes and a topology-aware transformer, yielding superior performance across four tasks.


<details>
  <summary>Details</summary>
Motivation: Road networks exhibit both coarse global traffic trends and fine-grained local fluctuations. Existing spatial-based GNNs tend to over-smooth local signals, while spectral-based methods miss localized variations, leading to a misalignment between spatial and frequency representations.

Method: HiFiNet constructs a multi-level hierarchy of virtual nodes to enable localized frequency analysis. It uses a decomposition-updating-reconstruction framework with a topology-aware graph transformer to separately model and fuse low- and high-frequency signals, thereby unifying spatial and spectral modeling.

Result: Empirically validated on real-world road network datasets across four downstream tasks, HiFiNet achieves superior performance and better generalization in capturing effective road network representations.

Conclusion: The proposed hierarchical frequency-decomposition framework bridges spatial and spectral modeling for road networks, offering improved representation learning that handles both global trends and local fluctuations.

Abstract: Road networks are critical infrastructures underpinning intelligent transportation systems and their related applications. Effective representation learning of road networks remains challenging due to the complex interplay between spatial structures and frequency characteristics in traffic patterns. Existing graph neural networks for modeling road networks predominantly fall into two paradigms: spatial-based methods that capture local topology but tend to over-smooth representations, and spectral-based methods that analyze global frequency components but often overlook localized variations. This spatial-spectral misalignment limits their modeling capacity for road networks exhibiting both coarse global trends and fine-grained local fluctuations. To bridge this gap, we propose HiFiNet, a novel hierarchical frequency-decomposition graph neural network that unifies spatial and spectral modeling. HiFiNet constructs a multi-level hierarchy of virtual nodes to enable localized frequency analysis, and employs a decomposition-updating-reconstruction framework with a topology-aware graph transformer to separately model and fuse low- and high-frequency signals. Theoretically justified and empirically validated on multiple real-world datasets across four downstream tasks, HiFiNet demonstrates superior performance and generalization ability in capturing effective road network representations.

</details>


### [591] [Spectral Bias Mitigation via xLSTM-PINN: Memory-Gated Representation Refinement for Physics-Informed Learning](https://arxiv.org/abs/2511.12512)
*Ze Tao,Darui Zhao,Fujun Liu,Ke Xu,Xiangsheng Hu*

Main category: cs.LG

TL;DR: A new PINN variant, xLSTM-PINN, uses gated-memory multiscale features and adaptive residual weighting to reduce spectral bias and boost extrapolation in PDE learning, showing consistent improvements across four benchmarks in spectral and statistical errors, and broader training stability.


<details>
  <summary>Details</summary>
Motivation: Physics-informed neural networks suffer from spectral bias, residual-data imbalance, and weak extrapolation. There is a need for representation-level techniques that enhance high-frequency fidelity and extrapolability without changing physics losses or autograd settings.

Method: Introduce xLSTM-PINN with gated cross-scale memory and multiscale feature extraction, a staged frequency curriculum, and adaptive residual reweighting. Evaluate on four benchmarks with analytic references and extrapolation tests. Analyze frequency-domain properties (kernel weights, resolvable bandwidth) and boundary behavior, comparing to baseline PINN.

Result: Achieves markedly lower spectral error and RMSE, and a broader stable learning-rate window. In frequency-domain metrics, higher high-frequency kernel weights, right-shifted resolvable bandwidth, faster high-k error decay, and narrower error bands with lower MSE/ RMSE/ MAE/ MaxAE. Outperforms baseline PINN on all four benchmarks with cleaner boundary transitions and reduced high-frequency ripples.

Conclusion: Spectral bias is suppressed, resolvable bandwidth is widened, and high-k time-to-threshold is shortened under the same compute budget. The approach improves accuracy, reproducibility, and transferability without modifying AD or physics losses.

Abstract: Physics-informed learning for PDEs is surging across scientific computing and industrial simulation, yet prevailing methods face spectral bias, residual-data imbalance, and weak extrapolation. We introduce a representation-level spectral remodeling xLSTM-PINN that combines gated-memory multiscale feature extraction with adaptive residual-data weighting to curb spectral bias and strengthen extrapolation. Across four benchmarks, we integrate gated cross-scale memory, a staged frequency curriculum, and adaptive residual reweighting, and verify with analytic references and extrapolation tests, achieving markedly lower spectral error and RMSE and a broader stable learning-rate window. Frequency-domain benchmarks show raised high-frequency kernel weights and a right-shifted resolvable bandwidth, shorter high-k error decay and time-to-threshold, and narrower error bands with lower MSE, RMSE, MAE, and MaxAE. Compared with the baseline PINN, we reduce MSE, RMSE, MAE, and MaxAE across all four benchmarks and deliver cleaner boundary transitions with attenuated high-frequency ripples in both frequency and field maps. This work suppresses spectral bias, widens the resolvable band and shortens the high-k time-to-threshold under the same budget, and without altering AD or physics losses improves accuracy, reproducibility, and transferability.

</details>


### [592] [Regret Guarantees for Linear Contextual Stochastic Shortest Path](https://arxiv.org/abs/2511.12534)
*Dor Polikar,Alon Cohen*

Main category: cs.LG

TL;DR: LR-CSSP provides a learning algorithm for linear contextual Stochastic Shortest Path with adversarial contexts, achieving sublinear regret bounds and guaranteed termination, even with continuous contexts and unknown dynamics.


<details>
  <summary>Details</summary>
Motivation: To extend contextual SSP to the CSSP setting where the MDP is determined by an unknown linear mapping from adversarial contexts; existing methods struggle with unknown transitions/losses and may allow non-terminating episodes, motivating a learning algorithm with termination guarantees.

Method: LR-CSSP uses a linear-context model to map contexts to MDPs and develops an online learning algorithm that explores and composes policies without prior transition or cost knowledge. It derives regret bounds that depend on the episode count K, context dimension d, state/action sizes, and the unknowns B_* and T_* (optimal cumulative loss and optimal policy horizon). It also analyzes a regime with all costs above a positive minimum to obtain an alternative regret bound, and ensures episode termination within reasonable time. 

Result: The main result is a regret upper bound of ~O~(K^{2/3} d^{2/3} |S| |A|^{1/3} B_*^2 T_* log(1/δ)) for LR-CSSP. In the special case where costs are bounded away from zero, a different bound ~O~(sqrt(K d^2 |S|^3 |A| B_*^3 log(1/δ)/ℓ_min)) is achieved. Importantly, unlike some contextual MDP settings, insufficient knowledge can prolong episodes or lead to non-termination, but LR-CSSP manages continuous contexts and guarantees termination within a reasonable time. 

Conclusion: LR-CSSP extends contextual SSP to linear CSSP with unknown mapping and provides provable regret guarantees alongside termination assurances, highlighting that in CSSP, partial knowledge can affect both cumulative loss and episode length. This framework handles continuous contexts effectively and ensures practical termination behavior.

Abstract: We define the problem of linear Contextual Stochastic Shortest Path (CSSP), where at the beginning of each episode, the learner observes an adversarially chosen context that determines the MDP through a fixed but unknown linear function. The learner's objective is to reach a designated goal state with minimal expected cumulative loss, despite having no prior knowledge of the transition dynamics, loss functions, or the mapping from context to MDP. In this work, we propose LR-CSSP, an algorithm that achieves a regret bound of $\widetilde{O}(K^{2/3} d^{2/3} |S| |A|^{1/3} B_\star^2 T_\star \log (1/ δ))$, where $K$ is the number of episodes, $d$ is the context dimension, $S$ and $A$ are the sets of states and actions respectively, $B_\star$ bounds the optimal cumulative loss and $T_\star$, unknown to the learner, bounds the expected time for the optimal policy to reach the goal. In the case where all costs exceed $\ell_{\min}$, LR-CSSP attains a regret of $\widetilde O(\sqrt{K \cdot d^2 |S|^3 |A| B_\star^3 \log(1/δ)/\ell_{\min}})$. Unlike in contextual finite-horizon MDPs, where limited knowledge primarily leads to higher losses and regret, in the CSSP setting, insufficient knowledge can also prolong episodes and may even lead to non-terminating episodes. Our analysis reveals that LR-CSSP effectively handles continuous context spaces, while ensuring all episodes terminate within a reasonable number of time steps.

</details>


### [593] [Center-Outward q-Dominance: A Sample-Computable Proxy for Strong Stochastic Dominance in Multi-Objective Optimisation](https://arxiv.org/abs/2511.12545)
*Robin van der Laag,Hao Wang,Thomas Bäck,Yingjie Fan*

Main category: cs.LG

TL;DR: Center-outward q-dominance (OT-based) ranks multivariate distributions in SMOOP, proves it implies strong FSD, provides a Type I error-controlled test with sample-size threshold, and demonstrates improved ranking in hyperparameter tuning (YAHPO-MO) and selection in NSGA-II on noisy benchmarks; positions q-dominance as principled foundation for stochastic dominance in SMOOP.


<details>
  <summary>Details</summary>
Motivation: Scalarization common in SMOOP discards information and can be unreliable; a multivariate, principled dominance concept is needed.

Method: Define center-outward q-dominance via optimal transport; prove it implies strong first-order stochastic dominance; develop empirical testing procedure with explicit sample-size threshold n*(δ) to control Type I error; apply as ranking method for hyperparameter tuners and as replacement for mean-based selection in NSGA-II.

Result: Empirical results on YAHPO-MO show q-dominance enables comparison of Pareto sets when expected HVI differences are indistinguishable; NSGA-II with q-dominance achieves superior convergence on noise-augmented ZDT benchmarks.

Conclusion: Center-outward q-dominance offers a principled, tractable foundation for seeking truly stochastically dominant solutions in SMOOPs.

Abstract: Stochastic multi-objective optimization (SMOOP) requires ranking multivariate distributions; yet, most empirical studies perform scalarization, which loses information and is unreliable. Based on the optimal transport theory, we introduce the center-outward q-dominance relation and prove it implies strong first-order stochastic dominance (FSD). Also, we develop an empirical test procedure based on q-dominance, and derive an explicit sample size threshold, $n^*(δ)$, to control the Type I error. We verify the usefulness of our approach in two scenarios: (1) as a ranking method in hyperparameter tuning; (2) as a selection method in multi-objective optimization algorithms. For the former, we analyze the final stochastic Pareto sets of seven multi-objective hyperparameter tuners on the YAHPO-MO benchmark tasks with q-dominance, which allows us to compare these tuners when the expected hypervolume indicator (HVI, the most common performance metric) of the Pareto sets becomes indistinguishable. For the latter, we replace the mean value-based selection in the NSGA-II algorithm with $q$-dominance, which shows a superior convergence rate on noise-augmented ZDT benchmark problems. These results establish center-outward q-dominance as a principled, tractable foundation for seeking truly stochastically dominant solutions for SMOOPs.

</details>


### [594] [CAO: Curvature-Adaptive Optimization via Periodic Low-Rank Hessian Sketching](https://arxiv.org/abs/2511.12548)
*Wenzhang Du*

Main category: cs.LG

TL;DR: A curvature-adaptive optimizer uses periodic Hessian sketches to precondition gradients in a low-rank subspace, treating the orthogonal directions with standard first-order updates. It achieves favorable theoretical guarantees and practical speedups on CNNs like ResNet on CIFAR-10/100, with robustness to the sketch rank and a simple setup.


<details>
  <summary>Details</summary>
Motivation: First-order methods are reliable but slow in sharp, anisotropic regions. The authors aim to accelerate convergence by adapting to curvature via a low-rank Hessian sketch, reducing updates to a informative subspace while keeping a simple fallback in the rest.

Method: Periodically sketch a low-rank Hessian subspace using Hessian-vector products, precondition gradients only within that subspace; the orthogonal complement uses standard first-order updates. Theoretical results under L-smooth non-convex objectives give O(1/T) stationarity with a wider stable stepsize; under PL condition with bounded residual curvature outside the sketch, the loss contracts at refresh steps. Empirically, evaluated on CIFAR-10/100 with ResNet-18/34, showing earlier entry into low-loss region and significant training-time speedups; the method is a single-parameter knob with robustness to sketch rank k in {1,3,5}, and a k=0 ablation (curvature-free).

Result: The method achieves faster progress to a pre-declared training loss threshold (0.75) by approximately 2.95x versus Adam on CIFAR-100/ResNet-18, while attaining comparable final test accuracy. Theoretical guarantees include O(1/T) stationarity for L-smooth non-convex objectives and contraction under PL with bounded outside curvature. Empirically, robustness to the sketch rank and a principled k=0 baseline. Anonymized logs and scripts are released to reproduce results.

Conclusion: A simple, one-knob curvature-adaptive optimizer that selectively preconditions in a low-rank Hessian subspace accelerates training in non-convex deep learning settings, providing both theoretical guarantees and practical speedups without sacrificing final accuracy. The solution is robust to the choice of sketch rank and includes a curvature-free ablation for comparison.

Abstract: First-order optimizers are reliable but slow in sharp, anisotropic regions. We study a curvature-adaptive method that periodically sketches a low-rank Hessian subspace via Hessian--vector products and preconditions gradients only in that subspace, leaving the orthogonal complement first-order. For L-smooth non-convex objectives, we recover the standard O(1/T) stationarity guarantee with a widened stable stepsize range; under a Polyak--Lojasiewicz (PL) condition with bounded residual curvature outside the sketch, the loss contracts at refresh steps. On CIFAR-10/100 with ResNet-18/34, the method enters the low-loss region substantially earlier: measured by epochs to a pre-declared train-loss threshold (0.75), it reaches the threshold 2.95x faster than Adam on CIFAR-100/ResNet-18, while matching final test accuracy. The approach is one-knob: performance is insensitive to the sketch rank k across {1,3,5}, and k=0 yields a principled curvature-free ablation. We release anonymized logs and scripts that regenerate all figures and tables.

</details>


### [595] [Training Instabilities Induce Flatness Bias in Gradient Descent](https://arxiv.org/abs/2511.12558)
*Lawrence Wang,Stephen J. Roberts*

Main category: cs.LG

TL;DR: Instabilities during gradient descent bias optimization toward flatter minima via Rotational Polarity of Eigenvectors (RPE), improving generalization; the effect extends to SGD and Adam, challenging the view that stability is always optimal.


<details>
  <summary>Details</summary>
Motivation: Explain the constructive role of training instabilities in deep learning and why larger learning rates can enhance generalization by steering the dynamics toward flatter regions of the loss landscape.

Method: Theoretically analyze the Hessian’s leading eigenvectors during unstable training; introduce Rotational Polarity of Eigenvectors (RPE) as leading eigenvector rotations that accompany instability, showing these rotations promote exploration and flattening. Extend the analysis to stochastic GD and empirical validation; examine the interplay with minibatch noise and the impact of reintroducing instability in Adam.

Result: Instabilities push parameters toward flatter regions, biasing the solution toward flatter minima and improving generalization. RPE dynamics explain why larger learning rates can be beneficial; in SGD, instability-driven flattening persists and can outweigh minibatch noise. Reintroducing instability in Adam further improves generalization.

Conclusion: Training instabilities can play a constructive role in deep learning by promoting convergence to flatter minima and improving generalization, suggesting a reconceptualization of learning-rate policies and optimization dynamics.

Abstract: Classical analyses of gradient descent (GD) define a stability threshold based on the largest eigenvalue of the loss Hessian, often termed sharpness. When the learning rate lies below this threshold, training is stable and the loss decreases monotonically. Yet, modern deep networks often achieve their best performance beyond this regime.
  We demonstrate that such instabilities induce an implicit bias in GD, driving parameters toward flatter regions of the loss landscape and thereby improving generalization. The key mechanism is the Rotational Polarity of Eigenvectors (RPE), a geometric phenomenon in which the leading eigenvectors of the Hessian rotate during training instabilities. These rotations, which increase with learning rates, promote exploration and provably lead to flatter minima.
  This theoretical framework extends to stochastic GD, where instability-driven flattening persists and its empirical effects outweigh minibatch noise. Finally, we show that restoring instabilities in Adam further improves generalization.
  Together, these results establish and understand the constructive role of training instabilities in deep learning.

</details>


### [596] [Enhancing Machine Learning Model Efficiency through Quantization and Bit Depth Optimization: A Performance Analysis on Healthcare Data](https://arxiv.org/abs/2511.12568)
*Mitul Goswami,Romit Chatterjee*

Main category: cs.LG

TL;DR: Quantization and bit-depth optimization reduce runtime of Logistic Regression on medical data with minor accuracy loss; downcasting data types to float32/int32 yields speedups; performance depends on parameter settings.


<details>
  <summary>Details</summary>
Motivation: To address prolonged execution times in complex models and improve efficiency without sizable accuracy loss.

Method: Apply quantization and bit-depth optimization by downcasting inputs from float64 to float32 and int32, using two medical datasets with a Logistic Regression model as case studies; evaluate time complexity and accuracy.

Result: Significant reduction in time complexity with only a small decrease in accuracy, indicating a state-of-the-art optimization approach.

Conclusion: The effectiveness of these optimization techniques depends on a set of parameters and may vary by context.

Abstract: This research aims to optimize intricate learning models by implementing quantization and bit-depth optimization techniques. The objective is to significantly cut time complexity while preserving model efficiency, thus addressing the challenge of extended execution times in intricate models. Two medical datasets were utilized as case studies to apply a Logistic Regression (LR) machine learning model. Using efficient quantization and bit depth optimization strategies the input data is downscaled from float64 to float32 and int32. The results demonstrated a significant reduction in time complexity, with only a minimal decrease in model accuracy post-optimization, showcasing the state-of-the-art optimization approach. This comprehensive study concludes that the impact of these optimization techniques varies depending on a set of parameters.

</details>


### [597] [LMM-IR: Large-Scale Netlist-Aware Multimodal Framework for Static IR-Drop Prediction](https://arxiv.org/abs/2511.12581)
*Kai Ma,Zhen Wang,Hongquan He,Qi Xu,Tinghuan Chen,Hao Geng*

Main category: cs.LG

TL;DR: A multimodal approach using a large-scale netlist transformer (LNT) that represents netlist topology as 3D point clouds to enable fast, scalable static IR drop prediction by fusing SPICE and image data in a latent space, achieving top performance in ICCAD 2023.


<details>
  <summary>Details</summary>
Motivation: Static IR drop analysis is time-consuming (can take hours) and often requires iterative analysis, imposing a heavy computational burden on chip design workflows; fast and accurate IR drop prediction is needed to shorten design time.

Method: Introduce a multimodal framework that processes SPICE files via a large-scale netlist transformer (LNT). Netlist topology is represented as 3D point clouds to efficiently handle very large nets (hundreds of thousands to millions of nodes). All data types (netlist, image data) are encoded into a shared latent space as features and fed into the model for static voltage drop prediction.

Result: Experimental results show that the proposed method achieves the best F1 score and the lowest MAE among the winning teams of the ICCAD 2023 contest and outperforms state-of-the-art algorithms.

Conclusion: The multimodal LNT enables fast and accurate static IR drop prediction on large-scale nets by integrating SPICE and image data in a unified latent representation, reducing design time and computational burden while achieving state-of-the-art accuracy.

Abstract: Static IR drop analysis is a fundamental and critical task in the field of chip design. Nevertheless, this process can be quite time-consuming, potentially requiring several hours. Moreover, addressing IR drop violations frequently demands iterative analysis, thereby causing the computational burden. Therefore, fast and accurate IR drop prediction is vital for reducing the overall time invested in chip design. In this paper, we firstly propose a novel multimodal approach that efficiently processes SPICE files through large-scale netlist transformer (LNT). Our key innovation is representing and processing netlist topology as 3D point cloud representations, enabling efficient handling of netlist with up to hundreds of thousands to millions nodes. All types of data, including netlist files and image data, are encoded into latent space as features and fed into the model for static voltage drop prediction. This enables the integration of data from multiple modalities for complementary predictions. Experimental results demonstrate that our proposed algorithm can achieve the best F1 score and the lowest MAE among the winning teams of the ICCAD 2023 contest and the state-of-the-art algorithms.

</details>


### [598] [Symmetry-Aware Graph Metanetwork Autoencoders: Model Merging through Parameter Canonicalization](https://arxiv.org/abs/2511.12601)
*Odysseas Boufalis,Jorge Carrasco-Pollo,Joshua Rosenthal,Eduardo Terres-Caballero,Alejandro García-Castellanos*

Main category: cs.LG

TL;DR: Proposes ScaleGMNs that are invariant to both permutation and parameter scaling, enabling automatic alignment of different network parameterizations without solving assignment problems; extends prior work to improve model merging and interpolation.


<details>
  <summary>Details</summary>
Motivation: Scale and permutation symmetries cause multiple equivalent minima in neural network loss landscapes. Exploiting these symmetries can facilitate merging and interpolation of models by mapping equivalent parameterizations into the same loss basin.

Method: Extend Ainsworth et al.'s approach by incorporating scaling symmetries via ScaleGMNs within an autoencoder framework. Use ScaleGMN-based encoders to align INRs and CNNs under permutation and scaling, avoiding explicit assignment problems. Train as an autoencoder where ScaleGMNs serve as invariant encoders.

Result: Experimental results show alignment of INRs and CNNs under both permutation and scaling symmetries, enabling smooth linear interpolation (model merging) without solving the combinatorial assignment problem; code is publicly available.

Conclusion: Incorporating both permutation and scaling symmetries with ScaleGMNs provides a practical, symmetry-aware pathway to merge similar networks and navigate loss landscapes more efficiently, broadening the scope of model merging for implicit and explicit architectures.

Abstract: Neural network parameterizations exhibit inherent symmetries that yield multiple equivalent minima within the loss landscape. Scale Graph Metanetworks (ScaleGMNs) explicitly leverage these symmetries by proposing an architecture equivariant to both permutation and parameter scaling transformations. Previous work by Ainsworth et al. (2023) addressed permutation symmetries through a computationally intensive combinatorial assignment problem, demonstrating that leveraging permutation symmetries alone can map networks into a shared loss basin. In this work, we extend their approach by also incorporating scaling symmetries, presenting an autoencoder framework utilizing ScaleGMNs as invariant encoders. Experimental results demonstrate that our method aligns Implicit Neural Representations (INRs) and Convolutional Neural Networks (CNNs) under both permutation and scaling symmetries without explicitly solving the assignment problem. This approach ensures that similar networks naturally converge within the same basin, facilitating model merging, i.e., smooth linear interpolation while avoiding regions of high loss. The code is publicly available on our GitHub repository.

</details>


### [599] [PID-controlled Langevin Dynamics for Faster Sampling of Generative Models](https://arxiv.org/abs/2511.12603)
*Hongyi Chen,Jianhai Shu,Jingtao Ding,Yong Li,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: A PID-controlled Langevin dynamics method that speeds up sampling by using integral and derivative gradient information without retraining.


<details>
  <summary>Details</summary>
Motivation: Langevin sampling is slow due to many fine-grained iterations; a training-free approach to faster sampling would broaden practical use.

Method: Reinterpret energy gradients as feedback signals and apply a PID controller using historical gradients (integral term) and gradient trends (derivative term) to update dynamics. No training required and it integrates with any Langevin-based method.

Result: Empirical results show higher sample quality with far fewer steps across image generation and reasoning tasks; easy integration; code available.

Conclusion: PIDLD provides a training-free, broadly applicable acceleration for Langevin-based generative models, enabling efficiency-critical applications.

Abstract: Langevin dynamics sampling suffers from extremely low generation speed, fundamentally limited by numerous fine-grained iterations to converge to the target distribution. We introduce PID-controlled Langevin Dynamics (PIDLD), a novel sampling acceleration algorithm that reinterprets the sampling process using control-theoretic principles. By treating energy gradients as feedback signals, PIDLD combines historical gradients (the integral term) and gradient trends (the derivative term) to efficiently traverse energy landscapes and adaptively stabilize, thereby significantly reducing the number of iterations required to produce high-quality samples. Our approach requires no additional training, datasets, or prior information, making it immediately integrable with any Langevin-based method. Extensive experiments across image generation and reasoning tasks demonstrate that PIDLD achieves higher quality with fewer steps, making Langevin-based generative models more practical for efficiency-critical applications. The implementation can be found at \href{https://github.com/tsinghua-fib-lab/PIDLD}{https://github.com/tsinghua-fib-lab/PIDLD}.

</details>


### [600] [FedTopo: Topology-Informed Representation Alignment in Federated Learning under Non-I.I.D. Conditions](https://arxiv.org/abs/2511.12628)
*Ke Hu,Liyao Xiang,Peng Tang,Weidong Qiu*

Main category: cs.LG

TL;DR: FedTopo combines topology-guided block screening, topological embeddings, and a topology-alignment loss to mitigate non-IID-induced drift in federated learning, improving convergence and accuracy on standard vision datasets.


<details>
  <summary>Details</summary>
Motivation: Non-IID client data causes representation drift and misalignment across clients. Topological information captures global structure that pixel- or patch-level losses may miss, offering a robust signal for alignment.

Method: 1) Use Topology-Guided Block Screening (TGBS) to pick the most topology-informative block based on persistence signatures; 2) Build a compact Topological Embedding (TE) per client from that block; 3) Apply a Topological Alignment Loss (TAL) to keep client representations topologically consistent with the global model during optimization.

Result: Experiments on Fashion-MNIST, CIFAR-10, and CIFAR-100 under four non-IID partitions show FedTopo accelerates convergence and improves accuracy over strong baselines.

Conclusion: Topology-aware screening and alignment enable coherent cross-client representation despite non-IID data, making FedTopo a promising approach for topology-guided federated learning.

Abstract: Current federated-learning models deteriorate under heterogeneous (non-I.I.D.) client data, as their feature representations diverge and pixel- or patch-level objectives fail to capture the global topology which is essential for high-dimensional visual tasks. We propose FedTopo, a framework that integrates Topological-Guided Block Screening (TGBS) and Topological Embedding (TE) to leverage topological information, yielding coherently aligned cross-client representations by Topological Alignment Loss (TAL). First, Topology-Guided Block Screening (TGBS) automatically selects the most topology-informative block, i.e., the one with maximal topological separability, whose persistence-based signatures best distinguish within- versus between-class pairs, ensuring that subsequent analysis focuses on topology-rich features. Next, this block yields a compact Topological Embedding, which quantifies the topological information for each client. Finally, a Topological Alignment Loss (TAL) guides clients to maintain topological consistency with the global model during optimization, reducing representation drift across rounds. Experiments on Fashion-MNIST, CIFAR-10, and CIFAR-100 under four non-I.I.D. partitions show that FedTopo accelerates convergence and improves accuracy over strong baselines.

</details>


### [601] [NFQ2.0: The CartPole Benchmark Revisited](https://arxiv.org/abs/2511.12644)
*Sascha Lange,Roland Hafner,Martin Riedmiller*

Main category: cs.LG

TL;DR: Revisit NFQ with a modern NFQ2.0 variant applied to CartPole real-world system; ablation studies identify key design choices and hyperparameters to improve stability and reproducibility for industrial DRL.


<details>
  <summary>Details</summary>
Motivation: Address reproducibility, stability, and practical deployment challenges of the original neural fitted Q-iteration (NFQ) and bridge traditional batch RL approaches with modern DRL for real-world control.

Method: Recreate NFQ in a modernized form (NFQ2.0) and test on CartPole implemented with standard industrial components; conduct ablations on critical hyperparameters and architectural/design decisions to study their impact on learning stability and repeatability.

Result: NFQ2.0 shows improved stability and repeatability over the original NFQ; yields actionable insights on hyperparameter choices and design decisions that enhance robustness; demonstrates potential for reliable application of deep RL in industry.

Conclusion: NFQ2.0 provides a more reproducible, robust path for applying deep reinforcement learning to real-world control problems, offering practical guidance for practitioners to reproduce and improve results.

Abstract: This article revisits the 20-year-old neural fitted Q-iteration (NFQ) algorithm on its classical CartPole benchmark. NFQ was a pioneering approach towards modern Deep Reinforcement Learning (Deep RL) in applying multi-layer neural networks to reinforcement learning for real-world control problems. We explore the algorithm's conceptual simplicity and its transition from online to batch learning, which contributed to its stability. Despite its initial success, NFQ required extensive tuning and was not easily reproducible on real-world control problems. We propose a modernized variant NFQ2.0 and apply it to the CartPole task, concentrating on a real-world system build from standard industrial components, to investigate and improve the learning process's repeatability and robustness. Through ablation studies, we highlight key design decisions and hyperparameters that enhance performance and stability of NFQ2.0 over the original variant. Finally, we demonstrate how our findings can assist practitioners in reproducing and improving results and applying deep reinforcement learning more effectively in industrial contexts.

</details>


### [602] [Sample Complexity of Agnostic Multiclass Classification: Natarajan Dimension Strikes Back](https://arxiv.org/abs/2511.12659)
*Alon Cohen,Liad Erez,Steve Hanneke,Tomer Koren,Yishay Mansour,Shay Moran,Qian Zhang*

Main category: cs.LG

TL;DR: Agnostic multiclass PAC sample complexity is governed by two dimensions, DS and Nat. The paper proves a near-tight bound of DS^{1.5}/ε + Nat/ε^2 (up to log factors), showing DS dominates in the first term and Nat in the second for small ε.


<details>
  <summary>Details</summary>
Motivation: Extend the VC-dimension paradigm from binary to multiclass learning. Clarify which complexity measures govern learnability when Nat and DS can diverge, resolving whether a single parameter suffices.

Method: Introduces a novel online procedure based on a self-adaptive multiplicative-weights algorithm with label-space reduction. This approach departs from traditional agnostic methods based on uniform convergence or realizability reductions and may be of independent interest.

Result: Agnostic multiclass PAC sample complexity bound of DS^{1.5}/ε + Nat/ε^2 (up to log factors). This bound is tight up to a sqrt(DS) factor in the first term and nearly matches existing Nat/ε^2 and DS/ε lower bounds. Demonstrates two regimes: DS controls the first term, Nat controls the second for small ε.

Conclusion: Multiclass learning intrinsically involves two structural parameters rather than a single dimension. The work provides new insights into the complexity landscape and introduces an online technique that could influence future agnostic learning methods.

Abstract: The fundamental theorem of statistical learning states that binary PAC learning is governed by a single parameter -- the Vapnik-Chervonenkis (VC) dimension -- which determines both learnability and sample complexity. Extending this to multiclass classification has long been challenging, since Natarajan's work in the late 80s proposing the Natarajan dimension (Nat) as a natural analogue of VC. Daniely and Shalev-Shwartz (2014) introduced the DS dimension, later shown by Brukhim et al. (2022) to characterize multiclass learnability. Brukhim et al. also showed that Nat and DS can diverge arbitrarily, suggesting that multiclass learning is governed by DS rather than Nat. We show that agnostic multiclass PAC sample complexity is in fact governed by two distinct dimensions. Specifically, we prove nearly tight agnostic sample complexity bounds that, up to log factors, take the form $\frac{DS^{1.5}}ε + \frac{Nat}{ε^2}$ where $ε$ is the excess risk. This bound is tight up to a $\sqrt{DS}$ factor in the first term, nearly matching known $Nat/ε^2$ and $DS/ε$ lower bounds. The first term reflects the DS-controlled regime, while the second shows that the Natarajan dimension still dictates asymptotic behavior for small $ε$. Thus, unlike binary or online classification -- where a single dimension (VC or Littlestone) controls both phenomena -- multiclass learning inherently involves two structural parameters. Our technical approach departs from traditional agnostic learning methods based on uniform convergence or reductions to realizable cases. A key ingredient is a novel online procedure based on a self-adaptive multiplicative-weights algorithm performing a label-space reduction, which may be of independent interest.

</details>


### [603] [FLClear: Visually Verifiable Multi-Client Watermarking for Federated Learning](https://arxiv.org/abs/2511.12663)
*Chen Gu,Yingying Sun,Yifan She,Donghui Hu*

Main category: cs.LG

TL;DR: Proposes FLClear, a federated learning watermarking framework that achieves collision-free watermark aggregation, stronger watermark security, and intuitive ownership verification via a transposed model optimized with contrastive learning; watermark reconstruction enables visual and SSIM-based verification, outperforming prior FL watermarking methods.


<details>
  <summary>Details</summary>
Motivation: To protect client model intellectual property in federated learning by preventing server manipulation (erasing contributions or claiming ownership) while addressing watermark collisions, security weaknesses, and verification difficulties in existing methods.

Method: Introduce FLClear with a transposed model jointly optimized with a contrastive learning objective to align watermarking with the main learning task, enabling collision-free watermark aggregation. During verification, reconstruct the watermark from the transposed model and assess ownership through visual inspection and structural similarity (SSIM) metrics.

Result: Extensive experiments across diverse datasets, aggregation schemes, and attack scenarios show FLClear consistently outperforms state-of-the-art FL watermarking methods.

Conclusion: FLClear delivers collision-free watermark aggregation, enhanced watermark security, and visually intuitive verification, establishing a robust, interpretable ownership verification framework and advancing state-of-the-art in FL watermarking.

Abstract: Federated learning (FL) enables multiple clients to collaboratively train a shared global model while preserving the privacy of their local data. Within this paradigm, the intellectual property rights (IPR) of client models are critical assets that must be protected. In practice, the central server responsible for maintaining the global model may maliciously manipulate the global model to erase client contributions or falsely claim sole ownership, thereby infringing on clients' IPR. Watermarking has emerged as a promising technique for asserting model ownership and protecting intellectual property. However, existing FL watermarking approaches remain limited, suffering from potential watermark collisions among clients, insufficient watermark security, and non-intuitive verification mechanisms. In this paper, we propose FLClear, a novel framework that simultaneously achieves collision-free watermark aggregation, enhanced watermark security, and visually interpretable ownership verification. Specifically, FLClear introduces a transposed model jointly optimized with contrastive learning to integrate the watermarking and main task objectives. During verification, the watermark is reconstructed from the transposed model and evaluated through both visual inspection and structural similarity metrics, enabling intuitive and quantitative ownership verification. Comprehensive experiments conducted over various datasets, aggregation schemes, and attack scenarios demonstrate the effectiveness of FLClear and confirm that it consistently outperforms state-of-the-art FL watermarking methods.

</details>


### [604] [Attention-Enhanced Convolutional Autoencoder and Structured Delay Embeddings for Weather Prediction](https://arxiv.org/abs/2511.12682)
*Amirpasha Hedayat,Karthik Duraisamy*

Main category: cs.LG

TL;DR: An efficient ROM framework combining a ResNet-based autoencoder with block attention and a linear latent-dynamics operator; effective in-distribution weather prediction but limited in extrapolating beyond training data, with projection error as the main bottleneck.


<details>
  <summary>Details</summary>
Motivation: Address the need for efficient short-range weather prediction in chaotic, high-dimensional systems by exploring reduced-order modeling (ROM) and dimensionality reduction, while contrasting with resource-intensive AI approaches and identifying limits of current methods.

Method: A ResNet-based convolutional autoencoder with block attention reduces high-dimensional weather data to a latent space. A linear operator is learned in the time-delayed embedding of the latent space to capture dynamics. Evaluation uses the ERA5 reanalysis dataset, examining in-distribution performance and generalization, and analyzing error sources.

Result: The framework shows good in-distribution predictive capability and indicates that weather dynamics can be captured by linear operations in a suitable latent embedding. Projection error is identified as the main bottleneck rather than inference error, and generalization to future states beyond the training window remains challenging.

Conclusion: The study highlights key challenges in ROM-based chaotic-system modeling and suggests hybrid approaches that pair efficient ROM baselines with more sophisticated AI architectures for long-term climate modeling where computational efficiency is critical.

Abstract: Weather prediction is a quintessential problem involving the forecasting of a complex, nonlinear, and chaotic high-dimensional dynamical system. This work introduces an efficient reduced-order modeling (ROM) framework for short-range weather prediction and investigates fundamental questions in dimensionality reduction and reduced order modeling of such systems. Unlike recent AI-driven models, which require extensive computational resources, our framework prioritizes efficiency while achieving reasonable accuracy. Specifically, a ResNet-based convolutional autoencoder augmented by block attention modules is developed to reduce the dimensionality of high-dimensional weather data. Subsequently, a linear operator is learned in the time-delayed embedding of the latent space to efficiently capture the dynamics. Using the ERA5 reanalysis dataset, we demonstrate that this framework performs well in-distribution as evidenced by effectively predicting weather patterns within training data periods. We also identify important limitations in generalizing to future states, particularly in maintaining prediction accuracy beyond the training window. Our analysis reveals that weather systems exhibit strong temporal correlations that can be effectively captured through linear operations in an appropriately constructed embedding space, and that projection error rather than inference error is the main bottleneck. These findings shed light on some key challenges in reduced-order modeling of chaotic systems and point toward opportunities for hybrid approaches that combine efficient reduced-order models as baselines with more sophisticated AI architectures, particularly for applications in long-term climate modeling where computational efficiency is paramount.

</details>


### [605] [A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning](https://arxiv.org/abs/2511.12695)
*Minghui Chen,Hrad Ghoukasian,Ruinan Jin,Zehua Wang,Sai Praneeth Karimireddy,Xiaoxiao Li*

Main category: cs.LG

TL;DR: LP-FT adapts linear probing followed by full fine-tuning to federated learning, mitigating feature distortion and balancing personalization with global generalization.


<details>
  <summary>Details</summary>
Motivation: Federated learning struggles to balance global generalization and local personalization due to non-identical client data; standard personalized fine-tuning can overfit or fail under domain shifts.

Method: Adapt the centralized LP-FT strategy (linear probing then full fine-tuning) to the FL setting; conduct systematic evaluation across seven datasets and six PFT variants; analyze federated feature distortion; provide theoretical insights on phased parameter updates; identify conditions where LP-FT outperforms standard fine-tuning.

Result: LP-FT demonstrates superior balance between personalization and generalization across diverse datasets; uncovers federated feature distortion and provides theoretical characterization of how LP-FT mitigates it; offers conditions (e.g., partial feature overlap, covariate-concept shift) and actionable deployment guidelines.

Conclusion: LP-FT offers a principled, robust personalization approach for FL; under specified conditions it outperforms standard fine-tuning and informs practical deployment strategies for resilient FL personalization.

Abstract: Federated Learning (FL) enables decentralized, privacy-preserving model training but struggles to balance global generalization and local personalization due to non-identical data distributions across clients. Personalized Fine-Tuning (PFT), a popular post-hoc solution, fine-tunes the final global model locally but often overfits to skewed client distributions or fails under domain shifts. We propose adapting Linear Probing followed by full Fine-Tuning (LP-FT), a principled centralized strategy for alleviating feature distortion (Kumar et al., 2022), to the FL setting. Through systematic evaluation across seven datasets and six PFT variants, we demonstrate LP-FT's superiority in balancing personalization and generalization. Our analysis uncovers federated feature distortion, a phenomenon where local fine-tuning destabilizes globally learned features, and theoretically characterizes how LP-FT mitigates this via phased parameter updates. We further establish conditions (e.g., partial feature overlap, covariate-concept shift) under which LP-FT outperforms standard fine-tuning, offering actionable guidelines for deploying robust personalization in FL.

</details>


### [606] [Beyond Fixed Tasks: Unsupervised Environment Design for Task-Level Pairs](https://arxiv.org/abs/2511.12706)
*Daniel Furelos-Blanco,Charles Pert,Frederik Kelbel,Alex F. Spies,Alessandra Russo,Michael Dennis*

Main category: cs.LG

TL;DR: Jointly aligning tasks and levels to create autocurricula; ATLAS yields solvable yet challenging task-level pairs, outperforming random task sampling and accelerating learning via task–level and level mutations.


<details>
  <summary>Details</summary>
Motivation: Random task-level pair sampling often yields unsolvable combinations; co-design of tasks and levels is needed to train general agents on complex instructions.

Method: Extend unsupervised environment design (UED) to jointly generate solvable but challenging task-level pairs; model tasks as reward machines in Minigrid; generate joint autocurricula over tasks and levels; implement mutations exploiting the structure of both tasks and levels; introduce an evaluation suite for benchmarking.

Result: ATLAS vastly outperforms random sampling, especially when solvable pairs are unlikely; mutations leveraging task–level and level structure accelerate convergence to performant policies.

Conclusion: Co-design of tasks and levels is beneficial for efficient policy learning; the evaluation suite models tasks as reward machines to benchmark progress and enables robust progress tracking in the field.

Abstract: Training general agents to follow complex instructions (tasks) in intricate environments (levels) remains a core challenge in reinforcement learning. Random sampling of task-level pairs often produces unsolvable combinations, highlighting the need to co-design tasks and levels. While unsupervised environment design (UED) has proven effective at automatically designing level curricula, prior work has only considered a fixed task. We present ATLAS (Aligning Tasks and Levels for Autocurricula of Specifications), a novel method that generates joint autocurricula over tasks and levels. Our approach builds upon UED to automatically produce solvable yet challenging task-level pairs for policy training. To evaluate ATLAS and drive progress in the field, we introduce an evaluation suite that models tasks as reward machines in Minigrid levels. Experiments demonstrate that ATLAS vastly outperforms random sampling approaches, particularly when sampling solvable pairs is unlikely. We further show that mutations leveraging the structure of both tasks and levels accelerate convergence to performant policies.

</details>


### [607] [Adaptive Graph Rewiring to Mitigate Over-Squashing in Mesh-Based GNNs for Fluid Dynamics Simulations](https://arxiv.org/abs/2511.12709)
*Sangwoo Seo,Hyunsung Kim,Jiwan Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: Adaptive graph rewiring in mesh-based GNNs for fluid simulations; introduces layer-aware, delay-based rewiring to mimic gradual physical interactions and mitigate over-squashing, improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Over-squashing and unrealistic instantaneous interactions in mesh-based GNNs with fixed pre-applied rewiring; need to model distance-dependent propagation delays in mesh-based fluids.

Method: Compute a rewiring delay score for bottleneck nodes using shortest-path distance and velocity difference; use this score to dynamically select the message-passing layer where new edges are rewired during simulation.

Result: AdaMeshNet outperforms conventional rewiring methods, capturing sequential nature of interactions and achieving more accurate fluid predictions in mesh-based simulations.

Conclusion: Adaptive, physics-informed rewiring enables better modeling of gradual information propagation in mesh GNNs and mitigates over-squashing, improving prediction quality.

Abstract: Mesh-based simulation using Graph Neural Networks (GNNs) has been recognized as a promising approach for modeling fluid dynamics. However, the mesh refinement techniques which allocate finer resolution to regions with steep gradients can induce the over-squashing problem in mesh-based GNNs, which prevents the capture of long-range physical interactions. Conventional graph rewiring methods attempt to alleviate this issue by adding new edges, but they typically complete all rewiring operations before applying them to the GNN. These approaches are physically unrealistic, as they assume instantaneous interactions between distant nodes and disregard the distance information between particles. To address these limitations, we propose a novel framework, called Adaptive Graph Rewiring in Mesh-Based Graph Neural Networks (AdaMeshNet), that introduces an adaptive rewiring process into the message-passing procedure to model the gradual propagation of physical interactions. Our method computes a rewiring delay score for bottleneck nodes in the mesh graph, based on the shortest-path distance and the velocity difference. Using this score, it dynamically selects the message-passing layer at which new edges are rewired, which can lead to adaptive rewiring in a mesh graph. Extensive experiments on mesh-based fluid simulations demonstrate that AdaMeshNet outperforms conventional rewiring methods, effectively modeling the sequential nature of physical interactions and enabling more accurate predictions.

</details>


### [608] [Oxytrees: Model Trees for Bipartite Learning](https://arxiv.org/abs/2511.12713)
*Pedro Ilídio,Felipe Kenji Nakano,Alireza Gharahighehi,Robbe D'hondt,Ricardo Cerri,Celine Vens*

Main category: cs.LG

TL;DR: Oxytrees are proxy-based biclustering model trees for bipartite learning that compress interactions via proxy matrices, enabling fast training/prediction with competitive accuracy, especially in inductive settings, using Kronecker-product leaves and a new leaf-assignment, plus a Python API for reproducible research.


<details>
  <summary>Details</summary>
Motivation: Current bipartite learning methods struggle with generalization across domains and scalability; there is a need for fast, generalizable models that can handle various interaction types.

Method: Propose Oxytrees: ensembles of proxy-based biclustering model trees that compress the interaction matrix into row/column proxy matrices; introduce a fast leaf-assignment algorithm; leaves use linear models with Kronecker product kernel; provide Python API; evaluate on 15 datasets.

Result: On 15 datasets, ensembles of Oxytrees achieve up to 30x faster training than state-of-the-art biclustering forests while maintaining competitive or superior predictive performance, particularly in inductive settings.

Conclusion: Oxytrees offer scalable, generalizable bipartite learning with efficient training and inference, and an accessible API to support reproducible research.

Abstract: Bipartite learning is a machine learning task that aims to predict interactions between pairs of instances. It has been applied to various domains, including drug-target interactions, RNA-disease associations, and regulatory network inference. Despite being widely investigated, current methods still present drawbacks, as they are often designed for a specific application and thus do not generalize to other problems or present scalability issues. To address these challenges, we propose Oxytrees: proxy-based biclustering model trees. Oxytrees compress the interaction matrix into row- and column-wise proxy matrices, significantly reducing training time without compromising predictive performance. We also propose a new leaf-assignment algorithm that significantly reduces the time taken for prediction. Finally, Oxytrees employ linear models using the Kronecker product kernel in their leaves, resulting in shallower trees and thus even faster training. Using 15 datasets, we compared the predictive performance of ensembles of Oxytrees with that of the current state-of-the-art. We achieved up to 30-fold improvement in training times compared to state-of-the-art biclustering forests, while demonstrating competitive or superior performance in most evaluation settings, particularly in the inductive setting. Finally, we provide an intuitive Python API to access all datasets, methods and evaluation measures used in this work, thus enabling reproducible research in this field.

</details>


### [609] [On Robustness of Linear Classifiers to Targeted Data Poisoning](https://arxiv.org/abs/2511.12722)
*Nakshatra Gupta,Sumanth Prabhu,Supratik Chakraborty,R Venkatesh*

Main category: cs.LG

TL;DR: This work studies dataset robustness to label-only poisoning; exact robustness is NP-hard, even for linear classifiers; it provides lower/upper bounds that are efficiently computable in practice and demonstrates effectiveness on public datasets.


<details>
  <summary>Details</summary>
Motivation: To detect and quantify the security risk of data poisoning without exhaustively checking all label perturbations, especially given large training sets; bridging the gap between intractable exact robustness and practical defense.

Method: Formulates robustness under a label-flipping adversary within the victim model's hypothesis space; proves NP-Complete for exact computation; introduces a bounds-based technique that computes provable lower and upper bounds; implements an efficient algorithm and evaluates on datasets.

Result: Bounds are computable efficiently on many datasets; poisoning beyond bounds significantly changes test-point predictions; the method succeeds where state-of-the-art fails to compute bounds.

Conclusion: Provides practical robustness guarantees against label-only data poisoning and helps detect vulnerability; reduces the intractability of exact robustness via tight bounds; suggests applicability to dataset curation and defense.

Abstract: Data poisoning is a training-time attack that undermines the trustworthiness of learned models. In a targeted data poisoning attack, an adversary manipulates the training dataset to alter the classification of a targeted test point. Given the typically large size of training dataset, manual detection of poisoning is difficult. An alternative is to automatically measure a dataset's robustness against such an attack, which is the focus of this paper. We consider a threat model wherein an adversary can only perturb the labels of the training dataset, with knowledge limited to the hypothesis space of the victim's model. In this setting, we prove that finding the robustness is an NP-Complete problem, even when hypotheses are linear classifiers. To overcome this, we present a technique that finds lower and upper bounds of robustness. Our implementation of the technique computes these bounds efficiently in practice for many publicly available datasets. We experimentally demonstrate the effectiveness of our approach. Specifically, a poisoning exceeding the identified robustness bounds significantly impacts test point classification. We are also able to compute these bounds in many more cases where state-of-the-art techniques fail.

</details>


### [610] [LAYA: Layer-wise Attention Aggregation for Interpretable Depth-Aware Neural Networks](https://arxiv.org/abs/2511.12723)
*Gennaro Vessio*

Main category: cs.LG

TL;DR: LAYA is a layer-wise attention output head that dynamically aggregates multi-layer representations for prediction, improving accuracy slightly and providing interpretable layer-level attributions.


<details>
  <summary>Details</summary>
Motivation: Intermediate layers contain rich, complementary information that is discarded when the decision head relies only on the final hidden state; leveraging multi-layer representations could improve performance and interpretability.

Method: Introduce LAYA, an input-conditioned attention-based output head that learns weights over layer-wise features to synthesize predictions in a architecture-agnostic way; yields interpretable attribution signals directly from computation.

Result: LAYA matches or improves standard heads on vision and language benchmarks, with up to ~1% relative accuracy gains; provides explicit layer-attribution scores revealing the contribution of different abstraction levels; interpretability signals emerge from the model's computation without post hoc explanations; code released at the project URL.

Conclusion: LAYA enables an interpretable, architecture-agnostic output head that effectively leverages multi-layer representations, maintaining or boosting performance while exposing per-layer contributions.

Abstract: Deep neural networks typically rely on the representation produced by their final hidden layer to make predictions, implicitly assuming that this single vector fully captures the semantics encoded across all preceding transformations. However, intermediate layers contain rich and complementary information -- ranging from low-level patterns to high-level abstractions -- that is often discarded when the decision head depends solely on the last representation. This paper revisits the role of the output layer and introduces LAYA (Layer-wise Attention Aggregator), a novel output head that dynamically aggregates internal representations through attention. Instead of projecting only the deepest embedding, LAYA learns input-conditioned attention weights over layer-wise features, yielding an interpretable and architecture-agnostic mechanism for synthesizing predictions. Experiments on vision and language benchmarks show that LAYA consistently matches or improves the performance of standard output heads, with relative gains of up to about one percentage point in accuracy, while providing explicit layer-attribution scores that reveal how different abstraction levels contribute to each decision. Crucially, these interpretability signals emerge directly from the model's computation, without any external post hoc explanations. The code to reproduce LAYA is publicly available at: https://github.com/gvessio/LAYA.

</details>


### [611] [Convolutional Model Trees](https://arxiv.org/abs/2511.12725)
*William Ward Armstrong*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: A method for creating a forest of model trees to fit samples of a function defined on images is described in several steps: down-sampling the images, determining a tree's hyperplanes, applying convolutions to the hyperplanes to handle small distortions of training images, and creating forests of model trees to increase accuracy and achieve a smooth fit. A 1-to-1 correspondence among pixels of images, coefficients of hyperplanes and coefficients of leaf functions offers the possibility of dealing with larger distortions such as arbitrary rotations or changes of perspective. A theoretical method for smoothing forest outputs to produce a continuously differentiable approximation is described. Within that framework, a training procedure is proved to converge.

</details>


### [612] [Stabilizing Self-Consuming Diffusion Models with Latent Space Filtering](https://arxiv.org/abs/2511.12742)
*Zhongteng Cai,Yaxuan Wang,Yang Liu,Xueru Zhang*

Main category: cs.LG

TL;DR: Latent Space Filtering (LSF) mitigates model collapse in self-consuming diffusion models by filtering out less realistic synthetic data from mixed datasets, stabilizing training without extra cost or human annotation.


<details>
  <summary>Details</summary>
Motivation: Self-consuming loops, where synthetic data are reused to train successive generations of generative models, can cause training instability and model collapse. Existing fixes either raise cost or require costly human annotation; a low-cost solution is needed.

Method: Analyze the latent space dynamics of synthetic data across generations, observe degradation of the low-dimensional latent structure, and introduce Latent Space Filtering (LSF) to filter out less realistic synthetic data from mixed datasets. Provide a theoretical framework linking latent-space degradation to empirical observations while empirically validating LSF on real-world datasets.

Result: LSF consistently outperforms existing baselines across multiple real-world datasets, effectively mitigating model collapse without increasing training cost or relying on human annotation.

Conclusion: LSF provides a cost-effective, annotation-free approach to stabilizing self-consuming diffusion pipelines by maintaining healthier latent-space representations of synthetic data, and can be integrated into training workflows to prevent model collapse.

Abstract: As synthetic data proliferates across the Internet, it is often reused to train successive generations of generative models. This creates a ``self-consuming loop" that can lead to training instability or \textit{model collapse}. Common strategies to address the issue -- such as accumulating historical training data or injecting fresh real data -- either increase computational cost or require expensive human annotation. In this paper, we empirically analyze the latent space dynamics of self-consuming diffusion models and observe that the low-dimensional structure of latent representations extracted from synthetic data degrade over generations. Based on this insight, we propose \textit{Latent Space Filtering} (LSF), a novel approach that mitigates model collapse by filtering out less realistic synthetic data from mixed datasets. Theoretically, we present a framework that connects latent space degradation to empirical observations. Experimentally, we show that LSF consistently outperforms existing baselines across multiple real-world datasets, effectively mitigating model collapse without increasing training cost or relying on human annotation.

</details>


### [613] [DIVIDE: A Framework for Learning from Independent Multi-Mechanism Data Using Deep Encoders and Gaussian Processes](https://arxiv.org/abs/2511.12745)
*Vivek Chawla,Boris Slautin,Utkarsh Pratiush,Dayakar Penumadu,Sergei Kalinin*

Main category: cs.LG

TL;DR: DIVIDE disentangles multiple generative factors by using mechanism-specific encoders and a structured Gaussian Process in a joint latent space, enabling interpretable, uncertainty-calibrated predictions and active learning; demonstrated on synthetic, FerroSIM, and PbTiO3 PFM data; robust to noise and extendable to multifunctional datasets.


<details>
  <summary>Details</summary>
Motivation: Scientific datasets encode several independent mechanisms whose combined effect hides individual contributions; there's a need for disentangling these factors to enable interpretable predictions and uncertainty-aware decision-making.

Method: Use mechanism-specific deep encoders to isolate distinct generative factors, and a structured Gaussian Process in a shared latent space to model their combined effect with calibrated uncertainty; supports structured priors and mechanism-aware predictions; enables efficient active learning.

Result: DIVIDE successfully separates mechanisms, reproduces additive and scaled interactions, and remains robust under noise across synthetic categorical-spatial data, FerroSIM ferroelectric patterns, and experimental PbTiO3 PFM hysteresis loops.

Conclusion: DIVIDE provides a general framework for disentangling mechanisms in multifunctional datasets and can be extended to mechanical, electromagnetic, or optical responses, enabling interpretable predictions and active learning.

Abstract: Scientific datasets often arise from multiple independent mechanisms such as spatial, categorical or structural effects, whose combined influence obscures their individual contributions. We introduce DIVIDE, a framework that disentangles these influences by integrating mechanism-specific deep encoders with a structured Gaussian Process in a joint latent space. Disentanglement here refers to separating independently acting generative factors. The encoders isolate distinct mechanisms while the Gaussian Process captures their combined effect with calibrated uncertainty. The architecture supports structured priors, enabling interpretable and mechanism-aware prediction as well as efficient active learning. DIVIDE is demonstrated on synthetic datasets combining categorical image patches with nonlinear spatial fields, on FerroSIM spin lattice simulations of ferroelectric patterns, and on experimental PFM hysteresis loops from PbTiO3 films. Across benchmarks, DIVIDE separates mechanisms, reproduces additive and scaled interactions, and remains robust under noise. The framework extends naturally to multifunctional datasets where mechanical, electromagnetic or optical responses coexist.

</details>


### [614] [Conformal Online Learning of Deep Koopman Linear Embeddings](https://arxiv.org/abs/2511.12760)
*Ben Gao,Jordan Patracone,Stéphane Chrétien,Olivier Alata*

Main category: cs.LG

TL;DR: COLoKe is an online conformal learning framework that adaptively updates Koopman embeddings from streaming data using a conformal-style trigger to balance predictive accuracy with update cost.


<details>
  <summary>Details</summary>
Motivation: Nonlinear dynamical systems can be represented by linear dynamics in a Koopman embedding, but online updating of the embedding and operator must avoid overfitting and excessive updates while maintaining long-term accuracy.

Method: Combine deep feature learning to obtain Koopman-invariant embeddings with multistep prediction consistency in the lifted (linear) space; employ a conformal-style mechanism that triggers updates only when prediction error exceeds a dynamically calibrated threshold, allowing selective refinement of the Koopman operator and embedding.

Result: Empirical results on benchmark dynamical systems show COLoKe maintains long-term predictive accuracy while significantly reducing unnecessary updates and avoiding overfitting.

Conclusion: COLoKe provides a practical adaptive online learning framework for Koopman embeddings with a principled update mechanism that balances accuracy and update efficiency.

Abstract: We introduce Conformal Online Learning of Koopman embeddings (COLoKe), a novel framework for adaptively updating Koopman-invariant representations of nonlinear dynamical systems from streaming data. Our modeling approach combines deep feature learning with multistep prediction consistency in the lifted space, where the dynamics evolve linearly. To prevent overfitting, COLoKe employs a conformal-style mechanism that shifts the focus from evaluating the conformity of new states to assessing the consistency of the current Koopman model. Updates are triggered only when the current model's prediction error exceeds a dynamically calibrated threshold, allowing selective refinement of the Koopman operator and embedding. Empirical results on benchmark dynamical systems demonstrate the effectiveness of COLoKe in maintaining long-term predictive accuracy while significantly reducing unnecessary updates and avoiding overfitting.

</details>


### [615] [INC: An Indirect Neural Corrector for Auto-Regressive Hybrid PDE Solvers](https://arxiv.org/abs/2511.12764)
*Hao Wei,Aleksandra Franz,Bjoern List,Nils Thuerey*

Main category: cs.LG

TL;DR: Indirect Neural Corrector (INC) integrates neural corrections into the governing PDEs rather than applying direct state updates, addressing error amplification in long-term, potentially chaotic simulations. It is architecture-agnostic, reduces error growth approximately as Δt^{-1} + L, and yields large gains in long-term trajectory accuracy, stability under coarsening, and large speedups in 3D turbulence, with code released.


<details>
  <summary>Details</summary>
Motivation: Directly applying learned corrections to solver outputs causes autoregressive error amplification in long-term simulations, especially in chaotic regimes. There is a need for a stable, physics-consistent, efficient hybrid PDE solver framework that can incorporate neural corrections without destabilizing long-term trajectories.

Method: Introduce Indirect Neural Corrector (INC) that embeds learned corrections into the governing equations rather than updating states directly. Theoretically bound error amplification to ~Δt^{-1} + L. Architecture-agnostic: compatible with any neural backbones and solvers. Evaluate across differentiable solvers, from 1D chaotic systems to 3D turbulence, across various neural architectures.

Result: INC improves long-term trajectory performance (R^2) by up to 158.7%, stabilizes blowups under aggressive coarsening, and yields several orders of magnitude speed-ups in complex 3D turbulence cases. Demonstrates stable, efficient PDE emulation with formal error reduction. Source code available at the provided GitHub link.

Conclusion: INC provides a stable, efficient framework for emulating PDEs with learned corrections by modifying the governing equations rather than direct state updates. It is architecture-agnostic, scales to chaotic and turbulent regimes, and delivers substantial accuracy improvements and speedups while maintaining physics-guaranteed behavior.

Abstract: When simulating partial differential equations, hybrid solvers combine coarse numerical solvers with learned correctors. They promise accelerated simulations while adhering to physical constraints. However, as shown in our theoretical framework, directly applying learned corrections to solver outputs leads to significant autoregressive errors, which originate from amplified perturbations that accumulate during long-term rollouts, especially in chaotic regimes. To overcome this, we propose the Indirect Neural Corrector (\(\mathrm{INC}\)), which integrates learned corrections into the governing equations rather than applying direct state updates. Our key insight is that \(\mathrm{INC}\) reduces the error amplification on the order of \(Δt^{-1} + L\), where \(Δt\) is the timestep and $L$ the Lipschitz constant. At the same time, our framework poses no architectural requirements and integrates seamlessly with arbitrary neural networks and solvers. We test \(\mathrm{INC}\) in extensive benchmarks, covering numerous differentiable solvers, neural backbones, and test cases ranging from a 1D chaotic system to 3D turbulence. INC improves the long-term trajectory performance (\(R^2\)) by up to 158.7\%, stabilizes blowups under aggressive coarsening, and for complex 3D turbulence cases yields speed-ups of several orders of magnitude. INC thus enables stable, efficient PDE emulation with formal error reduction, paving the way for faster scientific and engineering simulations with reliable physics guarantees. Our source code is available at https://github.com/tum-pbs/INC

</details>


### [616] [MolEdit: Knowledge Editing for Multimodal Molecule Language Models](https://arxiv.org/abs/2511.12770)
*Zhenyu Lei,Patrick Soga,Yaochen Zhu,Yinhan He,Yushun Dong,Jundong Li*

Main category: cs.LG

TL;DR: MolEdit is a framework for editing multimodal molecular knowledge in Molecule Language Models, using a Multi-Expert Knowledge Adapter and an Expertise-Aware Editing Switcher to target edits while preserving unrelated knowledge. It introduces MEBench to evaluate Reliability, Locality, and Generality. Across two backbones, MolEdit improves Reliability by up to 18.8% and Locality by 12.0%, with maintained efficiency; code is released.


<details>
  <summary>Details</summary>
Motivation: To mitigate inaccuracies in MoLMs arising from outdated or manipulated training data, and to pioneer targeted editing within MoLMs—addressing the unique, interdependent, and domain-specific knowledge in chemistry and biomedicine.

Method: MolEdit couples a Multi-Expert Knowledge Adapter (specialized components for different molecular facets) with an Expertise-Aware Editing Switcher that activates adapters only when the input closely matches stored edits across all experts, thereby minimizing interference with unrelated knowledge. It is evaluated via MEBench, a benchmark that measures Reliability, Locality, and Generality across two MoLM backbones.

Result: MolEdit yields up to 18.8% higher Reliability and 12.0% better Locality than baselines, while maintaining efficiency, demonstrated on two MoLM backbones. The work provides code at the cited GitHub repository.

Conclusion: This work demonstrates the feasibility and effectiveness of targeted MoLM editing, establishing a practical framework (MolEdit) and evaluation suite (MEBench) for improving the trustworthiness and robustness of Molecule Language Models in practical applications.

Abstract: Understanding and continuously refining multimodal molecular knowledge is crucial for advancing biomedicine, chemistry, and materials science. Molecule language models (MoLMs) have become powerful tools in these domains, integrating structural representations (e.g., SMILES strings, molecular graphs) with rich contextual descriptions (e.g., physicochemical properties). However, MoLMs can encode and propagate inaccuracies due to outdated web-mined training corpora or malicious manipulation, jeopardizing downstream discovery pipelines. While knowledge editing has been explored for general-domain AI, its application to MoLMs remains uncharted, presenting unique challenges due to the multifaceted and interdependent nature of molecular knowledge. In this paper, we take the first step toward MoLM editing for two critical tasks: molecule-to-caption generation and caption-to-molecule generation. To address molecule-specific challenges, we propose MolEdit, a powerful framework that enables targeted modifications while preserving unrelated molecular knowledge. MolEdit combines a Multi-Expert Knowledge Adapter that routes edits to specialized experts for different molecular facets with an Expertise-Aware Editing Switcher that activates the adapters only when input closely matches the stored edits across all expertise, minimizing interference with unrelated knowledge. To systematically evaluate editing performance, we introduce MEBench, a comprehensive benchmark assessing multiple dimensions, including Reliability (accuracy of the editing), Locality (preservation of irrelevant knowledge), and Generality (robustness to reformed queries). Across extensive experiments on two popular MoLM backbones, MolEdit delivers up to 18.8% higher Reliability and 12.0% better Locality than baselines while maintaining efficiency. The code is available at: https://github.com/LzyFischer/MolEdit.

</details>


### [617] [Scalable Multi-Objective and Meta Reinforcement Learning via Gradient Estimation](https://arxiv.org/abs/2511.12779)
*Zhenshuo Zhang,Minxuan Duan,Youran Ye,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: PolicyGradEx clusters n objectives into k groups via a two-stage meta-training and adaptation framework, enabling efficient multi-objective RL with strong empirical gains (≈16% on average; up to 26x speedups). It learns a task-affinity matrix to guide clustering and uses Hessian-based analysis for generalization.


<details>
  <summary>Details</summary>
Motivation: As the number of objectives n grows, training a single policy for all tasks becomes suboptimal and costly. Grouping related objectives into few trainable cohorts (k<<n) can improve performance and efficiency in domains such as robotics, control, and preference optimization in language models.

Method: Two-stage approach: (1) meta-train a policy across all objectives via multitask learning to obtain a meta-policy. (2) adapt the meta-policy to many randomly sampled objective subsets using a first-order approximation of well-trained networks to estimate task affinities. Use the resulting affinity matrix to cluster objectives into k groups by maximizing intra-cluster affinity. Evaluate on robotic control and Meta-World benchmarks; perform ablations (random grouping, gradient-similarity grouping) and Hessian-trace analysis for generalization.

Result: PolicyGradEx outperforms state-of-the-art baselines by ~16% on average; provides up to 26× speedup relative to training all objectives jointly to form the clusters. Ablations show a 19% improvement over random or gradient-similarity-based groupings. Hessian-trace analysis yields non-vacuous insights into generalization errors.

Conclusion: A scalable framework for multi-objective RL that identifies and exploits task affinities to form a small number of trainable groups, achieving strong empirical gains and substantial training efficiency. The use of first-order adaptation and Hessian-based generalization analysis supports the method's effectiveness and provides interpretability of task relationships.

Abstract: We study the problem of efficiently estimating policies that simultaneously optimize multiple objectives in reinforcement learning (RL). Given $n$ objectives (or tasks), we seek the optimal partition of these objectives into $k \ll n$ groups, where each group comprises related objectives that can be trained together. This problem arises in applications such as robotics, control, and preference optimization in language models, where learning a single policy for all $n$ objectives is suboptimal as $n$ grows. We introduce a two-stage procedure -- meta-training followed by fine-tuning -- to address this problem. We first learn a meta-policy for all objectives using multitask learning. Then, we adapt the meta-policy to multiple randomly sampled subsets of objectives. The adaptation step leverages a first-order approximation property of well-trained policy networks, which is empirically verified to be accurate within a $2\%$ error margin across various RL environments. The resulting algorithm, PolicyGradEx, efficiently estimates an aggregate task-affinity score matrix given a policy evaluation algorithm. Based on the estimated affinity score matrix, we cluster the $n$ objectives into $k$ groups by maximizing the intra-cluster affinity scores. Experiments on three robotic control and the Meta-World benchmarks demonstrate that our approach outperforms state-of-the-art baselines by $16\%$ on average, while delivering up to $26\times$ faster speedup relative to performing full training to obtain the clusters. Ablation studies validate each component of our approach. For instance, compared with random grouping and gradient-similarity-based grouping, our loss-based clustering yields an improvement of $19\%$. Finally, we analyze the generalization error of policy networks by measuring the Hessian trace of the loss surface, which gives non-vacuous measures relative to the observed generalization errors.

</details>


### [618] [Physics-Constrained Adaptive Neural Networks Enable Real-Time Semiconductor Manufacturing Optimization with Minimal Training Data](https://arxiv.org/abs/2511.12788)
*Rubén Darío Guerrero*

Main category: cs.LG

TL;DR: Physics-constrained adaptive learning for EUV lithography optimization: calibrates electromagnetic approximations with learnable parameters to minimize Edge Placement Error, achieving sub-nm EPE with few samples and cross-geometry generalization, outperforming CNN baselines and enabling faster post-training inference.


<details>
  <summary>Details</summary>
Motivation: EUV lithography optimization is computationally intensive and currently fails to achieve sub-nanometer precision; need efficient, accurate, real-time optimization bridging physics-informed models and industrial deployment.

Method: Develops differentiable modules for Fresnel diffraction, absorption, PSF blur, phase shifts, and contrast modulation, with learnable θ parameters; optimizes both physics calibration and EPE-based objectives; cross-geometry generalization; trained on 15 patterns with 50 samples each.

Result: Sub-nanometer EPE range (0.664-2.536 nm) across tests; ~69.9% improvement over CNN baselines without physics constraints; significant post-training inference speedup compared to full EM solvers; 90% fewer training samples vs pattern-specific CNNs.

Conclusion: Physics-constrained adaptive learning can serve as a foundational approach for real-time semiconductor manufacturing optimization, aligning physics-informed NN with industrial demands via joint physics calibration and manufacturing precision objectives.

Abstract: The semiconductor industry faces a computational crisis in extreme ultraviolet (EUV) lithography optimization, where traditional methods consume billions of CPU hours while failing to achieve sub-nanometer precision. We present a physics-constrained adaptive learning framework that automatically calibrates electromagnetic approximations through learnable parameters $\boldsymbolθ = \{θ_d, θ_a, θ_b, θ_p, θ_c\}$ while simultaneously minimizing Edge Placement Error (EPE) between simulated aerial images and target photomasks. The framework integrates differentiable modules for Fresnel diffraction, material absorption, optical point spread function blur, phase-shift effects, and contrast modulation with direct geometric pattern matching objectives, enabling cross-geometry generalization with minimal training data. Through physics-constrained learning on 15 representative patterns spanning current production to future research nodes, we demonstrate consistent sub-nanometer EPE performance (0.664-2.536 nm range) using only 50 training samples per pattern. Adaptive physics learning achieves an average improvement of 69.9\% over CNN baselines without physics constraints, with a significant inference speedup over rigorous electromagnetic solvers after training completion. This approach requires 90\% fewer training samples through cross-geometry generalization compared to pattern-specific CNN training approaches. This work establishes physics-constrained adaptive learning as a foundational methodology for real-time semiconductor manufacturing optimization, addressing the critical gap between academic physics-informed neural networks and industrial deployment requirements through joint physics calibration and manufacturing precision objectives.

</details>


### [619] [Optimal Look-back Horizon for Time Series Forecasting in Federated Learning](https://arxiv.org/abs/2511.12791)
*Dahao Tang,Nan Yang,Yanli Li,Zhiyu Zhu,Zhibo Jin,Dong Yuan*

Main category: cs.LG

TL;DR: A principled intrinsic-space framework for adaptive horizon selection in federated time series forecasting; introduces a synthetic data generator capturing autoregressive structure, seasonality, trend, and client heterogeneity; decomposes forecasting loss into irreducible Bayesian uncertainty and finite-sample/ model- capacity approximation terms; shows the optimal horizon is the smallest that saturates irreducible loss, despite increasing approximation loss beyond that point.


<details>
  <summary>Details</summary>
Motivation: In federated TSF, selecting look-back horizons is challenging due to decentralized, heterogeneous, non-IID data. Existing horizon-selection methods assume centralized settings; this work aims to provide a principled, intrinsic-space framework to adaptively choose horizons across clients.

Method: Propose a synthetic data generator (SDG) that encodes AR, seasonality, trend, and client-specific heterogeneity. Define a transformation mapping time-series windows into an intrinsic representation space with defined geometric/statistical properties. Decompose forecasting loss into a Bayesian irreducible term and an approximation term capturing finite-sample effects and limited model capacity. Prove that the total loss is minimized at the smallest horizon where the irreducible loss saturates, while the approximation loss grows.

Result: Theoretical framework showing a trade-off: longer horizons improve identifiability of deterministic patterns but worsen approximation error. The optimal horizon is the smallest one that saturates the irreducible loss. Provides a rigorous foundation for adaptive horizon selection in federated TSF.

Conclusion: Establishes a rigorous theoretical basis for adaptive horizon selection in federated time series forecasting, guiding principled horizon choices and suggesting directions for practical federated algorithms that balance identifiability and sample efficiency.

Abstract: Selecting an appropriate look-back horizon remains a fundamental challenge in time series forecasting (TSF), particularly in the federated learning scenarios where data is decentralized, heterogeneous, and often non-independent. While recent work has explored horizon selection by preserving forecasting-relevant information in an intrinsic space, these approaches are primarily restricted to centralized and independently distributed settings. This paper presents a principled framework for adaptive horizon selection in federated time series forecasting through an intrinsic space formulation. We introduce a synthetic data generator (SDG) that captures essential temporal structures in client data, including autoregressive dependencies, seasonality, and trend, while incorporating client-specific heterogeneity. Building on this model, we define a transformation that maps time series windows into an intrinsic representation space with well-defined geometric and statistical properties. We then derive a decomposition of the forecasting loss into a Bayesian term, which reflects irreducible uncertainty, and an approximation term, which accounts for finite-sample effects and limited model capacity. Our analysis shows that while increasing the look-back horizon improves the identifiability of deterministic patterns, it also increases approximation error due to higher model complexity and reduced sample efficiency. We prove that the total forecasting loss is minimized at the smallest horizon where the irreducible loss starts to saturate, while the approximation loss continues to rise. This work provides a rigorous theoretical foundation for adaptive horizon selection for time series forecasting in federated learning.

</details>


### [620] [Genomic Next-Token Predictors are In-Context Learners](https://arxiv.org/abs/2511.12797)
*Nathan Breslow,Aayush Mishra,Mahler Revsine,Michael C. Schatz,Anqi Liu,Daniel Khashabi*

Main category: cs.LG

TL;DR: Genomic ICL emerges organically during large-scale next-nucleotide prediction, mirroring linguistic ICL and suggesting a modality-agnostic meta-learning capability.


<details>
  <summary>Details</summary>
Motivation: Test whether in-context learning is a general consequence of predictive modeling beyond human language by evaluating genomic sequence models on symbolic reasoning tasks.

Method: Train Evo2 genomic model on next-nucleotide prediction; design a controlled framework with symbolic reasoning tasks in linguistic/genomic forms; compare ICL as demonstrations increase; analyze pattern induction gains.

Result: Genomic models show log-linear gains in pattern induction with more in-context demonstrations; first evidence of organically emergent ICL in genomic sequences; supports hypothesis that ICL arises from predictive modeling over rich data.

Conclusion: ICL and meta-learning are not language-specific and may be modality-agnostic; findings extend emergent meta-learning to genomic data, suggesting a unified view of in-context learning across domains.

Abstract: In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training?
  To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning.

</details>


### [621] [The Alignment Game: A Theory of Long-Horizon Alignment Through Recursive Curation](https://arxiv.org/abs/2511.12804)
*Ali Falahati,Mohammad Mohammadi Amiri,Kate Larson,Lukasz Golab*

Main category: cs.LG

TL;DR: Recursive retraining with two-stage BT-based curation yields evolving alignment outcomes with three convergence regimes and an impossibility theorem: you cannot preserve diversity, ensure symmetric influence, and be independent of initialization; alignment is an evolving equilibrium shaped by power and path dependence.


<details>
  <summary>Details</summary>
Motivation: To understand the long-term implications and risks of self-consuming generative models trained on their own outputs, particularly how curation and user preference shaping interact to affect alignment, diversity, and control.

Method: Formal analysis using a two-stage Bradley-Terry (BT) curation framework, modeling a Model Owner versus Public User and their interactions; analyzes dynamic social choice and convergence regimes; proves an impossibility theorem and discusses path dependence.

Result: Identifies three structural convergence regimes driven by preference alignment: consensus collapse, compromise on shared optima, and asymmetric refinement. Proves an impossibility theorem: no recursive BT-based curation can simultaneously preserve diversity, ensure symmetric influence, and eliminate dependence on initialization. Frames alignment as an evolving equilibrium shaped by power asymmetries and path dependence.

Conclusion: The work reframes alignment as a dynamic process rather than a static goal, highlighting tradeoffs and the central role of power dynamics and initialization, with implications for designing and regulating recursive retraining and curation mechanisms.

Abstract: In self-consuming generative models that train on their own outputs, alignment with user preferences becomes a recursive rather than one-time process. We provide the first formal foundation for analyzing the long-term effects of such recursive retraining on alignment. Under a two-stage curation mechanism based on the Bradley-Terry (BT) model, we model alignment as an interaction between two factions: the Model Owner, who filters which outputs should be learned by the model, and the Public User, who determines which outputs are ultimately shared and retained through interactions with the model. Our analysis reveals three structural convergence regimes depending on the degree of preference alignment: consensus collapse, compromise on shared optima, and asymmetric refinement. We prove a fundamental impossibility theorem: no recursive BT-based curation mechanism can simultaneously preserve diversity, ensure symmetric influence, and eliminate dependence on initialization. Framing the process as dynamic social choice, we show that alignment is not a static goal but an evolving equilibrium, shaped both by power asymmetries and path dependence.

</details>


### [622] [Expressive Temporal Specifications for Reward Monitoring](https://arxiv.org/abs/2511.12808)
*Omar Adalat,Francesco Belardinelli*

Main category: cs.LG

TL;DR: The paper uses quantitative LTL on finite traces to synthesize dense reward monitors that provide non-Markovian, dense feedback during RL training, improving on Boolean (sparse) rewards and speeding convergence.


<details>
  <summary>Details</summary>
Motivation: Sparse, long-horizon rewards impede efficient RL. The work aims to deliver dense, nuanced feedback by leveraging quantitative temporal logic to guide learning and to handle non-Markovian properties.

Method: Construct and synthesize reward monitors from quantitative LTL_f[F] that operate on observable state traces with a labeling function; monitors generate dense reward signals during training and are algorithm-agnostic, accommodating non-Markovian properties.

Result: Empirical results indicate quantitative monitors outperform Boolean monitors in maximizing a quantitative task-measure and in reducing convergence time, with performance varying by environment, and they consistently subsume Boolean semantics.

Conclusion: Quantitative LTL_f[F]-based reward monitors provide dense, informative feedback that improves RL training efficiency and can handle non-Markovian properties, in a framework compatible with any RL algorithm.

Abstract: Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\text{LTL}_f[\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.

</details>


### [623] [Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs](https://arxiv.org/abs/2511.12817)
*Shasha Zhou,Mingyu Huang,Jack Cole,Charles Britton,Ming Yin,Jan Wolber,Ke Li*

Main category: cs.LG

TL;DR: FAITH: a KG-grounded framework to evaluate the factuality of LLM medical responses without reference answers, achieving higher agreement with clinicians and robustness to wording, with explainable scoring.


<details>
  <summary>Details</summary>
Motivation: Ensure safe deployment of LLMs in high-stakes healthcare by reliable factuality verification; current evaluation methods struggle with reliability and explainability; medical knowledge graphs offer structured, verifiable evidence.

Method: Decompose LLM outputs into atomic claims, map each claim to a medical knowledge graph, and score based on evidence paths. Aggregate claim scores into an overall evaluation. Compare with human clinician judgments across diverse medical tasks, and test robustness to textual variances.

Result: KG-grounded evaluation correlates more strongly with clinician judgments than non-KG baselines and can distinguish LLMs of different capabilities; robust to wording differences; provides explainable scores indicating where factual gaps exist.

Conclusion: Leveraging medical knowledge graphs is a promising direction for automated factuality assessment in healthcare, with FAITH illustrating strengths and remaining limitations to be addressed.

Abstract: The recent proliferation of large language models (LLMs) holds the potential to revolutionize healthcare, with strong capabilities in diverse medical tasks. Yet, deploying LLMs in high-stakes healthcare settings requires rigorous verification and validation to understand any potential harm. This paper investigates the reliability and viability of using medical knowledge graphs (KGs) for the automated factuality evaluation of LLM-generated responses. To ground this investigation, we introduce FAITH, a framework designed to systematically probe the strengths and limitations of this KG-based approach. FAITH operates without reference answers by decomposing responses into atomic claims, linking them to a medical KG, and scoring them based on evidence paths. Experiments on diverse medical tasks with human subjective evaluations demonstrate that KG-grounded evaluation achieves considerably higher correlations with clinician judgments and can effectively distinguish LLMs with varying capabilities. It is also robust to textual variances. The inherent explainability of its scoring can further help users understand and mitigate the limitations of current LLMs. We conclude that while limitations exist, leveraging KGs is a prominent direction for automated factuality assessment in healthcare.

</details>


### [624] [Catastrophic Forgetting in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2511.12828)
*Mohammad Marufur Rahman,Guanchu Wang,Kaixiong Zhou,Minghan Chen,Fan Yang*

Main category: cs.LG

TL;DR: KANs offer some resistance to forgetting in low-dimensional tasks, but still forget in high-dimensional domains; a theoretical link between forgetting, activation overlap, and data dimension is established, and KAN-LoRA for efficient continual fine-tuning is proposed and evaluated.


<details>
  <summary>Details</summary>
Motivation: To clarify how Kolmogorov-Arnold Networks (KANs) behave under continual learning, understand their forgetting dynamics, and assess their practical viability for real-world high-dimensional tasks, addressing gaps about their limitations and design implications.

Method: Develop a theoretical framework linking forgetting to activation support overlap and intrinsic data dimensionality; validate with systematic experiments on synthetic and vision tasks to study forgetting dynamics across model configurations and data complexity; introduce and evaluate KAN-LoRA for parameter-efficient continual fine-tuning and knowledge editing in language models.

Result: KANs show improved retention in low-dimensional, algorithmic settings but exhibit forgetting in high-dimensional domains such as image classification and language modeling; the paper also introduces KAN-LoRA and evaluates its effectiveness for knowledge editing in language models.

Conclusion: KANs have promise in simple, low-dimensional regimes but face significant forgetting challenges in realistic, high-dimensional tasks. These findings offer practical guidance for designing continual learning systems and highlight the need for careful consideration of domain dimensionality when deploying KAN-based approaches, as well as potential utility for efficient fine-tuning via KAN-LoRA.

Abstract: Catastrophic forgetting is a longstanding challenge in continual learning, where models lose knowledge from earlier tasks when learning new ones. While various mitigation strategies have been proposed for Multi-Layer Perceptrons (MLPs), recent architectural advances like Kolmogorov-Arnold Networks (KANs) have been suggested to offer intrinsic resistance to forgetting by leveraging localized spline-based activations. However, the practical behavior of KANs under continual learning remains unclear, and their limitations are not well understood. To address this, we present a comprehensive study of catastrophic forgetting in KANs and develop a theoretical framework that links forgetting to activation support overlap and intrinsic data dimension. We validate these analyses through systematic experiments on synthetic and vision tasks, measuring forgetting dynamics under varying model configurations and data complexity. Further, we introduce KAN-LoRA, a novel adapter design for parameter-efficient continual fine-tuning of language models, and evaluate its effectiveness in knowledge editing tasks. Our findings reveal that while KANs exhibit promising retention in low-dimensional algorithmic settings, they remain vulnerable to forgetting in high-dimensional domains such as image classification and language modeling. These results advance the understanding of KANs' strengths and limitations, offering practical insights for continual learning system design.

</details>


### [625] [An Evaluation of Representation Learning Methods in Particle Physics Foundation Models](https://arxiv.org/abs/2511.12829)
*Michael Chen,Raghav Kansal,Abhijith Gandrakota,Zichun Hao,Jennifer Ngadiuba,Maria Spiropulu*

Main category: cs.LG

TL;DR: Systematic evaluation of representation learning objectives for particle physics using a unified transformer-based particle-cloud encoder, comparing supervised/self-supervised contrastive, masked modeling, and generative reconstruction objectives under a controlled setup; targeted supervised architectural tweaks achieve state-of-the-art on jet classification; provides reproducible baselines and positions foundation-model development in particle physics.


<details>
  <summary>Details</summary>
Motivation: Address how different representation learning objectives perform in particle physics, establish reproducible baselines, and enable transparent progress toward foundation models by standardizing preprocessing, sampling, and evaluation.

Method: Train a shared transformer-based particle-cloud encoder with standardized preprocessing, matched sampling, and a consistent evaluation protocol on a jet classification dataset. Systematically compare contrastive (supervised and self-supervised), masked particle modeling, and generative reconstruction objectives within a unified training regimen. Introduce targeted supervised architectural modifications to push state-of-the-art.

Result: The study isolates the effect of learning objectives, highlighting each approach's strengths and limitations. It achieves state-of-the-art performance on benchmark evaluations with the proposed architectural tweaks and provides reproducible baselines.

Conclusion: Serves as a reference point for future foundation-model development in particle physics, promoting transparent, robust progress and enabling more reliable comparisons across the community.

Abstract: We present a systematic evaluation of representation learning objectives for particle physics within a unified framework. Our study employs a shared transformer-based particle-cloud encoder with standardized preprocessing, matched sampling, and a consistent evaluation protocol on a jet classification dataset. We compare contrastive (supervised and self-supervised), masked particle modeling, and generative reconstruction objectives under a common training regimen. In addition, we introduce targeted supervised architectural modifications that achieve state-of-the-art performance on benchmark evaluations. This controlled comparison isolates the contributions of the learning objective, highlights their respective strengths and limitations, and provides reproducible baselines. We position this work as a reference point for the future development of foundation models in particle physics, enabling more transparent and robust progress across the community.

</details>


### [626] [Connectivity-Guided Sparsification of 2-FWL GNNs: Preserving Full Expressivity with Improved Efficiency](https://arxiv.org/abs/2511.12838)
*Rongqin Chen,Fan Mo,Pak Lon Ip,Shenghui Zhang,Dan Wu,Ye Li,Leong Hou U*

Main category: cs.LG

TL;DR: Co-Sparsify reduces the cubic-cost of higher-order GNNs by confining 3-node interactions to biconnected components, preserving full 2-FWL expressivity while achieving scalability; demonstrates strong empirical results on ZINC, QM9 and substructure counting tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome the O(n^3) bottleneck of 2-FWL-based HOGNNs without sacrificing expressivity, by leveraging graph topology to identify where higher-order interactions are actually needed.

Method: Introduce connectivity-aware sparsification: limit 2-node message passing to connected components and 3-node interactions to biconnected components; provide theoretical proof that the resulting GNNs are as expressive as the 2-FWL test; empirically evaluate on synthetic substructure counting tasks (PPGN) and real-world benchmarks (ZINC, QM9).

Result: Co-Sparsified GNNs achieve accuracy on substructure counting tasks comparable to or better than baselines; attain state-of-the-art performance on ZINC and QM9; demonstrate favorable scalability due to reduced computations while preserving expressivity.

Conclusion: Topology-guided sparsification can reconcile high expressivity with scalability in GNNs; preserving 2-FWL power while removing provably redundant computations leads to powerful, efficient models.

Abstract: Higher-order Graph Neural Networks (HOGNNs) based on the 2-FWL test achieve superior expressivity by modeling 2- and 3-node interactions, but at $\mathcal{O}(n^3)$ computational cost. However, this computational burden is typically mitigated by existing efficiency methods at the cost of reduced expressivity. We propose \textbf{Co-Sparsify}, a connectivity-aware sparsification framework that eliminates \emph{provably redundant} computations while preserving full 2-FWL expressive power. Our key insight is that 3-node interactions are expressively necessary only within \emph{biconnected components} -- maximal subgraphs where every pair of nodes lies on a cycle. Outside these components, structural relationships can be fully captured via 2-node message passing or global readout, rendering higher-order modeling unnecessary. Co-Sparsify restricts 2-node message passing to connected components and 3-node interactions to biconnected ones, removing computation without approximation or sampling. We prove that Co-Sparsified GNNs are as expressive as the 2-FWL test. Empirically, on PPGN, Co-Sparsify matches or exceeds accuracy on synthetic substructure counting tasks and achieves state-of-the-art performance on real-world benchmarks (ZINC, QM9). This study demonstrates that high expressivity and scalability are not mutually exclusive: principled, topology-guided sparsification enables powerful, efficient GNNs with theoretical guarantees.

</details>


### [627] [RoS-Guard: Robust and Scalable Online Change Detection with Delay-Optimal Guarantees](https://arxiv.org/abs/2511.12846)
*Zelin Zhu,Yancheng Huang,Kai Yang*

Main category: cs.LG

TL;DR: RoS-Guard is a robust, scalable online change detection method for linear systems with uncertainty. It uses a tight relaxation and neural unrolling to turn the OCD optimization into a GPU-accelerated, parallelizable procedure, with guarantees on false alarms and worst-case detection delay.


<details>
  <summary>Details</summary>
Motivation: Uncertainty in system knowledge and environmental variations degrade OCD performance; large-scale systems demand efficient, robust methods with theoretical guarantees.

Method: Formulate OCD as a robust optimization problem for linear systems, apply tight relaxation and reformulation to handle uncertainty, use neural unrolling to convert the optimization into a trainable feedforward structure for parallel GPU computation, and establish theoretical guarantees on expected false alarm rate and worst-case average detection delay.

Result: RoS-Guard achieves significant computational speedups in large-scale settings and provides theoretical performance guarantees; extensive experiments validate robustness to uncertainty and efficiency gains.

Conclusion: RoS-Guard offers a robust, optimal OCD solution that addresses both uncertainty and scalability, delivering provable performance guarantees and practical GPU-accelerated efficiency.

Abstract: Online change detection (OCD) aims to rapidly identify change points in streaming data and is critical in applications such as power system monitoring, wireless network sensing, and financial anomaly detection. Existing OCD methods typically assume precise system knowledge, which is unrealistic due to estimation errors and environmental variations. Moreover, existing OCD methods often struggle with efficiency in large-scale systems. To overcome these challenges, we propose RoS-Guard, a robust and optimal OCD algorithm tailored for linear systems with uncertainty. Through a tight relaxation and reformulation of the OCD optimization problem, RoS-Guard employs neural unrolling to enable efficient parallel computation via GPU acceleration. The algorithm provides theoretical guarantees on performance, including expected false alarm rate and worst-case average detection delay. Extensive experiments validate the effectiveness of RoS-Guard and demonstrate significant computational speedup in large-scale system scenarios.

</details>


### [628] [From Black-Box to White-Box: Control-Theoretic Neural Network Interpretability](https://arxiv.org/abs/2511.12852)
*Jihoon Moon*

Main category: cs.LG

TL;DR: A control-theoretic, local linearization framework treats a trained neural network as a nonlinear state-space system, using Gramians and Hankel singular values to quantify neuron-level controllability/observability and internal modes, enabling interpretable pruning guidance.


<details>
  <summary>Details</summary>
Motivation: Current DNN interpretability lacks principled, quantitative measures of how internal neurons and pathways contribute to inputs and outputs. The paper proposes a dynamical-systems viewpoint to quantify internal importance and energy-carrying modes via controllability/observability Gramians and Hankel singular values on a per-input local linearization.

Method: For a given input, linearize the network around the corresponding hidden activations to form a state-space model with state = hidden activations. Compute input-state and state-output Jacobians, derive local controllability and observability Gramians, and obtain Hankel singular values and modes. Interpret controllability as ease of exciting a neuron, observability as influence on output, and Hankel modes as energy-carrying internal directions. Apply to simple feedforward nets (e.g., SwiGLU 1-2-2-1 and GELU 2-3-3-2), compare operating points to show saturation effects shifting dominant modes and reducing controllability, and discuss implications for pruning or constraining for interpretability.

Result: Demonstrates that neural networks can be viewed as collections of local dynamical models; provides a principled ranking of internal neurons and modes via controllability/observability and Hankel singular values; activation saturation reduces controllability and shifts dominant modes, suggesting target directions for pruning or interpretability constraints.

Conclusion: The framework offers a principled, local dynamical perspective on DNN interpretability, enabling identification of natural internal directions for pruning or constraints. Validated on simple networks, it lays groundwork for more systematic white-box analysis, with potential extension to larger architectures and richer dynamical analyses.

Abstract: Deep neural networks achieve state of the art performance but remain difficult to interpret mechanistically. In this work, we propose a control theoretic framework that treats a trained neural network as a nonlinear state space system and uses local linearization, controllability and observability Gramians, and Hankel singular values to analyze its internal computation. For a given input, we linearize the network around the corresponding hidden activation pattern and construct a state space model whose state consists of hidden neuron activations. The input state and state output Jacobians define local controllability and observability Gramians, from which we compute Hankel singular values and associated modes. These quantities provide a principled notion of neuron and pathway importance: controllability measures how easily each neuron can be excited by input perturbations, observability measures how strongly each neuron influences the output, and Hankel singular values rank internal modes that carry input output energy. We illustrate the framework on simple feedforward networks, including a 1 2 2 1 SwiGLU network and a 2 3 3 2 GELU network. By comparing different operating points, we show how activation saturation reduces controllability, shrinks the dominant Hankel singular value, and shifts the dominant internal mode to a different subset of neurons. The proposed method turns a neural network into a collection of local white box dynamical models and suggests which internal directions are natural candidates for pruning or constraints to improve interpretability.

</details>


### [629] [An approach of deep reinforcement learning for maximizing the net present value of stochastic projects](https://arxiv.org/abs/2511.12865)
*Wei Xu,Fan Yang,Qinyuan Cui,Zhi Chen*

Main category: cs.LG

TL;DR: Double Deep Q-Network applied to stochastic project scheduling with NPV objective; achieves higher expected NPV and robust policies, especially in large-scale or highly uncertain settings.


<details>
  <summary>Details</summary>
Motivation: The abstract tackles projects with stochastic activity durations and cash flows under discrete scenarios, where precedence constraints govern cash inflows and outflows. The goal is to maximize expected net present value (NPV) by accelerating inflows and deferring outflows. This problem is hard due to uncertainty, sequencing, and large action spaces, motivating a learning-based optimization approach.

Method: Formulate the problem as a discrete-time Markov Decision Process (MDP) and propose a Double Deep Q-Network (DDQN) approach. Conduct ablation studies to examine the role of the dual-network architecture (reduces overestimation) and the target network (improves training convergence and robustness). Compare against traditional rigid and dynamic strategies, especially in large-scale or highly uncertain environments.

Result: DDQN outperforms traditional rigid and dynamic strategies, offering superior computational capability, policy reliability, and adaptability. The dual-network architecture mitigates overestimation of action values, while the target network enhances training convergence and robustness, leading to higher expected NPV in complex project optimization.

Conclusion: DDQN provides a reliable framework for stable and effective policy implementation in complex, uncertain project settings, delivering higher expected NPV and improved policy robustness compared to traditional methods.

Abstract: This paper investigates a project with stochastic activity durations and cash flows under discrete scenarios, where activities must satisfy precedence constraints generating cash inflows and outflows. The objective is to maximize expected net present value (NPV) by accelerating inflows and deferring outflows. We formulate the problem as a discrete-time Markov Decision Process (MDP) and propose a Double Deep Q-Network (DDQN) approach. Comparative experiments demonstrate that DDQN outperforms traditional rigid and dynamic strategies, particularly in large-scale or highly uncertain environments, exhibiting superior computational capability, policy reliability, and adaptability. Ablation studies further reveal that the dual-network architecture mitigates overestimation of action values, while the target network substantially improves training convergence and robustness. These results indicate that DDQN not only achieves higher expected NPV in complex project optimization but also provides a reliable framework for stable and effective policy implementation.

</details>


### [630] [On the Fundamental Limits of LLMs at Scale](https://arxiv.org/abs/2511.12869)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Zeeshan Memon,Muhammad Ibtsaam Qadir,Sagnik Bhattacharya,Hassan Rizwan,Abhiram R. Gorle,Maahe Zehra Kazmi,Ayesha Mohsin,Muhammad Usman Rafique,Zihao He,Pulkit Mehta,Muhammad Ali Jamshed,John M. Cioffi*

Main category: cs.LG

TL;DR: A theorem-backed synthesis linking LLM scaling to fundamental limits in computation, information, and learning. It shows irreducible error due to computability, bounds from information theory and sample complexity, and context/attention bottlenecks, with practical mitigations like bounded retrieval, curricula, and sparse attention.


<details>
  <summary>Details</summary>
Motivation: Current surveys describe LLM scaling phenomena empirically but lack a rigorous theoretical integration with foundational limits. A unified theory clarifies when scaling helps, saturates, or fails and guides mitigation.

Method: Develop a formal framework combining computability theory (diagonalization, undecidable queries), information-theoretic and statistical bounds (compression error, sample complexity), and geometric/context considerations (positional under-training, encoding attenuation, softmax crowding); connect training with likelihood bias; validate with theoretical results and empirical evidence; propose mitigations (bounded-oracle retrieval, positional curricula, sparse/hierarchical attention).

Result: Identifies irreducible error residues for computably enumerable models, finite-description compression limits, and long-tail knowledge challenges; shows context compression effects, pattern completion bias, retrieval drift under token limits, and shallow cross-modal alignment; maps scaling benefits and saturation regimes; offers mitigation paths.

Conclusion: Presents a theory-grounded map of LLM scaling ceilings and practical strategies to push beyond them, underscoring that scaling has fundamental ceilings and that targeted architectural/training interventions are needed to extend performance.

Abstract: Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.

</details>


### [631] [On the Information Processing of One-Dimensional Wasserstein Distances with Finite Samples](https://arxiv.org/abs/2511.12881)
*Cheongjae Jang,Jonghyun Won,Soyeon Jun,Chun Kee Chung,Keehyoung Joo,Yung-Kyun Noh*

Main category: cs.LG

TL;DR: Finite-sample 1D Wasserstein distance captures both density rate differences and support differences.


<details>
  <summary>Details</summary>
Motivation: To understand how Wasserstein distance processes information and detects pointwise density differences when supports overlap, in finite-sample settings.

Method: Model with Poisson process to isolate the rate factor; analyze information processing of the one-dimensional Wasserstein distance; validate findings on neural spike train decoding and amino acid contact frequency data.

Result: The 1D Wasserstein distance can reveal meaningful density differences tied to both rate and support, and the extracted information harmonizes density differences with support differences.

Conclusion: In finite-sample, one-dimensional Wasserstein distance is informative about both density and support structures, with potential applicability to real data analyses.

Abstract: Leveraging the Wasserstein distance -- a summation of sample-wise transport distances in data space -- is advantageous in many applications for measuring support differences between two underlying density functions. However, when supports significantly overlap while densities exhibit substantial pointwise differences, it remains unclear whether and how this transport information can accurately identify these differences, particularly their analytic characterization in finite-sample settings. We address this issue by conducting an analysis of the information processing capabilities of the one-dimensional Wasserstein distance with finite samples. By utilizing the Poisson process and isolating the rate factor, we demonstrate the capability of capturing the pointwise density difference with Wasserstein distances and how this information harmonizes with support differences. The analyzed properties are confirmed using neural spike train decoding and amino acid contact frequency data. The results reveal that the one-dimensional Wasserstein distance highlights meaningful density differences related to both rate and support.

</details>


### [632] [Method of Manufactured Learning for Solver-free Training of Neural Operators](https://arxiv.org/abs/2511.12890)
*Arth Sojitra,Omer San*

Main category: cs.LG

TL;DR: MML (Method of Manufactured Learning) trains neural operators using analytically crafted, physics-consistent datasets instead of solver-generated data, enabling solver-independent, scalable learning of operator mappings; demonstrated with Fourier neural operator on heat, advection, Burgers, and diffusion–reaction equations, achieving high spectral accuracy and good generalization.


<details>
  <summary>Details</summary>
Motivation: Data generation for neural operators is expensive and tied to specific solvers or experiments. A solver-agnostic, analytically constructed data approach can scale exploration across physical systems while preserving physical laws.

Method: Replace numerical data with analytically synthesized, smooth candidate solutions sampled from controlled analytical spaces. Derive corresponding forcing fields by applying the governing differential operators. During inference, set forcing terms to zero to recover the original PDE operator. The approach is agnostic to network architecture and can be paired with any operator learning method (demonstrated with Fourier neural operator).

Result: Achieves high spectral accuracy and low residual errors; strong generalization to unseen conditions across canonical PDE benchmarks (heat, advection, Burgers, diffusion–reaction). Provides scalable, solver-agnostic data generation while maintaining fidelity to governing laws.

Conclusion: MML offers a scalable, physically grounded pathway for training neural operators without reliance on expensive solvers or experimental data, and is compatible with various architectures and operator-learning frameworks.

Abstract: Training neural operators to approximate mappings between infinite-dimensional function spaces often requires extensive datasets generated by either demanding experimental setups or computationally expensive numerical solvers. This dependence on solver-based data limits scalability and constrains exploration across physical systems. Here we introduce the Method of Manufactured Learning (MML), a solver-independent framework for training neural operators using analytically constructed, physics-consistent datasets. Inspired by the classical method of manufactured solutions, MML replaces numerical data generation with functional synthesis, i.e., smooth candidate solutions are sampled from controlled analytical spaces, and the corresponding forcing fields are derived by direct application of the governing differential operators. During inference, setting these forcing terms to zero restores the original governing equations, allowing the trained neural operator to emulate the true solution operator of the system. The framework is agnostic to network architecture and can be integrated with any operator learning paradigm. In this paper, we employ Fourier neural operator as a representative example. Across canonical benchmarks including heat, advection, Burgers, and diffusion-reaction equations. MML achieves high spectral accuracy, low residual errors, and strong generalization to unseen conditions. By reframing data generation as a process of analytical synthesis, MML offers a scalable, solver-agnostic pathway toward constructing physically grounded neural operators that retain fidelity to governing laws without reliance on expensive numerical simulations or costly experimental data for training.

</details>


### [633] [Functional Mean Flow in Hilbert Space](https://arxiv.org/abs/2511.12898)
*Zhiqi Li,Yuchen Sun,Greg Turk,Bo Zhu*

Main category: cs.LG

TL;DR: FMF is a one-step generative model in infinite-dimensional Hilbert space that extends Mean Flow to functional data, introducing Functional Flow Matching and an x1-prediction variant for improved stability, enabling practical one-step generation for time series, images, PDEs, and 3D geometry.


<details>
  <summary>Details</summary>
Motivation: Need for efficient, scalable one-step generative models capable of handling functional/infinite-dimensional data and ensuring stable training, with broad applicability across domains.

Method: Introduce Functional Mean Flow (FMF), a one-step Flow Matching framework defined in a Hilbert space; provide a theoretical formulation for Functional Flow Matching and a practical training/sampling algorithm; propose an x1-prediction variant that improves stability over the original u-prediction.

Result: A practical one-step Flow Matching method for functional data generation with demonstrated stability benefits and applicability to diverse data types (time series, images, PDEs, 3D geometry).

Conclusion: FMF extends one-step Flow Matching to functional domains, delivering a unified, scalable framework for functional data generation and improved stability via the x1-prediction variant.

Abstract: We present Functional Mean Flow (FMF) as a one-step generative model defined in infinite-dimensional Hilbert space. FMF extends the one-step Mean Flow framework to functional domains by providing a theoretical formulation for Functional Flow Matching and a practical implementation for efficient training and sampling. We also introduce an $x_1$-prediction variant that improves stability over the original $u$-prediction form. The resulting framework is a practical one-step Flow Matching method applicable to a wide range of functional data generation tasks such as time series, images, PDEs, and 3D geometry.

</details>


### [634] [Contrastive Entropy Bounds for Density and Conditional Density Decomposition](https://arxiv.org/abs/2511.12903)
*Bo Hu,Jose C. Principe*

Main category: cs.LG

TL;DR: Reframes neural network interpretability via a Bayesian Gaussian lens: autoencoders maximize a trace-based objective of a Gaussian operator, MDNs use its nuclear norm; Hilbert-space bounds and a multi-center encoder-decoder are proposed to tighten bounds and increase diversity.


<details>
  <summary>Details</summary>
Motivation: Understand interpretability and training objectives for neural features by connecting probabilistic bounds, Gaussian mixtures, and Hilbert-space decompositions; extend beyond KL bounds and address multi-output/multi-center scenarios.

Method: Analyze with Gaussian mixture densities for MDNs and autoencoders; interpret training objectives as maximizing trace or moving to nuclear norm; derive bounds using inner products/norms in a Hilbert space; propose encoder-mixture-decoder architecture with multiple centers; assume small-variance Gaussian mixtures to track bounds.

Result: Autoencoder objective equals maximizing trace of Gaussian operator (sum of eigenvalues in appropriate basis); nuclear norm of the operator can be used as a divergence for MDNs; extra norm-based bounds increase sample diversity and avoid trivial solutions in multi-output networks; proposed a multi-center decoder to tighten bounds and enable quantitative tracking in small-variance Gaussian mixtures.

Conclusion: A pragmatic Gaussian/Bayesian framing yields alternative training objectives (trace or nuclear norm) and a multi-center architecture to tighten bounds; this provides a quantitative, divergence-based approach to training and evaluating interpretable neural features in mixtures.

Abstract: This paper studies the interpretability of neural network features from a Bayesian Gaussian view, where optimizing a cost is reaching a probabilistic bound; learning a model approximates a density that makes the bound tight and the cost optimal, often with a Gaussian mixture density. The two examples are Mixture Density Networks (MDNs) using the bound for the marginal and autoencoders using the conditional bound. It is a known result, not only for autoencoders, that minimizing the error between inputs and outputs maximizes the dependence between inputs and the middle.
  We use Hilbert space and decomposition to address cases where a multiple-output network produces multiple centers defining a Gaussian mixture. Our first finding is that an autoencoder's objective is equivalent to maximizing the trace of a Gaussian operator, the sum of eigenvalues under bases orthonormal w.r.t. the data and model distributions. This suggests that, when a one-to-one correspondence as needed in autoencoders is unnecessary, we can instead maximize the nuclear norm of this operator, the sum of singular values, to maximize overall rank rather than trace. Thus the trace of a Gaussian operator can be used to train autoencoders, and its nuclear norm can be used as divergence to train MDNs.
  Our second test uses inner products and norms in a Hilbert space to define bounds and costs. Such bounds often have an extra norm compared to KL-based bounds, which increases sample diversity and prevents the trivial solution where a multiple-output network produces the same constant, at the cost of requiring a sample batch to estimate and optimize. We propose an encoder-mixture-decoder architecture whose decoder is multiple-output, producing multiple centers per sample, potentially tightening the bound. Assuming the data are small-variance Gaussian mixtures, this upper bound can be tracked and analyzed quantitatively.

</details>


### [635] [LinkedIn Profile Characteristics and Professional Success Indicators](https://arxiv.org/abs/2511.12905)
*Tania-Amanda Fredrick Eneye,Ashlesha Malla,Pawan Paudel*

Main category: cs.LG

TL;DR: LinkedIn profile features can predict professional success; promotions are highly predictable while follower growth is more complex, based on ML analysis of 62k anonymized profiles, yielding actionable optimization guidance.


<details>
  <summary>Details</summary>
Motivation: To understand how online professional signals translate into real-world career outcomes and to help individuals and organizations optimize LinkedIn strategy and career planning.

Method: Large-scale analysis of over 62,000 anonymized LinkedIn profiles. Built machine learning predictive models to identify factors driving promotions, follower count, and career progression rate; evaluated predictive performance and feature importance.

Result: Promotions are highly predictable; follower growth exhibits greater complexity; the models identify influential indicators and offer actionable insights for optimizing LinkedIn presence and career strategy.

Conclusion: The study provides practical guidance for professionals seeking to optimize their LinkedIn presence and pursue effective career strategies.

Abstract: This study explores the relationship between LinkedIn profile characteristics and professional success, focusing on the indicators of promotions, follower count, and career progression rate. By leveraging a dataset of over 62,000 anonymized LinkedIn profiles, we developed predictive models using machine learning techniques to identify the most influential factors driving professional success. Results indicate that while promotions are highly predictable, follower growth exhibits greater complexity. This research provides actionable insights for professionals seeking to optimize their LinkedIn presence and career strategies.

</details>


### [636] [AIF: Asynchronous Inference Framework for Cost-Effective Pre-Ranking](https://arxiv.org/abs/2511.12934)
*Zhi Kou,Xiang-Rong Sheng,Shuguang Han,Zhishan Zhao,Yueyao Cheng,Han Zhu,Jian Xu,Bo Zheng*

Main category: cs.LG

TL;DR: AIF decouples interaction-independent components from real-time pre-ranking inference to reduce redundant computation and latency in industrial recommendation systems; enables parallel user-side and nearline item-side processing; uses approximations for interaction-dependent parts; deployed in Taobao display ads.


<details>
  <summary>Details</summary>
Motivation: Address bottlenecks of sequential feature fetching and forward computation by removing interaction coupling and enabling asynchronous inference.

Method: Propose Asynchronous Inference Framework (AIF) that computes user-side components in parallel with retrieval, computes item-side components nearline, and computes interaction-dependent components using approximations online; co-design framework and models.

Result: Improved computational efficiency and reduced latency; richer feature sets and model architectures for interaction-independent components; notable performance gains; successful deployment in Taobao display advertising.

Conclusion: Asynchronous, decoupled inference with approximated interaction-dependent predictions yields practical, scalable performance gains and can be deployed in large-scale industrial systems.

Abstract: In industrial recommendation systems, pre-ranking models based on deep neural networks (DNNs) commonly adopt a sequential execution framework: feature fetching and model forward computation are triggered only after receiving candidates from the upstream retrieval stage. This design introduces inherent bottlenecks, including redundant computations of identical users/items and increased latency due to strictly sequential operations, which jointly constrain the model's capacity and system efficiency. To address these limitations, we propose the Asynchronous Inference Framework (AIF), a cost-effective computational architecture that decouples interaction-independent components, those operating within a single user or item, from real-time prediction. AIF reorganizes the model inference process by performing user-side computations in parallel with the retrieval stage and conducting item-side computations in a nearline manner. This means that interaction-independent components are calculated just once and completed before the real-time prediction phase of the pre-ranking stage. As a result, AIF enhances computational efficiency and reduces latency, freeing up resources to significantly improve the feature set and model architecture of interaction-independent components. Moreover, we delve into model design within the AIF framework, employing approximated methods for interaction-dependent components in online real-time predictions. By co-designing both the framework and the model, our solution achieves notable performance gains without significantly increasing computational and latency costs. This has enabled the successful deployment of AIF in the Taobao display advertising system.

</details>


### [637] [APT: Affine Prototype-Timestamp For Time Series Forecasting Under Distribution Shift](https://arxiv.org/abs/2511.12945)
*Yujie Li,Zezhi Shao,Chengqing Yu,Yisong Fu,Tao Sun,Yongjun Xu,Fei Wang*

Main category: cs.LG

TL;DR: Affine Prototype Timestamp (APT) is a lightweight plug-in that injects global distribution features into normalization-forecasting by using timestamp-conditioned prototypes to dynamically generate affine parameters, improving forecasting under distribution shift across various backbones with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Time-series forecasting under distribution shift is hard because local normalization misses global distribution changes. Existing methods like RevIN decouple distribution and pattern but struggle with missing values, noisy observations, and channel-wise affine limitations. APT aims to capture global distribution information and adapt parameters dynamically.

Method: Introduce APT as a timestamp-conditioned prototype learning module that generates affine parameters to modulate both input and output series. It acts as a plug-in to the normalization-forecasting pipeline, supports self-supervised learning over distribution-aware clustered instances, and is compatible with arbitrary forecasting backbones and normalization strategies while adding minimal computational overhead.

Result: Empirical evaluation on six benchmark datasets across multiple backbone-normalization combinations shows that APT significantly improves forecasting performance under distribution shift, with robust gains and low additional cost.

Conclusion: APT provides a universal, flexible, and lightweight mechanism to incorporate distribution-aware, globally informed features into forecasting models, enabling better handling of distribution shift without major changes to existing architectures.

Abstract: Time series forecasting under distribution shift remains challenging, as existing deep learning models often rely on local statistical normalization (e.g., mean and variance) that fails to capture global distribution shift. Methods like RevIN and its variants attempt to decouple distribution and pattern but still struggle with missing values, noisy observations, and invalid channel-wise affine transformation. To address these limitations, we propose Affine Prototype Timestamp (APT), a lightweight and flexible plug-in module that injects global distribution features into the normalization-forecasting pipeline. By leveraging timestamp conditioned prototype learning, APT dynamically generates affine parameters that modulate both input and output series, enabling the backbone to learn from self-supervised, distribution-aware clustered instances. APT is compatible with arbitrary forecasting backbones and normalization strategies while introducing minimal computational overhead. Extensive experiments across six benchmark datasets and multiple backbone-normalization combinations demonstrate that APT significantly improves forecasting performance under distribution shift.

</details>


### [638] [A FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series](https://arxiv.org/abs/2511.12951)
*Ziling Fan,Ruijia Liang,Yiwen Hu*

Main category: cs.LG

TL;DR: A FEDformer-based hybrid framework for anomaly detection and risk forecasting in financial time series, combining frequency-domain decomposition with a residual anomaly detector and a risk head, achieving significant RMSE and F1 improvements on major datasets.


<details>
  <summary>Details</summary>
Motivation: Financial markets are highly volatile and nonstationary; traditional models like LSTM/GRU struggle with long-term dependencies and periodic patterns. Accurate anomaly detection and early risk forecasting are essential to prevent systemic risk and inform investment decisions.

Method: Utilizes a FEDformer module to model dynamics in time and frequency, decomposing signals into trend and seasonal components; a residual-based anomaly detector analyzes prediction errors; a risk forecasting head uses learned latent embeddings to predict financial distress; evaluated end-to-end on S&P 500, NASDAQ Composite, and Brent Crude Oil datasets (2000-2024).

Result: Outperforms benchmark methods with a 15.7% RMSE reduction and an 11.5% improvement in F1-score for anomaly detection, demonstrating effective capture of financial volatility and enabling reliable early-warning for crashes and risk management.

Conclusion: The approach effectively captures volatility and supports reliable early warning and risk management in financial markets; shows promise for real-time deployment and risk monitoring.

Abstract: Financial markets are inherently volatile and prone to sudden disruptions such as market crashes, flash collapses, and liquidity crises. Accurate anomaly detection and early risk forecasting in financial time series are therefore crucial for preventing systemic instability and supporting informed investment decisions. Traditional deep learning models, such as LSTM and GRU, often fail to capture long-term dependencies and complex periodic patterns in highly nonstationary financial data. To address this limitation, this study proposes a FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series, which integrates the Frequency Enhanced Decomposed Transformer (FEDformer) with a residual-based anomaly detector and a risk forecasting head. The FEDformer module models temporal dynamics in both time and frequency domains, decomposing signals into trend and seasonal components for improved interpretability. The residual-based detector identifies abnormal fluctuations by analyzing prediction errors, while the risk head predicts potential financial distress using learned latent embeddings. Experiments conducted on the S&P 500, NASDAQ Composite, and Brent Crude Oil datasets (2000-2024) demonstrate the superiority of the proposed model over benchmark methods, achieving a 15.7 percent reduction in RMSE and an 11.5 percent improvement in F1-score for anomaly detection. These results confirm the effectiveness of the model in capturing financial volatility, enabling reliable early-warning systems for market crash prediction and risk management.

</details>


### [639] [Global Cross-Time Attention Fusion for Enhanced Solar Flare Prediction from Multivariate Time Series](https://arxiv.org/abs/2511.12955)
*Onur Vural,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: A transformer-based Global Cross-Time Attention Fusion (GCTAF) uses learnable global tokens and cross-attention to summarize long-range temporal patterns for imbalanced solar flare prediction, improving detection of intense flares.


<details>
  <summary>Details</summary>
Motivation: Solar flare events are highly imbalanced; predicting intense flares requires capturing non-local temporal dynamics; traditional self-attention is largely local and may miss global patterns.

Method: Introduce GCTAF: a transformer architecture that adds learnable cross-time global tokens; cross-attention between tokens and input sequence to summarize globally salient time points; fuse back into temporal representations; dynamic attention-driven summarizer; tested on benchmark solar flare dataset.

Result: GCTAF improves predictive performance and enhances detection of intense flares on the benchmark dataset, indicating the value of global temporal summarization for solar flare prediction.

Conclusion: Refining transformer architectures with global cross-time attention tokens is a promising direction for improving multivariate time series forecasting in space weather and imbalanced flare prediction.

Abstract: Multivariate time series classification is increasingly investigated in space weather research as a means to predict intense solar flare events, which can cause widespread disruptions across modern technological systems. Magnetic field measurements of solar active regions are converted into structured multivariate time series, enabling predictive modeling across segmented observation windows. However, the inherently imbalanced nature of solar flare occurrences, where intense flares are rare compared to minor flare events, presents a significant barrier to effective learning. To address this challenge, we propose a novel Global Cross-Time Attention Fusion (GCTAF) architecture, a transformer-based model to enhance long-range temporal modeling. Unlike traditional self-attention mechanisms that rely solely on local interactions within time series, GCTAF injects a set of learnable cross-attentive global tokens that summarize salient temporal patterns across the entire sequence. These tokens are refined through cross-attention with the input sequence and fused back into the temporal representation, enabling the model to identify globally significant, non-contiguous time points that are critical for flare prediction. This mechanism functions as a dynamic attention-driven temporal summarizer that augments the model's capacity to capture discriminative flare-related dynamics. We evaluate our approach on the benchmark solar flare dataset and show that GCTAF effectively detects intense flares and improves predictive performance, demonstrating that refining transformer-based architectures presents a high-potential alternative for solar flare prediction tasks.

</details>


### [640] [RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems](https://arxiv.org/abs/2511.12979)
*Zhengchao Wang,Yitao Hu,Jianing Ye,Zhuxuan Chang,Jiazheng Yu,Youpeng Deng,Keqiu Li*

Main category: cs.LG

TL;DR: RAGPulse provides a real-world RAG workload trace from a university Q&A platform, enabling optimization strategies (content-aware batching, retrieval caching) and bridging the gap between academia and deployment.


<details>
  <summary>Details</summary>
Motivation: Existing LLM inference traces do not capture RAG-specific dynamics, leading to a gap between research and production deployment; a realistic RAG workload trace is needed to guide performance optimization.

Method: Collected from a university-wide Q&A system serving 40k+ users since Apr 2024; describes system architecture and a privacy-preserving hash-based data format; includes in-depth statistical analysis of access patterns; dataset and code released publicly.

Result: A high-fidelity RAG workload trace dataset revealing strong temporal locality and skewed hot-document access, enabling development and validation of optimization strategies such as content-aware batching and retrieval caching.

Conclusion: RAGPulse provides a foundation for researchers to improve the efficiency and reliability of RAG services and helps close the gap between research and real-world deployment.

Abstract: Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse.

</details>


### [641] [Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks](https://arxiv.org/abs/2511.12985)
*Minsoo Jo,Dongyoon Yang,Taesup Kim*

Main category: cs.LG

TL;DR: A geometry-aware adversarial attack in hyperbolic space that perturbs angular (semantic) directions in the tangent space, yielding higher fooling rates across image classification and cross-modal tasks while leveraging hyperbolic geometry.


<details>
  <summary>Details</summary>
Motivation: Hyperbolic representations capture hierarchical structure; traditional adversarial methods ignore the non-Euclidean geometry, risking inefficient or inconsistent attacks.

Method: Compute the loss gradient in the tangent space of hyperbolic space, decompose into radial (depth) and angular (semantic) components, and perturb only along the angular direction to produce semantically meaningful adversarial examples.

Result: Empirically achieves higher fooling rates than conventional attacks on image classification, cross-modal retrieval, and various architectures, revealing vulnerabilities of hyperbolic embeddings.

Conclusion: Geometry-aware adversarial strategies are essential in curved representation spaces; provides a principled framework for attacking hierarchical embeddings.

Abstract: Adversarial examples in neural networks have been extensively studied in Euclidean geometry, but recent advances in \textit{hyperbolic networks} call for a reevaluation of attack strategies in non-Euclidean geometries. Existing methods such as FGSM and PGD apply perturbations without regard to the underlying hyperbolic structure, potentially leading to inefficient or geometrically inconsistent attacks. In this work, we propose a novel adversarial attack that explicitly leverages the geometric properties of hyperbolic space. Specifically, we compute the gradient of the loss function in the tangent space of hyperbolic space, decompose it into a radial (depth) component and an angular (semantic) component, and apply perturbation derived solely from the angular direction. Our method generates adversarial examples by focusing perturbations in semantically sensitive directions encoded in angular movement within the hyperbolic geometry. Empirical results on image classification, cross-modal retrieval tasks and network architectures demonstrate that our attack achieves higher fooling rates than conventional adversarial attacks, while producing high-impact perturbations with deeper insights into vulnerabilities of hyperbolic embeddings. This work highlights the importance of geometry-aware adversarial strategies in curved representation spaces and provides a principled framework for attacking hierarchical embeddings.

</details>


### [642] [Learning Branching Policies for MILPs with Proximal Policy Optimization](https://arxiv.org/abs/2511.12986)
*Abdelouahed Ben Mhamed,Assia Kamal-Idrissi,Amal El Fallah Seghrouchni*

Main category: cs.LG

TL;DR: Introduces Tree-Gate PPO (TGPPO) to learn branching policies for MILP using PPO RL, achieving better generalization and efficiency, especially on out-of-distribution instances.


<details>
  <summary>Details</summary>
Motivation: Imitation Learning models for branching tend to overfit to expert demonstrations and struggle to generalize to structurally diverse MILPs. A reinforcement learning approach with a robust state representation can yield more generalizable branching strategies.

Method: Proposes Tree-Gate PPO (TGPPO), a reinforcement learning framework that trains a branching policy using Proximal Policy Optimization. It uses a parameterized, dynamic state space that captures evolving search-tree context and a tree-gate mechanism to adapt decisions as the tree grows.

Result: Empirical evaluations show TGPPO often outperforms existing learning-based policies by reducing the number of nodes explored and improving p-Primal-Dual Integrals (PDI), with particularly strong performance on out-of-distribution instances.

Conclusion: RL-based branching policies with tree-context gating can generalize better across heterogeneous MILP instances and offer practical improvements for MILP solvers; this points to the potential of reinforcement learning for robust branching strategies.

Abstract: Branch-and-Bound (B\&B) is the dominant exact solution method for Mixed Integer Linear Programs (MILP), yet its exponential time complexity poses significant challenges for large-scale instances. The growing capabilities of machine learning have spurred efforts to improve B\&B by learning data-driven branching policies. However, most existing approaches rely on Imitation Learning (IL), which tends to overfit to expert demonstrations and struggles to generalize to structurally diverse or unseen instances. In this work, we propose Tree-Gate Proximal Policy Optimization (TGPPO), a novel framework that employs Proximal Policy Optimization (PPO), a Reinforcement Learning (RL) algorithm, to train a branching policy aimed at improving generalization across heterogeneous MILP instances. Our approach builds on a parameterized state space representation that dynamically captures the evolving context of the search tree. Empirical evaluations show that TGPPO often outperforms existing learning-based policies in terms of reducing the number of nodes explored and improving p-Primal-Dual Integrals (PDI), particularly in out-of-distribution instances. These results highlight the potential of RL to develop robust and adaptable branching strategies for MILP solvers.

</details>


### [643] [Are Graph Transformers Necessary? Efficient Long-Range Message Passing with Fractal Nodes in MPNNs](https://arxiv.org/abs/2511.13010)
*Jeongwhan Choi,Seungjun Park,Sumin Park,Sung-Bae Cho,Noseong Park*

Main category: cs.LG

TL;DR: Introduce fractal nodes that co-exist with original nodes to capture subgraph-level structure and enable long-range information flow in MPNNs, mitigating over-squashing and delivering performance competitive with graph Transformers while preserving MPNN efficiency.


<details>
  <summary>Details</summary>
Motivation: There is a need to balance local and global information in graph learning. Graph Transformers capture long-range interactions but can neglect locality and efficiency, while MPNNs are efficient but suffer from over-squashing. Fractal structures observed in real networks and the idea that graph partitioning induces fractal patterns motivate introducing a new node type (fractal nodes) to propagate subgraph-level signals.

Method: Propose fractal nodes that co-exist with original nodes and adaptively aggregate subgraph-level feature representations. They enforce feature similarity within each subgraph and provide direct shortcut connections to enable long-range propagation of subgraph-level representations, thereby integrating subgraph cues into the MPNN framework.

Result: Empirical results show that fractal nodes improve the expressive power of MPNNs and achieve comparable or better performance to graph Transformers while maintaining the computational efficiency of MPNNs by enhancing long-range dependencies.

Conclusion: Fractal nodes provide a principled mechanism to inject subgraph-level information into MPNNs, alleviating over-squashing and enabling efficient long-range propagation. This yields competitive performance with graph Transformers without sacrificing MPNN efficiency.

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning on graph-structured data, but often struggle to balance local and global information. While graph Transformers aim to address this by enabling long-range interactions, they often overlook the inherent locality and efficiency of Message Passing Neural Networks (MPNNs). We propose a new concept called fractal nodes, inspired by the fractal structure observed in real-world networks. Our approach is based on the intuition that graph partitioning naturally induces fractal structure, where subgraphs often reflect the connectivity patterns of the full graph. Fractal nodes are designed to coexist with the original nodes and adaptively aggregate subgraph-level feature representations, thereby enforcing feature similarity within each subgraph. We show that fractal nodes alleviate the over-squashing problem by providing direct shortcut connections that enable long-range propagation of subgraph-level representations. Experiment results show that our method improves the expressive power of MPNNs and achieves comparable or better performance to graph Transformers while maintaining the computational efficiency of MPNN by improving the long-range dependencies of MPNN.

</details>


### [644] [The Good, The Bad, and The Hybrid: A Reward Structure Showdown in Reasoning Models Training](https://arxiv.org/abs/2511.13016)
*Subramanyam Sahoo*

Main category: cs.LG

TL;DR: Adaptive hybrid reward framework for RLHF in LLM math reasoning improves convergence speed and stability.


<details>
  <summary>Details</summary>
Motivation: To improve reward design in RLHF for aligning LLMs on mathematical reasoning tasks by combining discrete and continuous signals.

Method: Formalize reward formulations incorporating correctness, perplexity, reasoning quality, and consistency; implement an adaptive hybrid reward scheduler that transitions between discrete and continuous signals; validate using Qwen3-4B with LoRA on GSM8K.

Result: Hybrid rewards converge faster and with greater stability than purely hard or purely continuous rewards; the adaptive scheduler effectively balances exploration and stability during training.

Conclusion: Adaptive reward modeling that blends discrete and continuous signals enhances RLHF alignment for LLMs on mathematical reasoning tasks, suggesting broader applicability beyond purely hard or soft reward designs.

Abstract: Reward design is central to reinforcement learning from human feedback (RLHF) and alignment research. In this work, we propose a unified framework to study hard, continuous, and hybrid reward structures for fine-tuning large language models (LLMs) on mathematical reasoning tasks. Using Qwen3-4B with LoRA fine-tuning on the GSM8K dataset, we formalize and empirically evaluate reward formulations that incorporate correctness, perplexity, reasoning quality, and consistency. We introduce an adaptive hybrid reward scheduler that transitions between discrete and continuous signals, balancing exploration and stability. Our results show that hybrid reward structures improve convergence speed and training stability over purely hard or continuous approaches, offering insights for alignment via adaptive reward modeling.

</details>


### [645] [The Final-Stage Bottleneck: A Systematic Dissection of the R-Learner for Network Causal Inference](https://arxiv.org/abs/2511.13018)
*Sairam S,Sara Girdhar,Shivam Soni*

Main category: cs.LG

TL;DR: Graph-based R-Learner exhibits a catastrophic representation bottleneck when using a graph-blind final-stage, requiring end-to-end Graph R-Learner to achieve strong performance; nuisance bottlenecks linked to GNN oversquashing are also identified.


<details>
  <summary>Details</summary>
Motivation: To systematically dissect the R-Learner framework on graph data, identify what drives performance, and determine whether final-stage modeling or nuisance components limit effectiveness.

Method: Large-scale empirical study across diverse synthetic and semi-synthetic benchmarks, comparing graph-blind vs. end-to-end Graph R-Learner approaches; analysis of inductive bias; Hub-Periphery Trade-off to explain nuisance bottlenecks; release of reproducible benchmark code.

Result: Graph-blind final-stage R-Learners fail completely (MSE > 4.0) even with strong GNN nuisance models; end-to-end Graph R-Learner significantly outperforms a strong non-DML GNN T-Learner baseline; identification of a representation bottleneck and a topology-dependent nuisance bottleneck linked to GNN over-squashing; results validated across synthetic benchmarks; code released for reproducibility.

Conclusion: The final-stage model choice is critical for graph data; graph-aware, end-to-end R-Learner is necessary to overcome representation and nuisance bottlenecks; the study provides mechanistic explanations and a reproducible benchmark to advance future work.

Abstract: The R-Learner is a powerful, theoretically-grounded framework for estimating heterogeneous treatment effects, prized for its robustness to nuisance model errors. However, its application to network data, where causal heterogeneity is often graph-dependent, presents a critical challenge to its core assumption of a well-specified final-stage model. In this paper, we conduct a large-scale empirical study to systematically dissect the R-Learner framework on graphs. We provide the first rigorous evidence that the primary driver of performance is the inductive bias of the final-stage CATE estimator, an effect that dominates the choice of nuisance models. Our central finding is the quantification of a catastrophic "representation bottleneck": we prove with overwhelming statistical significance (p < 0.001) that R-Learners with a graph-blind final stage fail completely (MSE > 4.0), even when paired with powerful GNN nuisance models. Conversely, our proposed end-to-end Graph R-Learner succeeds and significantly outperforms a strong, non-DML GNN T-Learner baseline. Furthermore, we identify and provide a mechanistic explanation for a subtle, topology-dependent "nuisance bottleneck," linking it to GNN over-squashing via a targeted "Hub-Periphery Trade-off" analysis. Our findings are validated across diverse synthetic and semi-synthetic benchmarks. We release our code as a reproducible benchmark to facilitate future research on this critical "final-stage bottleneck."

</details>


### [646] [Learning Time-Scale Invariant Population-Level Neural Representations](https://arxiv.org/abs/2511.13022)
*Eshani Patel,Yisong Yue,Geeling Chau*

Main category: cs.LG

TL;DR: TSAP improves robustness to time-scale mismatches in neural time-series representations, achieving invariance and better generalization across tasks.


<details>
  <summary>Details</summary>
Motivation: Existing population-level representations learned across channels are sensitive to preprocessing time-scale differences between pretraining and downstream tasks; lack of invariance hampers generalization.

Method: Time-scale Augmented Pretraining (TSAP): apply time-scale augmentations during pretraining of population-level representations atop pretrained temporal encoders; evaluate on multiple decoding tasks to test cross-time-scale generalization.

Result: TSAP consistently improves robustness to different time-scales across decoding tasks and fosters invariance in representation space, enabling better transfer under time-scale variations.

Conclusion: Accounting for preprocessing diversity, particularly time-scale variations, is essential for building generalizable neural foundation models; TSAP provides a practical approach to achieve time-scale invariance in population representations.

Abstract: General-purpose foundation models for neural time series can help accelerate neuroscientific discoveries and enable applications such as brain computer interfaces (BCIs). A key component in scaling these models is population-level representation learning, which leverages information across channels to capture spatial as well as temporal structure. Population-level approaches have recently shown that such representations can be both efficient to learn on top of pretrained temporal encoders and produce useful representations for decoding a variety of downstream tasks. However, these models remain sensitive to mismatches in preprocessing, particularly on time-scales, between pretraining and downstream settings. We systematically examine how time-scale mismatches affects generalization and find that existing representations lack invariance. To address this, we introduce Time-scale Augmented Pretraining (TSAP), which consistently improves robustness to different time-scales across decoding tasks and builds invariance in the representation space. These results highlight handling preprocessing diversity as a key step toward building generalizable neural foundation models.

</details>


### [647] [SLMQuant:Benchmarking Small Language Model Quantization for Practical Deployment](https://arxiv.org/abs/2511.13023)
*Jiacheng Wang,Yejun Zeng,Jinyang Guo,Yuqing Ma,Aishan Liu,Xianglong Liu*

Main category: cs.LG

TL;DR: SLMQuant benchmarks quantization for Small Language Models, showing that LLM-optimized quantization methods do not transfer well to SLMs; identifies factors and design principles for SLM-specific compression and provides a framework for edge deployment.


<details>
  <summary>Details</summary>
Motivation: There is growing interest in resource-efficient Small Language Models and a need to deploy them on edge devices. While quantization is effective for LLMs, its applicability to SLMs is understudied, and there is a gap in understanding the unique bottlenecks and efficiency profiles of SLMs.

Method: Introduce SLMQuant, a systematic benchmark for evaluating how LLM compression techniques perform on SLMs. Conduct multi-track evaluations across diverse architectures and tasks, compare state-of-the-art quantization methods, analyze quantization sensitivity, identify bottlenecks, and derive design principles for SLM-tailored compression.

Result: SLMQuant reveals fundamental disparities between SLMs and LLMs in quantization sensitivity; directly transferring LLM-optimized techniques yields suboptimal results due to SLMs’ unique architectures and training dynamics. The work identifies key factors that govern effective SLM quantization and proposes actionable design principles, establishing a foundational framework for efficient SLM deployment on edge devices.

Conclusion: SLMQuant provides a foundational framework for advancing efficient SLM deployment on low-end devices and offers critical insights for deploying lightweight language models in resource-constrained scenarios, guiding both research and practical compression strategies.

Abstract: Despite the growing interest in Small Language Models (SLMs) as resource-efficient alternatives to Large Language Models (LLMs), their deployment on edge devices remains challenging due to unresolved efficiency gaps in model compression. While quantization has proven effective for LLMs, its applicability to SLMs is significantly underexplored, with critical questions about differing quantization bottlenecks and efficiency profiles. This paper introduces SLMQuant, the first systematic benchmark for evaluating LLM compression techniques when applied to SLMs. Through comprehensive multi-track evaluations across diverse architectures and tasks, we analyze how state-of-the-art quantization methods perform on SLMs. Our findings reveal fundamental disparities between SLMs and LLMs in quantization sensitivity, demonstrating that direct transfer of LLM-optimized techniques leads to suboptimal results due to SLMs' unique architectural characteristics and training dynamics. We identify key factors governing effective SLM quantization and propose actionable design principles for SLM-tailored compression. SLMQuant establishes a foundational framework for advancing efficient SLM deployment on low-end devices in edge applications, and provides critical insights for deploying lightweight language models in resource-constrained scenarios.

</details>


### [648] [One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow](https://arxiv.org/abs/2511.13035)
*Zeyuan Wang,Da Li,Yulin Chen,Ye Shi,Liang Bai,Tianyuan Yu,Yanwei Fu*

Main category: cs.LG

TL;DR: A new one-step, noise-to-action policy for offline RL that unifies velocity estimation and noise transformation into a single residual model, enabling expressive, multimodal actions and stable Q-learning without two-stage training. It extends MeanFlow to a one-step framework, showing strong results on OGBench and D4RL across offline and offline-to-online settings.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing one-step Gaussian policies (poor multimodal expressivity) and flow-based methods (often require distillation/two-stage training) by creating a direct noise-to-action policy that is compatible with Q-learning and supports multimodal action distributions in a single-stage training setup.

Method: Reformulate MeanFlow into a residual, single-network policy that integrates the velocity field and noise-to-action transform, eliminating separate velocity estimation and distillation steps. Explore and select an effective residual formulation that enables expressive, stable learning, enabling one-step generation of actions from noise.

Result: Demonstrates that the proposed residual MeanFlowQL achieves strong performance on 73 tasks from OGBench and D4RL, in both offline and offline-to-online RL, with efficient one-step generation and stable Q-learning.

Conclusion: A single-stage, noise-to-action policy for offline RL that combines expressivity for multimodal actions with efficient, stable Q-learning, reducing training complexity while maintaining performance across diverse benchmarks.

Abstract: We introduce a one-step generative policy for offline reinforcement learning that maps noise directly to actions via a residual reformulation of MeanFlow, making it compatible with Q-learning. While one-step Gaussian policies enable fast inference, they struggle to capture complex, multimodal action distributions. Existing flow-based methods improve expressivity but typically rely on distillation and two-stage training when trained with Q-learning. To overcome these limitations, we propose to reformulate MeanFlow to enable direct noise-to-action generation by integrating the velocity field and noise-to-action transformation into a single policy network-eliminating the need for separate velocity estimation. We explore several reformulation variants and identify an effective residual formulation that supports expressive and stable policy learning. Our method offers three key advantages: 1) efficient one-step noise-to-action generation, 2) expressive modelling of multimodal action distributions, and 3) efficient and stable policy learning via Q-learning in a single-stage training setup. Extensive experiments on 73 tasks across the OGBench and D4RL benchmarks demonstrate that our method achieves strong performance in both offline and offline-to-online reinforcement learning settings. Code is available at https://github.com/HiccupRL/MeanFlowQL.

</details>


### [649] [Bi-View Embedding Fusion: A Hybrid Learning Approach for Knowledge Graph's Nodes Classification Addressing Problems with Limited Data](https://arxiv.org/abs/2511.13044)
*Rosario Napoli,Giovanni Lonia,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.LG

TL;DR: Bi-View blends Node2Vec topology embeddings with GraphSAGE-driven neighborhood features, enriched by centrality metrics, via a fusion layer to create dual-perspective graph embeddings for knowledge graphs, improving GML performance without synthetic data.


<details>
  <summary>Details</summary>
Motivation: Traditional ML needs lots of data; knowledge graphs are rich but semantically dense and can hide information. The study aims to extract more informative node features to enhance graph embeddings and downstream GML tasks without relying on synthetic data.

Method: Compute Node2Vec embeddings for topology; enrich node features with centrality-based metrics; feed enriched features into GraphSAGE for supervised neighborhood aggregation; use a fusion layer to combine Node2Vec and GraphSAGE representations into a dual-perspective embedding space that captures both topology and semantics.

Result: The dual-perspective embeddings improve downstream task performance, especially when initial features are poor, enabling more accurate KG-enhanced GML models.

Conclusion: Bi-View provides a way to exploit both topological and semantic properties of knowledge graphs to improve GML without leveraging synthetic data, particularly beneficial in sparse or weak-feature scenarios.

Abstract: Traditional Machine Learning (ML) methods require large amounts of data to perform well, limiting their applicability in sparse or incomplete scenarios and forcing the usage of additional synthetic data to improve the model training. To overcome this challenge, the research community is looking more and more at Graph Machine Learning (GML) as it offers a powerful alternative by using relationships within data. However, this method also faces limitations, particularly when dealing with Knowledge Graphs (KGs), which can hide huge information due to their semantic nature. This study introduces Bi-View, a novel hybrid approach that increases the informative content of node features in KGs to generate enhanced Graph Embeddings (GEs) that are used to improve GML models without relying on additional synthetic data. The proposed work combines two complementary GE techniques: Node2Vec, which captures structural patterns through unsupervised random walks, and GraphSAGE, which aggregates neighbourhood information in a supervised way. Node2Vec embeddings are first computed to represent the graph topology, and node features are then enriched with centrality-based metrics, which are used as input for the GraphSAGE model. Moreover, a fusion layer combines the original Node2Vec embeddings with the GraphSAGE-influenced representations, resulting in a dual-perspective embedding space. Such a fusion captures both topological and semantic properties of the graph, enabling the model to exploit informative features that may exist in the dataset but that are not explicitly represented. Our approach improves downstream task performance, especially in scenarios with poor initial features, giving the basis for more accurate and precise KG-enanched GML models.

</details>


### [650] [Generalization Bounds for Semi-supervised Matrix Completion with Distributional Side Information](https://arxiv.org/abs/2511.13049)
*Antoine Ledent,Mun Chong Soo,Nong Minh Hieu*

Main category: cs.LG

TL;DR: A joint low-rank matrix completion framework where the ground-truth matrix R and the sampling distribution P share a common subspace; leveraging abundant unlabeled data from P and a small amount of labeled data with noisy entries yields error bounds that decompose into two terms ~O~(sqrt{nd/M}) and ~O~(sqrt{dr/N}), with empirical validation on synthetic and real recommender datasets showing benefits over explicit-only baselines.


<details>
  <summary>Details</summary>
Motivation: In recommender systems, implicit (unlabeled) feedback and explicit ratings both carry structure. Exploiting their shared subspace can improve generalization in matrix completion.

Method: Combine low-rank subspace recovery with standard matrix completion generalization bounds. Assume R and P are low-rank and share a subspace; derive a two-term error bound depending on unlabeled data M and labeled data N; validate via both theory and experiments.

Result: Theoretical error bounds decompose into two terms sqrt{nd/M} and sqrt{dr/N}. Synthetic experiments show the error splits according to the two sources; real-world experiments on Douban and MovieLens (with explicit ratings largely removed) demonstrate the method outperforms explicit-only baselines, illustrating interaction between explicit and implicit feedback.

Conclusion: Shared subspace structure between ground truth and sampling distribution is beneficial; leveraging both implicit and explicit data improves generalization in matrix completion, providing a toy yet useful framework for recommender systems.

Abstract: We study a matrix completion problem where both the ground truth $R$ matrix and the unknown sampling distribution $P$ over observed entries are low-rank matrices, and \textit{share a common subspace}. We assume that a large amount $M$ of \textit{unlabeled} data drawn from the sampling distribution $P$ is available, together with a small amount $N$ of labeled data drawn from the same distribution and noisy estimates of the corresponding ground truth entries. This setting is inspired by recommender systems scenarios where the unlabeled data corresponds to `implicit feedback' (consisting in interactions such as purchase, click, etc. ) and the labeled data corresponds to the `explicit feedback', consisting of interactions where the user has given an explicit rating to the item. Leveraging powerful results from the theory of low-rank subspace recovery, together with classic generalization bounds for matrix completion models, we show error bounds consisting of a sum of two error terms scaling as $\widetilde{O}\left(\sqrt{\frac{nd}{M}}\right)$ and $\widetilde{O}\left(\sqrt{\frac{dr}{N}}\right)$ respectively, where $d$ is the rank of $P$ and $r$ is the rank of $M$. In synthetic experiments, we confirm that the true generalization error naturally splits into independent error terms corresponding to the estimations of $P$ and and the ground truth matrix $\ground$ respectively. In real-life experiments on Douban and MovieLens with most explicit ratings removed, we demonstrate that the method can outperform baselines relying only on the explicit ratings, demonstrating that our assumptions provide a valid toy theoretical setting to study the interaction between explicit and implicit feedbacks in recommender systems.

</details>


### [651] [Learning from the Undesirable: Robust Adaptation of Language Models without Forgetting](https://arxiv.org/abs/2511.13052)
*Yunhun Nam,Jaehyung Kim,Jongheon Jeong*

Main category: cs.LG

TL;DR: LfU is a simple consistency-regularization for SFT that aligns model representations with those after undesirable updates to combat overfitting with limited data, improving math tasks and robustness while preserving pretrained knowledge.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning with limited data leads to overfitting and loss of general capabilities; we need a regularization that makes the model robust against undesirable updates and preserves prior knowledge.

Method: Introduce representation-level consistency regularization by aligning hidden representations before and after an undesirable update (e.g., gradient ascent toward undesirable behavior), using representation-level data augmentation via such updates.

Result: Empirical gains across tasks; math tasks see ~16.8% avg improvement over vanilla SFT (which degrades on those tasks). LfU also shows improved robustness to prompt variations, with 92.1% lower std dev in outputs compared to SFT.

Conclusion: LfU provides a simple, effective prior for SFT under limited data, improving adaptability while maintaining pretrained capabilities.

Abstract: Language models (LMs) are often adapted through supervised fine-tuning (SFT) to specialize their capabilities for downstream tasks. However, in typical scenarios where the fine-tuning data is limited, e.g., compared to pre-training, SFT can lead LMs to overfit, causing them to rely on spurious patterns within the target task or to compromise other broadly useful capabilities as a side effect of narrow specialization. In this paper, we propose Learning-from-the-Undesirable (LfU), a simple yet effective regularization scheme for SFT to mitigate overfitting issues when fine-tuning LMs with limited data. Specifically, we aim to regularize the fine-tuning process to favor solutions that are resilient to "undesirable" model updates, e.g., gradient ascent steps that steer the model toward undesirable behaviors. To this end, we propose a novel form of consistency regularization that directly aligns internal representations of the model with those after an undesirable update. By leveraging representation-level data augmentation through undesirable updates, LfU effectively promotes generalization under limited data. Our experiments on diverse LM downstream tasks show that LfU serves as an effective prior that enhances adaptability while preserving pretrained knowledge. For example, our LM from LfU achieves a 16.8% average improvement on math tasks compared to vanilla SFT on the same dataset, where the latter even leads to degraded performance on those tasks. Furthermore, LfU exhibits improved robustness to prompt variations, e.g., yielding a 92.1% lower standard deviation in output performances compared to SFT, highlighting its versatile effects.

</details>


### [652] [Self-Organization of Attractor Landscapes in High-Capacity Kernel Logistic Regression Hopfield Networks](https://arxiv.org/abs/2511.13053)
*Akira Tamamori*

Main category: cs.LG

TL;DR: Kernel-based Hopfield networks gain storage by shaping their energy landscape; introduction of Pinnacle Sharpness to measure local attractor stability reveals a ridge of optimization under high-load/global-kernel conditions; gradient decomposed into driving and feedback forces, anti-correlated to produce robust high-capacity memory via a cooperative feedback mechanism; provides design principles for stable high-capacity associative memories.


<details>
  <summary>Details</summary>
Motivation: To uncover the dynamical mechanism by which kernel-based improvements to Hopfield networks enhance storage capacity, moving beyond descriptive performance to a mechanistic energy-landscape picture.

Method: Geometric analysis of the energy landscape; define Pinnacle Sharpness as a local stability metric; systematically vary kernel width and storage load to map a phase diagram of attractor shapes; decompose the landscape gradient into a direct driving term and an indirect feedback term; theoretical analysis of their interaction.

Result: Discovery of a ridge of optimization where attractor stability is maximized under high-load and global-kernel settings; strong anti-correlation between driving and feedback forces with the direct force dominating due to high load; interpretation as a self-organized cooperative feedback control shaping a robust energy landscape.

Conclusion: Provides a new physical picture for the stability of high-capacity associative memories and offers design principles for kernel-based Hopfield networks; suggests ways to harness inter-pattern interactions to sculpt robust memory landscapes.

Abstract: Kernel-based learning methods can dramatically increase the storage capacity of Hopfield networks, yet the dynamical mechanism behind this enhancement remains poorly understood. We address this gap by conducting a geometric analysis of the network's energy landscape. We introduce a novel metric, ``Pinnacle Sharpness,'' to quantify the local stability of attractors. By systematically varying the kernel width and storage load, we uncover a rich phase diagram of attractor shapes. Our central finding is the emergence of a ``ridge of optimization,'' where the network maximizes attractor stability under challenging high-load and global-kernel conditions. Through a theoretical decomposition of the landscape gradient into a direct ``driving'' force and an indirect ``feedback'' force, we reveal the origin of this phenomenon. The optimization ridge corresponds to a regime of strong anti-correlation between the two forces, where the direct force, amplified by the high storage load, dominates the opposing collective feedback force. This demonstrates a sophisticated self-organization mechanism: the network adaptively harnesses inter-pattern interactions as a cooperative feedback control system to sculpt a robust energy landscape. Our findings provide a new physical picture for the stability of high-capacity associative memories and offer principles for their design.

</details>


### [653] [Latency and Ordering Effects in Online Decisions](https://arxiv.org/abs/2511.13060)
*Duo Yi*

Main category: cs.LG

TL;DR: A structured lower-bound decomposition for excess loss in online decision systems with delayed feedback and noncommutative dynamics, using a Bregman divergence; introduces penalties for latency and order sensitivity, a cross-term interaction, and a nonconvexity penalty that vanishes under convex Legendre conditions; extends to prox-regular and weakly convex settings; provides a practical 2x2 randomized experiment-based estimation recipe; aims at robust, interpretable stress-testing in real-world systems.


<details>
  <summary>Details</summary>
Motivation: To provide interpretable guarantees and diagnostics for complex online decision processes where feedback is delayed and observations depend on action order, enabling quantification and monitoring of latency, noncommutativity, and implementation-gap effects.

Method: Derive structured lower bound L ≥ L_ideal + g1(λ) + g2(ε_*) + g12(λ, ε_*) − D_ncx with D_ncx ≥ 0; define g1, g2, g12 as calibrated penalties; extend the inequality to prox-regular and weakly convex regimes; propose measurement recipe via simple 2×2 randomized experiments and streaming diagnostics (effective sample size, clipping rate, heatmaps).

Result: Shows excess benchmark loss lower bound with interpretable components; nonconvexity penalty vanishes under convex Legendre cases; extends guarantees beyond convex; provides procedures to estimate and monitor the four terms; a robust framework packaging latency, noncommutativity, and implementation-gap effects.

Conclusion: A practical, interpretable, stress-testable framework for combining latency, noncommutativity, and implementation-gap effects into a single lower-bound statement, bridging theory and real-world deployment.

Abstract: Online decision systems routinely operate under delayed feedback and order-sensitive (noncommutative) dynamics: actions affect which observations arrive, and in what sequence. Taking a Bregman divergence $D_Φ$ as the loss benchmark, we prove that the excess benchmark loss admits a structured lower bound $L \ge L_{\mathrm{ideal}} + g_1(λ) + g_2(\varepsilon_\star) + g_{12}(λ,\varepsilon_\star) - D_{\mathrm{ncx}}$, where $g_1$ and $g_2$ are calibrated penalties for latency and order-sensitivity, $g_{12}$ captures their geometric interaction, and $D_{\mathrm{ncx}}\ge 0$ is a nonconvexity/approximation penalty that vanishes under convex Legendre assumptions. We extend this inequality to prox-regular and weakly convex settings, obtaining robust guarantees beyond the convex case. We also give an operational recipe for estimating and monitoring the four terms via simple $2\times 2$ randomized experiments and streaming diagnostics (effective sample size, clipping rate, interaction heatmaps). The framework packages heterogeneous latency, noncommutativity, and implementation-gap effects into a single interpretable lower-bound statement that can be stress-tested and tuned in real-world systems.

</details>


### [654] [MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity](https://arxiv.org/abs/2511.13061)
*Vladimír Macko,Vladimír Boža*

Main category: cs.LG

TL;DR: MACKO-SpMV provides a GPU-optimized sparse format and kernel co-design that reduces storage and accelerates SpMV for unstructured sparsity (30-90%) in pruned LLMs, enabling practical 50% pruning without tensor cores. It achieves up to 1.5x memory reduction and 1.2-1.5x speedup over dense, with 2.8-13.0x speedups over cuSPARSE, 1.9-2.6x over Sputnik, and 2.2-2.5x over DASP; applied to Wanda-pruned Llama2-7B to 50% sparsity yields ~1.5x memory reduction and 1.5x faster FP16 inference.


<details>
  <summary>Details</summary>
Motivation: Unstructured pruning often yields high sparsity but existing SpMV formats underperform, yielding limited memory reductions and speedups. There is a need for a GPU-friendly format and kernel that handle irregular sparsity efficiently, without specialized hardware or format-specific preprocessing, to make realistic pruning beneficial for LLM inference.

Method: Introduce MACKO-SpMV, a GPU-optimized sparse matrix format and a co-designed kernel that reduces storage overhead while staying compatible with the GPU execution model. The approach avoids tensor cores and precomputation, and targets unstructured sparsity in the 30-90% range, enabling efficient SpMV for pruned LLMs.

Result: Empirical results show at 50% sparsity, MACKO achieves ~1.5x memory reduction and 1.2-1.5x speedup over the dense representation. It outperforms baselines by 2.8-13.0x vs cuSPARSE, 1.9-2.6x vs Sputnik, and 2.2-2.5x vs DASP. On Llama2-7B pruned to 50% via Wanda, it provides ~1.5x memory reduction and ~1.5x faster FP16 inference.

Conclusion: MACKO-SpMV makes unstructured pruning at 50% sparsity viable in real-world LLM workloads by delivering meaningful memory and time savings without specialized hardware, potentially broadening the practical adoption of unstructured pruning in inference pipelines.

Abstract: Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation in the inference of sparse Large Language Models (LLMs). Because existing SpMV methods perform poorly under the low and unstructured sparsity (30-90%) commonly observed in pruned LLMs, unstructured pruning provided only limited memory reduction and speedup. We propose MACKO-SpMV, a GPU-optimized format and kernel co-designed to reduce storage overhead while preserving compatibility with the GPU's execution model. This enables efficient SpMV for unstructured sparsity without specialized hardware units (e.g., tensor cores) or format-specific precomputation. Empirical results show that at sparsity 50%, MACKO is the first approach with significant 1.5x memory reduction and 1.2-1.5x speedup over dense representation. Speedups over other SpMV baselines: 2.8-13.0x over cuSPARSE, 1.9-2.6x over Sputnik, and 2.2-2.5x over DASP. Applied to Llama2-7B pruned with Wanda to sparsity 50%, it delivers 1.5x memory reduction and 1.5x faster inference at fp16 precision. Thanks to MACKO, unstructured pruning at 50% sparsity is now justified in real-world LLM workloads.

</details>


### [655] [Self-Adaptive Graph Mixture of Models](https://arxiv.org/abs/2511.13062)
*Mohit Meena,Yash Punjabi,Abhishek A,Vishal Sharma,Mahesh Chandran*

Main category: cs.LG

TL;DR: SAGMM is a modular mixture-of-experts framework for GNNs that automatically selects and combines diverse models using topology-aware gating, with pruning and a training-efficient variant, achieving strong results across 16 benchmarks.


<details>
  <summary>Details</summary>
Motivation: GNN performance has plateaued; selecting the right model per graph is hard; a diverse, adaptive ensemble can improve robustness and accuracy.

Method: Proposes SAGMM: a mixture of diverse GNN experts; topology-aware attention gates assign per-node experts; pruning reduces active experts; optional variant where experts are pretrained and frozen; evaluated on node classification, graph classification, regression, link prediction.

Result: SAGMM consistently matches or outperforms leading baselines and prior mixture methods across 16 benchmarks; demonstrates robustness and adaptability.

Conclusion: Architectural diversity plus topology-aware gating with pruning yields a practical, effective approach for real-world graph learning; the training-efficient variant broadens applicability.

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning over graph-structured data, yet recent studies have shown that their performance gains are beginning to plateau. In many cases, well-established models such as GCN and GAT, when appropriately tuned, can match or even exceed the performance of more complex, state-of-the-art architectures. This trend highlights a key limitation in the current landscape: the difficulty of selecting the most suitable model for a given graph task or dataset. To address this, we propose Self-Adaptive Graph Mixture of Models (SAGMM), a modular and practical framework that learns to automatically select and combine the most appropriate GNN models from a diverse pool of architectures. Unlike prior mixture-of-experts approaches that rely on variations of a single base model, SAGMM leverages architectural diversity and a topology-aware attention gating mechanism to adaptively assign experts to each node based on the structure of the input graph. To improve efficiency, SAGMM includes a pruning mechanism that reduces the number of active experts during training and inference without compromising performance. We also explore a training-efficient variant in which expert models are pretrained and frozen, and only the gating and task-specific layers are trained. We evaluate SAGMM on 16 benchmark datasets covering node classification, graph classification, regression, and link prediction tasks, and demonstrate that it consistently outperforms or matches leading GNN baselines and prior mixture-based methods, offering a robust and adaptive solution for real-world graph learning.

</details>


### [656] [A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning](https://arxiv.org/abs/2511.13078)
*Liuyi Jin,Pasan Gunawardena,Amran Haroon,Runzhi Wang,Sangwoo Lee,Radu Stoleru,Michael Middleton,Zepeng Huo,Jeeeun Kim,Jason Moats*

Main category: cs.LG

TL;DR: EMSGlass uses EMSNet and EMSServe to provide real-time, multimodal EMS decision support on smart glasses, achieving up to 11.7x speedups and improved accuracy; a six-EMT user study reports enhanced situational awareness and efficiency; offers directions for future AI-enabled EMS systems.


<details>
  <summary>Details</summary>
Motivation: EMS work is high-pressure, time-critical and cognitively demanding. There is a need for real-time, multimodal understanding (text, vitals, scene data) and low-latency inference on heterogeneous field hardware; current unimodal baselines lag behind multimodal needs.

Method: EMSNet is a multimodal multitask model that fuses text, vital signs, and scene images to support up to five EMS tasks. Trained on real-world EMS datasets. EMSServe is a low-latency, modality-aware serving framework with a model splitter and feature caching to adapt inference across heterogeneous hardware and handle asynchronous modality arrival, built on PyTorch.

Result: EMSNet achieves superior accuracy over state-of-the-art unimodal baselines across EMS tasks. EMSServe delivers 1.9x–11.7x speedups compared with direct PyTorch multimodal inference. A user study with six EMTs shows improved real-time situational awareness, faster decision-making, and greater operational efficiency, with qualitative insights guiding future expansion toward AI-enabled EMS workflows.

Conclusion: The work demonstrates a viable pathway toward AI-enabled EMS systems that integrate multimodal perception with real-world emergency response workflows, highlighting design choices (multimodal multitask modeling, modality-aware serving, and asynchronous modality handling) that enable real-time performance on heterogeneous field hardware and informing future extensions.

Abstract: Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.

</details>


### [657] [Real-time prediction of breast cancer sites using deformation-aware graph neural network](https://arxiv.org/abs/2511.13082)
*Kyunghyun Lee,Yong-Min Shin,Minwoo Shin,Jihun Kim,Sunghwan Lim,Won-Yong Shin,Kyungho Yoon*

Main category: cs.LG

TL;DR: Deformation-aware GNN enables real-time prediction of breast tumor displacement during MRI-guided biopsy with high accuracy and massive speed-up, by combining MRI-informed FE modeling with surface/displacement graph data.


<details>
  <summary>Details</summary>
Motivation: To enable faster, cheaper, and more accurate MRI-guided breast biopsy by replacing time-consuming FE simulations with a real-time, data-driven model that can predict tissue deformation and tumor displacement.

Method: Develop an individual-specific finite element (FE) model from MR-derived breast/tumor structure. Train a graph neural network (GNN) that processes surface displacement and distance-based graph features to predict overall tissue displacement, including tumor deformation. Validate on phantom and patient data, compare to FE simulations for accuracy and support real-time inference.

Result: RMSE for cancer node displacement ~0.2 mm; DSC ~0.977 indicating high spatial overlap with actual cancer regions; real-time inference with ~4000x speed-up over conventional FE simulations.

Conclusion: Deformation-aware GNN shows strong potential for real-time tumor displacement prediction in breast biopsy, offering high accuracy and substantial computational efficiency, which could improve precision and efficiency in clinical breast cancer diagnosis.

Abstract: Early diagnosis of breast cancer is crucial, enabling the establishment of appropriate treatment plans and markedly enhancing patient prognosis. While direct magnetic resonance imaging-guided biopsy demonstrates promising performance in detecting cancer lesions, its practical application is limited by prolonged procedure times and high costs. To overcome these issues, an indirect MRI-guided biopsy that allows the procedure to be performed outside of the MRI room has been proposed, but it still faces challenges in creating an accurate real-time deformable breast model. In our study, we tackled this issue by developing a graph neural network (GNN)-based model capable of accurately predicting deformed breast cancer sites in real time during biopsy procedures. An individual-specific finite element (FE) model was developed by incorporating magnetic resonance (MR) image-derived structural information of the breast and tumor to simulate deformation behaviors. A GNN model was then employed, designed to process surface displacement and distance-based graph data, enabling accurate prediction of overall tissue displacement, including the deformation of the tumor region. The model was validated using phantom and real patient datasets, achieving an accuracy within 0.2 millimeters (mm) for cancer node displacement (RMSE) and a dice similarity coefficient (DSC) of 0.977 for spatial overlap with actual cancerous regions. Additionally, the model enabled real-time inference and achieved a speed-up of over 4,000 times in computational cost compared to conventional FE simulations. The proposed deformation-aware GNN model offers a promising solution for real-time tumor displacement prediction in breast biopsy, with high accuracy and real-time capability. Its integration with clinical procedures could significantly enhance the precision and efficiency of breast cancer diagnosis.

</details>


### [658] [Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions](https://arxiv.org/abs/2511.13103)
*Vidur Sinha,Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: A transformer-based MARL framework (STACCA) with a centralized Graph Transformer Critic for long-range dependencies and a shared Graph Transformer Actor for cross-graph generalization, augmented by a counterfactual advantage estimator for better credit assignment.


<details>
  <summary>Details</summary>
Motivation: Address two key limitations in MARL for large-scale networks: (1) reliance on decay-prone local interactions that miss long-range dependencies such as cascades or outbreaks, and (2) lack of generalization across different network topologies that necessitates retraining.

Method: STACCA combines a centralized Graph Transformer Critic to capture long-range dependencies and provide system-level feedback with a shared Graph Transformer Actor that learns a topology-generalizable policy. It also introduces a counterfactual advantage estimator compatible with state-value critics to improve credit assignment during training.

Result: Empirical evaluation on epidemic containment and rumor-spreading network control tasks shows improved performance, better generalization across networks, and scalability.

Conclusion: Transformer-based MARL can enable scalable and generalizable control of large-scale networked systems; STACCA demonstrates a unified approach to address long-range dependencies and cross-topology generalization in MARL.

Abstract: Multi-agent reinforcement learning (MARL) has shown promise for large-scale network control, yet existing methods face two major limitations. First, they typically rely on assumptions leading to decay properties of local agent interactions, limiting their ability to capture long-range dependencies such as cascading power failures or epidemic outbreaks. Second, most approaches lack generalizability across network topologies, requiring retraining when applied to new graphs. We introduce STACCA (Shared Transformer Actor-Critic with Counterfactual Advantage), a unified transformer-based MARL framework that addresses both challenges. STACCA employs a centralized Graph Transformer Critic to model long-range dependencies and provide system-level feedback, while its shared Graph Transformer Actor learns a generalizable policy capable of adapting across diverse network structures. Further, to improve credit assignment during training, STACCA integrates a novel counterfactual advantage estimator that is compatible with state-value critic estimates. We evaluate STACCA on epidemic containment and rumor-spreading network control tasks, demonstrating improved performance, network generalization, and scalability. These results highlight the potential of transformer-based MARL architectures to achieve scalable and generalizable control in large-scale networked systems.

</details>


### [659] [Synthetic Forgetting without Access: A Few-shot Zero-glance Framework for Machine Unlearning](https://arxiv.org/abs/2511.13116)
*Qipeng Song,Nan Yang,Ziqi Xu,Yue Li,Wei Shao,Feng Xia*

Main category: cs.LG

TL;DR: GFOES uses a Generative Feedback Network to synthesize Optimal Erasure Samples for few-shot zero-glance unlearning, enabling forgetting of target classes with only 5% of data, via a two-phase fine-tuning that aggressively forgets and then restores utility.


<details>
  <summary>Details</summary>
Motivation: Address privacy-preserving machine learning under data-constrained conditions where access to the forget data is unavailable, acknowledging that full dataset access is often impractical.

Method: GFN generates Optimal Erasure Samples that induce high loss on the forget/target classes; a two-phase fine-tuning pipeline first aggressively forgets, then recovers performance on retained classes.

Result: Effective forgetting at both logit and representation levels across three image datasets, while maintaining strong performance with only 5% of the original data.

Conclusion: GFOES provides a practical, scalable framework for privacy-preserving ML under data-limited scenarios, filling a gap where forget data cannot be accessed.

Abstract: Machine unlearning aims to eliminate the influence of specific data from trained models to ensure privacy compliance. However, most existing methods assume full access to the original training dataset, which is often impractical. We address a more realistic yet challenging setting: few-shot zero-glance, where only a small subset of the retained data is available and the forget set is entirely inaccessible. We introduce GFOES, a novel framework comprising a Generative Feedback Network (GFN) and a two-phase fine-tuning procedure. GFN synthesises Optimal Erasure Samples (OES), which induce high loss on target classes, enabling the model to forget class-specific knowledge without access to the original forget data, while preserving performance on retained classes. The two-phase fine-tuning procedure enables aggressive forgetting in the first phase, followed by utility restoration in the second. Experiments on three image classification datasets demonstrate that GFOES achieves effective forgetting at both logit and representation levels, while maintaining strong performance using only 5% of the original data. Our framework offers a practical and scalable solution for privacy-preserving machine learning under data-constrained conditions.

</details>


### [660] [Departures: Distributional Transport for Single-Cell Perturbation Prediction with Neural Schrödinger Bridges](https://arxiv.org/abs/2511.13124)
*Changxi Chi,Yufei Huang,Jun Xia,Jiangbin Zheng,Yunfan Liu,Zelin Zang,Stan Z. Li*

Main category: cs.LG

TL;DR: A scalable Schrödinger-Bridge-based approach using minibatch optimal-transport pairing to align unpaired control and perturbed single-cell distributions, modeling both discrete gene activation and continuous expression, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Unpaired single-cell perturbation data are common due to destructive sequencing; existing methods either lack explicit conditioning or rely on indirect distribution alignment via prior spaces, hindering precise perturbation modeling and capturing cellular heterogeneity.

Method: Approximate Schrödinger Bridge (SB) to recover entropy-regularized OT for distribution alignment, using minibatch-OT-based pairing to avoid bidirectional reverse-process ill-posedness. Two SB models are trained: one for discrete gene activation states and one for continuous expression distributions, with joint training to model perturbations directly across conditions.

Result: On public genetic and drug perturbation datasets, the method effectively captures heterogeneous single-cell responses and achieves state-of-the-art performance.

Conclusion: The proposed framework provides a scalable, accurate approach to perturbation modeling in unpaired single-cell data, enabling explicit conditioning on perturbations and robust handling of cellular heterogeneity compared to prior SB methods.

Abstract: Predicting single-cell perturbation outcomes directly advances gene function analysis and facilitates drug candidate selection, making it a key driver of both basic and translational biomedical research. However, a major bottleneck in this task is the unpaired nature of single-cell data, as the same cell cannot be observed both before and after perturbation due to the destructive nature of sequencing. Although some neural generative transport models attempt to tackle unpaired single-cell perturbation data, they either lack explicit conditioning or depend on prior spaces for indirect distribution alignment, limiting precise perturbation modeling. In this work, we approximate Schrödinger Bridge (SB), which defines stochastic dynamic mappings recovering the entropy-regularized optimal transport (OT), to directly align the distributions of control and perturbed single-cell populations across different perturbation conditions. Unlike prior SB approximations that rely on bidirectional modeling to infer optimal source-target sample coupling, we leverage Minibatch-OT based pairing to avoid such bidirectional inference and the associated ill-posedness of defining the reverse process. This pairing directly guides bridge learning, yielding a scalable approximation to the SB. We approximate two SB models, one modeling discrete gene activation states and the other continuous expression distributions. Joint training enables accurate perturbation modeling and captures single-cell heterogeneity. Experiments on public genetic and drug perturbation datasets show that our model effectively captures heterogeneous single-cell responses and achieves state-of-the-art performance.

</details>


### [661] [Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning](https://arxiv.org/abs/2511.13133)
*Shudong Wang,Xinfei Wang,Chenhao Zhang,Shanchen Pang,Haiyuan Gui,Wenhao Ji,Xiaojian Liao*

Main category: cs.LG

TL;DR: SoCo-DT introduces Soft Conflict-resolution for MTRL using Fisher-based parameter importance and IQR-driven dynamic sparsity with an asymmetric cosine schedule, achieving state-of-the-art gains on Meta-World MT50 and suboptimal data.


<details>
  <summary>Details</summary>
Motivation: Address gradient conflicts and over-suppression from coarse binary masks in multi-task reinforcement learning. Tasks have varying conflict levels, but existing fixed sparsity strategies hinder sharing and efficiency.

Method: A soft-masking framework SoCo-DT where Fisher information guides mask updates to preserve important parameters and suppress conflicting ones. Dynamic sparsity is achieved via Interquartile Range (IQR) based thresholds, adapting to training distributions of conflict/harmony scores. An asymmetric cosine annealing schedule continuously updates the threshold to allow adaptive sparsity over time.

Result: On Meta-World benchmark, SoCo-DT outperforms the state-of-the-art by 7.6% on MT50 and 10.5% on the suboptimal dataset, indicating improved gradient conflict mitigation and overall multi-task performance.

Conclusion: Soft, adaptive, task-aware sparsity and parameter importance-based masking effectively mitigate gradient conflicts in MTRL and improve generalization and learning efficiency across tasks.

Abstract: Multi-task reinforcement learning (MTRL) seeks to learn a unified policy for diverse tasks, but often suffers from gradient conflicts across tasks. Existing masking-based methods attempt to mitigate such conflicts by assigning task-specific parameter masks. However, our empirical study shows that coarse-grained binary masks have the problem of over-suppressing key conflicting parameters, hindering knowledge sharing across tasks. Moreover, different tasks exhibit varying conflict levels, yet existing methods use a one-size-fits-all fixed sparsity strategy to keep training stability and performance, which proves inadequate. These limitations hinder the model's generalization and learning efficiency.
  To address these issues, we propose SoCo-DT, a Soft Conflict-resolution method based by parameter importance. By leveraging Fisher information, mask values are dynamically adjusted to retain important parameters while suppressing conflicting ones. In addition, we introduce a dynamic sparsity adjustment strategy based on the Interquartile Range (IQR), which constructs task-specific thresholding schemes using the distribution of conflict and harmony scores during training. To enable adaptive sparsity evolution throughout training, we further incorporate an asymmetric cosine annealing schedule to continuously update the threshold. Experimental results on the Meta-World benchmark show that SoCo-DT outperforms the state-of-the-art method by 7.6% on MT50 and by 10.5% on the suboptimal dataset, demonstrating its effectiveness in mitigating gradient conflicts and improving overall multi-task performance.

</details>


### [662] [Personalized Federated Learning with Bidirectional Communication Compression via One-Bit Random Sketching](https://arxiv.org/abs/2511.13144)
*Jiacheng Cheng,Xu Zhang,Guanghui Qiu,Yifang Zhang,Yinchuan Li,Kaiyuan Feng*

Main category: cs.LG

TL;DR: pFed1BS uses extreme communication compression in personalized federated learning via one-bit random sketching with a sign-based regularizer and Hadamard projection, achieving competitive results with substantially reduced communication.


<details>
  <summary>Details</summary>
Motivation: Federated learning suffers significant bidirectional communication overhead and data heterogeneity across clients. Personalization is needed to tailor models per client while keeping data decentralized.

Method: Clients send compressed one-bit sketches; server aggregates to a global one-bit consensus. A sign-based regularizer guides local models toward the global consensus while preserving local data characteristics. Random sketching is accelerated by Fast Hadamard Transform for efficient projection.

Result: Theoretical convergence guarantee to a stationary neighborhood of the global potential function. Numerical simulations show substantial communication cost reduction with competitive performance compared to advanced communication-efficient FL algorithms.

Conclusion: pFed1BS effectively reduces communication while enabling personalization, offering a promising approach for communication-efficient personalized federated learning and a framework for further improvements.

Abstract: Federated Learning (FL) enables collaborative training across decentralized data, but faces key challenges of bidirectional communication overhead and client-side data heterogeneity. To address communication costs while embracing data heterogeneity, we propose pFed1BS, a novel personalized federated learning framework that achieves extreme communication compression through one-bit random sketching. In personalized FL, the goal shifts from training a single global model to creating tailored models for each client. In our framework, clients transmit highly compressed one-bit sketches, and the server aggregates and broadcasts a global one-bit consensus. To enable effective personalization, we introduce a sign-based regularizer that guides local models to align with the global consensus while preserving local data characteristics. To mitigate the computational burden of random sketching, we employ the Fast Hadamard Transform for efficient projection. Theoretical analysis guarantees that our algorithm converges to a stationary neighborhood of the global potential function. Numerical simulations demonstrate that pFed1BS substantially reduces communication costs while achieving competitive performance compared to advanced communication-efficient FL algorithms.

</details>


### [663] [OTARo: Once Tuning for All Precisions toward Robust On-Device LLMs](https://arxiv.org/abs/2511.13147)
*Shaoyuan Chen,Zhixuan Chen,Dawei Yang,Zhihang Yuan,Qiang Wu*

Main category: cs.LG

TL;DR: OTARo enables on-device LLMs to flexibly switch quantization precisions with robustness, via Shared Exponent Floating Point (SEFP) and bit-width aware training, using BPS path search and LAA asynchronous accumulation.


<details>
  <summary>Details</summary>
Motivation: Practical on-device deployment needs variable quantization precisions to match diverse tasks and real-world constraints, but conventional quantization lacks cross-bitwidth compatibility and adaptability.

Method: Introduce SEFP to allow multiple bit-widths via mantissa truncation on a single model. Train to resist losses across bit-widths using two strategies: (1) Exploitation-Exploration Bit-Width Path Search (BPS) to iteratively refine a scoring-based search path; (2) Low-Precision Asynchronous Accumulation (LAA) to perform gradient accumulation and delayed updates under low-bit-widths.

Result: OTARo demonstrates robust performance across multiple bit-widths on LLMs (e.g., LLaMA3.2-1B, LLaMA3-8B), preserving accuracy and degradation-latency trade-offs compared to full-precision baselines.

Conclusion: A practical on-device quantization framework that supports dynamic precision switching with minimal fine-tuning, enabling robust performance across tasks and bit-widths through SEFP and bit-width-aware training.

Abstract: Large Language Models (LLMs) fine-tuning techniques not only improve the adaptability to diverse downstream tasks, but also mitigate adverse effects of model quantization. Despite this, conventional quantization suffers from its structural limitation that hinders flexibility during the fine-tuning and deployment stages. Practical on-device tasks demand different quantization precisions (i.e. different bit-widths), e.g., understanding tasks tend to exhibit higher tolerance to reduced precision compared to generation tasks. Conventional quantization, typically relying on scaling factors that are incompatible across bit-widths, fails to support the on-device switching of precisions when confronted with complex real-world scenarios. To overcome the dilemma, we propose OTARo, a novel method that enables on-device LLMs to flexibly switch quantization precisions while maintaining performance robustness through once fine-tuning. OTARo introduces Shared Exponent Floating Point (SEFP), a distinct quantization mechanism, to produce different bit-widths through simple mantissa truncations of a single model. Moreover, to achieve bit-width robustness in downstream applications, OTARo performs a learning process toward losses induced by different bit-widths. The method involves two critical strategies: (1) Exploitation-Exploration Bit-Width Path Search (BPS), which iteratively updates the search path via a designed scoring mechanism; (2) Low-Precision Asynchronous Accumulation (LAA), which performs asynchronous gradient accumulations and delayed updates under low bit-widths. Experiments on popular LLMs, e.g., LLaMA3.2-1B, LLaMA3-8B, demonstrate that OTARo achieves consistently strong and robust performance for all precisions.

</details>


### [664] [Warm-starting active-set solvers using graph neural networks](https://arxiv.org/abs/2511.13174)
*Ella J. Schmidtobreick,Daniel Arnström,Paul Häusner,Jens Sjölund*

Main category: cs.LG

TL;DR: A graph neural network predicts dual active sets for a QP solver to warm-start DAQP, reducing iterations and generalizing across problem sizes.


<details>
  <summary>Details</summary>
Motivation: Real-time optimization (e.g., MPC) requires faster QP solving; traditional solvers do not exploit problem structure; learn to accelerate by predicting active sets.

Method: Represent QPs as bipartite graphs; train a GNN to identify the optimal active set for warm-starting the dual active-set solver DAQP; compare against cold-start and an MLP baseline across varying sizes; test generalization to unseen dimensions.

Result: GNN reduces the number of solver iterations compared with cold-start; performance comparable to MLP baseline; a GNN trained on varying sizes generalizes well to unseen dimensions.

Conclusion: Structure-aware learning with GNNs can accelerate QP solving in real-time contexts, offering scalable and generalizable improvements for applications like MPC.

Abstract: Quadratic programming (QP) solvers are widely used in real-time control and optimization, but their computational cost often limits applicability in time-critical settings. We propose a learning-to-optimize approach using graph neural networks (GNNs) to predict active sets in the dual active-set solver DAQP. The method exploits the structural properties of QPs by representing them as bipartite graphs and learning to identify the optimal active set for efficiently warm-starting the solver. Across varying problem sizes, the GNN consistently reduces the number of solver iterations compared to cold-starting, while performance is comparable to a multilayer perceptron (MLP) baseline. Furthermore, a GNN trained on varying problem sizes generalizes effectively to unseen dimensions, demonstrating flexibility and scalability. These results highlight the potential of structure-aware learning to accelerate optimization in real-time applications such as model predictive control.

</details>


### [665] [Real-time distortion prediction in metallic additive manufacturing via a physics-informed neural operator approach](https://arxiv.org/abs/2511.13178)
*Mingxuan Tian,Haochen Mu,Donghong Ding,Mengjiao Li,Yuhan Ding,Jianping Zhao*

Main category: cs.LG

TL;DR: A physics-informed neural operator (PINO) predicts 15 s of distortion in metal additive manufacturing, decoupling thermo-mechanical fields with a trunk–branch architecture and a soft heat-conduction constraint, achieving sub-mm accuracy for real-time control.


<details>
  <summary>Details</summary>
Motivation: Real-time distortion prediction is needed to control defects in metal AM, but FEM simulations are too slow and standard ML struggles with long-horizon spatiotemporal dynamics and enforcing physical consistency.

Method: PIDeepONet-RNN architecture: a trunk network processes temperature history while a branch network encodes distortion fields to decouple thermo-mechanical responses. A heat-conduction equation is imposed as a soft constraint (physics-informed loss). Trained on FEM-generated data to map thermal history to distortion via a PINO surrogate.

Result: Max absolute errors: z = 0.9733 mm; y = 0.2049 mm. Error distribution shows higher errors in the molten pool region but lower gradient norms in deposited/key areas. Demonstrates accuracy, low error accumulation, and faster-than-FEM performance suitable for real-time long-horizon predictions.

Conclusion: A PINO-based surrogate provides physically consistent, real-time long-horizon predictions of distortion in metal AM, enabling effective defect control through decoupled thermo-mechanical mapping.

Abstract: With the development of digital twins and smart manufacturing systems, there is an urgent need for real-time distortion field prediction to control defects in metal Additive Manufacturing (AM). However, numerical simulation methods suffer from high computational cost, long run-times that prevent real-time use, while conventional Machine learning (ML) models struggle to extract spatiotemporal features for long-horizon prediction and fail to decouple thermo-mechanical fields. This paper proposes a Physics-informed Neural Operator (PINO) to predict z and y-direction distortion for the future 15 s. Our method, Physics-informed Deep Operator Network-Recurrent Neural Network (PIDeepONet-RNN) employs trunk and branch network to process temperature history and encode distortion fields, respectively, enabling decoupling of thermo-mechanical responses. By incorporating the heat conduction equation as a soft constraint, the model ensures physical consistency and suppresses unphysical artifacts, thereby establishing a more physically consistent mapping between the thermal history and distortion. This is important because such a basis function, grounded in physical laws, provides a robust and interpretable foundation for predictions. The proposed models are trained and tested using datasets generated from experimentally validated Finite Element Method (FEM). Evaluation shows that the model achieves high accuracy, low error accumulation, time efficiency. The max absolute errors in the z and y-directions are as low as 0.9733 mm and 0.2049 mm, respectively. The error distribution shows high errors in the molten pool but low gradient norms in the deposited and key areas. The performance of PINO surrogate model highlights its potential for real-time long-horizon physics field prediction in controlling defects.

</details>


### [666] [Uncertainty-aware Physics-informed Neural Networks for Robust CARS-to-Raman Signal Reconstruction](https://arxiv.org/abs/2511.13185)
*Aishwarya Venkataramanan,Sai Karthikeya Vemuri,Adithya Ashok Chalain Valapil,Joachim Denzler*

Main category: cs.LG

TL;DR: Uncertainty quantification (UQ) for CARS-to-Raman reconstruction is systematically evaluated, showing physics-informed constraints improve model calibration and reliability.


<details>
  <summary>Details</summary>
Motivation: CARS spectroscopy suffers from a non-resonant background that distorts Raman signals. While deep learning can reconstruct the true signal using labeled data, standard models cannot quantify uncertainty, which is critical for high-stakes medical and scientific applications. Incorporating physics (Kramers-Kronig relations and smoothness) into loss functions addresses this gap.

Method: The study evaluates and compares multiple UQ techniques within CARS-to-Raman reconstruction, and investigates the impact of physics-informed constraints (e.g., Kramers-Kronig, smoothness) on model calibration. Likely approaches include Bayesian neural networks, MC dropout, deep ensembles, and related methods, with neural networks trained to map CARS measurements to Raman spectra under physics-informed losses.

Result: Physics-informed constraints improve the calibration of UQ-enabled models, yielding more trustworthy Raman reconstructions from CARS data.

Conclusion: Integrating physics-informed losses with uncertainty-aware learning enhances reliability for CARS data analysis and represents a promising path for robust, trustworthy deployment in scientific and biomedical contexts.

Abstract: Coherent anti-Stokes Raman scattering (CARS) spectroscopy is a powerful and rapid technique widely used in medicine, material science, and chemical analyses. However, its effectiveness is hindered by the presence of a non-resonant background that interferes with and distorts the true Raman signal. Deep learning methods have been employed to reconstruct the true Raman spectrum from measured CARS data using labeled datasets. A more recent development integrates the domain knowledge of Kramers-Kronig relationships and smoothness constraints in the form of physics-informed loss functions. However, these deterministic models lack the ability to quantify uncertainty, an essential feature for reliable deployment in high-stakes scientific and biomedical applications. In this work, we evaluate and compare various uncertainty quantification (UQ) techniques within the context of CARS-to-Raman signal reconstruction. Furthermore, we demonstrate that incorporating physics-informed constraints into these models improves their calibration, offering a promising path toward more trustworthy CARS data analysis.

</details>


### [667] [DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play](https://arxiv.org/abs/2511.13186)
*Akash Karthikeyan,Yash Vardhan Pant*

Main category: cs.LG

TL;DR: A diffusion-based fictitious-play framework (DiffFP) for continuous-space zero-sum games that learns robust multimodal policies and converges toward ε-Nash, achieving faster convergence and stronger robustness against unseen opponents.


<details>
  <summary>Details</summary>
Motivation: Self-play RL in continuous decision spaces struggles with slow or unstable convergence to Nash equilibria and vulnerability to unseen opponents. There is a need for adaptable, generalizable strategies in dynamic multi-agent settings.

Method: Proposes DiffFP: use fictitious play to estimate best responses to opponents, modeling the best response with a diffusion policy via generative modeling to capture diverse, adaptive strategies; validated in racing and multi-particle zero-sum environments.

Result: Empirically converges toward ε-Nash in continuous-space zero-sum games; policies robust to diverse opponents; outperforms RL baselines; up to 3x faster convergence and 30x higher success rates.

Conclusion: DiffFP enhances stability and robustness of self-play in continuous multi-agent settings by combining fictitious play with diffusion-based generative policies, enabling faster convergence and resilience to opponent strategies.

Abstract: Self-play reinforcement learning has demonstrated significant success in learning complex strategic and interactive behaviors in competitive multi-agent games. However, achieving such behaviors in continuous decision spaces remains challenging. Ensuring adaptability and generalization in self-play settings is critical for achieving competitive performance in dynamic multi-agent environments. These challenges often cause methods to converge slowly or fail to converge at all to a Nash equilibrium, making agents vulnerable to strategic exploitation by unseen opponents. To address these challenges, we propose DiffFP, a fictitious play (FP) framework that estimates the best response to unseen opponents while learning a robust and multimodal behavioral policy. Specifically, we approximate the best response using a diffusion policy that leverages generative modeling to learn adaptive and diverse strategies. Through empirical evaluation, we demonstrate that the proposed FP framework converges towards $ε$-Nash equilibria in continuous- space zero-sum games. We validate our method on complex multi-agent environments, including racing and multi-particle zero-sum games. Simulation results show that the learned policies are robust against diverse opponents and outperform baseline reinforcement learning policies. Our approach achieves up to 3$\times$ faster convergence and 30$\times$ higher success rates on average against RL-based baselines, demonstrating its robustness to opponent strategies and stability across training iterations

</details>


### [668] [ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer](https://arxiv.org/abs/2511.13198)
*Zhixin Ou,Peng Liang,Jianchen Han,Baihui Liu,Linbo Qiao*

Main category: cs.LG

TL;DR: ParaDySe is an adaptive, on-the-fly parallel-strategy switching framework for training Transformer-based LLMs with dynamic sequences, using modular libraries and cost-guided heuristics to select layer-wise strategies, mitigating OOM and CPC bottlenecks for very long sequences.


<details>
  <summary>Details</summary>
Motivation: Static, pre-defined parallel strategies are ill-suited for dynamic sequences: they can fail to cancel communication on short sequences and cause out-of-memory on long sequences. An adaptive, sequence-aware approach is needed.

Method: Develop modular libraries of parallel strategies with unified tensor layouts; build sequence-aware memory and time cost models using hybrid methods; guide strategy selection with cost models via an efficient heuristic to assign layer-wise strategies; enable seamless hot-switching through designed libraries.

Result: Results on representative LLMs with sequence lengths up to 624K show that ParaDySe mitigates OOM and CPC bottlenecks by integrating long-sequence optimizations with existing frameworks, outperforming baselines.

Conclusion: Adaptive, on-demand strategy switching improves training efficiency and memory robustness for dynamic sequences in Transformer-based LLMs, and can be integrated into existing training frameworks to handle extreme sequence lengths.

Abstract: Dynamic sequences with varying lengths have been widely used in the training of Transformer-based large language models (LLMs). However, current training frameworks adopt a pre-defined static parallel strategy for these sequences, causing neither communication-parallelization cancellation on short sequences nor out-of-memory on long sequences. To mitigate these issues, we propose ParaDySe, a novel adaptive Parallel strategy switching framework for Dynamic Sequences. ParaDySe enables on-the-fly optimal strategy adoption according to the immediate input sequence. It first implements the modular function libraries for parallel strategies with unified tensor layout specifications, and then builds sequence-aware memory and time cost models with hybrid methods. Guided by cost models, ParaDySe selects optimal layer-wise strategies for dynamic sequences via an efficient heuristic algorithm. By integrating these techniques together, ParaDySe achieves seamless hot-switching of optimal strategies through its well-designed function libraries. We compare ParaDySe with baselines on representative LLMs under datasets with sequence lengths up to 624K. Experimental results indicate that ParaDySe addresses OOM and CPC bottlenecks in LLM training by systematically integrating long-sequence optimizations with existing frameworks.

</details>


### [669] [TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs](https://arxiv.org/abs/2511.13223)
*Yuxiang Zhang,Zhengxu Yu,Weihang Pan,Zhongming Jin,Qiang Fu,Deng Cai,Binbin Lin,Jieping Ye*

Main category: cs.LG

TL;DR: TokenSqueeze is a Long2Short method that adaptively selects self-generated reasoning samples and uses distribution-aligned linguistic refinement to compress CoTs while preserving accuracy, achieving around 50% token reduction on MATH500 with DeepSeek-R1-Distill-Qwen-7B; code available at GitHub.


<details>
  <summary>Details</summary>
Motivation: Long chain-of-thought (CoT) generations improve reasoning but incur high token costs, latency, and memory usage. Existing Long2Short approaches often degrade accuracy. TokenSqueeze aims to maintain performance while reducing token costs by leveraging self-generated data only.

Method: 1) Adaptive sampling of self-generated samples with reasoning depth matched to the problem's complexity to avoid excessive compression. 2) Distribution-aligned linguistic refinement to make the reasoning path concise and clear without altering its logical content. 3) Fully self-generated data; evaluation on DeepSeek-R1-Distill-Qwen-7B where fine-tuning yields TokenSqueeze performance. 4) Benchmarking on MATH500 shows token reductions with preserved accuracy.

Result: Demonstrates ~50% average token reduction on MATH500 with preserved accuracy for DeepSeek-R1-Distill-Qwen-7B after applying TokenSqueeze; claims robustness across diverse applications; code available at the provided GitHub URL.

Conclusion: TokenSqueeze shows that carefully compressing reasoning traces using adaptive self-generated data and distribution-aligned linguistic refinement can reduce token usage without sacrificing performance, enabling more efficient deployment of reasoning LLMs. Further work could explore broader benchmarks, ablations, and generalization across models and tasks.

Abstract: Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze.

</details>


### [670] [Laplace Learning in Wasserstein Space](https://arxiv.org/abs/2511.13229)
*Mary Chriselda Antony Oliver,Michael Roberts,Carola-Bibiane Schönlieb,Matthew Thorpe*

Main category: cs.LG

TL;DR: Extends Laplace learning to Wasserstein space, proves convergence of discrete graph p-Dirichlet energy to its continuum counterpart, characterizes the Laplace-Beltrami operator on a Wasserstein submanifold, and validates with experiments.


<details>
  <summary>Details</summary>
Motivation: To generalize graph-based semi-supervised learning to infinite-dimensional, Wasserstein-geometry settings under the manifold hypothesis, enabling principled continuum limits and operator analysis for high-dimensional data.

Method: Prove variational convergence (Gamma-convergence) of discrete graph p-Dirichlet energy to its continuum counterpart; characterize Laplace-Beltrami operator on a submanifold of Wasserstein space; perform numerical experiments on benchmarks to assess SSL performance.

Result: Establishes variational convergence and operator characterization; empirical experiments demonstrate consistent classification performance in high-dimensional regimes.

Conclusion: Provides a rigorous theoretical foundation for Laplace-learning in Wasserstein spaces and demonstrates practical viability for high-dimensional semi-supervised learning.

Abstract: The manifold hypothesis posits that high-dimensional data typically resides on low-dimensional sub spaces. In this paper, we assume manifold hypothesis to investigate graph-based semi-supervised learning
  methods. In particular, we examine Laplace Learning in the Wasserstein space, extending the classical
  notion of graph-based semi-supervised learning algorithms from finite-dimensional Euclidean spaces to
  an infinite-dimensional setting. To achieve this, we prove variational convergence of a discrete graph p- Dirichlet energy to its continuum counterpart. In addition, we characterize the Laplace-Beltrami operator
  on asubmanifold of the Wasserstein space. Finally, we validate the proposed theoretical framework through
  numerical experiments conducted on benchmark datasets, demonstrating the consistency of our classification performance in high-dimensional settings.

</details>


### [671] [MorphBoost: Self-Organizing Universal Gradient Boosting with Adaptive Tree Morphing](https://arxiv.org/abs/2511.13234)
*Boris Kriuk*

Main category: cs.LG

TL;DR: MorphBoost introduces adaptive, self-organizing trees for gradient boosting, enabling dynamic splitting rules and problem-aware configuration, yielding state-of-the-art accuracy and robustness across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional gradient boosting uses static trees and fixed splits, which cannot adapt to evolving gradient distributions or varying problem complexity across learning stages. This motivates a framework with morphing trees and adaptive splits to better fit data throughout training.

Method: MorphBoost implements self-organizing trees whose split functions evolve based on accumulated gradient statistics and iteration-dependent learning pressures. Key components include a morphing split criterion combining gradient-based scores with information-theoretic metrics weighted by training progress, automatic problem fingerprinting for intelligent parameter configuration across binary/multiclass/regression tasks, vectorized tree prediction for speed, interaction-aware feature importance to detect multiplicative relationships, and fast-mode optimization to balance speed and accuracy.

Result: On 10 diverse datasets, MorphBoost outperforms competitive models (XGBoost, LightGBM, GradientBoosting, HistGradientBoosting, ensembles), achieving an average improvement of 0.84% over XGBoost. It secured 4 wins (40% win rate) and 6 top-3 finishes (20%), with the lowest variance (0.0948) and highest minimum accuracy among models, indicating strong consistency and robustness. Performance is especially notable on harder problems that require higher adaptation; easy datasets show competitive results.

Conclusion: MorphBoost demonstrates state-of-the-art performance through adaptive, self-organizing trees that dynamically morph split behavior during training. The framework provides automatic problem fingerprinting, vectorized prediction, and interaction-aware insights, delivering robust accuracy with strong generalization across diverse tasks.

Abstract: Traditional gradient boosting algorithms employ static tree structures with fixed splitting criteria that remain unchanged throughout training, limiting their ability to adapt to evolving gradient distributions and problem-specific characteristics across different learning stages. This work introduces MorphBoost, a new gradient boosting framework featuring self-organizing tree structures that dynamically morph their splitting behavior during training. The algorithm implements adaptive split functions that evolve based on accumulated gradient statistics and iteration-dependent learning pressures, enabling automatic adjustment to problem complexity. Key innovations include: (1) morphing split criterion combining gradient-based scores with information-theoretic metrics weighted by training progress; (2) automatic problem fingerprinting for intelligent parameter configuration across binary/multiclass/regression tasks; (3) vectorized tree prediction achieving significant computational speedups; (4) interaction-aware feature importance detecting multiplicative relationships; and (5) fast-mode optimization balancing speed and accuracy. Comprehensive benchmarking across 10 diverse datasets against competitive models (XGBoost, LightGBM, GradientBoosting, HistGradientBoosting, ensemble methods) demonstrates that MorphBoost achieves state-of-the-art performance, outperforming XGBoost by 0.84% on average. MorphBoost secured the overall winner position with 4/10 dataset wins (40% win rate) and 6/30 top-3 finishes (20%), while maintaining the lowest variance (σ=0.0948) and highest minimum accuracy across all models, revealing superior consistency and robustness. Performance analysis across difficulty levels shows competitive results on easy datasets while achieving notable improvements on advanced problems due to higher adaptation levels.

</details>


### [672] [Counterfactual Explainable AI (XAI) Method for Deep Learning-Based Multivariate Time Series Classification](https://arxiv.org/abs/2511.13237)
*Alan G. Paredes Cetina,Kaouther Benguessoum,Raoni Lourenço,Sylvain Kubler*

Main category: cs.LG

TL;DR: A multi-objective counterfactual explanation method for multivariate time series (CONFETTI) balances prediction confidence, proximity, and sparsity to improve interpretability; empirically outperforms state-of-the-art CE methods on seven UEA MTS datasets.


<details>
  <summary>Details</summary>
Motivation: DL-based MTS models are powerful but opaque. XAI offers partial explanations; CE methods exist but trade off accuracy, proximity, and sparsity. A method that jointly optimizes multiple objectives can provide actionable, minimal-change explanations to support decision-making.

Method: CONFETTI identifies key MTS subsequences, locates a counterfactual target, and optimally modifies the time series by solving a multi-objective optimization that balances confidence, proximity, and sparsity across the identified subsequences.

Result: On seven MTS datasets from the UEA archive, CONFETTI consistently outperforms state-of-the-art CE methods in its optimization objectives and in six additional literature metrics, delivering ≥10% higher confidence and improved sparsity (≥40%).

Conclusion: CONFETTI provides actionable, minimal-change counterfactual explanations for MTS, enhancing interpretability and decision support across domains.

Abstract: Recent advances in deep learning have improved multivariate time series (MTS) classification and regression by capturing complex patterns, but their lack of transparency hinders decision-making. Explainable AI (XAI) methods offer partial insights, yet often fall short of conveying the full decision space. Counterfactual Explanations (CE) provide a promising alternative, but current approaches typically prioritize either accuracy, proximity or sparsity -- rarely all -- limiting their practical value. To address this, we propose CONFETTI, a novel multi-objective CE method for MTS. CONFETTI identifies key MTS subsequences, locates a counterfactual target, and optimally modifies the time series to balance prediction confidence, proximity and sparsity. This method provides actionable insights with minimal changes, improving interpretability, and decision support. CONFETTI is evaluated on seven MTS datasets from the UEA archive, demonstrating its effectiveness in various domains. CONFETTI consistently outperforms state-of-the-art CE methods in its optimization objectives, and in six other metrics from the literature, achieving $\geq10\%$ higher confidence while improving sparsity in $\geq40\%$.

</details>


### [673] [Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms](https://arxiv.org/abs/2511.13238)
*Patrick Parschan,Charlott Jakob*

Main category: cs.LG

TL;DR: A systematic review of unsupervised and semi-supervised text-based ideal point estimation (CT-IPE) algorithms, identifying 25 methods, classifying them into four families, and offering practical guidance and benchmarking recommendations.


<details>
  <summary>Details</summary>
Motivation: The CT-IPE field is fragmented and lacks systematic comparison or guidance for applied researchers. Synthesizing two decades of methods can clarify relationships, improve interpretability, and inform benchmarking and validation decisions.

Method: Systematic literature review of 25 CT-IPE algorithms, plus manual content analysis of modeling assumptions and development contexts. Development of a conceptual framework that separates how algorithms generate, capture, and aggregate textual variance, leading to four methodological families: word-frequency, topic modeling, word embedding, and LLM-based approaches.

Result: Identification of four methodological families with critical assessment of their interpretability, scalability, and limitations. Provides structured synthesis, practical guidance on trade-offs for transparency, technical requirements, and validation strategies, and stresses that cross-algorithm estimation differences are informative and warrant systematic benchmarking.

Conclusion: The review clarifies relationships among methods over two decades, offers actionable guidance for applied researchers, and emphasizes the need for systematic benchmarking to understand how methodological choices affect estimation outcomes.

Abstract: This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.

</details>


### [674] [Incoherent Beliefs & Inconsistent Actions in Large Language Models](https://arxiv.org/abs/2511.13240)
*Arka Pal,Teo Kitanovski,Arthur Liang,Akilesh Potti,Micah Goldblum*

Main category: cs.LG

TL;DR: LLMs show unreliable belief updates and action alignment; strong models still exhibit these issues, complicating predictions of behavior in dynamic real-world tasks.


<details>
  <summary>Details</summary>
Motivation: Real-world tasks differ from static benchmarks; understanding how LLMs update beliefs and act on them in sequence is crucial for deployment.

Method: Experiments on sequential tasks: compare elicited posteriors to correct Bayesian updates of priors; betting-market style tasks to check consistency between held beliefs and actions; test response to user challenges; include various model sizes, including strong calibrated models.

Result: LLMs can exhibit up to ~30% average difference between elicited posterior and correct update; many actions misalign with internal beliefs; moderate self-inconsistency under challenges; these patterns persist even in strong models.

Conclusion: Predicting LLM behavior in dynamic, real-world settings remains difficult; results motivate improved evaluation and alignment of belief updates and action decisions.

Abstract: Real-world tasks and environments exhibit differences from the static datasets that large language models (LLMs) are typically evaluated on. Such tasks can involve sequential interaction, requiring coherent updating of beliefs in light of new evidence, and making appropriate decisions based on those beliefs. Predicting how LLMs will perform in such dynamic environments is important, but can be tricky to determine from measurements in static settings. In this work, we examine two critical components of LLM performance: the ability of LLMs to coherently update their beliefs, and the extent to which the actions they take are consistent with those beliefs. First, we find that LLMs are largely inconsistent in how they update their beliefs; models can exhibit up to a 30% average difference between the directly elicited posterior, and the correct update of their prior. Second, we find that LLMs also often take actions which are inconsistent with the beliefs they hold. On a betting market, for example, LLMs often do not even bet in the same direction as their internally held beliefs over the underlying outcomes. We also find they have moderate self-inconsistency in how they respond to challenges by users to given answers. Finally, we show that the above properties hold even for strong models that obtain high accuracy or that are well-calibrated on the tasks at hand. Our results highlight the difficulties of predicting LLM behavior in complex real-world settings.

</details>


### [675] [Uncovering and Mitigating Transient Blindness in Multimodal Model Editing](https://arxiv.org/abs/2511.13243)
*Xiaoqi Han,Ru Li,Ran Yi,Hongye Tan,Zhuomin Liang,Víctor Gutiérrez-Basulto,Jeff Z. Pan*

Main category: cs.LG

TL;DR: Proposes a locality-focused evaluation framework for Multimodal Model Editing (MMED) with De-VQA dynamic evaluation, identifies transient blindness, and demonstrates significant locality improvements over baselines.


<details>
  <summary>Details</summary>
Motivation: Current MMED evaluations adapted from textual editing overstate success and fail to reveal overfitting or cross-modal locality issues. A structured, locality-aware framework is needed to reliably assess edits across modalities.

Method: Introduce three locality dimensions—random-image locality, no-image locality, and consistent-image locality—operationalized through seven data types. Develop De-VQA for dynamic visual question answering to detect transient blindness. Perform token-level analysis showing edits skew toward textual tokens. Propose locality-aware adversarial losses to balance cross-modal representations.

Result: Empirical results show the proposed framework outperforms baselines, reducing transient blindness and improving locality by 17% on average.

Conclusion: A comprehensive, locality-focused evaluation framework yields more reliable MMED assessment, mitigates overfitting to edit-similar text, and offers a practical path to better cross-modal edits with potential generalization beyond VQA.

Abstract: Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.

</details>


### [676] [Seek and You Shall Fold](https://arxiv.org/abs/2511.13244)
*Nadav Bojan Sellam,Meital Bojan,Paul Schanda,Alex Bronstein*

Main category: cs.LG

TL;DR: A diffusion-based protein generator guided by non-differentiable, black-box objectives via a genetic-algorithm wrapper, enabling incorporation of experimental data like chemical shifts, NOEs, and distance restraints.


<details>
  <summary>Details</summary>
Motivation: Non-differentiable experimental observables hinder gradient-based conditioning in protein generation. Rich NMR data such as chemical shifts are valuable but difficult to integrate; a general, differentiability-agnostic framework is needed to fuse diverse experimental signals with generative models.

Method: Couple a continuous diffusion-based generator with a tailored genetic algorithm that optimizes generated structures to satisfy a black-box objective from experimental data. The framework supports constraints from three modalities—pairwise distance constraints, nuclear Overhauser effect (NOE) restraints, and chemical shifts—without requiring differentiability of the objective.

Result: Demonstrated effectiveness across three modalities. Chemical shift guided structure generation is feasible for the first time. Exposes weaknesses in current predictors and illustrates a general strategy for incorporating diverse experimental signals into protein modeling.

Conclusion: Non-differentiable guidance enables automated, data-conditioned protein modeling beyond differentiability limits and offers a general path to integrate varied experimental signals, including chemical shifts, into generative design.

Abstract: Accurate protein structures are essential for understanding biological function, yet incorporating experimental data into protein generative models remains a major challenge. Most predictors of experimental observables are non-differentiable, making them incompatible with gradient-based conditional sampling. This is especially limiting in nuclear magnetic resonance, where rich data such as chemical shifts are hard to directly integrate into generative modeling. We introduce a framework for non-differentiable guidance of protein generative models, coupling a continuous diffusion-based generator with any black-box objective via a tailored genetic algorithm. We demonstrate its effectiveness across three modalities: pairwise distance constraints, nuclear Overhauser effect restraints, and for the first time chemical shifts. These results establish chemical shift guided structure generation as feasible, expose key weaknesses in current predictors, and showcase a general strategy for incorporating diverse experimental signals. Our work points toward automated, data-conditioned protein modeling beyond the limits of differentiability.

</details>


### [677] [Edge-aware baselines for ogbn-proteins in PyTorch Geometric: species-wise normalization, post-hoc calibration, and cost-accuracy trade-offs](https://arxiv.org/abs/2511.13250)
*Aleksandar Stanković,Dejan Lisica*

Main category: cs.LG

TL;DR: Edge-to-node feature aggregation with sum in GraphSAGE is a strong, reproducible baseline for ogbn-proteins; BatchNorm often yields the best ROC-AUC, while conditioning LayerNorm offers competitive F1. Post-hoc calibration (per-label temperature scaling and thresholds) improves micro-F1 and calibration error with little AUC change; small gains from light label-smoothing.


<details>
  <summary>Details</summary>
Motivation: To establish reproducible, edge-aware baselines for ogbn-proteins in PyTorch Geometric and understand how architectural choices—edge aggregation and edge usage in message passing, plus normalization—affect performance and calibration.

Method: Empirical study using GraphSAGE baselines with 8-dimensional edge features. Compare three edge-to-node aggregation options (sum, mean, max) and three normalization schemes (LayerNorm, BatchNorm, and a species-aware Conditional LayerNorm). Report compute cost (time, VRAM, parameters) along with ROC-AUC (accuracy) and decision quality. Primary setup: hidden size 512, 3 layers, 3 seeds. Also apply post-hoc per-label temperature scaling and per-label thresholds, plus light label-correlation smoothing; release standardized artifacts and scripts.

Result: Sum-based edge-to-node features outperform mean and max across setups. BatchNorm achieves the best ROC-AUC; CLN matches the AUC frontier while delivering better thresholded F1. Post-hoc per-label temperature scaling and per-label thresholds substantially improve micro-F1 and Expected Calibration Error (ECE) with negligible change to AUC; light label-correlation smoothing yields small additional gains.

Conclusion: The study provides reproducible, edge-aware baselines for ogbn-proteins and reveals how edge aggregation and normalization choices impact accuracy and calibration. Calibration techniques can significantly improve decision quality without sacrificing AUC, and releasing artifacts supports reproducibility.

Abstract: We present reproducible, edge-aware baselines for ogbn-proteins in PyTorch Geometric (PyG). We study two system choices that dominate practice: (i) how 8-dimensional edge evidence is aggregated into node inputs, and (ii) how edges are used inside message passing. Our strongest baseline is GraphSAGE with sum-based edge-to-node features. We compare LayerNorm (LN), BatchNorm (BN), and a species-aware Conditional LayerNorm (CLN), and report compute cost (time, VRAM, parameters) together with accuracy (ROC-AUC) and decision quality. In our primary experimental setup (hidden size 512, 3 layers, 3 seeds), sum consistently beats mean and max; BN attains the best AUC, while CLN matches the AUC frontier with better thresholded F1. Finally, post-hoc per-label temperature scaling plus per-label thresholds substantially improves micro-F1 and expected calibration error (ECE) with negligible AUC change, and light label-correlation smoothing yields small additional gains. We release standardized artifacts and scripts used for all of the runs presented in the paper.

</details>


### [678] [KForge: Program Synthesis for Diverse AI Hardware Accelerators](https://arxiv.org/abs/2511.13274)
*Taras Sereda,Tom St. John,Burak Bartan,Natalie Serrino,Sachin Katti,Zain Asgar*

Main category: cs.LG

TL;DR: KForge uses two collaborating LLM agents to iteratively synthesize and optimize GPU kernels across accelerators (CUDA/Metal) with platform-agnostic transfer.


<details>
  <summary>Details</summary>
Motivation: GPU kernel optimization remains difficult across diverse accelerators; a unified, automated method is needed.

Method: An iterative system with a generation agent that writes/refines programs via compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization, enabling platform-agnostic synthesis with cross-platform knowledge transfer and single-shot adaptation.

Result: Demonstrates cross-platform program synthesis across NVIDIA CUDA and Apple Metal; a reference implementation on one architecture improves generation quality for others, validating platform-agnostic optimization.

Conclusion: Platform-agnostic approach validated; capable of targeted synthesis across heterogeneous accelerators with minimal setup, including new platforms.

Abstract: GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms.
  We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.

</details>


### [679] [Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning](https://arxiv.org/abs/2511.13322)
*Senne Deproost,Dennis Steckelmacher,Ann Nowé*

Main category: cs.LG

TL;DR: A model-agnostic distillation approach that partitions the state space with Voronoi regions and learns local linear models to mimic a deep RL controller. This yields explainable policies that match or slightly surpass the original black-box performance.


<details>
  <summary>Details</summary>
Motivation: Improve transparency and trust for RL controllers to meet regulations by transforming the learned behavior into human-readable, region-specific models.

Method: Partition the state space into Voronoi regions and fit simple linear models in each region that approximate the Deep RL policy; evaluate on gridworld and a classic control task to compare with the original policy.

Result: The locally specialized linear models achieve explainability while matching or slightly outperforming the original black-box policy they distill from.

Conclusion: Voronoi-based local distillation provides a practical path to explainable RL controllers without sacrificing performance; it balances flexibility and simplicity and is applicable to dynamic environments.

Abstract: Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.

</details>


### [680] [Tab-PET: Graph-Based Positional Encodings for Tabular Transformers](https://arxiv.org/abs/2511.13338)
*Yunze Leng,Rohan Ghosh,Mehul Motani*

Main category: cs.LG

TL;DR: Tab-PET introduces graph-based positional encodings (PEs) for tabular transformers to improve generalization by reducing intrinsic dimensionality; association-based PEs outperform causality-based PEs across 50 datasets.


<details>
  <summary>Details</summary>
Motivation: Tabular data lack inherent structural cues, making self-attention less effective and existing tabular transformers (3T) miss potential generalization gains from structural information like positional encodings.

Method: Propose Tab-PET, a graph-based framework to estimate and inject PEs into embeddings. Compare two graph-estimation paradigms: association-based and causality-based. Evaluate on 50 classification and regression datasets using 3T models (TabTransformer, SAINT, FT-Transformer).

Result: Graph-derived PEs significantly improve 3T performance across 50 datasets; association-based graphs yield more stable and pronounced gains than causality-driven graphs. PEs reduce the effective rank (intrinsic dimensionality) of features, simplifying the task and enhancing generalization.

Conclusion: PEs play an unexpected but valuable role in tabular transformers. The Tab-PET framework shows that incorporating graph-derived PEs—especially association-based ones—can substantially improve generalization by lowering intrinsic dimensionality, suggesting a new direction for structuring tabular data through learned positional cues.

Abstract: Supervised learning with tabular data presents unique challenges, including low data sizes, the absence of structural cues, and heterogeneous features spanning both categorical and continuous domains. Unlike vision and language tasks, where models can exploit inductive biases in the data, tabular data lacks inherent positional structure, hindering the effectiveness of self-attention mechanisms. While recent transformer-based models like TabTransformer, SAINT, and FT-Transformer (which we refer to as 3T) have shown promise on tabular data, they typically operate without leveraging structural cues such as positional encodings (PEs), as no prior structural information is usually available. In this work, we find both theoretically and empirically that structural cues, specifically PEs can be a useful tool to improve generalization performance for tabular transformers. We find that PEs impart the ability to reduce the effective rank (a form of intrinsic dimensionality) of the features, effectively simplifying the task by reducing the dimensionality of the problem, yielding improved generalization. To that end, we propose Tab-PET (PEs for Tabular Transformers), a graph-based framework for estimating and inculcating PEs into embeddings. Inspired by approaches that derive PEs from graph topology, we explore two paradigms for graph estimation: association-based and causality-based. We empirically demonstrate that graph-derived PEs significantly improve performance across 50 classification and regression datasets for 3T. Notably, association-based graphs consistently yield more stable and pronounced gains compared to causality-driven ones. Our work highlights an unexpected role of PEs in tabular transformers, revealing how they can be harnessed to improve generalization.

</details>


### [681] [Statistically Accurate and Robust Generative Prediction of Rock Discontinuities with A Tabular Foundation Model](https://arxiv.org/abs/2511.13339)
*Han Meng,Gang Mei,Hong Tian,Nengxiong Xu,Jianbing Peng*

Main category: cs.LG

TL;DR: A robust, data-efficient generative approach using a tabular foundation model to predict rock discontinuities from sparse surface data, outperforming traditional statistics and deep generative methods across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Rock discontinuities have unobservable internal distributions; surface observations are sparse and unreliable for capturing complex distribution patterns. There is a need for accurate, robust generative predictions under data-sparse conditions to improve geotechnical design.

Method: Utilizes a tabular foundation model designed for small data to learn the underlying distribution of discontinuities from limited measured samples. Conducted comparative experiments on ten datasets with diverse scales and distribution patterns.

Result: The proposed method achieves superior accuracy and robustness compared with conventional statistical models and deep generative approaches across the ten datasets.

Conclusion: This approach advances quantitative characterization of rock mass structures and supports safer and more reliable data-driven geotechnical design.

Abstract: Rock discontinuities critically govern the mechanical behavior and stability of rock masses. Their internal distributions remain largely unobservable and are typically inferred from surface-exposed discontinuities using generative prediction approaches. However, surface-exposed observations are inherently sparse, and existing generative prediction approaches either fail to capture the underlying complex distribution patterns or lack robustness under data-sparse conditions. Here, we proposed a simple yet robust approach for statistically accurate generative prediction of rock discontinuities by utilizing a tabular foundation model. By leveraging the powerful sample learning capability of the foundation model specifically designed for small data, our approach can effectively capture the underlying complex distribution patterns within limited measured discontinuities. Comparative experiments on ten datasets with diverse scales and distribution patterns of discontinuities demonstrate superior accuracy and robustness over conventional statistical models and deep generative approaches. This work advances quantitative characterization of rock mass structures, supporting safer and more reliable data-driven geotechnical design.

</details>


### [682] [Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning](https://arxiv.org/abs/2511.13351)
*Xinlan Wu,Bin Zhu,Feng Han,Pengkun Jiao,Jingjing Chen*

Main category: cs.LG

TL;DR: A continual learning framework for multimodal food analysis using Dual-LoRA adapters and Quality-Enhanced Pseudo Replay to mitigate forgetting, achieving strong results on Uni-Food.


<details>
  <summary>Details</summary>
Motivation: Large multimodal food models suffer catastrophic forgetting when adapting to new tasks; need effective continual learning to support complex food analytics.

Method: Introduce Dual-LoRA: per-task specialized LoRA with orthogonal constraints to previous tasks' subspaces; cooperative LoRA to consolidate shared knowledge via pseudo replay; Quality-Enhanced Pseudo Replay using self-consistency and semantic similarity to generate reliable replay data.

Result: Outperforms baselines in mitigating forgetting on Uni-Food; first effective continual learning approach for complex food tasks.

Conclusion: The framework enables robust continual learning for multimodal food analysis and reduces hallucinations in replay data, with potential for broader LMM continual learning applications.

Abstract: Food analysis has become increasingly critical for health-related tasks such as personalized nutrition and chronic disease prevention. However, existing large multimodal models (LMMs) in food analysis suffer from catastrophic forgetting when learning new tasks, requiring costly retraining from scratch. To address this, we propose a novel continual learning framework for multimodal food learning, integrating a Dual-LoRA architecture with Quality-Enhanced Pseudo Replay. We introduce two complementary low-rank adapters for each task: a specialized LoRA that learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces, and a cooperative LoRA that consolidates shared knowledge across tasks via pseudo replay. To improve the reliability of replay data, our Quality-Enhanced Pseudo Replay strategy leverages self-consistency and semantic similarity to reduce hallucinations in generated samples. Experiments on the comprehensive Uni-Food dataset show superior performance in mitigating forgetting, representing the first effective continual learning approach for complex food tasks.

</details>


### [683] [A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs](https://arxiv.org/abs/2511.13373)
*Prakrit Timilsina,Anuj Nepal,Rajan Kadel,Robin Doss*

Main category: cs.LG

TL;DR: Simple averaging-based merging of architecturally compatible medical LLMs yields strong performance and efficiency; Task Arithmetic reaches 45.8% MedQA accuracy, outperforming pruning-based methods, while the proposed Hierarchical OT+cosine method tackles permutation variance with low overhead.


<details>
  <summary>Details</summary>
Motivation: Distributed healthcare faces privacy constraints, data silos, and limited compute at edge devices. Consolidating specialized domain knowledge across institutions without catastrophic forgetting or heavy computation is essential for scalable, privacy-preserving medical AI.

Method: Systematic evaluation of six parameter-space merging techniques (Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and a Hierarchical method) on two architecturally compatible medical LLMs derived from the Mistral-7B base. Introduces a novel Hierarchical approach that combines selective Optimal Transport alignment for attention layers with cosine similarity-weighted interpolation to address permutation variance with reduced computation for edge deployment.

Result: Architecturally compatible models benefit from simple averaging methods; Task Arithmetic achieves 45.80% accuracy on MedQA, outperforming complex pruning-based approaches.

Conclusion: For architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems in resource-constrained environments.

Abstract: Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.

</details>


### [684] [Finding Kissing Numbers with Game-theoretic Reinforcement Learning](https://arxiv.org/abs/2511.13391)
*Chengdong Ma,Théo Tao Zhaowei,Pengyu Li,Minghao Liu,Haojun Chen,Zihao Mao,Yuan Cheng,Yuan Qi,Yaodong Yang*

Main category: cs.LG

TL;DR: AI-driven cooperative reinforcement learning (PackingStar) tackles the high-dimensional Kissing Number Problem, surpassing human records from dims 25–31 and discovering thousands of new structures, including links to the Leech lattice; demonstrates potential of AI for complex geometry beyond 8D.


<details>
  <summary>Details</summary>
Motivation: Resolve the Kissing Number Problem in high dimensions, connect to Hilbert’s 18th problem on sphere packing, and overcome combinatorial explosion that limits traditional lattice/code methods beyond 8 dimensions.

Method: Model the problem as a two-player matrix completion game. PackingStar uses cooperative reinforcement learning where one player fills matrix entries (cosines between sphere centers) and the other corrects suboptimal entries, jointly maximizing the matrix size (the kissing number). This exploration yields efficient navigation of extremely large high-dimensional spaces.

Result: Replicates known configurations and surpasses human-known records in dimensions 25–31; the 25D solution aligns with the Leech lattice and suggests optimality; achieves breakthroughs beyond rational structures since 1971 in 13D; discovers over 6,000 new structures in 14D and other dims.

Conclusion: AI can effectively explore high-dimensional geometric spaces beyond human intuition, offering new directions for the Kissing Number Problem and broader geometry problems, with potential implications for lattice theory and coding theory.

Abstract: Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert's 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions and discovers over 6000 new structures in 14 and other dimensions. These results demonstrate AI's power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.

</details>


### [685] [Fast and Robust Simulation-Based Inference With Optimization Monte Carlo](https://arxiv.org/abs/2511.13394)
*Vasilis Gkolemis,Christos Diou,Michael Gutmann*

Main category: cs.LG

TL;DR: A differentiable-simulator Bayesian inference method based on Optimization Monte Carlo that uses gradient-based optimization and JAX to achieve accurate posteriors with substantially reduced runtimes.


<details>
  <summary>Details</summary>
Motivation: Bayesian parameter inference for complex stochastic simulators is hampered by intractable likelihoods and costly simulations, especially in high-dimensional parameter spaces or when outputs are partially informative.

Method: Reformulate stochastic simulation as deterministic optimization problems within the Optimization Monte Carlo framework; apply gradient-based optimization to efficiently locate high-density posterior regions and avoid wasteful simulations; implement with a vectorized, JAX-based approach to accelerate computation.

Result: Extensive experiments show the method matches or surpasses state-of-the-art accuracy while substantially reducing runtime, handling high-dimensional spaces, uninformative outputs, multiple observations, and multimodal posteriors.

Conclusion: The proposed approach delivers accurate, scalable Bayesian inference for differentiable simulators with notable runtime gains, demonstrating robustness across challenging inference scenarios.

Abstract: Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin.

</details>


### [686] [PAST: A Primary-Auxiliary Spatio-Temporal Network for Traffic Time Series Imputation](https://arxiv.org/abs/2511.13414)
*Hanwen Hu,Zimo Wen,Shiyou Qian,Jian Co*

Main category: cs.LG

TL;DR: PAST provides a two-pattern framework for traffic time series imputation, combining a graph-integrated module for primary internal patterns and a cross-gated module for auxiliary external-patterns, achieving notable improvements over baselines across varied missing data conditions.


<details>
  <summary>Details</summary>
Motivation: Address robust imputation in traffic time series under random, fiber, and block missing; existing methods struggle with random missing positions and long-range dependencies, and often ignore external factors like timestamps and node attributes.

Method: Introduce Primary-Auxiliary Spatio-Temporal network (PAST) with two modules: GIM and CGM. GIM learns primary patterns via dynamic graphs with interval-aware dropout and multi-order convolutions. CGM learns auxiliary patterns through bidirectional gating on embedded external features. The modules exchange information via shared hidden vectors and are trained under an ensemble self-supervised framework.

Result: PAST outperforms seven state-of-the-art baselines by up to 26.2% in RMSE and 31.6% in MAE across 27 missing data conditions on three datasets.

Conclusion: A two-pattern decomposition with joint training effectively handles diverse missing data scenarios in traffic time series, capturing long-term and large-scale dependencies and leveraging external features for robust imputation.

Abstract: Traffic time series imputation is crucial for the safety and reliability of intelligent transportation systems, while diverse types of missing data, including random, fiber, and block missing make the imputation task challenging. Existing models often focus on disentangling and separately modeling spatial and temporal patterns based on relationships between data points. However, these approaches struggle to adapt to the random missing positions, and fail to learn long-term and large-scale dependencies, which are essential in extensive missing conditions. In this paper, patterns are categorized into two types to handle various missing data conditions: primary patterns, which originate from internal relationships between data points, and auxiliary patterns, influenced by external factors like timestamps and node attributes. Accordingly, we propose the Primary-Auxiliary Spatio-Temporal network (PAST). It comprises a graph-integrated module (GIM) and a cross-gated module (CGM). GIM captures primary patterns via dynamic graphs with interval-aware dropout and multi-order convolutions, and CGM extracts auxiliary patterns through bidirectional gating on embedded external features. The two modules interact via shared hidden vectors and are trained under an ensemble self-supervised framework. Experiments on three datasets under 27 missing data conditions demonstrate that the imputation accuracy of PAST outperforms seven state-of-the-art baselines by up to 26.2% in RMSE and 31.6% in MAE.

</details>


### [687] [MMWSTM-ADRAN+: A Novel Hybrid Deep Learning Architecture for Enhanced Climate Time Series Forecasting and Extreme Event Prediction](https://arxiv.org/abs/2511.13419)
*Shaheen Mohammed Saleh Ahmed,Hakan Hakan Guneyli*

Main category: cs.LG

TL;DR: A dual-stream neural architecture (MMWSTM-ADRAN+) for short-range extreme temperature prediction, combining regime-aware dynamics and anomaly-focused attention, with a custom loss and data augmentation to emphasize extremes.


<details>
  <summary>Details</summary>
Motivation: Extreme temperature events are hard to predict; accurate short-range forecasts are critical for climate-risk management and operational decision-making.

Method: Two streams: MMWSTM (BiLSTM + learnable Markov transition) captures regime changes; ADRAN (BiGRU + multi-head self-attention + anomaly amplification) boosts sensitivity to low-probability signals; a dynamic fusion gate combines streams; training uses ExtremeWeatherLoss (upweights top/bottom 5%) and data augmentation (jittering, scaling, time/magnitude warping) to quadruple data.

Result: The abstract outlines the architecture and training strategy but does not provide empirical results; thus no quantified performance is reported.

Conclusion: The approach aims to improve daily maximum temperature forecasts and extreme event prediction by integrating regime-aware dynamics with anomaly-driven attention and specialized training techniques to improve sensitivity to extremes.

Abstract: Accurate short-range prediction of extreme air temperature events remains a fundamental challenge in operational climate-risk management. We present Multi-Modal Weather State Transition Model with Anomaly-Driven Recurrent Attention Network Plus (MMWSTM-ADRAN+), a dual-stream deep learning architecture that couples a regime-aware dynamics model with an anomaly-focused attention mechanism to forecast daily maximum temperature and its extremes. The first stream, MMWSTM, combines bidirectional Long Short-Term Memory (BiLSTM) units with a learnable Markov state transition matrix to capture synoptic-scale weather regime changes. The second stream, ADRAN, integrates bidirectional Gated Recurrent Units (BiGRUs), multi-head self-attention, and a novel anomaly amplification layer to enhance sensitivity to low-probability signals. A lightweight attentive fusion gate adaptively determines the contribution of each stream to the final prediction. Model optimization employs a custom ExtremeWeatherLoss function that up-weights errors on the upper 5% and lower 5% of the temperature distribution, and a time-series data augmentation suite (jittering, scaling, time/magnitude warping) that effectively quadruples the training data

</details>


### [688] [Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression](https://arxiv.org/abs/2511.13421)
*Tingkai Yan,Haodong Wen,Binghui Li,Kairong Luo,Wenguang Chen,Kaifeng Lyu*

Main category: cs.LG

TL;DR: This work studies how training with multiple epochs on the same data affects data scaling laws in linear regression. It defines the effective reuse rate E(K,N) and shows that for SGD under strong convexity or Zipf-distributed data, E(K,N) behaves as: (i) E ≈ K when K is small, meaning linear gains per extra epoch, and (ii) E saturates with K, approaching a problem-dependent value that grows with N (≈ Θ(log N) in the strongly convex case). The results reconcile and extend prior empirical findings by showing that the maximum beneficial K depends on data size/distribution, highlighting the need to model both factors in scaling laws with data reuse.


<details>
  <summary>Details</summary>
Motivation: To understand how data reuse through multiple epochs on the same dataset reshapes data scaling laws, especially for regimes with limited data where repeated passes are common, and to connect theoretical predictions with observed LLM training behavior.

Method: Theoretical analysis of SGD in linear regression under two settings: strong convexity and Zipf-distributed data. Derives the effective reuse rate E(K,N) — the dataset growth factor needed to match K-epoch performance with a single-pass run — and characterizes its asymptotic behavior in terms of K and N. Includes empirical validation with large language models.

Result: For small K, E(K,N) ≈ K, indicating each additional epoch yields linear gains. For larger K, E(K,N) plateaus at a problem-dependent level that grows with N (Θ(log N) in the strongly convex case). Data size and distribution influence the maximum beneficial K, challenging the view that up to 4 epochs approximate fresh-data performance.

Conclusion: Both factors—number of epochs (K) and data properties (size N and distribution) — must be modeled in scaling laws with data reuse. The work explains when data reuse is most beneficial and reconciles prior empirical results by showing dependence on data characteristics, informing future studies on LLM data-efficient training.

Abstract: While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($Θ(\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \textit{i.e.}, $E(K, N) \approx K$ for $K \le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse.

</details>


### [689] [Discovering Operational Patterns Using Image-Based Convolutional Clustering and Composite Evaluation: A Case Study in Foundry Melting Processes](https://arxiv.org/abs/2511.13444)
*Zhipeng Ma,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.LG

TL;DR: Unsupervised discovery of operational modes in univariate time-series via image-based convolutional clustering with a composite internal evaluation score (S_eva); converts series to grayscale images via sliding windows, uses a deep convolutional autoencoder, combines soft/hard clustering in a two-stage refinement, and demonstrates seven interpretable patterns across 3900 furnace-melting operations, outperforming baselines in robustness and explainability.


<details>
  <summary>Details</summary>
Motivation: Industrial process monitoring often lacks labeled data and faces high variability and noise. Conventional clustering relies on fixed metrics or static models, limiting handling of dynamic, unstructured sequences. There is a need for an unsupervised, robust, and interpretable framework for operational mode discovery in time-series data.

Method: Transform univariate time-series into overlapping grayscale image representations using sliding windows, extract features with a deep convolutional autoencoder, fuse soft and hard clustering outputs via a two-stage refinement, and evaluate clustering with a new composite score S_eva that combines normalized Silhouette, Calinski-Harabasz, and Davies-Bouldin indices.

Result: Applied to ~3900 furnace-melting operations from a Nordic foundry, the approach identifies seven explainable operational patterns, revealing significant differences in energy consumption, thermal dynamics, and production duration. Compared to classical and deep clustering baselines, the method shows superior overall performance, robustness, and domain-aligned explainability.

Conclusion: The framework addresses key challenges in unsupervised time-series analysis—sequence irregularity, overlapping modes, and metric inconsistency—and offers a generalizable solution for data-driven diagnostics and energy optimization in industrial systems.

Abstract: Industrial process monitoring increasingly relies on sensor-generated time-series data, yet the lack of labels, high variability, and operational noise make it difficult to extract meaningful patterns using conventional methods. Existing clustering techniques either rely on fixed distance metrics or deep models designed for static data, limiting their ability to handle dynamic, unstructured industrial sequences. Addressing this gap, this paper proposes a novel framework for unsupervised discovery of operational modes in univariate time-series data using image-based convolutional clustering with composite internal evaluation. The proposed framework improves upon existing approaches in three ways: (1) raw time-series sequences are transformed into grayscale matrix representations via overlapping sliding windows, allowing effective feature extraction using a deep convolutional autoencoder; (2) the framework integrates both soft and hard clustering outputs and refines the selection through a two-stage strategy; and (3) clustering performance is objectively evaluated by a newly developed composite score, S_eva, which combines normalized Silhouette, Calinski-Harabasz, and Davies-Bouldin indices. Applied to over 3900 furnace melting operations from a Nordic foundry, the method identifies seven explainable operational patterns, revealing significant differences in energy consumption, thermal dynamics, and production duration. Compared to classical and deep clustering baselines, the proposed approach achieves superior overall performance, greater robustness, and domain-aligned explainability. The framework addresses key challenges in unsupervised time-series analysis, such as sequence irregularity, overlapping modes, and metric inconsistency, and provides a generalizable solution for data-driven diagnostics and energy optimization in industrial systems.

</details>


### [690] [Hardware optimization on Android for inference of AI models](https://arxiv.org/abs/2511.13453)
*Iulius Gherasim,Carlos García Sánchez*

Main category: cs.LG

TL;DR: This work investigates optimal on-device execution configurations for AI models on Android, focusing on YOLO object detection and ResNet image classification, by exploring quantization schemes and on-device accelerators (GPU and NPU) to maximize inference speed with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Mobile AI deployment demands low latency and high responsiveness under real-time constraints, while exploiting heterogeneous hardware to improve performance.

Method: Empirical evaluation of multiple quantization schemes and accelerator usage across Android devices for YOLO and ResNet, measuring accuracy degradation and inference speed-ups to identify best trade-offs.

Result: Identified configurations that achieve the best balance between minimal accuracy degradation and maximal speed-up, depending on model and hardware setup.

Conclusion: On-device AI inference on Android can be significantly improved by carefully selecting quantization and accelerator configurations; the study provides actionable guidelines for deploying YOLO and ResNet with favorable trade-offs.

Abstract: The pervasive integration of Artificial Intelligence models into contemporary mobile computing is notable across numerous use cases, from virtual assistants to advanced image processing. Optimizing the mobile user experience involves minimal latency and high responsiveness from deployed AI models with challenges from execution strategies that fully leverage real time constraints to the exploitation of heterogeneous hardware architecture. In this paper, we research and propose the optimal execution configurations for AI models on an Android system, focusing on two critical tasks: object detection (YOLO family) and image classification (ResNet). These configurations evaluate various model quantization schemes and the utilization of on device accelerators, specifically the GPU and NPU. Our core objective is to empirically determine the combination that achieves the best trade-off between minimal accuracy degradation and maximal inference speed-up.

</details>


### [691] [Artificial Intelligence-Enabled Spirometry for Early Detection of Right Heart Failure](https://arxiv.org/abs/2511.13457)
*Bin Liu,Qinghao Zhao,Yuxi Zhou,Zhejun Sun,Kaijie Lei,Deyun Zhang,Shijia Geng,Shenda Hong*

Main category: cs.LG

TL;DR: A self-supervised two-stage pipeline uses spirogram time series and demographics to detect right heart failure (RHF) in cor pulmonale; achieves AUROC 0.750 overall, higher in CKD and VHD subgroups; demonstrates potential for early RHF screening.


<details>
  <summary>Details</summary>
Motivation: RH F is driven by lung disease; early screening in cor pulmonale is needed. Limited labeled RHF data; leverage unlabeled spirogram data with self-supervised learning to improve detection and generalization, especially in high-risk groups.

Method: Stage 1: SLSE - train VAE-based encoder with data-augmented unlabeled spirogram time series to obtain low-dimensional representations. Stage 2: concatenate representation with demographic features and train CatBoost classifier for RHF prediction. Dataset: UK Biobank subset (n=26,617). Evaluation on high-risk subgroups: CKD (n=74) and VHD (n=64).

Result: AUROC 0.7501 overall; 0.8194 in CKD subgroup; 0.8413 in VHD subgroup.

Conclusion: Self-supervised spirogram-based representation learning combined with demographics shows promise for early RHF detection in clinical practice, with potential utility in targeted screening among high-risk patients.

Abstract: Right heart failure (RHF) is a disease characterized by abnormalities in the structure or function of the right ventricle (RV), which is associated with high morbidity and mortality. Lung disease often causes increased right ventricular load, leading to RHF. Therefore, it is very important to screen out patients with cor pulmonale who develop RHF from people with underlying lung diseases. In this work, we propose a self-supervised representation learning method to early detecting RHF from patients with cor pulmonale, which uses spirogram time series to predict patients with RHF at an early stage. The proposed model is divided into two stages. The first stage is the self-supervised representation learning-based spirogram embedding (SLSE) network training process, where the encoder of the Variational autoencoder (VAE-encoder) learns a robust low-dimensional representation of the spirogram time series from the data-augmented unlabeled data. Second, this low-dimensional representation is fused with demographic information and fed into a CatBoost classifier for the downstream RHF prediction task. Trained and tested on a carefully selected subset of 26,617 individuals from the UK Biobank, our model achieved an AUROC of 0.7501 in detecting RHF, demonstrating strong population-level distinction ability. We further evaluated the model on high-risk clinical subgroups, achieving AUROC values of 0.8194 on a test set of 74 patients with chronic kidney disease (CKD) and 0.8413 on a set of 64 patients with valvular heart disease (VHD). These results highlight the model's potential utility in predicting RHF among clinically elevated-risk populations. In conclusion, this study presents a self-supervised representation learning approach combining spirogram time series and demographic data, demonstrating promising potential for early RHF detection in clinical practice.

</details>


### [692] [Multi-task GINN-LP for Multi-target Symbolic Regression](https://arxiv.org/abs/2511.13463)
*Hussein Rajabu,Lijun Qian,Xishuang Dong*

Main category: cs.LG

TL;DR: Introduces MTRGINN-LP, an interpretable multi-task symbolic regression model that combines GINN-LP with multi-task learning to handle multiple interdependent targets.


<details>
  <summary>Details</summary>
Motivation: SR often evaluated on simple scientific datasets and mainly targets single-output regression, which limits generalization to real-world, multi-target problems with interdependent outputs; there is a need for interpretable models that can capture cross-target relationships.

Method: Develops multi-task regression GINN-LP (MTRGINN-LP) with a shared backbone containing multiple power-term approximator blocks and task-specific output layers, integrating GINN-LP into a multi-task deep learning framework to capture inter-target dependencies while preserving interpretability.

Result: Demonstrates competitive predictive performance and high interpretability on practical multi-target applications, including energy efficiency prediction and sustainable agriculture.

Conclusion: Extends symbolic regression to multi-output real-world tasks without sacrificing interpretability, enabling better modeling of inter-target dependencies in multi-target SR.

Abstract: In the area of explainable artificial intelligence, Symbolic Regression (SR) has emerged as a promising approach by discovering interpretable mathematical expressions that fit data. However, SR faces two main challenges: most methods are evaluated on scientific datasets with well-understood relationships, limiting generalization, and SR primarily targets single-output regression, whereas many real-world problems involve multi-target outputs with interdependent variables. To address these issues, we propose multi-task regression GINN-LP (MTRGINN-LP), an interpretable neural network for multi-target symbolic regression. By integrating GINN-LP with a multi-task deep learning, the model combines a shared backbone including multiple power-term approximator blocks with task-specific output layers, capturing inter-target dependencies while preserving interpretability. We validate multi-task GINN-LP on practical multi-target applications, including energy efficiency prediction and sustainable agriculture. Experimental results demonstrate competitive predictive performance alongside high interpretability, effectively extending symbolic regression to broader real-world multi-output tasks.

</details>


### [693] [AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate](https://arxiv.org/abs/2511.13465)
*Meng Zhu,Quan Xiao,Weidong Min*

Main category: cs.LG

TL;DR: AdamX introduces a novel exponential decay rate for the second-order moment estimation in Adam, gradually weakening the learning step correction as training progresses and effectively devolving to SGD in the stable phase, aiming to improve training stability and potentially generalization in large-scale models.


<details>
  <summary>Details</summary>
Motivation: Adam can bias toward non-flat minima and may struggle with stability and generalization in very large models; there is a need for optimization dynamics that transition toward SGD-like behavior during stable training to improve robustness and generalization.

Method: Propose AdamX, which modifies the second-order moment estimation with an exponential decay rate that decreases over time, reducing adaptive correction and eventually behaving like SGD in the stable period; assess performance against Adam and variants on large-scale model tasks; code is open-source at GitHub.

Result: Experimental results suggest the new second-order moment estimation decay rate outperforms existing estimations, and AdamX achieves stable, improved performance over Adam and its variants.

Conclusion: The approach offers improved training stability and potential generalization by transitioning toward SGD during stable training; further validation across tasks is warranted; code is publicly available at the provided GitHub repository.

Abstract: Since the 21st century, artificial intelligence has been leading a new round of industrial revolution. Under the training framework, the optimization algorithm aims to stably converge high-dimensional optimization to local and even global minima. Entering the era of large language models, although the scale of model parameters and data has increased, Adam remains the mainstream optimization algorithm. However, compared with stochastic gradient descent (SGD) based optimization algorithms, Adam is more likely to converge to non-flat minima. To address this issue, the AdamX algorithm is proposed. Its core innovation lies in the proposition of a novel type of second-order moment estimation exponential decay rate, which gradually weakens the learning step correction strength as training progresses, and degrades to SGD in the stable training period, thereby improving the stability of training in the stable period and possibly enhancing generalization ability. Experimental results show that our second-order moment estimation exponential decay rate is better than the current second-order moment estimation exponential decay rate, and AdamX can stably outperform Adam and its variants in terms of performance. Our code is open-sourced at https://github.com/mengzhu0308/AdamX.

</details>


### [694] [GREAT: Generalizable Representation Enhancement via Auxiliary Transformations for Zero-Shot Environmental Prediction](https://arxiv.org/abs/2511.13469)
*Shiyuan Luo,Chonghao Qiu,Runlong Yu,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: GREAT is a data-augmentation framework for environmental modeling that learns multi-layer transformations and uses bi-level training to preserve core physical patterns and temporal coherence, improving zero-shot generalization to unseen regions; demonstrated on stream temperature across six eastern U.S. watersheds with strong performance gains.


<details>
  <summary>Details</summary>
Motivation: Environmental modeling faces limited, geographically imbalanced data and spatial heterogeneity, causing models to learn spurious, region-specific patterns. Augmentation must preserve invariant physical relationships and temporal coherence to generalize to unmonitored regions.

Method: GREAT (Generalizable Representation Enhancement via Auxiliary Transformations) learns transformation functions at multiple neural-network layers to augment both raw environmental features and temporal influence. These transformations are refined through a bi-level training process that constrains augmented data to preserve key patterns of the source data and ensure recoverability of the original governing processes.

Result: Experimental results show GREAT significantly outperforms existing methods in zero-shot scenarios for stream temperature prediction across six diverse watersheds in the eastern U.S.

Conclusion: GREAT offers a practical augmentation-based solution for environmental applications with limited monitoring, enhancing generalization to unseen regions while maintaining core physical and temporal relationships.

Abstract: Environmental modeling faces critical challenges in predicting ecosystem dynamics across unmonitored regions due to limited and geographically imbalanced observation data. This challenge is compounded by spatial heterogeneity, causing models to learn spurious patterns that fit only local data. Unlike conventional domain generalization, environmental modeling must preserve invariant physical relationships and temporal coherence during augmentation. In this paper, we introduce Generalizable Representation Enhancement via Auxiliary Transformations (GREAT), a framework that effectively augments available datasets to improve predictions in completely unseen regions. GREAT guides the augmentation process to ensure that the original governing processes can be recovered from the augmented data, and the inclusion of the augmented data leads to improved model generalization. Specifically, GREAT learns transformation functions at multiple layers of neural networks to augment both raw environmental features and temporal influence. They are refined through a novel bi-level training process that constrains augmented data to preserve key patterns of the original source data. We demonstrate GREAT's effectiveness on stream temperature prediction across six ecologically diverse watersheds in the eastern U.S., each containing multiple stream segments. Experimental results show that GREAT significantly outperforms existing methods in zero-shot scenarios. This work provides a practical solution for environmental applications where comprehensive monitoring is infeasible.

</details>


### [695] [Quantum Machine Learning via Contrastive Training](https://arxiv.org/abs/2511.13497)
*Liudmila A. Zhukas,Vivian Ni Zhang,Qiang Miao,Qingfeng Wang,Marko Cetina,Jungsang Kim,Lawrence Carin,Christopher Monroe*

Main category: cs.LG

TL;DR: Self-supervised contrastive pretraining of quantum representations on hardware improves label-efficient image classification on a trapped-ion quantum computer.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of labeled data in quantum machine learning and to leverage unlabeled data to learn invariances, enabling scalable, hardware-efficient quantum representations.

Method: Encode images as quantum states on a programmable trapped-ion device; perform in situ contrastive pretraining using measured quantum overlaps to learn invariant representations; train and classify entirely on hardware.

Result: Pretrained representations achieve higher mean test accuracy and lower run-to-run variability than random initialization, with strongest gains in low-label regimes; learned invariances generalize beyond the pretraining samples.

Conclusion: Demonstrates a practical, hardware-based, label-efficient route to quantum representation learning with applicability to quantum-native datasets and larger classical inputs.

Abstract: Quantum machine learning (QML) has attracted growing interest with the rapid parallel advances in large-scale classical machine learning and quantum technologies. Similar to classical machine learning, QML models also face challenges arising from the scarcity of labeled data, particularly as their scale and complexity increase. Here, we introduce self-supervised pretraining of quantum representations that reduces reliance on labeled data by learning invariances from unlabeled examples. We implement this paradigm on a programmable trapped-ion quantum computer, encoding images as quantum states. In situ contrastive pretraining on hardware yields a representation that, when fine-tuned, classifies image families with higher mean test accuracy and lower run-to-run variability than models trained from random initialization. Performance improvement is especially significant in regimes with limited labeled training data. We show that the learned invariances generalize beyond the pretraining image samples. Unlike prior work, our pipeline derives similarity from measured quantum overlaps and executes all training and classification stages on hardware. These results establish a label-efficient route to quantum representation learning, with direct relevance to quantum-native datasets and a clear path to larger classical inputs.

</details>


### [696] [Naga: Vedic Encoding for Deep State Space Models](https://arxiv.org/abs/2511.13510)
*Melanie Schaller,Nick Janssen,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: Naga: bidirectional deep SSM encoding inspired by Vedic mathematics for long-term time-series forecasting; shows strong performance and efficiency across benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve long-range temporal modeling with interpretability and efficiency by introducing structured, math-inspired decomposition.

Method: Bidirectional processing of forward and time-reversed sequences, fused via Hadamard interaction within a deep State Space Model framework.

Result: Outperforms 28 state-of-the-art models and demonstrates improved efficiency versus existing deep SSM approaches on datasets ETTh1, ETTh2, ETTm1, ETTm2, Weather, Traffic, and ILI.

Conclusion: Vedic-inspired structured decomposition provides an interpretable and computationally efficient alternative for long-range sequence modeling.

Abstract: This paper presents Naga, a deep State Space Model (SSM) encoding approach inspired by structural concepts from Vedic mathematics. The proposed method introduces a bidirectional representation for time series by jointly processing forward and time-reversed input sequences. These representations are then combined through an element-wise (Hadamard) interaction, resulting in a Vedic-inspired encoding that enhances the model's ability to capture temporal dependencies across distant time steps. We evaluate Naga on multiple long-term time series forecasting (LTSF) benchmarks, including ETTh1, ETTh2, ETTm1, ETTm2, Weather, Traffic, and ILI. The experimental results show that Naga outperforms 28 current state of the art models and demonstrates improved efficiency compared to existing deep SSM-based approaches. The findings suggest that incorporating structured, Vedic-inspired decomposition can provide an interpretable and computationally efficient alternative for long-range sequence modeling.

</details>


### [697] [A Quantum Tensor Network-Based Viewpoint for Modeling and Analysis of Time Series Data](https://arxiv.org/abs/2511.13514)
*Pragatheeswaran Vipulananthan,Kamal Premaratne,Dilip Sarkar,Manohar N. Murthi*

Main category: cs.LG

TL;DR: A quantum-inspired white-box framework using kernel mean embeddings to build a spin-chain Hamiltonian in RKHS, solving Schrödinger equation with perturbation theory to quantify uncertainty in time-series tasks, improving interpretability for change point detection and clustering.


<details>
  <summary>Details</summary>
Motivation: Address the interpretability-accuracy gap: provide accurate uncertainty quantification while remaining interpretable, leveraging quantum-inspired modeling and RKHS embeddings.

Method: Map time-series data via the kernel mean embedding into an RKHS; construct a tensor-network-inspired 1D spin chain Hamiltonian with the KME as an eigenfunction; solve the Schrödinger equation; apply perturbation theory to quantify uncertainties; compare against state-of-the-art white-box models on change point detection and time-series clustering.

Result: Reportedly competitive with state-of-the-art white-box models, with enhanced interpretability and explicit uncertainty quantification for decision-making in change point detection and clustering.

Conclusion: The approach offers a principled, interpretable framework combining accuracy with quantified uncertainty, potentially bridging the gap between neural networks and traditional white-box models in time-series analysis.

Abstract: Accurate uncertainty quantification is a critical challenge in machine learning. While neural networks are highly versatile and capable of learning complex patterns, they often lack interpretability due to their ``black box'' nature. On the other hand, probabilistic ``white box'' models, though interpretable, often suffer from a significant performance gap when compared to neural networks. To address this, we propose a novel quantum physics-based ``white box'' method that offers both accurate uncertainty quantification and enhanced interpretability. By mapping the kernel mean embedding (KME) of a time series data vector to a reproducing kernel Hilbert space (RKHS), we construct a tensor network-inspired 1D spin chain Hamiltonian, with the KME as one of its eigen-functions or eigen-modes. We then solve the associated Schr{ö}dinger equation and apply perturbation theory to quantify uncertainty, thereby improving the interpretability of tasks performed with the quantum tensor network-based model. We demonstrate the effectiveness of this methodology, compared to state-of-the-art ``white box" models, in change point detection and time series clustering, providing insights into the uncertainties associated with decision-making throughout the process.

</details>


### [698] [Mitigating Spurious Correlations in Patch-wise Tumor Classification on High-Resolution Multimodal Images](https://arxiv.org/abs/2511.13527)
*Ihab Asaad,Maha Shadaydeh,Joachim Denzler*

Main category: cs.LG

TL;DR: Patch-wise binary tumor detection on high-res multimodal microscopy shows spurious correlation between patch size and label; applying GERNE debiasing improves worst-group accuracy by ~7% over ERM, boosting performance on minority cases and underscoring the need for spurious correlation-aware learning.


<details>
  <summary>Details</summary>
Motivation: Patch-wise classification reduces annotation cost and simplifies training, but can exploit spurious correlations (e.g., tumor patches have larger tissue regions) that bias predictions; addressing this bias is crucial for reliable detection, especially for minority groups.

Method: Apply GERNE debiasing, adapted to maximize worst-group accuracy (WGA), to patch-wise binary classification. Evaluate using two binarization thresholds for the spurious feature to assess robustness across groupings.

Result: WGA improved by about 7% compared to ERM across two thresholds, with better performance on minority cases such as tumor patches with small tissues and non-tumor patches with large tissues.

Conclusion: Spurious correlations can distort patch-wise classifiers; debiasing via GERNE enhances fairness/robustness and performance on minority groups, highlighting the need for spurious-correlation-aware learning in patch-wise tasks.

Abstract: Patch-wise multi-label classification provides an efficient alternative to full pixel-wise segmentation on high-resolution images, particularly when the objective is to determine the presence or absence of target objects within a patch rather than their precise spatial extent. This formulation substantially reduces annotation cost, simplifies training, and allows flexible patch sizing aligned with the desired level of decision granularity. In this work, we focus on a special case, patch-wise binary classification, applied to the detection of a single class of interest (tumor) on high-resolution multimodal nonlinear microscopy images. We show that, although this simplified formulation enables efficient model development, it can introduce spurious correlations between patch composition and labels: tumor patches tend to contain larger tissue regions, whereas non-tumor patches often consist mostly of background with small tissue areas. We further quantify the bias in model predictions caused by this spurious correlation, and propose to use a debiasing strategy to mitigate its effect. Specifically, we apply GERNE, a debiasing method that can be adapted to maximize worst-group accuracy (WGA). Our results show an improvement in WGA by approximately 7% compared to ERM for two different thresholds used to binarize the spurious feature. This enhancement boosts model performance on critical minority cases, such as tumor patches with small tissues and non-tumor patches with large tissues, and underscores the importance of spurious correlation-aware learning in patch-wise classification problems.

</details>


### [699] [Fairness-Aware Graph Representation Learning with Limited Demographic Information](https://arxiv.org/abs/2511.13540)
*Zichong Wang,Zhipeng Yin,Liping Yang,Jun Zhuang,Rui Yu,Qingzhao Kong,Wenbin Zhang*

Main category: cs.LG

TL;DR: A fair graph learning framework, FairGLite, mitigates bias with limited demographic data by generating proxies, enforcing cross-group embedding consistency, and using adaptive node weighting, with theoretical fairness guarantees and strong empirical performance.


<details>
  <summary>Details</summary>
Motivation: Fairness in Graph Neural Networks (GNNs) is crucial for trustworthy ML, but real-world applications often lack full demographic information due to privacy, legal, or regulatory constraints. There is a need for methods that can mitigate bias under partial demographic data.

Method: The approach uses partial demographic data to generate proxies for demographic information. It enforces consistent node embeddings across demographic groups. It introduces an adaptive confidence strategy that dynamically adjusts each node's contribution to fairness and utility based on prediction confidence. The framework, FairGLite, is analyzed theoretically to provide provable upper bounds on group fairness metrics.

Result: The framework achieves provable upper bounds on group fairness metrics and demonstrates effectiveness in reducing bias while maintaining model utility across multiple datasets and fair graph learning frameworks through extensive experiments.

Conclusion: FairGLite offers formal fairness guarantees under limited demographic information and practical improvement in bias mitigation with preserved utility, contributing to trustworthy fair graph learning under privacy constraints.

Abstract: Ensuring fairness in Graph Neural Networks is fundamental to promoting trustworthy and socially responsible machine learning systems. In response, numerous fair graph learning methods have been proposed in recent years. However, most of them assume full access to demographic information, a requirement rarely met in practice due to privacy, legal, or regulatory restrictions. To this end, this paper introduces a novel fair graph learning framework that mitigates bias in graph learning under limited demographic information. Specifically, we propose a mechanism guided by partial demographic data to generate proxies for demographic information and design a strategy that enforces consistent node embeddings across demographic groups. In addition, we develop an adaptive confidence strategy that dynamically adjusts each node's contribution to fairness and utility based on prediction confidence. We further provide theoretical analysis demonstrating that our framework, FairGLite, achieves provable upper bounds on group fairness metrics, offering formal guarantees for bias mitigation. Through extensive experiments on multiple datasets and fair graph learning frameworks, we demonstrate the framework's effectiveness in both mitigating bias and maintaining model utility.

</details>


### [700] [Graph Out-of-Distribution Detection via Test-Time Calibration with Dual Dynamic Dictionaries](https://arxiv.org/abs/2511.13541)
*Yue Hou,Ruomei Liu,Yingke Su,Junran Wu,Ke Xu*

Main category: cs.LG

TL;DR: BaCa is a test-time graph OOD detection method that calibrates OOD scores without fine-tuning pre-trained models by using dual dynamic dictionaries to capture in-distribution and out-of-distribution representations, estimating graphons, and applying mix-up on test samples to create boundary-aware topologies, achieving state-of-the-art detection performance.


<details>
  <summary>Details</summary>
Motivation: Ground-truth OOD samples are absent during training, making it hard for models to delineate OOD boundaries. Existing ID-focused methods fail to capture distribution boundaries, and graph data often have multiple latent factors that are underexplored. A boundary-aware, calibration-based approach at test time is needed to improve OOD detection without requiring fine-tuning or extra OOD data.

Method: BaCa constructs dual dynamic dictionaries (via priority queues and attention mechanisms) to adaptively model latent ID and OOD representations. It estimates graphons and uses a test-time mix-up strategy with only test samples to generate diverse, boundary-aware topologies, eliminating dependence on auxiliary outlier datasets. The dictionaries enable boundary-aware OOD score calibration by comparing test samples against learned ID and OOD representations.

Result: Extensive experiments on real-world datasets show that BaCa significantly outperforms existing state-of-the-art methods for graph OOD detection.

Conclusion: BaCa offers a practical, tuning-free approach to graph OOD detection at test time by dynamically learning dual ID/OOD representations and boundary-aware scores through graphon estimation and test-time mix-ups, yielding superior detection performance.

Abstract: A key challenge in graph out-of-distribution (OOD) detection lies in the absence of ground-truth OOD samples during training. Existing methods are typically optimized to capture features within the in-distribution (ID) data and calculate OOD scores, which often limits pre-trained models from representing distributional boundaries, leading to unreliable OOD detection. Moreover, the latent structure of graph data is often governed by multiple underlying factors, which remains less explored. To address these challenges, we propose a novel test-time graph OOD detection method, termed BaCa, that calibrates OOD scores using dual dynamically updated dictionaries without requiring fine-tuning the pre-trained model. Specifically, BaCa estimates graphons and applies a mix-up strategy solely with test samples to generate diverse boundary-aware discriminative topologies, eliminating the need for exposing auxiliary datasets as outliers. We construct dual dynamic dictionaries via priority queues and attention mechanisms to adaptively capture latent ID and OOD representations, which are then utilized for boundary-aware OOD score calibration. To the best of our knowledge, extensive experiments on real-world datasets show that BaCa significantly outperforms existing state-of-the-art methods in OOD detection.

</details>


### [701] [RAC-DMVC: Reliability-Aware Contrastive Deep Multi-View Clustering under Multi-Source Noise](https://arxiv.org/abs/2511.13561)
*Shihao Dong,Yue Liu,Xiaotong Zhou,Yuhui Zheng,Huiying Xu,Xinzhong Zhu*

Main category: cs.LG

TL;DR: Reliability-aware RAC-DMVC for robust multi-view clustering under multi-source noise, using a reliability graph, cross-view reconstruction, noise-contrastive learning, dual-attention imputation, and cluster distillation; achieves SOTA on five datasets.


<details>
  <summary>Details</summary>
Motivation: Multi-view clustering (MVC) faces real-world challenges from missing views and observation noise. Existing MVC methods struggle to learn robust representations under such noise; a reliability-guided framework is needed to improve performance and robustness.

Method: 1) Build a reliability graph to guide robust representation learning. 2) Address observation noise with cross-view reconstruction and reliability-aware noise-contrastive learning to reduce bias in positive/negative pair selection. 3) Handle missing noise using dual-attention imputation to capture shared information across views while preserving view-specific features. 4) Apply a self-supervised cluster distillation module to refine representations and enhance clustering. 

Result: Experiments on five benchmark datasets show RAC-DMVC outperforms state-of-the-art methods on multiple metrics and maintains strong performance across varying noise ratios.

Conclusion: RAC-DMVC effectively addresses multi-source noise in MVC, delivering superior clustering performance. The reliability-guided approach, cross-view reconstruction, noise-contrastive learning, and dual-attention imputation contribute to robustness and accuracy across noisy scenarios.

Abstract: Multi-view clustering (MVC), which aims to separate the multi-view data into distinct clusters in an unsupervised manner, is a fundamental yet challenging task. To enhance its applicability in real-world scenarios, this paper addresses a more challenging task: MVC under multi-source noises, including missing noise and observation noise. To this end, we propose a novel framework, Reliability-Aware Contrastive Deep Multi-View Clustering (RAC-DMVC), which constructs a reliability graph to guide robust representation learning under noisy environments. Specifically, to address observation noise, we introduce a cross-view reconstruction to enhances robustness at the data level, and a reliability-aware noise contrastive learning to mitigates bias in positive and negative pairs selection caused by noisy representations. To handle missing noise, we design a dual-attention imputation to capture shared information across views while preserving view-specific features. In addition, a self-supervised cluster distillation module further refines the learned representations and improves the clustering performance. Extensive experiments on five benchmark datasets demonstrate that RAC-DMVC outperforms SOTA methods on multiple evaluation metrics and maintains excellent performance under varying ratios of noise.

</details>


### [702] [P1: Mastering Physics Olympiads with Reinforcement Learning](https://arxiv.org/abs/2511.13612)
*Jiacheng Chen,Qianjia Cheng,Fangchen Yu,Haiyuan Wan,Yuchen Zhang,Shenghe Zheng,Junchi Yao,Qingyang Zhang,Haonan He,Yun Luo,Yufeng Zhao,Futing Wang,Li Sheng,Chengxing Xie,Yuxin Zuo,Yizhuo Li,Wenxauan Zeng,Yulun Wu,Rui Huang,Dongzhan Zhou,Kai Chen,Yu Qiao,Lei Bai,Yu Cheng,Ning Ding,Bowen Zhou,Peng Ye,Ganqu Cui*

Main category: cs.LG

TL;DR: Open-source physics-reasoning LLM family (P1) trained entirely with RL; achieves top performance at the International Physics Olympiad (IPhO 2025), with P1-235B-A22B leading via Gold medals and a further agentic extension (PhysicsMinions) securing No.1 overall; shows strong generalization to math and coding.


<details>
  <summary>Details</summary>
Motivation: Advance physics research and science-grade reasoning in LLMs by binding symbols to physical reality; create accessible, open-source models capable of solving Olympiad-level physics problems and broader reasoning tasks.

Method: Develop a family of open-source LLMs (P1) trained entirely through reinforcement learning (RL). Evaluate on IPhO 2025 and related competitions; introduce PhysicsMinions as an agentic framework to augment performance.

Result: P1-235B-A22B achieves Gold medals at IPhO 2025 and, along with 12 Golds out of 13 medals in 2024/2025 across competitions; P1-30B-A3B achieves Silver at IPhO 2025; P1-235B-A22B+PhysicsMinions attains No.1 overall on IPhO 2025 with highest average score; P1 models also perform well on math and coding tasks.

Conclusion: RL-trained, open-source physics-reasoning models can achieve state-of-the-art performance on physics olympiad problems and general reasoning tasks; agentic augmentation can yield further gains; this approach supports broader generalization and open scientific collaboration.

Abstract: Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.

</details>


### [703] [Batch Acquisition Function Evaluations and Decouple Optimizer Updates for Faster Bayesian Optimization](https://arxiv.org/abs/2511.13625)
*Kaichi Irie,Shuhei Watanabe,Masaki Onishi*

Main category: cs.LG

TL;DR: Decouples quasi-Newton (QN) updates in multi-start optimization for Bayesian optimization using coroutines while batching acquisition-function calls; achieves identical convergence to sequential MSO with substantially faster wall-clock time.


<details>
  <summary>Details</summary>
Motivation: Acquisition-function optimization in Bayesian optimization is non-convex. Multi-start optimization with QN methods is a bottleneck, and current BoTorch strategies batch the sum of the acquisition function across points, which introduces off-diagonal inverse-Hessian approximation errors that slow convergence.

Method: Introduce coroutine-based decoupling of QN updates from batched acquisition-function evaluations. This allows overlapping computations: QN updates are performed in a decoupled manner while acquisition function calls are batched, preserving the convergence characteristics of sequential MSO while exploiting batching efficiencies.

Result: The approach yields theoretically identical convergence to sequential MSO and substantially reduces wall-clock time compared to prior batched approaches by mitigating Hessian-geometry issues and enabling efficient batching.

Conclusion: Coroutine-based decoupling provides a practical and generalizable mechanism to accelerate MSO in Bayesian optimization without sacrificing convergence guarantees, with potential applicability to other batched optimization settings.

Abstract: Bayesian optimization (BO) efficiently finds high-performing parameters by maximizing an acquisition function, which models the promise of parameters. A major computational bottleneck arises in acquisition function optimization, where multi-start optimization (MSO) with quasi-Newton (QN) methods is required due to the non-convexity of the acquisition function. BoTorch, a widely used BO library, currently optimizes the summed acquisition function over multiple points, leading to the speedup of MSO owing to PyTorch batching. Nevertheless, this paper empirically demonstrates the suboptimality of this approach in terms of off-diagonal approximation errors in the inverse Hessian of a QN method, slowing down its convergence. To address this problem, we propose to decouple QN updates using a coroutine while batching the acquisition function calls. Our approach not only yields the theoretically identical convergence to the sequential MSO but also drastically reduces the wall-clock time compared to the previous approaches.

</details>


### [704] [Towards Multimodal Representation Learning in Paediatric Kidney Disease](https://arxiv.org/abs/2511.13637)
*Ana Durica,John Booth,Ivana Drobnjak*

Main category: cs.LG

TL;DR: A pilot study using an RNN on pediatric EHR lab sequences and demographics to predict abnormal serum creatinine within 30 days, showing feasibility of simple temporal representations for pediatric renal monitoring.


<details>
  <summary>Details</summary>
Motivation: Pediatric kidney diseases vary widely; continuous, timely monitoring is needed; leveraging routine EHR data to forecast near-term renal dysfunction could enable earlier intervention.

Method: Retrospective analysis of EHR data (2019–2025) from Great Ormond Street Hospital; train a recurrent neural network on longitudinal laboratory values plus demographics to predict abnormal serum creatinine within the next 30 days; framed as a pilot; discusses potential multimodal extensions.

Result: The approach demonstrates that simple temporal representations can capture predictive patterns in routine pediatric data; initial demonstration of feasibility for near-term renal risk prediction.

Conclusion: Temporal modeling on EHR data is feasible for pediatric renal monitoring and sets the stage for richer multimodal models and more granular renal outcomes in future work.

Abstract: Paediatric kidney disease varies widely in its presentation and progression, which calls for continuous monitoring of renal function. Using electronic health records collected between 2019 and 2025 at Great Ormond Street Hospital, a leading UK paediatric hospital, we explored a temporal modelling approach that integrates longitudinal laboratory sequences with demographic information. A recurrent neural model trained on these data was used to predict whether a child would record an abnormal serum creatinine value within the following thirty days. Framed as a pilot study, this work provides an initial demonstration that simple temporal representations can capture useful patterns in routine paediatric data and lays the groundwork for future multimodal extensions using additional clinical signals and more detailed renal outcomes.

</details>


### [705] [Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures](https://arxiv.org/abs/2511.13640)
*Haohui Wang,Jingyuan Qi,Jianpeng Chen,Jun Wu,Lifu Huang,Lecheng Zheng,Kevin Choi,Balaji Veeramani,Edward Bowen,Alison Hu,Tyler Cody,Dawei Zhou*

Main category: cs.LG

TL;DR: The paper studies mixed real-synthetic data for LLMs, identifies a three-phase scaling law with two breakpoints, derives a generalization bound for such mixtures, and introduces a scalable data-valuing method that outperforms baselines on four tasks with lower computation.


<details>
  <summary>Details</summary>
Motivation: Synthetic data enable scalable model training but introduce distributional gaps, especially underrepresenting long-tail knowledge due to truncation in generation (top-p, temperature, finite sampling). This creates challenges in characterizing, evaluating, and valuing mixed real-synthetic datasets; a theory-backed, efficient valuation method is needed.

Method: 1) Characterize a three-phase scaling behavior with two breakpoints reflecting transitions in head vs tail knowledge; 2) derive an LLM generalization bound for real-synthetic mixtures; 3) propose and validate a scalable data valuation method; 4) empirical validation across four tasks.

Result: Observed the three-phase scaling with two breakpoints indicating transitions in learning head and tail knowledge; the derived generalization bound identifies factors that govern performance on real-synthetic mixtures; the proposed data-valuation method achieves state-of-the-art or competitive results with substantially lower computational cost across the four tasks (image classification, sentiment classification, instruction following, and complex reasoning).

Conclusion: The work provides both theoretical and practical contributions: a formal understanding of how real-synthetic data mixtures scale and generalize in LLMs, and a scalable data-valuing approach that consistently outperforms baselines while reducing computation, validated across diverse tasks.

Abstract: The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.

</details>


### [706] [FuseSampleAgg: Fused Neighbor Sampling and Aggregation for Mini-batch GNNs](https://arxiv.org/abs/2511.13645)
*Aleksandar Stanković*

Main category: cs.LG

TL;DR: FuseSampleAgg is a CUDA operator that fuses neighbor sampling and mean aggregation for one- and two-hop GraphSAGE. It eliminates block materialization and extra kernel launches, reducing memory traffic while preserving GraphSAGE mean semantics via saved index replay. On Reddit, ogbn-arxiv, and ogbn-products (batch 1024, AMP enabled) it delivers up to 51x speedups (ogbn-products), ~4x on Reddit (fanouts 10-10 and 15-10), and ~3.3x on ogbn-arxiv at larger fanouts, with peak memory reductions up to 100x, 36x, and ~3.5x respectively. The operator is deterministic, PyTorch-compatible, and comes with scripts to reproduce results. Code is available at https://github.com/SV25-22/FuseSampleAgg.


<details>
  <summary>Details</summary>
Motivation: GraphSAGE with neighbor sampling incurs data movement and kernel launch overhead due to separate sampling and aggregation steps, plus block materialization. This degrades throughput and inflates memory usage. A single-pass, semantics-preserving fused operator could reduce memory traffic and runtime while remaining compatible with standard PyTorch training.

Method: Introduce FuseSampleAgg: a CUDA operator that fuses neighbor sampling and mean aggregation into a single pass for 1-2 hop GraphSAGE. It eliminates block materialization and extra kernel launches, and uses saved index replay to keep mean semantics. The implementation is deterministic, AMP-compatible, and integrates with PyTorch optimizers. It ships with scripts that reproduce all results from CSV logs.

Result: Performance improvements observed across benchmarks: up to 51x speedups on ogbn-products, about 4x on Reddit (fanouts 10-10 and 15-10), and about 3.3x on ogbn-arxiv at larger fanouts. Peak GPU memory reductions were up to 100x (ogbn-products), 36x (Reddit), and about 3.5x (ogbn-arxiv). The operator is deterministic and PyTorch-compatible; code and scripts are provided for reproducibility.

Conclusion: FuseSampleAgg delivers substantial throughput and memory benefits while preserving GraphSAGE mean semantics and ensuring deterministic behavior. It integrates smoothly with standard PyTorch workflows and is openly accessible for reproduction and adaptation.

Abstract: We present FuseSampleAgg, a CUDA operator that fuses neighbor sampling and mean aggregation into a single pass for one and two hop GraphSAGE. By eliminating block materialization and extra kernel launches, FuseSampleAgg reduces memory traffic and overhead while preserving GraphSAGE mean semantics via saved index replay. Across the Reddit, ogbn-arxiv, and ogbn-products benchmarks (batch size 1024, automatic mixed precision enabled), we observe step time speedups up to 51x on ogbn-products, about 4x on Reddit with fanouts 10-10 and 15-10, and about 3.3x on ogbn-arxiv at larger fanouts, with peak GPU memory reductions up to 100x, 36x, and about 3.5x, respectively. The operator is deterministic, integrates with standard PyTorch optimizers, and ships with scripts that reproduce all tables and figures from CSV logs. Code and scripts are available at https://github.com/SV25-22/FuseSampleAgg.

</details>


### [707] [Weight-sparse transformers have interpretable circuits](https://arxiv.org/abs/2511.13653)
*Leo Gao,Achyuta Rajaram,Jacob Coxon,Soham V. Govande,Bowen Baker,Dan Mossing*

Main category: cs.LG

TL;DR: The authors propose training language models with heavy weight sparsity to reveal human-understandable circuits. By constraining and pruning connections, they isolate subnetworks responsible for specific tasks, yielding neurons and channels that map to interpretable concepts. They find that sparsity reduces capability while scaling model size improves the interpretability-capability frontier; however, scaling sparse models beyond tens of millions of nonzero parameters remains challenging. They also show preliminary results suggesting their approach can be adapted to explain existing dense models. Overall, they claim unprecedented levels of human understandability, validated with rigorous evidence.


<details>
  <summary>Details</summary>
Motivation: A central goal of mechanistic interpretability is to identify circuits inside language models that humans can understand. Sparse connectivity may reveal modular, interpretable subnetworks and disentangle concepts, but there is a tension between interpretability and performance, especially as models scale. The work investigates whether sparsity can yield interpretable circuits without sacrificing too much capability and whether the approach can generalize to dense models.

Method: Train models with strong weight sparsity (most weights zero) so that each neuron has few connections. Prune the model to isolate the component responsible for a given hand-crafted task, resulting in circuits that can be examined for interpretable structure (neurons and residual channels corresponding to natural concepts with direct, interpretable connections). Study scaling by varying model size and sparsity to assess the capability-interpretability frontier. Explore adapting the sparsity approach to explain existing dense models (preliminary).

Result: Circuits identified within sparse models often align with natural concepts and include interpretable connections. There is a trade-off: increasing sparsity tends to reduce capability, while larger models improve the capability-interpretability frontier. Scaling sparse models beyond tens of millions of nonzero parameters remains challenging to maintain interpretability. The authors also present preliminary results suggesting their method can be adapted to explain dense models. The work reports unprecedented interpretability and validates findings with rigorous methods.

Conclusion: The study demonstrates that weight-sparse training can produce highly interpretable circuits in language models and that interpretability can improve with model scaling up to a point. However, scaling sparse models to very large sizes without losing interpretability is an open challenge. The approach shows promise for explaining existing dense models and provides a rigorous validation of the interpretability gains achieved.

Abstract: Finding human-understandable circuits in language models is a central goal of the field of mechanistic interpretability. We train models to have more understandable circuits by constraining most of their weights to be zeros, so that each neuron only has a few connections. To recover fine-grained circuits underlying each of several hand-crafted tasks, we prune the models to isolate the part responsible for the task. These circuits often contain neurons and residual channels that correspond to natural concepts, with a small number of straightforwardly interpretable connections between them. We study how these models scale and find that making weights sparser trades off capability for interpretability, and scaling model size improves the capability-interpretability frontier. However, scaling sparse models beyond tens of millions of nonzero parameters while preserving interpretability remains a challenge. In addition to training weight-sparse models de novo, we show preliminary results suggesting our method can also be adapted to explain existing dense models. Our work produces circuits that achieve an unprecedented level of human understandability and validates them with considerable rigor.

</details>


### [708] [Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning](https://arxiv.org/abs/2511.13654)
*Pascal Zimmer,Ghassan Karame*

Main category: cs.LG

TL;DR: Hyperparameter tuning reveals opposite effects for attacks: smaller learning rates boost transfer-based robustness, larger learning rates boost query-based robustness; distributed training with tuned hyperparameters offers the best joint defense against both attack types.


<details>
  <summary>Details</summary>
Motivation: Understand how optimization hyperparameters affect robustness to transfer-based and query-based attacks, across centralized, ensemble, and distributed training, and explore a joint design space for dual-attack resilience.

Method: Theoretical analysis supported by extensive experiments across centralized, ensemble, and distributed settings. Systematically vary optimization hyperparameters—learning rate, weight decay, momentum, and batch size—and measure robustness against transfer-based and query-based attacks under diverse data distributions.

Result: For transfer-based attacks, lowering the learning rate yields up to 64% robustness gain. For query-based attacks, increasing the learning rate yields up to 28% robustness gain. Distributed models benefit the most, achieving a stronger joint defense against both attack types than other training setups, demonstrating the first characterization of the optimization hyperparameter space for dual-attack robustness.

Conclusion: Optimization hyperparameters, particularly learning rate, have asymmetric effects on different attack modalities; jointly tuning them—especially in distributed training—enables a robust defense against both transfer-based and query-based attacks.

Abstract: In this paper, we present the first detailed analysis of how optimization hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the optimization hyperparameter design space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.

</details>


### [709] [Scientific Data Compression and Super-Resolution Sampling](https://arxiv.org/abs/2511.13675)
*Minh Vu,Andrey Lokhov*

Main category: cs.LG

TL;DR: A framework for compressing and upsampling scientific data using exponential-family learning, preserving uncertainty and enabling adjustable trade-offs between compression and fidelity.


<details>
  <summary>Details</summary>
Motivation: The growing volume of scientific data from simulations, observations, and experiments outpaces storage and analysis capabilities. There is a need to compress data while preserving essential physical quantities and to recover high-fidelity data from compressed representations with uncertainty guarantees, enabling reliable checkpointing and restarting.

Method: A novel framework built on learning exponential families to perform data compression and super-resolution. The approach is probabilistic, preserving and quantifying uncertainty in quantities of interest and allowing flexible rate–distortion trade-offs for different fidelity requirements.

Result: Introduction of a principled, uncertainty-aware framework for scientific data compression and super-resolution. It preserves and quantifies uncertainty in physical quantities and supports flexible trade-offs between compression ratio and reconstruction fidelity.

Conclusion: The framework offers a principled approach to manage and recover large-scale scientific data, enabling efficient storage and robust super-resolution for workflows such as checkpointing and restarting while maintaining key physical characteristics and their uncertainties.

Abstract: Modern scientific simulations, observations, and large-scale experiments generate data at volumes that often exceed the limits of storage, processing, and analysis. This challenge drives the development of data reduction methods that efficiently manage massive datasets while preserving essential physical features and quantities of interest. In many scientific workflows, it is also crucial to enable data recovery from compressed representations - a task known as super-resolution - with guarantees on the preservation of key physical characteristics. A notable example is checkpointing and restarting, which is essential for long-running simulations to recover from failures, resume after interruptions, or examine intermediate results. In this work, we introduce a novel framework for scientific data compression and super-resolution, grounded in recent advances in learning exponential families. Our method preserves and quantifies uncertainty in physical quantities of interest and supports flexible trade-offs between compression ratio and reconstruction fidelity.

</details>


### [710] [Protein Secondary Structure Prediction Using 3D Graphs and Relation-Aware Message Passing Transformers](https://arxiv.org/abs/2511.13685)
*Disha Varshney,Samarth Garg,Sarthak Tyagi,Deeksha Varshney,Nayan Deep,Asif Ekbal*

Main category: cs.LG

TL;DR: SSRGNet combines graph neural networks with a pre-trained protein language model to predict protein secondary structure by leveraging residue graphs with sequential and geometric connections, achieving improved F1-scores on NetSurfP-2.0.


<details>
  <summary>Details</summary>
Motivation: Explicitly leveraging 3D structural information, recognized as crucial for protein function, to improve secondary structure prediction beyond approaches that rely mainly on unlabeled sequence data.

Method: Construct residue-level graphs with diverse sequential/structural connections; encode sequences with a pre-trained transformer-based protein language model; apply graph neural networks (GCN/R-GCN) to propagate geometric information; perform local convolutions over neighborhoods; stack multiple layers to fuse sequence and 3D structural cues.

Result: SSRGNet surpasses baselines on F1-scores for both 3-state and 8-state secondary structure prediction on the NetSurfP-2.0 dataset.

Conclusion: Integrating GNNs with protein language models and explicit 3D residue graphs effectively captures spatial dependencies, yielding enhanced secondary structure prediction and showcasing the value of leveraging structural data alongside sequence models.

Abstract: In this study, we tackle the challenging task of predicting secondary structures from protein primary sequences, a pivotal initial stride towards predicting tertiary structures, while yielding crucial insights into protein activity, relationships, and functions. Existing methods often utilize extensive sets of unlabeled amino acid sequences. However, these approaches neither explicitly capture nor harness the accessible protein 3D structural data, which is recognized as a decisive factor in dictating protein functions. To address this, we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information. We adeptly combine Graph Neural Networks (GNNs) and Language Models (LMs), specifically utilizing a pre-trained transformer-based protein language model to encode amino acid sequences and employing message-passing mechanisms like GCN and R-GCN to capture geometric characteristics of protein structures. Employing convolution within a specific node's nearby region, including relations, we stack multiple convolutional layers to efficiently learn combined insights from the protein's spatial graph, revealing intricate interconnections and dependencies in its structural arrangement. To assess our model's performance, we employed the training dataset provided by NetSurfP-2.0, which outlines secondary structure in 3-and 8-states. Extensive experiments show that our proposed model, SSRGNet surpasses the baseline on f1-scores.

</details>


### [711] [Cross-Learning from Scarce Data via Multi-Task Constrained Optimization](https://arxiv.org/abs/2511.13680)
*Leopoldo Agorio,Juan Cerviño,Miguel Calvo-Fullana,Alejandro Ribeiro,Juan Andrés Bazerque*

Main category: cs.LG

TL;DR: A cross-learning multi-task framework for transferring knowledge across related tasks by jointly estimating deterministic parameters under similarity constraints, enabling better parameter inference with limited data.


<details>
  <summary>Details</summary>
Motivation: Data scarcity in supervised learning leads to poor generalization. By sharing information across related tasks, we can improve parameter estimates for tasks with scarce data while allowing task-specific differences.

Method: Formulate joint parameter estimation as a constrained optimization problem that enforces similarity constraints across tasks; enables information transfer from data-rich to data-scarce tasks while preserving task-specific parameters; provide theoretical guarantees in Gaussian settings and validate on real data.

Result: The approach yields more accurate and reliable parameter estimates under data limitations, with theoretical guarantees; demonstrated effectiveness on image classification and infectious disease propagation datasets.

Conclusion: Cross-learning offers a principled way to perform parameter inference under limited data by leveraging relationships among tasks, enabling effective knowledge transfer and improved generalization.

Abstract: A learning task, understood as the problem of fitting a parametric model from supervised data, fundamentally requires the dataset to be large enough to be representative of the underlying distribution of the source. When data is limited, the learned models fail generalize to cases not seen during training. This paper introduces a multi-task \emph{cross-learning} framework to overcome data scarcity by jointly estimating \emph{deterministic} parameters across multiple, related tasks. We formulate this joint estimation as a constrained optimization problem, where the constraints dictate the resulting similarity between the parameters of the different models, allowing the estimated parameters to differ across tasks while still combining information from multiple data sources. This framework enables knowledge transfer from tasks with abundant data to those with scarce data, leading to more accurate and reliable parameter estimates, providing a solution for scenarios where parameter inference from limited data is critical. We provide theoretical guarantees in a controlled framework with Gaussian data, and show the efficiency of our cross-learning method in applications with real data including image classification and propagation of infectious diseases.

</details>


### [712] [ST-ProC: A Graph-Prototypical Framework for Robust Semi-Supervised Travel Mode Identification](https://arxiv.org/abs/2511.13702)
*Luyao Niu,Nuoxian Huang*

Main category: cs.LG

TL;DR: ST-ProC is a graph-prototypical multi-objective SSL framework for travel mode identification from GPS with scarce labels, achieving large gains over state-of-the-art SSL methods.


<details>
  <summary>Details</summary>
Motivation: Label-scarce GPS-based travel mode identification; standard SSL suffers from confirmation bias and ignores the data manifold; need a framework that leverages graph structure, prototypes, and robust pseudo-labeling.

Method: Graph-prototypical core: graph regularization, prototypical anchoring, and margin-aware pseudo-labeling; supported by contrastive learning and teacher-student consistency losses; multi-objective optimization.

Result: Outperforms baselines by significant margin; 21.5% improvement over FixMatch; effective in real-world sparse-label settings.

Conclusion: ST-ProC effectively addresses label scarcity in TMI by integrating graph-based structure with multi-objective SSL, yielding robust representations and optimization.

Abstract: Travel mode identification (TMI) from GPS trajectories is critical for urban intelligence, but is hampered by the high cost of annotation, leading to severe label scarcity. Prevailing semi-supervised learning (SSL) methods are ill-suited for this task, as they suffer from catastrophic confirmation bias and ignore the intrinsic data manifold. We propose ST-ProC, a novel graph-prototypical multi-objective SSL framework to address these limitations. Our framework synergizes a graph-prototypical core with foundational SSL Support. The core exploits the data manifold via graph regularization, prototypical anchoring, and a novel, margin-aware pseudo-labeling strategy to actively reject noise. This core is supported and stabilized by foundational contrastive and teacher-student consistency losses, ensuring high-quality representations and robust optimization. ST-ProC outperforms all baselines by a significant margin, demonstrating its efficacy in real-world sparse-label settings, with a performance boost of 21.5% over state-of-the-art methods like FixMatch.

</details>


### [713] [Efficient Calibration for Decision Making](https://arxiv.org/abs/2511.13699)
*Parikshit Gopalan,Konstantinos Stavropoulos,Kunal Talwar,Pranay Tankala*

Main category: cs.LG

TL;DR: Introduces CDL_K, a tractable, structured calibration decision loss; develops theory on when CDL_K is information-theoretically and computationally tractable; derives upper and lower bounds for natural classes K; provides guarantees for recalibration procedures.


<details>
  <summary>Details</summary>
Motivation: CDL is intractable to approximate in the offline (black-box) setting. A practical, computable calibration measure is needed for decision-making under post-processing of predictors.

Method: Define CDL_K as the calibration decision loss with a restricted post-processing family K, allow all proper losses but restrict post-processing to K; develop a comprehensive theory of tractability (information-theoretic and computational) for CDL_K; prove upper and lower bounds for natural classes K; introduce new definitions and algorithmic techniques in calibration theory; derive guarantees for recalibration procedures.

Result: A theoretical framework establishing when CDL_K is tractable, with upper and lower bounds for natural classes K, plus new definitions and algorithms; yields rigorous performance guarantees for several widely used recalibration procedures in machine learning.

Conclusion: CDL_K provides a practical, theoretically grounded calibration measure that enables tractable analysis and guarantees for decision-making under recalibration, bridging theory and ML practice.

Abstract: A decision-theoretic characterization of perfect calibration is that an agent seeking to minimize a proper loss in expectation cannot improve their outcome by post-processing a perfectly calibrated predictor. Hu and Wu (FOCS'24) use this to define an approximate calibration measure called calibration decision loss ($\mathsf{CDL}$), which measures the maximal improvement achievable by any post-processing over any proper loss. Unfortunately, $\mathsf{CDL}$ turns out to be intractable to even weakly approximate in the offline setting, given black-box access to the predictions and labels.
  We suggest circumventing this by restricting attention to structured families of post-processing functions $K$. We define the calibration decision loss relative to $K$, denoted $\mathsf{CDL}_K$ where we consider all proper losses but restrict post-processings to a structured family $K$. We develop a comprehensive theory of when $\mathsf{CDL}_K$ is information-theoretically and computationally tractable, and use it to prove both upper and lower bounds for natural classes $K$. In addition to introducing new definitions and algorithmic techniques to the theory of calibration for decision making, our results give rigorous guarantees for some widely used recalibration procedures in machine learning.

</details>


### [714] [From Black Box to Insight: Explainable AI for Extreme Event Preparedness](https://arxiv.org/abs/2511.13712)
*Kiana Vu,İsmet Selçuk Özer,Phung Lai,Zheng Wu,Thilanka Munasinghe,Jennifer Wei*

Main category: cs.LG

TL;DR: XAI with SHAP improves wildfire forecasting by making predictions interpretable and actionable for practitioners and policymakers.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between predictive accuracy and actionable decision support in extreme event forecasting; address trust, explainability, and operational readiness in AI-based disaster prediction.

Method: Evaluate multiple AI models for wildfire prediction and apply SHAP to identify key features, decision pathways, and biases; supplement with visualizations of feature importance, seasonality, and geospatial patterns to aid interpretation.

Result: XAI clarifies model reasoning, supports critical decision-making by domain experts, and highlights biases; improves usability and trust in AI outputs for disaster response.

Conclusion: AI systems for disaster forecasting should be accurate, interpretable, accessible, and trustworthy to be effectively integrated into preparedness, risk mitigation, and climate resilience planning.

Abstract: As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.

</details>


### [715] [Learning stochasticity: a nonparametric framework for intrinsic noise estimation](https://arxiv.org/abs/2511.13701)
*Gianluigi Pillonetto,Alberto Giaretta,Mauro Bisiacco*

Main category: cs.LG

TL;DR: Trine is a nonparametric, kernel-based method that discovers state-dependent intrinsic noise from time-series data via a three-stage regression framework, performing comparably to an oracle across biological and ecological benchmarks.


<details>
  <summary>Details</summary>
Motivation: Incomplete knowledge of nonlinear interactions and stochastic effects undermines bottom-up modeling; intrinsic noise is often essential to understanding dynamics in gene networks and signaling pathways, necessitating methods that infer noise directly from data.

Method: Three-phase regression (Trine) that combines analytically solvable subproblems with a structured kernel to capture both abrupt, noise-driven fluctuations and smooth, state-dependent variance changes; nonparametric, time-series based inference of intrinsic noise.

Result: Validated on biological and ecological systems, Trine uncovers hidden dynamics without predefined parametric forms and achieves performance comparable to an ideal observer (oracle) that tracks random fluctuations.

Conclusion: Trine offers a new avenue to study how intrinsic noise influences complex system behavior, enabling accurate inference of state-dependent noise without strong parametric priors.

Abstract: Understanding the principles that govern dynamical systems is a central challenge across many scientific domains, including biology and ecology. Incomplete knowledge of nonlinear interactions and stochastic effects often renders bottom-up modeling approaches ineffective, motivating the development of methods that can discover governing equations directly from data. In such contexts, parametric models often struggle without strong prior knowledge, especially when estimating intrinsic noise. Nonetheless, incorporating stochastic effects is often essential for understanding the dynamic behavior of complex systems such as gene regulatory networks and signaling pathways. To address these challenges, we introduce Trine (Three-phase Regression for INtrinsic noisE), a nonparametric, kernel-based framework that infers state-dependent intrinsic noise from time-series data. Trine features a three-stage algorithm that com- bines analytically solvable subproblems with a structured kernel architecture that captures both abrupt noise-driven fluctuations and smooth, state-dependent changes in variance. We validate Trine on biological and ecological systems, demonstrating its ability to uncover hidden dynamics without relying on predefined parametric assumptions. Across several benchmark problems, Trine achieves performance comparable to that of an oracle. Biologically, this oracle can be viewed as an idealized observer capable of directly tracking the random fluctuations in molecular concentrations or reaction events within a cell. The Trine framework thus opens new avenues for understanding how intrinsic noise affects the behavior of complex systems.

</details>


### [716] [Rare Genomic Subtype Discovery from RNA-seq via Autoencoder Embeddings and Stability-Aware Clustering](https://arxiv.org/abs/2511.13705)
*Alaa Mezghiche*

Main category: cs.LG

TL;DR: Unsupervised autoencoder-based clustering reveals a rare reproducible KIRC subtype within kidney cancer after pan-cancer analysis dominated by tissue type.


<details>
  <summary>Details</summary>
Motivation: To detect rare, stable molecular subtypes in high-dimensional RNA-seq data without labels, avoiding confounding by tissue-of-origin by first assessing pan-cancer structure and then focusing within a cancer type.

Method: Autoencoder-based representation (top 2,000 highly variable genes; 128-d latent) followed by k-means clustering (k=2–10). Stability via a discovery rule: rare <10% and stability with Jaccard >=0.60 across 20 seeds after Hungarian alignment. Pan-cancer analysis across UCI RNA-Seq dataset, with differential expression (Welch t-test, BH-FDR).

Result: Pan-cancer clustering largely mirrors tissue of origin (Cramer's V=0.887). Within KIRC (n=146), k=5 best under the discovery rule, yielding a rare cluster C0 (6.85% of samples) that is highly stable (Jaccard=0.787). Silhouette=0.129; DBI=2.045. Differential expression identifies coherent markers for clusters.

Conclusion: Stability-aware, within-cancer clustering can uncover rare, reproducible subtypes that pan-cancer analyses miss due to tissue-origin dominance.

Abstract: Unsupervised learning on high-dimensional RNA-seq data can reveal molecular subtypes beyond standard labels. We combine an autoencoder-based representation with clustering and stability analysis to search for rare but reproducible genomic subtypes. On the UCI "Gene Expression Cancer RNA-Seq" dataset (801 samples, 20,531 genes; BRCA, COAD, KIRC, LUAD, PRAD), a pan-cancer analysis shows clusters aligning almost perfectly with tissue of origin (Cramer's V = 0.887), serving as a negative control. We therefore reframe the problem within KIRC (n = 146): we select the top 2,000 highly variable genes, standardize them, train a feed-forward autoencoder (128-dimensional latent space), and run k-means for k = 2-10. While global indices favor small k, scanning k with a pre-specified discovery rule (rare < 10 percent and stable with Jaccard >= 0.60 across 20 seeds after Hungarian alignment) yields a simple solution at k = 5 (silhouette = 0.129, DBI = 2.045) with a rare cluster C0 (6.85 percent of patients) that is highly stable (Jaccard = 0.787). Cluster-vs-rest differential expression (Welch's t-test, Benjamini-Hochberg FDR) identifies coherent markers. Overall, pan-cancer clustering is dominated by tissue of origin, whereas a stability-aware within-cancer approach reveals a rare, reproducible KIRC subtype.

</details>
